{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:14.141061Z",
     "start_time": "2020-12-03T07:37:14.133519Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:14.151012Z",
     "start_time": "2020-12-03T07:37:14.143217Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 10000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 10  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "fixed_seed_lambda_training = True\n",
    "initialize_network_zero = False\n",
    "initialize_network_one = False\n",
    "initialize_network_fixed_random = True\n",
    "\n",
    "\n",
    "n_jobs = -5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:14.163204Z",
     "start_time": "2020-12-03T07:37:14.152739Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if each_epochs_save != None:\n",
    "    epochs_save_range = range(1, epochs//each_epochs_save+1) if each_epochs_save == 1 else range(epochs//each_epochs_save+1)\n",
    "else:\n",
    "    epochs_save_range = None\n",
    "    \n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_SeedMethod'\n",
    "elif not fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_NoSeedMethod'\n",
    "    \n",
    "if initialize_network_zero:\n",
    "    seed_shuffle_string = seed_shuffle_string + '_zero_initialize'\n",
    "\n",
    "if initialize_network_one:\n",
    "    seed_shuffle_string = seed_shuffle_string + '_ones_initialize'   \n",
    "\n",
    "if initialize_network_fixed_random:\n",
    "    seed_shuffle_string = seed_shuffle_string + '_fixedRandom_initialize'   \n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:18.490953Z",
     "start_time": "2020-12-03T07:37:14.165421Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:18.498890Z",
     "start_time": "2020-12-03T07:37:18.493760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:18.516455Z",
     "start_time": "2020-12-03T07:37:18.500672Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "    \n",
    "    \n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:18.523411Z",
     "start_time": "2020-12-03T07:37:18.518183Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcualate_function_value_with_X_data_entry(coefficient_list, X_data_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "     \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [X_data_value**int(coefficient_multiplier) for coefficient_multiplier, X_data_value in zip(coefficient_multipliers, X_data_entry)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "        \n",
    "    return result, np.append(X_data_entry, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:18.559185Z",
     "start_time": "2020-12-03T07:37:18.525150Z"
    },
    "code_folding": [
     0,
     20,
     43,
     66,
     88,
     91,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:18.618865Z",
     "start_time": "2020-12-03T07:37:18.560929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d39924fe3d446c6ae1c8928bd3378e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6de6790cb124fa7a3de218d89fe8b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:28.324418Z",
     "start_time": "2020-12-03T07:37:18.620397Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:28.492071Z",
     "start_time": "2020-12-03T07:37:28.327577Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        \n",
    "        for i in epochs_save_range:\n",
    "            index = i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:28.516297Z",
     "start_time": "2020-12-03T07:37:28.496370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.480</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.830</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.420  0.820 -0.680 -0.640\n",
       "1 -0.980 -0.820  0.060 -0.300\n",
       "2 -0.480  0.000 -0.370  0.600\n",
       "3 -0.250  0.630 -0.480 -0.960\n",
       "4 -0.830  0.230 -0.170 -0.460"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:28.526766Z",
     "start_time": "2020-12-03T07:37:28.518194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.610</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.800</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.950</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0  0.610  0.220 -0.980  0.170\n",
       "1 -0.230 -0.050 -0.900  0.600\n",
       "2  0.800 -0.540  0.550  0.440\n",
       "3  0.030 -0.600 -0.320 -0.430\n",
       "4  0.950  0.770 -0.250  0.530"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:28.533457Z",
     "start_time": "2020-12-03T07:37:28.528654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:28.539871Z",
     "start_time": "2020-12-03T07:37:28.535187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:28.584986Z",
     "start_time": "2020-12-03T07:37:28.541707Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lambda_net(identifier, \n",
    "                        X_data_real_lambda, \n",
    "                        y_data_real_lambda, \n",
    "                        y_data_pred_lambda, \n",
    "                        y_data_pred_lambda_poly_lstsq, \n",
    "                        y_data_real_lambda_poly_lstsq):\n",
    "    \n",
    "    mae_real_VS_predLambda = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    mae_predLambda_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_realPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    rmse_real_VS_predLambda = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    rmse_predLambda_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_realPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    mape_real_VS_predLambda = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    mape_predLambda_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_realPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)            \n",
    "\n",
    "    r2_real_VS_predLambda = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_predPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    r2_predLambda_VS_predPolyLstsq = np.round(r2_score(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_realPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    raae_real_VS_predLambda = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    raae_predLambda_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_realPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    rmae_real_VS_predLambda = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    rmae_predLambda_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_realPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    fd_real_VS_predLambda = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_predLambda_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_realPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "    dtw_real_VS_predLambda, dtw_complete_real_VS_predLambda = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predLambda = np.round(dtw_real_VS_predLambda, 4)\n",
    "    dtw_real_VS_predPolyLstsq, dtw_complete_real_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predPolyLstsq = np.round(dtw_real_VS_predPolyLstsq, 4)\n",
    "    dtw_predLambda_VS_predPolyLstsq, dtw_complete_predLambda_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_predLambda_VS_predPolyLstsq = np.round(dtw_predLambda_VS_predPolyLstsq, 4)    \n",
    "    dtw_real_VS_realPolyLstsq, dtw_complete_real_VS_realPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_realPolyLstsq = np.round(dtw_real_VS_realPolyLstsq, 4) \n",
    "        \n",
    "    std_data_real_lambda = np.round(np.std(y_data_real_lambda), 4) \n",
    "    std_data_pred_lambda = np.round(np.std(y_data_pred_lambda), 4) \n",
    "    std_data_pred_lambda_poly_lstsq = np.round(np.std(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    std_data_real_lambda_poly_lstsq = np.round(np.std(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    mean_data_real_lambda = np.round(np.mean(y_data_real_lambda), 4) \n",
    "    mean_data_pred_lambda = np.round(np.mean(y_data_pred_lambda), 4) \n",
    "    mean_data_pred_lambda_poly_lstsq = np.round(np.mean(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    mean_data_real_lambda_poly_lstsq = np.round(np.mean(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    return [{\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mae_real_VS_predLambda,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_real_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_predLambda_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mae_real_VS_realPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmse_real_VS_predLambda,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_real_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_predLambda_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmse_real_VS_realPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mape_real_VS_predLambda,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_real_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_predLambda_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mape_real_VS_realPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': r2_real_VS_predLambda,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_real_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_predLambda_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': r2_real_VS_realPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': raae_real_VS_predLambda,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_real_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_predLambda_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': raae_real_VS_realPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmae_real_VS_predLambda,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_real_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_predLambda_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmae_real_VS_realPolyLstsq,\n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': fd_real_VS_predLambda,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_real_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_predLambda_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': fd_real_VS_realPolyLstsq,   \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': dtw_real_VS_predLambda, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_real_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_predLambda_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': dtw_real_VS_realPolyLstsq, \n",
    "            },\n",
    "            {\n",
    "             'STD FV ' + identifier + ' REAL LAMBDA': std_data_real_lambda,\n",
    "             'STD FV ' + identifier + ' PRED LAMBDA': std_data_pred_lambda, \n",
    "             'STD FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': std_data_pred_lambda_poly_lstsq, \n",
    "             'STD FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': std_data_real_lambda_poly_lstsq, \n",
    "            },\n",
    "            {\n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA': mean_data_real_lambda,\n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA': mean_data_pred_lambda, \n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': mean_data_pred_lambda_poly_lstsq, \n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': mean_data_real_lambda_poly_lstsq, \n",
    "            }]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:28.591250Z",
     "start_time": "2020-12-03T07:37:28.586846Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_function_values_from_polynomial(X_data, polynomial):\n",
    "    function_value_list = []\n",
    "    for entry in X_data:\n",
    "        function_value, _ = calcualate_function_value_with_X_data_entry(polynomial, entry)\n",
    "        function_value_list.append(function_value)\n",
    "    function_value_array = np.array(function_value_list).reshape(len(function_value_list), 1)     \n",
    "\n",
    "    return function_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:28.599556Z",
     "start_time": "2020-12-03T07:37:28.593057Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_term_matric_for_lstsq(X_data, polynomial_indices):\n",
    "    term_list_all = []\n",
    "    y = 0\n",
    "    for term in list(polynomial_indices):\n",
    "        term_list = [int(value_mult) for value_mult in term]\n",
    "        term_list_all.append(term_list)\n",
    "    terms_matrix = []\n",
    "    for unknowns in X_data:\n",
    "        terms = []\n",
    "        for term_multipliers in term_list_all:\n",
    "            term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "            terms.append(term_value)\n",
    "        terms_matrix.append(np.array(terms))\n",
    "    terms_matrix = np.array(terms_matrix)\n",
    "    \n",
    "    return terms_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:28.666193Z",
     "start_time": "2020-12-03T07:37:28.601394Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn(X_data_lambda, y_data_real_lambda, polynomial, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    if fixed_seed_lambda_training:\n",
    "        random.seed(RANDOM_SEED)\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        if int(tf.__version__[0]) >= 2:\n",
    "            tf.random.set_seed(RANDOM_SEED)\n",
    "        else:\n",
    "            tf.set_random_seed(RANDOM_SEED) \n",
    "        \n",
    "    if isinstance(X_data_lambda, pd.DataFrame):\n",
    "        X_data_lambda = X_data_lambda.values\n",
    "    if isinstance(y_data_real_lambda, pd.DataFrame):\n",
    "        y_data_real_lambda = y_data_real_lambda.values\n",
    "                \n",
    "    X_train_lambda_with_valid, X_test_lambda, y_train_real_lambda_with_valid, y_test_real_lambda = train_test_split(X_data_lambda, y_data_real_lambda, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    X_train_lambda, X_valid_lambda, y_train_real_lambda, y_valid_real_lambda = train_test_split(X_train_lambda_with_valid, y_train_real_lambda_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "     \n",
    "        \n",
    "    model = Sequential()\n",
    "\n",
    "    if initialize_network_one:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer='ones', bias_initializer='ones')) #1024\n",
    "    elif initialize_network_zero:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer='zeros', bias_initializer='zeros')) #1024\n",
    "    elif initialize_network_fixed_random:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED), bias_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED))) #1024\n",
    "    else:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1])) #1024\n",
    "        \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        if initialize_network_one:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer='ones', bias_initializer='ones'))\n",
    "        elif initialize_network_zero:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer='zeros', bias_initializer='zeros'))\n",
    "        elif initialize_network_fixed_random:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED), bias_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED)))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "    \n",
    "    if initialize_network_one:\n",
    "        model.add(Dense(1, kernel_initializer='ones', bias_initializer='ones'))\n",
    "    if initialize_network_zero:\n",
    "        model.add(Dense(1, kernel_initializer='zeros', bias_initializer='zeros'))\n",
    "    if initialize_network_fixed_random:\n",
    "        model.add(Dense(1, kernel_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED), bias_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED)))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_lstsq_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_train_pred_lambda = model.predict(X_train_lambda) \n",
    "        y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "        y_test_pred_lambda = model.predict(X_test_lambda)\n",
    "    \n",
    "        terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                \n",
    "        polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "        y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "        y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "        y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)    \n",
    "        y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)\n",
    "        y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)  \n",
    "        \n",
    "        pred_list = (y_train_real_lambda, \n",
    "                     y_train_pred_lambda, \n",
    "                     y_train_pred_lambda_poly_lstsq,\n",
    "                     #y_train_real_lambda_poly_lstsq,\n",
    "                     X_train_lambda, \n",
    "                     y_valid_real_lambda,\n",
    "                     y_valid_pred_lambda, \n",
    "                     y_valid_pred_lambda_poly_lstsq,\n",
    "                     #y_valid_real_lambda_poly_lstsq,\n",
    "                     X_valid_lambda, \n",
    "                     y_test_real_lambda, \n",
    "                     y_test_pred_lambda, \n",
    "                     y_test_pred_lambda_poly_lstsq, \n",
    "                     #y_test_real_lambda_poly_lstsq,\n",
    "                     X_test_lambda)\n",
    "\n",
    "        scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "        scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "        scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "        scores_std = {}\n",
    "        for aDict in (std_train, std_valid, std_test):\n",
    "            scores_std.update(aDict)      \n",
    "        scores_mean = {}\n",
    "        for aDict in (mean_train, mean_valid, mean_test):\n",
    "            scores_mean.update(aDict)\n",
    "        \n",
    "        scores_list =  [scores_train,\n",
    "                             scores_valid,\n",
    "                             scores_test,\n",
    "                             scores_std,\n",
    "                             scores_mean]            \n",
    "                            \n",
    "    else:\n",
    "        scores_list = []\n",
    "        pred_list = []\n",
    "        for i in epochs_save_range:\n",
    "            train_epochs_step = each_epochs_save if i > 1 else max(each_epochs_save-1, 1) if i==1 else 1\n",
    "            \n",
    "            model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=train_epochs_step, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=1,\n",
    "                      use_multiprocessing=False)\n",
    "            \n",
    "            #history adjustment for continuing training\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                history = model_history.history\n",
    "            else:\n",
    "                history = mergeDict(history, model_history.history)\n",
    "                #for key_1 in history.keys():\n",
    "                #    for key_2 in model_history.history.keys():\n",
    "                #        if key_1 == key_2:\n",
    "                #            history[key_1] += model_history.history[key_2]  \n",
    "\n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_train_pred_lambda = model.predict(X_train_lambda)                \n",
    "            y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "            y_test_pred_lambda = model.predict(X_test_lambda)        \n",
    "\n",
    "            terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                        \n",
    "            polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            \n",
    "            y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "            y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "            y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)           \n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "                y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)  \n",
    "                y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)                    \n",
    "                \n",
    "            pred_list.append((y_train_real_lambda, \n",
    "                              y_train_pred_lambda, \n",
    "                              y_train_pred_lambda_poly_lstsq,\n",
    "                              #y_train_real_lambda_poly_lstsq,\n",
    "                              X_train_lambda, \n",
    "                              y_valid_real_lambda,\n",
    "                              y_valid_pred_lambda, \n",
    "                              y_valid_pred_lambda_poly_lstsq,\n",
    "                              #y_valid_real_lambda_poly_lstsq,\n",
    "                              X_valid_lambda, \n",
    "                              y_test_real_lambda, \n",
    "                              y_test_pred_lambda, \n",
    "                              y_test_pred_lambda_poly_lstsq, \n",
    "                              #y_test_real_lambda_poly_lstsq,\n",
    "                              X_test_lambda))\n",
    "    \n",
    "            scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "            scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "            scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "            scores_std = {}\n",
    "            for aDict in (std_train, std_valid, std_test):\n",
    "                scores_std.update(aDict)\n",
    "            scores_mean = {}\n",
    "            for aDict in (mean_train, mean_valid, mean_test):\n",
    "                scores_mean.update(aDict)\n",
    "\n",
    "            scores_list_single_epoch =  [scores_train,\n",
    "                                              scores_valid,\n",
    "                                              scores_test,\n",
    "                                              scores_std,\n",
    "                                              scores_mean]        \n",
    "                  \n",
    "            scores_list.append(scores_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_lstsq_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_lstsq_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save == None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                for i, value in enumerate(polynomial.values):\n",
    "                    if i == 0:\n",
    "                        text_file.write(str(value))  \n",
    "                    else:\n",
    "                        text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_lstsq_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, #polynomial_lstsq_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:24:42.117886Z",
     "start_time": "2020-12-03T07:37:28.667846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    5.018, max:    9.438, cur:    5.018)\n",
      "\tvalidation       \t (min:    4.997, max:    9.846, cur:    4.997)\n",
      "mean_absolute_percentage_error_keras\n",
      "\ttraining         \t (min:    1.002, max:    2.463, cur:    1.315)\n",
      "\tvalidation       \t (min:    0.807, max:    1.095, cur:    1.007)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    6.908, max:   12.554, cur:    6.908)\n",
      "\tvalidation       \t (min:    6.448, max:   12.600, cur:    6.448)\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(X_data[1].values, y_data[1].values, X_data[0], return_history=True, each_epochs_save=each_epochs_save, printing=True) for X_data, y_data in zip(X_data_list_split, y_data_list_split))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:24:54.633489Z",
     "start_time": "2020-12-03T12:24:42.119869Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    scores_list_train = [clf[1][0] for clf in clf_list]\n",
    "    scores_list_valid = [clf[1][1] for clf in clf_list]\n",
    "    scores_list_test = [clf[1][2] for clf in clf_list]\n",
    "    scores_list_stds = [clf[1][3] for clf in clf_list]\n",
    "    scores_list_means = [clf[1][4] for clf in clf_list]\n",
    "\n",
    "    scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_list_by_epochs = [[] for i in epochs_save_range]\n",
    "    for scores_list in scores_list:   \n",
    "        for index, scores in enumerate(scores_list):\n",
    "            scores_list_by_epochs[index].append(scores)\n",
    "            \n",
    "        \n",
    "    for i, scores_list_single_epoch in enumerate(scores_list_by_epochs):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "           \n",
    "        scores_list_train = [scores_list[0] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_valid = [scores_list[1] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_test = [scores_list[2] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_stds = [scores_list[3] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_means = [scores_list[4] for scores_list in scores_list_single_epoch]\n",
    "        \n",
    "        scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()  \n",
    "        scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()  \n",
    "        scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:24:54.658097Z",
     "start_time": "2020-12-03T12:24:54.635296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E1</th>\n",
       "      <th>TRAIN POLY E1</th>\n",
       "      <th>TRAIN POLY PRED E1</th>\n",
       "      <th>TRAIN LSTSQ E1</th>\n",
       "      <th>TRAIN PRED E10</th>\n",
       "      <th>TRAIN POLY E10</th>\n",
       "      <th>TRAIN POLY PRED E10</th>\n",
       "      <th>TRAIN LSTSQ E10</th>\n",
       "      <th>TRAIN PRED E20</th>\n",
       "      <th>TRAIN POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E180</th>\n",
       "      <th>TRAIN LSTSQ E180</th>\n",
       "      <th>TRAIN PRED E190</th>\n",
       "      <th>TRAIN POLY E190</th>\n",
       "      <th>TRAIN POLY PRED E190</th>\n",
       "      <th>TRAIN LSTSQ E190</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.217</td>\n",
       "      <td>10.217</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.997</td>\n",
       "      <td>9.997</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.718</td>\n",
       "      <td>9.718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.225</td>\n",
       "      <td>5.222</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.167</td>\n",
       "      <td>5.165</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>13.016</td>\n",
       "      <td>13.016</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.798</td>\n",
       "      <td>12.798</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.522</td>\n",
       "      <td>12.522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.248</td>\n",
       "      <td>7.235</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.170</td>\n",
       "      <td>7.157</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.050</td>\n",
       "      <td>1.049</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.447</td>\n",
       "      <td>1.447</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.944</td>\n",
       "      <td>1.945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.462</td>\n",
       "      <td>4.256</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.416</td>\n",
       "      <td>4.337</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.924</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.472</td>\n",
       "      <td>3.472</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.468</td>\n",
       "      <td>3.468</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.453</td>\n",
       "      <td>3.453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.645</td>\n",
       "      <td>2.642</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.627</td>\n",
       "      <td>2.621</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>25.196</td>\n",
       "      <td>25.196</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.890</td>\n",
       "      <td>24.890</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.484</td>\n",
       "      <td>24.484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.520</td>\n",
       "      <td>13.467</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.349</td>\n",
       "      <td>13.293</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.364</td>\n",
       "      <td>102.364</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.149</td>\n",
       "      <td>100.149</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.355</td>\n",
       "      <td>97.356</td>\n",
       "      <td>...</td>\n",
       "      <td>2.717</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.304</td>\n",
       "      <td>50.255</td>\n",
       "      <td>2.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.723</td>\n",
       "      <td>49.691</td>\n",
       "      <td>2.906</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E1  TRAIN POLY E1  TRAIN POLY PRED E1  TRAIN LSTSQ E1  \\\n",
       "MAE FV          10.217         10.217               0.001           0.000   \n",
       "RMSE FV         13.016         13.016               0.001           0.000   \n",
       "MAPE FV          1.050          1.049               0.087           0.000   \n",
       "R2 FV           -0.419         -0.419               1.000           1.000   \n",
       "RAAE FV          0.924          0.924               0.014           0.000   \n",
       "RMAE FV          3.472          3.472               0.065           0.000   \n",
       "FD FV           25.196         25.196               0.001           0.000   \n",
       "DTW FV         102.364        102.364               0.005           0.000   \n",
       "\n",
       "         TRAIN PRED E10  TRAIN POLY E10  TRAIN POLY PRED E10  TRAIN LSTSQ E10  \\\n",
       "MAE FV            9.997           9.997                0.001            0.000   \n",
       "RMSE FV          12.798          12.798                0.001            0.000   \n",
       "MAPE FV           1.447           1.447                0.014            0.000   \n",
       "R2 FV            -0.363          -0.363                0.999            1.000   \n",
       "RAAE FV           0.902           0.902                0.018            0.000   \n",
       "RMAE FV           3.468           3.468                0.085            0.000   \n",
       "FD FV            24.890          24.890                0.002            0.000   \n",
       "DTW FV          100.149         100.149                0.007            0.000   \n",
       "\n",
       "         TRAIN PRED E20  TRAIN POLY E20  ...  TRAIN POLY PRED E180  \\\n",
       "MAE FV            9.718           9.718  ...                 0.271   \n",
       "RMSE FV          12.522          12.522  ...                 0.362   \n",
       "MAPE FV           1.944           1.945  ...                 0.484   \n",
       "R2 FV            -0.293          -0.293  ...                 0.997   \n",
       "RAAE FV           0.876           0.876  ...                 0.038   \n",
       "RMAE FV           3.453           3.453  ...                 0.240   \n",
       "FD FV            24.484          24.484  ...                 0.713   \n",
       "DTW FV           97.355          97.356  ...                 2.717   \n",
       "\n",
       "         TRAIN LSTSQ E180  TRAIN PRED E190  TRAIN POLY E190  \\\n",
       "MAE FV              0.000            5.225            5.222   \n",
       "RMSE FV             0.000            7.248            7.235   \n",
       "MAPE FV             0.000            4.462            4.256   \n",
       "R2 FV               1.000            0.540            0.541   \n",
       "RAAE FV             0.000            0.481            0.481   \n",
       "RMAE FV             0.000            2.645            2.642   \n",
       "FD FV               0.000           13.520           13.467   \n",
       "DTW FV              0.000           50.304           50.255   \n",
       "\n",
       "         TRAIN POLY PRED E190  TRAIN LSTSQ E190  TRAIN PRED E200  \\\n",
       "MAE FV                  0.281             0.000            5.167   \n",
       "RMSE FV                 0.372             0.000            7.170   \n",
       "MAPE FV                 0.439             0.000            4.416   \n",
       "R2 FV                   0.997             1.000            0.549   \n",
       "RAAE FV                 0.039             0.000            0.476   \n",
       "RMAE FV                 0.239             0.000            2.627   \n",
       "FD FV                   0.729             0.000           13.349   \n",
       "DTW FV                  2.810             0.000           49.723   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             5.165                 0.290             0.000  \n",
       "RMSE FV            7.157                 0.382             0.000  \n",
       "MAPE FV            4.337                 0.439             0.000  \n",
       "R2 FV              0.550                 0.997             1.000  \n",
       "RAAE FV            0.476                 0.040             0.000  \n",
       "RMAE FV            2.621                 0.238             0.000  \n",
       "FD FV             13.293                 0.745             0.000  \n",
       "DTW FV            49.691                 2.906             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:24:54.679899Z",
     "start_time": "2020-12-03T12:24:54.659617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E1</th>\n",
       "      <th>VALID POLY E1</th>\n",
       "      <th>VALID POLY PRED E1</th>\n",
       "      <th>VALID LSTSQ E1</th>\n",
       "      <th>VALID PRED E10</th>\n",
       "      <th>VALID POLY E10</th>\n",
       "      <th>VALID POLY PRED E10</th>\n",
       "      <th>VALID LSTSQ E10</th>\n",
       "      <th>VALID PRED E20</th>\n",
       "      <th>VALID POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E180</th>\n",
       "      <th>VALID LSTSQ E180</th>\n",
       "      <th>VALID PRED E190</th>\n",
       "      <th>VALID POLY E190</th>\n",
       "      <th>VALID POLY PRED E190</th>\n",
       "      <th>VALID LSTSQ E190</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.201</td>\n",
       "      <td>10.201</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.982</td>\n",
       "      <td>9.982</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.706</td>\n",
       "      <td>9.706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.315</td>\n",
       "      <td>5.303</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.259</td>\n",
       "      <td>5.249</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.976</td>\n",
       "      <td>12.976</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.760</td>\n",
       "      <td>12.760</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.485</td>\n",
       "      <td>12.485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.307</td>\n",
       "      <td>7.290</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.233</td>\n",
       "      <td>7.214</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.034</td>\n",
       "      <td>1.034</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.281</td>\n",
       "      <td>1.281</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.599</td>\n",
       "      <td>1.598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.902</td>\n",
       "      <td>4.917</td>\n",
       "      <td>2.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.892</td>\n",
       "      <td>4.901</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.928</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.031</td>\n",
       "      <td>3.031</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.027</td>\n",
       "      <td>3.027</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.012</td>\n",
       "      <td>3.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.254</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.238</td>\n",
       "      <td>2.231</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.834</td>\n",
       "      <td>24.834</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.529</td>\n",
       "      <td>24.529</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.123</td>\n",
       "      <td>24.123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.249</td>\n",
       "      <td>13.184</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.094</td>\n",
       "      <td>13.027</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>101.668</td>\n",
       "      <td>101.668</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.465</td>\n",
       "      <td>99.465</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>96.693</td>\n",
       "      <td>96.693</td>\n",
       "      <td>...</td>\n",
       "      <td>2.906</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.342</td>\n",
       "      <td>50.243</td>\n",
       "      <td>3.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.807</td>\n",
       "      <td>49.719</td>\n",
       "      <td>3.101</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E1  VALID POLY E1  VALID POLY PRED E1  VALID LSTSQ E1  \\\n",
       "MAE FV          10.201         10.201               0.001           0.000   \n",
       "RMSE FV         12.976         12.976               0.001           0.000   \n",
       "MAPE FV          1.034          1.034               0.086           0.000   \n",
       "R2 FV           -0.427         -0.427               1.000           1.000   \n",
       "RAAE FV          0.928          0.928               0.015           0.000   \n",
       "RMAE FV          3.031          3.031               0.064           0.000   \n",
       "FD FV           24.834         24.834               0.002           0.000   \n",
       "DTW FV         101.668        101.668               0.006           0.000   \n",
       "\n",
       "         VALID PRED E10  VALID POLY E10  VALID POLY PRED E10  VALID LSTSQ E10  \\\n",
       "MAE FV            9.982           9.982                0.001            0.000   \n",
       "RMSE FV          12.760          12.760                0.001            0.000   \n",
       "MAPE FV           1.281           1.281                0.021            0.000   \n",
       "R2 FV            -0.371          -0.371                0.999            1.000   \n",
       "RAAE FV           0.906           0.906                0.020            0.000   \n",
       "RMAE FV           3.027           3.027                0.081            0.000   \n",
       "FD FV            24.529          24.529                0.002            0.000   \n",
       "DTW FV           99.465          99.465                0.008            0.000   \n",
       "\n",
       "         VALID PRED E20  VALID POLY E20  ...  VALID POLY PRED E180  \\\n",
       "MAE FV            9.706           9.706  ...                 0.292   \n",
       "RMSE FV          12.485          12.485  ...                 0.396   \n",
       "MAPE FV           1.599           1.598  ...                 0.445   \n",
       "R2 FV            -0.300          -0.300  ...                 0.996   \n",
       "RAAE FV           0.880           0.880  ...                 0.041   \n",
       "RMAE FV           3.012           3.012  ...                 0.206   \n",
       "FD FV            24.123          24.123  ...                 0.765   \n",
       "DTW FV           96.693          96.693  ...                 2.906   \n",
       "\n",
       "         VALID LSTSQ E180  VALID PRED E190  VALID POLY E190  \\\n",
       "MAE FV              0.000            5.315            5.303   \n",
       "RMSE FV             0.000            7.307            7.290   \n",
       "MAPE FV             0.000            4.902            4.917   \n",
       "R2 FV               1.000            0.528            0.529   \n",
       "RAAE FV             0.000            0.492            0.491   \n",
       "RMAE FV             0.000            2.254            2.250   \n",
       "FD FV               0.000           13.249           13.184   \n",
       "DTW FV              0.000           50.342           50.243   \n",
       "\n",
       "         VALID POLY PRED E190  VALID LSTSQ E190  VALID PRED E200  \\\n",
       "MAE FV                  0.301             0.000            5.259   \n",
       "RMSE FV                 0.406             0.000            7.233   \n",
       "MAPE FV                 2.055             0.000            4.892   \n",
       "R2 FV                   0.996             1.000            0.536   \n",
       "RAAE FV                 0.042             0.000            0.487   \n",
       "RMAE FV                 0.206             0.000            2.238   \n",
       "FD FV                   0.782             0.000           13.094   \n",
       "DTW FV                  3.002             0.000           49.807   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             5.249                 0.311             0.000  \n",
       "RMSE FV            7.214                 0.417             0.000  \n",
       "MAPE FV            4.901                 0.571             0.000  \n",
       "R2 FV              0.538                 0.996             1.000  \n",
       "RAAE FV            0.486                 0.043             0.000  \n",
       "RMAE FV            2.231                 0.206             0.000  \n",
       "FD FV             13.027                 0.801             0.000  \n",
       "DTW FV            49.719                 3.101             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:24:54.702565Z",
     "start_time": "2020-12-03T12:24:54.681467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E1</th>\n",
       "      <th>TEST POLY E1</th>\n",
       "      <th>TEST POLY PRED E1</th>\n",
       "      <th>TEST LSTSQ E1</th>\n",
       "      <th>TEST PRED E10</th>\n",
       "      <th>TEST POLY E10</th>\n",
       "      <th>TEST POLY PRED E10</th>\n",
       "      <th>TEST LSTSQ E10</th>\n",
       "      <th>TEST PRED E20</th>\n",
       "      <th>TEST POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E180</th>\n",
       "      <th>TEST LSTSQ E180</th>\n",
       "      <th>TEST PRED E190</th>\n",
       "      <th>TEST POLY E190</th>\n",
       "      <th>TEST POLY PRED E190</th>\n",
       "      <th>TEST LSTSQ E190</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.213</td>\n",
       "      <td>10.213</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.994</td>\n",
       "      <td>9.994</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.717</td>\n",
       "      <td>9.717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.314</td>\n",
       "      <td>5.302</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.258</td>\n",
       "      <td>5.247</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.991</td>\n",
       "      <td>12.991</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.774</td>\n",
       "      <td>12.774</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.499</td>\n",
       "      <td>12.499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.308</td>\n",
       "      <td>7.290</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.234</td>\n",
       "      <td>7.214</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.080</td>\n",
       "      <td>1.080</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.630</td>\n",
       "      <td>1.630</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.392</td>\n",
       "      <td>2.391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.697</td>\n",
       "      <td>6.827</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.747</td>\n",
       "      <td>6.899</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.928</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.150</td>\n",
       "      <td>3.150</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.146</td>\n",
       "      <td>3.147</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.132</td>\n",
       "      <td>3.132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.363</td>\n",
       "      <td>2.357</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.338</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>25.014</td>\n",
       "      <td>25.014</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.708</td>\n",
       "      <td>24.708</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.302</td>\n",
       "      <td>24.302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.395</td>\n",
       "      <td>13.330</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.232</td>\n",
       "      <td>13.167</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.284</td>\n",
       "      <td>102.284</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.075</td>\n",
       "      <td>100.075</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.294</td>\n",
       "      <td>97.294</td>\n",
       "      <td>...</td>\n",
       "      <td>2.906</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.727</td>\n",
       "      <td>50.592</td>\n",
       "      <td>3.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.186</td>\n",
       "      <td>50.058</td>\n",
       "      <td>3.100</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E1  TEST POLY E1  TEST POLY PRED E1  TEST LSTSQ E1  \\\n",
       "MAE FV         10.213        10.213              0.001          0.000   \n",
       "RMSE FV        12.991        12.991              0.001          0.000   \n",
       "MAPE FV         1.080         1.080              0.100          0.000   \n",
       "R2 FV          -0.428        -0.428              1.000          1.000   \n",
       "RAAE FV         0.928         0.928              0.015          0.000   \n",
       "RMAE FV         3.150         3.150              0.066          0.000   \n",
       "FD FV          25.014        25.014              0.002          0.000   \n",
       "DTW FV        102.284       102.284              0.006          0.000   \n",
       "\n",
       "         TEST PRED E10  TEST POLY E10  TEST POLY PRED E10  TEST LSTSQ E10  \\\n",
       "MAE FV           9.994          9.994               0.001           0.000   \n",
       "RMSE FV         12.774         12.774               0.001           0.000   \n",
       "MAPE FV          1.630          1.630               0.012           0.000   \n",
       "R2 FV           -0.372         -0.372               0.999           1.000   \n",
       "RAAE FV          0.907          0.907               0.020           0.000   \n",
       "RMAE FV          3.146          3.147               0.085           0.000   \n",
       "FD FV           24.708         24.708               0.002           0.000   \n",
       "DTW FV         100.075        100.075               0.008           0.000   \n",
       "\n",
       "         TEST PRED E20  TEST POLY E20  ...  TEST POLY PRED E180  \\\n",
       "MAE FV           9.717          9.717  ...                0.292   \n",
       "RMSE FV         12.499         12.499  ...                0.396   \n",
       "MAPE FV          2.392          2.391  ...                0.597   \n",
       "R2 FV           -0.301         -0.301  ...                0.996   \n",
       "RAAE FV          0.880          0.880  ...                0.041   \n",
       "RMAE FV          3.132          3.132  ...                0.222   \n",
       "FD FV           24.302         24.302  ...                0.770   \n",
       "DTW FV          97.294         97.294  ...                2.906   \n",
       "\n",
       "         TEST LSTSQ E180  TEST PRED E190  TEST POLY E190  TEST POLY PRED E190  \\\n",
       "MAE FV             0.000           5.314           5.302                0.301   \n",
       "RMSE FV            0.000           7.308           7.290                0.406   \n",
       "MAPE FV            0.000           6.697           6.827                0.729   \n",
       "R2 FV              1.000           0.529           0.531                0.996   \n",
       "RAAE FV            0.000           0.491           0.490                0.042   \n",
       "RMAE FV            0.000           2.363           2.357                0.222   \n",
       "FD FV              0.000          13.395          13.330                0.787   \n",
       "DTW FV             0.000          50.727          50.592                3.002   \n",
       "\n",
       "         TEST LSTSQ E190  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           5.258           5.247                0.311   \n",
       "RMSE FV            0.000           7.234           7.214                0.417   \n",
       "MAPE FV            0.000           6.747           6.899                0.487   \n",
       "R2 FV              1.000           0.538           0.540                0.996   \n",
       "RAAE FV            0.000           0.486           0.485                0.043   \n",
       "RMAE FV            0.000           2.346           2.338                0.222   \n",
       "FD FV              0.000          13.232          13.167                0.806   \n",
       "DTW FV             0.000          50.186          50.058                3.100   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "FD FV              0.000  \n",
       "DTW FV             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:24:54.726539Z",
     "start_time": "2020-12-03T12:24:54.704264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA</th>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>...</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.826</td>\n",
       "      <td>1.378</td>\n",
       "      <td>2.029</td>\n",
       "      <td>2.732</td>\n",
       "      <td>3.430</td>\n",
       "      <td>...</td>\n",
       "      <td>4.719</td>\n",
       "      <td>5.284</td>\n",
       "      <td>5.773</td>\n",
       "      <td>6.175</td>\n",
       "      <td>6.497</td>\n",
       "      <td>6.749</td>\n",
       "      <td>6.948</td>\n",
       "      <td>7.106</td>\n",
       "      <td>7.235</td>\n",
       "      <td>7.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.826</td>\n",
       "      <td>1.377</td>\n",
       "      <td>2.028</td>\n",
       "      <td>2.730</td>\n",
       "      <td>3.426</td>\n",
       "      <td>...</td>\n",
       "      <td>4.713</td>\n",
       "      <td>5.277</td>\n",
       "      <td>5.765</td>\n",
       "      <td>6.167</td>\n",
       "      <td>6.487</td>\n",
       "      <td>6.739</td>\n",
       "      <td>6.938</td>\n",
       "      <td>7.096</td>\n",
       "      <td>7.224</td>\n",
       "      <td>7.331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>...</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA</th>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>...</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.820</td>\n",
       "      <td>1.367</td>\n",
       "      <td>2.014</td>\n",
       "      <td>2.713</td>\n",
       "      <td>3.405</td>\n",
       "      <td>...</td>\n",
       "      <td>4.687</td>\n",
       "      <td>5.249</td>\n",
       "      <td>5.735</td>\n",
       "      <td>6.136</td>\n",
       "      <td>6.456</td>\n",
       "      <td>6.707</td>\n",
       "      <td>6.905</td>\n",
       "      <td>7.062</td>\n",
       "      <td>7.190</td>\n",
       "      <td>7.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.820</td>\n",
       "      <td>1.367</td>\n",
       "      <td>2.013</td>\n",
       "      <td>2.710</td>\n",
       "      <td>3.402</td>\n",
       "      <td>...</td>\n",
       "      <td>4.681</td>\n",
       "      <td>5.243</td>\n",
       "      <td>5.728</td>\n",
       "      <td>6.128</td>\n",
       "      <td>6.447</td>\n",
       "      <td>6.698</td>\n",
       "      <td>6.895</td>\n",
       "      <td>7.052</td>\n",
       "      <td>7.179</td>\n",
       "      <td>7.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>...</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA</th>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>...</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.821</td>\n",
       "      <td>1.368</td>\n",
       "      <td>2.015</td>\n",
       "      <td>2.714</td>\n",
       "      <td>3.407</td>\n",
       "      <td>...</td>\n",
       "      <td>4.689</td>\n",
       "      <td>5.252</td>\n",
       "      <td>5.739</td>\n",
       "      <td>6.141</td>\n",
       "      <td>6.461</td>\n",
       "      <td>6.713</td>\n",
       "      <td>6.911</td>\n",
       "      <td>7.068</td>\n",
       "      <td>7.196</td>\n",
       "      <td>7.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.820</td>\n",
       "      <td>1.368</td>\n",
       "      <td>2.014</td>\n",
       "      <td>2.712</td>\n",
       "      <td>3.404</td>\n",
       "      <td>...</td>\n",
       "      <td>4.684</td>\n",
       "      <td>5.246</td>\n",
       "      <td>5.732</td>\n",
       "      <td>6.133</td>\n",
       "      <td>6.453</td>\n",
       "      <td>6.704</td>\n",
       "      <td>6.902</td>\n",
       "      <td>7.059</td>\n",
       "      <td>7.186</td>\n",
       "      <td>7.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>...</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        E1    E10    E20    E30    E40    E50  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.169 11.169 11.169 11.169 11.169 11.169   \n",
       "STD FV TRAIN PRED LAMBDA             0.038  0.047  0.080  0.175  0.413  0.826   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  0.038  0.047  0.080  0.175  0.413  0.826   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.169 11.169 11.169 11.169 11.169 11.169   \n",
       "STD FV VALID REAL LAMBDA            11.114 11.114 11.114 11.114 11.114 11.114   \n",
       "STD FV VALID PRED LAMBDA             0.038  0.047  0.080  0.174  0.410  0.820   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  0.038  0.047  0.080  0.174  0.410  0.820   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.114 11.114 11.114 11.114 11.114 11.114   \n",
       "STD FV TEST REAL LAMBDA             11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "STD FV TEST PRED LAMBDA              0.038  0.047  0.080  0.174  0.410  0.821   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   0.038  0.047  0.080  0.174  0.410  0.820   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "\n",
       "                                       E60    E70    E80    E90  ...   E110  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.169 11.169 11.169 11.169  ... 11.169   \n",
       "STD FV TRAIN PRED LAMBDA             1.378  2.029  2.732  3.430  ...  4.719   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  1.377  2.028  2.730  3.426  ...  4.713   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.169 11.169 11.169 11.169  ... 11.169   \n",
       "STD FV VALID REAL LAMBDA            11.114 11.114 11.114 11.114  ... 11.114   \n",
       "STD FV VALID PRED LAMBDA             1.367  2.014  2.713  3.405  ...  4.687   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  1.367  2.013  2.710  3.402  ...  4.681   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.114 11.114 11.114 11.114  ... 11.114   \n",
       "STD FV TEST REAL LAMBDA             11.124 11.124 11.124 11.124  ... 11.124   \n",
       "STD FV TEST PRED LAMBDA              1.368  2.015  2.714  3.407  ...  4.689   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   1.368  2.014  2.712  3.404  ...  4.684   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.124 11.124 11.124 11.124  ... 11.124   \n",
       "\n",
       "                                      E120   E130   E140   E150   E160   E170  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.169 11.169 11.169 11.169 11.169 11.169   \n",
       "STD FV TRAIN PRED LAMBDA             5.284  5.773  6.175  6.497  6.749  6.948   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  5.277  5.765  6.167  6.487  6.739  6.938   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.169 11.169 11.169 11.169 11.169 11.169   \n",
       "STD FV VALID REAL LAMBDA            11.114 11.114 11.114 11.114 11.114 11.114   \n",
       "STD FV VALID PRED LAMBDA             5.249  5.735  6.136  6.456  6.707  6.905   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  5.243  5.728  6.128  6.447  6.698  6.895   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.114 11.114 11.114 11.114 11.114 11.114   \n",
       "STD FV TEST REAL LAMBDA             11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "STD FV TEST PRED LAMBDA              5.252  5.739  6.141  6.461  6.713  6.911   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   5.246  5.732  6.133  6.453  6.704  6.902   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "\n",
       "                                      E180   E190   E200  \n",
       "STD FV TRAIN REAL LAMBDA            11.169 11.169 11.169  \n",
       "STD FV TRAIN PRED LAMBDA             7.106  7.235  7.343  \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.096  7.224  7.331  \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.169 11.169 11.169  \n",
       "STD FV VALID REAL LAMBDA            11.114 11.114 11.114  \n",
       "STD FV VALID PRED LAMBDA             7.062  7.190  7.297  \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.052  7.179  7.285  \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.114 11.114 11.114  \n",
       "STD FV TEST REAL LAMBDA             11.124 11.124 11.124  \n",
       "STD FV TEST PRED LAMBDA              7.068  7.196  7.303  \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.059  7.186  7.293  \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.124 11.124 11.124  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:24:54.750671Z",
     "start_time": "2020-12-03T12:24:54.728296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.208</td>\n",
       "      <td>1.192</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.208</td>\n",
       "      <td>1.192</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA</th>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.188</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.188</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA</th>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.909</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.188</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.909</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.188</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         E1    E10    E20    E30    E40  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TRAIN PRED LAMBDA             0.039  0.058  0.161  0.458  0.912   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  0.039  0.058  0.161  0.458  0.912   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV VALID REAL LAMBDA            -0.083 -0.083 -0.083 -0.083 -0.083   \n",
       "MEAN FV VALID PRED LAMBDA             0.040  0.058  0.161  0.457  0.910   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  0.040  0.058  0.161  0.457  0.910   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.083 -0.083 -0.083 -0.083 -0.083   \n",
       "MEAN FV TEST REAL LAMBDA             -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "MEAN FV TEST PRED LAMBDA              0.039  0.058  0.161  0.457  0.909   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   0.039  0.058  0.161  0.457  0.909   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "\n",
       "                                        E50    E60    E70    E80    E90  ...  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA             1.208  1.192  0.966  0.705  0.484  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  1.208  1.192  0.966  0.705  0.484  ...   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078  ...   \n",
       "MEAN FV VALID REAL LAMBDA            -0.083 -0.083 -0.083 -0.083 -0.083  ...   \n",
       "MEAN FV VALID PRED LAMBDA             1.205  1.188  0.965  0.705  0.485  ...   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  1.204  1.188  0.964  0.705  0.486  ...   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.083 -0.083 -0.083 -0.083 -0.083  ...   \n",
       "MEAN FV TEST REAL LAMBDA             -0.080 -0.080 -0.080 -0.080 -0.080  ...   \n",
       "MEAN FV TEST PRED LAMBDA              1.205  1.188  0.965  0.705  0.486  ...   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   1.204  1.188  0.964  0.705  0.486  ...   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.080 -0.080 -0.080 -0.080 -0.080  ...   \n",
       "\n",
       "                                       E110   E120   E130   E140   E150  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TRAIN PRED LAMBDA             0.194  0.108  0.051  0.014 -0.011   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  0.194  0.108  0.051  0.014 -0.011   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV VALID REAL LAMBDA            -0.083 -0.083 -0.083 -0.083 -0.083   \n",
       "MEAN FV VALID PRED LAMBDA             0.195  0.110  0.053  0.015 -0.010   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  0.196  0.111  0.054  0.016 -0.009   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.083 -0.083 -0.083 -0.083 -0.083   \n",
       "MEAN FV TEST REAL LAMBDA             -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "MEAN FV TEST PRED LAMBDA              0.197  0.112  0.056  0.018 -0.007   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   0.197  0.113  0.056  0.019 -0.006   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "\n",
       "                                       E160   E170   E180   E190   E200  \n",
       "MEAN FV TRAIN REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.028 -0.042 -0.053 -0.062 -0.069  \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.028 -0.042 -0.053 -0.062 -0.069  \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "MEAN FV VALID REAL LAMBDA            -0.083 -0.083 -0.083 -0.083 -0.083  \n",
       "MEAN FV VALID PRED LAMBDA            -0.027 -0.041 -0.053 -0.061 -0.069  \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.026 -0.040 -0.052 -0.060 -0.068  \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.083 -0.083 -0.083 -0.083 -0.083  \n",
       "MEAN FV TEST REAL LAMBDA             -0.080 -0.080 -0.080 -0.080 -0.080  \n",
       "MEAN FV TEST PRED LAMBDA             -0.024 -0.037 -0.049 -0.058 -0.065  \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.023 -0.037 -0.048 -0.057 -0.064  \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.080 -0.080 -0.080 -0.080 -0.080  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Net Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:44:51.171476Z",
     "start_time": "2020-12-03T12:24:54.752502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a52c64b5a94b4796aa4c31b49da056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d77bad7baf4dbea44f8cbba67cf9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list] \n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list] \n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list] \n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "\n",
    "    y_train_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][0])))\n",
    "    y_train_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][1])))\n",
    "    y_train_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][2])))\n",
    "    X_train_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][3].shape)]][0])\n",
    "    y_valid_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][4])))\n",
    "    y_valid_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][5])))\n",
    "    y_valid_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][6])))\n",
    "    X_valid_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][7].shape)]][0])\n",
    "    y_test_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][8])))\n",
    "    y_test_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][9])))\n",
    "    y_test_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][10])))\n",
    "    X_test_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][11].shape)]][0])\n",
    "\n",
    "    for index, (y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate([clf[2] for clf in clf_list]):\n",
    "        y_train_real_lambda_list[index] = y_train_real_lambda.ravel()\n",
    "        y_train_pred_lambda_list[index] = y_train_pred_lambda.ravel()\n",
    "        y_train_pred_lambda_poly_lstsq_list[index] = y_train_pred_lambda_poly_lstsq.ravel()\n",
    "        X_train_lambda_list[index] = X_train_lambda#.ravel()\n",
    "\n",
    "        y_valid_real_lambda_list[index] = y_valid_real_lambda.ravel()\n",
    "        y_valid_pred_lambda_list[index] = y_valid_pred_lambda.ravel()\n",
    "        y_valid_pred_lambda_poly_lstsq_list[index] = y_valid_pred_lambda_poly_lstsq.ravel()\n",
    "        X_valid_lambda_list[index] = X_valid_lambda#.ravel()\n",
    "\n",
    "        y_test_real_lambda_list[index] = y_test_real_lambda.ravel()\n",
    "        y_test_pred_lambda_list[index] = y_test_pred_lambda.ravel()\n",
    "        y_test_pred_lambda_poly_lstsq_list[index] = y_test_pred_lambda_poly_lstsq.ravel()\n",
    "        X_test_lambda_list[index] = X_test_lambda#.ravel()\n",
    "    \n",
    "    #add x_data before each pred\n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda.reshape(len(y_train_real_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_list, y_train_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda.reshape(len(y_valid_real_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_list, y_valid_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda.reshape(len(y_test_real_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_list, y_test_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda.reshape(len(y_train_pred_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_list, y_train_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda.reshape(len(y_valid_pred_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_list, y_valid_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda.reshape(len(y_test_pred_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_list, y_test_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq.reshape(len(y_train_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_list, y_train_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq.reshape(len(y_valid_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_list, y_valid_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq.reshape(len(y_test_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_list, y_test_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())    \n",
    "    \n",
    "    y_train_real_lambda_df = pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)\n",
    "       \n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "    \n",
    "    y_train_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][0]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]), 1)) for i in epochs_save_range]\n",
    "    X_train_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][3].shape)]][0]) for i in epochs_save_range]\n",
    "    y_valid_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][4]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]), 1)) for i in epochs_save_range]\n",
    "    X_valid_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][7].shape)]][0]) for i in epochs_save_range]\n",
    "    y_test_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][8]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]), 1)) for i in epochs_save_range]\n",
    "    X_test_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][11].shape)]][0]) for i in epochs_save_range]\n",
    "    \n",
    "    for i, y_data_list_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate(y_data_list_per_epoch):\n",
    "            y_train_real_lambda_list[index][i] = y_train_real_lambda#.ravel()\n",
    "            y_train_pred_lambda_list[index][i] = y_train_pred_lambda#.ravel()\n",
    "            y_train_pred_lambda_poly_lstsq_list[index][i] = y_train_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_train_lambda_list[index][i] = X_train_lambda#.ravel()\n",
    "            \n",
    "            y_valid_real_lambda_list[index][i] = y_valid_real_lambda#.ravel()\n",
    "            y_valid_pred_lambda_list[index][i] = y_valid_pred_lambda#.ravel()\n",
    "            y_valid_pred_lambda_poly_lstsq_list[index][i] = y_valid_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_valid_lambda_list[index][i] = X_valid_lambda#.ravel()\n",
    "            \n",
    "            y_test_real_lambda_list[index][i] = y_test_real_lambda#.ravel()\n",
    "            y_test_pred_lambda_list[index][i] = y_test_pred_lambda#.ravel()\n",
    "            y_test_pred_lambda_poly_lstsq_list[index][i] = y_test_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_test_lambda_list[index][i] = X_test_lambda#.ravel()\n",
    "    \n",
    "    for i, (y_train_real_lambda_by_epoch, y_train_pred_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch, X_train_lambda_by_epoch, y_valid_real_lambda_by_epoch, y_valid_pred_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch, X_valid_lambda_by_epoch, y_test_real_lambda_by_epoch, y_test_pred_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch, X_test_lambda_by_epoch) in tqdm(enumerate(zip(y_train_real_lambda_list, y_train_pred_lambda_list, y_train_pred_lambda_poly_lstsq_list, X_train_lambda_list, y_valid_real_lambda_list, y_valid_pred_lambda_list, y_valid_pred_lambda_poly_lstsq_list, X_valid_lambda_list, y_test_real_lambda_list, y_test_pred_lambda_list, y_test_pred_lambda_poly_lstsq_list, X_test_lambda_list)), total=len(y_train_pred_lambda_list)):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "        \n",
    "        y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_by_epoch, y_train_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_by_epoch, y_valid_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_by_epoch, y_test_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_by_epoch, y_train_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_by_epoch, y_test_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())    \n",
    "\n",
    "        y_train_real_lambda_df = pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)\n",
    "        y_valid_real_lambda_df = pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)\n",
    "        y_test_real_lambda_df = pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)\n",
    "        y_train_pred_lambda_df = pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)\n",
    "        y_valid_pred_lambda_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)\n",
    "        y_test_pred_lambda_df = pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)\n",
    "        y_train_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)\n",
    "        y_valid_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)\n",
    "        y_test_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)\n",
    "\n",
    "        path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "        y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "        y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)         \n",
    "        y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "        y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "        y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)    \n",
    "        y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "        y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "        y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:44:51.202319Z",
     "start_time": "2020-12-03T13:44:51.175130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.530</td>\n",
       "      <td>19.032</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>23.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1.935</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-13.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-34.270</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-7.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>9.774</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>7.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>1.342</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>3.687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  0.380  0.960 -0.820  0.530  19.032  0.480  0.970 -1.000  0.350  23.972  \n",
       "1 -0.890 -0.980 -0.440  0.730   1.935 -0.040  0.630 -0.100  0.170 -13.036  \n",
       "2 -0.930 -0.730 -0.910 -0.980 -34.270  0.330 -0.210  0.120 -0.100  -7.948  \n",
       "3  0.220  0.680  0.710 -0.460   9.774  0.910 -0.130  0.050 -0.910   7.523  \n",
       "4  0.280  0.340 -0.520 -0.690   1.342 -0.280 -0.520  0.070 -0.370   3.687  \n",
       "\n",
       "[5 rows x 1285 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:44:51.393206Z",
     "start_time": "2020-12-03T13:44:51.203971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.530</td>\n",
       "      <td>19.409</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>20.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-5.193</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-14.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-28.935</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-15.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>13.603</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>10.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>7.464</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>4.131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  0.380  0.960 -0.820  0.530  19.409  0.480  0.970 -1.000  0.350  20.763  \n",
       "1 -0.890 -0.980 -0.440  0.730  -5.193 -0.040  0.630 -0.100  0.170 -14.350  \n",
       "2 -0.930 -0.730 -0.910 -0.980 -28.935  0.330 -0.210  0.120 -0.100 -15.351  \n",
       "3  0.220  0.680  0.710 -0.460  13.603  0.910 -0.130  0.050 -0.910  10.239  \n",
       "4  0.280  0.340 -0.520 -0.690   7.464 -0.280 -0.520  0.070 -0.370   4.131  \n",
       "\n",
       "[5 rows x 1285 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:44:51.413098Z",
     "start_time": "2020-12-03T13:44:51.394809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.530</td>\n",
       "      <td>19.666</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>21.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-4.861</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-14.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-29.008</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-15.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>13.907</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>10.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>7.559</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>4.053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  0.380  0.960 -0.820  0.530  19.666  0.480  0.970 -1.000  0.350  21.304  \n",
       "1 -0.890 -0.980 -0.440  0.730  -4.861 -0.040  0.630 -0.100  0.170 -14.390  \n",
       "2 -0.930 -0.730 -0.910 -0.980 -29.008  0.330 -0.210  0.120 -0.100 -15.118  \n",
       "3  0.220  0.680  0.710 -0.460  13.907  0.910 -0.130  0.050 -0.910  10.821  \n",
       "4  0.280  0.340 -0.520 -0.690   7.559 -0.280 -0.520  0.070 -0.370   4.053  \n",
       "\n",
       "[5 rows x 1285 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_poly_lstsq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:44:55.181364Z",
     "start_time": "2020-12-03T13:44:51.414846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6ec4a7cb554487b97c024df19615cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:45:08.820245Z",
     "start_time": "2020-12-03T13:44:55.187497Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:45:09.475652Z",
     "start_time": "2020-12-03T13:45:08.822617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.231</td>\n",
       "      <td>10.207</td>\n",
       "      <td>10.182</td>\n",
       "      <td>10.158</td>\n",
       "      <td>10.133</td>\n",
       "      <td>10.109</td>\n",
       "      <td>10.085</td>\n",
       "      <td>10.060</td>\n",
       "      <td>10.035</td>\n",
       "      <td>10.011</td>\n",
       "      <td>...</td>\n",
       "      <td>5.225</td>\n",
       "      <td>5.219</td>\n",
       "      <td>5.213</td>\n",
       "      <td>5.207</td>\n",
       "      <td>5.202</td>\n",
       "      <td>5.196</td>\n",
       "      <td>5.190</td>\n",
       "      <td>5.184</td>\n",
       "      <td>5.178</td>\n",
       "      <td>5.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.348</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.321</td>\n",
       "      <td>2.308</td>\n",
       "      <td>2.294</td>\n",
       "      <td>2.281</td>\n",
       "      <td>2.268</td>\n",
       "      <td>2.255</td>\n",
       "      <td>2.242</td>\n",
       "      <td>2.228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.700</td>\n",
       "      <td>4.699</td>\n",
       "      <td>4.699</td>\n",
       "      <td>4.698</td>\n",
       "      <td>4.698</td>\n",
       "      <td>4.698</td>\n",
       "      <td>4.697</td>\n",
       "      <td>4.697</td>\n",
       "      <td>4.696</td>\n",
       "      <td>4.696</td>\n",
       "      <td>...</td>\n",
       "      <td>3.079</td>\n",
       "      <td>3.070</td>\n",
       "      <td>3.064</td>\n",
       "      <td>3.057</td>\n",
       "      <td>3.051</td>\n",
       "      <td>3.045</td>\n",
       "      <td>3.038</td>\n",
       "      <td>3.034</td>\n",
       "      <td>3.027</td>\n",
       "      <td>3.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.500</td>\n",
       "      <td>8.485</td>\n",
       "      <td>8.470</td>\n",
       "      <td>8.451</td>\n",
       "      <td>8.436</td>\n",
       "      <td>8.423</td>\n",
       "      <td>8.411</td>\n",
       "      <td>8.397</td>\n",
       "      <td>8.389</td>\n",
       "      <td>8.376</td>\n",
       "      <td>...</td>\n",
       "      <td>4.790</td>\n",
       "      <td>4.784</td>\n",
       "      <td>4.778</td>\n",
       "      <td>4.773</td>\n",
       "      <td>4.766</td>\n",
       "      <td>4.761</td>\n",
       "      <td>4.755</td>\n",
       "      <td>4.748</td>\n",
       "      <td>4.743</td>\n",
       "      <td>4.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.969</td>\n",
       "      <td>9.952</td>\n",
       "      <td>9.931</td>\n",
       "      <td>9.913</td>\n",
       "      <td>9.889</td>\n",
       "      <td>9.872</td>\n",
       "      <td>9.851</td>\n",
       "      <td>9.826</td>\n",
       "      <td>9.806</td>\n",
       "      <td>9.781</td>\n",
       "      <td>...</td>\n",
       "      <td>5.215</td>\n",
       "      <td>5.209</td>\n",
       "      <td>5.203</td>\n",
       "      <td>5.198</td>\n",
       "      <td>5.192</td>\n",
       "      <td>5.185</td>\n",
       "      <td>5.180</td>\n",
       "      <td>5.173</td>\n",
       "      <td>5.168</td>\n",
       "      <td>5.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.640</td>\n",
       "      <td>11.612</td>\n",
       "      <td>11.582</td>\n",
       "      <td>11.550</td>\n",
       "      <td>11.518</td>\n",
       "      <td>11.490</td>\n",
       "      <td>11.464</td>\n",
       "      <td>11.437</td>\n",
       "      <td>11.403</td>\n",
       "      <td>11.377</td>\n",
       "      <td>...</td>\n",
       "      <td>5.649</td>\n",
       "      <td>5.642</td>\n",
       "      <td>5.635</td>\n",
       "      <td>5.629</td>\n",
       "      <td>5.624</td>\n",
       "      <td>5.618</td>\n",
       "      <td>5.612</td>\n",
       "      <td>5.606</td>\n",
       "      <td>5.601</td>\n",
       "      <td>5.594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.689</td>\n",
       "      <td>20.597</td>\n",
       "      <td>20.507</td>\n",
       "      <td>20.418</td>\n",
       "      <td>20.329</td>\n",
       "      <td>20.242</td>\n",
       "      <td>20.155</td>\n",
       "      <td>20.068</td>\n",
       "      <td>19.982</td>\n",
       "      <td>19.896</td>\n",
       "      <td>...</td>\n",
       "      <td>8.391</td>\n",
       "      <td>8.390</td>\n",
       "      <td>8.386</td>\n",
       "      <td>8.384</td>\n",
       "      <td>8.381</td>\n",
       "      <td>8.379</td>\n",
       "      <td>8.376</td>\n",
       "      <td>8.374</td>\n",
       "      <td>8.370</td>\n",
       "      <td>8.367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean         10.231        10.207        10.182        10.158        10.133   \n",
       "std           2.348         2.334         2.321         2.308         2.294   \n",
       "min           4.700         4.699         4.699         4.698         4.698   \n",
       "25%           8.500         8.485         8.470         8.451         8.436   \n",
       "50%           9.969         9.952         9.931         9.913         9.889   \n",
       "75%          11.640        11.612        11.582        11.550        11.518   \n",
       "max          20.689        20.597        20.507        20.418        20.329   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000      10000.000   \n",
       "mean         10.109        10.085        10.060        10.035         10.011   \n",
       "std           2.281         2.268         2.255         2.242          2.228   \n",
       "min           4.698         4.697         4.697         4.696          4.696   \n",
       "25%           8.423         8.411         8.397         8.389          8.376   \n",
       "50%           9.872         9.851         9.826         9.806          9.781   \n",
       "75%          11.490        11.464        11.437        11.403         11.377   \n",
       "max          20.242        20.155        20.068        19.982         19.896   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...       10000.000       10000.000       10000.000       10000.000   \n",
       "mean   ...           5.225           5.219           5.213           5.207   \n",
       "std    ...           0.645           0.645           0.644           0.644   \n",
       "min    ...           3.079           3.070           3.064           3.057   \n",
       "25%    ...           4.790           4.784           4.778           4.773   \n",
       "50%    ...           5.215           5.209           5.203           5.198   \n",
       "75%    ...           5.649           5.642           5.635           5.629   \n",
       "max    ...           8.391           8.390           8.386           8.384   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            5.202           5.196           5.190           5.184   \n",
       "std             0.643           0.643           0.643           0.642   \n",
       "min             3.051           3.045           3.038           3.034   \n",
       "25%             4.766           4.761           4.755           4.748   \n",
       "50%             5.192           5.185           5.180           5.173   \n",
       "75%             5.624           5.618           5.612           5.606   \n",
       "max             8.381           8.379           8.376           8.374   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count       10000.000       10000.000  \n",
       "mean            5.178           5.173  \n",
       "std             0.642           0.641  \n",
       "min             3.027           3.021  \n",
       "25%             4.743           4.739  \n",
       "50%             5.168           5.163  \n",
       "75%             5.601           5.594  \n",
       "max             8.370           8.367  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:45:10.100250Z",
     "start_time": "2020-12-03T13:45:09.477321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.201</td>\n",
       "      <td>10.176</td>\n",
       "      <td>10.152</td>\n",
       "      <td>10.128</td>\n",
       "      <td>10.104</td>\n",
       "      <td>10.079</td>\n",
       "      <td>10.055</td>\n",
       "      <td>10.031</td>\n",
       "      <td>10.006</td>\n",
       "      <td>9.982</td>\n",
       "      <td>...</td>\n",
       "      <td>5.309</td>\n",
       "      <td>5.303</td>\n",
       "      <td>5.298</td>\n",
       "      <td>5.292</td>\n",
       "      <td>5.287</td>\n",
       "      <td>5.281</td>\n",
       "      <td>5.276</td>\n",
       "      <td>5.270</td>\n",
       "      <td>5.265</td>\n",
       "      <td>5.259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.382</td>\n",
       "      <td>2.368</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.342</td>\n",
       "      <td>2.329</td>\n",
       "      <td>2.316</td>\n",
       "      <td>2.303</td>\n",
       "      <td>2.290</td>\n",
       "      <td>2.277</td>\n",
       "      <td>2.264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.597</td>\n",
       "      <td>4.595</td>\n",
       "      <td>4.593</td>\n",
       "      <td>4.592</td>\n",
       "      <td>4.590</td>\n",
       "      <td>4.589</td>\n",
       "      <td>4.587</td>\n",
       "      <td>4.585</td>\n",
       "      <td>4.584</td>\n",
       "      <td>4.582</td>\n",
       "      <td>...</td>\n",
       "      <td>2.790</td>\n",
       "      <td>2.790</td>\n",
       "      <td>2.787</td>\n",
       "      <td>2.786</td>\n",
       "      <td>2.783</td>\n",
       "      <td>2.782</td>\n",
       "      <td>2.783</td>\n",
       "      <td>2.785</td>\n",
       "      <td>2.782</td>\n",
       "      <td>2.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.463</td>\n",
       "      <td>8.449</td>\n",
       "      <td>8.433</td>\n",
       "      <td>8.417</td>\n",
       "      <td>8.405</td>\n",
       "      <td>8.387</td>\n",
       "      <td>8.375</td>\n",
       "      <td>8.360</td>\n",
       "      <td>8.343</td>\n",
       "      <td>8.330</td>\n",
       "      <td>...</td>\n",
       "      <td>4.810</td>\n",
       "      <td>4.806</td>\n",
       "      <td>4.801</td>\n",
       "      <td>4.796</td>\n",
       "      <td>4.790</td>\n",
       "      <td>4.785</td>\n",
       "      <td>4.781</td>\n",
       "      <td>4.776</td>\n",
       "      <td>4.771</td>\n",
       "      <td>4.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.928</td>\n",
       "      <td>9.905</td>\n",
       "      <td>9.881</td>\n",
       "      <td>9.859</td>\n",
       "      <td>9.835</td>\n",
       "      <td>9.811</td>\n",
       "      <td>9.792</td>\n",
       "      <td>9.769</td>\n",
       "      <td>9.753</td>\n",
       "      <td>9.732</td>\n",
       "      <td>...</td>\n",
       "      <td>5.275</td>\n",
       "      <td>5.269</td>\n",
       "      <td>5.263</td>\n",
       "      <td>5.258</td>\n",
       "      <td>5.253</td>\n",
       "      <td>5.247</td>\n",
       "      <td>5.242</td>\n",
       "      <td>5.238</td>\n",
       "      <td>5.233</td>\n",
       "      <td>5.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.623</td>\n",
       "      <td>11.597</td>\n",
       "      <td>11.574</td>\n",
       "      <td>11.545</td>\n",
       "      <td>11.509</td>\n",
       "      <td>11.481</td>\n",
       "      <td>11.450</td>\n",
       "      <td>11.421</td>\n",
       "      <td>11.389</td>\n",
       "      <td>11.357</td>\n",
       "      <td>...</td>\n",
       "      <td>5.790</td>\n",
       "      <td>5.783</td>\n",
       "      <td>5.776</td>\n",
       "      <td>5.771</td>\n",
       "      <td>5.765</td>\n",
       "      <td>5.758</td>\n",
       "      <td>5.753</td>\n",
       "      <td>5.747</td>\n",
       "      <td>5.741</td>\n",
       "      <td>5.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.608</td>\n",
       "      <td>20.533</td>\n",
       "      <td>20.457</td>\n",
       "      <td>20.379</td>\n",
       "      <td>20.299</td>\n",
       "      <td>20.216</td>\n",
       "      <td>20.131</td>\n",
       "      <td>20.043</td>\n",
       "      <td>19.951</td>\n",
       "      <td>19.856</td>\n",
       "      <td>...</td>\n",
       "      <td>8.565</td>\n",
       "      <td>8.563</td>\n",
       "      <td>8.560</td>\n",
       "      <td>8.557</td>\n",
       "      <td>8.553</td>\n",
       "      <td>8.549</td>\n",
       "      <td>8.546</td>\n",
       "      <td>8.543</td>\n",
       "      <td>8.540</td>\n",
       "      <td>8.537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             10.201            10.176            10.152            10.128   \n",
       "std               2.382             2.368             2.355             2.342   \n",
       "min               4.597             4.595             4.593             4.592   \n",
       "25%               8.463             8.449             8.433             8.417   \n",
       "50%               9.928             9.905             9.881             9.859   \n",
       "75%              11.623            11.597            11.574            11.545   \n",
       "max              20.608            20.533            20.457            20.379   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             10.104            10.079            10.055            10.031   \n",
       "std               2.329             2.316             2.303             2.290   \n",
       "min               4.590             4.589             4.587             4.585   \n",
       "25%               8.405             8.387             8.375             8.360   \n",
       "50%               9.835             9.811             9.792             9.769   \n",
       "75%              11.509            11.481            11.450            11.421   \n",
       "max              20.299            20.216            20.131            20.043   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count         10000.000          10000.000  ...           10000.000   \n",
       "mean             10.006              9.982  ...               5.309   \n",
       "std               2.277              2.264  ...               0.724   \n",
       "min               4.584              4.582  ...               2.790   \n",
       "25%               8.343              8.330  ...               4.810   \n",
       "50%               9.753              9.732  ...               5.275   \n",
       "75%              11.389             11.357  ...               5.790   \n",
       "max              19.951             19.856  ...               8.565   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                5.303               5.298               5.292   \n",
       "std                 0.724               0.723               0.723   \n",
       "min                 2.790               2.787               2.786   \n",
       "25%                 4.806               4.801               4.796   \n",
       "50%                 5.269               5.263               5.258   \n",
       "75%                 5.783               5.776               5.771   \n",
       "max                 8.563               8.560               8.557   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                5.287               5.281               5.276   \n",
       "std                 0.722               0.722               0.721   \n",
       "min                 2.783               2.782               2.783   \n",
       "25%                 4.790               4.785               4.781   \n",
       "50%                 5.253               5.247               5.242   \n",
       "75%                 5.765               5.758               5.753   \n",
       "max                 8.553               8.549               8.546   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count           10000.000           10000.000           10000.000  \n",
       "mean                5.270               5.265               5.259  \n",
       "std                 0.721               0.721               0.720  \n",
       "min                 2.785               2.782               2.780  \n",
       "25%                 4.776               4.771               4.764  \n",
       "50%                 5.238               5.233               5.227  \n",
       "75%                 5.747               5.741               5.735  \n",
       "max                 8.543               8.540               8.537  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:45:10.716954Z",
     "start_time": "2020-12-03T13:45:10.101877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.067</td>\n",
       "      <td>1.054</td>\n",
       "      <td>1.088</td>\n",
       "      <td>1.126</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.224</td>\n",
       "      <td>1.316</td>\n",
       "      <td>1.328</td>\n",
       "      <td>1.365</td>\n",
       "      <td>1.410</td>\n",
       "      <td>...</td>\n",
       "      <td>4.436</td>\n",
       "      <td>4.421</td>\n",
       "      <td>4.421</td>\n",
       "      <td>4.447</td>\n",
       "      <td>4.411</td>\n",
       "      <td>4.472</td>\n",
       "      <td>4.383</td>\n",
       "      <td>4.390</td>\n",
       "      <td>4.395</td>\n",
       "      <td>4.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.899</td>\n",
       "      <td>1.739</td>\n",
       "      <td>2.976</td>\n",
       "      <td>4.402</td>\n",
       "      <td>7.662</td>\n",
       "      <td>9.379</td>\n",
       "      <td>15.568</td>\n",
       "      <td>14.396</td>\n",
       "      <td>15.653</td>\n",
       "      <td>17.597</td>\n",
       "      <td>...</td>\n",
       "      <td>76.780</td>\n",
       "      <td>76.100</td>\n",
       "      <td>76.381</td>\n",
       "      <td>76.684</td>\n",
       "      <td>75.831</td>\n",
       "      <td>77.534</td>\n",
       "      <td>74.891</td>\n",
       "      <td>74.875</td>\n",
       "      <td>75.231</td>\n",
       "      <td>75.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.002</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.393</td>\n",
       "      <td>1.391</td>\n",
       "      <td>1.391</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.389</td>\n",
       "      <td>1.391</td>\n",
       "      <td>1.386</td>\n",
       "      <td>1.387</td>\n",
       "      <td>1.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.004</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.009</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.022</td>\n",
       "      <td>1.028</td>\n",
       "      <td>1.033</td>\n",
       "      <td>1.040</td>\n",
       "      <td>...</td>\n",
       "      <td>1.877</td>\n",
       "      <td>1.878</td>\n",
       "      <td>1.873</td>\n",
       "      <td>1.873</td>\n",
       "      <td>1.876</td>\n",
       "      <td>1.876</td>\n",
       "      <td>1.872</td>\n",
       "      <td>1.877</td>\n",
       "      <td>1.874</td>\n",
       "      <td>1.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.014</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1.035</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.058</td>\n",
       "      <td>1.071</td>\n",
       "      <td>1.085</td>\n",
       "      <td>1.099</td>\n",
       "      <td>1.112</td>\n",
       "      <td>...</td>\n",
       "      <td>2.682</td>\n",
       "      <td>2.682</td>\n",
       "      <td>2.683</td>\n",
       "      <td>2.682</td>\n",
       "      <td>2.679</td>\n",
       "      <td>2.690</td>\n",
       "      <td>2.686</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.682</td>\n",
       "      <td>2.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>263.628</td>\n",
       "      <td>167.789</td>\n",
       "      <td>219.569</td>\n",
       "      <td>357.652</td>\n",
       "      <td>695.420</td>\n",
       "      <td>877.262</td>\n",
       "      <td>1499.434</td>\n",
       "      <td>1358.593</td>\n",
       "      <td>1482.849</td>\n",
       "      <td>1671.647</td>\n",
       "      <td>...</td>\n",
       "      <td>6673.955</td>\n",
       "      <td>6661.793</td>\n",
       "      <td>6683.099</td>\n",
       "      <td>6609.913</td>\n",
       "      <td>6649.119</td>\n",
       "      <td>6689.004</td>\n",
       "      <td>6620.376</td>\n",
       "      <td>6598.322</td>\n",
       "      <td>6640.296</td>\n",
       "      <td>6612.517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            1.067           1.054           1.088           1.126   \n",
       "std             2.899           1.739           2.976           4.402   \n",
       "min             0.982           0.966           0.953           0.941   \n",
       "25%             1.000           0.998           0.998           0.999   \n",
       "50%             1.004           1.004           1.005           1.009   \n",
       "75%             1.014           1.017           1.024           1.035   \n",
       "max           263.628         167.789         219.569         357.652   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            1.187           1.224           1.316           1.328   \n",
       "std             7.662           9.379          15.568          14.396   \n",
       "min             0.930           0.918           0.905           0.893   \n",
       "25%             1.000           1.001           1.002           1.004   \n",
       "50%             1.013           1.017           1.022           1.028   \n",
       "75%             1.046           1.058           1.071           1.085   \n",
       "max           695.420         877.262        1499.434        1358.593   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count       10000.000        10000.000  ...         10000.000   \n",
       "mean            1.365            1.410  ...             4.436   \n",
       "std            15.653           17.597  ...            76.780   \n",
       "min             0.880            0.868  ...             0.255   \n",
       "25%             1.006            1.008  ...             1.393   \n",
       "50%             1.033            1.040  ...             1.877   \n",
       "75%             1.099            1.112  ...             2.682   \n",
       "max          1482.849         1671.647  ...          6673.955   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              4.421             4.421             4.447             4.411   \n",
       "std              76.100            76.381            76.684            75.831   \n",
       "min               0.255             0.254             0.254             0.257   \n",
       "25%               1.391             1.391             1.392             1.392   \n",
       "50%               1.878             1.873             1.873             1.876   \n",
       "75%               2.682             2.683             2.682             2.679   \n",
       "max            6661.793          6683.099          6609.913          6649.119   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              4.472             4.383             4.390             4.395   \n",
       "std              77.534            74.891            74.875            75.231   \n",
       "min               0.255             0.254             0.254             0.253   \n",
       "25%               1.389             1.391             1.386             1.387   \n",
       "50%               1.876             1.872             1.877             1.874   \n",
       "75%               2.690             2.686             2.680             2.682   \n",
       "max            6689.004          6620.376          6598.322          6640.296   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count         10000.000  \n",
       "mean              4.429  \n",
       "std              75.837  \n",
       "min               0.252  \n",
       "25%               1.383  \n",
       "50%               1.869  \n",
       "75%               2.679  \n",
       "max            6612.517  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:45:11.339803Z",
     "start_time": "2020-12-03T13:45:10.718598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.034</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.076</td>\n",
       "      <td>1.103</td>\n",
       "      <td>1.132</td>\n",
       "      <td>1.161</td>\n",
       "      <td>1.190</td>\n",
       "      <td>1.219</td>\n",
       "      <td>1.249</td>\n",
       "      <td>1.279</td>\n",
       "      <td>...</td>\n",
       "      <td>4.887</td>\n",
       "      <td>4.887</td>\n",
       "      <td>4.885</td>\n",
       "      <td>4.883</td>\n",
       "      <td>4.885</td>\n",
       "      <td>4.889</td>\n",
       "      <td>4.884</td>\n",
       "      <td>4.883</td>\n",
       "      <td>4.880</td>\n",
       "      <td>4.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.649</td>\n",
       "      <td>1.100</td>\n",
       "      <td>2.263</td>\n",
       "      <td>3.457</td>\n",
       "      <td>4.654</td>\n",
       "      <td>5.832</td>\n",
       "      <td>6.999</td>\n",
       "      <td>8.143</td>\n",
       "      <td>9.275</td>\n",
       "      <td>10.407</td>\n",
       "      <td>...</td>\n",
       "      <td>110.011</td>\n",
       "      <td>109.944</td>\n",
       "      <td>109.813</td>\n",
       "      <td>109.748</td>\n",
       "      <td>109.756</td>\n",
       "      <td>109.860</td>\n",
       "      <td>109.704</td>\n",
       "      <td>109.684</td>\n",
       "      <td>109.584</td>\n",
       "      <td>109.547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.965</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.996</td>\n",
       "      <td>...</td>\n",
       "      <td>1.188</td>\n",
       "      <td>1.188</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.186</td>\n",
       "      <td>1.186</td>\n",
       "      <td>1.186</td>\n",
       "      <td>1.185</td>\n",
       "      <td>1.183</td>\n",
       "      <td>1.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.020</td>\n",
       "      <td>...</td>\n",
       "      <td>1.635</td>\n",
       "      <td>1.633</td>\n",
       "      <td>1.635</td>\n",
       "      <td>1.634</td>\n",
       "      <td>1.632</td>\n",
       "      <td>1.633</td>\n",
       "      <td>1.632</td>\n",
       "      <td>1.632</td>\n",
       "      <td>1.631</td>\n",
       "      <td>1.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.009</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.019</td>\n",
       "      <td>1.027</td>\n",
       "      <td>1.036</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.057</td>\n",
       "      <td>1.067</td>\n",
       "      <td>1.078</td>\n",
       "      <td>1.090</td>\n",
       "      <td>...</td>\n",
       "      <td>2.446</td>\n",
       "      <td>2.442</td>\n",
       "      <td>2.442</td>\n",
       "      <td>2.442</td>\n",
       "      <td>2.446</td>\n",
       "      <td>2.447</td>\n",
       "      <td>2.450</td>\n",
       "      <td>2.445</td>\n",
       "      <td>2.444</td>\n",
       "      <td>2.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>38.815</td>\n",
       "      <td>89.950</td>\n",
       "      <td>212.579</td>\n",
       "      <td>332.830</td>\n",
       "      <td>452.102</td>\n",
       "      <td>569.036</td>\n",
       "      <td>684.647</td>\n",
       "      <td>797.737</td>\n",
       "      <td>909.673</td>\n",
       "      <td>1021.523</td>\n",
       "      <td>...</td>\n",
       "      <td>9735.540</td>\n",
       "      <td>9718.138</td>\n",
       "      <td>9709.277</td>\n",
       "      <td>9708.974</td>\n",
       "      <td>9705.702</td>\n",
       "      <td>9698.605</td>\n",
       "      <td>9694.153</td>\n",
       "      <td>9691.210</td>\n",
       "      <td>9688.167</td>\n",
       "      <td>9691.044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.034               1.049               1.076   \n",
       "std                 0.649               1.100               2.263   \n",
       "min                 0.965               0.954               0.939   \n",
       "25%                 0.996               0.994               0.993   \n",
       "50%                 1.001               1.000               1.001   \n",
       "75%                 1.009               1.013               1.019   \n",
       "max                38.815              89.950             212.579   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.103               1.132               1.161   \n",
       "std                 3.457               4.654               5.832   \n",
       "min                 0.927               0.916               0.906   \n",
       "25%                 0.993               0.993               0.993   \n",
       "50%                 1.003               1.005               1.007   \n",
       "75%                 1.027               1.036               1.046   \n",
       "max               332.830             452.102             569.036   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.190               1.219               1.249   \n",
       "std                 6.999               8.143               9.275   \n",
       "min                 0.893               0.879               0.865   \n",
       "25%                 0.994               0.994               0.995   \n",
       "50%                 1.010               1.013               1.016   \n",
       "75%                 1.057               1.067               1.078   \n",
       "max               684.647             797.737             909.673   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count            10000.000  ...             10000.000             10000.000   \n",
       "mean                 1.279  ...                 4.887                 4.887   \n",
       "std                 10.407  ...               110.011               109.944   \n",
       "min                  0.852  ...                 0.247                 0.247   \n",
       "25%                  0.996  ...                 1.188                 1.188   \n",
       "50%                  1.020  ...                 1.635                 1.633   \n",
       "75%                  1.090  ...                 2.446                 2.442   \n",
       "max               1021.523  ...              9735.540              9718.138   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  4.885                 4.883                 4.885   \n",
       "std                 109.813               109.748               109.756   \n",
       "min                   0.247                 0.246                 0.246   \n",
       "25%                   1.187                 1.187                 1.186   \n",
       "50%                   1.635                 1.634                 1.632   \n",
       "75%                   2.442                 2.442                 2.446   \n",
       "max                9709.277              9708.974              9705.702   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  4.889                 4.884                 4.883   \n",
       "std                 109.860               109.704               109.684   \n",
       "min                   0.246                 0.245                 0.246   \n",
       "25%                   1.186                 1.186                 1.185   \n",
       "50%                   1.633                 1.632                 1.632   \n",
       "75%                   2.447                 2.450                 2.445   \n",
       "max                9698.605              9694.153              9691.210   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count             10000.000             10000.000  \n",
       "mean                  4.880                 4.877  \n",
       "std                 109.584               109.547  \n",
       "min                   0.245                 0.245  \n",
       "25%                   1.183                 1.183  \n",
       "50%                   1.631                 1.630  \n",
       "75%                   2.444                 2.442  \n",
       "max                9688.167              9691.044  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:45:12.961080Z",
     "start_time": "2020-12-03T13:45:11.341452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3Xd81dX9x/HX967c7JuQQAh7HgQZTnDgqHvhpq5Wa6u1P7W10y6rndpW2zqq1apVWycKCnWBExcgoMg8QCCQELLvzbi5ufP7++Ne0kQCJCF3JPk8Hw8e5I7v/X7yzc07557v+Z5jmKaJEEKI/s+S7AKEEEIkhgS+EEIMEBL4QggxQEjgCyHEACGBL4QQA4QEvhBCDBAS+GJAUEo9oZT6XRefW6qUOjXeNXWVUup1pdTVya5D9H22ZBcgxECllLoDGK+1vmp/z9Nan5WYikR/Jy18IVKUUspQSsnvqOg10sIXKUMpVQr8HfgaMA54Dvg58ARwPLAcuFRr7Y49fw5wJzAM+Bz4jtZ6Y+yxw4DHgAnAa0CHS8qVUucCvwNGAxuAG7TWX3ShxieAFmAMMBtYA1wM/BS4GqgCLtdafxZ7fjFwP3AC0Az8VWt9n1LqzNj3ZiilLgBKtNbTlVLvAR8BJwGHA1OVUo8C/9FaPxp7zeuAHwDDgTLgKq316gPVLoS0HkSquRg4DZgInAe8TjQYC4m+X78LoJSaCDwL3BJ77DVgkVLKoZRyAC8D/wbygXmx1yW27WHA48C3gUHAw8BCpVRaF2ucC/wSKAD8wCfA6tjtF4G/xPZjARYR/aMwDDgFuEUpdYbW+g3gD8DzWussrfX0dq//NeB6IBvY0X7HSqlLgTuArwM5wBygrot1iwFOWvgi1dyvta4CUEp9AFS3ay0vIBqaAF8FXtVaL4k9djfwPeBYIALYgb9prU3gRaXUD9rt43rgYa318tjtJ5VSPwdmAe93ocYFWutV7Wr6P631U7HbzwM3xZ53FFCotf5N7PY2pdQ/gcuAN/fz+k9ordfvuaGUav/Yt4A/aa0/jd3e2oV6hQAk8EXqqWr3ta+T21mxr4tp1/rVWkeUUmVEW9JhYFcs7Pdo31IeBVytlLq53X2O2Gv2Zo2jgGKllKfd41bggwO8ftl+HhsBlHSxTiE6kMAXfVUFMHXPDaWUQTQMdxHtrx+mlDLahf5I/heUZcDvtda/j3ONZcB2rfWEfTy+r6lq9zeFbRnR8xtCdJsEvuirXgB+qpQ6BVhKtDvHD3wcezwEfFcp9SDRcwFHA+/GHvsnsEAp9RawAsggepJ0qda6qRdrXAE0KaVuBe4DAsAhQHqsS6YKOE0pZdFaR7r4mo8Cf1FKfUj0vME4IKi13rH/zYSQk7aij9Jaa+AqoiNgaomG+nla64DWOgBcBFwD1BPt75/fbtuVwHXAA4CbaD/4NXGoMQycC8wAtsfqfBTIjT1lXuz/OqVUl0bZaK3nAb8HngGaiJ6czu/FskU/ZsgCKEIIMTBIC18IIQYICXwhhBggJPCFEGKAkMAXQogBIqWGZUYiETMc7tlJZKvVoKfbxpPU1X2pWpvU1T1SV/f1pDa73VpLdHqRA0qpwA+HTTyelh5t63Jl9HjbeJK6ui9Va5O6ukfq6r6e1FZYmN3lazCkS0cIIQYICXwhhBggJPCFEGKASKk+/M6EwyHc7hpCocB+n1dVZZCKVw13tS6bzUFeXiFWa8r/SIQQfVTKp4vbXYPTmUFmZhGGYezzeVarhXC4q/NPJU5X6jJNE6+3Ebe7hoKCoQmqTAgx0KR8l04oFCAzM2e/Yd/XGYZBZmbOAT/FCCHEwUj5wAf6ddjvMRC+RyFEcvWJwBeiKz7aVs/uxtZklyFEypLAP4Cmpibmz5934Cd+yY9+9F2amnpzLQ2xP6ZpcuuiDTyzaleySxEiZUngH0BzcxMLFuwd+KFQaL/b3X33fWRnZ8erLPEl3kAYfyiCu0XOgwixLyk/SifZ/vGP+9m1axfXXHMFNpsNh8NBdnY2O3bs4Lnn5vOzn/2QqqoqAoEAl156GeeffxEAl1xyHo8++m8CgVa+//2bmDZtBmvXfkFhYSF33XUPaWnOJH9n/YvHFwTA3RJMciVCpK4+Ffivrq9i4brKTh8zDOjJMPw5hxZxzpQh+3z8hhtuZtu2Ep544hlWr17JT35yC0899TzFxcMA+NnPfkVOTi5+fyvf+tbXOemkr5Cb6+rwGuXlZdxxx++59dZfctttP+W9997hjDPO7n6xYp/2BP6e/4UQe+tTgZ8KDjlkSlvYA8yb9xxLl74HQHV1FWVlZXsF/tChxUyYoABQahK7d1ckrN6BQgJfiAPrU4F/zpQh+2yNJ+rCq/T09LavV69eycqVK3j44X/hdDq56abrCQT8e21jt9vbvrZYrITDez9HHJz2gW+apgxzFaITctL2ADIyMmhp6Xy6Uq+3mezsHJxOJzt2lLJhw7oEVyf22NN3Hwib+IKpd8W1EKmgT7XwkyE318XUqdP52tfmkpbmJD8/v+2xmTOP5eWX53PllZcwcuQoJk8+NImVDmweX6jd10EyHNYkViNEapLA74I77vh9p/c7HA7uuee+Th978cVFQLSr6d//fqHt/iuu+FrvFyhoaNd37/YFKc6VUVBCfJl06YiUUlrXs5WI2p+slRO3QnROAl+kjPWVTVz6xEo+L/N0e1u3L0hRdhrQsbXfVc+t3sUf39rS7e2E6Esk8EXK2N0QnQdnZ333W/keX5DR+RltX3fX0pI63tta1+3thOhLJPBFymhojQZ1nbf70yM0+IIMczmxGj272ram2Y87NqRTiP5KAl+kjIbYSJuapu5dpxCKmDS2hsjPsJObbu9RC7+mOUA4YtLk3/8cSUL0ZRL4ImXsaeHXersX+I2tQUzAle7A1YPA9wZCeANhQObiEf2bBH4vO+202ckuoc/aE9S1Td3r0tmznSvd1qPAr2m3Pwl80Z9J4IuU0dal09y9Fv7/At9OXkYPAr/dJwq3DOkU/ZhceHUADz10P4MHD+Hii+cC8NhjD2O1Wvnss1U0NTUSCoW47rrvMHv2SckttB9oO2nb3N0WfvQPhSvdHmvhd68fvqbd/iTwRX8W18BXSrmAR4FDARO4Vmv9SU9fL23Tizg3PtfpY4Zh9GiEReshl+GfdMk+Hz/llNO4776/tAX+u+++xT333M+ll15GZmYWHo+Hb3/7Go4//kSZsOsg7Rk/X+f1E46YWC0HPp66qpnPyhsAyIudtG3wBQlFTDBNPi3zsHBtFRMHZ3LN0SM6/RlVtztJ7JEuHdGPxbuFfy/whtb6EqWUA8iI8/563cSJk3C766mtrcHtdpOdnc2gQQXcd989rFnzGYZhoaamhvr6OgYNKkh2uX2axxfCZjEIRUwaWoPkZzj2+/x3ttRy68INANitBrlOO0OyHJjAOQ8vwx+K4A2EcdosvLW5Bm8gzI3Hj94r9GuaA2SlWTFNqJcVs0Q/FrfAV0rlAicA1wBorQPAQf02+Sddss/WeDynRz755FN59923qa+v4ytfOZ3Fi1/H4/Hw2GP/wWazcckl5xEISFAcjFBsSOTYQRlsq2uhzhvYZ+AHwxE+KKnj9tc1hw7N5qbZY8hy2HDYLJx3aBHpDisfb3eT6bBy9Kg8jhuTz1/fK+HJFWWEIybfPWFMh9CvbvZTmJVGMByRaRlEvxbPFv4YoAb4l1JqOrAK+J7W2ruvDaxWA5er44eAqioDq7Vr55a7+rzuOu20M7jzzt/S0ODhwQf/ydtvLyE/P5+0NAerVn1KZeVurFZL2/6/XEdX6zKMvb//eLFaLQnbV1fsudhq0tActtW10Gr8r74GX5Afv/QFR47KIy/DwT1LNlPnDTAiL51HvnYkhbEpFfa4/JgsLj9mTIf77rx4GhlOO/9evhNvOMKtZ0xiUGb0D0p9a4hiVzotgTBNwcg+j0uqHbM9pK7uSdW6IP61xTPwbcDhwM1a6+VKqXuBnwK37WuDcNjE4+l4Wb1pml1qucezhT9q1BhaWrwUFBSSlzeIU089k1tv/T5XXnkpkyZNZtSo0YTDkbb9t6+jO3WZ5t7ff7y4XBkJ21dXlMUmTRseC++y6iY8BdE3/l/fK+FdXcO7ugaAacU5/PL0CcwalYctHO7y93HzcaOwY/LkijKWbKjiW7NGMfewYio9PkaOysMK7G5s3efrpdox20Pq6p5UrQt6VlthYXaXnxvPwC8HyrXWy2O3XyQa+H3SU0893/a1y+Xi4Yf/1enzliz5IFEl9St7ulLGxkJ+T4t/V4OPeZ9XcN6UIVw0fSjuliDHjc3H0oMT5IZh8J3jx3DmIUP42/sl/O39bSzWNdR5AwzOcmA1DDZUNvXeNyVEionbOHytdSVQppRSsbtOATbEa3+ib9szJHNojhOn3UKdNzrS5q4lW7EYBjccN5pDh+Ywe9ygHoV9e2MGZXDvRVO589xD0FVNhE0ozEprG8Mv8+mI/ireo3RuBp6OjdDZBnwjzvsTfVRDu7H0BZlpVDW1cteSLSzb4ebnp01g8Jf66XvDqaqQ1lCY37yxmTGDMgiEI4QiJs3+MNlOuURF9D9xfVdrrT8HjjzY1xkIi1IP9FblnhZ+brqNgmwHb22uBeDamSO4cNrQuO333ClFnDiugGynjarYePz6loAEvuiXUv5dbbM58HobyczM6behb5omXm8jNtv+x533Zx5fEJvFIMNu5fzpxeQ77Zw/tYhjRufFfd97wj0vw95Wy6i471WIxEv5wM/LK8TtrqG5ef+rIPX0Stt462pdNpuDvLzCBFSUmhp8IXLT7RiGwVUzR3GuSvyxyE+P/sHtyXz8QvQFKR/4VquNgoIDf6RP1aFWqVpXqmloDeJKT+7bcZjLSXaajb+8t41xBZmMyk/NsdpC9JTMlimSyjRN3t5cw7rdTbjS7UmtJSvNxkNzpxEIRbj++TXo6uak1iNEb5PAF0n11uZafrpoI+l2C9+alfyeczU4i0cum47dauGGF9Yw7/MKmlr3P/tmY2uQ51bvSskuRSHak8AXSTV/TQXFuU7mfeMojhzpSnY5AIzOz+DRy6YzMi+DP729lblPrMS9n0nV3thYzT3vllBSJ113IrVJ4IukKff4WFnWwPmHFnVpKuREKspx8sQVM/jH3Gl4fEHufqdkn88trfcBHadZFiIVSeCLpFm0rhKLAedOGZLsUjplGAZHjHBx7ayRLNY1vL2xutPn7aiPtuxru7lwixCJJoEvkmJrjZfnP6vg2DH5cbmKtjddc/QIxhdk8qtF6zvtz9/hjrXwu7k0oxCJJoEvelVVk5/lpe79Pqe6yc/35q8l02Hl1lPGJ6iynrNbLdx2xkRqm/385b2SDidnfcFw2xW6NdLCFylOAl/0qmdWlfP9l9dFlxjch7vfLaGhNcS9F02lKMeZwOp6bnJRNtfPHst/11fxg5fXt53E3Rnrvwdp4YvUJ4EvelWdN0AwbFK7j/BbVlrPu1tquXbmSMYXZia4uoPz/VMm8IOTx7Fih5vrn19DbbOfHe5o/31Rdpq08EXKk8AXvcodWwS8orG1w/2mabJ4UzV3vLGZES4nVx05PBnlHRSLxeDyw4fxwCXTqGry8515X7C+sgkDOGJELjXSwhcpTgJf9Cp3bCGTioaOgb9gbSW/eHUTgzLs3HneZBy2vvvWO2x4Ln+98FB2un08t3oXQ3OdDHOlU98SJBinVdeE6A1997dOpKQ9K1ftbujY2l24tpIJhZk8ddXhqMFZySitVx0xwsU3Zo4kYsKovHQGZ0UnXquViddECpPAF73GNM1Ou3TK3D7WVzZx1iGDU+4Cq4PxrVkjOXHcIE6eUEBBVnRoqVx8JVJZys+WKfoObyDcNjqnfZfOG5uqMYDTJw1OUmXxYbNauPuCKQBsqYlOtCYnbkUqkxa+6DX1sda9zWKwO9bCD0dM3thYzeEjchmS4hdYHYzCPS18OXErUpgEvug1e8amTyjMpLrJTyhismhdJTvdPi6eXpzk6uIr12nDYTWkhS9SmgS+6DV7TthOLsombML2Oi8PfVTK9OIcTp1YkOTq4sswDIpynLy9uYalJXXJLkeITkngi16z54Tt5KJsAG5/XVPfEuQHJ4/rt+sRt/fTU8djt1r44cvreWL5zi5tEwxH+N78tazZ1RDn6oSQwBe9yO3rGPhbarxcf8yottv93VEj83j+6iM4Y1Ihf/+wlFsXbuD+pdtp9u97AZWttV4+3u7mg231CaxUDFQySkf0Go8vSLrdwui8dHKcNmaPG8S3jhmZ7LISyma1cMdZk3DarSwvdfPe1lrqWgLccabq9Plbqr0A7HT7On1ciN4kgS96jbslSF66HZvVwsLrjibDbh0QXTlfZrMY/PL0iQA89FEpjy/byakTCzh+7KC9nrs5Npxzp1tWyxLxJ106ote4fUFcGdErTjMdtgEZ9l/2zZkjGV+QyR2va7Z3sgTi5ppoC7/M7SMia+KKOJPAF73GE2vhi/9x2Cz8+fzJWC0GN7+0ltJ2oW+aJltqmsmwWwmEzbZ59YWIFwl80WuiLXwJ/C8b7krnvoum0hoMc9V/VvPAB9v5pLSeXQ2tNPvDzB6XD3ScW7+rVpV5+Gzn/hecEWKPuPbhK6VKgSYgDIS01kfGc38ieUzTxOMLki8t/E6pIVk8d82R3LVkC0+tKOPJFWWMyc8A4NSJhby5qYYdbh8zR+d163X/sGQLrkwHj311ejzKFv1MIk7anqy1rk3AfkQS6epm/KEIedLC36eCTAd3XzCFZn+IZ1ft4pFPdmAAM0fnkWG3dvvEbbM/xE63jzpvgIhpYpFzJuIAZJSOOGiflzdwy4J1DM5ycMrEwmSXk/Ky0mxcd+woLBbYUe8j3W5lRF56t4dm6uroCB9vIExlo5/i3L6xXKRInngHvgksVkqZwMNa60f292Sr1cDlyujRjqxWS4+3jaeBUNfDL64lN93O89fP6pU1agfCMQP44ZmHtH09fnAWn2yvZ966Ko4clUcwHOG2V9ZzztQivvuVCZ1uv31dVdvXla0hJo9KrWM2UH6OvSnetcU78I/XWu9SSg0GliilNmmtl+7ryeGwicfTs/HILldGj7eNp/5eV3WTn1U73Fx3zCickUivvGZ/P2adObw4hzc3VPGnN3XbfWk2C/e/W8JYl7PTMfyfldbjSrfj8QVZu8PN4UO6t7DMonWVPPzxDhZ88yjs1t4fvzEQf44Hqye1FRZ2/Ur2uAa+1npX7P9qpdQC4Ghgn4Ev+p63t9RiAqcp6co5GHOmFnHeoUNobA3x/tY6ar0BLpxWxI0vruWXr27irEMGc/qkwUwrzqHc46MwK41N1c0cNjyXjVXNlNR6u73PpSV1VDX5Kan1MmnIwJj+AgAztgxlJIQRaMJMc0FXz38EW8BiA6sDTLPr26WIuAW+UioTsGitm2Jfnw78Jl77E8nxlq5hQmEmowel5kfkvsQwDHLT7cyZWtR2393nT+H+pdtYtL6KF9fsxmYxCEVMBmc5qG4OcO6UIYRM2NbJRV37Y5omX1Q0ArCpqjk1A980MQKNGK1uLL56LK1uLC012KpWYW3ahWl1YoR8GEFvNHgNC2BgYmBEAhAOgj0D056OaXFgBJqwNpVjbSqnwOKASBADE9PmJJJeAIYV07CAxQpYwGLBNKxgWMEwMPwN2BpKo6XZnBDyYzpdRDKHYlodYLFiGrbo9oY1ettii34NWHx1YBiEs4rB6sC0OKLbO/MIFs8iNGRG3A9pPFv4Q4AFSqk9+3lGa/1GHPcnEmxLTTNfVDTyneNGJ7uUfqs418md502mJRDmg5I61lc2Mdzl5IkVZQBMGpKF34Tl2+sIR8y2JSR3Nfho9of3uX7wrobWtgVrNsVO/iaLpXEn1sayaCC7t2Kr24TVvQWbZztGaO8/ZJG0XMK5Y7CEqzHtmZiOrGhrGzPWejcxbTmYVjtG0IfR2oARCWLaswgWHQHTvorf54sGblouluYKLK1uMMPR7SNhjD1fmxEwo7cjWcX41SUA0U8GNicWXx0WbyVGJASRcPQ1ImEM0x/9BGFGoo+ZkegfFSLYqz6PvmaoFcPvwYiECAw7hoYL5sX9WMct8LXW2wAZHNxPhSImv31zM/kZdi6aPjTZ5fR7GQ4rZxwymDMOiS4TedL4At7ZUstRI/NoiUAgbPL7xZsZkZdOXrqdv763jVAkwt8vmcaM4bl7vd7a3dHWfX6GnU1V3Q/8ysZWrn32c/58/hSmdHc21FAradtex16xHPvuT7HV6w4Ph7OKCedPwFc8i0j2MCLOfExnHhGni0j6ICI5I2Ot+Z5xuTLwpkofvmliBL2Y1sSsBifDMkWPPLOynI1VzfzxvENwycVWCTc4O43LDh8GwFGj8ynMcrC0pI6G1uhUzJOLsmn2h/jRK+u5afYYjhmTT0Gmg3W7G/GHInyxq5EMu5XTJw1m/poKQuEItm6cuF1aUk9Nc4APSuq6FPhGSy2OXR9jL3uftO2LsbS6iThyCA2eRvPkywkVTMa0ZxJ2jcV0pGD3UrwYRvQTSoJI4Itua/AFeXz5TmaPzecrMu4+6UbmZ/Dat2cBUOcNsKWmmcOGu6hp9nPL/HX8fskWAKwWg3BskXm71WDGsFymFGXz3GqT7fUtTCiMBo8/FGFzdTOHDs3e5wR4y3dEp3NYEzsP0KmqdWSufBpH2fvY6jYB0e6YwIgTaZ18BcHhx/W5k559nQS+6LanPi2nJRDm/2aPSXYp4ksGZToYlBmdm2e4K5153ziSzTVe1uxqoKLBz8TBmZS5fTy2bCdHjXQxKTaU84XPKlCDsxjhSufvH25nY1UzNxw3im/OGrXXPkLhCKvKPACs391IKGJii507MHz1ODfPJ23TPOy167FZ7ASLZ9E86wKCw44lNHgaWGw8uaKM8SE3x43JT9CRESCBL7ppp9vH85/t4vRJhYwvyEx2OeIADMNADc7a6+TtnKlFFGY6sFgM8tLtvLy2su2xTIeVWaPy+MdHO6hpDnDGpMFMKMxkp9vHxqomCjLT8AbCnDKxgLc311JS5WZq60qcm17AUfo2RiRIcPB0wmf8CffwszCdHecHqvMG+PsH25k+LEcCP8Ek8EWXVTa2cuO8L0i3W/nO8aOTXY44CEPbXRH9zNVH0BoMY7UYbK5uZuLgLAqz0rhzyWZeWVvJS2t2d9jWYkT/3aR8HLPt3xy+cBkZITeR9EK8U7/Ba9aTOWTKUUwamY/ZycnR97dGr91YW9FIsz9EVprEUKLIkRZ7eWlNBQWZaZw4vuPVnb9fvIUmf4h/zJ3GsNz0JFUneltBpqPt6/Z/CG47Q3HLieNYXe5hR70PV7qdUU4fa99+gguM9xizpIRDbTZWGEdTPeFCvMNO4OUNdazc6WHkxrW88O1jsHayv3e21JJms+APRVi508NJEwq6Ve/H2+vR1c18Y+bAWj6zN0jgi708vmwnw3KdHQK/ttnP8h1uvjlrZGpepCPiIttp48QxuTisq9u6bE6NBAkUTqVp0m/5XdkUnt/UCmuBtSXYrQbXzhzB06t28dVHlnHiuHyG5jjZ1dDKq+urmDQki5VlDVx22DAWfLGbZTvcbYFvmibzPt9NhsPCuVOKOq3HNE3uebeEMrePc6cMoTArMcMZ+wsJfNFBxDSp8wbwBSOYptk2SmPJ5ujH8NMnDU5ugSJhrLUbcG6ah3PzfCy+OiLpBfimXUvrpEsID4pO/Pa9KRGunB0gO81GZZOfTIeVoTlODh/u4rEVZTyzahehiInVgFmj81lZ5iEcMTl9UiE73S18sr2eT0rrMU34pNTNc6t3AWAxDM6ePKStlmA4ggGsLm9om1X03S11zD2suNvf1y6Pj4F69kkCX3TgbgkSNqHJH6K6OcCQ7GgLasmmaiYUZjJGplDo1wxfPWlbXsa5aR72mrWYFjuBMafROmkugREngrXjNRd2q6WtG2h8u774maPzOGPGMGrqmmloDWGzGLjS7eyob2F9ZROHDMni2DH5fLCtnu++tK5tu0tnFFNa38Jv3tzMByX1TBuWg6clwIIvKomYJkU5TlzpdnKcNt7dUtMh8L2BEOWe1n1eXQzw2oYqbn9d8+hl05k+bO8L0vo7CXzRQW1zoO3rrbVehmSnUeb2sXZ3EzfJMMz+KRzEsfO9WJfNW9FRNoXTaJr9W/wTL9hrlE132K2WDucIRuVnMCq20tecQ4sozHKQ67RjsRgYwKFDs/EGwjz0YSlLdA1vba4B4Lgx+dR6A+jqZr5+1AhsVoMnlu9ka62XTIcVq2Hwvfnr2Frr5ddnqQ6fDkzTpNkfJsNh5bFlOwF4eW1ljwK/sTVIpsPWNoVFXyOBLzqo8f5vIe2SGi/Hjcnn4Y9LSbNZOOsQ6c7pT6yebTjX/SfWZVMb7bKZ+o1ol03B5Ljv32GzcOL4vU/YZqXZ+PEp4/nByeNo8oewWw0yHTYCoQhLdA0nTRjELk8rjy/byeVPrmrbLt1uYXJRNr95Q1PV5Gf22EE0tAZ5bNlOVpV5mDkqj51uH6MHZfCWruGHJ49rGyFU2djKEyvKuPzwYW1/kL6sqsnPV59YyRVHDOP6Y0fH5ZjEmwS+6KAm1sJ3WA221npZW9HIm5tquHbWSAZnywmy/sC2eyUZnz2EY/tisNgIjD412mUz8qS9umySyRrrBtrDYbNwzpRoy31CYSa3nTERfyiC1WJQ0dDKaRMLGZ7n5KcLN/Lgh6U8+GEpADlOGyeMG8R7W+sYMyiDuy6aylf/uZzHl+1kfGEmraEI//x4B7XeAB9vr+fRy2a0vdeD4Qhvbqrm8OEuHvm4FG8gzAufVfD1o0bgtP9vDFJpfQuudPsBpxlpf14sGSTwRQd7unSmD8tlU3Uzf35nK4MyHVx91IgkVyYOhqWxnLStr+Dc/Aq2ug1E0nJpOeJmfNO+gZnR96bHMAyDOYd2PpLn/kumUu7xsXZ3IzlpdqYWZ5PjtLNmVwODMh1MGeFiXEEG/15Z3rZNUXYavz17Encu2cLX/rOaUyYWMjjLwWJdw5YaL+l2C63BCEeOdLFyp4fXN1Zz4bQ4gIVdAAAgAElEQVTopIGLN1Vz++uaIdlpPPLV6R0aRqFwBAwDqwG3vbaJUMTkrvPi/+lpXyTwRQc1Xj/5GXbU4Cw+jf1C/GnOZDIcnY2oFqnMCDSTtnk+zs0LsO/+FIDgkMNoOuF3tKpLwdF/x6oMd6Uz3NXxWpE9ffaGYfDAxVOpaPTjSrdjtUBhZhoOm4URLidPflrOK2t3EwibFGY5uP3MibyxsZqdbh9/njOZbz+/hoc+LGXe5xU0tYaoavIzuSib0voWbnhhDdfMHMnkIdlsrfVy/9JtZDpsnDB+EG9uip6P+Ly8ocMMpltrvHh8QY4c6Yr7cZHAFx3UNAcoyHQwoTAaBl89rJiTu3lhjEguS2M56Wv/hXPDs1gCjYTyFd6Zt9I6YQ6R3L3nxhmICrLSKOhkDP+UoTn8ac5kwhGTYDiC3WrBajE4d0oREdPEYhh85/jR/OOjHRRmOZhYmMnQHCdXHz0CXd3Mr9/Q/PbNzW2vN6Ewk+omP0+uKGPmKBebq708vnwnfxt2KNVNfj7cVs9f3yth2rBcCXyReLXNAQqz0vjKhAJaQxHObTfaQaQ2W+Uq0tc8SlrJawD4x52Db/q3CBUdnuTK+h6rxcBq6fip1hLrez9+7KBO1xiePiyXl649ig2VTVQ2+XHarcwclUdlYysvfFbB1UePYOG6Sh78sJQT7vsIfyi61OJRI1384ZxD9nq9eJDAFx3UeAOoIVk47VYumiYLmxyQGYkuweetwtJSjRFoxgi3YoT8EPZjhPwY4ejqR20MAzAw03KIOF2Yae0W98gaCt1ZDCMSwtj4Mq6P/469chURRw6+Gdfhm/oNItnDev3bFftnGAZThuYwpd2vznBXOj84eRwAcw8rpqKhlQyHlZF56YzOz2D6sNy22UbjTQJftAlFTOq9AQrbjZsWQKgVW+16jJ3lZFRuxdq4E2vjTixN5VhaaqJL2PUSE4NIxmAi2cWEs4cTySomnDOCcN4EQvkTMdMLYuurNuLc8Czpa/+FtamccM4ommb/Bv+kuQldUEN0T6bDxi9On5i0/Uvgizb13gAmUJg1sAPfaKnFXrkS++5PsVeuwlb9RXRRbMCKQSRrKOGckQSHHUckcwiRjMGEY/+babmYtjSwpmHanNGl66wOsHzpVy0Sxgg0RT8d+D1YWt0YLbVYm3dhadqFtWkXttr1WLcvjn5C2LOZM49w3nistRuwBL0EimdinnEX7sLZscW3hdg3CXzRpsYbDbXOTmb1Z5bm3djLluKoWIZt96fYGkoBMC0OQoOn4Zt+LcGiI8kYPR2Pmd+9Lpd97tSK6XRhOl1E9vc808TirYwu7l2vsdZvxubeSmDsmdH++cKpuFwZkCprtIqUJoEv2uxuaAUGQAs/HMBesQzHjveiy+/FFtGOpA8iWHQkrZOvJDj0KEKDp3YI94xkBKsR/UQRyRpKcMTsxO5b9DsS+AKIrmR1z7slDMp0MCqvH06QFglj3/URaVsWkrbtdSz+BkxrGsGhR9M86VICI06IzgApa6yKfkwCXxAxTb6/YB2hiMk/5k7rVxdZGa1unBueI33dU1ibyojYswiMOR3/+PMIDD8e7LKQixg4uhT4SqkLgXe01g2x2y7gJK31y/EsTiTG6rLoHOO/PXsS4/rJOrWWhh1kfP4wzo3PY4T9BIpn4T3m5/jHnAY254FfQIh+qKst/Nu11gv23NBae5RStwMS+P3AovWVZKVZOWn83heT9DXWmvVkfPYgaVsXgWGjVV2Eb/o32xbsEGIg62rgWw5iW5HCmv0h3t5cyzmTh3SY/a+vsdZtIvOTP5C24x0i9ix8M67HN/1bRDI7n2BLiIGoq6G9Uin1F+Dvsds3Aqv28/w2SikrsBLYpbU+t/slinhauK4SfyjCuVP65hQKhreazBV349z4HKYjG+/MW/FN/Tpm2sBbzUiIA+lq4N8M3AY8H7u9hGjod8X3gI1ATvdKE/FW7vHx0IelzBqVx6FD+9jC5EEfGWseIX31gxjhAL5p19Jy5PcOanUmIfq7LgW+1toL/LS7L66UGg6cA/we+EF3txfxY5omv31zM1aLwS9On5DURRm6y7F9CVkf/AprUxn+sWfRfMzPibhk+UUhDmS/ga+U+pvW+hal1CLA/PLjWus5B3j9vwE/AfpY87H/e2N9FavLG/jZqeMpyukbo1YsTbvIWnobaaWLCeVNxHPBPILDjkl2WUL0GQdq4f879v/d3X1hpdS5QLXWepVS6qSubGO1GtHLxHvAarX0eNt4SsW6/KEIf178KWpIFlfPHpdyCzJ3dsyMdfOwvvEjiIQJf+UOzKO/Q2aCl+NLxZ8lSF3dlap1QfxrM0xzr4Z7B7GTrk9pra/szgsrpe4EvgaEACfRPvz5Wuur9rVNMBg2PT28dN3lyqCn28ZTKtb1z0928MjHO7j/4kOZNTo/2eXspf0xM/wNZL3/c5xbXiFYdCSNp96btEU8UvFnCVJXd6VqXdCz2goLs1cBR3bluZ0Nt+xAax0GRimlujXBitb6Z1rr4Vrr0cBlRC/c2mfYi8RYXurmnx/vYM60oSkZ9u3ZqteQ9/yZpJW8infmT/Bc+KKs2CTEQejqKJ1twEdKqYWAd8+dWuu/xKUqERetwTC3vbaJMYMy+O35Uwi0BJJdUudME+faJ8n68NdEMgrwXPgSoaIjkl2VEH1eVwO/JPbPwv9OwO6/L6gdrfV7wHvdKUz0viW6BrcvyF1zDiHDYUvJwDcCzVhf/i7ZG+bjH3kyTafdJ0MtheglXQ38DVrree3vUEpdGod6RC9oCYT5/oJ1/OCkcagh/1v9aMEXlYzOT+ewYal5UZLVvZWc17+F4dmGd+attBxxIxgH7HUUQnRRV3+bftbF+0QKKKn1srq8gdc3Vrfdt7XWy9rdjVwwdWhKjrm373wf14tzsLS6CV+xgJYjb5awF6KXHWgc/lnA2cAwpdR97R7KITr6RqSgithCJp/tami77z8ry7FbDc6ZnGJTKJgm6V88TuZHvyacr2g4+1/kjJwoKzgJEQcH6tKpIDoPzhw6zp3TBHw/XkWJg1PRGA18XdWENxBia42XV9dX8fWjRuDKSOzY9f0KB8haehvpG57GP+YMGk+9Dxz9Y3pmIVLRfgNfa70GWKOUeib23JFaa52QykSP7Wnhh01YubOBhz8uZUh2Gt86ZmSSK/sfo9VNzuvX4ahYRsvhN+Kddat04QgRZ139DTsT+Bx4A0ApNSM2RFOkoN2NrYwdlIHVYvC7xZvZUuPlx18ZR3qKTH9srd9M3rxzsVd9RuOp9+I95mcS9kIkQFd/y+4AjgY8AFrrzwGZrSpFVTS0MnZQJpOHZOPxBbniiGGcOL4g2WUB4NjxDq6XzscItuC54AX86uJklyTEgNHVwA/uWd6wnS6PwxeJEzFNdjf6Kc51cvH0oZwxqZCbZ6fG32bnhmfIefUawjkjcV/6qlxMJUSCdXUc/nql1BWAVSk1Afgu8HH8yhI9VdMcIBQxKc5N45wpQzgnFRY2MU0yVt5L5oq7CYw8kYYzHpGTs0IkQVdb+DcDUwA/8AzQQHRhE5Fi9pywLc5NkSmPI2Gy3v85mSvuplVdQsPZT0jYC5EkXQ38ybF/NqIzX54PfBqvokTP7Y4NySxOhTnuQz5y3rie9PX/puXwG2k65a+Q4CmNhRD/09UunaeBHwHrgEj8yhEHa1eshZ/sRU2MVje5r12LbfdKmmb/htZp1ya1HiFE1wO/Rmu9KK6ViINmmia6qpnCLAdptuQNc7Q0VZC76CqsDaU0nvEQgfGydr0QqaCrgX+7UupR4G2i/fgAaK3nx6Uq0SP/WVnO+yV1XH30iKTVYK3bRO6iqzCCXhrm/IfgsGOTVosQoqOuBv43gEmAnf916ZiABH6KWL+7kfuWbufUiYX83/Gjk1KDrepzchddhWlNw3PhS4QLJielDiFE57oa+EdprVVcKxEH5T8ry8lKs/LLMyZgScJsmPaKZeT89xrM9Hw8c56VlamESEFd7ej9WCklzbUUtavBxztbarlo2lAyHV39G9577DveJXfhlUSyhuK58CUJeyFSVFfTYRbwuVJqO9E+fAMwtdbT4laZ6LKnV+7CMAzmHjYs4ft2lLxKzuKbCOUrGuY8jZk+KOE1CCG6pquBf2ZcqxA9tqrMw4ufV3DR9KEMyU5L6L7TNs0j+50fEhpyOA3nPomZlporaQkhoroU+FrrHfEuRHRfY2uQX722iRF56Xz3hLEJ3bdz7RNkL/0lgeHH03D242DPSOj+hRDdl/gOX9FrHv5oB7XeAP+64jAyHImb+jh91QNkLbsrumjJ6X8HWwpc1SuEOCAJ/D5qS00zL66p4OLpxUwuyk7MTk2TzGV/JGP1A7ROuECmShCij5HA74NC4Qh3LtlKdpqNbx+boBExZoSsD35F+ton8E2+kuYT/wCW1FhQRQjRNRL4fdBDH5Wydncjvzt7ErnpCWhhR0Jkv/0DnJvn0zLj23iP/SUkYay/EOLgSOD3MavLPTz1aXl0cZNDBsd/h6FWchbfSNr2N/HO/AktR9wsYS9EHyWB38c8uaKM/Aw7t5wY/1E5RqCZnNe+iWPXRzSd8Dtap14T930KIeJHVo7uQ7bWevl4u5uvHjYMZ5wXJDda3eS+chn2imU0nvo3CXsh+oG4tfCVUk5gKZAW28+LWuvb47W//mZXg4/XNlRz7cyRWC3RLpSnV5bjtFm4aPrQuO7b4q0kd+GV0emNz3yEwNgz4ro/IURixLOF7we+orWeDswAzlRKzYrj/vqVV9dX8cjHO/isPLp2/NqKRl5dX8VF04fiiuOJWkvjTlzzL8baWEbDuU9J2AvRj8Stha+1NoHm2E177J8Zr/31N9vrfAC8sama6cNy+N3izRRmObg+jsMwrXWa3IVXYIT9eC54ntCQw+K2LyFE4sX1pK1SygqsAsYDf9daL9/f861WA5erZ5foW62WHm8bTz2tqzy2VOG7W2rJyXSwra6Fh686nGGDc+JSl1GxGusrl4I1jdCV/yVrcPImR+1vP8t4k7q6J1XrgvjXZphm/BvdSikXsAC4WWu9bl/PCwbDpsfT0qN9uFwZ9HTbeOpJXeGIyQn3fciIvHRKaqPbzp1RzI9PGR+XuuzlH5Hz2rWY6YPwzHmGSO7oXtvPwdaWSqSu7pG6uq8ntRUWZq8CjuzKcxMySkdr7QHeRWbd7JLdja0EwiaXzihmcJaD6cU53HJSfIZhOra9Qe5/v04keziei+YnPeyFEPETz1E6hUBQa+1RSqUDpwF/jNf++pPS+uhf+PEFmTz99SPISrNhs/T+xU5pG18g+90fERo8nYZzn8J05vX6PoQQqSOeffhDgSdj/fgW4AWt9X/juL9+o7Q+esJ2dH5G3KZOsCx/kJx3fklg+GwaznoUHJlx2Y8QInXEc5TOF4AM8+iB0roW8jPs8Ql70yRj+Z+xrroP/7izaTztfrAmduEUIURyyNQKKWh7fQuj8uNwpt6MkLX0l6Sve4rI9KtoPPb3MuOlEAOITK2QYqqb/JTUehnT24EfDpC95GbS1z1Fy2HfIXzOvRL2Qgww0sJPIc3+ELcsiI5avXRGce+9cNBHzhvXk7bzXZqP+Rm+w2/EJTNeCjHgSOCnkAc+2M62Wi/3XjSV8YW9cxLV8DeQ++o12HavpOmkP9I65cpeeV0hRN8jgZ8idjX4eHltJRdMG8rM0b0zPNJoqcG18Eqs7i00nf4g/gnn9crrCiH6Jgn8FPHYJzuxGnDtzJG98noWbyW5L8/F2rybhnP+RXDkSb3yukKIvksCPwWs2OHm1Q1VfPWwYQzOPvghkpbm3eS+PBdLSzUN5/2HYPHMXqhSCNHXSeAnWWVjK794dROj8jO44bjRB/16lqYKXC9fiuGro+G8pwkN7dIUG0KIAUACP4kipsmv39AEwxH+NGcyGY6DGyZpaSzH9cpcjFY3DXOeJlR0RC9VKoToD2QcfhK9/MVuVpY1cMuJYxl9kOPuLY1l0ZZ9q4eGOc9I2Ash9iIt/CTZXtfCfUu3c9RIF+dPLTqo17I07MD18lyMYDMN5z9LaPD0XqpSCNGfSOAnQX1LgFsWrCPNZuFXZ0zEOIiLoCye7dFunKCPhvOfI1Q4tRcrFUL0JxL4SXDnki3UeQM8PHcaRTnOHr+O1bON3JfnxpYkfIFwQfJWqRJCpD7pw0+wtRWNvLe1jmuOHsGUoT1frtDqLiF3waUY4QCe85+XsBdCHJC08BPINE0e+GA7+Rl2rjhieI9fx1q/BdfLcwEz2rIfNKn3ihRC9FvSwo+jLTXNrCn3tN1etK6K1eUNfHPWqB4PwbTWaVwvXwogYS+E6BZp4cfRnUu2stPjY8G1R1Hd7OdP72zlqJEuLp4+tEevZ63biOuVyzANGw0XPE84r/cWNRdC9H8S+HESDEfQ1U0Ewib3f7CNFTs8ZDqs/ObsSVh7sD6ttXZDNOytdhoumEfYFZ9FzYUQ/ZcEfpyU1HoJhE0Ks9JY8EUlTpuFf8ydRkGmo9uvZatZR+4rl2Ha0/Gc/wIR15g4VCyE6O+kDz9O1lc2AXDPpdOYUpTNXXMm92hUjq36C3Jf+SqmPRPPBS9K2Ashekxa+HGyobKJXKeNWWPyeeLKnq3lbqv6nNxFV2I6svFcMI9IzoherlIIMZBICz9ONlQ2M7kou8dX0doqV5O78HLMtNxoy17CXghxkCTw48AXDLOtzsuUouwebW+rXEXuwiswnfmxln3Px+wLIcQeEvhx8NKa3URMmDK0+4Fvq1hB7sIriGQU4rlwHpHsYXGoUAgxEEng97IXPqvg3ve3ceK4Qcwand+tbe0Vy3AtuopI5hAaLpxHJKs4TlUKIQYiCfxetNPt42/vl3D82HzuOu8QbN0Yb28vW0ruoqsIZxfTcME8IpkHN2WyEEJ8WdxG6SilRgBPAUMAE3hEa31vvPaXbKZp8ue3t+KwWvjF6ROxWbv+t9RR+hY5b3ybsGssnjnPYmYUxLFSIcRAFc8Wfgj4odZ6MjALuFEp1W+ndJz3eQXLdri54bjR3bq4ylHyKjmvf4vQoEl4LnhBwl4IETdxC3yt9W6t9erY103ARqBfnoF8dX0Vf36nhBPGDeKSGV3vd0/T88l58/8IDZ5Bw5xnMZ15caxSCDHQJaQPXyk1GjgMWJ6I/SXS2opGfrd4M0eNdPGHc7veb+/c8CzZb32PYPFMPOc9jZnW87nxhRCiKwzTNOO6A6VUFvA+8Hut9fz9PTcSiZjhcM/qsVothMORHm3bUzVNfi78x8c4rBYWfOdYctPtXarL8ukjWBf/lMjYUwhf8hTY0xNV8n7rShWpWpvU1T1SV/f1pDa73boKOLIrz43r1ApKKTvwEvD0gcIeIBw28XhaerQvlyujx9v2xK4GHze/uJaGliCPXj4D0x/E4w/uvy7TJGPF3WSuvBf/2DNpPP3v4DWBxNXdaV0pJlVrk7q6R+rqvp7UVljY9et94jlKxwAeAzZqrf8Sr/0kgzcQ4v9e+ILmQJi/XzoNNTjrwBtFQmS9/3PSNzyDb/LlNJ94J1hkKiMhROLEM3GOA74GrFVKfR677+da69fiuM+EeGDpdnY3+vnnZdOZVtyFvvdQKzmLbyRt+5t4j/guLTN/DD2cY0cIIXoqboGvtf4Q6HeptrzUzYtrdnP54cOYPiz3wBv4POQuugpHxTKaZv+G1mnXxr9IIYTohPQpdMPm6mZuXbSBMYMy+M7xow/4fEvDDmyvXwPuUhpPewD/xAviXqMQQuyLBP5+hCImP3x5HTOG5XLcmHxufmktmQ4r9110KOn2/S9Cbtu9ktzXrgXDpOH8ZwkWz0pQ1UII0TkJ/P34oqKBj7e7+Xi7m4c/3sGgDDsPXDKNohznfrdL2/wy2e/8kHBWMeYVLxC09GzRciGE6E0yedp+fFhSj81icP2xo5henMOjl89gzKCMfW9gRshY8RdyltxEcMgMPJcshPxxiStYCCH2Q1r4+/HBtjoOH57LdceM4rpjRu33uUagiey3biFt+5u0qktoOvmPYE1LUKVCCHFgEvj7UOb2UVrv4+LpB54bx+rZRs5r38Tq2Ubz8Xfgm/ZNGXYphEg5Evj78N8NVQAcP3b/i5g4St8me8lNYLHTMOcZgsOPS0R5QgjRbRL4nXjk41IeX7aTk8YPYrhrH/PcmBEyVt1PxvK7CRVMofGsR2XtWSFESpPA/5LFm6r55yc7OXfKEH5x2oROn2P4G8h+54ekbXuD1okXRfvrbYmfAE0IIbpDAr+dWm+AP729lSlF2dFVqzqZ6thWvYacN7+DpblC+uuFEH2KBH5MY2uQn7yyHl8wzO1nqr3D3jRxrn2CrI9+SySjAM+FLxEqOiI5xQohRA9I4BMN+xte+ILS+hZ+f84he421N/yNZL37E5wl/8U/6hSaTv2brE4lhOhzBnzgh8IRbl20ke11Lfz1winMGt1xVI6tZi05b9yApamc5mN+ge+wb4Mh16sJIfqeAR34pmly51tbWLnTwx1nqo5hb5o41z1F1oe/JpKej+fCFwkNPSp5xQohxEEa0IH/wAelLFxXxXXHjOScKUPa7jda3WS/91PSSl4lMPIkGk+9DzN9/+PxhRAi1Q3YwP/3p2U89WkZl0wf2mHaBHv5R2S/9T0svlqaj/k5vsNukC4cIUS/MCADf/4Xu7lv6XZOU4X86CvjMQwDwgEyV9xN+uqHCLvG4Dl7IaHB05JdqhBC9JoBFfjeQIgnV5Txr+VlHDsmj1+fpbBaDKzuErKX3IS9Zi2+yVfSfPztYN/PrJhCCNEHDZjAv3/pNp5ZtYtQxOT8Q4v46anjsVkMnOufJuvDOzCtaTSc9U8CY89KdqlCCBEXAyLwS+tb+Pen5ZwwbhCXHzGMw4fnYml1k/3uj0nb/iaB4cfTdOrfiGQWJbtUIYSImwER+P/5tByHzcLPTpvAoEwH9rKlZL/1fSyt9TQfexu+GdfJiVkhRL/X7wO/3OPj1Q1VXDC1iEH2IFnv3U76+n8TyhuP+9ynCBdOSXaJQgiREP068DdVNfH9BetJs1m4YWQF+c9fjaWxjJbp1+Od9WOZ4VIIMaD028CvbfZz04trcdlCLJr0JkVLniKSM5KGC18kWDwz2eUJIUTC9cvAj5gmv35jM5NCm3gi+1+kb96Ob+rVNB/zCxluKYQYsPpd4IfCEe5ZsoETdz3It22vYppD8cx5juCI45NdmhBCJFW/CnyPL8h/XnqW/3Pfx1hbJb5DLsd7/K8wHdnJLk0IIZIuboGvlHocOBeo1lofGq/9ANzwwhrSvWVc3vocv468R2PmcDynPUNwxAnx3K0QQvQp8WzhPwE8ADwVx30AcI/tQUa3vEoIK9snXk/WyTICRwghvixuga+1XqqUGh2v12+vYNzRhCcdScOIc8jKLk7ELoUQos9JqT58q9XA5erBKJoTb8awWsgJR3q/qINktVp69j3FWarWBalbm9TVPVJX98W7tpQK/HDYxONp6dG2LldGj7eNJ6mr+1K1Nqmre6Su7utJbYWFXR+UIhPICCHEACGBL4QQA0TcAl8p9SzwSfRLVa6U+ma89iWEEOLA4jlK5/J4vbYQQojuky4dIYQYICTwhRBigJDAF0KIAcIwTTPZNbRXA+xIdhFCCNGHjAIKu/LEVAt8IYQQcSJdOkIIMUBI4AshxAAhgS+EEAOEBL4QQgwQEvhCCDFASOALIcQAkVLz4feEUupM4F7ACjyqtb4rSXWMILqc4xDABB7RWt+rlLoDuI7oNQYAP9dav5aE+kqBJiAMhLTWRyql8oHngdFAKTBXa+1OYE0qtv89xgK/Alwk+Jh1tgbzvo6PUsog+p47G2gBrtFar05wbX8GzgMCQAnwDa21J7bK3EZAxzZfprW+IYF13cE+fnZKqZ8B3yT6Hvyu1vrNBNb1PKBiT3EBHq31jAQfr31lRMLeZ326ha+UsgJ/B84CJgOXK6UmJ6mcEPBDrfVkYBZwY7ta/qq1nhH7l/Cwb+fkWA1Hxm7/FHhbaz0BeDt2O2F01Ayt9QzgCKJv6gWxhxN9zJ4AzvzSffs6PmcBE2L/rgceSkJtS4BDtdbTgM3Az9o9VtLu2MUlvPZTF3Tys4v9LlwGTIlt82Ds9zchdWmtv9ruvfYSML/dw4k6XvvKiIS9z/p04ANHA1u11tu01gHgOeD8ZBSitd6956+v1rqJaKthWDJq6YbzgSdjXz8JXJDEWk4h+ouXlCuttdZLgfov3b2v43M+8JTW2tRaLwNcSqmhiaxNa71Yax2K3VwGDI/X/rtT136cDzyntfZrrbcDW4n+/ia0rlireS7wbDz2vT/7yYiEvc/6euAPA8ra3S4nBUI29jHxMGB57K6blFJfKKUeV0rlJaksE1islFqllLo+dt8QrfXu2NeVRD9qJstldPwlTIVjtq/jk2rvu2uB19vdHqOU+kwp9b5SanYS6unsZ5cqx2w2UKW13tLuvoQfry9lRMLeZ3098FOOUiqL6EfGW7TWjUQ/ho0DZgC7gXuSVNrxWuvDiX5MvFEpdUL7B7XWJtE/CgmnlHIAc4B5sbtS5Zi1Sebx2R+l1C+IdhU8HbtrNzBSa30Y8APgGaVUTgJLSrmf3ZdcTseGRcKPVycZ0Sbe77O+Hvi7gBHtbg+P3ZcUSik70R/k01rr+QBa6yqtdVhrHQH+SZw+xh6I1npX7P9qov3kRwNVez4ixv6vTkZtRP8IrdZaV8VqTIljxr6PT0q875RS1xA9OXllLCiIdZnUxb5eRfSE7sRE1bSfn13Sj5lSygZcRLuBAok+Xp1lBAl8n/X1wP8UmKCUGhNrJV4GLExGIbG+wceAjVrrv7S7v32f24XAuiTUlqmUyt7zNXB6rI6FwNWxp10NvJLo2mI6tLpS4ZjF7B/4FxkAAAMFSURBVOv4LAS+rpQylFKzgIZ2H8kTIjY67SfAHK11S7v7C/ecDFVKjSV6wm9bAuva189uIXCZUipNKTUmVteKRNUVcyqwSWtdvueORB6vfWUECXyf9elhmVrrkFLqJuBNosMyH9dar09SOccBXwPWKqU+j933c6Ijh2YQ/ZhWCnw7CbUNARZER0FiA57RWr+hlPoUeCG23vAOoiezEir2B+g0Oh6XPyX6mMXWYD4JKFBKlQO3A3fR+fF5jehQua1ERxZ9Iwm1/X979w4aRRTFYfyzMRgFbRTFQhHhIApaWShIULC2iCiaIFiJQrATQUHSWgs2FgFtFLSxE4uFYOGj8RE4vZ2CBBQUSbC4d3HxRVySTcL9ftVymR1mdnYOw8zc/7kKDAFP6nHtvk54BJiMiO/APHAhMxf6YHUxtmvkT8cuM99FxH1ghnIL6lJmzg1quzLzDr8/J4IB/l78vUYM7H9mPLIkNWK139KRJC2QBV+SGmHBl6RGWPAlqREWfElqhAVfWgQRMRIRj5d7O6R/seBLUiN8D19NiYgxYAJYSwmuugjMUmIAjlPCq05n5oc6geg2MEyZcn++5pTvruObKdnuJylT4G8AH4F9wCtgrBt5IK0EXuGrGRGxBzgFHK656HPAWWA98DIz9wIdyoxRKM0qrtTM+Tc94/eAW5m5HzhECeCCkn54mdKbYRdlZqW0YqzqaAXpPx2jNFp5UeMI1lGCqub5Gah1F3gYERuBTZnZqeNTwIOaSbQ9Mx8BZOZXgLq+592cljp1ficwvfS7JS2MBV8tWQNMZWZvdygi4vovy/V7G+Zbz+c5PL+0wnhLRy15CoxGxBYoPWsjYgflPBity5wBpjNzFvjU0xBjHOjUTkXvI+JEXcdQRAwPdC+kPlnw1YzMnAGuUTp/vab0hd0GfAEORsRb4CgwWb9yDrhZlz3QMz4OTNTxZ8DWwe2F1D/f0lHzIuJzZm5Y7u2QlppX+JLUCK/wJakRXuFLUiMs+JLUCAu+JDXCgi9JjbDgS1IjfgBA0py/y2y5lgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:45:14.418045Z",
     "start_time": "2020-12-03T13:45:12.962937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
