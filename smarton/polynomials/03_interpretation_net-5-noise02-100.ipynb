{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 2, #degree\n",
    "        'n': 5, #number of variables\n",
    "        'monomial_vars': None, #int or None\n",
    "        'laurent': False, #use Laurent polynomials (negative degree with up to -d)  \n",
    "        'neg_d': 0,#int or None\n",
    "        'neg_d_prob': 0,\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': 5,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 100,\n",
    "        'a_min': -100,\n",
    "        'lambda_nets_total': 1000,\n",
    "        'noise': 0.2,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'border_min': 0.2, #needs to be between 0 and (x_max-x_min)/2\n",
    "        'border_max': 0.4,\n",
    "        'lower_degree_prob': 0.5,\n",
    "        'a_zero_prob': 0.25,\n",
    "        'a_random_prob': 0.1,      \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True,  #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sample_sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000,\n",
    "    },\n",
    "    'i_net': {\n",
    "        'optimizer': 'custom',#adam\n",
    "        'inet_loss': 'mae',\n",
    "        'inet_metrics': ['r2'],\n",
    "        'dropout': 0.25,\n",
    "        'dropout_output': 0,\n",
    "        'epochs': 500, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "        'dense_layers': [512, 1024],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'interpretation_net_output_monomials': 5, #(None, int) #CONSTANT IS NOT INCLUDED\n",
    "        'interpretation_net_output_shape': None, #calculated automatically later\n",
    "        'test_size': 100, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'normalize_inet_data': False,\n",
    "        'inet_training_without_noise': True, #dataset size without noise hardcoded to 50k in generate_paths\n",
    "        \n",
    "\n",
    "        'evaluate_with_real_function': False,\n",
    "        'consider_labels_training': False,\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },\n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': True,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "        \n",
    "        #set if samples analysis should be performed\n",
    "        'samples_list': None,#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "       \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "        \n",
    "        'symbolic_metamodeling_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_evaluation': False,\n",
    "        'symbolic_metamodeling_function_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_function_evaluation': False,\n",
    "        \n",
    "        'symbolic_regression_evaluation': True,\n",
    "        'per_network_evaluation': False,\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 10,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "n_jobs = min((epochs_lambda//each_epochs_save_lambda+1, n_jobs)) if multi_epoch_analysis else min(len(samples_list), n_jobs) if samples_list!=None else 1\n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "data_reshape_version = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 243\n",
      "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 2], [0, 0, 0, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 1, 2], [0, 0, 0, 2, 0], [0, 0, 0, 2, 1], [0, 0, 0, 2, 2], [0, 0, 1, 0, 0], [0, 0, 1, 0, 1], [0, 0, 1, 0, 2], [0, 0, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 2], [0, 0, 1, 2, 0], [0, 0, 1, 2, 1], [0, 0, 1, 2, 2], [0, 0, 2, 0, 0], [0, 0, 2, 0, 1], [0, 0, 2, 0, 2], [0, 0, 2, 1, 0], [0, 0, 2, 1, 1], [0, 0, 2, 1, 2], [0, 0, 2, 2, 0], [0, 0, 2, 2, 1], [0, 0, 2, 2, 2], [0, 1, 0, 0, 0], [0, 1, 0, 0, 1], [0, 1, 0, 0, 2], [0, 1, 0, 1, 0], [0, 1, 0, 1, 1], [0, 1, 0, 1, 2], [0, 1, 0, 2, 0], [0, 1, 0, 2, 1], [0, 1, 0, 2, 2], [0, 1, 1, 0, 0], [0, 1, 1, 0, 1], [0, 1, 1, 0, 2], [0, 1, 1, 1, 0], [0, 1, 1, 1, 1], [0, 1, 1, 1, 2], [0, 1, 1, 2, 0], [0, 1, 1, 2, 1], [0, 1, 1, 2, 2], [0, 1, 2, 0, 0], [0, 1, 2, 0, 1], [0, 1, 2, 0, 2], [0, 1, 2, 1, 0], [0, 1, 2, 1, 1], [0, 1, 2, 1, 2], [0, 1, 2, 2, 0], [0, 1, 2, 2, 1], [0, 1, 2, 2, 2], [0, 2, 0, 0, 0], [0, 2, 0, 0, 1], [0, 2, 0, 0, 2], [0, 2, 0, 1, 0], [0, 2, 0, 1, 1], [0, 2, 0, 1, 2], [0, 2, 0, 2, 0], [0, 2, 0, 2, 1], [0, 2, 0, 2, 2], [0, 2, 1, 0, 0], [0, 2, 1, 0, 1], [0, 2, 1, 0, 2], [0, 2, 1, 1, 0], [0, 2, 1, 1, 1], [0, 2, 1, 1, 2], [0, 2, 1, 2, 0], [0, 2, 1, 2, 1], [0, 2, 1, 2, 2], [0, 2, 2, 0, 0], [0, 2, 2, 0, 1], [0, 2, 2, 0, 2], [0, 2, 2, 1, 0], [0, 2, 2, 1, 1], [0, 2, 2, 1, 2], [0, 2, 2, 2, 0], [0, 2, 2, 2, 1], [0, 2, 2, 2, 2], [1, 0, 0, 0, 0], [1, 0, 0, 0, 1], [1, 0, 0, 0, 2], [1, 0, 0, 1, 0], [1, 0, 0, 1, 1], [1, 0, 0, 1, 2], [1, 0, 0, 2, 0], [1, 0, 0, 2, 1], [1, 0, 0, 2, 2], [1, 0, 1, 0, 0], [1, 0, 1, 0, 1], [1, 0, 1, 0, 2], [1, 0, 1, 1, 0], [1, 0, 1, 1, 1], [1, 0, 1, 1, 2], [1, 0, 1, 2, 0], [1, 0, 1, 2, 1], [1, 0, 1, 2, 2], [1, 0, 2, 0, 0], [1, 0, 2, 0, 1], [1, 0, 2, 0, 2], [1, 0, 2, 1, 0], [1, 0, 2, 1, 1], [1, 0, 2, 1, 2], [1, 0, 2, 2, 0], [1, 0, 2, 2, 1], [1, 0, 2, 2, 2], [1, 1, 0, 0, 0], [1, 1, 0, 0, 1], [1, 1, 0, 0, 2], [1, 1, 0, 1, 0], [1, 1, 0, 1, 1], [1, 1, 0, 1, 2], [1, 1, 0, 2, 0], [1, 1, 0, 2, 1], [1, 1, 0, 2, 2], [1, 1, 1, 0, 0], [1, 1, 1, 0, 1], [1, 1, 1, 0, 2], [1, 1, 1, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 2], [1, 1, 1, 2, 0], [1, 1, 1, 2, 1], [1, 1, 1, 2, 2], [1, 1, 2, 0, 0], [1, 1, 2, 0, 1], [1, 1, 2, 0, 2], [1, 1, 2, 1, 0], [1, 1, 2, 1, 1], [1, 1, 2, 1, 2], [1, 1, 2, 2, 0], [1, 1, 2, 2, 1], [1, 1, 2, 2, 2], [1, 2, 0, 0, 0], [1, 2, 0, 0, 1], [1, 2, 0, 0, 2], [1, 2, 0, 1, 0], [1, 2, 0, 1, 1], [1, 2, 0, 1, 2], [1, 2, 0, 2, 0], [1, 2, 0, 2, 1], [1, 2, 0, 2, 2], [1, 2, 1, 0, 0], [1, 2, 1, 0, 1], [1, 2, 1, 0, 2], [1, 2, 1, 1, 0], [1, 2, 1, 1, 1], [1, 2, 1, 1, 2], [1, 2, 1, 2, 0], [1, 2, 1, 2, 1], [1, 2, 1, 2, 2], [1, 2, 2, 0, 0], [1, 2, 2, 0, 1], [1, 2, 2, 0, 2], [1, 2, 2, 1, 0], [1, 2, 2, 1, 1], [1, 2, 2, 1, 2], [1, 2, 2, 2, 0], [1, 2, 2, 2, 1], [1, 2, 2, 2, 2], [2, 0, 0, 0, 0], [2, 0, 0, 0, 1], [2, 0, 0, 0, 2], [2, 0, 0, 1, 0], [2, 0, 0, 1, 1], [2, 0, 0, 1, 2], [2, 0, 0, 2, 0], [2, 0, 0, 2, 1], [2, 0, 0, 2, 2], [2, 0, 1, 0, 0], [2, 0, 1, 0, 1], [2, 0, 1, 0, 2], [2, 0, 1, 1, 0], [2, 0, 1, 1, 1], [2, 0, 1, 1, 2], [2, 0, 1, 2, 0], [2, 0, 1, 2, 1], [2, 0, 1, 2, 2], [2, 0, 2, 0, 0], [2, 0, 2, 0, 1], [2, 0, 2, 0, 2], [2, 0, 2, 1, 0], [2, 0, 2, 1, 1], [2, 0, 2, 1, 2], [2, 0, 2, 2, 0], [2, 0, 2, 2, 1], [2, 0, 2, 2, 2], [2, 1, 0, 0, 0], [2, 1, 0, 0, 1], [2, 1, 0, 0, 2], [2, 1, 0, 1, 0], [2, 1, 0, 1, 1], [2, 1, 0, 1, 2], [2, 1, 0, 2, 0], [2, 1, 0, 2, 1], [2, 1, 0, 2, 2], [2, 1, 1, 0, 0], [2, 1, 1, 0, 1], [2, 1, 1, 0, 2], [2, 1, 1, 1, 0], [2, 1, 1, 1, 1], [2, 1, 1, 1, 2], [2, 1, 1, 2, 0], [2, 1, 1, 2, 1], [2, 1, 1, 2, 2], [2, 1, 2, 0, 0], [2, 1, 2, 0, 1], [2, 1, 2, 0, 2], [2, 1, 2, 1, 0], [2, 1, 2, 1, 1], [2, 1, 2, 1, 2], [2, 1, 2, 2, 0], [2, 1, 2, 2, 1], [2, 1, 2, 2, 2], [2, 2, 0, 0, 0], [2, 2, 0, 0, 1], [2, 2, 0, 0, 2], [2, 2, 0, 1, 0], [2, 2, 0, 1, 1], [2, 2, 0, 1, 2], [2, 2, 0, 2, 0], [2, 2, 0, 2, 1], [2, 2, 0, 2, 2], [2, 2, 1, 0, 0], [2, 2, 1, 0, 1], [2, 2, 1, 0, 2], [2, 2, 1, 1, 0], [2, 2, 1, 1, 1], [2, 2, 1, 1, 2], [2, 2, 1, 2, 0], [2, 2, 1, 2, 1], [2, 2, 1, 2, 2], [2, 2, 2, 0, 0], [2, 2, 2, 0, 1], [2, 2, 2, 0, 2], [2, 2, 2, 1, 0], [2, 2, 2, 1, 1], [2, 2, 2, 1, 2], [2, 2, 2, 2, 0], [2, 2, 2, 2, 1], [2, 2, 2, 2, 2]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192e6a0c8eb04738a6e71324da655bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 21\n",
      "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 2], [0, 0, 0, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 2, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 1], [0, 0, 1, 1, 0], [0, 0, 2, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 1], [0, 1, 0, 1, 0], [0, 1, 1, 0, 0], [0, 2, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 1], [1, 0, 0, 1, 0], [1, 0, 1, 0, 0], [1, 1, 0, 0, 0], [2, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import flatten, rec_gen\n",
    "\n",
    "list_of_monomial_identifiers_extended = []\n",
    "\n",
    "if laurent:\n",
    "    variable_sets = [list(flatten([[_d for _d in range(d+1)], [-_d for _d in range(1, neg_d+1)]])) for _ in range(n)]\n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)    \n",
    "        \n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity:' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)        \n",
    "else:\n",
    "    variable_sets = [[_d for _d in range(d+1)] for _ in range(n)]  \n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)\n",
    "\n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity: ' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)    \n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    if np.sum(monomial_identifier) <= d:\n",
    "        if monomial_vars == None or len(list(filter(lambda x: x != 0, monomial_identifier))) <= monomial_vars:\n",
    "            list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "#print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "#print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "config['i_net']['data_reshape_version'] = data_reshape_version\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d']) if not laurent else len(list_of_monomial_identifiers)\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "\n",
    "config['i_net']['interpretation_net_output_shape'] = config['data']['sparsity'] if config['i_net']['interpretation_net_output_monomials'] is None else config['data']['sparsity']*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials']\n",
    "\n",
    "\n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sample_sparsity')*config['data']['sample_sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "\n",
    "\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='interpretation_net'))\n",
    "create_folders_inet()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inet_dense512-1024-output_110_drop0.25e500b256_custom/lnets_10000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-100_amax_100_xdist_uniform_noise_normal_0.2bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n",
      "lnets_1000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-100_amax_100_xdist_uniform_noise_normal_0.2bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net_data)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index, no_noise=False):\n",
    "        \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path_identifier_lambda_net_data_loading = None \n",
    "                \n",
    "    if no_noise==True:\n",
    "        path_identifier_lambda_net_data_loading = generate_paths(path_type='interpretation_net_no_noise')['path_identifier_lambda_net_data']\n",
    "        print('interpretation_net_no_noise', path_identifier_lambda_net_data_loading)\n",
    "    else:\n",
    "        path_identifier_lambda_net_data_loading = path_identifier_lambda_net_data \n",
    "        print(path_identifier_lambda_net_data_loading)\n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_identifier_lambda_net_data_loading + '/'\n",
    "    path_weights = directory + 'weights_epoch_' + str(index).zfill(3) + '.txt'\n",
    "    path_X_data = directory + 'lambda_X_test_data.txt'\n",
    "    path_y_data = directory + 'lambda_y_test_data.txt'        \n",
    "    \n",
    "    weight_data = pd.read_csv(path_weights, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == True:\n",
    "        weight_data = weight_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_X_test_data = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == True:\n",
    "        lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_y_test_data = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == True:\n",
    "        lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "    lambda_nets = [None] * weight_data.shape[0]\n",
    "    for i, (row_weights, row_lambda_X_test_data, row_lambda_y_test_data) in enumerate(zip(weight_data.values, lambda_X_test_data.values, lambda_y_test_data.values)):        \n",
    "        lambda_net = LambdaNet(row_weights, row_lambda_X_test_data, row_lambda_y_test_data)\n",
    "        lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpretation_net_no_noise lnets_10000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-100_amax_100_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend MultiprocessingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 out of   1 | elapsed:   49.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lnets_1000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-100_amax_100_xdist_uniform_noise_normal_0.2bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend MultiprocessingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 out of   1 | elapsed:    7.5s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if inet_training_without_noise:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list_without_noise = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1, no_noise=True) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "else:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:30:49.711839Z",
     "start_time": "2021-01-05T09:29:48.873305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-37.792</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-88.373</td>\n",
       "      <td>85.823</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.935</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>70.591</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-20.020</td>\n",
       "      <td>-37.431</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>4.189</td>\n",
       "      <td>-82.886</td>\n",
       "      <td>144.033</td>\n",
       "      <td>1.590</td>\n",
       "      <td>88.264</td>\n",
       "      <td>-38.581</td>\n",
       "      <td>2.760</td>\n",
       "      <td>-1.511</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-1.617</td>\n",
       "      <td>-1.058</td>\n",
       "      <td>33.377</td>\n",
       "      <td>-2.924</td>\n",
       "      <td>-2.825</td>\n",
       "      <td>41.985</td>\n",
       "      <td>1.985</td>\n",
       "      <td>-17.119</td>\n",
       "      <td>3.841</td>\n",
       "      <td>-41.826</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-6.902</td>\n",
       "      <td>4.344</td>\n",
       "      <td>-85.652</td>\n",
       "      <td>82.791</td>\n",
       "      <td>1.692</td>\n",
       "      <td>104.740</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-2.073</td>\n",
       "      <td>5.079</td>\n",
       "      <td>-1.907</td>\n",
       "      <td>-1.490</td>\n",
       "      <td>0.215</td>\n",
       "      <td>1.099</td>\n",
       "      <td>-1.795</td>\n",
       "      <td>1.470</td>\n",
       "      <td>70.988</td>\n",
       "      <td>2.448</td>\n",
       "      <td>-2.394</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.386</td>\n",
       "      <td>1.425</td>\n",
       "      <td>2.452</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.437</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.221</td>\n",
       "      <td>1.355</td>\n",
       "      <td>2.878</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.126</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.395</td>\n",
       "      <td>1.650</td>\n",
       "      <td>2.227</td>\n",
       "      <td>2.579</td>\n",
       "      <td>2.499</td>\n",
       "      <td>2.556</td>\n",
       "      <td>2.398</td>\n",
       "      <td>0.264</td>\n",
       "      <td>2.402</td>\n",
       "      <td>-2.252</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>1.125</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.601</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.136</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.076</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.249</td>\n",
       "      <td>0.581</td>\n",
       "      <td>1.198</td>\n",
       "      <td>1.033</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.323</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.904</td>\n",
       "      <td>-3.001</td>\n",
       "      <td>-2.536</td>\n",
       "      <td>-3.267</td>\n",
       "      <td>-3.319</td>\n",
       "      <td>2.636</td>\n",
       "      <td>3.390</td>\n",
       "      <td>3.541</td>\n",
       "      <td>-3.098</td>\n",
       "      <td>-2.480</td>\n",
       "      <td>-3.545</td>\n",
       "      <td>-2.762</td>\n",
       "      <td>0.209</td>\n",
       "      <td>2.384</td>\n",
       "      <td>1.115</td>\n",
       "      <td>3.307</td>\n",
       "      <td>-3.263</td>\n",
       "      <td>2.623</td>\n",
       "      <td>3.489</td>\n",
       "      <td>3.007</td>\n",
       "      <td>2.788</td>\n",
       "      <td>3.523</td>\n",
       "      <td>-3.208</td>\n",
       "      <td>3.175</td>\n",
       "      <td>0.806</td>\n",
       "      <td>-2.189</td>\n",
       "      <td>2.039</td>\n",
       "      <td>2.969</td>\n",
       "      <td>1.880</td>\n",
       "      <td>3.270</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.257</td>\n",
       "      <td>2.081</td>\n",
       "      <td>1.691</td>\n",
       "      <td>2.926</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-6.110</td>\n",
       "      <td>-3.986</td>\n",
       "      <td>0.668</td>\n",
       "      <td>3.175</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2.055</td>\n",
       "      <td>0.078</td>\n",
       "      <td>1.959</td>\n",
       "      <td>1.356</td>\n",
       "      <td>1.388</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>1.790</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>1.739</td>\n",
       "      <td>1.321</td>\n",
       "      <td>0.751</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.270</td>\n",
       "      <td>1.710</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>1.776</td>\n",
       "      <td>1.460</td>\n",
       "      <td>-1.019</td>\n",
       "      <td>1.804</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>1.109</td>\n",
       "      <td>1.049</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.282</td>\n",
       "      <td>0.813</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.089</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>1.104</td>\n",
       "      <td>1.144</td>\n",
       "      <td>1.053</td>\n",
       "      <td>1.128</td>\n",
       "      <td>1.169</td>\n",
       "      <td>1.515</td>\n",
       "      <td>1.205</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>1.428</td>\n",
       "      <td>-3.766</td>\n",
       "      <td>-4.637</td>\n",
       "      <td>-3.807</td>\n",
       "      <td>-4.844</td>\n",
       "      <td>2.480</td>\n",
       "      <td>2.624</td>\n",
       "      <td>2.540</td>\n",
       "      <td>-3.782</td>\n",
       "      <td>-3.258</td>\n",
       "      <td>-3.485</td>\n",
       "      <td>-6.607</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-7.397</td>\n",
       "      <td>-5.215</td>\n",
       "      <td>2.509</td>\n",
       "      <td>-4.932</td>\n",
       "      <td>2.793</td>\n",
       "      <td>2.286</td>\n",
       "      <td>2.684</td>\n",
       "      <td>2.405</td>\n",
       "      <td>2.449</td>\n",
       "      <td>-3.720</td>\n",
       "      <td>2.424</td>\n",
       "      <td>-4.191</td>\n",
       "      <td>-3.282</td>\n",
       "      <td>0.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>9.300</td>\n",
       "      <td>90.715</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-57.684</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-18.513</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-94.709</td>\n",
       "      <td>11.504</td>\n",
       "      <td>88.303</td>\n",
       "      <td>-2.565</td>\n",
       "      <td>-6.115</td>\n",
       "      <td>-49.017</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.008</td>\n",
       "      <td>3.513</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>2.078</td>\n",
       "      <td>-1.244</td>\n",
       "      <td>2.056</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-4.418</td>\n",
       "      <td>-17.972</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-1.439</td>\n",
       "      <td>-2.247</td>\n",
       "      <td>-87.818</td>\n",
       "      <td>9.349</td>\n",
       "      <td>88.152</td>\n",
       "      <td>1.944</td>\n",
       "      <td>0.702</td>\n",
       "      <td>-57.383</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.883</td>\n",
       "      <td>2.330</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.646</td>\n",
       "      <td>-0.778</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-1.168</td>\n",
       "      <td>-1.252</td>\n",
       "      <td>1.460</td>\n",
       "      <td>2.997</td>\n",
       "      <td>-19.602</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-1.398</td>\n",
       "      <td>0.804</td>\n",
       "      <td>-96.146</td>\n",
       "      <td>1.298</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>5.042</td>\n",
       "      <td>1.410</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>1.818</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>3.711</td>\n",
       "      <td>1.787</td>\n",
       "      <td>1.846</td>\n",
       "      <td>1.248</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>2.504</td>\n",
       "      <td>-1.009</td>\n",
       "      <td>1.703</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.878</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>3.480</td>\n",
       "      <td>-1.269</td>\n",
       "      <td>1.426</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.588</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.908</td>\n",
       "      <td>1.072</td>\n",
       "      <td>0.423</td>\n",
       "      <td>1.057</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.173</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.750</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0.726</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.297</td>\n",
       "      <td>1.052</td>\n",
       "      <td>0.952</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.399</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.868</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>1.042</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>1.188</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.153</td>\n",
       "      <td>0.465</td>\n",
       "      <td>1.147</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-2.503</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.971</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-4.271</td>\n",
       "      <td>1.948</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.958</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>2.401</td>\n",
       "      <td>2.009</td>\n",
       "      <td>-0.657</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-2.372</td>\n",
       "      <td>2.407</td>\n",
       "      <td>-0.726</td>\n",
       "      <td>2.231</td>\n",
       "      <td>2.617</td>\n",
       "      <td>2.239</td>\n",
       "      <td>2.346</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2.247</td>\n",
       "      <td>2.431</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-2.499</td>\n",
       "      <td>0.420</td>\n",
       "      <td>1.443</td>\n",
       "      <td>1.388</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.746</td>\n",
       "      <td>1.403</td>\n",
       "      <td>0.276</td>\n",
       "      <td>1.442</td>\n",
       "      <td>1.459</td>\n",
       "      <td>1.434</td>\n",
       "      <td>1.518</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-3.677</td>\n",
       "      <td>1.504</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-2.120</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-9.993</td>\n",
       "      <td>-2.049</td>\n",
       "      <td>2.053</td>\n",
       "      <td>1.979</td>\n",
       "      <td>-1.584</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-6.686</td>\n",
       "      <td>-1.684</td>\n",
       "      <td>-1.702</td>\n",
       "      <td>-2.187</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-4.966</td>\n",
       "      <td>1.973</td>\n",
       "      <td>-1.821</td>\n",
       "      <td>2.499</td>\n",
       "      <td>1.782</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.927</td>\n",
       "      <td>5.399</td>\n",
       "      <td>-10.045</td>\n",
       "      <td>1.937</td>\n",
       "      <td>-1.716</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-40.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>48.856</td>\n",
       "      <td>49.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>61.743</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>60.947</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-20.566</td>\n",
       "      <td>-11.635</td>\n",
       "      <td>-13.282</td>\n",
       "      <td>70.383</td>\n",
       "      <td>23.628</td>\n",
       "      <td>-10.092</td>\n",
       "      <td>60.333</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.634</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-4.225</td>\n",
       "      <td>4.041</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-1.522</td>\n",
       "      <td>62.396</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>-1.314</td>\n",
       "      <td>1.970</td>\n",
       "      <td>-1.009</td>\n",
       "      <td>-1.621</td>\n",
       "      <td>-33.767</td>\n",
       "      <td>-2.618</td>\n",
       "      <td>47.623</td>\n",
       "      <td>49.487</td>\n",
       "      <td>5.269</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-1.937</td>\n",
       "      <td>-3.644</td>\n",
       "      <td>64.303</td>\n",
       "      <td>4.946</td>\n",
       "      <td>-4.090</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-6.595</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>56.457</td>\n",
       "      <td>-1.773</td>\n",
       "      <td>-0.655</td>\n",
       "      <td>3.443</td>\n",
       "      <td>1.369</td>\n",
       "      <td>3.655</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>2.145</td>\n",
       "      <td>2.045</td>\n",
       "      <td>2.228</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>2.424</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-1.014</td>\n",
       "      <td>0.169</td>\n",
       "      <td>2.097</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>1.870</td>\n",
       "      <td>2.269</td>\n",
       "      <td>2.195</td>\n",
       "      <td>2.251</td>\n",
       "      <td>2.088</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>2.089</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-1.472</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.789</td>\n",
       "      <td>1.455</td>\n",
       "      <td>1.297</td>\n",
       "      <td>0.807</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>1.073</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.164</td>\n",
       "      <td>1.021</td>\n",
       "      <td>1.191</td>\n",
       "      <td>0.543</td>\n",
       "      <td>1.147</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.933</td>\n",
       "      <td>1.220</td>\n",
       "      <td>0.106</td>\n",
       "      <td>1.468</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>1.802</td>\n",
       "      <td>2.519</td>\n",
       "      <td>2.639</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>2.103</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.029</td>\n",
       "      <td>2.446</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>1.742</td>\n",
       "      <td>2.600</td>\n",
       "      <td>2.157</td>\n",
       "      <td>1.932</td>\n",
       "      <td>2.622</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>2.267</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>1.401</td>\n",
       "      <td>0.954</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>1.555</td>\n",
       "      <td>2.734</td>\n",
       "      <td>2.162</td>\n",
       "      <td>2.591</td>\n",
       "      <td>-1.511</td>\n",
       "      <td>-1.789</td>\n",
       "      <td>2.774</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-2.526</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>2.560</td>\n",
       "      <td>1.079</td>\n",
       "      <td>1.849</td>\n",
       "      <td>2.169</td>\n",
       "      <td>2.327</td>\n",
       "      <td>2.233</td>\n",
       "      <td>2.028</td>\n",
       "      <td>-1.298</td>\n",
       "      <td>2.094</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-2.754</td>\n",
       "      <td>-1.698</td>\n",
       "      <td>-1.080</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>2.022</td>\n",
       "      <td>1.618</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.524</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>1.961</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-1.251</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.558</td>\n",
       "      <td>1.647</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>2.630</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.305</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.721</td>\n",
       "      <td>0.748</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>1.772</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.689</td>\n",
       "      <td>1.667</td>\n",
       "      <td>0.674</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>2.263</td>\n",
       "      <td>-3.412</td>\n",
       "      <td>-2.063</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-3.346</td>\n",
       "      <td>2.054</td>\n",
       "      <td>2.004</td>\n",
       "      <td>1.922</td>\n",
       "      <td>-3.132</td>\n",
       "      <td>-2.967</td>\n",
       "      <td>1.928</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-3.313</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>1.948</td>\n",
       "      <td>-1.982</td>\n",
       "      <td>2.195</td>\n",
       "      <td>1.685</td>\n",
       "      <td>2.099</td>\n",
       "      <td>1.860</td>\n",
       "      <td>1.785</td>\n",
       "      <td>-2.678</td>\n",
       "      <td>1.760</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-4.025</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-38.064</td>\n",
       "      <td>-40.577</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-38.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.878</td>\n",
       "      <td>-27.399</td>\n",
       "      <td>-58.160</td>\n",
       "      <td>3.975</td>\n",
       "      <td>-2.145</td>\n",
       "      <td>1.487</td>\n",
       "      <td>0.168</td>\n",
       "      <td>58.313</td>\n",
       "      <td>3.866</td>\n",
       "      <td>-1.395</td>\n",
       "      <td>-2.692</td>\n",
       "      <td>-16.492</td>\n",
       "      <td>-15.983</td>\n",
       "      <td>0.715</td>\n",
       "      <td>5.109</td>\n",
       "      <td>1.646</td>\n",
       "      <td>-3.230</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-36.226</td>\n",
       "      <td>-38.880</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-4.018</td>\n",
       "      <td>0.456</td>\n",
       "      <td>2.056</td>\n",
       "      <td>59.452</td>\n",
       "      <td>-2.651</td>\n",
       "      <td>1.977</td>\n",
       "      <td>-1.566</td>\n",
       "      <td>-3.362</td>\n",
       "      <td>-37.719</td>\n",
       "      <td>1.530</td>\n",
       "      <td>3.293</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>0.571</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>0.929</td>\n",
       "      <td>1.646</td>\n",
       "      <td>-3.908</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.061</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>1.014</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.498</td>\n",
       "      <td>1.122</td>\n",
       "      <td>1.262</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.275</td>\n",
       "      <td>1.703</td>\n",
       "      <td>0.348</td>\n",
       "      <td>2.129</td>\n",
       "      <td>1.429</td>\n",
       "      <td>0.891</td>\n",
       "      <td>1.238</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-1.243</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>-1.788</td>\n",
       "      <td>-1.270</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>2.719</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>-1.887</td>\n",
       "      <td>-1.352</td>\n",
       "      <td>-1.496</td>\n",
       "      <td>-1.024</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>-1.122</td>\n",
       "      <td>2.551</td>\n",
       "      <td>-1.379</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-1.913</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-1.897</td>\n",
       "      <td>-1.886</td>\n",
       "      <td>-1.388</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>-1.228</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.650</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.477</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.733</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.068</td>\n",
       "      <td>1.214</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.667</td>\n",
       "      <td>1.074</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-1.430</td>\n",
       "      <td>1.217</td>\n",
       "      <td>1.804</td>\n",
       "      <td>1.432</td>\n",
       "      <td>1.324</td>\n",
       "      <td>1.154</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.594</td>\n",
       "      <td>1.486</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>1.193</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>1.997</td>\n",
       "      <td>-1.087</td>\n",
       "      <td>-2.491</td>\n",
       "      <td>1.893</td>\n",
       "      <td>1.885</td>\n",
       "      <td>1.831</td>\n",
       "      <td>1.219</td>\n",
       "      <td>1.972</td>\n",
       "      <td>1.022</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.931</td>\n",
       "      <td>1.016</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.909</td>\n",
       "      <td>1.167</td>\n",
       "      <td>1.077</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.034</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.247</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.795</td>\n",
       "      <td>1.072</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.994</td>\n",
       "      <td>-1.698</td>\n",
       "      <td>-1.619</td>\n",
       "      <td>-2.089</td>\n",
       "      <td>-1.677</td>\n",
       "      <td>0.122</td>\n",
       "      <td>2.451</td>\n",
       "      <td>-1.074</td>\n",
       "      <td>-2.161</td>\n",
       "      <td>-1.707</td>\n",
       "      <td>-1.458</td>\n",
       "      <td>-1.464</td>\n",
       "      <td>-1.453</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.522</td>\n",
       "      <td>2.107</td>\n",
       "      <td>-1.574</td>\n",
       "      <td>1.861</td>\n",
       "      <td>-1.721</td>\n",
       "      <td>2.303</td>\n",
       "      <td>4.313</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>-2.015</td>\n",
       "      <td>-1.510</td>\n",
       "      <td>-1.224</td>\n",
       "      <td>-2.090</td>\n",
       "      <td>-0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>61.796</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>65.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.238</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-81.773</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-60.609</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-4.601</td>\n",
       "      <td>9.082</td>\n",
       "      <td>-8.435</td>\n",
       "      <td>6.703</td>\n",
       "      <td>60.399</td>\n",
       "      <td>-7.485</td>\n",
       "      <td>18.047</td>\n",
       "      <td>-3.654</td>\n",
       "      <td>-5.445</td>\n",
       "      <td>53.112</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>0.980</td>\n",
       "      <td>5.249</td>\n",
       "      <td>3.942</td>\n",
       "      <td>-3.711</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-81.639</td>\n",
       "      <td>2.591</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-53.400</td>\n",
       "      <td>-3.441</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-2.460</td>\n",
       "      <td>62.512</td>\n",
       "      <td>1.127</td>\n",
       "      <td>2.893</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>0.330</td>\n",
       "      <td>62.776</td>\n",
       "      <td>-2.465</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>0.107</td>\n",
       "      <td>6.945</td>\n",
       "      <td>3.718</td>\n",
       "      <td>0.906</td>\n",
       "      <td>-82.911</td>\n",
       "      <td>1.232</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-62.360</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>2.100</td>\n",
       "      <td>-1.538</td>\n",
       "      <td>2.439</td>\n",
       "      <td>2.385</td>\n",
       "      <td>-2.346</td>\n",
       "      <td>-1.816</td>\n",
       "      <td>2.619</td>\n",
       "      <td>1.850</td>\n",
       "      <td>2.102</td>\n",
       "      <td>2.638</td>\n",
       "      <td>1.275</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>2.030</td>\n",
       "      <td>2.401</td>\n",
       "      <td>-2.829</td>\n",
       "      <td>2.541</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-1.831</td>\n",
       "      <td>-2.578</td>\n",
       "      <td>4.001</td>\n",
       "      <td>-3.065</td>\n",
       "      <td>2.188</td>\n",
       "      <td>2.364</td>\n",
       "      <td>2.572</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>1.127</td>\n",
       "      <td>1.762</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.142</td>\n",
       "      <td>1.272</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-2.352</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.840</td>\n",
       "      <td>2.052</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.806</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>1.740</td>\n",
       "      <td>2.297</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-1.136</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-1.149</td>\n",
       "      <td>5.493</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>2.222</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>2.410</td>\n",
       "      <td>1.882</td>\n",
       "      <td>-1.923</td>\n",
       "      <td>2.281</td>\n",
       "      <td>-1.081</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>2.753</td>\n",
       "      <td>-1.061</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>2.731</td>\n",
       "      <td>2.059</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.735</td>\n",
       "      <td>-1.337</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>-1.116</td>\n",
       "      <td>1.798</td>\n",
       "      <td>-2.008</td>\n",
       "      <td>-1.503</td>\n",
       "      <td>2.155</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>2.067</td>\n",
       "      <td>2.042</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>1.571</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>-1.326</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-3.205</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.314</td>\n",
       "      <td>1.857</td>\n",
       "      <td>1.160</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.688</td>\n",
       "      <td>1.840</td>\n",
       "      <td>0.514</td>\n",
       "      <td>-1.930</td>\n",
       "      <td>-2.316</td>\n",
       "      <td>2.081</td>\n",
       "      <td>1.657</td>\n",
       "      <td>1.941</td>\n",
       "      <td>0.599</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>1.716</td>\n",
       "      <td>1.501</td>\n",
       "      <td>-2.766</td>\n",
       "      <td>1.615</td>\n",
       "      <td>0.854</td>\n",
       "      <td>2.056</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.983</td>\n",
       "      <td>-1.332</td>\n",
       "      <td>0.962</td>\n",
       "      <td>-2.306</td>\n",
       "      <td>0.913</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>1.063</td>\n",
       "      <td>-1.636</td>\n",
       "      <td>1.715</td>\n",
       "      <td>-1.639</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.720</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>1.047</td>\n",
       "      <td>0.275</td>\n",
       "      <td>1.981</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.850</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-2.494</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-2.206</td>\n",
       "      <td>-6.113</td>\n",
       "      <td>-2.394</td>\n",
       "      <td>-2.323</td>\n",
       "      <td>4.209</td>\n",
       "      <td>2.640</td>\n",
       "      <td>-5.199</td>\n",
       "      <td>-2.126</td>\n",
       "      <td>-2.748</td>\n",
       "      <td>-1.995</td>\n",
       "      <td>10.226</td>\n",
       "      <td>-5.121</td>\n",
       "      <td>-5.074</td>\n",
       "      <td>-2.566</td>\n",
       "      <td>2.862</td>\n",
       "      <td>-2.139</td>\n",
       "      <td>0.391</td>\n",
       "      <td>2.361</td>\n",
       "      <td>2.844</td>\n",
       "      <td>9.477</td>\n",
       "      <td>2.663</td>\n",
       "      <td>-2.136</td>\n",
       "      <td>-2.704</td>\n",
       "      <td>-7.079</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "521  1373158606         0.000       -37.792         0.000         0.000   \n",
       "737  1373158606         9.300        90.715         0.000         0.000   \n",
       "740  1373158606         0.000       -40.002         0.000        48.856   \n",
       "660  1373158606       -38.064       -40.577         0.000         0.000   \n",
       "411  1373158606         0.000         0.000         0.000         0.000   \n",
       "\n",
       "     00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "521         0.000       -88.373        85.823         0.000        99.935   \n",
       "737       -57.684         0.000         0.000         0.000         0.000   \n",
       "740        49.219         0.000         0.000         0.000         0.000   \n",
       "660         0.000         0.000        59.750         0.000         0.000   \n",
       "411        61.796         0.000         0.000         0.000         0.000   \n",
       "\n",
       "     00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "521         0.000         0.000         0.000         0.000         0.000   \n",
       "737         0.000         0.000         0.000         0.000         0.000   \n",
       "740        61.743         0.000         0.000         0.000         0.000   \n",
       "660         0.000         0.000       -38.149         0.000         0.000   \n",
       "411        65.432         0.000         0.000         0.000         7.238   \n",
       "\n",
       "     02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "521         0.000         0.000         0.000         0.000        70.591   \n",
       "737         0.000         0.000       -18.513         0.000         0.000   \n",
       "740         0.000        60.947         0.000         0.000         0.000   \n",
       "660         0.000         0.000         0.000         0.000         0.000   \n",
       "411         0.000         0.000       -81.773         0.000         0.000   \n",
       "\n",
       "     11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "521         0.000         0.000             -20.020             -37.431   \n",
       "737         0.000       -94.709              11.504              88.303   \n",
       "740         0.000         0.000             -20.566             -11.635   \n",
       "660         0.000        -2.878             -27.399             -58.160   \n",
       "411       -60.609         0.000              -4.601               9.082   \n",
       "\n",
       "     00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "521              -0.076              -0.253               4.189   \n",
       "737              -2.565              -6.115             -49.017   \n",
       "740             -13.282              70.383              23.628   \n",
       "660               3.975              -2.145               1.487   \n",
       "411              -8.435               6.703              60.399   \n",
       "\n",
       "     00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "521             -82.886             144.033               1.590   \n",
       "737               1.017               0.008               3.513   \n",
       "740             -10.092              60.333              -0.120   \n",
       "660               0.168              58.313               3.866   \n",
       "411              -7.485              18.047              -3.654   \n",
       "\n",
       "     00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "521              88.264             -38.581               2.760   \n",
       "737              -0.374              -0.890               2.078   \n",
       "740              -0.634               0.367               0.190   \n",
       "660              -1.395              -2.692             -16.492   \n",
       "411              -5.445              53.112              -2.114   \n",
       "\n",
       "     01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "521              -1.511               0.364              -1.617   \n",
       "737              -1.244               2.056              -1.688   \n",
       "740              -4.225               4.041               0.021   \n",
       "660             -15.983               0.715               5.109   \n",
       "411               0.980               5.249               3.942   \n",
       "\n",
       "     02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "521              -1.058              33.377              -2.924   \n",
       "737              -0.057              -4.418             -17.972   \n",
       "740              -1.522              62.396               0.194   \n",
       "660               1.646              -3.230              -0.427   \n",
       "411              -3.711              -0.985             -81.639   \n",
       "\n",
       "     10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "521              -2.825              41.985               1.985   \n",
       "737              -0.312              -1.439              -2.247   \n",
       "740              -1.050              -1.314               1.970   \n",
       "660               0.386               0.256               0.902   \n",
       "411               2.591              -0.028             -53.400   \n",
       "\n",
       "     20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "521             -17.119               3.841             -41.826   \n",
       "737             -87.818               9.349              88.152   \n",
       "740              -1.009              -1.621             -33.767   \n",
       "660               0.190             -36.226             -38.880   \n",
       "411              -3.441               0.217              -0.093   \n",
       "\n",
       "     00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "521              -0.902              -6.902               4.344   \n",
       "737               1.944               0.702             -57.383   \n",
       "740              -2.618              47.623              49.487   \n",
       "660              -0.977              -4.018               0.456   \n",
       "411               0.421              -2.460              62.512   \n",
       "\n",
       "     00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "521             -85.652              82.791               1.692   \n",
       "737              -0.490              -0.883               2.330   \n",
       "740               5.269              -0.538              -1.937   \n",
       "660               2.056              59.452              -2.651   \n",
       "411               1.127               2.893              -0.791   \n",
       "\n",
       "     00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "521             104.740              -0.321              -2.073   \n",
       "737              -0.068               0.646              -0.778   \n",
       "740              -3.644              64.303               4.946   \n",
       "660               1.977              -1.566              -3.362   \n",
       "411               0.330              62.776              -2.465   \n",
       "\n",
       "     01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "521               5.079              -1.907              -1.490   \n",
       "737               0.151              -1.168              -1.252   \n",
       "740              -4.090              -0.462              -6.595   \n",
       "660             -37.719               1.530               3.293   \n",
       "411              -0.617               0.107               6.945   \n",
       "\n",
       "     02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "521               0.215               1.099              -1.795   \n",
       "737               1.460               2.997             -19.602   \n",
       "740              -0.909              56.457              -1.773   \n",
       "660              -0.574               0.571              -0.036   \n",
       "411               3.718               0.906             -82.911   \n",
       "\n",
       "     10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "521               1.470              70.988               2.448   \n",
       "737              -0.329              -1.398               0.804   \n",
       "740              -0.655               3.443               1.369   \n",
       "660              -1.526               0.929               1.646   \n",
       "411               1.232               0.384             -62.360   \n",
       "\n",
       "     20000-lstsq_target   wb_0   wb_1  wb_2   wb_3   wb_4   wb_5  wb_6   wb_7  \\\n",
       "521              -2.394  0.288  0.773 0.386  1.425  2.452  2.355 2.437  0.221   \n",
       "737             -96.146  1.298 -0.108 5.042  1.410 -1.006 -0.950 1.818 -0.422   \n",
       "740               3.655 -0.348 -0.146 0.207 -0.207  2.145  2.045 2.228 -0.953   \n",
       "660              -3.908  0.322  0.033 0.603  0.503  0.061  1.006 0.559 -0.271   \n",
       "411              -0.046  2.100 -1.538 2.439  2.385 -2.346 -1.816 2.619  1.850   \n",
       "\n",
       "      wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "521  0.221 1.355  2.878 -0.213  0.126  1.010  2.395  1.650  2.227  2.579   \n",
       "737  3.711 1.787  1.846  1.248 -0.050  2.504 -1.009  1.703 -0.994 -0.941   \n",
       "740 -0.642 2.424  0.264 -0.221 -1.014  0.169  2.097 -0.065  1.870  2.269   \n",
       "660  0.386 0.693  0.784  0.261  0.274  0.605  0.950  0.672 -0.430 -0.139   \n",
       "411  2.102 2.638  1.275 -1.272  2.030  2.401 -2.829  2.541 -0.043 -1.831   \n",
       "\n",
       "     wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "521  2.499  2.556  2.398  0.264  2.402 -2.252 -0.098  0.910 -0.135  1.125   \n",
       "737 -0.878 -0.963 -0.645  3.480 -1.269  1.426 -0.498  0.629 -0.052 -0.097   \n",
       "740  2.195  2.251  2.088 -0.500  2.089 -0.224 -1.472 -0.065  0.063 -0.342   \n",
       "660  1.014  0.063 -0.045  0.305  0.350  0.244 -0.114  0.663  0.760  0.565   \n",
       "411 -2.578  4.001 -3.065  2.188  2.364  2.572 -0.456  0.140 -0.350  0.168   \n",
       "\n",
       "     wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "521 -0.103  0.601  1.013  1.032  1.136  1.040  0.235  0.016 -0.418 -0.074   \n",
       "737  0.608  0.388  0.869  0.785  0.064  0.126  0.697  0.548  0.588 -0.124   \n",
       "740 -0.296  0.534  0.975  0.789  1.455  1.297  0.807 -0.444 -0.417  1.073   \n",
       "660  0.540 -0.351  0.498  1.122  1.262  1.005  0.802  0.474  0.424  0.675   \n",
       "411  0.187 -1.724  1.127  1.762  0.391  0.607  0.357 -0.471 -0.110  0.420   \n",
       "\n",
       "     wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "521  0.115  0.545  0.076  1.056  1.249  0.581  1.198  1.033  1.049  1.323   \n",
       "737  0.125  0.387  0.738  0.908  1.072  0.423  1.057 -0.237  0.173  1.093   \n",
       "740 -0.294  0.493  0.164  1.021  1.191  0.543  1.147  0.977  0.933  1.220   \n",
       "660  0.546  0.251  0.873  0.275  1.703  0.348  2.129  1.429  0.891  1.238   \n",
       "411  0.186 -0.158  0.351  0.142  1.272  0.123 -2.352  0.711  0.349  0.840   \n",
       "\n",
       "     wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "521  0.333  0.904 -3.001 -2.536 -3.267 -3.319  2.636  3.390  3.541 -3.098   \n",
       "737  0.750 -0.494  0.726 -0.003 -0.080  0.724  0.297  1.052  0.952 -0.521   \n",
       "740  0.106  1.468 -0.196 -0.080 -0.456 -0.282  1.802  2.519  2.639 -0.942   \n",
       "660  1.035  0.404 -1.243 -1.060 -1.788 -1.270 -0.440  2.719 -0.794 -1.887   \n",
       "411  2.052 -0.484 -0.817  0.194 -0.806 -0.910  1.740  2.297 -0.475 -1.136   \n",
       "\n",
       "     wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "521 -2.480 -3.545 -2.762  0.209  2.384  1.115  3.307 -3.263  2.623  3.489   \n",
       "737  0.159  0.735  0.921  0.906  0.173  0.500  0.943  0.746  0.399  1.075   \n",
       "740 -0.678  2.103  0.014  0.178 -0.281  0.029  2.446 -0.059  1.742  2.600   \n",
       "660 -1.352 -1.496 -1.024 -0.724 -0.894 -1.122  2.551 -1.379 -0.986 -1.913   \n",
       "411 -0.282 -1.149  5.493  0.131 -0.226 -0.254  2.222 -0.845 -0.374  2.410   \n",
       "\n",
       "     wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "521  3.007  2.788  3.523 -3.208  3.175  0.806 -2.189  2.039  2.969  1.880   \n",
       "737  0.705  0.459  0.439 -0.171  0.664  0.868 -0.045  1.042  0.212 -0.639   \n",
       "740  2.157  1.932  2.622 -0.852  2.267  0.111 -0.714  1.401  0.954 -0.459   \n",
       "660  2.250  0.258 -1.897 -1.886 -1.388 -0.772 -1.228  0.592  0.684 -0.018   \n",
       "411  1.882 -1.923  2.281 -1.081 -0.396 -0.351 -0.080 -0.601  2.753 -1.061   \n",
       "\n",
       "     wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "521  3.270  0.892  0.304  0.257  2.081  1.691  2.926  0.736  0.102 -6.110   \n",
       "737  1.188  0.399 -0.073  0.563  0.038  0.036  1.153  0.465  1.147 -0.480   \n",
       "740  1.555  2.734  2.162  2.591 -1.511 -1.789  2.774 -0.272  0.095 -2.526   \n",
       "660  0.740  0.219  0.907  0.497  0.375  0.205  0.650  0.234  0.650 -0.085   \n",
       "411 -0.558  2.731  2.059 -0.463 -0.735 -1.337 -0.707 -1.116  1.798 -2.008   \n",
       "\n",
       "     wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "521 -3.986  0.668  3.175  0.077  0.253  0.479  0.357  0.086  2.055  0.078   \n",
       "737 -2.503  0.258  0.971 -0.217 -0.165  0.022 -0.112 -4.271  1.948 -0.376   \n",
       "740 -0.212  2.560  1.079  1.849  2.169  2.327  2.233  2.028 -1.298  2.094   \n",
       "660  0.207  0.975  0.477 -0.267 -0.733  0.890  0.029 -0.640  0.379 -0.058   \n",
       "411 -1.503  2.155 -0.835 -0.456  2.067  2.042 -0.941  1.571 -0.842 -1.326   \n",
       "\n",
       "     wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "521  1.959  1.356   1.388  -0.138   1.790   0.316   0.218  -0.089  -0.578   \n",
       "737  0.958 -0.351  -0.770  -0.065   0.015  -0.852   2.401   2.009  -0.657   \n",
       "740  0.154 -2.754  -1.698  -1.080  -0.022  -1.839   0.448   0.184  -0.376   \n",
       "660  0.697  0.068   1.214   1.500   1.667   1.074  -0.054  -1.430   1.217   \n",
       "411  0.216 -0.340   0.320  -3.205   0.754   0.314   1.857   1.160   0.871   \n",
       "\n",
       "     wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "521   1.739   1.321   0.751  -0.115  -0.520   0.696   0.126   0.297   0.150   \n",
       "737   0.185  -0.087  -0.884  -0.826  -0.869   0.018  -2.372   2.407  -0.726   \n",
       "740   2.022   1.618   0.024  -0.524  -0.539   1.961  -0.159   0.557  -1.251   \n",
       "660   1.804   1.432   1.324   1.154   0.994   1.594   1.486  -0.969   1.193   \n",
       "411   0.688   1.840   0.514  -1.930  -2.316   2.081   1.657   1.941   0.599   \n",
       "\n",
       "     wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "521   0.204   0.473   0.065   0.132   0.270   1.710   0.197  -0.146   1.776   \n",
       "737   2.231   2.617   2.239   2.346   4.000   2.247   2.431  -0.471   0.286   \n",
       "740   0.477   0.748   0.327   0.392   0.558   1.647   0.476  -0.355   2.630   \n",
       "660  -0.877   1.997  -1.087  -2.491   1.893   1.885   1.831   1.219   1.972   \n",
       "411  -0.001   1.716   1.501  -2.766   1.615   0.854   2.056   0.417   0.283   \n",
       "\n",
       "     wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "521   1.460  -1.019   1.804  -0.407   1.109   1.049   0.964   1.559   1.282   \n",
       "737   0.379  -0.176  -2.499   0.420   1.443   1.388  -0.161  -0.183  -1.420   \n",
       "740   0.487   0.175  -0.079   0.552   0.616   0.587   0.305   1.910   1.721   \n",
       "660   1.022   0.914   0.931   1.016  -0.173   0.321   0.988   0.651   0.909   \n",
       "411   0.888   0.674   0.931   0.983  -1.332   0.962  -2.306   0.913  -0.433   \n",
       "\n",
       "     wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "521   0.813  -0.594  -0.175   1.199   1.010   1.089  -0.602   1.104   1.144   \n",
       "737   0.403   0.170   0.445  -0.132   0.746   1.403   0.276   1.442   1.459   \n",
       "740   0.748  -0.140  -0.183   1.772  -0.110   0.609   0.085   0.654   0.660   \n",
       "660   1.167   1.077   0.969   0.952   1.034   0.212   0.970   0.070   0.305   \n",
       "411   1.063  -1.636   1.715  -1.639  -0.042  -0.029   0.720  -0.088   1.047   \n",
       "\n",
       "     wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "521   1.053   1.128   1.169   1.515   1.205  -0.417   1.428  -3.766  -4.637   \n",
       "737   1.434   1.518   0.437  -3.677   1.504   0.053  -0.134  -2.120  -0.275   \n",
       "740   0.589   0.651   0.689   1.667   0.674  -0.260   2.263  -3.412  -2.063   \n",
       "660   0.247  -0.136   0.553   0.795   1.072   0.923   0.994  -1.698  -1.619   \n",
       "411   0.275   1.981   0.336   0.850  -0.700  -2.494  -0.144  -2.206  -6.113   \n",
       "\n",
       "     wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "521  -3.807  -4.844   2.480   2.624   2.540  -3.782  -3.258  -3.485  -6.607   \n",
       "737  -9.993  -2.049   2.053   1.979  -1.584  -0.332  -6.686  -1.684  -1.702   \n",
       "740  -0.268  -3.346   2.054   2.004   1.922  -3.132  -2.967   1.928  -0.061   \n",
       "660  -2.089  -1.677   0.122   2.451  -1.074  -2.161  -1.707  -1.458  -1.464   \n",
       "411  -2.394  -2.323   4.209   2.640  -5.199  -2.126  -2.748  -1.995  10.226   \n",
       "\n",
       "     wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "521  -0.268  -7.397  -5.215   2.509  -4.932   2.793   2.286   2.684   2.405   \n",
       "737  -2.187  -0.312  -4.966   1.973  -1.821   2.499   1.782   1.990   1.927   \n",
       "740  -0.273  -3.313  -0.091   1.948  -1.982   2.195   1.685   2.099   1.860   \n",
       "660  -1.453  -1.736  -1.522   2.107  -1.574   1.861  -1.721   2.303   4.313   \n",
       "411  -5.121  -5.074  -2.566   2.862  -2.139   0.391   2.361   2.844   9.477   \n",
       "\n",
       "     wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "521   2.449  -3.720   2.424  -4.191  -3.282   0.769  \n",
       "737   5.399 -10.045   1.937  -1.716  -0.332   0.947  \n",
       "740   1.785  -2.678   1.760  -0.162  -4.025   0.480  \n",
       "660  -1.911  -2.015  -1.510  -1.224  -2.090  -0.712  \n",
       "411   2.663  -2.136  -2.704  -7.079  -0.316   0.653  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:31:56.898548Z",
     "start_time": "2021-01-05T09:30:49.715497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>1.078</td>\n",
       "      <td>1.097</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>1.204</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>1.313</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.924</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.648</td>\n",
       "      <td>-2.696</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>1.271</td>\n",
       "      <td>-3.091</td>\n",
       "      <td>6.159</td>\n",
       "      <td>-4.918</td>\n",
       "      <td>5.443</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-5.224</td>\n",
       "      <td>6.755</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-4.940</td>\n",
       "      <td>4.533</td>\n",
       "      <td>0.712</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-4.305</td>\n",
       "      <td>5.062</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-2.083</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>-4.380</td>\n",
       "      <td>1.135</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>1.107</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>1.315</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.978</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>0.593</td>\n",
       "      <td>-2.602</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>1.296</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.621</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.618</td>\n",
       "      <td>1.442</td>\n",
       "      <td>1.288</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.644</td>\n",
       "      <td>1.059</td>\n",
       "      <td>0.477</td>\n",
       "      <td>1.069</td>\n",
       "      <td>1.252</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.892</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.719</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.015</td>\n",
       "      <td>1.206</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>1.317</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.253</td>\n",
       "      <td>1.451</td>\n",
       "      <td>0.768</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>1.309</td>\n",
       "      <td>1.271</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.627</td>\n",
       "      <td>1.075</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.041</td>\n",
       "      <td>1.180</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.388</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.987</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.104</td>\n",
       "      <td>1.364</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.024</td>\n",
       "      <td>1.356</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.780</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.489</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.933</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.772</td>\n",
       "      <td>1.213</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1.047</td>\n",
       "      <td>1.060</td>\n",
       "      <td>0.968</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>1.162</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-2.130</td>\n",
       "      <td>-2.555</td>\n",
       "      <td>-2.019</td>\n",
       "      <td>-2.505</td>\n",
       "      <td>2.804</td>\n",
       "      <td>2.702</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-2.651</td>\n",
       "      <td>-2.501</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-1.976</td>\n",
       "      <td>-2.454</td>\n",
       "      <td>-2.431</td>\n",
       "      <td>-2.169</td>\n",
       "      <td>2.359</td>\n",
       "      <td>-2.391</td>\n",
       "      <td>2.467</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>2.842</td>\n",
       "      <td>2.150</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-2.708</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-2.162</td>\n",
       "      <td>-2.500</td>\n",
       "      <td>0.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>26.624</td>\n",
       "      <td>26.772</td>\n",
       "      <td>29.241</td>\n",
       "      <td>28.648</td>\n",
       "      <td>28.618</td>\n",
       "      <td>27.238</td>\n",
       "      <td>28.292</td>\n",
       "      <td>25.811</td>\n",
       "      <td>29.951</td>\n",
       "      <td>27.897</td>\n",
       "      <td>26.075</td>\n",
       "      <td>27.995</td>\n",
       "      <td>27.721</td>\n",
       "      <td>27.461</td>\n",
       "      <td>29.148</td>\n",
       "      <td>26.615</td>\n",
       "      <td>28.066</td>\n",
       "      <td>30.084</td>\n",
       "      <td>27.029</td>\n",
       "      <td>28.314</td>\n",
       "      <td>30.021</td>\n",
       "      <td>28.182</td>\n",
       "      <td>37.592</td>\n",
       "      <td>20.168</td>\n",
       "      <td>39.474</td>\n",
       "      <td>22.783</td>\n",
       "      <td>19.632</td>\n",
       "      <td>40.902</td>\n",
       "      <td>19.553</td>\n",
       "      <td>22.522</td>\n",
       "      <td>19.437</td>\n",
       "      <td>37.779</td>\n",
       "      <td>21.050</td>\n",
       "      <td>21.168</td>\n",
       "      <td>20.680</td>\n",
       "      <td>21.842</td>\n",
       "      <td>37.292</td>\n",
       "      <td>22.232</td>\n",
       "      <td>23.450</td>\n",
       "      <td>20.927</td>\n",
       "      <td>21.575</td>\n",
       "      <td>22.253</td>\n",
       "      <td>26.776</td>\n",
       "      <td>26.849</td>\n",
       "      <td>29.422</td>\n",
       "      <td>28.847</td>\n",
       "      <td>28.643</td>\n",
       "      <td>27.463</td>\n",
       "      <td>28.617</td>\n",
       "      <td>25.884</td>\n",
       "      <td>30.068</td>\n",
       "      <td>28.009</td>\n",
       "      <td>26.556</td>\n",
       "      <td>28.096</td>\n",
       "      <td>27.746</td>\n",
       "      <td>27.633</td>\n",
       "      <td>29.252</td>\n",
       "      <td>26.874</td>\n",
       "      <td>28.147</td>\n",
       "      <td>30.091</td>\n",
       "      <td>27.233</td>\n",
       "      <td>28.502</td>\n",
       "      <td>30.198</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.192</td>\n",
       "      <td>0.981</td>\n",
       "      <td>1.027</td>\n",
       "      <td>1.237</td>\n",
       "      <td>1.348</td>\n",
       "      <td>1.286</td>\n",
       "      <td>1.196</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.121</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.338</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.261</td>\n",
       "      <td>1.216</td>\n",
       "      <td>1.170</td>\n",
       "      <td>1.259</td>\n",
       "      <td>1.285</td>\n",
       "      <td>1.115</td>\n",
       "      <td>1.102</td>\n",
       "      <td>1.156</td>\n",
       "      <td>1.161</td>\n",
       "      <td>1.077</td>\n",
       "      <td>1.072</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.174</td>\n",
       "      <td>1.515</td>\n",
       "      <td>1.182</td>\n",
       "      <td>1.239</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.014</td>\n",
       "      <td>1.119</td>\n",
       "      <td>1.237</td>\n",
       "      <td>1.141</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.131</td>\n",
       "      <td>1.451</td>\n",
       "      <td>1.093</td>\n",
       "      <td>1.055</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.442</td>\n",
       "      <td>1.221</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.120</td>\n",
       "      <td>1.124</td>\n",
       "      <td>1.011</td>\n",
       "      <td>1.130</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.043</td>\n",
       "      <td>1.035</td>\n",
       "      <td>1.174</td>\n",
       "      <td>1.476</td>\n",
       "      <td>1.128</td>\n",
       "      <td>1.235</td>\n",
       "      <td>1.194</td>\n",
       "      <td>1.094</td>\n",
       "      <td>1.172</td>\n",
       "      <td>1.131</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.951</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.114</td>\n",
       "      <td>1.142</td>\n",
       "      <td>1.203</td>\n",
       "      <td>1.234</td>\n",
       "      <td>1.248</td>\n",
       "      <td>1.570</td>\n",
       "      <td>1.186</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.133</td>\n",
       "      <td>0.942</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.962</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.070</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.292</td>\n",
       "      <td>1.035</td>\n",
       "      <td>1.147</td>\n",
       "      <td>1.088</td>\n",
       "      <td>1.247</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.185</td>\n",
       "      <td>1.148</td>\n",
       "      <td>1.220</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.291</td>\n",
       "      <td>1.318</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1.679</td>\n",
       "      <td>1.420</td>\n",
       "      <td>1.164</td>\n",
       "      <td>1.228</td>\n",
       "      <td>0.976</td>\n",
       "      <td>1.156</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1.087</td>\n",
       "      <td>0.961</td>\n",
       "      <td>1.113</td>\n",
       "      <td>1.443</td>\n",
       "      <td>1.504</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.099</td>\n",
       "      <td>1.258</td>\n",
       "      <td>1.281</td>\n",
       "      <td>1.103</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1.108</td>\n",
       "      <td>1.242</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.151</td>\n",
       "      <td>1.261</td>\n",
       "      <td>1.478</td>\n",
       "      <td>1.496</td>\n",
       "      <td>1.283</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.143</td>\n",
       "      <td>1.180</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.082</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1.264</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.018</td>\n",
       "      <td>1.026</td>\n",
       "      <td>1.099</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.120</td>\n",
       "      <td>1.060</td>\n",
       "      <td>0.980</td>\n",
       "      <td>1.591</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.424</td>\n",
       "      <td>1.097</td>\n",
       "      <td>1.120</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.018</td>\n",
       "      <td>1.420</td>\n",
       "      <td>1.876</td>\n",
       "      <td>1.581</td>\n",
       "      <td>1.824</td>\n",
       "      <td>2.005</td>\n",
       "      <td>1.928</td>\n",
       "      <td>3.001</td>\n",
       "      <td>1.968</td>\n",
       "      <td>1.693</td>\n",
       "      <td>2.140</td>\n",
       "      <td>2.740</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.755</td>\n",
       "      <td>1.554</td>\n",
       "      <td>1.861</td>\n",
       "      <td>1.685</td>\n",
       "      <td>1.771</td>\n",
       "      <td>2.969</td>\n",
       "      <td>1.987</td>\n",
       "      <td>3.110</td>\n",
       "      <td>3.032</td>\n",
       "      <td>1.864</td>\n",
       "      <td>2.345</td>\n",
       "      <td>1.872</td>\n",
       "      <td>1.831</td>\n",
       "      <td>0.715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-99.955</td>\n",
       "      <td>-99.450</td>\n",
       "      <td>-99.716</td>\n",
       "      <td>-98.507</td>\n",
       "      <td>-99.857</td>\n",
       "      <td>-98.289</td>\n",
       "      <td>-99.256</td>\n",
       "      <td>-97.010</td>\n",
       "      <td>-98.528</td>\n",
       "      <td>-98.896</td>\n",
       "      <td>-98.870</td>\n",
       "      <td>-99.125</td>\n",
       "      <td>-98.199</td>\n",
       "      <td>-99.324</td>\n",
       "      <td>-99.869</td>\n",
       "      <td>-96.368</td>\n",
       "      <td>-99.214</td>\n",
       "      <td>-99.800</td>\n",
       "      <td>-98.054</td>\n",
       "      <td>-99.496</td>\n",
       "      <td>-99.934</td>\n",
       "      <td>-94.271</td>\n",
       "      <td>-151.177</td>\n",
       "      <td>-97.348</td>\n",
       "      <td>-204.934</td>\n",
       "      <td>-98.605</td>\n",
       "      <td>-95.208</td>\n",
       "      <td>-184.564</td>\n",
       "      <td>-87.912</td>\n",
       "      <td>-92.219</td>\n",
       "      <td>-97.219</td>\n",
       "      <td>-156.203</td>\n",
       "      <td>-93.886</td>\n",
       "      <td>-98.495</td>\n",
       "      <td>-90.964</td>\n",
       "      <td>-97.014</td>\n",
       "      <td>-124.499</td>\n",
       "      <td>-96.614</td>\n",
       "      <td>-95.181</td>\n",
       "      <td>-93.794</td>\n",
       "      <td>-98.894</td>\n",
       "      <td>-98.797</td>\n",
       "      <td>-106.540</td>\n",
       "      <td>-100.124</td>\n",
       "      <td>-101.633</td>\n",
       "      <td>-102.212</td>\n",
       "      <td>-101.823</td>\n",
       "      <td>-101.385</td>\n",
       "      <td>-105.766</td>\n",
       "      <td>-96.050</td>\n",
       "      <td>-102.167</td>\n",
       "      <td>-99.895</td>\n",
       "      <td>-102.575</td>\n",
       "      <td>-100.240</td>\n",
       "      <td>-99.638</td>\n",
       "      <td>-103.015</td>\n",
       "      <td>-99.358</td>\n",
       "      <td>-97.002</td>\n",
       "      <td>-105.184</td>\n",
       "      <td>-100.593</td>\n",
       "      <td>-101.177</td>\n",
       "      <td>-103.998</td>\n",
       "      <td>-104.898</td>\n",
       "      <td>-3.405</td>\n",
       "      <td>-6.436</td>\n",
       "      <td>-1.379</td>\n",
       "      <td>-3.092</td>\n",
       "      <td>-4.155</td>\n",
       "      <td>-4.647</td>\n",
       "      <td>-6.625</td>\n",
       "      <td>-7.320</td>\n",
       "      <td>-3.563</td>\n",
       "      <td>-2.030</td>\n",
       "      <td>-1.637</td>\n",
       "      <td>-3.725</td>\n",
       "      <td>-5.336</td>\n",
       "      <td>-1.629</td>\n",
       "      <td>-5.203</td>\n",
       "      <td>-1.729</td>\n",
       "      <td>-4.493</td>\n",
       "      <td>-4.164</td>\n",
       "      <td>-3.068</td>\n",
       "      <td>-3.213</td>\n",
       "      <td>-4.036</td>\n",
       "      <td>-3.530</td>\n",
       "      <td>-2.849</td>\n",
       "      <td>-5.960</td>\n",
       "      <td>-6.897</td>\n",
       "      <td>-4.485</td>\n",
       "      <td>-5.575</td>\n",
       "      <td>-3.930</td>\n",
       "      <td>-5.490</td>\n",
       "      <td>-8.583</td>\n",
       "      <td>-4.007</td>\n",
       "      <td>-3.859</td>\n",
       "      <td>-1.931</td>\n",
       "      <td>-2.148</td>\n",
       "      <td>-2.764</td>\n",
       "      <td>-7.965</td>\n",
       "      <td>-4.550</td>\n",
       "      <td>-7.498</td>\n",
       "      <td>-5.886</td>\n",
       "      <td>-8.130</td>\n",
       "      <td>-6.421</td>\n",
       "      <td>-1.750</td>\n",
       "      <td>-3.101</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-3.949</td>\n",
       "      <td>-3.762</td>\n",
       "      <td>-5.368</td>\n",
       "      <td>-2.476</td>\n",
       "      <td>-2.360</td>\n",
       "      <td>-6.158</td>\n",
       "      <td>-3.346</td>\n",
       "      <td>-5.340</td>\n",
       "      <td>-4.449</td>\n",
       "      <td>-7.101</td>\n",
       "      <td>-6.808</td>\n",
       "      <td>-1.872</td>\n",
       "      <td>-4.390</td>\n",
       "      <td>-5.003</td>\n",
       "      <td>-4.917</td>\n",
       "      <td>-4.357</td>\n",
       "      <td>-3.794</td>\n",
       "      <td>-1.945</td>\n",
       "      <td>-1.569</td>\n",
       "      <td>-3.056</td>\n",
       "      <td>-2.515</td>\n",
       "      <td>-6.052</td>\n",
       "      <td>-6.921</td>\n",
       "      <td>-4.084</td>\n",
       "      <td>-4.583</td>\n",
       "      <td>-8.554</td>\n",
       "      <td>-2.629</td>\n",
       "      <td>-6.673</td>\n",
       "      <td>-2.633</td>\n",
       "      <td>-3.228</td>\n",
       "      <td>-4.565</td>\n",
       "      <td>-1.479</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>-3.721</td>\n",
       "      <td>-1.449</td>\n",
       "      <td>-1.915</td>\n",
       "      <td>-7.355</td>\n",
       "      <td>-5.022</td>\n",
       "      <td>-2.286</td>\n",
       "      <td>-5.009</td>\n",
       "      <td>-1.918</td>\n",
       "      <td>-5.549</td>\n",
       "      <td>-3.046</td>\n",
       "      <td>-6.919</td>\n",
       "      <td>-4.695</td>\n",
       "      <td>-3.857</td>\n",
       "      <td>-3.418</td>\n",
       "      <td>-8.077</td>\n",
       "      <td>-5.461</td>\n",
       "      <td>-7.963</td>\n",
       "      <td>-8.206</td>\n",
       "      <td>-6.277</td>\n",
       "      <td>-5.949</td>\n",
       "      <td>-3.550</td>\n",
       "      <td>-1.957</td>\n",
       "      <td>-6.290</td>\n",
       "      <td>-4.062</td>\n",
       "      <td>-3.572</td>\n",
       "      <td>-3.891</td>\n",
       "      <td>-7.725</td>\n",
       "      <td>-4.420</td>\n",
       "      <td>-7.451</td>\n",
       "      <td>-7.571</td>\n",
       "      <td>-2.268</td>\n",
       "      <td>-6.450</td>\n",
       "      <td>-4.620</td>\n",
       "      <td>-7.436</td>\n",
       "      <td>-5.569</td>\n",
       "      <td>-3.224</td>\n",
       "      <td>-5.417</td>\n",
       "      <td>-2.996</td>\n",
       "      <td>-7.162</td>\n",
       "      <td>-3.572</td>\n",
       "      <td>-2.346</td>\n",
       "      <td>-9.348</td>\n",
       "      <td>-6.313</td>\n",
       "      <td>-3.966</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>-6.250</td>\n",
       "      <td>-5.337</td>\n",
       "      <td>-1.383</td>\n",
       "      <td>-3.171</td>\n",
       "      <td>-3.709</td>\n",
       "      <td>-3.346</td>\n",
       "      <td>-4.012</td>\n",
       "      <td>-3.159</td>\n",
       "      <td>-4.253</td>\n",
       "      <td>-4.844</td>\n",
       "      <td>-4.202</td>\n",
       "      <td>-3.384</td>\n",
       "      <td>-2.776</td>\n",
       "      <td>-3.736</td>\n",
       "      <td>-3.073</td>\n",
       "      <td>-4.053</td>\n",
       "      <td>-3.140</td>\n",
       "      <td>-3.964</td>\n",
       "      <td>-3.754</td>\n",
       "      <td>-3.182</td>\n",
       "      <td>-4.242</td>\n",
       "      <td>-4.487</td>\n",
       "      <td>-3.843</td>\n",
       "      <td>-3.741</td>\n",
       "      <td>-4.577</td>\n",
       "      <td>-4.323</td>\n",
       "      <td>-4.557</td>\n",
       "      <td>-2.721</td>\n",
       "      <td>-8.957</td>\n",
       "      <td>-10.828</td>\n",
       "      <td>-9.993</td>\n",
       "      <td>-10.828</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-9.161</td>\n",
       "      <td>-13.577</td>\n",
       "      <td>-9.573</td>\n",
       "      <td>-7.669</td>\n",
       "      <td>-9.436</td>\n",
       "      <td>-9.704</td>\n",
       "      <td>-13.517</td>\n",
       "      <td>-10.176</td>\n",
       "      <td>-6.458</td>\n",
       "      <td>-10.563</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-10.677</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-13.015</td>\n",
       "      <td>-12.220</td>\n",
       "      <td>-11.116</td>\n",
       "      <td>-9.134</td>\n",
       "      <td>-10.318</td>\n",
       "      <td>-10.011</td>\n",
       "      <td>-1.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-14.851</td>\n",
       "      <td>-6.216</td>\n",
       "      <td>-6.735</td>\n",
       "      <td>-8.891</td>\n",
       "      <td>-1.659</td>\n",
       "      <td>-6.920</td>\n",
       "      <td>-7.275</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>-1.488</td>\n",
       "      <td>-6.347</td>\n",
       "      <td>-7.586</td>\n",
       "      <td>-1.658</td>\n",
       "      <td>-2.110</td>\n",
       "      <td>-1.622</td>\n",
       "      <td>-6.835</td>\n",
       "      <td>-8.824</td>\n",
       "      <td>-1.568</td>\n",
       "      <td>-1.467</td>\n",
       "      <td>-1.752</td>\n",
       "      <td>-1.942</td>\n",
       "      <td>-5.995</td>\n",
       "      <td>-1.418</td>\n",
       "      <td>-2.159</td>\n",
       "      <td>-1.891</td>\n",
       "      <td>-2.379</td>\n",
       "      <td>-1.728</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-2.386</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-1.636</td>\n",
       "      <td>-1.777</td>\n",
       "      <td>-2.467</td>\n",
       "      <td>-1.803</td>\n",
       "      <td>-1.931</td>\n",
       "      <td>-1.679</td>\n",
       "      <td>-1.675</td>\n",
       "      <td>-2.617</td>\n",
       "      <td>-1.672</td>\n",
       "      <td>-1.859</td>\n",
       "      <td>-1.729</td>\n",
       "      <td>-1.841</td>\n",
       "      <td>-1.797</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.656</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-1.026</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.756</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.585</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.788</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.387</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.752</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-2.789</td>\n",
       "      <td>-3.259</td>\n",
       "      <td>-2.886</td>\n",
       "      <td>-3.126</td>\n",
       "      <td>1.827</td>\n",
       "      <td>1.757</td>\n",
       "      <td>-1.683</td>\n",
       "      <td>-3.465</td>\n",
       "      <td>-3.054</td>\n",
       "      <td>-1.799</td>\n",
       "      <td>-3.005</td>\n",
       "      <td>-3.137</td>\n",
       "      <td>-3.154</td>\n",
       "      <td>-2.790</td>\n",
       "      <td>1.570</td>\n",
       "      <td>-2.916</td>\n",
       "      <td>1.673</td>\n",
       "      <td>-1.934</td>\n",
       "      <td>1.881</td>\n",
       "      <td>1.685</td>\n",
       "      <td>-1.925</td>\n",
       "      <td>-3.532</td>\n",
       "      <td>-1.769</td>\n",
       "      <td>-2.628</td>\n",
       "      <td>-3.308</td>\n",
       "      <td>-0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.323</td>\n",
       "      <td>0.978</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>1.354</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.923</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.453</td>\n",
       "      <td>1.421</td>\n",
       "      <td>1.062</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.337</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.128</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.861</td>\n",
       "      <td>1.049</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.789</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.866</td>\n",
       "      <td>1.237</td>\n",
       "      <td>0.157</td>\n",
       "      <td>1.232</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.159</td>\n",
       "      <td>1.443</td>\n",
       "      <td>0.548</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.127</td>\n",
       "      <td>1.196</td>\n",
       "      <td>1.160</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.979</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.235</td>\n",
       "      <td>1.345</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.878</td>\n",
       "      <td>1.258</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1.321</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.608</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.883</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.689</td>\n",
       "      <td>1.207</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.596</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.957</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-2.066</td>\n",
       "      <td>-2.037</td>\n",
       "      <td>-2.005</td>\n",
       "      <td>-2.190</td>\n",
       "      <td>2.244</td>\n",
       "      <td>2.145</td>\n",
       "      <td>-1.181</td>\n",
       "      <td>-2.221</td>\n",
       "      <td>-2.084</td>\n",
       "      <td>-1.146</td>\n",
       "      <td>-2.042</td>\n",
       "      <td>-2.161</td>\n",
       "      <td>-2.139</td>\n",
       "      <td>-1.947</td>\n",
       "      <td>2.014</td>\n",
       "      <td>-2.035</td>\n",
       "      <td>2.361</td>\n",
       "      <td>1.338</td>\n",
       "      <td>2.305</td>\n",
       "      <td>2.082</td>\n",
       "      <td>1.584</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>-1.713</td>\n",
       "      <td>-2.262</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.140</td>\n",
       "      <td>17.419</td>\n",
       "      <td>0.360</td>\n",
       "      <td>17.854</td>\n",
       "      <td>2.498</td>\n",
       "      <td>0.396</td>\n",
       "      <td>16.863</td>\n",
       "      <td>2.179</td>\n",
       "      <td>2.356</td>\n",
       "      <td>0.373</td>\n",
       "      <td>14.756</td>\n",
       "      <td>2.370</td>\n",
       "      <td>2.135</td>\n",
       "      <td>2.216</td>\n",
       "      <td>0.368</td>\n",
       "      <td>15.572</td>\n",
       "      <td>2.264</td>\n",
       "      <td>2.251</td>\n",
       "      <td>1.836</td>\n",
       "      <td>2.108</td>\n",
       "      <td>0.474</td>\n",
       "      <td>1.462</td>\n",
       "      <td>2.512</td>\n",
       "      <td>1.990</td>\n",
       "      <td>2.721</td>\n",
       "      <td>1.805</td>\n",
       "      <td>2.012</td>\n",
       "      <td>2.911</td>\n",
       "      <td>1.761</td>\n",
       "      <td>1.961</td>\n",
       "      <td>2.031</td>\n",
       "      <td>2.634</td>\n",
       "      <td>1.801</td>\n",
       "      <td>1.625</td>\n",
       "      <td>1.591</td>\n",
       "      <td>2.129</td>\n",
       "      <td>2.423</td>\n",
       "      <td>1.515</td>\n",
       "      <td>1.660</td>\n",
       "      <td>1.866</td>\n",
       "      <td>1.456</td>\n",
       "      <td>2.160</td>\n",
       "      <td>1.031</td>\n",
       "      <td>0.449</td>\n",
       "      <td>1.523</td>\n",
       "      <td>1.339</td>\n",
       "      <td>1.618</td>\n",
       "      <td>1.494</td>\n",
       "      <td>1.811</td>\n",
       "      <td>0.519</td>\n",
       "      <td>1.229</td>\n",
       "      <td>2.108</td>\n",
       "      <td>1.920</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.923</td>\n",
       "      <td>1.560</td>\n",
       "      <td>1.467</td>\n",
       "      <td>1.696</td>\n",
       "      <td>1.273</td>\n",
       "      <td>1.912</td>\n",
       "      <td>1.968</td>\n",
       "      <td>1.964</td>\n",
       "      <td>1.770</td>\n",
       "      <td>1.113</td>\n",
       "      <td>1.663</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.907</td>\n",
       "      <td>1.036</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.922</td>\n",
       "      <td>1.684</td>\n",
       "      <td>1.969</td>\n",
       "      <td>1.399</td>\n",
       "      <td>1.693</td>\n",
       "      <td>1.557</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.865</td>\n",
       "      <td>1.265</td>\n",
       "      <td>1.705</td>\n",
       "      <td>2.029</td>\n",
       "      <td>0.872</td>\n",
       "      <td>2.118</td>\n",
       "      <td>1.859</td>\n",
       "      <td>0.983</td>\n",
       "      <td>2.222</td>\n",
       "      <td>1.411</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.842</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.877</td>\n",
       "      <td>2.057</td>\n",
       "      <td>2.061</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.703</td>\n",
       "      <td>1.273</td>\n",
       "      <td>1.377</td>\n",
       "      <td>1.551</td>\n",
       "      <td>1.516</td>\n",
       "      <td>1.290</td>\n",
       "      <td>1.742</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.739</td>\n",
       "      <td>2.043</td>\n",
       "      <td>1.445</td>\n",
       "      <td>0.945</td>\n",
       "      <td>2.213</td>\n",
       "      <td>0.182</td>\n",
       "      <td>1.599</td>\n",
       "      <td>1.376</td>\n",
       "      <td>1.074</td>\n",
       "      <td>1.418</td>\n",
       "      <td>1.611</td>\n",
       "      <td>0.339</td>\n",
       "      <td>1.749</td>\n",
       "      <td>2.221</td>\n",
       "      <td>1.254</td>\n",
       "      <td>1.794</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.723</td>\n",
       "      <td>2.117</td>\n",
       "      <td>0.719</td>\n",
       "      <td>1.387</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.641</td>\n",
       "      <td>1.891</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.823</td>\n",
       "      <td>1.528</td>\n",
       "      <td>1.536</td>\n",
       "      <td>1.276</td>\n",
       "      <td>1.244</td>\n",
       "      <td>1.105</td>\n",
       "      <td>1.110</td>\n",
       "      <td>1.392</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.919</td>\n",
       "      <td>1.069</td>\n",
       "      <td>0.233</td>\n",
       "      <td>1.664</td>\n",
       "      <td>1.116</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1.459</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.293</td>\n",
       "      <td>1.165</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.808</td>\n",
       "      <td>0.350</td>\n",
       "      <td>1.527</td>\n",
       "      <td>2.060</td>\n",
       "      <td>1.394</td>\n",
       "      <td>1.453</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.683</td>\n",
       "      <td>1.796</td>\n",
       "      <td>0.449</td>\n",
       "      <td>1.758</td>\n",
       "      <td>1.163</td>\n",
       "      <td>0.598</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.067</td>\n",
       "      <td>1.335</td>\n",
       "      <td>1.194</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.880</td>\n",
       "      <td>1.369</td>\n",
       "      <td>1.225</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.187</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.232</td>\n",
       "      <td>1.159</td>\n",
       "      <td>1.320</td>\n",
       "      <td>1.299</td>\n",
       "      <td>1.240</td>\n",
       "      <td>0.882</td>\n",
       "      <td>1.205</td>\n",
       "      <td>0.449</td>\n",
       "      <td>1.139</td>\n",
       "      <td>-1.458</td>\n",
       "      <td>-1.547</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-1.586</td>\n",
       "      <td>3.099</td>\n",
       "      <td>2.834</td>\n",
       "      <td>1.986</td>\n",
       "      <td>-1.555</td>\n",
       "      <td>-1.567</td>\n",
       "      <td>1.927</td>\n",
       "      <td>-1.444</td>\n",
       "      <td>-1.555</td>\n",
       "      <td>-1.547</td>\n",
       "      <td>-1.413</td>\n",
       "      <td>2.557</td>\n",
       "      <td>-1.490</td>\n",
       "      <td>2.899</td>\n",
       "      <td>1.837</td>\n",
       "      <td>3.074</td>\n",
       "      <td>2.903</td>\n",
       "      <td>2.043</td>\n",
       "      <td>-1.610</td>\n",
       "      <td>1.889</td>\n",
       "      <td>-1.248</td>\n",
       "      <td>-1.445</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>98.901</td>\n",
       "      <td>99.248</td>\n",
       "      <td>99.640</td>\n",
       "      <td>99.430</td>\n",
       "      <td>98.990</td>\n",
       "      <td>97.283</td>\n",
       "      <td>99.944</td>\n",
       "      <td>99.327</td>\n",
       "      <td>99.935</td>\n",
       "      <td>99.213</td>\n",
       "      <td>99.305</td>\n",
       "      <td>99.415</td>\n",
       "      <td>98.835</td>\n",
       "      <td>99.870</td>\n",
       "      <td>99.765</td>\n",
       "      <td>99.985</td>\n",
       "      <td>99.433</td>\n",
       "      <td>99.841</td>\n",
       "      <td>98.545</td>\n",
       "      <td>97.649</td>\n",
       "      <td>98.828</td>\n",
       "      <td>109.913</td>\n",
       "      <td>154.254</td>\n",
       "      <td>84.443</td>\n",
       "      <td>169.775</td>\n",
       "      <td>89.966</td>\n",
       "      <td>79.298</td>\n",
       "      <td>159.347</td>\n",
       "      <td>87.314</td>\n",
       "      <td>89.973</td>\n",
       "      <td>89.481</td>\n",
       "      <td>160.955</td>\n",
       "      <td>92.449</td>\n",
       "      <td>87.883</td>\n",
       "      <td>99.006</td>\n",
       "      <td>94.999</td>\n",
       "      <td>192.300</td>\n",
       "      <td>101.354</td>\n",
       "      <td>96.646</td>\n",
       "      <td>93.683</td>\n",
       "      <td>88.345</td>\n",
       "      <td>87.300</td>\n",
       "      <td>102.147</td>\n",
       "      <td>106.071</td>\n",
       "      <td>100.577</td>\n",
       "      <td>105.828</td>\n",
       "      <td>98.007</td>\n",
       "      <td>99.431</td>\n",
       "      <td>103.585</td>\n",
       "      <td>98.660</td>\n",
       "      <td>104.740</td>\n",
       "      <td>103.712</td>\n",
       "      <td>102.117</td>\n",
       "      <td>100.060</td>\n",
       "      <td>99.872</td>\n",
       "      <td>101.780</td>\n",
       "      <td>102.656</td>\n",
       "      <td>105.898</td>\n",
       "      <td>100.688</td>\n",
       "      <td>99.383</td>\n",
       "      <td>102.175</td>\n",
       "      <td>99.346</td>\n",
       "      <td>101.520</td>\n",
       "      <td>3.341</td>\n",
       "      <td>3.064</td>\n",
       "      <td>5.205</td>\n",
       "      <td>4.179</td>\n",
       "      <td>5.850</td>\n",
       "      <td>5.507</td>\n",
       "      <td>6.973</td>\n",
       "      <td>2.890</td>\n",
       "      <td>3.783</td>\n",
       "      <td>6.306</td>\n",
       "      <td>6.171</td>\n",
       "      <td>3.193</td>\n",
       "      <td>3.503</td>\n",
       "      <td>5.331</td>\n",
       "      <td>5.871</td>\n",
       "      <td>4.778</td>\n",
       "      <td>5.664</td>\n",
       "      <td>5.090</td>\n",
       "      <td>5.564</td>\n",
       "      <td>6.354</td>\n",
       "      <td>5.278</td>\n",
       "      <td>3.879</td>\n",
       "      <td>4.699</td>\n",
       "      <td>4.648</td>\n",
       "      <td>2.869</td>\n",
       "      <td>5.005</td>\n",
       "      <td>4.161</td>\n",
       "      <td>3.030</td>\n",
       "      <td>3.665</td>\n",
       "      <td>3.574</td>\n",
       "      <td>5.410</td>\n",
       "      <td>8.284</td>\n",
       "      <td>5.335</td>\n",
       "      <td>4.669</td>\n",
       "      <td>3.462</td>\n",
       "      <td>3.946</td>\n",
       "      <td>2.920</td>\n",
       "      <td>3.916</td>\n",
       "      <td>3.053</td>\n",
       "      <td>3.205</td>\n",
       "      <td>4.436</td>\n",
       "      <td>5.378</td>\n",
       "      <td>6.016</td>\n",
       "      <td>2.996</td>\n",
       "      <td>5.483</td>\n",
       "      <td>6.266</td>\n",
       "      <td>4.042</td>\n",
       "      <td>6.693</td>\n",
       "      <td>5.318</td>\n",
       "      <td>3.952</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.709</td>\n",
       "      <td>4.659</td>\n",
       "      <td>3.123</td>\n",
       "      <td>3.949</td>\n",
       "      <td>6.202</td>\n",
       "      <td>7.639</td>\n",
       "      <td>2.679</td>\n",
       "      <td>3.129</td>\n",
       "      <td>3.701</td>\n",
       "      <td>6.378</td>\n",
       "      <td>4.461</td>\n",
       "      <td>5.518</td>\n",
       "      <td>4.222</td>\n",
       "      <td>5.163</td>\n",
       "      <td>3.108</td>\n",
       "      <td>3.468</td>\n",
       "      <td>6.322</td>\n",
       "      <td>4.725</td>\n",
       "      <td>3.550</td>\n",
       "      <td>5.466</td>\n",
       "      <td>2.669</td>\n",
       "      <td>6.340</td>\n",
       "      <td>4.719</td>\n",
       "      <td>5.215</td>\n",
       "      <td>4.678</td>\n",
       "      <td>4.829</td>\n",
       "      <td>2.768</td>\n",
       "      <td>5.804</td>\n",
       "      <td>7.567</td>\n",
       "      <td>6.143</td>\n",
       "      <td>7.275</td>\n",
       "      <td>6.814</td>\n",
       "      <td>2.994</td>\n",
       "      <td>5.632</td>\n",
       "      <td>3.545</td>\n",
       "      <td>4.848</td>\n",
       "      <td>2.578</td>\n",
       "      <td>3.295</td>\n",
       "      <td>5.250</td>\n",
       "      <td>4.029</td>\n",
       "      <td>3.370</td>\n",
       "      <td>4.595</td>\n",
       "      <td>6.425</td>\n",
       "      <td>4.420</td>\n",
       "      <td>4.540</td>\n",
       "      <td>4.535</td>\n",
       "      <td>3.582</td>\n",
       "      <td>4.912</td>\n",
       "      <td>3.183</td>\n",
       "      <td>2.493</td>\n",
       "      <td>3.935</td>\n",
       "      <td>3.543</td>\n",
       "      <td>2.734</td>\n",
       "      <td>6.128</td>\n",
       "      <td>3.345</td>\n",
       "      <td>4.432</td>\n",
       "      <td>5.019</td>\n",
       "      <td>3.382</td>\n",
       "      <td>3.217</td>\n",
       "      <td>2.428</td>\n",
       "      <td>2.463</td>\n",
       "      <td>3.867</td>\n",
       "      <td>3.001</td>\n",
       "      <td>5.230</td>\n",
       "      <td>2.814</td>\n",
       "      <td>6.152</td>\n",
       "      <td>6.133</td>\n",
       "      <td>4.946</td>\n",
       "      <td>6.262</td>\n",
       "      <td>5.665</td>\n",
       "      <td>5.370</td>\n",
       "      <td>4.860</td>\n",
       "      <td>4.601</td>\n",
       "      <td>5.198</td>\n",
       "      <td>2.954</td>\n",
       "      <td>3.279</td>\n",
       "      <td>2.955</td>\n",
       "      <td>2.700</td>\n",
       "      <td>3.661</td>\n",
       "      <td>3.563</td>\n",
       "      <td>3.880</td>\n",
       "      <td>3.001</td>\n",
       "      <td>3.327</td>\n",
       "      <td>3.881</td>\n",
       "      <td>3.486</td>\n",
       "      <td>2.651</td>\n",
       "      <td>2.949</td>\n",
       "      <td>2.774</td>\n",
       "      <td>3.669</td>\n",
       "      <td>2.679</td>\n",
       "      <td>3.547</td>\n",
       "      <td>3.693</td>\n",
       "      <td>3.610</td>\n",
       "      <td>4.390</td>\n",
       "      <td>3.763</td>\n",
       "      <td>3.107</td>\n",
       "      <td>3.847</td>\n",
       "      <td>2.576</td>\n",
       "      <td>3.271</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>9.294</td>\n",
       "      <td>12.339</td>\n",
       "      <td>13.444</td>\n",
       "      <td>12.424</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>8.962</td>\n",
       "      <td>14.566</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>5.188</td>\n",
       "      <td>10.721</td>\n",
       "      <td>1.225</td>\n",
       "      <td>11.894</td>\n",
       "      <td>7.740</td>\n",
       "      <td>12.043</td>\n",
       "      <td>12.658</td>\n",
       "      <td>7.209</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>12.969</td>\n",
       "      <td>10.316</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>2.634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "count       1000.000      1000.000      1000.000      1000.000      1000.000   \n",
       "mean  1373158606.000         1.078         1.097         0.452         0.637   \n",
       "std            0.000        26.624        26.772        29.241        28.648   \n",
       "min   1373158606.000       -99.955       -99.450       -99.716       -98.507   \n",
       "25%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "50%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "75%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "max   1373158606.000        98.901        99.248        99.640        99.430   \n",
       "\n",
       "       00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "count      1000.000      1000.000      1000.000      1000.000      1000.000   \n",
       "mean          0.161        -0.184         1.204        -0.595         1.313   \n",
       "std          28.618        27.238        28.292        25.811        29.951   \n",
       "min         -99.857       -98.289       -99.256       -97.010       -98.528   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max          98.990        97.283        99.944        99.327        99.935   \n",
       "\n",
       "       00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "count      1000.000      1000.000      1000.000      1000.000      1000.000   \n",
       "mean          0.891         0.362         0.924        -0.107         0.396   \n",
       "std          27.897        26.075        27.995        27.721        27.461   \n",
       "min         -98.896       -98.870       -99.125       -98.199       -99.324   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max          99.213        99.305        99.415        98.835        99.870   \n",
       "\n",
       "       02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "count      1000.000      1000.000      1000.000      1000.000      1000.000   \n",
       "mean         -0.301        -0.186         0.648        -2.696        -0.966   \n",
       "std          29.148        26.615        28.066        30.084        27.029   \n",
       "min         -99.869       -96.368       -99.214       -99.800       -98.054   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max          99.765        99.985        99.433        99.841        98.545   \n",
       "\n",
       "       11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "count      1000.000      1000.000            1000.000            1000.000   \n",
       "mean         -0.763         1.271              -3.091               6.159   \n",
       "std          28.314        30.021              28.182              37.592   \n",
       "min         -99.496       -99.934             -94.271            -151.177   \n",
       "25%           0.000         0.000             -14.851              -6.216   \n",
       "50%           0.000         0.000              -2.323               0.978   \n",
       "75%           0.000         0.000               6.140              17.419   \n",
       "max          97.649        98.828             109.913             154.254   \n",
       "\n",
       "       00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -4.918               5.443               0.358   \n",
       "std                20.168              39.474              22.783   \n",
       "min               -97.348            -204.934             -98.605   \n",
       "25%                -6.735              -8.891              -1.659   \n",
       "50%                -0.962               0.115               0.343   \n",
       "75%                 0.360              17.854               2.498   \n",
       "max                84.443             169.775              89.966   \n",
       "\n",
       "       00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -5.224               6.755               0.163   \n",
       "std                19.632              40.902              19.553   \n",
       "min               -95.208            -184.564             -87.912   \n",
       "25%                -6.920              -7.275              -1.370   \n",
       "50%                -0.854               0.732               0.296   \n",
       "75%                 0.396              16.863               2.179   \n",
       "max                79.298             159.347              87.314   \n",
       "\n",
       "       00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                0.859              -4.940               4.533   \n",
       "std                22.522              19.437              37.779   \n",
       "min               -92.219             -97.219            -156.203   \n",
       "25%                -1.488              -6.347              -7.586   \n",
       "50%                 0.112              -0.837               1.354   \n",
       "75%                 2.356               0.373              14.756   \n",
       "max                89.973              89.481             160.955   \n",
       "\n",
       "       01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                0.712              -0.052               0.255   \n",
       "std                21.050              21.168              20.680   \n",
       "min               -93.886             -98.495             -90.964   \n",
       "25%                -1.658              -2.110              -1.622   \n",
       "50%                 0.070               0.083               0.055   \n",
       "75%                 2.370               2.135               2.216   \n",
       "max                92.449              87.883              99.006   \n",
       "\n",
       "       02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -4.305               5.062               0.578   \n",
       "std                21.842              37.292              22.232   \n",
       "min               -97.014            -124.499             -96.614   \n",
       "25%                -6.835              -8.824              -1.568   \n",
       "50%                -0.688               0.793               0.173   \n",
       "75%                 0.368              15.572               2.264   \n",
       "max                94.999             192.300             101.354   \n",
       "\n",
       "       10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -2.083              -0.611              -0.810   \n",
       "std                23.450              20.927              21.575   \n",
       "min               -95.181             -93.794             -98.894   \n",
       "25%                -1.467              -1.752              -1.942   \n",
       "50%                 0.166               0.004               0.063   \n",
       "75%                 2.251               1.836               2.108   \n",
       "max                96.646              93.683              88.345   \n",
       "\n",
       "       20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -4.380               1.135               0.951   \n",
       "std                22.253              26.776              26.849   \n",
       "min               -98.797            -106.540            -100.124   \n",
       "25%                -5.995              -1.418              -2.159   \n",
       "50%                -0.953               0.055               0.032   \n",
       "75%                 0.474               1.462               2.512   \n",
       "max                87.300             102.147             106.071   \n",
       "\n",
       "       00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                0.586               0.736               0.038   \n",
       "std                29.422              28.847              28.643   \n",
       "min              -101.633            -102.212            -101.823   \n",
       "25%                -1.891              -2.379              -1.728   \n",
       "50%                 0.092              -0.034              -0.113   \n",
       "75%                 1.990               2.721               1.805   \n",
       "max               100.577             105.828              98.007   \n",
       "\n",
       "       00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -0.249               1.107              -0.431   \n",
       "std                27.463              28.617              25.884   \n",
       "min              -101.385            -105.766             -96.050   \n",
       "25%                -1.801              -2.386              -1.798   \n",
       "50%                 0.074               0.031              -0.032   \n",
       "75%                 2.012               2.911               1.761   \n",
       "max                99.431             103.585              98.660   \n",
       "\n",
       "       00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                1.315               0.889               0.197   \n",
       "std                30.068              28.009              26.556   \n",
       "min              -102.167             -99.895            -102.575   \n",
       "25%                -1.636              -1.777              -2.467   \n",
       "50%                 0.104               0.195               0.018   \n",
       "75%                 1.961               2.031               2.634   \n",
       "max               104.740             103.712             102.117   \n",
       "\n",
       "       01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                0.978              -0.130               0.365   \n",
       "std                28.096              27.746              27.633   \n",
       "min              -100.240             -99.638            -103.015   \n",
       "25%                -1.803              -1.931              -1.679   \n",
       "50%                 0.125              -0.086              -0.032   \n",
       "75%                 1.801               1.625               1.591   \n",
       "max               100.060              99.872             101.780   \n",
       "\n",
       "       02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -0.114              -0.228               0.593   \n",
       "std                29.252              26.874              28.147   \n",
       "min               -99.358             -97.002            -105.184   \n",
       "25%                -1.675              -2.617              -1.672   \n",
       "50%                 0.160               0.076              -0.011   \n",
       "75%                 2.129               2.423               1.515   \n",
       "max               102.656             105.898             100.688   \n",
       "\n",
       "       10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -2.602              -0.903              -0.836   \n",
       "std                30.091              27.233              28.502   \n",
       "min              -100.593            -101.177            -103.998   \n",
       "25%                -1.859              -1.729              -1.841   \n",
       "50%                 0.073               0.014              -0.142   \n",
       "75%                 1.660               1.866               1.456   \n",
       "max                99.383             102.175              99.346   \n",
       "\n",
       "       20000-lstsq_target     wb_0     wb_1     wb_2     wb_3     wb_4  \\\n",
       "count            1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean                1.296    0.395   -0.165    0.966    0.698    0.878   \n",
       "std                30.198    1.003    1.192    0.981    1.027    1.237   \n",
       "min              -104.898   -3.405   -6.436   -1.379   -3.092   -4.155   \n",
       "25%                -1.797   -0.080   -0.568    0.199    0.022    0.054   \n",
       "50%                 0.151    0.232   -0.161    0.655    0.520    0.893   \n",
       "75%                 2.160    1.031    0.449    1.523    1.339    1.618   \n",
       "max               101.520    3.341    3.064    5.205    4.179    5.850   \n",
       "\n",
       "          wb_5     wb_6     wb_7     wb_8     wb_9    wb_10    wb_11    wb_12  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.621    1.012   -0.200    0.618    1.442    1.288    0.172    0.261   \n",
       "std      1.348    1.286    1.196    0.983    1.020    1.121    1.080    1.100   \n",
       "min     -4.647   -6.625   -7.320   -3.563   -2.030   -1.637   -3.725   -5.336   \n",
       "25%     -0.025    0.249   -0.549    0.007    0.750    0.368   -0.229   -0.130   \n",
       "50%      0.674    0.923   -0.255    0.453    1.421    1.062    0.060    0.087   \n",
       "75%      1.494    1.811    0.519    1.229    2.108    1.920    0.826    0.923   \n",
       "max      5.507    6.973    2.890    3.783    6.306    6.171    3.193    3.503   \n",
       "\n",
       "         wb_13    wb_14    wb_15    wb_16    wb_17    wb_18    wb_19    wb_20  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.991    0.644    1.059    0.477    1.069    1.252    1.150    0.892   \n",
       "std      0.989    1.338    1.004    1.261    1.216    1.170    1.259    1.285   \n",
       "min     -1.629   -5.203   -1.729   -4.493   -4.164   -3.068   -3.213   -4.036   \n",
       "25%      0.171   -0.000    0.221   -0.046    0.238    0.300    0.239    0.109   \n",
       "50%      0.730    0.684    0.841    0.337    1.073    1.128    1.093    0.899   \n",
       "75%      1.560    1.467    1.696    1.273    1.912    1.968    1.964    1.770   \n",
       "max      5.331    5.871    4.778    5.664    5.090    5.564    6.354    5.278   \n",
       "\n",
       "         wb_21    wb_22    wb_23    wb_24    wb_25    wb_26    wb_27    wb_28  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.430    0.914    0.105   -0.188    0.202    0.342   -0.050   -0.035   \n",
       "std      1.115    1.102    1.156    1.161    1.077    1.072    1.007    1.174   \n",
       "min     -3.530   -2.849   -5.960   -6.897   -4.485   -5.575   -3.930   -5.490   \n",
       "25%     -0.069    0.242   -0.310   -0.507   -0.156   -0.120   -0.361   -0.398   \n",
       "50%      0.288    0.911    0.018   -0.337    0.011    0.196   -0.337   -0.108   \n",
       "75%      1.113    1.663    0.791    0.537    0.907    1.036    0.554    0.694   \n",
       "max      3.879    4.699    4.648    2.869    5.005    4.161    3.030    3.665   \n",
       "\n",
       "         wb_29    wb_30    wb_31    wb_32    wb_33    wb_34    wb_35    wb_36  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -0.029    0.892    1.130    0.729    0.978    0.719   -0.192   -0.204   \n",
       "std      1.515    1.182    1.239    1.039    1.014    1.119    1.237    1.141   \n",
       "min     -8.583   -4.007   -3.859   -1.931   -2.148   -2.764   -7.965   -4.550   \n",
       "25%     -0.535    0.037    0.390    0.055    0.185    0.045   -0.656   -0.551   \n",
       "50%      0.149    0.861    1.049    0.476    0.745    0.789   -0.201   -0.265   \n",
       "75%      0.922    1.684    1.969    1.399    1.693    1.557    0.598    0.526   \n",
       "max      3.574    5.410    8.284    5.335    4.669    3.462    3.946    2.920   \n",
       "\n",
       "         wb_37    wb_38    wb_39    wb_40    wb_41    wb_42    wb_43    wb_44  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.214   -0.044   -0.056    0.585    1.015    1.206   -0.009    1.317   \n",
       "std      1.100    1.131    1.451    1.093    1.055    1.225    1.442    1.221   \n",
       "min     -7.498   -5.886   -8.130   -6.421   -1.750   -3.101   -8.600   -3.949   \n",
       "25%     -0.149   -0.470   -0.530   -0.007    0.144    0.325   -0.498    0.438   \n",
       "50%      0.059   -0.122    0.051    0.407    0.866    1.237    0.157    1.232   \n",
       "75%      0.845    0.661    0.865    1.265    1.705    2.029    0.872    2.118   \n",
       "max      3.916    3.053    3.205    4.436    5.378    6.016    2.996    5.483   \n",
       "\n",
       "         wb_45    wb_46    wb_47    wb_48    wb_49    wb_50    wb_51    wb_52  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.998    0.253    1.451    0.768   -0.216    0.203    0.414   -0.240   \n",
       "std      1.280    1.120    1.124    1.011    1.130    1.060    1.043    1.035   \n",
       "min     -3.762   -5.368   -2.476   -2.360   -6.158   -3.346   -5.340   -4.449   \n",
       "25%      0.178   -0.169    0.719    0.084   -0.515   -0.109   -0.015   -0.462   \n",
       "50%      1.006    0.159    1.443    0.548   -0.359    0.094    0.317   -0.440   \n",
       "75%      1.859    0.983    2.222    1.411    0.466    0.842    1.015    0.310   \n",
       "max      6.266    4.042    6.693    5.318    3.952    3.200    3.709    4.659   \n",
       "\n",
       "         wb_53    wb_54    wb_55    wb_56    wb_57    wb_58    wb_59    wb_60  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.057   -0.025    1.309    1.271   -0.485    0.048    0.511    0.696   \n",
       "std      1.174    1.476    1.128    1.235    1.194    1.094    1.172    1.131   \n",
       "min     -7.101   -6.808   -1.872   -4.390   -5.003   -4.917   -4.357   -3.794   \n",
       "25%     -0.224   -0.503    0.421    0.515   -0.859   -0.322   -0.087    0.014   \n",
       "50%      0.042    0.127    1.196    1.160   -0.506    0.014    0.574    0.560   \n",
       "75%      0.789    0.877    2.057    2.061    0.215    0.703    1.273    1.377   \n",
       "max      3.123    3.949    6.202    7.639    2.679    3.129    3.701    6.378   \n",
       "\n",
       "         wb_61    wb_62    wb_63    wb_64    wb_65    wb_66    wb_67    wb_68  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.997    0.905    0.627    1.075   -0.090    0.041    1.180    0.639   \n",
       "std      0.960    0.951    1.005    1.114    1.142    1.203    1.234    1.248   \n",
       "min     -1.945   -1.569   -3.056   -2.515   -6.052   -6.921   -4.084   -4.583   \n",
       "25%      0.204    0.175    0.018    0.118   -0.488   -0.379    0.282   -0.044   \n",
       "50%      0.785    0.651    0.438    0.979   -0.076   -0.059    1.183    0.674   \n",
       "75%      1.551    1.516    1.290    1.742    0.593    0.739    2.043    1.445   \n",
       "max      4.461    5.518    4.222    5.163    3.108    3.468    6.322    4.725   \n",
       "\n",
       "         wb_69    wb_70    wb_71    wb_72    wb_73    wb_74    wb_75    wb_76  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.017    1.388   -0.532    0.804    0.834    0.426    0.836    0.987   \n",
       "std      1.570    1.186    1.187    1.133    0.942    1.122    0.910    0.962   \n",
       "min     -8.554   -2.629   -6.673   -2.633   -3.228   -4.565   -1.479   -1.450   \n",
       "25%     -0.644    0.543   -1.026    0.113    0.132   -0.075    0.163    0.214   \n",
       "50%      0.235    1.345   -0.465    0.784    0.658    0.231    0.642    0.781   \n",
       "75%      0.945    2.213    0.182    1.599    1.376    1.074    1.418    1.611   \n",
       "max      3.550    5.466    2.669    6.340    4.719    5.215    4.678    4.829   \n",
       "\n",
       "         wb_77    wb_78    wb_79    wb_80    wb_81    wb_82    wb_83    wb_84  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -0.227    1.104    1.364    0.276    0.923    0.682    0.024    1.356   \n",
       "std      1.010    1.070    1.204    1.488    1.292    1.035    1.147    1.088   \n",
       "min     -3.721   -1.449   -1.915   -7.355   -5.022   -2.286   -5.009   -1.918   \n",
       "25%     -0.465    0.241    0.328   -0.379    0.126    0.040   -0.389    0.629   \n",
       "50%     -0.441    0.878    1.258    0.333    0.832    0.482    0.016    1.321   \n",
       "75%      0.339    1.749    2.221    1.254    1.794    1.268    0.723    2.117   \n",
       "max      2.768    5.804    7.567    6.143    7.275    6.814    2.994    5.632   \n",
       "\n",
       "         wb_85    wb_86    wb_87    wb_88    wb_89    wb_90    wb_91    wb_92  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -0.070    0.771   -0.462   -0.031    1.004    0.557   -0.001    0.546   \n",
       "std      1.247    0.974    1.185    1.148    1.220    1.005    1.291    1.318   \n",
       "min     -5.549   -3.046   -6.919   -4.695   -3.857   -3.418   -8.077   -5.461   \n",
       "25%     -0.545    0.111   -0.756   -0.384    0.045   -0.002   -0.463   -0.329   \n",
       "50%     -0.074    0.608   -0.457   -0.080    0.932    0.413   -0.165    0.595   \n",
       "75%      0.719    1.387    0.210    0.641    1.891    1.190    0.823    1.528   \n",
       "max      3.545    4.848    2.578    3.295    5.250    4.029    3.370    4.595   \n",
       "\n",
       "         wb_93    wb_94    wb_95    wb_96    wb_97    wb_98    wb_99   wb_100  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.564    0.164    0.244    0.415    0.271    0.780   -0.103   -0.144   \n",
       "std      1.464    1.679    1.420    1.164    1.228    0.976    1.156    0.987   \n",
       "min     -7.963   -8.206   -6.277   -5.949   -3.550   -1.957   -6.290   -4.062   \n",
       "25%     -0.069   -0.585   -0.623   -0.044   -0.413    0.114   -0.394   -0.446   \n",
       "50%      0.650    0.361    0.299    0.324    0.279    0.582   -0.260   -0.257   \n",
       "75%      1.536    1.276    1.244    1.105    1.110    1.392    0.584    0.461   \n",
       "max      6.425    4.420    4.540    4.535    3.582    4.912    3.183    2.493   \n",
       "\n",
       "        wb_101   wb_102   wb_103   wb_104   wb_105   wb_106   wb_107   wb_108  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.342    0.489   -0.426    0.699    0.129    0.105    0.882    0.133   \n",
       "std      1.087    0.961    1.113    1.443    1.504    1.284    1.020    1.099   \n",
       "min     -3.572   -3.891   -7.725   -4.420   -7.451   -7.571   -2.268   -6.450   \n",
       "25%     -0.119   -0.028   -0.899   -0.061   -0.366   -0.599    0.170   -0.283   \n",
       "50%      0.195    0.151   -0.454    0.740    0.341    0.127    0.640    0.097   \n",
       "75%      0.919    1.069    0.233    1.664    1.116    0.936    1.459    0.787   \n",
       "max      3.935    3.543    2.734    6.128    3.345    4.432    5.019    3.382   \n",
       "\n",
       "        wb_109   wb_110   wb_111   wb_112   wb_113   wb_114   wb_115   wb_116  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.101   -0.418   -0.347    0.594    0.081    0.933   -0.290    0.772   \n",
       "std      1.258    1.281    1.103    0.944    1.108    1.242    1.178    1.151   \n",
       "min     -4.620   -7.436   -5.569   -3.224   -5.417   -2.996   -7.162   -3.572   \n",
       "25%     -0.630   -0.970   -0.750    0.023   -0.279    0.043   -0.788   -0.003   \n",
       "50%      0.253   -0.338   -0.417    0.427    0.027    0.883   -0.278    0.689   \n",
       "75%      0.925    0.384    0.293    1.165    0.750    1.808    0.350    1.527   \n",
       "max      3.217    2.428    2.463    3.867    3.001    5.230    2.814    6.152   \n",
       "\n",
       "        wb_117   wb_118   wb_119   wb_120   wb_121   wb_122   wb_123   wb_124  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     1.213    0.410    0.452    1.047    1.060    0.968   -0.203    1.162   \n",
       "std      1.261    1.478    1.496    1.283    1.020    1.143    1.180    1.034   \n",
       "min     -2.346   -9.348   -6.313   -3.966   -1.500   -6.250   -5.337   -1.383   \n",
       "25%      0.387   -0.106   -0.278    0.231    0.246    0.316   -0.752    0.292   \n",
       "50%      1.207    0.526    0.596    1.043    0.857    0.957   -0.280    0.886   \n",
       "75%      2.060    1.394    1.453    1.910    1.683    1.796    0.449    1.758   \n",
       "max      6.133    4.946    6.262    5.665    5.370    4.860    4.601    5.198   \n",
       "\n",
       "        wb_125   wb_126   wb_127   wb_128   wb_129   wb_130   wb_131   wb_132  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.416    0.058    0.435    0.307    0.736    0.634    0.215    0.097   \n",
       "std      1.005    0.967    0.995    1.062    1.082    1.025    1.264    1.073   \n",
       "min     -3.171   -3.709   -3.346   -4.012   -3.159   -4.253   -4.844   -4.202   \n",
       "25%     -0.091   -0.259   -0.095   -0.166    0.068    0.084   -0.241   -0.207   \n",
       "50%      0.247    0.023    0.050    0.197    0.825    0.711    0.304   -0.041   \n",
       "75%      1.163    0.598    1.178    1.067    1.335    1.194    0.888    0.748   \n",
       "max      2.954    3.279    2.955    2.700    3.661    3.563    3.880    3.001   \n",
       "\n",
       "        wb_133   wb_134   wb_135   wb_136   wb_137   wb_138   wb_139   wb_140  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.226    0.764    0.441    0.298    0.254    0.360    0.543    0.151   \n",
       "std      1.018    1.026    1.099    1.000    1.012    1.000    1.120    1.060   \n",
       "min     -3.384   -2.776   -3.736   -3.073   -4.053   -3.140   -3.964   -3.754   \n",
       "25%     -0.144    0.197   -0.154   -0.183   -0.138   -0.123   -0.166   -0.202   \n",
       "50%      0.140    0.814    0.338    0.172    0.105    0.205    0.668    0.115   \n",
       "75%      0.880    1.369    1.225    0.967    0.875    1.031    1.187    0.824   \n",
       "max      3.327    3.881    3.486    2.651    2.949    2.774    3.669    2.679   \n",
       "\n",
       "        wb_141   wb_142   wb_143   wb_144   wb_145   wb_146   wb_147   wb_148  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.680    0.169    0.765    0.702    0.401    0.173    0.574   -0.065   \n",
       "std      0.980    1.591    1.039    1.100    1.424    1.097    1.120    0.984   \n",
       "min     -3.182   -4.242   -4.487   -3.843   -3.741   -4.577   -4.323   -4.557   \n",
       "25%     -0.083   -0.701    0.171    0.187   -0.165   -0.192    0.023   -0.289   \n",
       "50%      0.691    0.617    0.807    0.796    0.705    0.132    0.620   -0.093   \n",
       "75%      1.232    1.159    1.320    1.299    1.240    0.882    1.205    0.449   \n",
       "max      3.547    3.693    3.610    4.390    3.763    3.107    3.847    2.576   \n",
       "\n",
       "        wb_149   wb_150   wb_151   wb_152   wb_153   wb_154   wb_155   wb_156  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.409   -2.130   -2.555   -2.019   -2.505    2.804    2.702    0.088   \n",
       "std      1.018    1.420    1.876    1.581    1.824    2.005    1.928    3.001   \n",
       "min     -2.721   -8.957  -10.828   -9.993  -10.828    0.094    0.111   -9.161   \n",
       "25%     -0.143   -2.789   -3.259   -2.886   -3.126    1.827    1.757   -1.683   \n",
       "50%      0.164   -2.066   -2.037   -2.005   -2.190    2.244    2.145   -1.181   \n",
       "75%      1.139   -1.458   -1.547   -0.268   -1.586    3.099    2.834    1.986   \n",
       "max      3.271   -0.191   -0.247   -0.214    9.294   12.339   13.444   12.424   \n",
       "\n",
       "        wb_157   wb_158   wb_159   wb_160   wb_161   wb_162   wb_163   wb_164  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -2.651   -2.501   -0.089   -1.976   -2.454   -2.431   -2.169    2.359   \n",
       "std      1.968    1.693    2.140    2.740    1.709    1.755    1.554    1.861   \n",
       "min    -13.577   -9.573   -7.669   -9.436   -9.704  -13.517  -10.176   -6.458   \n",
       "25%     -3.465   -3.054   -1.799   -3.005   -3.137   -3.154   -2.790    1.570   \n",
       "50%     -2.221   -2.084   -1.146   -2.042   -2.161   -2.139   -1.947    2.014   \n",
       "75%     -1.555   -1.567    1.927   -1.444   -1.555   -1.547   -1.413    2.557   \n",
       "max     -0.195   -0.255    8.962   14.566   -0.132   -0.243    5.188   10.721   \n",
       "\n",
       "        wb_165   wb_166   wb_167   wb_168   wb_169   wb_170   wb_171   wb_172  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -2.391    2.467   -0.356    2.842    2.150   -0.019   -2.708   -0.108   \n",
       "std      1.685    1.771    2.969    1.987    3.110    3.032    1.864    2.345   \n",
       "min    -10.563    0.371  -10.677    0.187  -13.015  -12.220  -11.116   -9.134   \n",
       "25%     -2.916    1.673   -1.934    1.881    1.685   -1.925   -3.532   -1.769   \n",
       "50%     -2.035    2.361    1.338    2.305    2.082    1.584   -2.246   -1.153   \n",
       "75%     -1.490    2.899    1.837    3.074    2.903    2.043   -1.610    1.889   \n",
       "max      1.225   11.894    7.740   12.043   12.658    7.209   -0.236   12.969   \n",
       "\n",
       "        wb_173   wb_174   wb_175  \n",
       "count 1000.000 1000.000 1000.000  \n",
       "mean    -2.162   -2.500    0.315  \n",
       "std      1.872    1.831    0.715  \n",
       "min    -10.318  -10.011   -1.877  \n",
       "25%     -2.628   -3.308   -0.172  \n",
       "50%     -1.713   -2.262    0.384  \n",
       "75%     -1.248   -1.445    0.739  \n",
       "max     10.316   -0.258    2.634  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.41681430e-02, 3.72841132e-01, 6.39588164e-01, 4.31336366e-01,\n",
       "        8.56509560e-01],\n",
       "       [7.96713547e-01, 3.73376287e-01, 2.66019144e-01, 5.03038595e-01,\n",
       "        7.23443984e-01],\n",
       "       [5.71495801e-01, 2.77435866e-01, 8.10306934e-01, 4.42926452e-01,\n",
       "        6.03366085e-01],\n",
       "       [1.95610034e-01, 7.27693240e-01, 1.76130307e-01, 6.94681505e-01,\n",
       "        9.26936666e-01],\n",
       "       [7.98376944e-01, 8.60156528e-01, 2.02660185e-01, 4.99962330e-01,\n",
       "        8.25956287e-01],\n",
       "       [8.89907347e-01, 6.92556560e-01, 4.23117334e-01, 3.95357490e-01,\n",
       "        4.34007680e-01],\n",
       "       [7.99697868e-02, 1.41172526e-01, 2.86208851e-01, 7.98252916e-01,\n",
       "        1.44731288e-01],\n",
       "       [8.06024056e-02, 8.76677670e-01, 1.23288125e-01, 5.51061324e-01,\n",
       "        8.09642353e-01],\n",
       "       [9.47447594e-01, 3.43203002e-01, 6.69193460e-01, 4.65364961e-01,\n",
       "        8.87984698e-01],\n",
       "       [7.66968762e-01, 7.97805359e-01, 3.51139847e-01, 1.41215150e-04,\n",
       "        6.35638876e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.X_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 31.24544161],\n",
       "       [-15.22227833],\n",
       "       [ 87.40496528],\n",
       "       [-54.87906589],\n",
       "       [-13.12457862],\n",
       "       [ 60.26032896],\n",
       "       [ 10.1981285 ],\n",
       "       [-26.28847303],\n",
       "       [ 84.55101181],\n",
       "       [ 38.24732019]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.y_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:32:09.782470Z",
     "start_time": "2021-01-05T09:31:56.901018Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "\n",
    "if inet_training_without_noise:\n",
    "   \n",
    "    for lambda_net_dataset, lambda_net_dataset_without_noise in zip(lambda_net_dataset_list, lambda_net_dataset_list_without_noise):\n",
    "        if inet_holdout_seed_evaluation:\n",
    "            raise SystemExit('Holdout Evaluation not implemented with inet training without noise')\n",
    "            \n",
    "        else:\n",
    "            lambda_net_train_dataset = lambda_net_dataset_without_noise\n",
    "\n",
    "            lambda_net_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_dataset_list_without_noise\n",
    "        \n",
    "else:\n",
    "\n",
    "    for lambda_net_dataset in lambda_net_dataset_list:\n",
    "\n",
    "        if inet_holdout_seed_evaluation:\n",
    "\n",
    "            complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "\n",
    "            if isinstance(test_size, float):\n",
    "                test_size = int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-test_size)))\n",
    "\n",
    "            test_seeds = random.sample(complete_seed_list, test_size)\n",
    "            lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "            valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-0.1))))\n",
    "            lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "            train_seeds = complete_seed_list\n",
    "            lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset\n",
    "        else:\n",
    "\n",
    "            lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "            lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.1)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "\n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:06.495716Z",
     "start_time": "2021-01-05T09:32:09.784760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 240)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:08.945802Z",
     "start_time": "2021-01-05T09:33:06.499150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 240)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:11.543306Z",
     "start_time": "2021-01-05T09:33:08.947468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 240)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.490</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.763</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>70.476</td>\n",
       "      <td>83.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-41.185</td>\n",
       "      <td>8.102</td>\n",
       "      <td>-2.361</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>70.312</td>\n",
       "      <td>4.155</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-7.685</td>\n",
       "      <td>45.664</td>\n",
       "      <td>1.059</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>-22.699</td>\n",
       "      <td>-22.228</td>\n",
       "      <td>67.163</td>\n",
       "      <td>-4.917</td>\n",
       "      <td>0.747</td>\n",
       "      <td>31.916</td>\n",
       "      <td>57.046</td>\n",
       "      <td>-32.837</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.490</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.763</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>70.476</td>\n",
       "      <td>83.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.535</td>\n",
       "      <td>-1.360</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.032</td>\n",
       "      <td>2.249</td>\n",
       "      <td>2.175</td>\n",
       "      <td>2.428</td>\n",
       "      <td>-1.586</td>\n",
       "      <td>0.007</td>\n",
       "      <td>2.545</td>\n",
       "      <td>2.848</td>\n",
       "      <td>-2.743</td>\n",
       "      <td>-2.918</td>\n",
       "      <td>2.354</td>\n",
       "      <td>2.211</td>\n",
       "      <td>2.566</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.412</td>\n",
       "      <td>2.316</td>\n",
       "      <td>2.388</td>\n",
       "      <td>2.234</td>\n",
       "      <td>-1.104</td>\n",
       "      <td>2.259</td>\n",
       "      <td>-3.106</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>1.359</td>\n",
       "      <td>1.806</td>\n",
       "      <td>2.103</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.176</td>\n",
       "      <td>1.731</td>\n",
       "      <td>-2.936</td>\n",
       "      <td>1.512</td>\n",
       "      <td>1.893</td>\n",
       "      <td>-2.204</td>\n",
       "      <td>1.344</td>\n",
       "      <td>-2.573</td>\n",
       "      <td>1.774</td>\n",
       "      <td>2.078</td>\n",
       "      <td>1.368</td>\n",
       "      <td>2.008</td>\n",
       "      <td>1.869</td>\n",
       "      <td>0.072</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.320</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-1.358</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>-1.710</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>1.801</td>\n",
       "      <td>2.547</td>\n",
       "      <td>2.753</td>\n",
       "      <td>-1.258</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>2.124</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>2.089</td>\n",
       "      <td>1.331</td>\n",
       "      <td>-1.423</td>\n",
       "      <td>2.456</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>1.775</td>\n",
       "      <td>2.644</td>\n",
       "      <td>2.174</td>\n",
       "      <td>1.968</td>\n",
       "      <td>2.669</td>\n",
       "      <td>-1.414</td>\n",
       "      <td>2.343</td>\n",
       "      <td>1.604</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>1.134</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.325</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.769</td>\n",
       "      <td>1.116</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>1.296</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>1.133</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.647</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.561</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.422</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.650</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-1.219</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>1.140</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1.340</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.971</td>\n",
       "      <td>1.152</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.122</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>1.171</td>\n",
       "      <td>2.409</td>\n",
       "      <td>1.767</td>\n",
       "      <td>2.446</td>\n",
       "      <td>2.111</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>2.201</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-1.015</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.497</td>\n",
       "      <td>2.063</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>2.312</td>\n",
       "      <td>-3.131</td>\n",
       "      <td>-2.432</td>\n",
       "      <td>-3.276</td>\n",
       "      <td>-2.749</td>\n",
       "      <td>1.911</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.990</td>\n",
       "      <td>-2.923</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>1.832</td>\n",
       "      <td>-4.610</td>\n",
       "      <td>-4.665</td>\n",
       "      <td>-5.314</td>\n",
       "      <td>-4.788</td>\n",
       "      <td>1.856</td>\n",
       "      <td>-4.437</td>\n",
       "      <td>2.250</td>\n",
       "      <td>1.728</td>\n",
       "      <td>2.036</td>\n",
       "      <td>1.875</td>\n",
       "      <td>1.835</td>\n",
       "      <td>-2.751</td>\n",
       "      <td>1.862</td>\n",
       "      <td>-4.165</td>\n",
       "      <td>-2.982</td>\n",
       "      <td>0.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>21.501</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.649</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.613</td>\n",
       "      <td>-31.331</td>\n",
       "      <td>-86.712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21.778</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.989</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-1.449</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>27.798</td>\n",
       "      <td>-1.441</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-42.118</td>\n",
       "      <td>-30.896</td>\n",
       "      <td>-86.983</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.137</td>\n",
       "      <td>21.501</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>27.649</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.613</td>\n",
       "      <td>-31.331</td>\n",
       "      <td>-86.712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.457</td>\n",
       "      <td>1.951</td>\n",
       "      <td>2.862</td>\n",
       "      <td>2.649</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.054</td>\n",
       "      <td>2.546</td>\n",
       "      <td>3.061</td>\n",
       "      <td>2.993</td>\n",
       "      <td>2.102</td>\n",
       "      <td>1.684</td>\n",
       "      <td>2.832</td>\n",
       "      <td>-2.667</td>\n",
       "      <td>2.894</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>2.774</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2.689</td>\n",
       "      <td>2.505</td>\n",
       "      <td>2.417</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.658</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-1.593</td>\n",
       "      <td>3.252</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.086</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.639</td>\n",
       "      <td>1.217</td>\n",
       "      <td>0.709</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.157</td>\n",
       "      <td>1.766</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>2.227</td>\n",
       "      <td>1.570</td>\n",
       "      <td>0.787</td>\n",
       "      <td>1.583</td>\n",
       "      <td>4.509</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>3.694</td>\n",
       "      <td>0.779</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>2.800</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-4.630</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1.226</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.134</td>\n",
       "      <td>1.825</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.231</td>\n",
       "      <td>1.889</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-2.702</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>2.423</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>1.777</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-2.300</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-1.596</td>\n",
       "      <td>-2.369</td>\n",
       "      <td>-2.116</td>\n",
       "      <td>0.107</td>\n",
       "      <td>5.442</td>\n",
       "      <td>-7.049</td>\n",
       "      <td>-1.882</td>\n",
       "      <td>-1.946</td>\n",
       "      <td>-1.859</td>\n",
       "      <td>-2.118</td>\n",
       "      <td>-1.840</td>\n",
       "      <td>-1.574</td>\n",
       "      <td>-1.978</td>\n",
       "      <td>3.529</td>\n",
       "      <td>-2.011</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-1.595</td>\n",
       "      <td>0.208</td>\n",
       "      <td>5.516</td>\n",
       "      <td>-1.897</td>\n",
       "      <td>-2.041</td>\n",
       "      <td>-1.495</td>\n",
       "      <td>-6.235</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.972</td>\n",
       "      <td>47.932</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-35.026</td>\n",
       "      <td>79.336</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-64.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-6.966</td>\n",
       "      <td>46.836</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>49.512</td>\n",
       "      <td>-1.387</td>\n",
       "      <td>-3.075</td>\n",
       "      <td>4.746</td>\n",
       "      <td>0.526</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-15.663</td>\n",
       "      <td>0.802</td>\n",
       "      <td>4.168</td>\n",
       "      <td>1.668</td>\n",
       "      <td>-34.058</td>\n",
       "      <td>73.139</td>\n",
       "      <td>3.937</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>2.379</td>\n",
       "      <td>-44.606</td>\n",
       "      <td>3.116</td>\n",
       "      <td>-16.918</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.972</td>\n",
       "      <td>47.932</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-35.026</td>\n",
       "      <td>79.336</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-64.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.361</td>\n",
       "      <td>2.719</td>\n",
       "      <td>1.734</td>\n",
       "      <td>3.569</td>\n",
       "      <td>2.739</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.748</td>\n",
       "      <td>1.138</td>\n",
       "      <td>7.690</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>1.144</td>\n",
       "      <td>1.429</td>\n",
       "      <td>0.710</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.219</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>1.630</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-2.118</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>2.537</td>\n",
       "      <td>1.898</td>\n",
       "      <td>3.675</td>\n",
       "      <td>2.422</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>0.247</td>\n",
       "      <td>2.441</td>\n",
       "      <td>1.840</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>2.478</td>\n",
       "      <td>-1.954</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-2.081</td>\n",
       "      <td>2.237</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.592</td>\n",
       "      <td>1.866</td>\n",
       "      <td>2.011</td>\n",
       "      <td>1.955</td>\n",
       "      <td>1.728</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>1.866</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>2.378</td>\n",
       "      <td>2.066</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>2.059</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-1.365</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>2.487</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>2.299</td>\n",
       "      <td>2.707</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.523</td>\n",
       "      <td>0.239</td>\n",
       "      <td>2.602</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-2.834</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.544</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.747</td>\n",
       "      <td>-3.771</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>1.887</td>\n",
       "      <td>1.649</td>\n",
       "      <td>10.813</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>1.604</td>\n",
       "      <td>-2.985</td>\n",
       "      <td>-2.603</td>\n",
       "      <td>-9.552</td>\n",
       "      <td>-2.988</td>\n",
       "      <td>1.709</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>2.085</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1.830</td>\n",
       "      <td>1.650</td>\n",
       "      <td>1.536</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>1.634</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.349</td>\n",
       "      <td>0.000</td>\n",
       "      <td>90.525</td>\n",
       "      <td>-36.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-17.307</td>\n",
       "      <td>-11.759</td>\n",
       "      <td>43.727</td>\n",
       "      <td>-21.109</td>\n",
       "      <td>25.278</td>\n",
       "      <td>69.439</td>\n",
       "      <td>-50.581</td>\n",
       "      <td>9.683</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-1.405</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>1.343</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-18.028</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>13.349</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>90.525</td>\n",
       "      <td>-36.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>8.893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-17.307</td>\n",
       "      <td>0.747</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>1.222</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.087</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>0.967</td>\n",
       "      <td>1.325</td>\n",
       "      <td>1.375</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.107</td>\n",
       "      <td>1.182</td>\n",
       "      <td>0.107</td>\n",
       "      <td>1.177</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.671</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.383</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.391</td>\n",
       "      <td>1.133</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.489</td>\n",
       "      <td>1.159</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.797</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>1.830</td>\n",
       "      <td>3.048</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>2.560</td>\n",
       "      <td>1.977</td>\n",
       "      <td>1.384</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.403</td>\n",
       "      <td>2.740</td>\n",
       "      <td>-4.148</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1.777</td>\n",
       "      <td>0.814</td>\n",
       "      <td>1.147</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.565</td>\n",
       "      <td>1.461</td>\n",
       "      <td>1.235</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>1.323</td>\n",
       "      <td>2.914</td>\n",
       "      <td>-4.159</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-1.984</td>\n",
       "      <td>-1.352</td>\n",
       "      <td>-1.841</td>\n",
       "      <td>1.921</td>\n",
       "      <td>1.602</td>\n",
       "      <td>-1.593</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-1.283</td>\n",
       "      <td>-2.064</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>2.827</td>\n",
       "      <td>-1.584</td>\n",
       "      <td>2.039</td>\n",
       "      <td>-1.704</td>\n",
       "      <td>1.858</td>\n",
       "      <td>2.246</td>\n",
       "      <td>1.766</td>\n",
       "      <td>1.869</td>\n",
       "      <td>2.071</td>\n",
       "      <td>0.264</td>\n",
       "      <td>2.167</td>\n",
       "      <td>-2.012</td>\n",
       "      <td>2.306</td>\n",
       "      <td>0.673</td>\n",
       "      <td>-1.044</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.955</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.136</td>\n",
       "      <td>1.279</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.534</td>\n",
       "      <td>1.151</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.673</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-2.380</td>\n",
       "      <td>-5.257</td>\n",
       "      <td>-2.280</td>\n",
       "      <td>-2.972</td>\n",
       "      <td>1.784</td>\n",
       "      <td>1.516</td>\n",
       "      <td>-1.550</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-2.016</td>\n",
       "      <td>-1.814</td>\n",
       "      <td>-2.032</td>\n",
       "      <td>-3.822</td>\n",
       "      <td>-5.174</td>\n",
       "      <td>-2.085</td>\n",
       "      <td>1.602</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>1.995</td>\n",
       "      <td>1.346</td>\n",
       "      <td>1.723</td>\n",
       "      <td>1.536</td>\n",
       "      <td>1.436</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>1.571</td>\n",
       "      <td>-4.513</td>\n",
       "      <td>-5.060</td>\n",
       "      <td>0.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-94.870</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-28.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-52.851</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-37.785</td>\n",
       "      <td>5.567</td>\n",
       "      <td>-6.165</td>\n",
       "      <td>1.923</td>\n",
       "      <td>-10.618</td>\n",
       "      <td>-89.536</td>\n",
       "      <td>5.157</td>\n",
       "      <td>1.762</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-31.265</td>\n",
       "      <td>1.290</td>\n",
       "      <td>1.220</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-6.202</td>\n",
       "      <td>2.913</td>\n",
       "      <td>-48.055</td>\n",
       "      <td>-2.042</td>\n",
       "      <td>2.466</td>\n",
       "      <td>-35.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-94.870</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-28.089</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-52.851</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-37.785</td>\n",
       "      <td>1.367</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.693</td>\n",
       "      <td>1.547</td>\n",
       "      <td>-1.377</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>1.640</td>\n",
       "      <td>1.299</td>\n",
       "      <td>1.434</td>\n",
       "      <td>1.883</td>\n",
       "      <td>1.843</td>\n",
       "      <td>1.228</td>\n",
       "      <td>1.239</td>\n",
       "      <td>1.694</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.744</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>2.466</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>1.460</td>\n",
       "      <td>4.547</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>1.507</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.176</td>\n",
       "      <td>1.283</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1.047</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.649</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>1.524</td>\n",
       "      <td>1.304</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.182</td>\n",
       "      <td>1.061</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>1.380</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.633</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.103</td>\n",
       "      <td>1.747</td>\n",
       "      <td>1.561</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>1.424</td>\n",
       "      <td>0.643</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.355</td>\n",
       "      <td>1.784</td>\n",
       "      <td>1.832</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.933</td>\n",
       "      <td>4.398</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>1.729</td>\n",
       "      <td>2.252</td>\n",
       "      <td>1.409</td>\n",
       "      <td>1.996</td>\n",
       "      <td>1.443</td>\n",
       "      <td>1.764</td>\n",
       "      <td>1.023</td>\n",
       "      <td>1.446</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.697</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>3.340</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-4.390</td>\n",
       "      <td>0.982</td>\n",
       "      <td>2.697</td>\n",
       "      <td>1.266</td>\n",
       "      <td>1.857</td>\n",
       "      <td>1.260</td>\n",
       "      <td>0.885</td>\n",
       "      <td>1.190</td>\n",
       "      <td>1.296</td>\n",
       "      <td>0.757</td>\n",
       "      <td>-3.556</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1.578</td>\n",
       "      <td>1.087</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.740</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.144</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.073</td>\n",
       "      <td>2.234</td>\n",
       "      <td>0.037</td>\n",
       "      <td>3.779</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.944</td>\n",
       "      <td>1.409</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.584</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>2.015</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-1.541</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>2.421</td>\n",
       "      <td>-4.040</td>\n",
       "      <td>1.454</td>\n",
       "      <td>-1.494</td>\n",
       "      <td>-2.834</td>\n",
       "      <td>-2.611</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-1.609</td>\n",
       "      <td>-1.587</td>\n",
       "      <td>-1.689</td>\n",
       "      <td>-1.651</td>\n",
       "      <td>6.844</td>\n",
       "      <td>2.094</td>\n",
       "      <td>-1.265</td>\n",
       "      <td>-2.889</td>\n",
       "      <td>-1.571</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>-1.459</td>\n",
       "      <td>-1.621</td>\n",
       "      <td>-1.551</td>\n",
       "      <td>-1.421</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-1.524</td>\n",
       "      <td>2.835</td>\n",
       "      <td>-7.792</td>\n",
       "      <td>1.709</td>\n",
       "      <td>6.428</td>\n",
       "      <td>-7.317</td>\n",
       "      <td>-6.289</td>\n",
       "      <td>-1.280</td>\n",
       "      <td>-1.407</td>\n",
       "      <td>-1.675</td>\n",
       "      <td>-0.256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "6252  1373158606         0.000         0.000         0.000         0.000   \n",
       "4684  1373158606        21.501         0.000         0.000         0.000   \n",
       "1731  1373158606         0.000         0.000        47.972        47.932   \n",
       "4742  1373158606         0.000         0.000        13.349         0.000   \n",
       "4521  1373158606         0.000         0.261         0.000         0.000   \n",
       "\n",
       "      00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "6252         0.000         0.000         0.000        13.490         0.000   \n",
       "4684         0.000         0.000         0.000         0.000         0.000   \n",
       "1731         0.000         0.000         0.000         0.000         0.000   \n",
       "4742        90.525       -36.009         0.000         0.000         0.000   \n",
       "4521       -94.870         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "6252        27.763         0.000        -1.570         0.000         0.000   \n",
       "4684        27.649         0.000         0.000         0.000       -44.613   \n",
       "1731         0.000         0.000         0.000         0.000       -35.026   \n",
       "4742         8.893         0.000         0.000         0.000         0.000   \n",
       "4521         0.000       -28.089         0.000         0.000         0.000   \n",
       "\n",
       "      02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "6252         0.000         0.000         0.000         0.000        70.476   \n",
       "4684       -31.331       -86.712         0.000         0.000         0.000   \n",
       "1731        79.336         0.000         0.000         0.000       -64.234   \n",
       "4742         0.000         0.000         0.000         0.000         0.000   \n",
       "4521         0.000         0.000         0.000       -52.851         0.000   \n",
       "\n",
       "      11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "6252        83.810         0.000             -41.185               8.102   \n",
       "4684         0.000         0.000              21.778               0.372   \n",
       "1731         0.000         0.000              -6.966              46.836   \n",
       "4742         0.000       -17.307             -11.759              43.727   \n",
       "4521         0.000       -37.785               5.567              -6.165   \n",
       "\n",
       "      00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "6252              -2.361              -0.098               0.420   \n",
       "4684               0.327               0.989              -0.265   \n",
       "1731              -0.257              49.512              -1.387   \n",
       "4742             -21.109              25.278              69.439   \n",
       "4521               1.923             -10.618             -89.536   \n",
       "\n",
       "      00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "6252              -0.787              70.312               4.155   \n",
       "4684              -0.262              -1.449              -0.276   \n",
       "1731              -3.075               4.746               0.526   \n",
       "4742             -50.581               9.683               1.056   \n",
       "4521               5.157               1.762              -0.711   \n",
       "\n",
       "      00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "6252               0.154              -7.685              45.664   \n",
       "4684              -0.300              27.798              -1.441   \n",
       "1731              -0.141             -15.663               0.802   \n",
       "4742              -1.405              -0.290              -0.017   \n",
       "4521              -0.171              -0.511             -31.265   \n",
       "\n",
       "      01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "6252               1.059              -0.689             -22.699   \n",
       "4684              -0.186              -0.502             -42.118   \n",
       "1731               4.168               1.668             -34.058   \n",
       "4742               1.343              -0.966              -0.418   \n",
       "4521               1.290               1.220              -0.147   \n",
       "\n",
       "      02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "6252             -22.228              67.163              -4.917   \n",
       "4684             -30.896             -86.983              -0.677   \n",
       "1731              73.139               3.937              -0.805   \n",
       "4742              -0.220             -18.028              -0.205   \n",
       "4521               0.458              -6.202               2.913   \n",
       "\n",
       "      10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "6252               0.747              31.916              57.046   \n",
       "4684               0.339              -0.071               0.637   \n",
       "1731               2.379             -44.606               3.116   \n",
       "4742               0.507              -0.218              -0.296   \n",
       "4521             -48.055              -2.042               2.466   \n",
       "\n",
       "      20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "6252             -32.837              -0.000               0.000   \n",
       "4684               0.137              21.501              -0.000   \n",
       "1731             -16.918              -0.000               0.000   \n",
       "4742              -0.136              -0.000              -0.000   \n",
       "4521             -35.436               0.000               0.261   \n",
       "\n",
       "      00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "6252               0.000              -0.000              -0.000   \n",
       "4684               0.000              -0.000               0.000   \n",
       "1731              47.972              47.932              -0.000   \n",
       "4742              13.349              -0.000              90.525   \n",
       "4521              -0.000               0.000             -94.870   \n",
       "\n",
       "      00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "6252               0.000               0.000              13.490   \n",
       "4684               0.000              -0.000               0.000   \n",
       "1731              -0.000               0.000               0.000   \n",
       "4742             -36.009               0.000               0.000   \n",
       "4521               0.000              -0.000              -0.000   \n",
       "\n",
       "      00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "6252               0.000              27.763               0.000   \n",
       "4684              -0.000              27.649              -0.000   \n",
       "1731              -0.000              -0.000               0.000   \n",
       "4742              -0.000               8.893               0.000   \n",
       "4521              -0.000               0.000             -28.089   \n",
       "\n",
       "      01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "6252              -1.570              -0.000               0.000   \n",
       "4684              -0.000               0.000             -44.613   \n",
       "1731              -0.000               0.000             -35.026   \n",
       "4742               0.000              -0.000               0.000   \n",
       "4521              -0.000              -0.000               0.000   \n",
       "\n",
       "      02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "6252              -0.000              -0.000               0.000   \n",
       "4684             -31.331             -86.712               0.000   \n",
       "1731              79.336               0.000              -0.000   \n",
       "4742              -0.000               0.000              -0.000   \n",
       "4521               0.000              -0.000               0.000   \n",
       "\n",
       "      10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "6252              -0.000              70.476              83.810   \n",
       "4684               0.000               0.000              -0.000   \n",
       "1731              -0.000             -64.234               0.000   \n",
       "4742               0.000              -0.000              -0.000   \n",
       "4521             -52.851               0.000               0.000   \n",
       "\n",
       "      20000-lstsq_target   wb_0   wb_1   wb_2   wb_3   wb_4   wb_5  wb_6  \\\n",
       "6252               0.000 -1.535 -1.360 -0.996 -1.032  2.249  2.175 2.428   \n",
       "4684               0.000  2.457  1.951  2.862  2.649  0.011  0.006 0.002   \n",
       "1731              -0.000 -0.070 -0.342  0.204  0.040  0.226  0.179 0.208   \n",
       "4742             -17.307  0.747 -0.579  1.222  0.697  0.185  0.121 1.087   \n",
       "4521             -37.785  1.367  1.032  1.693  1.547 -1.377 -0.567 1.640   \n",
       "\n",
       "       wb_7  wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  \\\n",
       "6252 -1.586 0.007 2.545  2.848 -2.743 -2.918  2.354  2.211  2.566  2.000   \n",
       "4684  2.054 2.546 3.061  2.993  2.102  1.684  2.832 -2.667  2.894 -0.039   \n",
       "1731 -0.413 0.003 0.361  2.719  1.734  3.569  2.739  0.169  0.200  0.092   \n",
       "4742 -0.424 0.967 1.325  1.375  0.251  0.107  1.182  0.107  1.177  0.006   \n",
       "4521  1.299 1.434 1.883  1.843  1.228  1.239  1.694  0.003  1.744 -0.701   \n",
       "\n",
       "      wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  \\\n",
       "6252  2.412  2.316  2.388  2.234 -1.104  2.259 -3.106 -1.624  0.023  0.119   \n",
       "4684  2.774  0.173  0.012  2.689  2.505  2.417 -0.027 -0.429  0.744  0.764   \n",
       "1731  0.289  0.332  0.292  0.104 -0.062 -0.083 -0.227 -0.497 -0.146 -0.045   \n",
       "4742  0.228  0.271  0.250  0.019 -0.066 -0.274 -0.052 -0.041  0.278 -0.158   \n",
       "4521  2.466 -0.008  1.460  4.547 -0.582  1.507  1.255  0.958  1.176  1.283   \n",
       "\n",
       "      wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  \\\n",
       "6252 -0.229 -0.179  1.359  1.806  2.103  0.326  0.176  1.731 -2.936  1.512   \n",
       "4684  0.661  0.658 -0.350 -1.593  3.252  0.912  1.086  0.985  0.680  0.639   \n",
       "1731 -0.317 -0.301  0.748  1.138  7.690  0.043  0.170  1.122  0.041  0.321   \n",
       "4742  0.657  0.097  0.261  0.684  0.732  0.043  0.697  0.545  0.617 -0.247   \n",
       "4521  1.024  1.047 -0.086  0.075  1.649 -0.024  1.524  1.304  0.981  0.932   \n",
       "\n",
       "      wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  \\\n",
       "6252  1.893 -2.204  1.344 -2.573  1.774  2.078  1.368  2.008  1.869  0.072   \n",
       "4684  1.217  0.709 -0.574  0.999  0.157  1.766 -0.274  2.227  1.570  0.787   \n",
       "1731  0.084  0.089  0.692 -0.020  1.144  1.429  0.710  1.392  1.219 -0.108   \n",
       "4742  0.081  0.660  0.205  0.547  0.717  0.890  0.263  0.850  0.671 -0.116   \n",
       "4521  1.182  1.061 -0.380  1.380  0.349 -0.086 -0.633 -0.046 -0.002  0.103   \n",
       "\n",
       "      wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  \\\n",
       "6252  2.216  2.320  0.008 -1.358 -0.793 -1.710 -1.290  1.801  2.547  2.753   \n",
       "4684  1.583  4.509 -0.492 -0.153 -0.039 -0.357 -0.180 -0.442  3.694  0.779   \n",
       "1731  1.630  0.091 -0.497 -0.106 -0.009 -0.433 -0.208 -0.421  0.359 -2.118   \n",
       "4742  0.887  0.035 -0.204 -0.103 -0.035  0.321 -0.284  0.383  1.082  0.471   \n",
       "4521  1.747  1.561  0.883  0.210  0.365 -0.095  0.124 -0.100  1.424  0.643   \n",
       "\n",
       "      wb_57  wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  \\\n",
       "6252 -1.258 -0.147  2.124 -0.736  2.089  1.331 -1.423  2.456 -0.833  1.775   \n",
       "4684 -0.385 -0.143 -0.285 -0.027  0.229  0.176 -0.072  2.800 -0.213 -0.371   \n",
       "1731 -0.530 -0.149 -0.286  2.537  1.898  3.675  2.422  0.215 -0.231 -0.316   \n",
       "4742 -0.530  0.343  0.102  0.410 -0.099 -0.137  0.380  0.991  0.140  0.391   \n",
       "4521 -0.245  0.141  0.023  0.439  0.629  0.557  0.382  0.110  0.003  1.268   \n",
       "\n",
       "      wb_67  wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  \\\n",
       "6252  2.644  2.174  1.968  2.669 -1.414  2.343  1.604 -1.120  1.134  0.769   \n",
       "4684  0.358 -0.051 -4.630  0.380 -0.407  0.143  1.226 -0.086  0.375  0.427   \n",
       "1731  0.307 -0.001 -0.339  0.324 -0.539 -0.256  0.125 -0.053  0.161  0.207   \n",
       "4742  1.133  0.733  0.489  1.159 -0.536  0.797 -0.082 -0.225  1.830  3.048   \n",
       "4521  0.015  0.255 -0.100  0.059  0.017  0.254  0.617  0.355  1.784  1.832   \n",
       "\n",
       "      wb_77  wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  \\\n",
       "6252  0.940  0.952  1.325  0.768  0.769  1.116 -0.138  1.296 -0.089  0.016   \n",
       "4684  0.105  0.485  0.222  0.010 -0.004  0.416  0.181  0.419  0.116  0.307   \n",
       "1731 -0.550  0.247  2.441  1.840  0.063  0.033 -0.139  2.478 -1.954 -0.921   \n",
       "4742 -0.237  2.560  1.977  1.384  0.427  0.021  0.001  1.017  0.403  2.740   \n",
       "4521  1.200  1.933  4.398 -1.300  1.729  2.252  1.409  1.996  1.443  1.764   \n",
       "\n",
       "      wb_87  wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  \\\n",
       "6252 -0.286 -0.285  1.133 -0.136  0.532  0.734  0.934  0.817  0.589  0.911   \n",
       "4684 -0.204  0.134  1.825  0.350 -0.462 -0.032 -0.070  0.040 -0.087  0.371   \n",
       "1731  0.110 -2.081  2.237  0.007  1.592  1.866  2.011  1.955  1.728 -0.027   \n",
       "4742 -4.148  0.111  1.777  0.814  1.147  1.380  1.565  1.461  1.235 -0.041   \n",
       "4521  1.023  1.446  0.034  1.697 -1.865  3.340 -0.758 -4.390  0.982  2.697   \n",
       "\n",
       "      wb_97  wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  \\\n",
       "6252  0.598  0.058  0.850   0.919   0.647   1.081   0.561   1.018   0.749   \n",
       "4684 -0.127  0.032 -0.338   0.102   0.226   0.391   0.071  -0.053  -0.004   \n",
       "1731  1.866  0.122 -0.359  -0.383  -0.051  -0.057  -0.534   2.378   2.066   \n",
       "4742  1.323  2.914 -4.159  -1.798  -1.984  -1.352  -1.841   1.921   1.602   \n",
       "4521  1.266  1.857  1.260   0.885   1.190   1.296   0.757  -3.556  -0.196   \n",
       "\n",
       "      wb_106  wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  \\\n",
       "6252   0.422   1.180  -0.121   0.650  -0.302  -1.219  -0.264  -0.129   1.140   \n",
       "4684  -0.031   0.539   0.255   0.101   0.024  -0.119   0.177   0.231   1.889   \n",
       "1731   0.166   0.161  -0.121   2.059  -0.717  -1.365   0.050  -0.356   2.487   \n",
       "4742  -1.593   0.178  -1.283  -2.064  -1.916  -2.114   2.827  -1.584   2.039   \n",
       "4521   0.944   1.578   1.087   0.951   0.841   0.740   1.250   1.144   0.037   \n",
       "\n",
       "      wb_115  wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  \\\n",
       "6252  -0.347   0.988   1.340   0.892   0.971   1.152   1.040   1.122  -0.394   \n",
       "4684   0.128   0.001   0.339  -0.097  -0.001   0.208   0.593   0.232   0.013   \n",
       "1731  -0.397   2.299   2.707   2.216   2.346   2.523   0.239   2.602  -0.346   \n",
       "4742  -1.704   1.858   2.246   1.766   1.869   2.071   0.264   2.167  -2.012   \n",
       "4521   0.856   0.073   2.234   0.037   3.779   0.123   2.944   1.409   0.955   \n",
       "\n",
       "      wb_124  wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  \\\n",
       "6252   1.171   2.409   1.767   2.446   2.111   0.472   0.433  -0.051   2.201   \n",
       "4684   0.279  -0.096  -0.232   0.004  -0.073  -0.174  -0.144  -2.702  -0.081   \n",
       "1731   0.288  -0.086  -0.168  -0.069  -0.152   0.368   0.331  -2.834  -0.193   \n",
       "4742   2.306   0.673  -1.044   1.190   0.159   0.639   0.592   0.955  -0.196   \n",
       "4521   1.584  -0.004  -0.033   0.052  -0.053  -0.599   2.015  -0.051  -1.541   \n",
       "\n",
       "      wb_133  wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  \\\n",
       "6252  -0.128   0.566   0.741   0.281  -1.015  -0.183   0.453   0.338   0.514   \n",
       "4684  -0.135  -0.074  -0.087  -0.205  -0.359  -0.130   2.423  -0.116  -0.076   \n",
       "1731  -0.136   0.544   1.062   1.747  -3.771   0.522   0.364  -0.164   0.368   \n",
       "4742   0.958   1.136   1.279  -0.258   0.534   1.151   0.617   0.944   0.643   \n",
       "4521  -0.055  -0.047   0.033   0.029   0.044  -0.022  -0.166  -0.156   2.421   \n",
       "\n",
       "      wb_142  wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  \\\n",
       "6252   0.471   0.444   0.475   0.497   2.063   0.436  -0.347   2.312  -3.131   \n",
       "4684  -0.490  -0.139   1.777  -0.374  -0.078  -0.259  -2.300  -0.148  -2.000   \n",
       "1731   0.416   0.336   0.406   0.450  -0.170   0.483  -0.242  -0.132  -0.224   \n",
       "4742   0.649   0.604   0.646   0.673  -0.174   0.649  -0.898   0.156  -2.380   \n",
       "4521  -4.040   1.454  -1.494  -2.834  -2.611  -0.007  -0.038   0.107  -1.609   \n",
       "\n",
       "      wb_151  wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  \\\n",
       "6252  -2.432  -3.276  -2.749   1.911   2.000   1.990  -2.923  -0.265   1.832   \n",
       "4684  -1.596  -2.369  -2.116   0.107   5.442  -7.049  -1.882  -1.946  -1.859   \n",
       "1731  -0.295  -0.307  -0.213   1.887   1.649  10.813  -0.319  -0.263   1.604   \n",
       "4742  -5.257  -2.280  -2.972   1.784   1.516  -1.550  -0.324  -2.016  -1.814   \n",
       "4521  -1.587  -1.689  -1.651   6.844   2.094  -1.265  -2.889  -1.571  -1.370   \n",
       "\n",
       "      wb_160  wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  \\\n",
       "6252  -4.610  -4.665  -5.314  -4.788   1.856  -4.437   2.250   1.728   2.036   \n",
       "4684  -2.118  -1.840  -1.574  -1.978   3.529  -2.011   0.400  -1.595   0.208   \n",
       "1731  -2.985  -2.603  -9.552  -2.988   1.709  -0.148   2.085   1.450   1.830   \n",
       "4742  -2.032  -3.822  -5.174  -2.085   1.602  -2.028   1.995   1.346   1.723   \n",
       "4521  -1.459  -1.621  -1.551  -1.421   0.085  -1.524   2.835  -7.792   1.709   \n",
       "\n",
       "      wb_169  wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "6252   1.875   1.835  -2.751   1.862  -4.165  -2.982   0.453  \n",
       "4684   5.516  -1.897  -2.041  -1.495  -6.235  -0.319   0.627  \n",
       "1731   1.650   1.536  -0.275   1.634  -0.129  -0.338   0.079  \n",
       "4742   1.536   1.436  -0.277   1.571  -4.513  -5.060   0.472  \n",
       "4521   6.428  -7.317  -6.289  -1.280  -1.407  -1.675  -0.256  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-10.130</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>60.792</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.729</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-18.850</td>\n",
       "      <td>1.493</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>6.193</td>\n",
       "      <td>-1.886</td>\n",
       "      <td>-5.492</td>\n",
       "      <td>3.206</td>\n",
       "      <td>0.374</td>\n",
       "      <td>56.049</td>\n",
       "      <td>-4.090</td>\n",
       "      <td>24.158</td>\n",
       "      <td>-1.469</td>\n",
       "      <td>2.125</td>\n",
       "      <td>2.809</td>\n",
       "      <td>-15.527</td>\n",
       "      <td>17.195</td>\n",
       "      <td>0.380</td>\n",
       "      <td>2.669</td>\n",
       "      <td>3.243</td>\n",
       "      <td>34.571</td>\n",
       "      <td>-13.052</td>\n",
       "      <td>-10.425</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.496</td>\n",
       "      <td>59.793</td>\n",
       "      <td>0.866</td>\n",
       "      <td>5.422</td>\n",
       "      <td>-0.873</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-1.642</td>\n",
       "      <td>-0.886</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.143</td>\n",
       "      <td>48.883</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-1.427</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.400</td>\n",
       "      <td>1.258</td>\n",
       "      <td>1.162</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.631</td>\n",
       "      <td>-2.849</td>\n",
       "      <td>1.540</td>\n",
       "      <td>2.193</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.084</td>\n",
       "      <td>1.196</td>\n",
       "      <td>0.853</td>\n",
       "      <td>1.047</td>\n",
       "      <td>1.368</td>\n",
       "      <td>1.313</td>\n",
       "      <td>1.366</td>\n",
       "      <td>1.197</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>1.262</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.656</td>\n",
       "      <td>1.412</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>1.233</td>\n",
       "      <td>1.662</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>2.885</td>\n",
       "      <td>1.594</td>\n",
       "      <td>-2.271</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>1.197</td>\n",
       "      <td>-0.733</td>\n",
       "      <td>1.681</td>\n",
       "      <td>1.905</td>\n",
       "      <td>1.231</td>\n",
       "      <td>1.856</td>\n",
       "      <td>1.702</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>2.084</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-1.562</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-2.799</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.947</td>\n",
       "      <td>2.799</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.221</td>\n",
       "      <td>1.626</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.850</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>0.239</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.352</td>\n",
       "      <td>1.037</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>1.466</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>2.736</td>\n",
       "      <td>1.960</td>\n",
       "      <td>1.389</td>\n",
       "      <td>2.813</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>2.018</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-2.090</td>\n",
       "      <td>-1.007</td>\n",
       "      <td>1.771</td>\n",
       "      <td>2.509</td>\n",
       "      <td>1.142</td>\n",
       "      <td>1.387</td>\n",
       "      <td>1.555</td>\n",
       "      <td>1.469</td>\n",
       "      <td>1.255</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>1.417</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.745</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.937</td>\n",
       "      <td>1.208</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>1.673</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-3.128</td>\n",
       "      <td>1.098</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.948</td>\n",
       "      <td>1.532</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.267</td>\n",
       "      <td>1.209</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>1.659</td>\n",
       "      <td>-1.767</td>\n",
       "      <td>-3.729</td>\n",
       "      <td>-2.447</td>\n",
       "      <td>-4.885</td>\n",
       "      <td>1.497</td>\n",
       "      <td>1.384</td>\n",
       "      <td>7.265</td>\n",
       "      <td>-1.345</td>\n",
       "      <td>-5.884</td>\n",
       "      <td>1.380</td>\n",
       "      <td>-3.705</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-3.153</td>\n",
       "      <td>-2.196</td>\n",
       "      <td>1.297</td>\n",
       "      <td>-4.394</td>\n",
       "      <td>1.734</td>\n",
       "      <td>1.136</td>\n",
       "      <td>1.469</td>\n",
       "      <td>1.376</td>\n",
       "      <td>1.199</td>\n",
       "      <td>-1.554</td>\n",
       "      <td>1.361</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-12.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-65.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.486</td>\n",
       "      <td>-49.433</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-94.376</td>\n",
       "      <td>8.670</td>\n",
       "      <td>6.765</td>\n",
       "      <td>-4.743</td>\n",
       "      <td>-25.510</td>\n",
       "      <td>-1.817</td>\n",
       "      <td>3.140</td>\n",
       "      <td>-18.912</td>\n",
       "      <td>-2.229</td>\n",
       "      <td>6.588</td>\n",
       "      <td>-50.181</td>\n",
       "      <td>9.196</td>\n",
       "      <td>31.840</td>\n",
       "      <td>-33.228</td>\n",
       "      <td>-2.832</td>\n",
       "      <td>-5.295</td>\n",
       "      <td>-10.468</td>\n",
       "      <td>1.439</td>\n",
       "      <td>8.846</td>\n",
       "      <td>5.640</td>\n",
       "      <td>-3.431</td>\n",
       "      <td>-88.568</td>\n",
       "      <td>-2.550</td>\n",
       "      <td>4.553</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>1.763</td>\n",
       "      <td>-15.322</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>2.282</td>\n",
       "      <td>-3.172</td>\n",
       "      <td>-1.423</td>\n",
       "      <td>-65.255</td>\n",
       "      <td>1.998</td>\n",
       "      <td>42.509</td>\n",
       "      <td>-47.248</td>\n",
       "      <td>-1.223</td>\n",
       "      <td>1.464</td>\n",
       "      <td>-1.914</td>\n",
       "      <td>2.219</td>\n",
       "      <td>0.490</td>\n",
       "      <td>1.153</td>\n",
       "      <td>-2.088</td>\n",
       "      <td>-93.193</td>\n",
       "      <td>1.166</td>\n",
       "      <td>0.795</td>\n",
       "      <td>1.532</td>\n",
       "      <td>1.355</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>1.501</td>\n",
       "      <td>0.681</td>\n",
       "      <td>1.212</td>\n",
       "      <td>1.746</td>\n",
       "      <td>3.244</td>\n",
       "      <td>0.171</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.157</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.466</td>\n",
       "      <td>-1.565</td>\n",
       "      <td>4.654</td>\n",
       "      <td>0.177</td>\n",
       "      <td>4.203</td>\n",
       "      <td>4.187</td>\n",
       "      <td>1.141</td>\n",
       "      <td>1.357</td>\n",
       "      <td>1.056</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.414</td>\n",
       "      <td>2.188</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>0.932</td>\n",
       "      <td>3.396</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-2.720</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.931</td>\n",
       "      <td>2.028</td>\n",
       "      <td>1.690</td>\n",
       "      <td>1.520</td>\n",
       "      <td>-1.063</td>\n",
       "      <td>0.210</td>\n",
       "      <td>2.473</td>\n",
       "      <td>1.432</td>\n",
       "      <td>1.849</td>\n",
       "      <td>1.921</td>\n",
       "      <td>0.841</td>\n",
       "      <td>4.461</td>\n",
       "      <td>2.233</td>\n",
       "      <td>2.420</td>\n",
       "      <td>0.104</td>\n",
       "      <td>1.501</td>\n",
       "      <td>-1.583</td>\n",
       "      <td>1.066</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-3.038</td>\n",
       "      <td>1.631</td>\n",
       "      <td>1.452</td>\n",
       "      <td>2.111</td>\n",
       "      <td>2.350</td>\n",
       "      <td>2.030</td>\n",
       "      <td>1.170</td>\n",
       "      <td>1.273</td>\n",
       "      <td>0.637</td>\n",
       "      <td>1.003</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>1.067</td>\n",
       "      <td>1.117</td>\n",
       "      <td>0.774</td>\n",
       "      <td>1.289</td>\n",
       "      <td>1.305</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.861</td>\n",
       "      <td>-3.704</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-1.818</td>\n",
       "      <td>1.021</td>\n",
       "      <td>0.648</td>\n",
       "      <td>1.291</td>\n",
       "      <td>0.740</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>-1.937</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-1.109</td>\n",
       "      <td>2.888</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.226</td>\n",
       "      <td>1.603</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-1.157</td>\n",
       "      <td>1.328</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-2.753</td>\n",
       "      <td>-2.320</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-1.534</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-2.710</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-2.060</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-1.639</td>\n",
       "      <td>-1.576</td>\n",
       "      <td>-1.659</td>\n",
       "      <td>-1.827</td>\n",
       "      <td>1.520</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-1.348</td>\n",
       "      <td>-1.481</td>\n",
       "      <td>-1.486</td>\n",
       "      <td>-1.330</td>\n",
       "      <td>-7.100</td>\n",
       "      <td>-7.587</td>\n",
       "      <td>-1.703</td>\n",
       "      <td>-2.174</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-1.640</td>\n",
       "      <td>5.864</td>\n",
       "      <td>-7.173</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-10.168</td>\n",
       "      <td>-8.397</td>\n",
       "      <td>-1.455</td>\n",
       "      <td>-1.274</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>-1.741</td>\n",
       "      <td>-0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-44.525</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.848</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>73.247</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-72.924</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.084</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-45.513</td>\n",
       "      <td>-5.211</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-25.198</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>29.594</td>\n",
       "      <td>11.857</td>\n",
       "      <td>-1.610</td>\n",
       "      <td>43.286</td>\n",
       "      <td>15.061</td>\n",
       "      <td>12.504</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-7.601</td>\n",
       "      <td>-1.540</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-9.229</td>\n",
       "      <td>0.920</td>\n",
       "      <td>-42.166</td>\n",
       "      <td>-21.851</td>\n",
       "      <td>7.600</td>\n",
       "      <td>13.551</td>\n",
       "      <td>-49.322</td>\n",
       "      <td>5.215</td>\n",
       "      <td>-7.681</td>\n",
       "      <td>2.811</td>\n",
       "      <td>-1.266</td>\n",
       "      <td>-1.579</td>\n",
       "      <td>3.899</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>73.276</td>\n",
       "      <td>-1.427</td>\n",
       "      <td>2.037</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>-1.944</td>\n",
       "      <td>2.123</td>\n",
       "      <td>6.926</td>\n",
       "      <td>-3.530</td>\n",
       "      <td>-71.714</td>\n",
       "      <td>-1.780</td>\n",
       "      <td>20.612</td>\n",
       "      <td>-2.039</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.244</td>\n",
       "      <td>1.052</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.524</td>\n",
       "      <td>1.116</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.953</td>\n",
       "      <td>1.344</td>\n",
       "      <td>1.351</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.795</td>\n",
       "      <td>1.192</td>\n",
       "      <td>-2.061</td>\n",
       "      <td>1.242</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>1.162</td>\n",
       "      <td>0.183</td>\n",
       "      <td>3.379</td>\n",
       "      <td>1.060</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.519</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>1.266</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>-1.340</td>\n",
       "      <td>-1.008</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>2.584</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>-1.318</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-1.237</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.827</td>\n",
       "      <td>2.524</td>\n",
       "      <td>-1.079</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-1.603</td>\n",
       "      <td>-1.168</td>\n",
       "      <td>-1.395</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.210</td>\n",
       "      <td>2.432</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>3.090</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-3.865</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.451</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.406</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.680</td>\n",
       "      <td>1.603</td>\n",
       "      <td>1.428</td>\n",
       "      <td>1.686</td>\n",
       "      <td>1.603</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-2.382</td>\n",
       "      <td>1.547</td>\n",
       "      <td>1.505</td>\n",
       "      <td>1.496</td>\n",
       "      <td>1.831</td>\n",
       "      <td>1.700</td>\n",
       "      <td>1.514</td>\n",
       "      <td>1.482</td>\n",
       "      <td>1.649</td>\n",
       "      <td>-2.354</td>\n",
       "      <td>1.569</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>1.759</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.090</td>\n",
       "      <td>1.877</td>\n",
       "      <td>1.537</td>\n",
       "      <td>1.713</td>\n",
       "      <td>1.429</td>\n",
       "      <td>1.612</td>\n",
       "      <td>-1.649</td>\n",
       "      <td>-1.347</td>\n",
       "      <td>-2.035</td>\n",
       "      <td>-1.723</td>\n",
       "      <td>0.118</td>\n",
       "      <td>5.695</td>\n",
       "      <td>-1.001</td>\n",
       "      <td>-1.712</td>\n",
       "      <td>-1.506</td>\n",
       "      <td>-1.469</td>\n",
       "      <td>-1.640</td>\n",
       "      <td>-1.554</td>\n",
       "      <td>-1.547</td>\n",
       "      <td>-1.503</td>\n",
       "      <td>6.767</td>\n",
       "      <td>-1.527</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-1.238</td>\n",
       "      <td>0.212</td>\n",
       "      <td>7.205</td>\n",
       "      <td>-1.590</td>\n",
       "      <td>-1.719</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>-1.062</td>\n",
       "      <td>-1.815</td>\n",
       "      <td>-1.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-37.519</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-17.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.264</td>\n",
       "      <td>-59.508</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-45.018</td>\n",
       "      <td>-20.347</td>\n",
       "      <td>-1.388</td>\n",
       "      <td>0.502</td>\n",
       "      <td>4.195</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>2.876</td>\n",
       "      <td>-31.795</td>\n",
       "      <td>1.708</td>\n",
       "      <td>-14.141</td>\n",
       "      <td>14.419</td>\n",
       "      <td>-18.882</td>\n",
       "      <td>-1.992</td>\n",
       "      <td>15.673</td>\n",
       "      <td>-34.850</td>\n",
       "      <td>14.892</td>\n",
       "      <td>-45.528</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-1.280</td>\n",
       "      <td>3.408</td>\n",
       "      <td>-2.480</td>\n",
       "      <td>1.228</td>\n",
       "      <td>-39.011</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>3.028</td>\n",
       "      <td>4.682</td>\n",
       "      <td>-1.904</td>\n",
       "      <td>-17.333</td>\n",
       "      <td>-2.254</td>\n",
       "      <td>2.494</td>\n",
       "      <td>-2.364</td>\n",
       "      <td>30.749</td>\n",
       "      <td>-64.090</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>1.519</td>\n",
       "      <td>-3.671</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-43.416</td>\n",
       "      <td>1.578</td>\n",
       "      <td>1.231</td>\n",
       "      <td>1.926</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.879</td>\n",
       "      <td>1.159</td>\n",
       "      <td>1.662</td>\n",
       "      <td>2.072</td>\n",
       "      <td>2.098</td>\n",
       "      <td>1.466</td>\n",
       "      <td>1.511</td>\n",
       "      <td>1.932</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>1.961</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>1.992</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>1.838</td>\n",
       "      <td>1.575</td>\n",
       "      <td>1.748</td>\n",
       "      <td>1.478</td>\n",
       "      <td>1.187</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.036</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.795</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>0.092</td>\n",
       "      <td>1.385</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.272</td>\n",
       "      <td>1.033</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.798</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>1.131</td>\n",
       "      <td>2.466</td>\n",
       "      <td>1.246</td>\n",
       "      <td>-1.590</td>\n",
       "      <td>2.735</td>\n",
       "      <td>1.067</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.482</td>\n",
       "      <td>1.309</td>\n",
       "      <td>0.640</td>\n",
       "      <td>1.023</td>\n",
       "      <td>1.111</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.965</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>0.260</td>\n",
       "      <td>1.517</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.307</td>\n",
       "      <td>1.401</td>\n",
       "      <td>1.316</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.919</td>\n",
       "      <td>-2.588</td>\n",
       "      <td>1.518</td>\n",
       "      <td>1.397</td>\n",
       "      <td>-2.687</td>\n",
       "      <td>1.563</td>\n",
       "      <td>0.537</td>\n",
       "      <td>1.159</td>\n",
       "      <td>1.407</td>\n",
       "      <td>1.109</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.375</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.520</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>1.251</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.854</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.831</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.798</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.863</td>\n",
       "      <td>-1.414</td>\n",
       "      <td>-1.292</td>\n",
       "      <td>-1.598</td>\n",
       "      <td>-1.448</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-1.187</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>-1.425</td>\n",
       "      <td>-1.201</td>\n",
       "      <td>-1.482</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>-1.546</td>\n",
       "      <td>-1.356</td>\n",
       "      <td>0.916</td>\n",
       "      <td>-1.387</td>\n",
       "      <td>4.768</td>\n",
       "      <td>-1.056</td>\n",
       "      <td>3.313</td>\n",
       "      <td>5.599</td>\n",
       "      <td>-1.226</td>\n",
       "      <td>-1.335</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>-1.233</td>\n",
       "      <td>-1.488</td>\n",
       "      <td>-0.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-99.955</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-75.927</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-13.893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.479</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-53.454</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-71.040</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-76.366</td>\n",
       "      <td>-1.189</td>\n",
       "      <td>-1.849</td>\n",
       "      <td>-16.278</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-25.738</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>1.307</td>\n",
       "      <td>1.024</td>\n",
       "      <td>4.411</td>\n",
       "      <td>-22.975</td>\n",
       "      <td>1.884</td>\n",
       "      <td>3.627</td>\n",
       "      <td>-1.328</td>\n",
       "      <td>-11.064</td>\n",
       "      <td>2.491</td>\n",
       "      <td>-100.160</td>\n",
       "      <td>-17.947</td>\n",
       "      <td>8.768</td>\n",
       "      <td>1.197</td>\n",
       "      <td>5.207</td>\n",
       "      <td>-76.890</td>\n",
       "      <td>2.414</td>\n",
       "      <td>13.571</td>\n",
       "      <td>0.872</td>\n",
       "      <td>-15.684</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-8.431</td>\n",
       "      <td>2.128</td>\n",
       "      <td>5.451</td>\n",
       "      <td>11.961</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.936</td>\n",
       "      <td>-18.912</td>\n",
       "      <td>-56.936</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.449</td>\n",
       "      <td>1.102</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.847</td>\n",
       "      <td>1.192</td>\n",
       "      <td>1.226</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.699</td>\n",
       "      <td>1.074</td>\n",
       "      <td>1.893</td>\n",
       "      <td>1.123</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1.079</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.053</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-2.124</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.619</td>\n",
       "      <td>1.161</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.639</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.781</td>\n",
       "      <td>1.098</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.825</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.562</td>\n",
       "      <td>2.447</td>\n",
       "      <td>2.506</td>\n",
       "      <td>1.847</td>\n",
       "      <td>2.588</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>2.414</td>\n",
       "      <td>2.332</td>\n",
       "      <td>2.077</td>\n",
       "      <td>2.638</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.445</td>\n",
       "      <td>1.707</td>\n",
       "      <td>2.107</td>\n",
       "      <td>0.153</td>\n",
       "      <td>2.359</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>2.034</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-1.374</td>\n",
       "      <td>1.903</td>\n",
       "      <td>2.243</td>\n",
       "      <td>1.935</td>\n",
       "      <td>2.547</td>\n",
       "      <td>1.926</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.406</td>\n",
       "      <td>1.353</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.998</td>\n",
       "      <td>1.896</td>\n",
       "      <td>2.034</td>\n",
       "      <td>1.988</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>2.026</td>\n",
       "      <td>1.903</td>\n",
       "      <td>1.915</td>\n",
       "      <td>2.169</td>\n",
       "      <td>2.065</td>\n",
       "      <td>1.956</td>\n",
       "      <td>1.909</td>\n",
       "      <td>2.028</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>1.967</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>2.002</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>2.045</td>\n",
       "      <td>1.912</td>\n",
       "      <td>2.093</td>\n",
       "      <td>1.939</td>\n",
       "      <td>1.987</td>\n",
       "      <td>-2.012</td>\n",
       "      <td>-1.936</td>\n",
       "      <td>-2.002</td>\n",
       "      <td>-2.070</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-1.577</td>\n",
       "      <td>-2.038</td>\n",
       "      <td>-1.850</td>\n",
       "      <td>-1.731</td>\n",
       "      <td>-1.851</td>\n",
       "      <td>-2.097</td>\n",
       "      <td>-1.884</td>\n",
       "      <td>-1.752</td>\n",
       "      <td>4.230</td>\n",
       "      <td>-1.849</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-1.297</td>\n",
       "      <td>2.102</td>\n",
       "      <td>-2.649</td>\n",
       "      <td>-1.463</td>\n",
       "      <td>-1.927</td>\n",
       "      <td>-1.535</td>\n",
       "      <td>-1.750</td>\n",
       "      <td>-2.057</td>\n",
       "      <td>-1.479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "123  1373158606       -10.130         0.000         0.000         0.000   \n",
       "274  1373158606         0.000         0.000         0.000         0.000   \n",
       "600  1373158606       -44.525         0.000        -5.848         0.000   \n",
       "519  1373158606       -37.519         0.000         0.000         0.000   \n",
       "225  1373158606       -99.955         0.000         0.000         0.000   \n",
       "\n",
       "     00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "123         0.000         0.000         0.000         0.000        60.792   \n",
       "274       -12.447         0.000         0.000         0.000         0.000   \n",
       "600         0.000         0.000         0.000         0.000        73.247   \n",
       "519         0.000         0.000         0.000         0.000       -17.096   \n",
       "225         0.000       -75.927         0.000         0.000         0.000   \n",
       "\n",
       "     00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "123         0.000         4.034         0.000         0.000         0.000   \n",
       "274       -65.044         0.000        47.486       -49.433         0.000   \n",
       "600         0.000         0.000         0.000         0.000         0.000   \n",
       "519         0.000         0.000         0.000        30.264       -59.508   \n",
       "225       -13.893         0.000         2.479         0.000         0.000   \n",
       "\n",
       "     02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "123        -0.661         0.000         0.000         0.000         0.000   \n",
       "274         0.000         0.000         0.000         0.000         0.000   \n",
       "600         0.000         0.000         0.000       -72.924         0.000   \n",
       "519         0.000         0.000         0.000         0.000         0.000   \n",
       "225         0.000         0.000         0.000         0.000         0.000   \n",
       "\n",
       "     11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "123        49.729         0.000             -18.850               1.493   \n",
       "274         0.000       -94.376               8.670               6.765   \n",
       "600        26.084         0.000             -45.513              -5.211   \n",
       "519         0.000       -45.018             -20.347              -1.388   \n",
       "225       -53.454         0.000             -71.040              -0.124   \n",
       "\n",
       "     00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "123              -0.071               6.193              -1.886   \n",
       "274              -4.743             -25.510              -1.817   \n",
       "600              -0.174             -25.198              -0.130   \n",
       "519               0.502               4.195              -0.152   \n",
       "225              -0.098             -76.366              -1.189   \n",
       "\n",
       "     00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "123              -5.492               3.206               0.374   \n",
       "274               3.140             -18.912              -2.229   \n",
       "600              29.594              11.857              -1.610   \n",
       "519               2.876             -31.795               1.708   \n",
       "225              -1.849             -16.278               0.132   \n",
       "\n",
       "     00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "123              56.049              -4.090              24.158   \n",
       "274               6.588             -50.181               9.196   \n",
       "600              43.286              15.061              12.504   \n",
       "519             -14.141              14.419             -18.882   \n",
       "225               0.091               0.206             -25.738   \n",
       "\n",
       "     01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "123              -1.469               2.125               2.809   \n",
       "274              31.840             -33.228              -2.832   \n",
       "600               0.177              -7.601              -1.540   \n",
       "519              -1.992              15.673             -34.850   \n",
       "225              -0.379               1.307               1.024   \n",
       "\n",
       "     02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "123             -15.527              17.195               0.380   \n",
       "274              -5.295             -10.468               1.439   \n",
       "600               0.199              -9.229               0.920   \n",
       "519              14.892             -45.528               0.030   \n",
       "225               4.411             -22.975               1.884   \n",
       "\n",
       "     10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "123               2.669               3.243              34.571   \n",
       "274               8.846               5.640              -3.431   \n",
       "600             -42.166             -21.851               7.600   \n",
       "519              -1.280               3.408              -2.480   \n",
       "225               3.627              -1.328             -11.064   \n",
       "\n",
       "     20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "123             -13.052             -10.425              -0.090   \n",
       "274             -88.568              -2.550               4.553   \n",
       "600              13.551             -49.322               5.215   \n",
       "519               1.228             -39.011               0.508   \n",
       "225               2.491            -100.160             -17.947   \n",
       "\n",
       "     00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "123               0.973               0.441              -0.868   \n",
       "274              -0.730               1.763             -15.322   \n",
       "600              -7.681               2.811              -1.266   \n",
       "519               0.019              -0.701              -0.531   \n",
       "225               8.768               1.197               5.207   \n",
       "\n",
       "     00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "123              -0.072               0.022               0.496   \n",
       "274              -0.688               2.282              -3.172   \n",
       "600              -1.579               3.899              -1.500   \n",
       "519               3.028               4.682              -1.904   \n",
       "225             -76.890               2.414              13.571   \n",
       "\n",
       "     00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "123              59.793               0.866               5.422   \n",
       "274              -1.423             -65.255               1.998   \n",
       "600              73.276              -1.427               2.037   \n",
       "519             -17.333              -2.254               2.494   \n",
       "225               0.872             -15.684               0.118   \n",
       "\n",
       "     01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "123              -0.873               0.391              -1.642   \n",
       "274              42.509             -47.248              -1.223   \n",
       "600              -0.961              -1.324              -1.944   \n",
       "519              -2.364              30.749             -64.090   \n",
       "225              -0.049              -8.431               2.128   \n",
       "\n",
       "     02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "123              -0.886              -0.095              -0.575   \n",
       "274               1.464              -1.914               2.219   \n",
       "600               2.123               6.926              -3.530   \n",
       "519              -0.155              -0.444               1.519   \n",
       "225               5.451              11.961              -0.262   \n",
       "\n",
       "     10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "123               0.060               0.143              48.883   \n",
       "274               0.490               1.153              -2.088   \n",
       "600             -71.714              -1.780              20.612   \n",
       "519              -3.671               0.441              -0.140   \n",
       "225               0.936             -18.912             -56.936   \n",
       "\n",
       "     20000-lstsq_target   wb_0   wb_1  wb_2  wb_3   wb_4   wb_5  wb_6   wb_7  \\\n",
       "123               0.450 -0.138 -1.427 0.022 0.400  1.258  1.162 0.443 -0.631   \n",
       "274             -93.193  1.166  0.795 1.532 1.355 -0.172 -0.011 1.501  0.681   \n",
       "600              -2.039  0.874  0.550 1.244 1.052  0.057 -0.524 1.116  0.516   \n",
       "519             -43.416  1.578  1.231 1.926 1.750  0.040  0.012 1.879  1.159   \n",
       "225               0.781  0.753  0.449 1.102 0.924  0.060 -0.015 1.018  0.402   \n",
       "\n",
       "      wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "123 -2.849 1.540  2.193 -0.223 -0.330  0.084  1.196  0.853  1.047  1.368   \n",
       "274  1.212 1.746  3.244  0.171  1.065  1.157  0.007  1.466 -1.565  4.654   \n",
       "600  0.953 1.344  1.351  0.756  0.795  1.192 -2.061  1.242 -0.045  1.162   \n",
       "519  1.662 2.072  2.098  1.466  1.511  1.932 -0.374  1.961 -0.359  1.992   \n",
       "225  0.847 1.192  1.226  0.647  0.699  1.074  1.893  1.123 -0.044  0.988   \n",
       "\n",
       "     wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "123  1.313  1.366  1.197 -0.144  1.262 -0.245 -0.182 -0.656  1.412 -0.918   \n",
       "274  0.177  4.203  4.187  1.141  1.357  1.056  0.744  0.381  0.384  0.092   \n",
       "600  0.183  3.379  1.060  0.914  0.996  0.758  0.519 -0.288 -0.094 -0.531   \n",
       "519  0.153 -0.145  1.838  1.575  1.748  1.478  1.187  0.920  1.036  0.776   \n",
       "225  1.079  0.991  0.830  0.794  0.882  0.663  0.402  0.559  0.669  0.427   \n",
       "\n",
       "     wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "123 -0.410  1.233  1.662  0.342 -0.125  2.885  1.594 -2.271 -0.414  0.292   \n",
       "274  0.812  0.117  0.037  0.656  0.590  0.520  0.414  2.188  0.081  0.211   \n",
       "600 -0.421 -0.349  0.621  0.191  0.025  0.096 -0.304 -0.539 -0.466 -0.207   \n",
       "519  0.795 -0.336  0.092  1.385  1.199  1.272  1.033  0.718  0.685  0.936   \n",
       "225  0.430 -0.349  0.050  1.053  0.828  0.917  0.700  0.378  0.320  0.587   \n",
       "\n",
       "     wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "123 -0.784  1.197 -0.733  1.681  1.905  1.231  1.856  1.702 -0.586  2.084   \n",
       "274 -0.027 -0.381  0.932  3.396 -0.152 -0.277  0.317 -2.720  0.338  0.754   \n",
       "600 -0.455 -0.357 -0.090  0.134 -0.331 -0.275  1.266 -0.499 -0.226  0.198   \n",
       "519  0.798 -0.162  1.131  2.466  1.246 -1.590  2.735  1.067  0.982  1.482   \n",
       "225  0.462 -2.124  0.779  0.145  0.985  0.866  0.751  0.793  0.619  1.161   \n",
       "\n",
       "     wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "123  0.089 -0.679 -0.169 -1.562  0.090 -2.799  0.217  0.947  2.799 -0.509   \n",
       "274  0.631  0.005  1.931  2.028  1.690  1.520 -1.063  0.210  2.473  1.432   \n",
       "600  0.183 -0.547 -0.942 -0.688 -1.340 -1.008 -0.441  2.584 -0.468 -1.318   \n",
       "519  1.309  0.640  1.023  1.111  0.748  0.965 -0.447  0.260  1.517  0.541   \n",
       "225  0.954  0.284  0.437  0.558  0.167  0.376 -0.439  0.202  0.899  0.007   \n",
       "\n",
       "     wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "123  0.223  0.519  0.567  0.221  1.626  0.366  0.850 -1.911  0.239  1.004   \n",
       "274  1.849  1.921  0.841  4.461  2.233  2.420  0.104  1.501 -1.583  1.066   \n",
       "600 -0.965 -1.237 -0.785 -0.453 -0.539 -0.827  2.524 -1.079 -0.376 -1.214   \n",
       "519  0.941  0.989  1.307  1.401  1.316  1.248  0.365  0.919 -2.588  1.518   \n",
       "225  0.368  0.350  0.687  0.823  0.753  0.639 -0.338  0.321 -0.379  0.781   \n",
       "\n",
       "     wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "123  0.584  0.352  1.037 -0.268  0.738  0.122  0.176 -0.408  1.466 -0.887   \n",
       "274 -0.047 -3.038  1.631  1.452  2.111  2.350  2.030  1.170  1.273  0.637   \n",
       "600 -0.039 -1.603 -1.168 -1.395 -0.983 -0.416 -0.796  0.314  0.439 -0.288   \n",
       "519  1.397 -2.687  1.563  0.537  1.159  1.407  1.109  0.244  0.375 -0.336   \n",
       "225  1.098  0.156  0.825 -0.015  0.542  0.827  0.562  2.447  2.506  1.847   \n",
       "\n",
       "     wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "123  2.736  1.960  1.389  2.813  0.061 -0.308  2.018 -0.970  0.114 -2.090   \n",
       "274  1.003  0.271 -0.251  1.067  1.117  0.774  1.289  1.305  0.102  0.425   \n",
       "600  0.452  0.210  2.432  0.237  0.262 -0.023  0.359 -0.074  0.368 -0.344   \n",
       "519  0.388  0.228 -0.166  0.097  0.244 -0.100  0.264 -0.183  0.278 -0.440   \n",
       "225  2.588  0.218 -0.242  2.414  2.332  2.077  2.638  2.107  2.445  1.707   \n",
       "\n",
       "     wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "123 -1.007  1.771  2.509  1.142  1.387  1.555  1.469  1.255 -0.396  1.417   \n",
       "274  0.157  0.038  0.861 -3.704  0.410 -0.060 -0.169 -1.818  1.021  0.648   \n",
       "600 -0.075  3.090  0.203 -0.459 -0.463 -0.062 -3.865 -0.521  0.141 -0.288   \n",
       "519 -0.178  0.825  0.121  1.520 -0.644 -0.773  1.251 -0.743  0.115 -0.402   \n",
       "225  2.107  0.153  2.359 -0.459  2.034 -0.372 -1.374  1.903  2.243  1.935   \n",
       "\n",
       "     wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "123  0.104 -1.002   0.218   0.476   0.745  -0.258   0.638   0.369   0.072   \n",
       "274  1.291  0.740  -0.514   0.086   0.111  -0.931   0.411  -0.272  -0.533   \n",
       "600  0.451 -0.134  -0.074   0.277   0.328  -0.201  -0.059  -0.042  -0.030   \n",
       "519  0.341 -0.192  -0.164   0.198   0.254  -0.290  -0.045  -0.356  -0.170   \n",
       "225  2.547  1.926  -0.150   0.187   0.299  -0.282  -0.053  -0.271  -0.113   \n",
       "\n",
       "     wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "123   0.629  -0.147   0.209  -0.120  -0.528   0.021   0.518   0.751   0.272   \n",
       "274   0.422  -0.300  -0.683  -1.937  -0.018   0.020  -0.156   0.037  -1.109   \n",
       "600   0.549   0.153  -0.099  -0.145  -0.176   0.356   0.154   0.070  -0.103   \n",
       "519   0.499   0.062  -0.222  -0.267  -0.275   0.254   0.038   0.854  -0.202   \n",
       "225   0.479   0.086  -0.167  -0.200  -0.264   0.299   0.102   0.262  -0.173   \n",
       "\n",
       "     wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "123   0.640   0.926   0.516   0.570   0.731   0.652   0.601  -0.356   0.937   \n",
       "274   2.888  -0.492  -0.093   0.226   1.603   0.439   0.111  -0.222   0.506   \n",
       "600  -0.007   0.406  -0.098  -0.032   0.285   0.604   0.399   0.034   0.680   \n",
       "519   0.185   0.188   0.333  -0.392   0.037   0.545   0.270  -0.079   0.600   \n",
       "225   0.005   0.406   1.353  -0.482   0.251   0.544   0.355  -0.067   0.625   \n",
       "\n",
       "     wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "123   1.208  -0.022   1.673   0.186   0.268   0.245  -3.128   1.098   0.114   \n",
       "274  -0.253  -0.133  -0.031  -1.157   1.328  -0.184  -0.357  -0.037  -0.398   \n",
       "600   1.603   1.428   1.686   1.603  -0.180  -2.382   1.547   1.505   1.496   \n",
       "519   0.839   0.757   0.897   0.831  -0.178  -0.266   0.829   0.775   0.769   \n",
       "225   1.998   1.896   2.034   1.988  -0.176  -0.188   2.026   1.903   1.915   \n",
       "\n",
       "     wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "123   0.297   0.515  -0.155   0.948   1.532   0.247   0.301   0.346   0.252   \n",
       "274  -0.235  -2.753  -2.320  -0.288  -1.534  -0.170  -0.984   0.457  -2.710   \n",
       "600   1.831   1.700   1.514   1.482   1.649  -2.354   1.569  -0.093   1.759   \n",
       "519   0.950   0.889   0.813   0.783   0.849   0.183   0.798  -0.507   0.701   \n",
       "225   2.169   2.065   1.956   1.909   2.028  -0.006   1.967  -0.083   2.002   \n",
       "\n",
       "     wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "123   0.255   0.261   0.267   1.209   0.083  -0.250   1.659  -1.767  -3.729   \n",
       "274  -0.127  -0.946  -2.060  -0.102  -0.091  -0.151   0.017  -1.639  -1.576   \n",
       "600  -0.122   0.090   1.877   1.537   1.713   1.429   1.612  -1.649  -1.347   \n",
       "519   0.351  -0.526   0.765   0.776   0.894   0.776   0.863  -1.414  -1.292   \n",
       "225  -0.493  -0.175   2.045   1.912   2.093   1.939   1.987  -2.012  -1.936   \n",
       "\n",
       "     wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "123  -2.447  -4.885   1.497   1.384   7.265  -1.345  -5.884   1.380  -3.705   \n",
       "274  -1.659  -1.827   1.520   0.158  -1.348  -1.481  -1.486  -1.330  -7.100   \n",
       "600  -2.035  -1.723   0.118   5.695  -1.001  -1.712  -1.506  -1.469  -1.640   \n",
       "519  -1.598  -1.448   0.121   0.688  -1.187  -1.343  -1.425  -1.201  -1.482   \n",
       "225  -2.002  -2.070   0.120   0.157  -1.577  -2.038  -1.850  -1.731  -1.851   \n",
       "\n",
       "     wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "123  -0.277  -3.153  -2.196   1.297  -4.394   1.734   1.136   1.469   1.376   \n",
       "274  -7.587  -1.703  -2.174   0.085  -1.640   5.864  -7.173   0.211 -10.168   \n",
       "600  -1.554  -1.547  -1.503   6.767  -1.527   0.392  -1.238   0.212   7.205   \n",
       "519  -1.500  -1.546  -1.356   0.916  -1.387   4.768  -1.056   3.313   5.599   \n",
       "225  -2.097  -1.884  -1.752   4.230  -1.849   0.395  -1.297   2.102  -2.649   \n",
       "\n",
       "     wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "123   1.199  -1.554   1.361  -0.131  -2.514   0.358  \n",
       "274  -8.397  -1.455  -1.274  -1.480  -1.741  -0.082  \n",
       "600  -1.590  -1.719  -1.257  -1.062  -1.815  -1.098  \n",
       "519  -1.226  -1.335  -1.200  -1.233  -1.488  -0.621  \n",
       "225  -1.463  -1.927  -1.535  -1.750  -2.057  -1.479  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-84.503</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.395</td>\n",
       "      <td>-2.612</td>\n",
       "      <td>0.000</td>\n",
       "      <td>83.254</td>\n",
       "      <td>-22.327</td>\n",
       "      <td>-1.452</td>\n",
       "      <td>-1.900</td>\n",
       "      <td>0.455</td>\n",
       "      <td>1.509</td>\n",
       "      <td>-83.291</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>2.239</td>\n",
       "      <td>-0.792</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>3.823</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>1.440</td>\n",
       "      <td>-1.575</td>\n",
       "      <td>4.999</td>\n",
       "      <td>29.598</td>\n",
       "      <td>-5.345</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>77.650</td>\n",
       "      <td>-22.542</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.573</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>-1.568</td>\n",
       "      <td>-86.010</td>\n",
       "      <td>1.671</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-1.119</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.928</td>\n",
       "      <td>30.397</td>\n",
       "      <td>-2.065</td>\n",
       "      <td>0.369</td>\n",
       "      <td>82.036</td>\n",
       "      <td>-22.947</td>\n",
       "      <td>-0.722</td>\n",
       "      <td>-1.496</td>\n",
       "      <td>3.707</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.133</td>\n",
       "      <td>1.848</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-3.165</td>\n",
       "      <td>-2.650</td>\n",
       "      <td>0.302</td>\n",
       "      <td>2.608</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.348</td>\n",
       "      <td>2.006</td>\n",
       "      <td>0.516</td>\n",
       "      <td>1.706</td>\n",
       "      <td>2.128</td>\n",
       "      <td>2.070</td>\n",
       "      <td>2.030</td>\n",
       "      <td>1.995</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>2.052</td>\n",
       "      <td>-1.990</td>\n",
       "      <td>-1.294</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-3.022</td>\n",
       "      <td>-1.034</td>\n",
       "      <td>0.833</td>\n",
       "      <td>1.286</td>\n",
       "      <td>0.297</td>\n",
       "      <td>1.758</td>\n",
       "      <td>1.230</td>\n",
       "      <td>-1.231</td>\n",
       "      <td>-2.178</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-1.148</td>\n",
       "      <td>0.909</td>\n",
       "      <td>-1.138</td>\n",
       "      <td>1.261</td>\n",
       "      <td>2.585</td>\n",
       "      <td>0.867</td>\n",
       "      <td>2.424</td>\n",
       "      <td>1.697</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>2.215</td>\n",
       "      <td>1.227</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.658</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.532</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.351</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.850</td>\n",
       "      <td>0.423</td>\n",
       "      <td>1.130</td>\n",
       "      <td>3.636</td>\n",
       "      <td>-1.066</td>\n",
       "      <td>1.769</td>\n",
       "      <td>1.461</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.859</td>\n",
       "      <td>1.388</td>\n",
       "      <td>1.032</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>1.575</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>1.995</td>\n",
       "      <td>-2.809</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-2.068</td>\n",
       "      <td>2.147</td>\n",
       "      <td>-1.525</td>\n",
       "      <td>1.684</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.154</td>\n",
       "      <td>1.587</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-3.607</td>\n",
       "      <td>0.227</td>\n",
       "      <td>1.699</td>\n",
       "      <td>1.417</td>\n",
       "      <td>1.749</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.712</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.582</td>\n",
       "      <td>1.578</td>\n",
       "      <td>1.868</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>1.258</td>\n",
       "      <td>2.124</td>\n",
       "      <td>1.334</td>\n",
       "      <td>1.548</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-0.756</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-1.525</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>-1.671</td>\n",
       "      <td>-0.777</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-1.999</td>\n",
       "      <td>-1.633</td>\n",
       "      <td>-2.411</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-1.665</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-1.423</td>\n",
       "      <td>-1.164</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.551</td>\n",
       "      <td>-1.228</td>\n",
       "      <td>-2.743</td>\n",
       "      <td>-6.284</td>\n",
       "      <td>-1.174</td>\n",
       "      <td>6.929</td>\n",
       "      <td>1.588</td>\n",
       "      <td>-3.510</td>\n",
       "      <td>-4.396</td>\n",
       "      <td>-5.591</td>\n",
       "      <td>-1.979</td>\n",
       "      <td>-4.139</td>\n",
       "      <td>-1.380</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-1.918</td>\n",
       "      <td>1.406</td>\n",
       "      <td>-5.230</td>\n",
       "      <td>5.223</td>\n",
       "      <td>4.648</td>\n",
       "      <td>1.804</td>\n",
       "      <td>3.426</td>\n",
       "      <td>2.363</td>\n",
       "      <td>-4.154</td>\n",
       "      <td>2.999</td>\n",
       "      <td>-3.198</td>\n",
       "      <td>-1.308</td>\n",
       "      <td>-0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>79.712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-39.417</td>\n",
       "      <td>-86.861</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-50.679</td>\n",
       "      <td>-3.185</td>\n",
       "      <td>1.397</td>\n",
       "      <td>-5.585</td>\n",
       "      <td>3.817</td>\n",
       "      <td>-1.870</td>\n",
       "      <td>4.558</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>3.896</td>\n",
       "      <td>-5.096</td>\n",
       "      <td>-5.590</td>\n",
       "      <td>0.774</td>\n",
       "      <td>3.595</td>\n",
       "      <td>77.983</td>\n",
       "      <td>1.227</td>\n",
       "      <td>1.346</td>\n",
       "      <td>-4.830</td>\n",
       "      <td>-7.121</td>\n",
       "      <td>-35.112</td>\n",
       "      <td>-83.716</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-46.992</td>\n",
       "      <td>-1.616</td>\n",
       "      <td>2.064</td>\n",
       "      <td>-3.997</td>\n",
       "      <td>2.846</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>2.754</td>\n",
       "      <td>-1.474</td>\n",
       "      <td>-1.295</td>\n",
       "      <td>-2.993</td>\n",
       "      <td>-1.687</td>\n",
       "      <td>2.980</td>\n",
       "      <td>-1.455</td>\n",
       "      <td>80.627</td>\n",
       "      <td>0.316</td>\n",
       "      <td>1.906</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-4.545</td>\n",
       "      <td>-37.985</td>\n",
       "      <td>-86.410</td>\n",
       "      <td>0.657</td>\n",
       "      <td>-48.961</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>2.238</td>\n",
       "      <td>1.799</td>\n",
       "      <td>2.383</td>\n",
       "      <td>2.428</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-3.795</td>\n",
       "      <td>2.221</td>\n",
       "      <td>1.798</td>\n",
       "      <td>1.707</td>\n",
       "      <td>2.586</td>\n",
       "      <td>2.808</td>\n",
       "      <td>2.112</td>\n",
       "      <td>2.089</td>\n",
       "      <td>2.641</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.458</td>\n",
       "      <td>-3.757</td>\n",
       "      <td>2.186</td>\n",
       "      <td>2.664</td>\n",
       "      <td>2.877</td>\n",
       "      <td>-3.011</td>\n",
       "      <td>2.123</td>\n",
       "      <td>1.953</td>\n",
       "      <td>1.548</td>\n",
       "      <td>1.029</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-1.332</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-2.241</td>\n",
       "      <td>1.875</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.734</td>\n",
       "      <td>1.795</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>1.044</td>\n",
       "      <td>2.282</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-2.340</td>\n",
       "      <td>0.097</td>\n",
       "      <td>2.696</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>0.939</td>\n",
       "      <td>1.171</td>\n",
       "      <td>-3.498</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.657</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.915</td>\n",
       "      <td>1.170</td>\n",
       "      <td>0.889</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.120</td>\n",
       "      <td>1.146</td>\n",
       "      <td>0.914</td>\n",
       "      <td>1.615</td>\n",
       "      <td>1.198</td>\n",
       "      <td>1.460</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.741</td>\n",
       "      <td>1.009</td>\n",
       "      <td>1.841</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-3.223</td>\n",
       "      <td>0.089</td>\n",
       "      <td>1.548</td>\n",
       "      <td>1.135</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.915</td>\n",
       "      <td>-1.080</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.384</td>\n",
       "      <td>-1.083</td>\n",
       "      <td>-2.648</td>\n",
       "      <td>2.037</td>\n",
       "      <td>-1.740</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-1.321</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-1.220</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-1.547</td>\n",
       "      <td>2.896</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-2.407</td>\n",
       "      <td>0.338</td>\n",
       "      <td>2.947</td>\n",
       "      <td>1.233</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-1.952</td>\n",
       "      <td>3.045</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-1.622</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>0.473</td>\n",
       "      <td>1.082</td>\n",
       "      <td>-1.914</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-1.919</td>\n",
       "      <td>-1.689</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.655</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-2.015</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>-2.466</td>\n",
       "      <td>1.462</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-1.960</td>\n",
       "      <td>-1.174</td>\n",
       "      <td>-0.806</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-2.215</td>\n",
       "      <td>-3.328</td>\n",
       "      <td>-3.074</td>\n",
       "      <td>-2.271</td>\n",
       "      <td>4.962</td>\n",
       "      <td>6.236</td>\n",
       "      <td>-1.844</td>\n",
       "      <td>-4.706</td>\n",
       "      <td>-4.466</td>\n",
       "      <td>-1.934</td>\n",
       "      <td>-2.391</td>\n",
       "      <td>-2.271</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>-2.199</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-5.073</td>\n",
       "      <td>5.353</td>\n",
       "      <td>-4.405</td>\n",
       "      <td>6.140</td>\n",
       "      <td>5.645</td>\n",
       "      <td>3.439</td>\n",
       "      <td>-4.692</td>\n",
       "      <td>-2.386</td>\n",
       "      <td>-2.434</td>\n",
       "      <td>-6.295</td>\n",
       "      <td>-0.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.488</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-24.402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-83.560</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-25.515</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.391</td>\n",
       "      <td>-1.793</td>\n",
       "      <td>-4.525</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>46.529</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-12.513</td>\n",
       "      <td>13.385</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>-12.716</td>\n",
       "      <td>0.554</td>\n",
       "      <td>1.071</td>\n",
       "      <td>-80.974</td>\n",
       "      <td>-1.114</td>\n",
       "      <td>-2.906</td>\n",
       "      <td>-6.776</td>\n",
       "      <td>-14.046</td>\n",
       "      <td>-1.238</td>\n",
       "      <td>3.976</td>\n",
       "      <td>2.442</td>\n",
       "      <td>-2.135</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-1.847</td>\n",
       "      <td>0.937</td>\n",
       "      <td>1.073</td>\n",
       "      <td>46.776</td>\n",
       "      <td>-1.491</td>\n",
       "      <td>0.379</td>\n",
       "      <td>13.304</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>-24.035</td>\n",
       "      <td>1.513</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-82.721</td>\n",
       "      <td>-1.702</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>-24.750</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>1.939</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1.296</td>\n",
       "      <td>-1.325</td>\n",
       "      <td>-0.583</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>1.167</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.036</td>\n",
       "      <td>0.451</td>\n",
       "      <td>1.385</td>\n",
       "      <td>-1.527</td>\n",
       "      <td>0.886</td>\n",
       "      <td>1.244</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>0.676</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.662</td>\n",
       "      <td>-0.674</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.979</td>\n",
       "      <td>1.037</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.614</td>\n",
       "      <td>1.876</td>\n",
       "      <td>1.054</td>\n",
       "      <td>0.835</td>\n",
       "      <td>1.209</td>\n",
       "      <td>-2.389</td>\n",
       "      <td>2.560</td>\n",
       "      <td>1.508</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.328</td>\n",
       "      <td>1.835</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.881</td>\n",
       "      <td>-1.586</td>\n",
       "      <td>1.120</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>2.416</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>2.336</td>\n",
       "      <td>2.483</td>\n",
       "      <td>0.962</td>\n",
       "      <td>1.649</td>\n",
       "      <td>1.372</td>\n",
       "      <td>0.665</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-1.656</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.089</td>\n",
       "      <td>1.068</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.908</td>\n",
       "      <td>3.123</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.814</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.975</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.723</td>\n",
       "      <td>1.007</td>\n",
       "      <td>0.761</td>\n",
       "      <td>1.965</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>2.836</td>\n",
       "      <td>3.002</td>\n",
       "      <td>-2.444</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>1.433</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>1.025</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>1.985</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.350</td>\n",
       "      <td>2.173</td>\n",
       "      <td>2.387</td>\n",
       "      <td>-2.826</td>\n",
       "      <td>2.437</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-1.511</td>\n",
       "      <td>1.177</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-2.488</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.088</td>\n",
       "      <td>1.996</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>0.618</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.878</td>\n",
       "      <td>-1.848</td>\n",
       "      <td>-1.327</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.744</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-1.278</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-1.517</td>\n",
       "      <td>-1.629</td>\n",
       "      <td>1.041</td>\n",
       "      <td>-2.197</td>\n",
       "      <td>0.571</td>\n",
       "      <td>1.221</td>\n",
       "      <td>-1.844</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-3.080</td>\n",
       "      <td>-1.047</td>\n",
       "      <td>-1.340</td>\n",
       "      <td>-5.538</td>\n",
       "      <td>4.145</td>\n",
       "      <td>4.377</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-1.215</td>\n",
       "      <td>-1.217</td>\n",
       "      <td>-2.324</td>\n",
       "      <td>-1.121</td>\n",
       "      <td>-5.572</td>\n",
       "      <td>-1.358</td>\n",
       "      <td>-1.052</td>\n",
       "      <td>4.926</td>\n",
       "      <td>-2.634</td>\n",
       "      <td>1.652</td>\n",
       "      <td>-4.571</td>\n",
       "      <td>3.237</td>\n",
       "      <td>4.599</td>\n",
       "      <td>-4.641</td>\n",
       "      <td>-1.135</td>\n",
       "      <td>-1.036</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-1.195</td>\n",
       "      <td>-0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>71.763</td>\n",
       "      <td>35.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.671</td>\n",
       "      <td>-12.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-67.927</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-19.420</td>\n",
       "      <td>37.480</td>\n",
       "      <td>-17.544</td>\n",
       "      <td>17.166</td>\n",
       "      <td>-15.592</td>\n",
       "      <td>-3.190</td>\n",
       "      <td>4.341</td>\n",
       "      <td>-3.520</td>\n",
       "      <td>3.302</td>\n",
       "      <td>-17.343</td>\n",
       "      <td>44.776</td>\n",
       "      <td>48.271</td>\n",
       "      <td>18.445</td>\n",
       "      <td>2.620</td>\n",
       "      <td>-27.698</td>\n",
       "      <td>49.345</td>\n",
       "      <td>-7.388</td>\n",
       "      <td>1.643</td>\n",
       "      <td>-42.857</td>\n",
       "      <td>4.846</td>\n",
       "      <td>-18.289</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>1.110</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>1.286</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-1.294</td>\n",
       "      <td>-2.666</td>\n",
       "      <td>71.543</td>\n",
       "      <td>36.601</td>\n",
       "      <td>0.275</td>\n",
       "      <td>1.082</td>\n",
       "      <td>49.583</td>\n",
       "      <td>-13.470</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-66.022</td>\n",
       "      <td>1.854</td>\n",
       "      <td>-1.105</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.627</td>\n",
       "      <td>-3.050</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>1.182</td>\n",
       "      <td>1.295</td>\n",
       "      <td>1.135</td>\n",
       "      <td>0.929</td>\n",
       "      <td>1.197</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.734</td>\n",
       "      <td>1.042</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.859</td>\n",
       "      <td>1.821</td>\n",
       "      <td>1.288</td>\n",
       "      <td>2.246</td>\n",
       "      <td>-3.180</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>1.404</td>\n",
       "      <td>1.808</td>\n",
       "      <td>2.164</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>2.725</td>\n",
       "      <td>1.770</td>\n",
       "      <td>-1.764</td>\n",
       "      <td>-1.635</td>\n",
       "      <td>-1.154</td>\n",
       "      <td>-1.660</td>\n",
       "      <td>1.350</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1.830</td>\n",
       "      <td>2.087</td>\n",
       "      <td>1.383</td>\n",
       "      <td>2.051</td>\n",
       "      <td>1.878</td>\n",
       "      <td>-2.669</td>\n",
       "      <td>2.278</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-2.261</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-1.460</td>\n",
       "      <td>-3.050</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>2.482</td>\n",
       "      <td>2.388</td>\n",
       "      <td>2.173</td>\n",
       "      <td>2.346</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>-0.865</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>1.818</td>\n",
       "      <td>1.917</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>1.071</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>0.227</td>\n",
       "      <td>1.785</td>\n",
       "      <td>1.202</td>\n",
       "      <td>1.454</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-1.171</td>\n",
       "      <td>1.809</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.501</td>\n",
       "      <td>1.580</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.373</td>\n",
       "      <td>1.295</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>0.742</td>\n",
       "      <td>-2.257</td>\n",
       "      <td>2.244</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>1.627</td>\n",
       "      <td>1.324</td>\n",
       "      <td>1.338</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-2.497</td>\n",
       "      <td>1.319</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>1.729</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>1.568</td>\n",
       "      <td>1.959</td>\n",
       "      <td>1.472</td>\n",
       "      <td>1.602</td>\n",
       "      <td>1.776</td>\n",
       "      <td>2.287</td>\n",
       "      <td>1.866</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>1.088</td>\n",
       "      <td>1.057</td>\n",
       "      <td>0.561</td>\n",
       "      <td>2.798</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>1.239</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1.084</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>1.099</td>\n",
       "      <td>1.123</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.112</td>\n",
       "      <td>1.156</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>1.160</td>\n",
       "      <td>-1.647</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-4.967</td>\n",
       "      <td>-5.997</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>1.801</td>\n",
       "      <td>1.599</td>\n",
       "      <td>1.746</td>\n",
       "      <td>-8.302</td>\n",
       "      <td>-5.637</td>\n",
       "      <td>1.622</td>\n",
       "      <td>-2.542</td>\n",
       "      <td>-2.598</td>\n",
       "      <td>-2.440</td>\n",
       "      <td>-2.537</td>\n",
       "      <td>1.534</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>2.144</td>\n",
       "      <td>1.408</td>\n",
       "      <td>1.726</td>\n",
       "      <td>1.705</td>\n",
       "      <td>1.488</td>\n",
       "      <td>-4.932</td>\n",
       "      <td>1.709</td>\n",
       "      <td>-4.730</td>\n",
       "      <td>-2.883</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-4.682</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-55.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>37.671</td>\n",
       "      <td>94.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>69.192</td>\n",
       "      <td>-8.833</td>\n",
       "      <td>-26.328</td>\n",
       "      <td>-1.761</td>\n",
       "      <td>-25.635</td>\n",
       "      <td>-1.556</td>\n",
       "      <td>0.548</td>\n",
       "      <td>1.729</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>36.965</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>0.194</td>\n",
       "      <td>160.853</td>\n",
       "      <td>2.460</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1.682</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>-5.567</td>\n",
       "      <td>7.611</td>\n",
       "      <td>-3.795</td>\n",
       "      <td>1.719</td>\n",
       "      <td>-56.951</td>\n",
       "      <td>3.091</td>\n",
       "      <td>-6.571</td>\n",
       "      <td>1.313</td>\n",
       "      <td>-1.567</td>\n",
       "      <td>4.221</td>\n",
       "      <td>1.010</td>\n",
       "      <td>-5.613</td>\n",
       "      <td>-2.508</td>\n",
       "      <td>2.152</td>\n",
       "      <td>38.393</td>\n",
       "      <td>93.898</td>\n",
       "      <td>-4.385</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>2.681</td>\n",
       "      <td>1.774</td>\n",
       "      <td>69.174</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-3.144</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.013</td>\n",
       "      <td>3.323</td>\n",
       "      <td>3.225</td>\n",
       "      <td>3.493</td>\n",
       "      <td>-3.348</td>\n",
       "      <td>0.006</td>\n",
       "      <td>3.630</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-2.990</td>\n",
       "      <td>-2.763</td>\n",
       "      <td>0.171</td>\n",
       "      <td>3.269</td>\n",
       "      <td>0.198</td>\n",
       "      <td>3.039</td>\n",
       "      <td>3.479</td>\n",
       "      <td>3.378</td>\n",
       "      <td>3.469</td>\n",
       "      <td>3.294</td>\n",
       "      <td>-2.571</td>\n",
       "      <td>3.326</td>\n",
       "      <td>-2.550</td>\n",
       "      <td>-3.870</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>1.313</td>\n",
       "      <td>1.739</td>\n",
       "      <td>1.961</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.166</td>\n",
       "      <td>1.669</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>1.281</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>1.720</td>\n",
       "      <td>2.007</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.948</td>\n",
       "      <td>1.796</td>\n",
       "      <td>0.326</td>\n",
       "      <td>2.128</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>1.179</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.635</td>\n",
       "      <td>1.359</td>\n",
       "      <td>1.191</td>\n",
       "      <td>1.090</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.276</td>\n",
       "      <td>1.152</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.262</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.646</td>\n",
       "      <td>1.410</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.752</td>\n",
       "      <td>1.432</td>\n",
       "      <td>0.734</td>\n",
       "      <td>1.042</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.406</td>\n",
       "      <td>0.168</td>\n",
       "      <td>1.737</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>0.226</td>\n",
       "      <td>1.012</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.404</td>\n",
       "      <td>1.838</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.955</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>1.722</td>\n",
       "      <td>1.315</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1.421</td>\n",
       "      <td>0.252</td>\n",
       "      <td>1.453</td>\n",
       "      <td>1.956</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>1.788</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>1.977</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>1.550</td>\n",
       "      <td>1.638</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.715</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.704</td>\n",
       "      <td>1.666</td>\n",
       "      <td>0.639</td>\n",
       "      <td>1.273</td>\n",
       "      <td>2.195</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>1.547</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.980</td>\n",
       "      <td>1.645</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>1.248</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>1.562</td>\n",
       "      <td>1.475</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>1.068</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>1.043</td>\n",
       "      <td>1.127</td>\n",
       "      <td>1.033</td>\n",
       "      <td>1.104</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.290</td>\n",
       "      <td>1.194</td>\n",
       "      <td>1.266</td>\n",
       "      <td>1.952</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-3.581</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>2.495</td>\n",
       "      <td>2.422</td>\n",
       "      <td>2.369</td>\n",
       "      <td>-3.784</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>2.378</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-3.481</td>\n",
       "      <td>-3.360</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>2.310</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>2.807</td>\n",
       "      <td>2.135</td>\n",
       "      <td>2.568</td>\n",
       "      <td>2.411</td>\n",
       "      <td>2.243</td>\n",
       "      <td>-3.087</td>\n",
       "      <td>2.309</td>\n",
       "      <td>-2.574</td>\n",
       "      <td>-4.217</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "666  1373158606         0.000         0.000         0.000         0.000   \n",
       "129  1373158606         0.000         0.000         0.000         0.000   \n",
       "784  1373158606         0.000         0.000         0.000         0.000   \n",
       "152  1373158606         0.000         0.000         0.000         0.000   \n",
       "920  1373158606        -4.682         0.000         0.000         0.000   \n",
       "\n",
       "     00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "666       -84.503         0.000         0.000         0.000         0.000   \n",
       "129         0.000         0.000         0.000         0.000         0.000   \n",
       "784        45.889         0.000         0.000        13.488         0.000   \n",
       "152         0.000         0.000         0.000         0.000         0.000   \n",
       "920       -55.131         0.000         0.000         0.000         0.000   \n",
       "\n",
       "     00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "666         0.000         0.000         0.000         0.000         0.000   \n",
       "129         0.000         0.000        79.712         0.000         0.000   \n",
       "784       -24.402         0.000         0.000       -83.560         0.000   \n",
       "152         0.000         0.000        71.763        35.388         0.000   \n",
       "920         0.000         0.000         0.000         0.000         0.000   \n",
       "\n",
       "     02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "666         0.000         0.000        30.395        -2.612         0.000   \n",
       "129         0.000         0.000       -39.417       -86.861         0.000   \n",
       "784         0.000         0.000       -25.515         0.000         0.000   \n",
       "152         0.000        49.671       -12.158         0.000       -67.927   \n",
       "920        37.671        94.206         0.000         0.000         0.000   \n",
       "\n",
       "     11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "666        83.254       -22.327              -1.452              -1.900   \n",
       "129       -50.679        -3.185               1.397              -5.585   \n",
       "784         0.000         0.000               5.391              -1.793   \n",
       "152         0.000         0.000             -19.420              37.480   \n",
       "920         0.000        69.192              -8.833             -26.328   \n",
       "\n",
       "     00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "666               0.455               1.509             -83.291   \n",
       "129               3.817              -1.870               4.558   \n",
       "784              -4.525              -0.346              46.529   \n",
       "152             -17.544              17.166             -15.592   \n",
       "920              -1.761             -25.635              -1.556   \n",
       "\n",
       "     00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "666              -0.020              -0.791               2.239   \n",
       "129              -0.012               3.896              -5.096   \n",
       "784              -0.024             -12.513              13.385   \n",
       "152              -3.190               4.341              -3.520   \n",
       "920               0.548               1.729              -1.801   \n",
       "\n",
       "     00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "666              -0.792              -0.969               3.823   \n",
       "129              -5.590               0.774               3.595   \n",
       "784              -1.916             -12.716               0.554   \n",
       "152               3.302             -17.343              44.776   \n",
       "920              -0.840              -0.743              36.965   \n",
       "\n",
       "     01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "666              -1.050              -0.209               1.440   \n",
       "129              77.983               1.227               1.346   \n",
       "784               1.071             -80.974              -1.114   \n",
       "152              48.271              18.445               2.620   \n",
       "920              -0.506              -0.261              -0.344   \n",
       "\n",
       "     02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "666              -1.575               4.999              29.598   \n",
       "129              -4.830              -7.121             -35.112   \n",
       "784              -2.906              -6.776             -14.046   \n",
       "152             -27.698              49.345              -7.388   \n",
       "920               0.194             160.853               2.460   \n",
       "\n",
       "     10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "666              -5.345              -0.023              77.650   \n",
       "129             -83.716              -0.008             -46.992   \n",
       "784              -1.238               3.976               2.442   \n",
       "152               1.643             -42.857               4.846   \n",
       "920               1.980               1.682               0.295   \n",
       "\n",
       "     20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "666             -22.542               0.000               1.573   \n",
       "129              -1.616               2.064              -3.997   \n",
       "784              -2.135               0.042              -1.847   \n",
       "152             -18.289              -0.209               1.110   \n",
       "920              -1.526              -5.567               7.611   \n",
       "\n",
       "     00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "666              -0.838              -1.568             -86.010   \n",
       "129               2.846              -0.097               2.754   \n",
       "784               0.937               1.073              46.776   \n",
       "152              -0.652               0.959               0.046   \n",
       "920              -3.795               1.719             -56.951   \n",
       "\n",
       "     00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "666               1.671               0.141               0.533   \n",
       "129              -1.474              -1.295              -2.993   \n",
       "784              -1.491               0.379              13.304   \n",
       "152              -0.446               1.286               0.468   \n",
       "920               3.091              -6.571               1.313   \n",
       "\n",
       "     00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "666               0.083              -1.119              -0.458   \n",
       "129              -1.687               2.980              -1.455   \n",
       "784              -0.866             -24.035               1.513   \n",
       "152              -1.624              -1.294              -2.666   \n",
       "920              -1.567               4.221               1.010   \n",
       "\n",
       "     01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "666              -0.503               0.042               0.912   \n",
       "129              80.627               0.316               1.906   \n",
       "784               0.182             -82.721              -1.702   \n",
       "152              71.543              36.601               0.275   \n",
       "920              -5.613              -2.508               2.152   \n",
       "\n",
       "     02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "666               0.727               0.928              30.397   \n",
       "129              -0.075              -4.545             -37.985   \n",
       "784              -0.892              -0.731             -24.750   \n",
       "152               1.082              49.583             -13.470   \n",
       "920              38.393              93.898              -4.385   \n",
       "\n",
       "     10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "666              -2.065               0.369              82.036   \n",
       "129             -86.410               0.657             -48.961   \n",
       "784              -0.198               1.939              -0.953   \n",
       "152              -0.027             -66.022               1.854   \n",
       "920              -0.944               2.681               1.774   \n",
       "\n",
       "     20000-lstsq_target   wb_0   wb_1  wb_2   wb_3   wb_4   wb_5   wb_6  \\\n",
       "666             -22.947 -0.722 -1.496 3.707 -0.164  0.133  1.848 -0.967   \n",
       "129              -0.611  2.238  1.799 2.383  2.428  0.380 -3.795  2.221   \n",
       "784              -0.009 -0.107  0.636 1.296 -1.325 -0.583 -0.682  1.167   \n",
       "152              -1.105 -0.569  0.182 0.193  0.001  0.943  0.844  0.627   \n",
       "920              69.174 -0.077 -3.144 0.201  0.013  3.323  3.225  3.493   \n",
       "\n",
       "      wb_7   wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  \\\n",
       "666 -3.165 -2.650 0.302  2.608 -0.600 -0.162  0.348  2.006  0.516  1.706   \n",
       "129  1.798  1.707 2.586  2.808  2.112  2.089  2.641  0.005  2.458 -3.757   \n",
       "784  0.600  1.036 0.451  1.385 -1.527  0.886  1.244 -1.214  0.676 -0.221   \n",
       "152 -3.050 -0.410 1.182  1.295  1.135  0.929  1.197  0.872  0.207  0.734   \n",
       "920 -3.348  0.006 3.630  0.265 -2.990 -2.763  0.171  3.269  0.198  3.039   \n",
       "\n",
       "     wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  \\\n",
       "666  2.128  2.070  2.030  1.995 -0.325  2.052 -1.990 -1.294 -0.548  0.186   \n",
       "129  2.186  2.664  2.877 -3.011  2.123  1.953  1.548  1.029  0.032  0.170   \n",
       "784  0.662 -0.674  0.600 -0.103  0.979  1.037  0.807  0.614  1.876  1.054   \n",
       "152  1.042  0.998  1.046  0.863  0.222  0.859  1.821  1.288  2.246 -3.180   \n",
       "920  3.479  3.378  3.469  3.294 -2.571  3.326 -2.550 -3.870 -0.145  0.385   \n",
       "\n",
       "     wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  \\\n",
       "666 -3.022 -1.034  0.833  1.286  0.297  1.758  1.230 -1.231 -2.178 -0.557   \n",
       "129 -1.332 -0.072 -2.241  1.875  0.971  0.734  1.795  0.462 -0.464 -0.151   \n",
       "784  0.835  1.209 -2.389  2.560  1.508  1.205  1.328  1.835  0.787  0.486   \n",
       "152 -0.354 -0.310  1.404  1.808  2.164 -0.061  2.725  1.770 -1.764 -1.635   \n",
       "920 -0.341 -0.305  1.313  1.739  1.961  0.412  0.166  1.669 -0.435  0.217   \n",
       "\n",
       "     wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  \\\n",
       "666 -0.145 -1.148  0.909 -1.138  1.261  2.585  0.867  2.424  1.697 -0.341   \n",
       "129 -0.423 -0.530 -0.386  1.044  2.282  0.347 -2.340  0.097  2.696 -0.653   \n",
       "784  0.952  0.881 -1.586  1.120 -0.651  2.416 -1.724  2.336  2.483  0.962   \n",
       "152 -1.154 -1.660  1.350  0.006  1.830  2.087  1.383  2.051  1.878 -2.669   \n",
       "920  0.307 -0.289  1.281 -0.004  1.720  2.007  1.310  1.948  1.796  0.326   \n",
       "\n",
       "     wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  \\\n",
       "666  2.215  1.227 -0.248  0.494  0.226 -0.274  0.574 -0.157  0.658 -0.063   \n",
       "129  0.939  1.171 -3.498  0.400  0.358 -0.255  0.392  0.752  0.021  0.213   \n",
       "784  1.649  1.372  0.665 -0.071  0.740  0.363 -1.656 -0.099  0.089  1.068   \n",
       "152  2.278  0.473 -2.261 -0.462  0.087 -0.441 -0.179 -0.929 -0.157 -1.460   \n",
       "920  2.128  0.532  0.357 -0.099  1.179 -0.445 -0.206  0.635  1.359  1.191   \n",
       "\n",
       "     wb_57  wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  \\\n",
       "666 -0.114 -0.194  0.013  0.176  0.806  0.175  0.288  0.629 -0.008 -0.068   \n",
       "129 -0.203 -0.084 -0.657  0.501  0.762  0.452  0.400  0.115 -0.422  0.618   \n",
       "784  0.193  0.546  0.428  0.908  3.123  0.917  0.814 -0.000  0.975 -0.084   \n",
       "152 -3.050 -0.255 -0.831  2.482  2.388  2.173  2.346 -0.296 -0.236 -0.840   \n",
       "920  1.090 -0.147  0.880  0.009  1.276  1.152  0.026  1.262 -0.239  0.646   \n",
       "\n",
       "     wb_67  wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  \\\n",
       "666  0.196  0.532 -0.233  0.148 -0.164  0.051  0.125  0.351  1.037  1.850   \n",
       "129  0.086  0.088  0.742  0.769  0.256 -0.163  0.800  0.440  0.915  1.170   \n",
       "784 -0.616  0.109  0.035  0.541  0.156  0.723  1.007  0.761  1.965  0.198   \n",
       "152 -0.219 -0.517 -0.865 -0.208 -0.171 -0.833  1.818  1.917 -0.370  1.071   \n",
       "920  1.410  0.999  0.752  1.432  0.734  1.042  1.062  1.406  0.168  1.737   \n",
       "\n",
       "     wb_77  wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  \\\n",
       "666  0.423  1.130  3.636 -1.066  1.769  1.461  1.100  1.859  1.388  1.032   \n",
       "129  0.889  1.068  1.120  1.146  0.914  1.615  1.198  1.460  0.653  0.876   \n",
       "784 -0.446  2.836  3.002 -2.444  0.006  0.045 -0.245  1.433 -0.176  1.025   \n",
       "152 -0.455  0.227  1.785  1.202  1.454  0.250 -1.171  1.809  0.516  0.580   \n",
       "920 -0.460  0.226  1.012  0.448  0.404  1.838 -0.140  0.955 -0.270  1.722   \n",
       "\n",
       "     wb_87  wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  \\\n",
       "666 -0.318  1.575 -0.518  1.995 -2.809 -0.976 -0.613 -0.259 -2.068  2.147   \n",
       "129  0.272  0.628  0.046  0.741  1.009  1.841 -1.099 -3.223  0.089  1.548   \n",
       "784 -0.588 -0.259  1.985  0.831  0.350  2.173  2.387 -2.826  2.437 -0.075   \n",
       "152  0.127  0.501  1.580  0.020  0.969  1.205  1.373  1.295  1.068  1.003   \n",
       "920  1.315 -0.210  0.814  0.005  0.230  0.400  0.617  0.481  0.256  1.421   \n",
       "\n",
       "     wb_97  wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  \\\n",
       "666 -1.525  1.684  0.857   0.154   1.587  -0.466   0.243  -3.607   0.227   \n",
       "129  1.135  0.869  0.915  -1.080   0.031   1.384  -1.083  -2.648   2.037   \n",
       "784 -0.427  0.201 -0.412  -0.701   0.330   0.357  -1.511   1.177  -0.578   \n",
       "152  1.180 -0.385  0.742  -2.257   2.244  -0.034  -0.538   1.627   1.324   \n",
       "920  0.252  1.453  1.956  -0.367   1.788  -0.029  -0.531   0.602   0.327   \n",
       "\n",
       "     wb_106  wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  \\\n",
       "666   1.699   1.417   1.749   0.997   0.712  -0.006   0.051   0.740   0.582   \n",
       "129  -1.740  -0.022  -1.321  -0.934  -0.950  -1.220  -0.485  -0.644   0.033   \n",
       "784  -0.281   0.642   0.162  -0.835  -0.366  -2.488   0.424   0.088   1.996   \n",
       "152   1.338  -0.025  -2.497   1.319  -0.197  -0.283  -0.133  -0.153   1.729   \n",
       "920  -0.139   1.977  -0.126   0.189  -0.521   1.550   1.638  -0.161   0.715   \n",
       "\n",
       "     wb_115  wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  \\\n",
       "666   1.578   1.868   0.622   0.476  -0.696   1.258   2.124   1.334   1.548   \n",
       "129  -1.547   2.896  -0.012  -2.407   0.338   2.947   1.233  -0.758  -1.952   \n",
       "784  -0.327   0.618  -0.526   0.878  -1.848  -1.327   0.718   0.252  -0.074   \n",
       "152  -0.401   1.568   1.959   1.472   1.602   1.776   2.287   1.866  -0.097   \n",
       "920  -0.401   0.592   0.893   0.474   0.525   0.704   1.666   0.639   1.273   \n",
       "\n",
       "     wb_124  wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  \\\n",
       "666   0.768   0.687  -0.756  -0.694   0.478  -0.559   0.115  -1.525  -0.835   \n",
       "129   3.045   0.415  -1.622  -0.532   0.473   1.082  -1.914  -0.102  -1.919   \n",
       "784   0.744  -0.784   0.171   0.281  -0.336   0.077   0.322  -0.027   0.214   \n",
       "152   0.945   0.820  -1.190  -0.093  -0.165   1.088   1.057   0.561   2.798   \n",
       "920   2.195  -0.078   1.547  -0.086  -0.162   1.068   1.028   0.980   1.645   \n",
       "\n",
       "     wb_133  wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  \\\n",
       "666  -1.671  -0.777  -0.538   0.880  -0.145  -0.606   0.174  -1.999  -1.633   \n",
       "129  -1.689  -0.518   0.837   0.418   0.529   0.655  -0.167  -2.015  -0.661   \n",
       "784   0.128  -1.278   0.053  -0.097   0.213   0.144  -1.517  -1.629   1.041   \n",
       "152  -0.046   1.239   0.904   0.857   0.797   0.829   1.084  -0.164   1.099   \n",
       "920  -0.137   1.248  -0.147   1.562   1.475  -0.113   1.068  -0.174   1.043   \n",
       "\n",
       "     wb_142  wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  \\\n",
       "666  -2.411   0.373  -1.665  -0.422  -1.423  -1.164  -0.445   0.551  -1.228   \n",
       "129  -2.466   1.462  -0.546   0.256  -1.960  -1.174  -0.806  -0.867  -2.215   \n",
       "784  -2.197   0.571   1.221  -1.844   0.229   0.108   0.055   0.315  -3.080   \n",
       "152   1.123   1.064   1.112   1.156  -0.527   1.160  -1.647   0.223  -4.967   \n",
       "920   1.127   1.033   1.104   1.158   1.290   1.194   1.266   1.952  -0.223   \n",
       "\n",
       "     wb_151  wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  \\\n",
       "666  -2.743  -6.284  -1.174   6.929   1.588  -3.510  -4.396  -5.591  -1.979   \n",
       "129  -3.328  -3.074  -2.271   4.962   6.236  -1.844  -4.706  -4.466  -1.934   \n",
       "784  -1.047  -1.340  -5.538   4.145   4.377  -0.856  -1.215  -1.217  -2.324   \n",
       "152  -5.997  -0.252  -0.177   1.801   1.599   1.746  -8.302  -5.637   1.622   \n",
       "920  -3.581  -0.261  -0.197   2.495   2.422   2.369  -3.784  -0.263   2.378   \n",
       "\n",
       "     wb_160  wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  \\\n",
       "666  -4.139  -1.380  -0.301  -1.918   1.406  -5.230   5.223   4.648   1.804   \n",
       "129  -2.391  -2.271  -2.186  -2.199   0.086  -5.073   5.353  -4.405   6.140   \n",
       "784  -1.121  -5.572  -1.358  -1.052   4.926  -2.634   1.652  -4.571   3.237   \n",
       "152  -2.542  -2.598  -2.440  -2.537   1.534  -0.157   2.144   1.408   1.726   \n",
       "920  -0.050  -3.481  -3.360  -0.088   2.310  -0.155   2.807   2.135   2.568   \n",
       "\n",
       "     wb_169  wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "666   3.426   2.363  -4.154   2.999  -3.198  -1.308  -0.058  \n",
       "129   5.645   3.439  -4.692  -2.386  -2.434  -6.295  -0.617  \n",
       "784   4.599  -4.641  -1.135  -1.036  -0.909  -1.195  -0.316  \n",
       "152   1.705   1.488  -4.932   1.709  -4.730  -2.883   0.896  \n",
       "920   2.411   2.243  -3.087   2.309  -2.574  -4.217   0.707  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------- TRAINING INTERPRETATION NET -----------------------------------------------\n",
      "[<function inet_coefficient_loss_wrapper.<locals>.inet_coefficient_loss at 0x7f5cb45cce50>, <function inet_lambda_fv_loss_wrapper.<locals>.inet_lambda_fv_loss at 0x7f5cc81b3040>, <function inet_coefficient_loss_wrapper.<locals>.inet_coefficient_loss at 0x7f5cc81b30d0>, <function inet_lambda_fv_loss_wrapper.<locals>.inet_lambda_fv_loss at 0x7f5cc81b3160>]\n",
      "Epoch 1/500\n",
      "40/40 [==============================] - 19s 320ms/step - loss: 35.9351 - r2_inet_coefficient_loss: 0.1610 - r2_inet_lambda_fv_loss: 1.6137 - mae_inet_coefficient_loss: 21.0015 - mae_inet_lambda_fv_loss: 35.9325 - val_loss: 25.2960 - val_r2_inet_coefficient_loss: 0.4877 - val_r2_inet_lambda_fv_loss: 0.2478 - val_mae_inet_coefficient_loss: 26.3102 - val_mae_inet_lambda_fv_loss: 25.2962\n",
      "Epoch 2/500\n",
      "40/40 [==============================] - 11s 276ms/step - loss: 25.7839 - r2_inet_coefficient_loss: 0.4173 - r2_inet_lambda_fv_loss: 0.2355 - mae_inet_coefficient_loss: 26.3016 - mae_inet_lambda_fv_loss: 25.7810 - val_loss: 22.7510 - val_r2_inet_coefficient_loss: 0.4401 - val_r2_inet_lambda_fv_loss: 0.0933 - val_mae_inet_coefficient_loss: 27.3282 - val_mae_inet_lambda_fv_loss: 22.8278\n",
      "Epoch 3/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 22.7600 - r2_inet_coefficient_loss: 0.4685 - r2_inet_lambda_fv_loss: 0.0531 - mae_inet_coefficient_loss: 27.9304 - mae_inet_lambda_fv_loss: 22.7597 - val_loss: 21.5427 - val_r2_inet_coefficient_loss: 0.8652 - val_r2_inet_lambda_fv_loss: 0.0037 - val_mae_inet_coefficient_loss: 29.4050 - val_mae_inet_lambda_fv_loss: 21.5563\n",
      "Epoch 4/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 22.2215 - r2_inet_coefficient_loss: 0.8699 - r2_inet_lambda_fv_loss: 0.0399 - mae_inet_coefficient_loss: 29.5273 - mae_inet_lambda_fv_loss: 22.2236 - val_loss: 21.9010 - val_r2_inet_coefficient_loss: 1.1969 - val_r2_inet_lambda_fv_loss: 0.0598 - val_mae_inet_coefficient_loss: 30.1931 - val_mae_inet_lambda_fv_loss: 21.9486\n",
      "Epoch 5/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 22.4602 - r2_inet_coefficient_loss: 1.0811 - r2_inet_lambda_fv_loss: 0.0629 - mae_inet_coefficient_loss: 30.5576 - mae_inet_lambda_fv_loss: 22.4668 - val_loss: 22.0472 - val_r2_inet_coefficient_loss: 1.3174 - val_r2_inet_lambda_fv_loss: 0.0328 - val_mae_inet_coefficient_loss: 30.4253 - val_mae_inet_lambda_fv_loss: 22.0574\n",
      "Epoch 6/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 22.7406 - r2_inet_coefficient_loss: 1.1527 - r2_inet_lambda_fv_loss: 0.0883 - mae_inet_coefficient_loss: 30.9874 - mae_inet_lambda_fv_loss: 22.7355 - val_loss: 22.1576 - val_r2_inet_coefficient_loss: 1.5439 - val_r2_inet_lambda_fv_loss: 0.0560 - val_mae_inet_coefficient_loss: 30.9412 - val_mae_inet_lambda_fv_loss: 22.1697\n",
      "Epoch 7/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 22.5163 - r2_inet_coefficient_loss: 1.2291 - r2_inet_lambda_fv_loss: 0.0815 - mae_inet_coefficient_loss: 31.3969 - mae_inet_lambda_fv_loss: 22.5146 - val_loss: 21.8781 - val_r2_inet_coefficient_loss: 1.7495 - val_r2_inet_lambda_fv_loss: 0.0355 - val_mae_inet_coefficient_loss: 32.4843 - val_mae_inet_lambda_fv_loss: 21.9356\n",
      "Epoch 8/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 22.2705 - r2_inet_coefficient_loss: 1.4104 - r2_inet_lambda_fv_loss: 0.0526 - mae_inet_coefficient_loss: 32.1952 - mae_inet_lambda_fv_loss: 22.2679 - val_loss: 21.1395 - val_r2_inet_coefficient_loss: 1.3668 - val_r2_inet_lambda_fv_loss: -0.0208 - val_mae_inet_coefficient_loss: 31.6872 - val_mae_inet_lambda_fv_loss: 21.1733\n",
      "Epoch 9/500\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 21.8919 - r2_inet_coefficient_loss: 1.3279 - r2_inet_lambda_fv_loss: 0.0203 - mae_inet_coefficient_loss: 31.9625 - mae_inet_lambda_fv_loss: 21.8940 - val_loss: 21.5457 - val_r2_inet_coefficient_loss: 1.5831 - val_r2_inet_lambda_fv_loss: 0.0165 - val_mae_inet_coefficient_loss: 32.4665 - val_mae_inet_lambda_fv_loss: 21.5021\n",
      "Epoch 10/500\n",
      "40/40 [==============================] - 11s 275ms/step - loss: 22.3819 - r2_inet_coefficient_loss: 1.3542 - r2_inet_lambda_fv_loss: 0.0453 - mae_inet_coefficient_loss: 31.5432 - mae_inet_lambda_fv_loss: 22.3825 - val_loss: 21.6358 - val_r2_inet_coefficient_loss: 1.2719 - val_r2_inet_lambda_fv_loss: -0.0265 - val_mae_inet_coefficient_loss: 29.6970 - val_mae_inet_lambda_fv_loss: 21.5884\n",
      "Epoch 11/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 22.3912 - r2_inet_coefficient_loss: 1.2016 - r2_inet_lambda_fv_loss: 0.0146 - mae_inet_coefficient_loss: 30.1451 - mae_inet_lambda_fv_loss: 22.3869 - val_loss: 20.9510 - val_r2_inet_coefficient_loss: 1.3317 - val_r2_inet_lambda_fv_loss: -0.1056 - val_mae_inet_coefficient_loss: 29.9373 - val_mae_inet_lambda_fv_loss: 20.9150\n",
      "Epoch 12/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 21.5259 - r2_inet_coefficient_loss: 1.4515 - r2_inet_lambda_fv_loss: -0.0541 - mae_inet_coefficient_loss: 31.5513 - mae_inet_lambda_fv_loss: 21.5255 - val_loss: 20.6803 - val_r2_inet_coefficient_loss: 1.6125 - val_r2_inet_lambda_fv_loss: -0.0810 - val_mae_inet_coefficient_loss: 31.6998 - val_mae_inet_lambda_fv_loss: 20.7020\n",
      "Epoch 13/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 21.4187 - r2_inet_coefficient_loss: 1.5651 - r2_inet_lambda_fv_loss: -0.0421 - mae_inet_coefficient_loss: 32.4374 - mae_inet_lambda_fv_loss: 21.4219 - val_loss: 20.4257 - val_r2_inet_coefficient_loss: 1.6024 - val_r2_inet_lambda_fv_loss: -0.1185 - val_mae_inet_coefficient_loss: 31.2371 - val_mae_inet_lambda_fv_loss: 20.4105\n",
      "Epoch 14/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 20.9228 - r2_inet_coefficient_loss: 1.4863 - r2_inet_lambda_fv_loss: -0.0807 - mae_inet_coefficient_loss: 31.8629 - mae_inet_lambda_fv_loss: 20.9271 - val_loss: 20.0612 - val_r2_inet_coefficient_loss: 1.6614 - val_r2_inet_lambda_fv_loss: -0.1308 - val_mae_inet_coefficient_loss: 31.0068 - val_mae_inet_lambda_fv_loss: 20.0936\n",
      "Epoch 15/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 20.7561 - r2_inet_coefficient_loss: 1.5758 - r2_inet_lambda_fv_loss: -0.0999 - mae_inet_coefficient_loss: 31.8830 - mae_inet_lambda_fv_loss: 20.7547 - val_loss: 19.8878 - val_r2_inet_coefficient_loss: 1.8401 - val_r2_inet_lambda_fv_loss: -0.1562 - val_mae_inet_coefficient_loss: 32.0139 - val_mae_inet_lambda_fv_loss: 19.9054\n",
      "Epoch 16/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 20.5505 - r2_inet_coefficient_loss: 1.6867 - r2_inet_lambda_fv_loss: -0.1283 - mae_inet_coefficient_loss: 32.6041 - mae_inet_lambda_fv_loss: 20.5488 - val_loss: 20.0224 - val_r2_inet_coefficient_loss: 1.7722 - val_r2_inet_lambda_fv_loss: -0.1435 - val_mae_inet_coefficient_loss: 32.1597 - val_mae_inet_lambda_fv_loss: 19.9654\n",
      "Epoch 17/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 20.7153 - r2_inet_coefficient_loss: 1.6619 - r2_inet_lambda_fv_loss: -0.0956 - mae_inet_coefficient_loss: 32.6873 - mae_inet_lambda_fv_loss: 20.7159 - val_loss: 20.2619 - val_r2_inet_coefficient_loss: 1.8280 - val_r2_inet_lambda_fv_loss: -0.1252 - val_mae_inet_coefficient_loss: 31.8450 - val_mae_inet_lambda_fv_loss: 20.2404\n",
      "Epoch 18/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 20.7869 - r2_inet_coefficient_loss: 1.5558 - r2_inet_lambda_fv_loss: -0.1145 - mae_inet_coefficient_loss: 32.0787 - mae_inet_lambda_fv_loss: 20.7860 - val_loss: 19.6052 - val_r2_inet_coefficient_loss: 1.6528 - val_r2_inet_lambda_fv_loss: -0.1795 - val_mae_inet_coefficient_loss: 31.2695 - val_mae_inet_lambda_fv_loss: 19.5875\n",
      "Epoch 19/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 20.2447 - r2_inet_coefficient_loss: 1.5672 - r2_inet_lambda_fv_loss: -0.1593 - mae_inet_coefficient_loss: 31.9202 - mae_inet_lambda_fv_loss: 20.2436 - val_loss: 19.7116 - val_r2_inet_coefficient_loss: 1.5285 - val_r2_inet_lambda_fv_loss: -0.1923 - val_mae_inet_coefficient_loss: 30.1960 - val_mae_inet_lambda_fv_loss: 19.7525\n",
      "Epoch 20/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 20.4096 - r2_inet_coefficient_loss: 1.4275 - r2_inet_lambda_fv_loss: -0.1569 - mae_inet_coefficient_loss: 31.2655 - mae_inet_lambda_fv_loss: 20.4103 - val_loss: 19.6019 - val_r2_inet_coefficient_loss: 1.5531 - val_r2_inet_lambda_fv_loss: -0.2351 - val_mae_inet_coefficient_loss: 30.4580 - val_mae_inet_lambda_fv_loss: 19.5630\n",
      "Epoch 21/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 20.6185 - r2_inet_coefficient_loss: 1.4075 - r2_inet_lambda_fv_loss: -0.1601 - mae_inet_coefficient_loss: 31.3547 - mae_inet_lambda_fv_loss: 20.6171 - val_loss: 20.2770 - val_r2_inet_coefficient_loss: 1.7426 - val_r2_inet_lambda_fv_loss: -0.1935 - val_mae_inet_coefficient_loss: 30.5325 - val_mae_inet_lambda_fv_loss: 20.2900\n",
      "Epoch 22/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 21.4464 - r2_inet_coefficient_loss: 1.7194 - r2_inet_lambda_fv_loss: -0.0943 - mae_inet_coefficient_loss: 32.1028 - mae_inet_lambda_fv_loss: 21.4428 - val_loss: 20.7033 - val_r2_inet_coefficient_loss: 1.8263 - val_r2_inet_lambda_fv_loss: -0.1527 - val_mae_inet_coefficient_loss: 30.2469 - val_mae_inet_lambda_fv_loss: 20.7408\n",
      "Epoch 23/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 21.3237 - r2_inet_coefficient_loss: 1.6574 - r2_inet_lambda_fv_loss: -0.0958 - mae_inet_coefficient_loss: 31.3111 - mae_inet_lambda_fv_loss: 21.3252 - val_loss: 20.6179 - val_r2_inet_coefficient_loss: 2.0135 - val_r2_inet_lambda_fv_loss: -0.1407 - val_mae_inet_coefficient_loss: 31.0791 - val_mae_inet_lambda_fv_loss: 20.7298\n",
      "Epoch 24/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 21.1779 - r2_inet_coefficient_loss: 1.6480 - r2_inet_lambda_fv_loss: -0.0969 - mae_inet_coefficient_loss: 31.6442 - mae_inet_lambda_fv_loss: 21.1732 - val_loss: 20.2331 - val_r2_inet_coefficient_loss: 2.0167 - val_r2_inet_lambda_fv_loss: -0.1877 - val_mae_inet_coefficient_loss: 31.5358 - val_mae_inet_lambda_fv_loss: 20.2482\n",
      "Epoch 25/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 21.0005 - r2_inet_coefficient_loss: 1.7496 - r2_inet_lambda_fv_loss: -0.1171 - mae_inet_coefficient_loss: 32.3608 - mae_inet_lambda_fv_loss: 20.9992 - val_loss: 20.6857 - val_r2_inet_coefficient_loss: 2.0469 - val_r2_inet_lambda_fv_loss: -0.1561 - val_mae_inet_coefficient_loss: 31.6934 - val_mae_inet_lambda_fv_loss: 20.6966\n",
      "Epoch 26/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 21.2566 - r2_inet_coefficient_loss: 1.8608 - r2_inet_lambda_fv_loss: -0.0963 - mae_inet_coefficient_loss: 32.0218 - mae_inet_lambda_fv_loss: 21.2562 - val_loss: 20.8748 - val_r2_inet_coefficient_loss: 2.0454 - val_r2_inet_lambda_fv_loss: -0.1123 - val_mae_inet_coefficient_loss: 31.8262 - val_mae_inet_lambda_fv_loss: 20.8729\n",
      "Epoch 27/500\n",
      "40/40 [==============================] - 11s 268ms/step - loss: 21.5502 - r2_inet_coefficient_loss: 1.6825 - r2_inet_lambda_fv_loss: -0.0729 - mae_inet_coefficient_loss: 32.3063 - mae_inet_lambda_fv_loss: 21.5498 - val_loss: 20.7132 - val_r2_inet_coefficient_loss: 1.7086 - val_r2_inet_lambda_fv_loss: -0.1471 - val_mae_inet_coefficient_loss: 30.4648 - val_mae_inet_lambda_fv_loss: 20.7180\n",
      "Epoch 28/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 21.1101 - r2_inet_coefficient_loss: 1.5504 - r2_inet_lambda_fv_loss: -0.1154 - mae_inet_coefficient_loss: 31.7484 - mae_inet_lambda_fv_loss: 21.1106 - val_loss: 20.9173 - val_r2_inet_coefficient_loss: 1.7656 - val_r2_inet_lambda_fv_loss: -0.1197 - val_mae_inet_coefficient_loss: 31.2096 - val_mae_inet_lambda_fv_loss: 20.9406\n",
      "Epoch 29/500\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 21.6063 - r2_inet_coefficient_loss: 1.6629 - r2_inet_lambda_fv_loss: -0.0791 - mae_inet_coefficient_loss: 32.1615 - mae_inet_lambda_fv_loss: 21.6057 - val_loss: 20.8471 - val_r2_inet_coefficient_loss: 1.7957 - val_r2_inet_lambda_fv_loss: -0.1252 - val_mae_inet_coefficient_loss: 31.4385 - val_mae_inet_lambda_fv_loss: 20.8629\n",
      "Epoch 30/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 21.1509 - r2_inet_coefficient_loss: 1.7293 - r2_inet_lambda_fv_loss: -0.1039 - mae_inet_coefficient_loss: 32.1934 - mae_inet_lambda_fv_loss: 21.1480 - val_loss: 20.7600 - val_r2_inet_coefficient_loss: 1.8401 - val_r2_inet_lambda_fv_loss: -0.1046 - val_mae_inet_coefficient_loss: 31.8102 - val_mae_inet_lambda_fv_loss: 20.7725\n",
      "Epoch 31/500\n",
      "40/40 [==============================] - 11s 268ms/step - loss: 20.9883 - r2_inet_coefficient_loss: 1.8117 - r2_inet_lambda_fv_loss: -0.1144 - mae_inet_coefficient_loss: 32.7476 - mae_inet_lambda_fv_loss: 20.9876 - val_loss: 20.2449 - val_r2_inet_coefficient_loss: 2.0294 - val_r2_inet_lambda_fv_loss: -0.1531 - val_mae_inet_coefficient_loss: 32.4220 - val_mae_inet_lambda_fv_loss: 20.2368\n",
      "Epoch 32/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 20.3624 - r2_inet_coefficient_loss: 1.9337 - r2_inet_lambda_fv_loss: -0.1219 - mae_inet_coefficient_loss: 33.0713 - mae_inet_lambda_fv_loss: 20.3637 - val_loss: 19.4414 - val_r2_inet_coefficient_loss: 2.2881 - val_r2_inet_lambda_fv_loss: -0.2101 - val_mae_inet_coefficient_loss: 33.7210 - val_mae_inet_lambda_fv_loss: 19.3959\n",
      "Epoch 33/500\n",
      "40/40 [==============================] - 11s 268ms/step - loss: 20.0677 - r2_inet_coefficient_loss: 2.2426 - r2_inet_lambda_fv_loss: -0.1453 - mae_inet_coefficient_loss: 34.4188 - mae_inet_lambda_fv_loss: 20.0678 - val_loss: 19.1062 - val_r2_inet_coefficient_loss: 2.3909 - val_r2_inet_lambda_fv_loss: -0.2308 - val_mae_inet_coefficient_loss: 33.8585 - val_mae_inet_lambda_fv_loss: 19.0931\n",
      "Epoch 34/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 19.6208 - r2_inet_coefficient_loss: 2.1675 - r2_inet_lambda_fv_loss: -0.1803 - mae_inet_coefficient_loss: 34.2306 - mae_inet_lambda_fv_loss: 19.6228 - val_loss: 18.9153 - val_r2_inet_coefficient_loss: 2.5345 - val_r2_inet_lambda_fv_loss: -0.2374 - val_mae_inet_coefficient_loss: 35.3777 - val_mae_inet_lambda_fv_loss: 18.8695\n",
      "Epoch 35/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 19.5449 - r2_inet_coefficient_loss: 2.3344 - r2_inet_lambda_fv_loss: -0.1821 - mae_inet_coefficient_loss: 35.4297 - mae_inet_lambda_fv_loss: 19.5434 - val_loss: 18.5348 - val_r2_inet_coefficient_loss: 2.3629 - val_r2_inet_lambda_fv_loss: -0.2742 - val_mae_inet_coefficient_loss: 34.4374 - val_mae_inet_lambda_fv_loss: 18.4744\n",
      "Epoch 36/500\n",
      "40/40 [==============================] - 11s 275ms/step - loss: 19.2171 - r2_inet_coefficient_loss: 2.1847 - r2_inet_lambda_fv_loss: -0.2194 - mae_inet_coefficient_loss: 35.1008 - mae_inet_lambda_fv_loss: 19.2206 - val_loss: 18.3835 - val_r2_inet_coefficient_loss: 2.6925 - val_r2_inet_lambda_fv_loss: -0.2579 - val_mae_inet_coefficient_loss: 35.3627 - val_mae_inet_lambda_fv_loss: 18.4000\n",
      "Epoch 37/500\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 19.1417 - r2_inet_coefficient_loss: 2.3666 - r2_inet_lambda_fv_loss: -0.2247 - mae_inet_coefficient_loss: 35.4764 - mae_inet_lambda_fv_loss: 19.1423 - val_loss: 18.1769 - val_r2_inet_coefficient_loss: 2.7086 - val_r2_inet_lambda_fv_loss: -0.2884 - val_mae_inet_coefficient_loss: 35.2260 - val_mae_inet_lambda_fv_loss: 18.1749\n",
      "Epoch 38/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 19.0347 - r2_inet_coefficient_loss: 2.4004 - r2_inet_lambda_fv_loss: -0.2422 - mae_inet_coefficient_loss: 36.1397 - mae_inet_lambda_fv_loss: 19.0353 - val_loss: 18.3096 - val_r2_inet_coefficient_loss: 2.8186 - val_r2_inet_lambda_fv_loss: -0.2803 - val_mae_inet_coefficient_loss: 35.8935 - val_mae_inet_lambda_fv_loss: 18.3119\n",
      "Epoch 39/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 18.8645 - r2_inet_coefficient_loss: 2.3700 - r2_inet_lambda_fv_loss: -0.2506 - mae_inet_coefficient_loss: 35.6815 - mae_inet_lambda_fv_loss: 18.8658 - val_loss: 18.2412 - val_r2_inet_coefficient_loss: 2.6991 - val_r2_inet_lambda_fv_loss: -0.2866 - val_mae_inet_coefficient_loss: 35.2220 - val_mae_inet_lambda_fv_loss: 18.2532\n",
      "Epoch 40/500\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 18.8319 - r2_inet_coefficient_loss: 2.3940 - r2_inet_lambda_fv_loss: -0.2418 - mae_inet_coefficient_loss: 35.8093 - mae_inet_lambda_fv_loss: 18.8365 - val_loss: 18.3112 - val_r2_inet_coefficient_loss: 2.7888 - val_r2_inet_lambda_fv_loss: -0.2835 - val_mae_inet_coefficient_loss: 35.3845 - val_mae_inet_lambda_fv_loss: 18.2955\n",
      "Epoch 41/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 19.0238 - r2_inet_coefficient_loss: 2.4444 - r2_inet_lambda_fv_loss: -0.2255 - mae_inet_coefficient_loss: 35.8346 - mae_inet_lambda_fv_loss: 19.0327 - val_loss: 18.4141 - val_r2_inet_coefficient_loss: 2.7395 - val_r2_inet_lambda_fv_loss: -0.2783 - val_mae_inet_coefficient_loss: 35.4959 - val_mae_inet_lambda_fv_loss: 18.3803\n",
      "Epoch 42/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 19.2057 - r2_inet_coefficient_loss: 2.3997 - r2_inet_lambda_fv_loss: -0.2125 - mae_inet_coefficient_loss: 35.7147 - mae_inet_lambda_fv_loss: 19.2055 - val_loss: 18.7377 - val_r2_inet_coefficient_loss: 2.7162 - val_r2_inet_lambda_fv_loss: -0.2595 - val_mae_inet_coefficient_loss: 35.5781 - val_mae_inet_lambda_fv_loss: 18.6757\n",
      "Epoch 43/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 19.2107 - r2_inet_coefficient_loss: 2.3867 - r2_inet_lambda_fv_loss: -0.2193 - mae_inet_coefficient_loss: 36.0227 - mae_inet_lambda_fv_loss: 19.2077 - val_loss: 18.6613 - val_r2_inet_coefficient_loss: 2.7897 - val_r2_inet_lambda_fv_loss: -0.2622 - val_mae_inet_coefficient_loss: 35.6491 - val_mae_inet_lambda_fv_loss: 18.5826\n",
      "Epoch 44/500\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 19.1496 - r2_inet_coefficient_loss: 2.4471 - r2_inet_lambda_fv_loss: -0.2260 - mae_inet_coefficient_loss: 36.0684 - mae_inet_lambda_fv_loss: 19.1532 - val_loss: 18.6257 - val_r2_inet_coefficient_loss: 2.7541 - val_r2_inet_lambda_fv_loss: -0.2636 - val_mae_inet_coefficient_loss: 35.4860 - val_mae_inet_lambda_fv_loss: 18.5532\n",
      "Epoch 45/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 19.3384 - r2_inet_coefficient_loss: 2.4703 - r2_inet_lambda_fv_loss: -0.2264 - mae_inet_coefficient_loss: 36.0506 - mae_inet_lambda_fv_loss: 19.3389 - val_loss: 18.8194 - val_r2_inet_coefficient_loss: 2.5790 - val_r2_inet_lambda_fv_loss: -0.2556 - val_mae_inet_coefficient_loss: 34.7218 - val_mae_inet_lambda_fv_loss: 18.7434\n",
      "Epoch 46/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 19.2384 - r2_inet_coefficient_loss: 2.3864 - r2_inet_lambda_fv_loss: -0.2244 - mae_inet_coefficient_loss: 35.7278 - mae_inet_lambda_fv_loss: 19.2371 - val_loss: 19.2919 - val_r2_inet_coefficient_loss: 2.7117 - val_r2_inet_lambda_fv_loss: -0.2182 - val_mae_inet_coefficient_loss: 34.7797 - val_mae_inet_lambda_fv_loss: 19.1724\n",
      "Epoch 47/500\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 19.5511 - r2_inet_coefficient_loss: 2.4142 - r2_inet_lambda_fv_loss: -0.1961 - mae_inet_coefficient_loss: 35.6201 - mae_inet_lambda_fv_loss: 19.5545 - val_loss: 19.1634 - val_r2_inet_coefficient_loss: 2.7682 - val_r2_inet_lambda_fv_loss: -0.2206 - val_mae_inet_coefficient_loss: 35.1402 - val_mae_inet_lambda_fv_loss: 19.0423\n",
      "Epoch 48/500\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 19.7457 - r2_inet_coefficient_loss: 2.3998 - r2_inet_lambda_fv_loss: -0.1990 - mae_inet_coefficient_loss: 35.7152 - mae_inet_lambda_fv_loss: 19.7467 - val_loss: 19.7984 - val_r2_inet_coefficient_loss: 2.9767 - val_r2_inet_lambda_fv_loss: -0.1765 - val_mae_inet_coefficient_loss: 34.9061 - val_mae_inet_lambda_fv_loss: 19.7134\n",
      "Epoch 49/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 20.6433 - r2_inet_coefficient_loss: 2.7046 - r2_inet_lambda_fv_loss: -0.1208 - mae_inet_coefficient_loss: 35.8616 - mae_inet_lambda_fv_loss: 20.6506 - val_loss: 20.3438 - val_r2_inet_coefficient_loss: 2.8754 - val_r2_inet_lambda_fv_loss: -0.1386 - val_mae_inet_coefficient_loss: 34.4518 - val_mae_inet_lambda_fv_loss: 20.2647\n",
      "Epoch 50/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 20.8417 - r2_inet_coefficient_loss: 2.8331 - r2_inet_lambda_fv_loss: -0.1135 - mae_inet_coefficient_loss: 35.6460 - mae_inet_lambda_fv_loss: 20.8418 - val_loss: 20.1549 - val_r2_inet_coefficient_loss: 2.8793 - val_r2_inet_lambda_fv_loss: -0.1661 - val_mae_inet_coefficient_loss: 34.0933 - val_mae_inet_lambda_fv_loss: 20.0815\n",
      "Epoch 51/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 20.7998 - r2_inet_coefficient_loss: 2.6986 - r2_inet_lambda_fv_loss: -0.1114 - mae_inet_coefficient_loss: 35.3493 - mae_inet_lambda_fv_loss: 20.8054 - val_loss: 20.2471 - val_r2_inet_coefficient_loss: 2.6428 - val_r2_inet_lambda_fv_loss: -0.1618 - val_mae_inet_coefficient_loss: 33.7928 - val_mae_inet_lambda_fv_loss: 20.1537\n",
      "Epoch 52/500\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 20.8863 - r2_inet_coefficient_loss: 2.4564 - r2_inet_lambda_fv_loss: -0.0878 - mae_inet_coefficient_loss: 34.8893 - mae_inet_lambda_fv_loss: 20.8910 - val_loss: 20.5198 - val_r2_inet_coefficient_loss: 2.5367 - val_r2_inet_lambda_fv_loss: -0.1415 - val_mae_inet_coefficient_loss: 33.5337 - val_mae_inet_lambda_fv_loss: 20.4587\n",
      "Epoch 53/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 20.9187 - r2_inet_coefficient_loss: 2.4519 - r2_inet_lambda_fv_loss: -0.0932 - mae_inet_coefficient_loss: 34.1341 - mae_inet_lambda_fv_loss: 20.9211 - val_loss: 20.6037 - val_r2_inet_coefficient_loss: 2.0154 - val_r2_inet_lambda_fv_loss: -0.1211 - val_mae_inet_coefficient_loss: 32.1054 - val_mae_inet_lambda_fv_loss: 20.5305\n",
      "Epoch 54/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 20.9972 - r2_inet_coefficient_loss: 2.3528 - r2_inet_lambda_fv_loss: -0.0656 - mae_inet_coefficient_loss: 33.7829 - mae_inet_lambda_fv_loss: 20.9996 - val_loss: 20.3022 - val_r2_inet_coefficient_loss: 2.4382 - val_r2_inet_lambda_fv_loss: -0.1351 - val_mae_inet_coefficient_loss: 33.0763 - val_mae_inet_lambda_fv_loss: 20.1874\n",
      "Epoch 55/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 20.9562 - r2_inet_coefficient_loss: 2.4966 - r2_inet_lambda_fv_loss: -0.1019 - mae_inet_coefficient_loss: 34.2105 - mae_inet_lambda_fv_loss: 20.9579 - val_loss: 20.3096 - val_r2_inet_coefficient_loss: 2.3997 - val_r2_inet_lambda_fv_loss: -0.1306 - val_mae_inet_coefficient_loss: 32.6635 - val_mae_inet_lambda_fv_loss: 20.2200\n",
      "Epoch 56/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 20.8436 - r2_inet_coefficient_loss: 2.4143 - r2_inet_lambda_fv_loss: -0.1158 - mae_inet_coefficient_loss: 33.5573 - mae_inet_lambda_fv_loss: 20.8390 - val_loss: 20.2955 - val_r2_inet_coefficient_loss: 2.5978 - val_r2_inet_lambda_fv_loss: -0.1254 - val_mae_inet_coefficient_loss: 33.0747 - val_mae_inet_lambda_fv_loss: 20.2101\n",
      "Epoch 57/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 20.9057 - r2_inet_coefficient_loss: 2.5833 - r2_inet_lambda_fv_loss: -0.0891 - mae_inet_coefficient_loss: 34.2353 - mae_inet_lambda_fv_loss: 20.9098 - val_loss: 20.2591 - val_r2_inet_coefficient_loss: 2.3483 - val_r2_inet_lambda_fv_loss: -0.1370 - val_mae_inet_coefficient_loss: 32.2079 - val_mae_inet_lambda_fv_loss: 20.2096\n",
      "Epoch 58/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 20.6100 - r2_inet_coefficient_loss: 2.5644 - r2_inet_lambda_fv_loss: -0.1075 - mae_inet_coefficient_loss: 34.0885 - mae_inet_lambda_fv_loss: 20.6098 - val_loss: 20.0141 - val_r2_inet_coefficient_loss: 2.5392 - val_r2_inet_lambda_fv_loss: -0.1478 - val_mae_inet_coefficient_loss: 33.3867 - val_mae_inet_lambda_fv_loss: 19.9508\n",
      "Epoch 59/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 20.5346 - r2_inet_coefficient_loss: 2.6632 - r2_inet_lambda_fv_loss: -0.1163 - mae_inet_coefficient_loss: 34.5618 - mae_inet_lambda_fv_loss: 20.5321 - val_loss: 20.0310 - val_r2_inet_coefficient_loss: 2.6273 - val_r2_inet_lambda_fv_loss: -0.1469 - val_mae_inet_coefficient_loss: 32.9000 - val_mae_inet_lambda_fv_loss: 19.9585\n",
      "Epoch 60/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 20.5822 - r2_inet_coefficient_loss: 2.7789 - r2_inet_lambda_fv_loss: -0.1210 - mae_inet_coefficient_loss: 34.9646 - mae_inet_lambda_fv_loss: 20.5822 - val_loss: 19.8980 - val_r2_inet_coefficient_loss: 2.6318 - val_r2_inet_lambda_fv_loss: -0.1544 - val_mae_inet_coefficient_loss: 33.5876 - val_mae_inet_lambda_fv_loss: 19.8753\n",
      "Epoch 61/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 20.4002 - r2_inet_coefficient_loss: 2.6532 - r2_inet_lambda_fv_loss: -0.1135 - mae_inet_coefficient_loss: 34.7069 - mae_inet_lambda_fv_loss: 20.3994 - val_loss: 19.7375 - val_r2_inet_coefficient_loss: 2.5482 - val_r2_inet_lambda_fv_loss: -0.1540 - val_mae_inet_coefficient_loss: 33.9193 - val_mae_inet_lambda_fv_loss: 19.7006\n",
      "Epoch 62/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 20.2347 - r2_inet_coefficient_loss: 2.7123 - r2_inet_lambda_fv_loss: -0.1068 - mae_inet_coefficient_loss: 34.8622 - mae_inet_lambda_fv_loss: 20.2339 - val_loss: 19.5902 - val_r2_inet_coefficient_loss: 2.5371 - val_r2_inet_lambda_fv_loss: -0.1640 - val_mae_inet_coefficient_loss: 33.7984 - val_mae_inet_lambda_fv_loss: 19.5850\n",
      "Epoch 63/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 20.1563 - r2_inet_coefficient_loss: 2.7239 - r2_inet_lambda_fv_loss: -0.1482 - mae_inet_coefficient_loss: 34.8346 - mae_inet_lambda_fv_loss: 20.1577 - val_loss: 19.6663 - val_r2_inet_coefficient_loss: 2.3419 - val_r2_inet_lambda_fv_loss: -0.1574 - val_mae_inet_coefficient_loss: 33.4187 - val_mae_inet_lambda_fv_loss: 19.6609\n",
      "Epoch 64/500\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 20.2417 - r2_inet_coefficient_loss: 2.4520 - r2_inet_lambda_fv_loss: -0.1407 - mae_inet_coefficient_loss: 34.4428 - mae_inet_lambda_fv_loss: 20.2406 - val_loss: 19.5334 - val_r2_inet_coefficient_loss: 2.3594 - val_r2_inet_lambda_fv_loss: -0.1612 - val_mae_inet_coefficient_loss: 33.5079 - val_mae_inet_lambda_fv_loss: 19.5041\n",
      "Epoch 65/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 19.9843 - r2_inet_coefficient_loss: 2.4584 - r2_inet_lambda_fv_loss: -0.1390 - mae_inet_coefficient_loss: 34.3408 - mae_inet_lambda_fv_loss: 19.9858 - val_loss: 19.4741 - val_r2_inet_coefficient_loss: 2.2100 - val_r2_inet_lambda_fv_loss: -0.1748 - val_mae_inet_coefficient_loss: 32.8727 - val_mae_inet_lambda_fv_loss: 19.5222\n",
      "Epoch 66/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 20.0479 - r2_inet_coefficient_loss: 2.1995 - r2_inet_lambda_fv_loss: -0.1488 - mae_inet_coefficient_loss: 33.9644 - mae_inet_lambda_fv_loss: 20.0455 - val_loss: 19.5009 - val_r2_inet_coefficient_loss: 2.3092 - val_r2_inet_lambda_fv_loss: -0.1687 - val_mae_inet_coefficient_loss: 33.2074 - val_mae_inet_lambda_fv_loss: 19.5089\n",
      "Epoch 67/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 20.0622 - r2_inet_coefficient_loss: 2.4404 - r2_inet_lambda_fv_loss: -0.1353 - mae_inet_coefficient_loss: 34.6492 - mae_inet_lambda_fv_loss: 20.0647 - val_loss: 19.3150 - val_r2_inet_coefficient_loss: 2.3859 - val_r2_inet_lambda_fv_loss: -0.1903 - val_mae_inet_coefficient_loss: 33.5760 - val_mae_inet_lambda_fv_loss: 19.2913\n",
      "Epoch 68/500\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 19.9724 - r2_inet_coefficient_loss: 2.4942 - r2_inet_lambda_fv_loss: -0.1453 - mae_inet_coefficient_loss: 34.5151 - mae_inet_lambda_fv_loss: 19.9763 - val_loss: 19.8623 - val_r2_inet_coefficient_loss: 2.1485 - val_r2_inet_lambda_fv_loss: -0.1758 - val_mae_inet_coefficient_loss: 32.3602 - val_mae_inet_lambda_fv_loss: 19.8884\n",
      "Epoch 69/500\n",
      "40/40 [==============================] - 11s 268ms/step - loss: 20.2958 - r2_inet_coefficient_loss: 2.4047 - r2_inet_lambda_fv_loss: -0.1101 - mae_inet_coefficient_loss: 34.5793 - mae_inet_lambda_fv_loss: 20.2969 - val_loss: 19.5902 - val_r2_inet_coefficient_loss: 2.5681 - val_r2_inet_lambda_fv_loss: -0.1620 - val_mae_inet_coefficient_loss: 34.2017 - val_mae_inet_lambda_fv_loss: 19.6361\n",
      "Epoch 70/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 19.9738 - r2_inet_coefficient_loss: 2.4652 - r2_inet_lambda_fv_loss: -0.1424 - mae_inet_coefficient_loss: 34.7452 - mae_inet_lambda_fv_loss: 19.9734 - val_loss: 19.4085 - val_r2_inet_coefficient_loss: 2.5238 - val_r2_inet_lambda_fv_loss: -0.1682 - val_mae_inet_coefficient_loss: 33.8657 - val_mae_inet_lambda_fv_loss: 19.4062\n",
      "Epoch 71/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 19.8330 - r2_inet_coefficient_loss: 2.5392 - r2_inet_lambda_fv_loss: -0.1691 - mae_inet_coefficient_loss: 34.7753 - mae_inet_lambda_fv_loss: 19.8305 - val_loss: 19.4300 - val_r2_inet_coefficient_loss: 2.3908 - val_r2_inet_lambda_fv_loss: -0.1806 - val_mae_inet_coefficient_loss: 33.4320 - val_mae_inet_lambda_fv_loss: 19.4252\n",
      "Epoch 72/500\n",
      "40/40 [==============================] - 11s 268ms/step - loss: 19.6970 - r2_inet_coefficient_loss: 2.3184 - r2_inet_lambda_fv_loss: -0.1643 - mae_inet_coefficient_loss: 34.3592 - mae_inet_lambda_fv_loss: 19.6937 - val_loss: 19.2213 - val_r2_inet_coefficient_loss: 2.4137 - val_r2_inet_lambda_fv_loss: -0.1862 - val_mae_inet_coefficient_loss: 33.5451 - val_mae_inet_lambda_fv_loss: 19.2100\n",
      "Epoch 73/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 19.8311 - r2_inet_coefficient_loss: 2.3982 - r2_inet_lambda_fv_loss: -0.1675 - mae_inet_coefficient_loss: 34.9533 - mae_inet_lambda_fv_loss: 19.8336 - val_loss: 19.0956 - val_r2_inet_coefficient_loss: 2.5138 - val_r2_inet_lambda_fv_loss: -0.1915 - val_mae_inet_coefficient_loss: 34.5637 - val_mae_inet_lambda_fv_loss: 19.1185\n",
      "Epoch 74/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 19.7020 - r2_inet_coefficient_loss: 2.4783 - r2_inet_lambda_fv_loss: -0.1639 - mae_inet_coefficient_loss: 35.5298 - mae_inet_lambda_fv_loss: 19.7007 - val_loss: 19.1542 - val_r2_inet_coefficient_loss: 2.4735 - val_r2_inet_lambda_fv_loss: -0.1828 - val_mae_inet_coefficient_loss: 34.0988 - val_mae_inet_lambda_fv_loss: 19.1712\n",
      "Epoch 75/500\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 19.6952 - r2_inet_coefficient_loss: 2.3580 - r2_inet_lambda_fv_loss: -0.1416 - mae_inet_coefficient_loss: 34.8805 - mae_inet_lambda_fv_loss: 19.6920 - val_loss: 19.1660 - val_r2_inet_coefficient_loss: 2.5331 - val_r2_inet_lambda_fv_loss: -0.1943 - val_mae_inet_coefficient_loss: 34.4461 - val_mae_inet_lambda_fv_loss: 19.1660\n",
      "Epoch 76/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 19.7018 - r2_inet_coefficient_loss: 2.4635 - r2_inet_lambda_fv_loss: -0.1446 - mae_inet_coefficient_loss: 35.1304 - mae_inet_lambda_fv_loss: 19.7021 - val_loss: 19.1695 - val_r2_inet_coefficient_loss: 2.2347 - val_r2_inet_lambda_fv_loss: -0.1954 - val_mae_inet_coefficient_loss: 33.8162 - val_mae_inet_lambda_fv_loss: 19.1489\n",
      "Epoch 77/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 19.6043 - r2_inet_coefficient_loss: 2.2469 - r2_inet_lambda_fv_loss: -0.1686 - mae_inet_coefficient_loss: 34.6717 - mae_inet_lambda_fv_loss: 19.6092 - val_loss: 18.9670 - val_r2_inet_coefficient_loss: 2.3902 - val_r2_inet_lambda_fv_loss: -0.2005 - val_mae_inet_coefficient_loss: 34.8007 - val_mae_inet_lambda_fv_loss: 18.9362\n",
      "Epoch 78/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 19.6975 - r2_inet_coefficient_loss: 2.4444 - r2_inet_lambda_fv_loss: -0.1489 - mae_inet_coefficient_loss: 34.8072 - mae_inet_lambda_fv_loss: 19.6968 - val_loss: 19.3226 - val_r2_inet_coefficient_loss: 2.4190 - val_r2_inet_lambda_fv_loss: -0.1855 - val_mae_inet_coefficient_loss: 33.7366 - val_mae_inet_lambda_fv_loss: 19.3280\n",
      "Epoch 79/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 19.8186 - r2_inet_coefficient_loss: 2.2433 - r2_inet_lambda_fv_loss: -0.1411 - mae_inet_coefficient_loss: 34.2222 - mae_inet_lambda_fv_loss: 19.8213 - val_loss: 19.1276 - val_r2_inet_coefficient_loss: 2.4409 - val_r2_inet_lambda_fv_loss: -0.1938 - val_mae_inet_coefficient_loss: 34.1889 - val_mae_inet_lambda_fv_loss: 19.0966\n",
      "Epoch 80/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 19.6549 - r2_inet_coefficient_loss: 2.3442 - r2_inet_lambda_fv_loss: -0.1774 - mae_inet_coefficient_loss: 34.6855 - mae_inet_lambda_fv_loss: 19.6588 - val_loss: 19.2242 - val_r2_inet_coefficient_loss: 2.1492 - val_r2_inet_lambda_fv_loss: -0.1909 - val_mae_inet_coefficient_loss: 33.5559 - val_mae_inet_lambda_fv_loss: 19.1831\n",
      "Epoch 81/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 19.7511 - r2_inet_coefficient_loss: 2.1171 - r2_inet_lambda_fv_loss: -0.1495 - mae_inet_coefficient_loss: 34.0274 - mae_inet_lambda_fv_loss: 19.7527 - val_loss: 19.2647 - val_r2_inet_coefficient_loss: 2.3489 - val_r2_inet_lambda_fv_loss: -0.1834 - val_mae_inet_coefficient_loss: 33.4959 - val_mae_inet_lambda_fv_loss: 19.2401\n",
      "Epoch 82/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 19.7241 - r2_inet_coefficient_loss: 2.2813 - r2_inet_lambda_fv_loss: -0.1643 - mae_inet_coefficient_loss: 34.5383 - mae_inet_lambda_fv_loss: 19.7250 - val_loss: 19.1926 - val_r2_inet_coefficient_loss: 2.5333 - val_r2_inet_lambda_fv_loss: -0.1838 - val_mae_inet_coefficient_loss: 33.8174 - val_mae_inet_lambda_fv_loss: 19.2052\n",
      "Epoch 83/500\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 19.8121 - r2_inet_coefficient_loss: 2.1915 - r2_inet_lambda_fv_loss: -0.1620 - mae_inet_coefficient_loss: 34.1617 - mae_inet_lambda_fv_loss: 19.8131 - val_loss: 19.3307 - val_r2_inet_coefficient_loss: 2.4916 - val_r2_inet_lambda_fv_loss: -0.1764 - val_mae_inet_coefficient_loss: 33.6016 - val_mae_inet_lambda_fv_loss: 19.3205\n",
      "Epoch 84/500\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 19.5776 - r2_inet_coefficient_loss: 2.2470 - r2_inet_lambda_fv_loss: -0.1757 - mae_inet_coefficient_loss: 34.0483 - mae_inet_lambda_fv_loss: 19.5779 - val_loss: 19.1955 - val_r2_inet_coefficient_loss: 2.0811 - val_r2_inet_lambda_fv_loss: -0.1986 - val_mae_inet_coefficient_loss: 32.8289 - val_mae_inet_lambda_fv_loss: 19.1953\n",
      "Epoch 85/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 19.3745 - r2_inet_coefficient_loss: 1.9872 - r2_inet_lambda_fv_loss: -0.2149 - mae_inet_coefficient_loss: 33.8350 - mae_inet_lambda_fv_loss: 19.3728 - val_loss: 18.8152 - val_r2_inet_coefficient_loss: 2.0122 - val_r2_inet_lambda_fv_loss: -0.2411 - val_mae_inet_coefficient_loss: 33.3776 - val_mae_inet_lambda_fv_loss: 18.8566\n",
      "Epoch 86/500\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 19.2984 - r2_inet_coefficient_loss: 1.9230 - r2_inet_lambda_fv_loss: -0.2386 - mae_inet_coefficient_loss: 33.6536 - mae_inet_lambda_fv_loss: 19.2952 - val_loss: 18.8501 - val_r2_inet_coefficient_loss: 1.8600 - val_r2_inet_lambda_fv_loss: -0.2180 - val_mae_inet_coefficient_loss: 32.5114 - val_mae_inet_lambda_fv_loss: 18.9001\n",
      "Epoch 87/500\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 19.2121 - r2_inet_coefficient_loss: 1.8913 - r2_inet_lambda_fv_loss: -0.2034 - mae_inet_coefficient_loss: 33.5565 - mae_inet_lambda_fv_loss: 19.2128 - val_loss: 18.9349 - val_r2_inet_coefficient_loss: 1.8551 - val_r2_inet_lambda_fv_loss: -0.2418 - val_mae_inet_coefficient_loss: 32.7976 - val_mae_inet_lambda_fv_loss: 18.9441\n",
      "Training Time: 0:15:54\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:01\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------- PREDICT INET ------------------------------------------------------\n",
      "Predict Time: 0:00:00\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=10)]: Done   2 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=10)]: Done   3 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=10)]: Done   4 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=10)]: Done   6 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=10)]: Done   7 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=10)]: Done   8 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=10)]: Done   9 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=10)]: Done  10 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=10)]: Done  11 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=10)]: Done  13 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=10)]: Done  14 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=10)]: Done  15 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=10)]: Done  16 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=10)]: Done  17 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=10)]: Done  18 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=10)]: Done  19 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=10)]: Done  20 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=10)]: Done  22 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=10)]: Done  23 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=10)]: Done  24 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=10)]: Done  25 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=10)]: Done  26 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=10)]: Done  27 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=10)]: Done  28 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=10)]: Done  29 tasks      | elapsed: 20.7min\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=10)]: Done  31 tasks      | elapsed: 21.9min\n",
      "[Parallel(n_jobs=10)]: Done  32 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=10)]: Done  33 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=10)]: Done  34 tasks      | elapsed: 23.5min\n",
      "[Parallel(n_jobs=10)]: Done  35 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=10)]: Done  36 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=10)]: Done  37 tasks      | elapsed: 24.1min\n",
      "[Parallel(n_jobs=10)]: Done  38 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=10)]: Done  39 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=10)]: Done  40 tasks      | elapsed: 27.2min\n",
      "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed: 28.6min\n",
      "[Parallel(n_jobs=10)]: Done  42 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=10)]: Done  43 tasks      | elapsed: 29.0min\n",
      "[Parallel(n_jobs=10)]: Done  44 tasks      | elapsed: 29.8min\n",
      "[Parallel(n_jobs=10)]: Done  45 tasks      | elapsed: 30.1min\n",
      "[Parallel(n_jobs=10)]: Done  46 tasks      | elapsed: 30.2min\n",
      "[Parallel(n_jobs=10)]: Done  47 tasks      | elapsed: 30.6min\n",
      "[Parallel(n_jobs=10)]: Done  48 tasks      | elapsed: 30.6min\n",
      "[Parallel(n_jobs=10)]: Done  49 tasks      | elapsed: 31.5min\n",
      "[Parallel(n_jobs=10)]: Done  50 tasks      | elapsed: 33.6min\n",
      "[Parallel(n_jobs=10)]: Done  51 tasks      | elapsed: 34.5min\n",
      "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed: 34.7min\n",
      "[Parallel(n_jobs=10)]: Done  53 tasks      | elapsed: 34.8min\n",
      "[Parallel(n_jobs=10)]: Done  54 tasks      | elapsed: 35.4min\n",
      "[Parallel(n_jobs=10)]: Done  55 tasks      | elapsed: 35.6min\n",
      "[Parallel(n_jobs=10)]: Done  56 tasks      | elapsed: 36.1min\n",
      "[Parallel(n_jobs=10)]: Done  57 tasks      | elapsed: 36.1min\n",
      "[Parallel(n_jobs=10)]: Done  58 tasks      | elapsed: 36.2min\n",
      "[Parallel(n_jobs=10)]: Done  59 tasks      | elapsed: 39.4min\n",
      "[Parallel(n_jobs=10)]: Done  60 tasks      | elapsed: 39.6min\n",
      "[Parallel(n_jobs=10)]: Done  61 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=10)]: Done  62 tasks      | elapsed: 40.4min\n",
      "[Parallel(n_jobs=10)]: Done  63 tasks      | elapsed: 40.8min\n",
      "[Parallel(n_jobs=10)]: Done  64 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=10)]: Done  66 tasks      | elapsed: 42.3min\n",
      "[Parallel(n_jobs=10)]: Done  67 tasks      | elapsed: 43.5min\n",
      "[Parallel(n_jobs=10)]: Done  68 tasks      | elapsed: 44.9min\n",
      "[Parallel(n_jobs=10)]: Done  69 tasks      | elapsed: 45.0min\n",
      "[Parallel(n_jobs=10)]: Done  70 tasks      | elapsed: 45.1min\n",
      "[Parallel(n_jobs=10)]: Done  71 tasks      | elapsed: 45.1min\n",
      "[Parallel(n_jobs=10)]: Done  72 tasks      | elapsed: 47.1min\n",
      "[Parallel(n_jobs=10)]: Done  73 tasks      | elapsed: 47.3min\n",
      "[Parallel(n_jobs=10)]: Done  74 tasks      | elapsed: 47.7min\n",
      "[Parallel(n_jobs=10)]: Done  75 tasks      | elapsed: 48.0min\n",
      "[Parallel(n_jobs=10)]: Done  76 tasks      | elapsed: 48.4min\n",
      "[Parallel(n_jobs=10)]: Done  77 tasks      | elapsed: 50.6min\n",
      "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed: 51.3min\n",
      "[Parallel(n_jobs=10)]: Done  79 tasks      | elapsed: 51.4min\n",
      "[Parallel(n_jobs=10)]: Done  80 tasks      | elapsed: 51.6min\n",
      "[Parallel(n_jobs=10)]: Done  81 tasks      | elapsed: 51.9min\n",
      "[Parallel(n_jobs=10)]: Done  91 out of 100 | elapsed: 58.6min remaining:  5.8min\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed: 62.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Regression Optimization Time: 1:02:52\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metamodel_poly\n",
      "Exit <class 'KeyError'>\n",
      "metamodel_functions\n",
      "Exit <class 'KeyError'>\n",
      "metamodel_functions_no_GD\n",
      "Exit <class 'KeyError'>\n",
      "symbolic_regression_functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:   39.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_network_polynomials\n",
      "Exit <class 'KeyError'>\n",
      "FV Calculation Time: 0:00:52\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------\n",
      "lambda_preds_VS_target_polynomials\n",
      "lambda_preds_VS_lstsq_lambda_pred_polynomials\n",
      "lambda_preds_VS_lstsq_target_polynomials\n",
      "lambda_preds_VS_inet_polynomials\n",
      "lambda_preds_VS_symbolic_regression_functions\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c5d89a0a4fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdistrib_dict_test_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_interpretation_net_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_train_dataset_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                                                    \u001b[0mlambda_net_valid_dataset_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                    lambda_net_test_dataset_list)\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/InterpretationNet.py\u001b[0m in \u001b[0;36mcalculate_interpretation_net_results\u001b[0;34m(lambda_net_train_dataset_list, lambda_net_valid_dataset_list, lambda_net_test_dataset_list)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfunction_values_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_values_test_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mscores_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistrib_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_all_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_values_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mscores_test_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mdistrib_dict_test_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistrib_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/InterpretationNet.py\u001b[0m in \u001b[0;36mevaluate_all_predictions\u001b[0;34m(function_value_dict, polynomial_dict)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0mevaluation_key_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m         evaluation_scores, evaluation_distrib = evaluate_interpretation_net(polynomials_1, \n\u001b[0m\u001b[1;32m   1169\u001b[0m                                                                             \u001b[0mpolynomials_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                                                                             \u001b[0mfunction_values_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/metrics.py\u001b[0m in \u001b[0;36mevaluate_interpretation_net\u001b[0;34m(function_1_coefficients, function_2_coefficients, function_1_fv, function_2_fv)\u001b[0m\n\u001b[1;32m    970\u001b[0m     \u001b[0mmape_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_percentage_error_function_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_1_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_2_fv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m     \u001b[0mr2_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score_function_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_1_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_2_fv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0mraae_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_absolute_average_error_function_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_1_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_2_fv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0mrmae_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_maximum_average_error_function_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_1_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_2_fv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/metrics.py\u001b[0m in \u001b[0;36mr2_score_function_values\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0mresult_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrue_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0mresult_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \"\"\"\n\u001b[0;32m--> 676\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    677\u001b[0m         y_true, y_pred, multioutput)\n\u001b[1;32m    678\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "(history_list, \n",
    "\n",
    "#scores_valid_list,\n",
    "scores_test_list, \n",
    "\n",
    "#function_values_valid_list, \n",
    "function_values_test_list, \n",
    "\n",
    "#polynomial_dict_valid_list,\n",
    "polynomial_dict_test_list,\n",
    "\n",
    "#distrib_dict_valid_list,\n",
    "distrib_dict_test_list,\n",
    "\n",
    "model_list) = calculate_interpretation_net_results(lambda_net_train_dataset_list, \n",
    "                                                   lambda_net_valid_dataset_list, \n",
    "                                                   lambda_net_test_dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_net_output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(interpretation_net_output_monomials+1)*sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list[-1]['loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list[-1]['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_dict_test_list[-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize = tf.constant([float(i) for i in range(interpretation_net_output_shape)])\n",
    "\n",
    "if interpretation_net_output_monomials != None:\n",
    "    poly_optimize_coeffs = poly_optimize[:interpretation_net_output_monomials]\n",
    "\n",
    "    poly_optimize_identifiers_list = []\n",
    "    for i in range(interpretation_net_output_monomials):\n",
    "        poly_optimize_identifiers = tf.math.softmax(poly_optimize[sparsity*i+interpretation_net_output_monomials:sparsity*(i+1)+interpretation_net_output_monomials])\n",
    "        poly_optimize_identifiers_list.append(poly_optimize_identifiers)\n",
    "    poly_optimize_identifiers_list = tf.keras.backend.flatten(poly_optimize_identifiers_list)\n",
    "    poly_optimize = tf.concat([poly_optimize_coeffs, poly_optimize_identifiers_list], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if evaluate_with_real_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
