{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:45:43.888662Z",
     "start_time": "2020-12-14T08:45:43.881615Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:45:43.960316Z",
     "start_time": "2020-12-14T08:45:43.891916Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 10000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 10  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "\n",
    "fixed_seed_lambda_training = False\n",
    "fixed_initialization_lambda_training = True\n",
    "number_different_lambda_trainings = 200\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:45:44.020143Z",
     "start_time": "2020-12-14T08:45:43.962887Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if each_epochs_save != None:\n",
    "    epochs_save_range = range(1, epochs//each_epochs_save+1) if each_epochs_save == 1 else range(epochs//each_epochs_save+1)\n",
    "else:\n",
    "    epochs_save_range = None\n",
    "    \n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_' + str(number_different_lambda_trainings) + '-FixedSeed'\n",
    "else:\n",
    "    seed_shuffle_string = '_NoFixedSeed'\n",
    "    \n",
    "if fixed_initialization_lambda_training:\n",
    "    seed_shuffle_string += '_' + str(number_different_lambda_trainings) + '-FixedEvaluation'\n",
    "else:\n",
    "    seed_shuffle_string += '_NoFixedEvaluation'\n",
    "    \n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:05.787594Z",
     "start_time": "2020-12-14T08:45:44.023438Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:05.795628Z",
     "start_time": "2020-12-14T08:46:05.790393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:05.816584Z",
     "start_time": "2020-12-14T08:46:05.797412Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "    \n",
    "    \n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:05.823720Z",
     "start_time": "2020-12-14T08:46:05.818359Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcualate_function_value_with_X_data_entry(coefficient_list, X_data_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "     \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [X_data_value**int(coefficient_multiplier) for coefficient_multiplier, X_data_value in zip(coefficient_multipliers, X_data_entry)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "        \n",
    "    return result, np.append(X_data_entry, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:05.859672Z",
     "start_time": "2020-12-14T08:46:05.825486Z"
    },
    "code_folding": [
     0,
     20,
     43,
     66,
     88,
     91,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:05.932010Z",
     "start_time": "2020-12-14T08:46:05.861459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af60a3d8b3604a6db26e3f0db2ea53b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d71722815ca4aa0bee55510cf3ec7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:17.515588Z",
     "start_time": "2020-12-14T08:46:05.933685Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:21.459550Z",
     "start_time": "2020-12-14T08:46:17.517714Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        \n",
    "        for i in epochs_save_range:\n",
    "            index = i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:21.490106Z",
     "start_time": "2020-12-14T08:46:21.463706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.620 -0.950  0.340 -0.760\n",
       "1 -0.300  0.950 -0.770  0.310\n",
       "2 -0.300  0.280  0.480  0.080\n",
       "3 -0.520  0.980  0.230  0.170\n",
       "4  0.710 -0.920 -0.310  0.260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:21.512872Z",
     "start_time": "2020-12-14T08:46:21.494758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1      2      3\n",
       "0  0.660 0.100 -0.040 -1.000\n",
       "1 -0.970 0.920  0.460  0.990\n",
       "2 -0.280 0.760  0.910 -0.230\n",
       "3 -0.250 0.210 -0.070 -0.300\n",
       "4 -0.330 0.770 -0.360  0.110"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:21.522177Z",
     "start_time": "2020-12-14T08:46:21.516485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:21.528623Z",
     "start_time": "2020-12-14T08:46:21.524089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:21.573702Z",
     "start_time": "2020-12-14T08:46:21.530373Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lambda_net(identifier, \n",
    "                        X_data_real_lambda, \n",
    "                        y_data_real_lambda, \n",
    "                        y_data_pred_lambda, \n",
    "                        y_data_pred_lambda_poly_lstsq, \n",
    "                        y_data_real_lambda_poly_lstsq):\n",
    "    \n",
    "    mae_real_VS_predLambda = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    mae_predLambda_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_realPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    rmse_real_VS_predLambda = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    rmse_predLambda_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_realPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    mape_real_VS_predLambda = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    mape_predLambda_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_realPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)            \n",
    "\n",
    "    r2_real_VS_predLambda = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_predPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    r2_predLambda_VS_predPolyLstsq = np.round(r2_score(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_realPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    raae_real_VS_predLambda = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    raae_predLambda_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_realPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    rmae_real_VS_predLambda = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    rmae_predLambda_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_realPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    fd_real_VS_predLambda = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_predLambda_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_realPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "    dtw_real_VS_predLambda, dtw_complete_real_VS_predLambda = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predLambda = np.round(dtw_real_VS_predLambda, 4)\n",
    "    dtw_real_VS_predPolyLstsq, dtw_complete_real_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predPolyLstsq = np.round(dtw_real_VS_predPolyLstsq, 4)\n",
    "    dtw_predLambda_VS_predPolyLstsq, dtw_complete_predLambda_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_predLambda_VS_predPolyLstsq = np.round(dtw_predLambda_VS_predPolyLstsq, 4)    \n",
    "    dtw_real_VS_realPolyLstsq, dtw_complete_real_VS_realPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_realPolyLstsq = np.round(dtw_real_VS_realPolyLstsq, 4) \n",
    "        \n",
    "    std_data_real_lambda = np.round(np.std(y_data_real_lambda), 4) \n",
    "    std_data_pred_lambda = np.round(np.std(y_data_pred_lambda), 4) \n",
    "    std_data_pred_lambda_poly_lstsq = np.round(np.std(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    std_data_real_lambda_poly_lstsq = np.round(np.std(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    mean_data_real_lambda = np.round(np.mean(y_data_real_lambda), 4) \n",
    "    mean_data_pred_lambda = np.round(np.mean(y_data_pred_lambda), 4) \n",
    "    mean_data_pred_lambda_poly_lstsq = np.round(np.mean(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    mean_data_real_lambda_poly_lstsq = np.round(np.mean(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    return [{\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mae_real_VS_predLambda,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_real_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_predLambda_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mae_real_VS_realPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmse_real_VS_predLambda,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_real_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_predLambda_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmse_real_VS_realPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mape_real_VS_predLambda,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_real_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_predLambda_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mape_real_VS_realPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': r2_real_VS_predLambda,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_real_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_predLambda_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': r2_real_VS_realPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': raae_real_VS_predLambda,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_real_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_predLambda_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': raae_real_VS_realPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmae_real_VS_predLambda,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_real_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_predLambda_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmae_real_VS_realPolyLstsq,\n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': fd_real_VS_predLambda,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_real_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_predLambda_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': fd_real_VS_realPolyLstsq,   \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': dtw_real_VS_predLambda, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_real_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_predLambda_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': dtw_real_VS_realPolyLstsq, \n",
    "            },\n",
    "            {\n",
    "             'STD FV ' + identifier + ' REAL LAMBDA': std_data_real_lambda,\n",
    "             'STD FV ' + identifier + ' PRED LAMBDA': std_data_pred_lambda, \n",
    "             'STD FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': std_data_pred_lambda_poly_lstsq, \n",
    "             'STD FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': std_data_real_lambda_poly_lstsq, \n",
    "            },\n",
    "            {\n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA': mean_data_real_lambda,\n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA': mean_data_pred_lambda, \n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': mean_data_pred_lambda_poly_lstsq, \n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': mean_data_real_lambda_poly_lstsq, \n",
    "            }]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:21.590445Z",
     "start_time": "2020-12-14T08:46:21.575497Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_function_values_from_polynomial(X_data, polynomial):\n",
    "    function_value_list = []\n",
    "    for entry in X_data:\n",
    "        function_value, _ = calcualate_function_value_with_X_data_entry(polynomial, entry)\n",
    "        function_value_list.append(function_value)\n",
    "    function_value_array = np.array(function_value_list).reshape(len(function_value_list), 1)     \n",
    "\n",
    "    return function_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:21.614600Z",
     "start_time": "2020-12-14T08:46:21.592930Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_term_matric_for_lstsq(X_data, polynomial_indices):\n",
    "    term_list_all = []\n",
    "    y = 0\n",
    "    for term in list(polynomial_indices):\n",
    "        term_list = [int(value_mult) for value_mult in term]\n",
    "        term_list_all.append(term_list)\n",
    "    terms_matrix = []\n",
    "    for unknowns in X_data:\n",
    "        terms = []\n",
    "        for term_multipliers in term_list_all:\n",
    "            term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "            terms.append(term_value)\n",
    "        terms_matrix.append(np.array(terms))\n",
    "    terms_matrix = np.array(terms_matrix)\n",
    "    \n",
    "    return terms_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:21.689084Z",
     "start_time": "2020-12-14T08:46:21.617689Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn(lambda_index, X_data_lambda, y_data_real_lambda, polynomial, seed_list, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    current_seed = seed_list[lambda_index%number_different_lambda_trainings]\n",
    "    \n",
    "    if fixed_seed_lambda_training:\n",
    "        random.seed(current_seed)\n",
    "        np.random.seed(current_seed)\n",
    "        if int(tf.__version__[0]) >= 2:\n",
    "            tf.random.set_seed(current_seed)\n",
    "        else:\n",
    "            tf.set_random_seed(current_seed) \n",
    "        \n",
    "    if isinstance(X_data_lambda, pd.DataFrame):\n",
    "        X_data_lambda = X_data_lambda.values\n",
    "    if isinstance(y_data_real_lambda, pd.DataFrame):\n",
    "        y_data_real_lambda = y_data_real_lambda.values\n",
    "                \n",
    "    X_train_lambda_with_valid, X_test_lambda, y_train_real_lambda_with_valid, y_test_real_lambda = train_test_split(X_data_lambda, y_data_real_lambda, test_size=0.25, random_state=current_seed)           \n",
    "    X_train_lambda, X_valid_lambda, y_train_real_lambda, y_valid_real_lambda = train_test_split(X_train_lambda_with_valid, y_train_real_lambda_with_valid, test_size=0.25, random_state=current_seed)           \n",
    "     \n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    #kerase defaults: kernel_initializer='glorot_uniform', bias_initializer='zeros'               \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros')) #1024\n",
    "    else:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1])) #1024\n",
    "        \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        if fixed_initialization_lambda_training:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "    \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_lstsq_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_train_pred_lambda = model.predict(X_train_lambda) \n",
    "        y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "        y_test_pred_lambda = model.predict(X_test_lambda)\n",
    "    \n",
    "        terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                \n",
    "        polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "        y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "        y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "        y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)    \n",
    "        y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)\n",
    "        y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)  \n",
    "        \n",
    "        pred_list = (lambda_index,\n",
    "                     y_train_real_lambda, \n",
    "                     y_train_pred_lambda, \n",
    "                     y_train_pred_lambda_poly_lstsq,\n",
    "                     #y_train_real_lambda_poly_lstsq,\n",
    "                     X_train_lambda, \n",
    "                     y_valid_real_lambda,\n",
    "                     y_valid_pred_lambda, \n",
    "                     y_valid_pred_lambda_poly_lstsq,\n",
    "                     #y_valid_real_lambda_poly_lstsq,\n",
    "                     X_valid_lambda, \n",
    "                     y_test_real_lambda, \n",
    "                     y_test_pred_lambda, \n",
    "                     y_test_pred_lambda_poly_lstsq, \n",
    "                     #y_test_real_lambda_poly_lstsq,\n",
    "                     X_test_lambda)\n",
    "\n",
    "        scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "        scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "        scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "        scores_std = {}\n",
    "        for aDict in (std_train, std_valid, std_test):\n",
    "            scores_std.update(aDict)      \n",
    "        scores_mean = {}\n",
    "        for aDict in (mean_train, mean_valid, mean_test):\n",
    "            scores_mean.update(aDict)\n",
    "        \n",
    "        scores_list = [lambda_index,\n",
    "                     scores_train,\n",
    "                     scores_valid,\n",
    "                     scores_test,\n",
    "                     scores_std,\n",
    "                     scores_mean]            \n",
    "                            \n",
    "    else:\n",
    "        scores_list = []\n",
    "        pred_list = []\n",
    "        for i in epochs_save_range:\n",
    "            train_epochs_step = each_epochs_save if i > 1 else max(each_epochs_save-1, 1) if i==1 else 1\n",
    "            \n",
    "            model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=train_epochs_step, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=1,\n",
    "                      use_multiprocessing=False)\n",
    "            \n",
    "            #history adjustment for continuing training\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                history = model_history.history\n",
    "            else:\n",
    "                history = mergeDict(history, model_history.history)\n",
    "                #for key_1 in history.keys():\n",
    "                #    for key_2 in model_history.history.keys():\n",
    "                #        if key_1 == key_2:\n",
    "                #            history[key_1] += model_history.history[key_2]  \n",
    "\n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_train_pred_lambda = model.predict(X_train_lambda)                \n",
    "            y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "            y_test_pred_lambda = model.predict(X_test_lambda)        \n",
    "\n",
    "            terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                        \n",
    "            polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            \n",
    "            y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "            y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "            y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)           \n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "                y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)  \n",
    "                y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)                    \n",
    "                \n",
    "            pred_list.append((lambda_index,\n",
    "                              y_train_real_lambda, \n",
    "                              y_train_pred_lambda, \n",
    "                              y_train_pred_lambda_poly_lstsq,\n",
    "                              #y_train_real_lambda_poly_lstsq,\n",
    "                              X_train_lambda, \n",
    "                              y_valid_real_lambda,\n",
    "                              y_valid_pred_lambda, \n",
    "                              y_valid_pred_lambda_poly_lstsq,\n",
    "                              #y_valid_real_lambda_poly_lstsq,\n",
    "                              X_valid_lambda, \n",
    "                              y_test_real_lambda, \n",
    "                              y_test_pred_lambda, \n",
    "                              y_test_pred_lambda_poly_lstsq, \n",
    "                              #y_test_real_lambda_poly_lstsq,\n",
    "                              X_test_lambda))\n",
    "    \n",
    "            scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "            scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "            scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "            scores_std = {}\n",
    "            for aDict in (std_train, std_valid, std_test):\n",
    "                scores_std.update(aDict)\n",
    "            scores_mean = {}\n",
    "            for aDict in (mean_train, mean_valid, mean_test):\n",
    "                scores_mean.update(aDict)\n",
    "\n",
    "            scores_list_single_epoch =  [lambda_index,\n",
    "                                         scores_train,\n",
    "                                          scores_valid,\n",
    "                                          scores_test,\n",
    "                                          scores_std,\n",
    "                                          scores_mean]        \n",
    "                  \n",
    "            scores_list.append(scores_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_lstsq_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_lstsq_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save == None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                text_file.write(str(lambda_index))\n",
    "                text_file.write(', ' + str(current_seed))\n",
    "                for i, value in enumerate(polynomial.values): \n",
    "                    text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_lstsq_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, #polynomial_lstsq_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:26:58.739423Z",
     "start_time": "2020-12-14T08:46:21.690904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    4.976, max:    9.504, cur:    4.976)\n",
      "\tvalidation       \t (min:    5.536, max:    9.201, cur:    5.536)\n",
      "mean_absolute_percentage_error_keras\n",
      "\ttraining         \t (min:    0.891, max:    1.061, cur:    0.941)\n",
      "\tvalidation       \t (min:    1.239, max:    5.331, cur:    5.331)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    6.865, max:   12.331, cur:    6.865)\n",
      "\tvalidation       \t (min:    7.682, max:   11.741, cur:    7.682)\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "max_seed = 2**32 - 1\n",
    "seed_list = random.sample(range(0, max_seed), number_different_lambda_trainings)\n",
    "chunk_multiplier = 0\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(chunksize*chunk_multiplier+index, X_data[1].values, y_data[1].values, X_data[0], seed_list, return_history=True, each_epochs_save=each_epochs_save, printing=True) for index, (X_data, y_data) in enumerate(zip(X_data_list_split, y_data_list_split)))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "    chunk_multiplier +=1\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(rand_index, X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], seed_list, callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:27:09.598279Z",
     "start_time": "2020-12-14T13:26:58.741378Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    scores_list_train = [clf[1][1] for clf in clf_list]\n",
    "    scores_list_valid = [clf[1][2] for clf in clf_list]\n",
    "    scores_list_test = [clf[1][3] for clf in clf_list]\n",
    "    scores_list_stds = [clf[1][4] for clf in clf_list]\n",
    "    scores_list_means = [clf[1][5] for clf in clf_list]\n",
    "\n",
    "    scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_list_by_epochs = [[] for i in epochs_save_range]\n",
    "    for scores_list in scores_list:   \n",
    "        for index, scores in enumerate(scores_list):\n",
    "            scores_list_by_epochs[index].append(scores)\n",
    "            \n",
    "        \n",
    "    for i, scores_list_single_epoch in enumerate(scores_list_by_epochs):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "           \n",
    "        scores_list_train = [scores_list[1] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_valid = [scores_list[2] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_test = [scores_list[3] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_stds = [scores_list[4] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_means = [scores_list[5] for scores_list in scores_list_single_epoch]\n",
    "        \n",
    "        scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()  \n",
    "        scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()  \n",
    "        scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:27:09.623347Z",
     "start_time": "2020-12-14T13:27:09.600767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E1</th>\n",
       "      <th>TRAIN POLY E1</th>\n",
       "      <th>TRAIN POLY PRED E1</th>\n",
       "      <th>TRAIN LSTSQ E1</th>\n",
       "      <th>TRAIN PRED E10</th>\n",
       "      <th>TRAIN POLY E10</th>\n",
       "      <th>TRAIN POLY PRED E10</th>\n",
       "      <th>TRAIN LSTSQ E10</th>\n",
       "      <th>TRAIN PRED E20</th>\n",
       "      <th>TRAIN POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E180</th>\n",
       "      <th>TRAIN LSTSQ E180</th>\n",
       "      <th>TRAIN PRED E190</th>\n",
       "      <th>TRAIN POLY E190</th>\n",
       "      <th>TRAIN POLY PRED E190</th>\n",
       "      <th>TRAIN LSTSQ E190</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.188</td>\n",
       "      <td>10.188</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.694</td>\n",
       "      <td>9.695</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.931</td>\n",
       "      <td>8.932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.396</td>\n",
       "      <td>4.431</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.309</td>\n",
       "      <td>4.346</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.982</td>\n",
       "      <td>12.982</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.472</td>\n",
       "      <td>12.472</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.682</td>\n",
       "      <td>11.682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.196</td>\n",
       "      <td>6.173</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.086</td>\n",
       "      <td>6.060</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>6.059</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.416</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.473</td>\n",
       "      <td>3.473</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.446</td>\n",
       "      <td>3.446</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.396</td>\n",
       "      <td>3.396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.389</td>\n",
       "      <td>2.337</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.361</td>\n",
       "      <td>2.304</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.365</td>\n",
       "      <td>24.365</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.636</td>\n",
       "      <td>23.634</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.310</td>\n",
       "      <td>22.308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.593</td>\n",
       "      <td>11.502</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.417</td>\n",
       "      <td>11.320</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>101.736</td>\n",
       "      <td>101.737</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>96.821</td>\n",
       "      <td>96.826</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.971</td>\n",
       "      <td>88.988</td>\n",
       "      <td>...</td>\n",
       "      <td>3.625</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.526</td>\n",
       "      <td>42.851</td>\n",
       "      <td>3.855</td>\n",
       "      <td>0.000</td>\n",
       "      <td>41.721</td>\n",
       "      <td>42.072</td>\n",
       "      <td>4.083</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E1  TRAIN POLY E1  TRAIN POLY PRED E1  TRAIN LSTSQ E1  \\\n",
       "MAE FV          10.188         10.188               0.013           0.000   \n",
       "RMSE FV         12.982         12.982               0.016           0.000   \n",
       "MAPE FV            inf            inf               0.446           0.000   \n",
       "R2 FV           -0.416         -0.416               0.999           1.000   \n",
       "RAAE FV          0.923          0.923               0.022           0.000   \n",
       "RMAE FV          3.473          3.473               0.095           0.000   \n",
       "FD FV           24.365         24.365               0.030           0.000   \n",
       "DTW FV         101.736        101.737               0.128           0.000   \n",
       "\n",
       "         TRAIN PRED E10  TRAIN POLY E10  TRAIN POLY PRED E10  TRAIN LSTSQ E10  \\\n",
       "MAE FV            9.694           9.695                0.015            0.000   \n",
       "RMSE FV          12.472          12.472                0.020            0.000   \n",
       "MAPE FV             inf             inf                0.213            0.000   \n",
       "R2 FV            -0.294          -0.294                0.999            1.000   \n",
       "RAAE FV           0.876           0.876                0.023            0.000   \n",
       "RMAE FV           3.446           3.446                0.118            0.000   \n",
       "FD FV            23.636          23.634                0.039            0.000   \n",
       "DTW FV           96.821          96.826                0.153            0.000   \n",
       "\n",
       "         TRAIN PRED E20  TRAIN POLY E20  ...  TRAIN POLY PRED E180  \\\n",
       "MAE FV            8.931           8.932  ...                 0.364   \n",
       "RMSE FV          11.682          11.682  ...                 0.468   \n",
       "MAPE FV             inf             inf  ...                 0.539   \n",
       "R2 FV            -0.117          -0.117  ...                 0.996   \n",
       "RAAE FV           0.804           0.804  ...                 0.049   \n",
       "RMAE FV           3.396           3.396  ...                 0.245   \n",
       "FD FV            22.310          22.308  ...                 0.891   \n",
       "DTW FV           88.971          88.988  ...                 3.625   \n",
       "\n",
       "         TRAIN LSTSQ E180  TRAIN PRED E190  TRAIN POLY E190  \\\n",
       "MAE FV              0.000            4.396            4.431   \n",
       "RMSE FV             0.000            6.196            6.173   \n",
       "MAPE FV             0.000              inf              inf   \n",
       "R2 FV               1.000            0.659            0.661   \n",
       "RAAE FV             0.000            0.406            0.410   \n",
       "RMAE FV             0.000            2.389            2.337   \n",
       "FD FV               0.000           11.593           11.502   \n",
       "DTW FV              0.000           42.526           42.851   \n",
       "\n",
       "         TRAIN POLY PRED E190  TRAIN LSTSQ E190  TRAIN PRED E200  \\\n",
       "MAE FV                  0.387             0.000            4.309   \n",
       "RMSE FV                 0.496             0.000            6.086   \n",
       "MAPE FV                 0.612             0.000              inf   \n",
       "R2 FV                   0.995             1.000            0.670   \n",
       "RAAE FV                 0.051             0.000            0.398   \n",
       "RMAE FV                 0.253             0.000            2.361   \n",
       "FD FV                   0.943             0.000           11.417   \n",
       "DTW FV                  3.855             0.000           41.721   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             4.346                 0.410             0.000  \n",
       "RMSE FV            6.060                 0.525             0.000  \n",
       "MAPE FV              inf                 6.059             0.000  \n",
       "R2 FV              0.673                 0.995             1.000  \n",
       "RAAE FV            0.402                 0.054             0.000  \n",
       "RMAE FV            2.304                 0.261             0.000  \n",
       "FD FV             11.320                 0.997             0.000  \n",
       "DTW FV            42.072                 4.083             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:27:09.677692Z",
     "start_time": "2020-12-14T13:27:09.624856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E1</th>\n",
       "      <th>VALID POLY E1</th>\n",
       "      <th>VALID POLY PRED E1</th>\n",
       "      <th>VALID LSTSQ E1</th>\n",
       "      <th>VALID PRED E10</th>\n",
       "      <th>VALID POLY E10</th>\n",
       "      <th>VALID POLY PRED E10</th>\n",
       "      <th>VALID LSTSQ E10</th>\n",
       "      <th>VALID PRED E20</th>\n",
       "      <th>VALID POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E180</th>\n",
       "      <th>VALID LSTSQ E180</th>\n",
       "      <th>VALID PRED E190</th>\n",
       "      <th>VALID POLY E190</th>\n",
       "      <th>VALID POLY PRED E190</th>\n",
       "      <th>VALID LSTSQ E190</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.198</td>\n",
       "      <td>10.198</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.710</td>\n",
       "      <td>9.710</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.956</td>\n",
       "      <td>8.957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.530</td>\n",
       "      <td>4.544</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.447</td>\n",
       "      <td>4.461</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.979</td>\n",
       "      <td>12.979</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.475</td>\n",
       "      <td>12.474</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.692</td>\n",
       "      <td>11.692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.313</td>\n",
       "      <td>6.271</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.207</td>\n",
       "      <td>6.161</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.286</td>\n",
       "      <td>1.285</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.432</td>\n",
       "      <td>1.433</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.878</td>\n",
       "      <td>1.880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.085</td>\n",
       "      <td>3.112</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.053</td>\n",
       "      <td>3.082</td>\n",
       "      <td>71.487</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.927</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.036</td>\n",
       "      <td>3.036</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.010</td>\n",
       "      <td>3.009</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.962</td>\n",
       "      <td>2.962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.021</td>\n",
       "      <td>1.976</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.995</td>\n",
       "      <td>1.946</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.415</td>\n",
       "      <td>24.415</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.692</td>\n",
       "      <td>23.690</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.372</td>\n",
       "      <td>22.371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.770</td>\n",
       "      <td>11.646</td>\n",
       "      <td>1.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.604</td>\n",
       "      <td>11.471</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.364</td>\n",
       "      <td>102.364</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.504</td>\n",
       "      <td>97.506</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>89.748</td>\n",
       "      <td>89.761</td>\n",
       "      <td>...</td>\n",
       "      <td>3.882</td>\n",
       "      <td>0.000</td>\n",
       "      <td>43.972</td>\n",
       "      <td>44.109</td>\n",
       "      <td>4.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>43.214</td>\n",
       "      <td>43.356</td>\n",
       "      <td>4.369</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E1  VALID POLY E1  VALID POLY PRED E1  VALID LSTSQ E1  \\\n",
       "MAE FV          10.198         10.198               0.014           0.000   \n",
       "RMSE FV         12.979         12.979               0.018           0.000   \n",
       "MAPE FV          1.286          1.285               0.233           0.000   \n",
       "R2 FV           -0.426         -0.426               0.999           1.000   \n",
       "RAAE FV          0.927          0.927               0.024           0.000   \n",
       "RMAE FV          3.036          3.036               0.096           0.000   \n",
       "FD FV           24.415         24.415               0.034           0.000   \n",
       "DTW FV         102.364        102.364               0.138           0.000   \n",
       "\n",
       "         VALID PRED E10  VALID POLY E10  VALID POLY PRED E10  VALID LSTSQ E10  \\\n",
       "MAE FV            9.710           9.710                0.016            0.000   \n",
       "RMSE FV          12.475          12.474                0.022            0.000   \n",
       "MAPE FV           1.432           1.433                0.174            0.000   \n",
       "R2 FV            -0.303          -0.303                0.999            1.000   \n",
       "RAAE FV           0.880           0.880                0.025            0.000   \n",
       "RMAE FV           3.010           3.009                0.111            0.000   \n",
       "FD FV            23.692          23.690                0.043            0.000   \n",
       "DTW FV           97.504          97.506                0.164            0.000   \n",
       "\n",
       "         VALID PRED E20  VALID POLY E20  ...  VALID POLY PRED E180  \\\n",
       "MAE FV            8.956           8.957  ...                 0.388   \n",
       "RMSE FV          11.692          11.692  ...                 0.507   \n",
       "MAPE FV           1.878           1.880  ...                 0.466   \n",
       "R2 FV            -0.126          -0.126  ...                 0.995   \n",
       "RAAE FV           0.809           0.809  ...                 0.052   \n",
       "RMAE FV           2.962           2.962  ...                 0.228   \n",
       "FD FV            22.372          22.371  ...                 0.980   \n",
       "DTW FV           89.748          89.761  ...                 3.882   \n",
       "\n",
       "         VALID LSTSQ E180  VALID PRED E190  VALID POLY E190  \\\n",
       "MAE FV              0.000            4.530            4.544   \n",
       "RMSE FV             0.000            6.313            6.271   \n",
       "MAPE FV             0.000            3.085            3.112   \n",
       "R2 FV               1.000            0.644            0.648   \n",
       "RAAE FV             0.000            0.420            0.421   \n",
       "RMAE FV             0.000            2.021            1.976   \n",
       "FD FV               0.000           11.770           11.646   \n",
       "DTW FV              0.000           43.972           44.109   \n",
       "\n",
       "         VALID POLY PRED E190  VALID LSTSQ E190  VALID PRED E200  \\\n",
       "MAE FV                  0.413             0.000            4.447   \n",
       "RMSE FV                 0.537             0.000            6.207   \n",
       "MAPE FV                 0.577             0.000            3.053   \n",
       "R2 FV                   0.994             1.000            0.655   \n",
       "RAAE FV                 0.055             0.000            0.412   \n",
       "RMAE FV                 0.237             0.000            1.995   \n",
       "FD FV                   1.036             0.000           11.604   \n",
       "DTW FV                  4.127             0.000           43.214   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             4.461                 0.437             0.000  \n",
       "RMSE FV            6.161                 0.568             0.000  \n",
       "MAPE FV            3.082                71.487             0.000  \n",
       "R2 FV              0.660                 0.994             1.000  \n",
       "RAAE FV            0.414                 0.058             0.000  \n",
       "RMAE FV            1.946                 0.247             0.000  \n",
       "FD FV             11.471                 1.093             0.000  \n",
       "DTW FV            43.356                 4.369             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:27:09.700025Z",
     "start_time": "2020-12-14T13:27:09.679272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E1</th>\n",
       "      <th>TEST POLY E1</th>\n",
       "      <th>TEST POLY PRED E1</th>\n",
       "      <th>TEST LSTSQ E1</th>\n",
       "      <th>TEST PRED E10</th>\n",
       "      <th>TEST POLY E10</th>\n",
       "      <th>TEST POLY PRED E10</th>\n",
       "      <th>TEST LSTSQ E10</th>\n",
       "      <th>TEST PRED E20</th>\n",
       "      <th>TEST POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E180</th>\n",
       "      <th>TEST LSTSQ E180</th>\n",
       "      <th>TEST PRED E190</th>\n",
       "      <th>TEST POLY E190</th>\n",
       "      <th>TEST POLY PRED E190</th>\n",
       "      <th>TEST LSTSQ E190</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.198</td>\n",
       "      <td>10.198</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.710</td>\n",
       "      <td>9.710</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.957</td>\n",
       "      <td>8.958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.535</td>\n",
       "      <td>4.549</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.452</td>\n",
       "      <td>4.466</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.987</td>\n",
       "      <td>12.987</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.482</td>\n",
       "      <td>12.482</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.700</td>\n",
       "      <td>11.699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.321</td>\n",
       "      <td>6.279</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.216</td>\n",
       "      <td>6.169</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.378</td>\n",
       "      <td>1.377</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.579</td>\n",
       "      <td>1.578</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.147</td>\n",
       "      <td>2.149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.523</td>\n",
       "      <td>3.572</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>3.520</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.421</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.925</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.165</td>\n",
       "      <td>3.165</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.138</td>\n",
       "      <td>3.138</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.090</td>\n",
       "      <td>3.090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.129</td>\n",
       "      <td>2.080</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.103</td>\n",
       "      <td>2.049</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.253</td>\n",
       "      <td>24.252</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.532</td>\n",
       "      <td>23.531</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.219</td>\n",
       "      <td>22.217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.599</td>\n",
       "      <td>11.486</td>\n",
       "      <td>1.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.434</td>\n",
       "      <td>11.311</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>101.484</td>\n",
       "      <td>101.484</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>96.601</td>\n",
       "      <td>96.605</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.828</td>\n",
       "      <td>88.841</td>\n",
       "      <td>...</td>\n",
       "      <td>3.858</td>\n",
       "      <td>0.000</td>\n",
       "      <td>43.427</td>\n",
       "      <td>43.549</td>\n",
       "      <td>4.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.676</td>\n",
       "      <td>42.801</td>\n",
       "      <td>4.341</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E1  TEST POLY E1  TEST POLY PRED E1  TEST LSTSQ E1  \\\n",
       "MAE FV         10.198        10.198              0.014          0.000   \n",
       "RMSE FV        12.987        12.987              0.018          0.000   \n",
       "MAPE FV         1.378         1.377              0.198          0.000   \n",
       "R2 FV          -0.421        -0.421              0.999          1.000   \n",
       "RAAE FV         0.925         0.925              0.024          0.000   \n",
       "RMAE FV         3.165         3.165              0.100          0.000   \n",
       "FD FV          24.253        24.252              0.034          0.000   \n",
       "DTW FV        101.484       101.484              0.136          0.000   \n",
       "\n",
       "         TEST PRED E10  TEST POLY E10  TEST POLY PRED E10  TEST LSTSQ E10  \\\n",
       "MAE FV           9.710          9.710               0.016           0.000   \n",
       "RMSE FV         12.482         12.482               0.022           0.000   \n",
       "MAPE FV          1.579          1.578               0.312           0.000   \n",
       "R2 FV           -0.299         -0.299               0.999           1.000   \n",
       "RAAE FV          0.878          0.878               0.025           0.000   \n",
       "RMAE FV          3.138          3.138               0.117           0.000   \n",
       "FD FV           23.532         23.531               0.042           0.000   \n",
       "DTW FV          96.601         96.605               0.163           0.000   \n",
       "\n",
       "         TEST PRED E20  TEST POLY E20  ...  TEST POLY PRED E180  \\\n",
       "MAE FV           8.957          8.958  ...                0.388   \n",
       "RMSE FV         11.700         11.699  ...                0.507   \n",
       "MAPE FV          2.147          2.149  ...                0.441   \n",
       "R2 FV           -0.123         -0.122  ...                0.995   \n",
       "RAAE FV          0.807          0.807  ...                0.052   \n",
       "RMAE FV          3.090          3.090  ...                0.244   \n",
       "FD FV           22.219         22.217  ...                0.968   \n",
       "DTW FV          88.828         88.841  ...                3.858   \n",
       "\n",
       "         TEST LSTSQ E180  TEST PRED E190  TEST POLY E190  TEST POLY PRED E190  \\\n",
       "MAE FV             0.000           4.535           4.549                0.413   \n",
       "RMSE FV            0.000           6.321           6.279                0.538   \n",
       "MAPE FV            0.000           3.523           3.572                0.754   \n",
       "R2 FV              1.000           0.645           0.649                0.994   \n",
       "RAAE FV            0.000           0.419           0.421                0.055   \n",
       "RMAE FV            0.000           2.129           2.080                0.254   \n",
       "FD FV              0.000          11.599          11.486                1.024   \n",
       "DTW FV             0.000          43.427          43.549                4.100   \n",
       "\n",
       "         TEST LSTSQ E190  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           4.452           4.466                0.437   \n",
       "RMSE FV            0.000           6.216           6.169                0.568   \n",
       "MAPE FV            0.000           3.473           3.520                0.641   \n",
       "R2 FV              1.000           0.656           0.660                0.994   \n",
       "RAAE FV            0.000           0.412           0.413                0.058   \n",
       "RMAE FV            0.000           2.103           2.049                0.264   \n",
       "FD FV              0.000          11.434          11.311                1.081   \n",
       "DTW FV             0.000          42.676          42.801                4.341   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "FD FV              0.000  \n",
       "DTW FV             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:27:09.723958Z",
     "start_time": "2020-12-14T13:27:09.701469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA</th>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>...</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.571</td>\n",
       "      <td>0.669</td>\n",
       "      <td>1.030</td>\n",
       "      <td>1.703</td>\n",
       "      <td>2.611</td>\n",
       "      <td>3.581</td>\n",
       "      <td>4.480</td>\n",
       "      <td>5.243</td>\n",
       "      <td>5.853</td>\n",
       "      <td>6.327</td>\n",
       "      <td>...</td>\n",
       "      <td>6.966</td>\n",
       "      <td>7.178</td>\n",
       "      <td>7.346</td>\n",
       "      <td>7.482</td>\n",
       "      <td>7.598</td>\n",
       "      <td>7.699</td>\n",
       "      <td>7.790</td>\n",
       "      <td>7.875</td>\n",
       "      <td>7.955</td>\n",
       "      <td>8.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.571</td>\n",
       "      <td>0.669</td>\n",
       "      <td>1.030</td>\n",
       "      <td>1.702</td>\n",
       "      <td>2.610</td>\n",
       "      <td>3.580</td>\n",
       "      <td>4.478</td>\n",
       "      <td>5.240</td>\n",
       "      <td>5.850</td>\n",
       "      <td>6.322</td>\n",
       "      <td>...</td>\n",
       "      <td>6.960</td>\n",
       "      <td>7.171</td>\n",
       "      <td>7.337</td>\n",
       "      <td>7.472</td>\n",
       "      <td>7.586</td>\n",
       "      <td>7.686</td>\n",
       "      <td>7.776</td>\n",
       "      <td>7.859</td>\n",
       "      <td>7.938</td>\n",
       "      <td>8.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>...</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "      <td>11.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA</th>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>...</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA</th>\n",
       "      <td>0.571</td>\n",
       "      <td>0.668</td>\n",
       "      <td>1.026</td>\n",
       "      <td>1.693</td>\n",
       "      <td>2.593</td>\n",
       "      <td>3.556</td>\n",
       "      <td>4.449</td>\n",
       "      <td>5.208</td>\n",
       "      <td>5.816</td>\n",
       "      <td>6.288</td>\n",
       "      <td>...</td>\n",
       "      <td>6.924</td>\n",
       "      <td>7.135</td>\n",
       "      <td>7.301</td>\n",
       "      <td>7.436</td>\n",
       "      <td>7.550</td>\n",
       "      <td>7.650</td>\n",
       "      <td>7.740</td>\n",
       "      <td>7.823</td>\n",
       "      <td>7.902</td>\n",
       "      <td>7.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.571</td>\n",
       "      <td>0.668</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1.692</td>\n",
       "      <td>2.593</td>\n",
       "      <td>3.555</td>\n",
       "      <td>4.447</td>\n",
       "      <td>5.205</td>\n",
       "      <td>5.812</td>\n",
       "      <td>6.283</td>\n",
       "      <td>...</td>\n",
       "      <td>6.918</td>\n",
       "      <td>7.128</td>\n",
       "      <td>7.294</td>\n",
       "      <td>7.428</td>\n",
       "      <td>7.541</td>\n",
       "      <td>7.640</td>\n",
       "      <td>7.729</td>\n",
       "      <td>7.811</td>\n",
       "      <td>7.889</td>\n",
       "      <td>7.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>...</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "      <td>11.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA</th>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>...</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA</th>\n",
       "      <td>0.570</td>\n",
       "      <td>0.668</td>\n",
       "      <td>1.026</td>\n",
       "      <td>1.694</td>\n",
       "      <td>2.597</td>\n",
       "      <td>3.561</td>\n",
       "      <td>4.455</td>\n",
       "      <td>5.215</td>\n",
       "      <td>5.823</td>\n",
       "      <td>6.295</td>\n",
       "      <td>...</td>\n",
       "      <td>6.932</td>\n",
       "      <td>7.144</td>\n",
       "      <td>7.310</td>\n",
       "      <td>7.445</td>\n",
       "      <td>7.559</td>\n",
       "      <td>7.659</td>\n",
       "      <td>7.749</td>\n",
       "      <td>7.832</td>\n",
       "      <td>7.911</td>\n",
       "      <td>7.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.570</td>\n",
       "      <td>0.668</td>\n",
       "      <td>1.026</td>\n",
       "      <td>1.693</td>\n",
       "      <td>2.596</td>\n",
       "      <td>3.559</td>\n",
       "      <td>4.453</td>\n",
       "      <td>5.211</td>\n",
       "      <td>5.819</td>\n",
       "      <td>6.291</td>\n",
       "      <td>...</td>\n",
       "      <td>6.927</td>\n",
       "      <td>7.137</td>\n",
       "      <td>7.303</td>\n",
       "      <td>7.437</td>\n",
       "      <td>7.551</td>\n",
       "      <td>7.650</td>\n",
       "      <td>7.739</td>\n",
       "      <td>7.821</td>\n",
       "      <td>7.899</td>\n",
       "      <td>7.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>...</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "      <td>11.145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        E1    E10    E20    E30    E40    E50  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.152 11.152 11.152 11.152 11.152 11.152   \n",
       "STD FV TRAIN PRED LAMBDA             0.571  0.669  1.030  1.703  2.611  3.581   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  0.571  0.669  1.030  1.702  2.610  3.580   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.152 11.152 11.152 11.152 11.152 11.152   \n",
       "STD FV VALID REAL LAMBDA            11.125 11.125 11.125 11.125 11.125 11.125   \n",
       "STD FV VALID PRED LAMBDA             0.571  0.668  1.026  1.693  2.593  3.556   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  0.571  0.668  1.025  1.692  2.593  3.555   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.125 11.125 11.125 11.125 11.125 11.125   \n",
       "STD FV TEST REAL LAMBDA             11.145 11.145 11.145 11.145 11.145 11.145   \n",
       "STD FV TEST PRED LAMBDA              0.570  0.668  1.026  1.694  2.597  3.561   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   0.570  0.668  1.026  1.693  2.596  3.559   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.145 11.145 11.145 11.145 11.145 11.145   \n",
       "\n",
       "                                       E60    E70    E80    E90  ...   E110  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.152 11.152 11.152 11.152  ... 11.152   \n",
       "STD FV TRAIN PRED LAMBDA             4.480  5.243  5.853  6.327  ...  6.966   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  4.478  5.240  5.850  6.322  ...  6.960   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.152 11.152 11.152 11.152  ... 11.152   \n",
       "STD FV VALID REAL LAMBDA            11.125 11.125 11.125 11.125  ... 11.125   \n",
       "STD FV VALID PRED LAMBDA             4.449  5.208  5.816  6.288  ...  6.924   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  4.447  5.205  5.812  6.283  ...  6.918   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.125 11.125 11.125 11.125  ... 11.125   \n",
       "STD FV TEST REAL LAMBDA             11.145 11.145 11.145 11.145  ... 11.145   \n",
       "STD FV TEST PRED LAMBDA              4.455  5.215  5.823  6.295  ...  6.932   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   4.453  5.211  5.819  6.291  ...  6.927   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.145 11.145 11.145 11.145  ... 11.145   \n",
       "\n",
       "                                      E120   E130   E140   E150   E160   E170  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.152 11.152 11.152 11.152 11.152 11.152   \n",
       "STD FV TRAIN PRED LAMBDA             7.178  7.346  7.482  7.598  7.699  7.790   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.171  7.337  7.472  7.586  7.686  7.776   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.152 11.152 11.152 11.152 11.152 11.152   \n",
       "STD FV VALID REAL LAMBDA            11.125 11.125 11.125 11.125 11.125 11.125   \n",
       "STD FV VALID PRED LAMBDA             7.135  7.301  7.436  7.550  7.650  7.740   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.128  7.294  7.428  7.541  7.640  7.729   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.125 11.125 11.125 11.125 11.125 11.125   \n",
       "STD FV TEST REAL LAMBDA             11.145 11.145 11.145 11.145 11.145 11.145   \n",
       "STD FV TEST PRED LAMBDA              7.144  7.310  7.445  7.559  7.659  7.749   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.137  7.303  7.437  7.551  7.650  7.739   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.145 11.145 11.145 11.145 11.145 11.145   \n",
       "\n",
       "                                      E180   E190   E200  \n",
       "STD FV TRAIN REAL LAMBDA            11.152 11.152 11.152  \n",
       "STD FV TRAIN PRED LAMBDA             7.875  7.955  8.033  \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.859  7.938  8.014  \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.152 11.152 11.152  \n",
       "STD FV VALID REAL LAMBDA            11.125 11.125 11.125  \n",
       "STD FV VALID PRED LAMBDA             7.823  7.902  7.978  \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.811  7.889  7.965  \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.125 11.125 11.125  \n",
       "STD FV TEST REAL LAMBDA             11.145 11.145 11.145  \n",
       "STD FV TEST PRED LAMBDA              7.832  7.911  7.988  \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.821  7.899  7.975  \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.145 11.145 11.145  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:27:09.748075Z",
     "start_time": "2020-12-14T13:27:09.725534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA</th>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA</th>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         E1    E10    E20    E30    E40  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.081 -0.081 -0.081 -0.081 -0.081   \n",
       "MEAN FV TRAIN PRED LAMBDA             0.004 -0.005 -0.019 -0.038 -0.060   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  0.004 -0.005 -0.019 -0.038 -0.060   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.081 -0.081 -0.081 -0.081 -0.081   \n",
       "MEAN FV VALID REAL LAMBDA            -0.072 -0.072 -0.072 -0.072 -0.072   \n",
       "MEAN FV VALID PRED LAMBDA             0.004 -0.005 -0.018 -0.038 -0.060   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  0.004 -0.005 -0.018 -0.038 -0.059   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.072 -0.072 -0.072 -0.072 -0.072   \n",
       "MEAN FV TEST REAL LAMBDA             -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TEST PRED LAMBDA              0.004 -0.005 -0.018 -0.038 -0.059   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   0.004 -0.005 -0.018 -0.038 -0.059   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "\n",
       "                                        E50    E60    E70    E80    E90  ...  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.081 -0.081 -0.081 -0.081 -0.081  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.070 -0.067 -0.066 -0.065 -0.065  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.070 -0.067 -0.066 -0.065 -0.065  ...   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.081 -0.081 -0.081 -0.081 -0.081  ...   \n",
       "MEAN FV VALID REAL LAMBDA            -0.072 -0.072 -0.072 -0.072 -0.072  ...   \n",
       "MEAN FV VALID PRED LAMBDA            -0.068 -0.064 -0.061 -0.059 -0.058  ...   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.068 -0.064 -0.061 -0.058 -0.057  ...   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.072 -0.072 -0.072 -0.072 -0.072  ...   \n",
       "MEAN FV TEST REAL LAMBDA             -0.078 -0.078 -0.078 -0.078 -0.078  ...   \n",
       "MEAN FV TEST PRED LAMBDA             -0.068 -0.064 -0.061 -0.060 -0.060  ...   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.068 -0.064 -0.061 -0.060 -0.059  ...   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.078 -0.078 -0.078 -0.078 -0.078  ...   \n",
       "\n",
       "                                       E110   E120   E130   E140   E150  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.081 -0.081 -0.081 -0.081 -0.081   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.063 -0.063 -0.062 -0.062 -0.062   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.063 -0.063 -0.062 -0.062 -0.062   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.081 -0.081 -0.081 -0.081 -0.081   \n",
       "MEAN FV VALID REAL LAMBDA            -0.072 -0.072 -0.072 -0.072 -0.072   \n",
       "MEAN FV VALID PRED LAMBDA            -0.054 -0.053 -0.052 -0.051 -0.051   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.054 -0.052 -0.051 -0.050 -0.050   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.072 -0.072 -0.072 -0.072 -0.072   \n",
       "MEAN FV TEST REAL LAMBDA             -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TEST PRED LAMBDA             -0.057 -0.057 -0.056 -0.055 -0.055   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.057 -0.056 -0.055 -0.055 -0.055   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "\n",
       "                                       E160   E170   E180   E190   E200  \n",
       "MEAN FV TRAIN REAL LAMBDA            -0.081 -0.081 -0.081 -0.081 -0.081  \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.063 -0.063 -0.064 -0.064 -0.065  \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.063 -0.063 -0.064 -0.064 -0.065  \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.081 -0.081 -0.081 -0.081 -0.081  \n",
       "MEAN FV VALID REAL LAMBDA            -0.072 -0.072 -0.072 -0.072 -0.072  \n",
       "MEAN FV VALID PRED LAMBDA            -0.051 -0.052 -0.052 -0.052 -0.052  \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.051 -0.051 -0.052 -0.052 -0.052  \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.072 -0.072 -0.072 -0.072 -0.072  \n",
       "MEAN FV TEST REAL LAMBDA             -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "MEAN FV TEST PRED LAMBDA             -0.057 -0.057 -0.058 -0.058 -0.058  \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.056 -0.056 -0.057 -0.057 -0.057  \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Net Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:27:09.850818Z",
     "start_time": "2020-12-14T13:27:09.749675Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_lambda_preds(i, \n",
    "                      lambda_indices,\n",
    "                      y_train_real_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_train_lambda_by_epoch, \n",
    "                      y_valid_real_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_valid_lambda_by_epoch, \n",
    "                      y_test_real_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_test_lambda_by_epoch):\n",
    "    \n",
    "    \n",
    "    index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "        \n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_by_epoch, y_train_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_by_epoch, y_valid_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_by_epoch, y_test_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_by_epoch, y_train_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_by_epoch, y_test_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())    \n",
    "\n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "\n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)         \n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)    \n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False) \n",
    "\n",
    "    return y_train_real_lambda_df, y_valid_real_lambda_df, y_test_real_lambda_df, y_train_pred_lambda_df, y_valid_pred_lambda_df, y_test_pred_lambda_df, y_train_pred_lambda_poly_lstsq_df, y_valid_pred_lambda_poly_lstsq_df, y_test_pred_lambda_poly_lstsq_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:35:37.296982Z",
     "start_time": "2020-12-14T13:27:09.852840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64864ec0f7b4ec3b9fefa9e34ceacd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of  21 | elapsed:  7.2min remaining: 68.2min\n",
      "[Parallel(n_jobs=-3)]: Done  10 out of  21 | elapsed:  7.6min remaining:  8.4min\n",
      "[Parallel(n_jobs=-3)]: Done  18 out of  21 | elapsed:  8.1min remaining:  1.3min\n",
      "[Parallel(n_jobs=-3)]: Done  21 out of  21 | elapsed:  8.2min finished\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    lambda_index_list = [clf[0][0] for clf in clf_list]\n",
    "    lambda_seed_list = [clf[0][1] for clf in clf_list] \n",
    "    polynomial_real_list = [clf[0][2] for clf in clf_list] \n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][3] for clf in clf_list] \n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][4] for clf in clf_list] \n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "\n",
    "    lambda_indices_list = np.zeros((len(clf_list), 1))\n",
    "    y_train_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][1])))\n",
    "    y_train_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][2])))\n",
    "    y_train_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][3])))\n",
    "    X_train_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][4].shape)]][0])\n",
    "    y_valid_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][5])))\n",
    "    y_valid_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][6])))\n",
    "    y_valid_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][7])))\n",
    "    X_valid_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][8].shape)]][0])\n",
    "    y_test_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][9])))\n",
    "    y_test_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][10])))\n",
    "    y_test_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][11])))\n",
    "    X_test_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][12].shape)]][0])\n",
    "\n",
    "    for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate([clf[2] for clf in clf_list]):\n",
    "        lambda_indices_list[index] = lambda_indices\n",
    "        y_train_real_lambda_list[index] = y_train_real_lambda.ravel()\n",
    "        y_train_pred_lambda_list[index] = y_train_pred_lambda.ravel()\n",
    "        y_train_pred_lambda_poly_lstsq_list[index] = y_train_pred_lambda_poly_lstsq.ravel()\n",
    "        X_train_lambda_list[index] = X_train_lambda#.ravel()\n",
    "\n",
    "        y_valid_real_lambda_list[index] = y_valid_real_lambda.ravel()\n",
    "        y_valid_pred_lambda_list[index] = y_valid_pred_lambda.ravel()\n",
    "        y_valid_pred_lambda_poly_lstsq_list[index] = y_valid_pred_lambda_poly_lstsq.ravel()\n",
    "        X_valid_lambda_list[index] = X_valid_lambda#.ravel()\n",
    "\n",
    "        y_test_real_lambda_list[index] = y_test_real_lambda.ravel()\n",
    "        y_test_pred_lambda_list[index] = y_test_pred_lambda.ravel()\n",
    "        y_test_pred_lambda_poly_lstsq_list[index] = y_test_pred_lambda_poly_lstsq.ravel()\n",
    "        X_test_lambda_list[index] = X_test_lambda#.ravel()\n",
    "    \n",
    "    #add x_data before each pred\n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda.reshape(len(y_train_real_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_list, y_train_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda.reshape(len(y_valid_real_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_list, y_valid_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda.reshape(len(y_test_real_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_list, y_test_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda.reshape(len(y_train_pred_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_list, y_train_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda.reshape(len(y_valid_pred_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_list, y_valid_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda.reshape(len(y_test_pred_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_list, y_test_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq.reshape(len(y_train_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_list, y_train_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq.reshape(len(y_valid_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_list, y_valid_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq.reshape(len(y_test_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_list, y_test_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())    \n",
    "    \n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "       \n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "    \n",
    "    lambda_indices_list = [np.zeros((len(clf_list), 1)) for i in epochs_save_range]\n",
    "    y_train_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][3]), 1)) for i in epochs_save_range]\n",
    "    X_train_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][4].shape)]][0]) for i in epochs_save_range]\n",
    "    y_valid_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][7]), 1)) for i in epochs_save_range]\n",
    "    X_valid_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][8].shape)]][0]) for i in epochs_save_range]\n",
    "    y_test_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][12]), 1)) for i in epochs_save_range]\n",
    "    X_test_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][12].shape)]][0]) for i in epochs_save_range]\n",
    "    \n",
    "    for i, y_data_list_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate(y_data_list_per_epoch):\n",
    "            lambda_indices_list[index][i] = lambda_indices\n",
    "            y_train_real_lambda_list[index][i] = y_train_real_lambda#.ravel()\n",
    "            y_train_pred_lambda_list[index][i] = y_train_pred_lambda#.ravel()\n",
    "            y_train_pred_lambda_poly_lstsq_list[index][i] = y_train_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_train_lambda_list[index][i] = X_train_lambda#.ravel()\n",
    "            \n",
    "            y_valid_real_lambda_list[index][i] = y_valid_real_lambda#.ravel()\n",
    "            y_valid_pred_lambda_list[index][i] = y_valid_pred_lambda#.ravel()\n",
    "            y_valid_pred_lambda_poly_lstsq_list[index][i] = y_valid_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_valid_lambda_list[index][i] = X_valid_lambda#.ravel()\n",
    "            \n",
    "            y_test_real_lambda_list[index][i] = y_test_real_lambda#.ravel()\n",
    "            y_test_pred_lambda_list[index][i] = y_test_pred_lambda#.ravel()\n",
    "            y_test_pred_lambda_poly_lstsq_list[index][i] = y_test_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_test_lambda_list[index][i] = X_test_lambda#.ravel()\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    y_data_lambda_list = parallel(delayed(save_lambda_preds)(i, \n",
    "                                                           lambda_indices,\n",
    "                                                           y_train_real_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_train_lambda_by_epoch, \n",
    "                                                           y_valid_real_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_valid_lambda_by_epoch, \n",
    "                                                           y_test_real_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_test_lambda_by_epoch) for i, \n",
    "                                                                                        (lambda_indices,\n",
    "                                                                                         y_train_real_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_train_lambda_by_epoch, \n",
    "                                                                                         y_valid_real_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_valid_lambda_by_epoch, \n",
    "                                                                                         y_test_real_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_test_lambda_by_epoch) in enumerate(zip(lambda_indices_list,\n",
    "                                                                                                                                  y_train_real_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_train_lambda_list, \n",
    "                                                                                                                                  y_valid_real_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_valid_lambda_list, \n",
    "                                                                                                                                  y_test_real_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_test_lambda_list)))  \n",
    "    y_test_real_lambda_df = y_data_lambda_list[-1][2]\n",
    "    y_test_pred_lambda_df = y_data_lambda_list[-1][5]\n",
    "    y_test_pred_lambda_poly_lstsq_df = y_data_lambda_list[-1][8]\n",
    "    del parallel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:35:37.323501Z",
     "start_time": "2020-12-14T13:35:37.299667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-7.580</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>10.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-11.955</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-19.621</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-11.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-2.020</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>20.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>25.836</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-7.972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -7.580 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -11.955  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -19.621  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940  -2.020  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920  25.836 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0  10.155  \n",
       "1  -0.622  \n",
       "2 -11.502  \n",
       "3  20.981  \n",
       "4  -7.972  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:35:37.350750Z",
     "start_time": "2020-12-14T13:35:37.325258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-4.473</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>3.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-16.977</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-1.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-24.207</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-8.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>9.793</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>6.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>5.007</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-7.049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -4.473 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -16.977  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -24.207  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940   9.793  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920   5.007 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0   3.033  \n",
       "1  -1.808  \n",
       "2  -8.311  \n",
       "3   6.075  \n",
       "4  -7.049  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:35:37.373258Z",
     "start_time": "2020-12-14T13:35:37.352479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-4.017</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>4.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-17.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-1.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-24.931</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-7.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>8.942</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>6.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>3.930</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-6.404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -4.017 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -17.260  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -24.931  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940   8.942  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920   3.930 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0   4.058  \n",
       "1  -1.916  \n",
       "2  -7.867  \n",
       "3   6.359  \n",
       "4  -6.404  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_poly_lstsq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:35:40.921715Z",
     "start_time": "2020-12-14T13:35:37.374984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d239b8f255e147f7b6af13fdf22aaee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:35:57.513900Z",
     "start_time": "2020-12-14T13:35:40.923955Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:35:58.270533Z",
     "start_time": "2020-12-14T13:35:57.518515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.218</td>\n",
       "      <td>10.165</td>\n",
       "      <td>10.112</td>\n",
       "      <td>10.060</td>\n",
       "      <td>10.007</td>\n",
       "      <td>9.953</td>\n",
       "      <td>9.899</td>\n",
       "      <td>9.844</td>\n",
       "      <td>9.787</td>\n",
       "      <td>9.728</td>\n",
       "      <td>...</td>\n",
       "      <td>4.396</td>\n",
       "      <td>4.387</td>\n",
       "      <td>4.378</td>\n",
       "      <td>4.369</td>\n",
       "      <td>4.361</td>\n",
       "      <td>4.352</td>\n",
       "      <td>4.343</td>\n",
       "      <td>4.335</td>\n",
       "      <td>4.326</td>\n",
       "      <td>4.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.341</td>\n",
       "      <td>2.318</td>\n",
       "      <td>2.295</td>\n",
       "      <td>2.272</td>\n",
       "      <td>2.249</td>\n",
       "      <td>2.225</td>\n",
       "      <td>2.201</td>\n",
       "      <td>2.177</td>\n",
       "      <td>2.152</td>\n",
       "      <td>2.126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.893</td>\n",
       "      <td>4.884</td>\n",
       "      <td>4.871</td>\n",
       "      <td>4.854</td>\n",
       "      <td>4.836</td>\n",
       "      <td>4.819</td>\n",
       "      <td>4.802</td>\n",
       "      <td>4.786</td>\n",
       "      <td>4.772</td>\n",
       "      <td>4.757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.494</td>\n",
       "      <td>2.492</td>\n",
       "      <td>2.488</td>\n",
       "      <td>2.483</td>\n",
       "      <td>2.479</td>\n",
       "      <td>2.474</td>\n",
       "      <td>2.471</td>\n",
       "      <td>2.468</td>\n",
       "      <td>2.464</td>\n",
       "      <td>2.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.505</td>\n",
       "      <td>8.473</td>\n",
       "      <td>8.439</td>\n",
       "      <td>8.402</td>\n",
       "      <td>8.367</td>\n",
       "      <td>8.332</td>\n",
       "      <td>8.298</td>\n",
       "      <td>8.255</td>\n",
       "      <td>8.211</td>\n",
       "      <td>8.172</td>\n",
       "      <td>...</td>\n",
       "      <td>4.004</td>\n",
       "      <td>3.995</td>\n",
       "      <td>3.986</td>\n",
       "      <td>3.977</td>\n",
       "      <td>3.970</td>\n",
       "      <td>3.961</td>\n",
       "      <td>3.954</td>\n",
       "      <td>3.945</td>\n",
       "      <td>3.936</td>\n",
       "      <td>3.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.948</td>\n",
       "      <td>9.901</td>\n",
       "      <td>9.860</td>\n",
       "      <td>9.814</td>\n",
       "      <td>9.763</td>\n",
       "      <td>9.711</td>\n",
       "      <td>9.664</td>\n",
       "      <td>9.619</td>\n",
       "      <td>9.571</td>\n",
       "      <td>9.519</td>\n",
       "      <td>...</td>\n",
       "      <td>4.369</td>\n",
       "      <td>4.361</td>\n",
       "      <td>4.352</td>\n",
       "      <td>4.342</td>\n",
       "      <td>4.334</td>\n",
       "      <td>4.325</td>\n",
       "      <td>4.316</td>\n",
       "      <td>4.306</td>\n",
       "      <td>4.298</td>\n",
       "      <td>4.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.648</td>\n",
       "      <td>11.584</td>\n",
       "      <td>11.518</td>\n",
       "      <td>11.448</td>\n",
       "      <td>11.379</td>\n",
       "      <td>11.315</td>\n",
       "      <td>11.250</td>\n",
       "      <td>11.184</td>\n",
       "      <td>11.109</td>\n",
       "      <td>11.033</td>\n",
       "      <td>...</td>\n",
       "      <td>4.759</td>\n",
       "      <td>4.749</td>\n",
       "      <td>4.740</td>\n",
       "      <td>4.732</td>\n",
       "      <td>4.722</td>\n",
       "      <td>4.713</td>\n",
       "      <td>4.703</td>\n",
       "      <td>4.694</td>\n",
       "      <td>4.685</td>\n",
       "      <td>4.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.716</td>\n",
       "      <td>20.558</td>\n",
       "      <td>20.402</td>\n",
       "      <td>20.246</td>\n",
       "      <td>20.090</td>\n",
       "      <td>19.932</td>\n",
       "      <td>19.768</td>\n",
       "      <td>19.599</td>\n",
       "      <td>19.422</td>\n",
       "      <td>19.237</td>\n",
       "      <td>...</td>\n",
       "      <td>7.196</td>\n",
       "      <td>7.182</td>\n",
       "      <td>7.162</td>\n",
       "      <td>7.144</td>\n",
       "      <td>7.128</td>\n",
       "      <td>7.110</td>\n",
       "      <td>7.095</td>\n",
       "      <td>7.075</td>\n",
       "      <td>7.063</td>\n",
       "      <td>7.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean         10.218        10.165        10.112        10.060        10.007   \n",
       "std           2.341         2.318         2.295         2.272         2.249   \n",
       "min           4.893         4.884         4.871         4.854         4.836   \n",
       "25%           8.505         8.473         8.439         8.402         8.367   \n",
       "50%           9.948         9.901         9.860         9.814         9.763   \n",
       "75%          11.648        11.584        11.518        11.448        11.379   \n",
       "max          20.716        20.558        20.402        20.246        20.090   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000      10000.000   \n",
       "mean          9.953         9.899         9.844         9.787          9.728   \n",
       "std           2.225         2.201         2.177         2.152          2.126   \n",
       "min           4.819         4.802         4.786         4.772          4.757   \n",
       "25%           8.332         8.298         8.255         8.211          8.172   \n",
       "50%           9.711         9.664         9.619         9.571          9.519   \n",
       "75%          11.315        11.250        11.184        11.109         11.033   \n",
       "max          19.932        19.768        19.599        19.422         19.237   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...       10000.000       10000.000       10000.000       10000.000   \n",
       "mean   ...           4.396           4.387           4.378           4.369   \n",
       "std    ...           0.581           0.580           0.579           0.579   \n",
       "min    ...           2.494           2.492           2.488           2.483   \n",
       "25%    ...           4.004           3.995           3.986           3.977   \n",
       "50%    ...           4.369           4.361           4.352           4.342   \n",
       "75%    ...           4.759           4.749           4.740           4.732   \n",
       "max    ...           7.196           7.182           7.162           7.144   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            4.361           4.352           4.343           4.335   \n",
       "std             0.578           0.577           0.577           0.576   \n",
       "min             2.479           2.474           2.471           2.468   \n",
       "25%             3.970           3.961           3.954           3.945   \n",
       "50%             4.334           4.325           4.316           4.306   \n",
       "75%             4.722           4.713           4.703           4.694   \n",
       "max             7.128           7.110           7.095           7.075   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count       10000.000       10000.000  \n",
       "mean            4.326           4.317  \n",
       "std             0.575           0.574  \n",
       "min             2.464           2.460  \n",
       "25%             3.936           3.928  \n",
       "50%             4.298           4.290  \n",
       "75%             4.685           4.676  \n",
       "max             7.063           7.045  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:35:58.994511Z",
     "start_time": "2020-12-14T13:35:58.272374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.198</td>\n",
       "      <td>10.146</td>\n",
       "      <td>10.094</td>\n",
       "      <td>10.042</td>\n",
       "      <td>9.989</td>\n",
       "      <td>9.936</td>\n",
       "      <td>9.882</td>\n",
       "      <td>9.826</td>\n",
       "      <td>9.769</td>\n",
       "      <td>9.710</td>\n",
       "      <td>...</td>\n",
       "      <td>4.522</td>\n",
       "      <td>4.513</td>\n",
       "      <td>4.505</td>\n",
       "      <td>4.497</td>\n",
       "      <td>4.488</td>\n",
       "      <td>4.480</td>\n",
       "      <td>4.472</td>\n",
       "      <td>4.463</td>\n",
       "      <td>4.455</td>\n",
       "      <td>4.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.382</td>\n",
       "      <td>2.359</td>\n",
       "      <td>2.337</td>\n",
       "      <td>2.314</td>\n",
       "      <td>2.291</td>\n",
       "      <td>2.268</td>\n",
       "      <td>2.244</td>\n",
       "      <td>2.220</td>\n",
       "      <td>2.195</td>\n",
       "      <td>2.169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.562</td>\n",
       "      <td>4.555</td>\n",
       "      <td>4.549</td>\n",
       "      <td>4.542</td>\n",
       "      <td>4.536</td>\n",
       "      <td>4.530</td>\n",
       "      <td>4.523</td>\n",
       "      <td>4.517</td>\n",
       "      <td>4.511</td>\n",
       "      <td>4.504</td>\n",
       "      <td>...</td>\n",
       "      <td>2.375</td>\n",
       "      <td>2.371</td>\n",
       "      <td>2.365</td>\n",
       "      <td>2.361</td>\n",
       "      <td>2.356</td>\n",
       "      <td>2.351</td>\n",
       "      <td>2.348</td>\n",
       "      <td>2.342</td>\n",
       "      <td>2.337</td>\n",
       "      <td>2.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.449</td>\n",
       "      <td>8.417</td>\n",
       "      <td>8.384</td>\n",
       "      <td>8.348</td>\n",
       "      <td>8.314</td>\n",
       "      <td>8.270</td>\n",
       "      <td>8.229</td>\n",
       "      <td>8.191</td>\n",
       "      <td>8.154</td>\n",
       "      <td>8.113</td>\n",
       "      <td>...</td>\n",
       "      <td>4.065</td>\n",
       "      <td>4.057</td>\n",
       "      <td>4.048</td>\n",
       "      <td>4.040</td>\n",
       "      <td>4.031</td>\n",
       "      <td>4.023</td>\n",
       "      <td>4.015</td>\n",
       "      <td>4.008</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.934</td>\n",
       "      <td>9.893</td>\n",
       "      <td>9.839</td>\n",
       "      <td>9.787</td>\n",
       "      <td>9.742</td>\n",
       "      <td>9.700</td>\n",
       "      <td>9.649</td>\n",
       "      <td>9.601</td>\n",
       "      <td>9.553</td>\n",
       "      <td>9.501</td>\n",
       "      <td>...</td>\n",
       "      <td>4.498</td>\n",
       "      <td>4.489</td>\n",
       "      <td>4.481</td>\n",
       "      <td>4.473</td>\n",
       "      <td>4.465</td>\n",
       "      <td>4.456</td>\n",
       "      <td>4.448</td>\n",
       "      <td>4.440</td>\n",
       "      <td>4.433</td>\n",
       "      <td>4.423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.643</td>\n",
       "      <td>11.582</td>\n",
       "      <td>11.516</td>\n",
       "      <td>11.440</td>\n",
       "      <td>11.376</td>\n",
       "      <td>11.311</td>\n",
       "      <td>11.246</td>\n",
       "      <td>11.172</td>\n",
       "      <td>11.094</td>\n",
       "      <td>11.019</td>\n",
       "      <td>...</td>\n",
       "      <td>4.933</td>\n",
       "      <td>4.924</td>\n",
       "      <td>4.916</td>\n",
       "      <td>4.908</td>\n",
       "      <td>4.899</td>\n",
       "      <td>4.889</td>\n",
       "      <td>4.881</td>\n",
       "      <td>4.875</td>\n",
       "      <td>4.867</td>\n",
       "      <td>4.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.357</td>\n",
       "      <td>21.196</td>\n",
       "      <td>21.035</td>\n",
       "      <td>20.874</td>\n",
       "      <td>20.710</td>\n",
       "      <td>20.541</td>\n",
       "      <td>20.367</td>\n",
       "      <td>20.186</td>\n",
       "      <td>19.996</td>\n",
       "      <td>19.793</td>\n",
       "      <td>...</td>\n",
       "      <td>8.009</td>\n",
       "      <td>7.997</td>\n",
       "      <td>7.985</td>\n",
       "      <td>7.974</td>\n",
       "      <td>7.962</td>\n",
       "      <td>7.950</td>\n",
       "      <td>7.941</td>\n",
       "      <td>7.925</td>\n",
       "      <td>7.916</td>\n",
       "      <td>7.902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             10.198            10.146            10.094            10.042   \n",
       "std               2.382             2.359             2.337             2.314   \n",
       "min               4.562             4.555             4.549             4.542   \n",
       "25%               8.449             8.417             8.384             8.348   \n",
       "50%               9.934             9.893             9.839             9.787   \n",
       "75%              11.643            11.582            11.516            11.440   \n",
       "max              21.357            21.196            21.035            20.874   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              9.989             9.936             9.882             9.826   \n",
       "std               2.291             2.268             2.244             2.220   \n",
       "min               4.536             4.530             4.523             4.517   \n",
       "25%               8.314             8.270             8.229             8.191   \n",
       "50%               9.742             9.700             9.649             9.601   \n",
       "75%              11.376            11.311            11.246            11.172   \n",
       "max              20.710            20.541            20.367            20.186   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count         10000.000          10000.000  ...           10000.000   \n",
       "mean              9.769              9.710  ...               4.522   \n",
       "std               2.195              2.169  ...               0.659   \n",
       "min               4.511              4.504  ...               2.375   \n",
       "25%               8.154              8.113  ...               4.065   \n",
       "50%               9.553              9.501  ...               4.498   \n",
       "75%              11.094             11.019  ...               4.933   \n",
       "max              19.996             19.793  ...               8.009   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                4.513               4.505               4.497   \n",
       "std                 0.658               0.657               0.656   \n",
       "min                 2.371               2.365               2.361   \n",
       "25%                 4.057               4.048               4.040   \n",
       "50%                 4.489               4.481               4.473   \n",
       "75%                 4.924               4.916               4.908   \n",
       "max                 7.997               7.985               7.974   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                4.488               4.480               4.472   \n",
       "std                 0.656               0.655               0.654   \n",
       "min                 2.356               2.351               2.348   \n",
       "25%                 4.031               4.023               4.015   \n",
       "50%                 4.465               4.456               4.448   \n",
       "75%                 4.899               4.889               4.881   \n",
       "max                 7.962               7.950               7.941   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count           10000.000           10000.000           10000.000  \n",
       "mean                4.463               4.455               4.447  \n",
       "std                 0.653               0.652               0.652  \n",
       "min                 2.342               2.337               2.334  \n",
       "25%                 4.008               4.000               3.993  \n",
       "50%                 4.440               4.433               4.423  \n",
       "75%                 4.875               4.867               4.858  \n",
       "max                 7.925               7.916               7.902  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:35:59.751430Z",
     "start_time": "2020-12-14T13:35:58.996371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.959</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.051</td>\n",
       "      <td>1.045</td>\n",
       "      <td>1.041</td>\n",
       "      <td>1.038</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.036</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.042</td>\n",
       "      <td>1.045</td>\n",
       "      <td>...</td>\n",
       "      <td>1.210</td>\n",
       "      <td>1.208</td>\n",
       "      <td>1.208</td>\n",
       "      <td>1.207</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.202</td>\n",
       "      <td>1.201</td>\n",
       "      <td>1.196</td>\n",
       "      <td>1.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.104</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.097</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.098</td>\n",
       "      <td>1.101</td>\n",
       "      <td>1.105</td>\n",
       "      <td>1.112</td>\n",
       "      <td>1.119</td>\n",
       "      <td>...</td>\n",
       "      <td>1.639</td>\n",
       "      <td>1.632</td>\n",
       "      <td>1.628</td>\n",
       "      <td>1.630</td>\n",
       "      <td>1.622</td>\n",
       "      <td>1.621</td>\n",
       "      <td>1.620</td>\n",
       "      <td>1.619</td>\n",
       "      <td>1.614</td>\n",
       "      <td>1.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.205</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.201</td>\n",
       "      <td>1.207</td>\n",
       "      <td>1.211</td>\n",
       "      <td>1.221</td>\n",
       "      <td>1.233</td>\n",
       "      <td>1.248</td>\n",
       "      <td>1.263</td>\n",
       "      <td>...</td>\n",
       "      <td>2.337</td>\n",
       "      <td>2.332</td>\n",
       "      <td>2.328</td>\n",
       "      <td>2.318</td>\n",
       "      <td>2.314</td>\n",
       "      <td>2.313</td>\n",
       "      <td>2.313</td>\n",
       "      <td>2.312</td>\n",
       "      <td>2.305</td>\n",
       "      <td>2.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean              inf             inf             inf             inf   \n",
       "std               nan             nan             nan             nan   \n",
       "min             0.959           0.952           0.944           0.922   \n",
       "25%             1.051           1.045           1.041           1.038   \n",
       "50%             1.104           1.100           1.097           1.096   \n",
       "75%             1.205           1.200           1.199           1.201   \n",
       "max               inf             inf             inf             inf   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean              inf             inf             inf             inf   \n",
       "std               nan             nan             nan             nan   \n",
       "min             0.903           0.884           0.877           0.857   \n",
       "25%             1.037           1.036           1.037           1.039   \n",
       "50%             1.096           1.098           1.101           1.105   \n",
       "75%             1.207           1.211           1.221           1.233   \n",
       "max               inf             inf             inf             inf   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count       10000.000        10000.000  ...         10000.000   \n",
       "mean              inf              inf  ...               inf   \n",
       "std               nan              nan  ...               nan   \n",
       "min             0.834            0.812  ...             0.195   \n",
       "25%             1.042            1.045  ...             1.210   \n",
       "50%             1.112            1.119  ...             1.639   \n",
       "75%             1.248            1.263  ...             2.337   \n",
       "max               inf              inf  ...               inf   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean                inf               inf               inf               inf   \n",
       "std                 nan               nan               nan               nan   \n",
       "min               0.194             0.195             0.193             0.193   \n",
       "25%               1.208             1.208             1.207             1.206   \n",
       "50%               1.632             1.628             1.630             1.622   \n",
       "75%               2.332             2.328             2.318             2.314   \n",
       "max                 inf               inf               inf               inf   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean                inf               inf               inf               inf   \n",
       "std                 nan               nan               nan               nan   \n",
       "min               0.194             0.194             0.192             0.190   \n",
       "25%               1.200             1.202             1.201             1.196   \n",
       "50%               1.621             1.620             1.619             1.614   \n",
       "75%               2.313             2.313             2.312             2.305   \n",
       "max                 inf               inf               inf               inf   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count         10000.000  \n",
       "mean                inf  \n",
       "std                 nan  \n",
       "min               0.190  \n",
       "25%               1.195  \n",
       "50%               1.612  \n",
       "75%               2.303  \n",
       "max                 inf  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:36:00.474341Z",
     "start_time": "2020-12-14T13:35:59.753321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.286</td>\n",
       "      <td>1.289</td>\n",
       "      <td>1.295</td>\n",
       "      <td>1.304</td>\n",
       "      <td>1.317</td>\n",
       "      <td>1.333</td>\n",
       "      <td>1.353</td>\n",
       "      <td>1.375</td>\n",
       "      <td>1.402</td>\n",
       "      <td>1.433</td>\n",
       "      <td>...</td>\n",
       "      <td>3.087</td>\n",
       "      <td>3.082</td>\n",
       "      <td>3.080</td>\n",
       "      <td>3.077</td>\n",
       "      <td>3.073</td>\n",
       "      <td>3.070</td>\n",
       "      <td>3.067</td>\n",
       "      <td>3.064</td>\n",
       "      <td>3.060</td>\n",
       "      <td>3.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.642</td>\n",
       "      <td>2.864</td>\n",
       "      <td>3.121</td>\n",
       "      <td>3.411</td>\n",
       "      <td>3.734</td>\n",
       "      <td>4.078</td>\n",
       "      <td>4.446</td>\n",
       "      <td>4.844</td>\n",
       "      <td>5.265</td>\n",
       "      <td>5.708</td>\n",
       "      <td>...</td>\n",
       "      <td>25.200</td>\n",
       "      <td>25.147</td>\n",
       "      <td>25.125</td>\n",
       "      <td>25.169</td>\n",
       "      <td>25.191</td>\n",
       "      <td>25.190</td>\n",
       "      <td>25.277</td>\n",
       "      <td>25.290</td>\n",
       "      <td>25.283</td>\n",
       "      <td>25.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.936</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.021</td>\n",
       "      <td>1.015</td>\n",
       "      <td>1.011</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.005</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.044</td>\n",
       "      <td>1.042</td>\n",
       "      <td>1.041</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.036</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.067</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.061</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.072</td>\n",
       "      <td>1.077</td>\n",
       "      <td>...</td>\n",
       "      <td>1.441</td>\n",
       "      <td>1.439</td>\n",
       "      <td>1.437</td>\n",
       "      <td>1.435</td>\n",
       "      <td>1.432</td>\n",
       "      <td>1.430</td>\n",
       "      <td>1.428</td>\n",
       "      <td>1.425</td>\n",
       "      <td>1.423</td>\n",
       "      <td>1.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.165</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.168</td>\n",
       "      <td>1.176</td>\n",
       "      <td>1.185</td>\n",
       "      <td>1.196</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.224</td>\n",
       "      <td>...</td>\n",
       "      <td>2.166</td>\n",
       "      <td>2.163</td>\n",
       "      <td>2.158</td>\n",
       "      <td>2.154</td>\n",
       "      <td>2.152</td>\n",
       "      <td>2.149</td>\n",
       "      <td>2.147</td>\n",
       "      <td>2.144</td>\n",
       "      <td>2.142</td>\n",
       "      <td>2.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>191.258</td>\n",
       "      <td>222.089</td>\n",
       "      <td>253.743</td>\n",
       "      <td>286.758</td>\n",
       "      <td>321.458</td>\n",
       "      <td>357.165</td>\n",
       "      <td>394.249</td>\n",
       "      <td>433.743</td>\n",
       "      <td>474.984</td>\n",
       "      <td>517.738</td>\n",
       "      <td>...</td>\n",
       "      <td>2139.568</td>\n",
       "      <td>2136.728</td>\n",
       "      <td>2133.159</td>\n",
       "      <td>2140.765</td>\n",
       "      <td>2145.220</td>\n",
       "      <td>2144.263</td>\n",
       "      <td>2154.480</td>\n",
       "      <td>2153.558</td>\n",
       "      <td>2153.874</td>\n",
       "      <td>2155.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.286               1.289               1.295   \n",
       "std                 2.642               2.864               3.121   \n",
       "min                 0.936               0.927               0.911   \n",
       "25%                 1.021               1.015               1.011   \n",
       "50%                 1.067               1.064               1.062   \n",
       "75%                 1.165               1.162               1.162   \n",
       "max               191.258             222.089             253.743   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.304               1.317               1.333   \n",
       "std                 3.411               3.734               4.078   \n",
       "min                 0.895               0.884               0.861   \n",
       "25%                 1.007               1.005               1.003   \n",
       "50%                 1.060               1.061               1.062   \n",
       "75%                 1.162               1.168               1.176   \n",
       "max               286.758             321.458             357.165   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.353               1.375               1.402   \n",
       "std                 4.446               4.844               5.265   \n",
       "min                 0.839               0.824               0.806   \n",
       "25%                 1.003               1.003               1.004   \n",
       "50%                 1.065               1.068               1.072   \n",
       "75%                 1.185               1.196               1.209   \n",
       "max               394.249             433.743             474.984   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count            10000.000  ...             10000.000             10000.000   \n",
       "mean                 1.433  ...                 3.087                 3.082   \n",
       "std                  5.708  ...                25.200                25.147   \n",
       "min                  0.787  ...                 0.204                 0.204   \n",
       "25%                  1.005  ...                 1.049                 1.046   \n",
       "50%                  1.077  ...                 1.441                 1.439   \n",
       "75%                  1.224  ...                 2.166                 2.163   \n",
       "max                517.738  ...              2139.568              2136.728   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  3.080                 3.077                 3.073   \n",
       "std                  25.125                25.169                25.191   \n",
       "min                   0.203                 0.203                 0.202   \n",
       "25%                   1.044                 1.042                 1.041   \n",
       "50%                   1.437                 1.435                 1.432   \n",
       "75%                   2.158                 2.154                 2.152   \n",
       "max                2133.159              2140.765              2145.220   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  3.070                 3.067                 3.064   \n",
       "std                  25.190                25.277                25.290   \n",
       "min                   0.202                 0.201                 0.201   \n",
       "25%                   1.039                 1.037                 1.036   \n",
       "50%                   1.430                 1.428                 1.425   \n",
       "75%                   2.149                 2.147                 2.144   \n",
       "max                2144.263              2154.480              2153.558   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count             10000.000             10000.000  \n",
       "mean                  3.060                 3.057  \n",
       "std                  25.283                25.315  \n",
       "min                   0.200                 0.200  \n",
       "25%                   1.034                 1.033  \n",
       "50%                   1.423                 1.422  \n",
       "75%                   2.142                 2.140  \n",
       "max                2153.874              2155.422  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:36:02.348277Z",
     "start_time": "2020-12-14T13:36:00.476193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:36:04.045492Z",
     "start_time": "2020-12-14T13:36:02.350311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['trainover', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
