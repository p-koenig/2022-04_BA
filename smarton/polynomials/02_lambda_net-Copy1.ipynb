{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:24.455738Z",
     "start_time": "2020-12-02T16:38:24.448748Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:24.467485Z",
     "start_time": "2020-12-02T16:38:24.457645Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 10000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 20  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "fixed_seed_lambda_training = True\n",
    "initialize_network_zero = False\n",
    "initialize_network_one = True\n",
    "\n",
    "\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:24.478648Z",
     "start_time": "2020-12-02T16:38:24.469240Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if each_epochs_save != None:\n",
    "    epochs_save_range = range(1, epochs//each_epochs_save+1) if each_epochs_save == 1 else range(epochs//each_epochs_save+1)\n",
    "else:\n",
    "    epochs_save_range = None\n",
    "    \n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_SeedMethod'\n",
    "elif not fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_NoSeedMethod'\n",
    "    \n",
    "if initialize_network_zero:\n",
    "    seed_shuffle_string = seed_shuffle_string + '_zero_initialize'\n",
    "\n",
    "if initialize_network_one:\n",
    "    seed_shuffle_string = seed_shuffle_string + '_ones_initialize'   \n",
    "    \n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:29.088962Z",
     "start_time": "2020-12-02T16:38:24.480719Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:29.097376Z",
     "start_time": "2020-12-02T16:38:29.091587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:29.115375Z",
     "start_time": "2020-12-02T16:38:29.099537Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "    \n",
    "    \n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:29.122106Z",
     "start_time": "2020-12-02T16:38:29.117097Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcualate_function_value_with_X_data_entry(coefficient_list, X_data_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "     \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [X_data_value**int(coefficient_multiplier) for coefficient_multiplier, X_data_value in zip(coefficient_multipliers, X_data_entry)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "        \n",
    "    return result, np.append(X_data_entry, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:29.155463Z",
     "start_time": "2020-12-02T16:38:29.124853Z"
    },
    "code_folding": [
     0,
     20,
     43,
     66,
     88,
     91,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:29.215618Z",
     "start_time": "2020-12-02T16:38:29.158125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1bde7fa9e549758c5358142b4dbad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e363b9f0b44a45199a82e6b6dc05547f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:38.872387Z",
     "start_time": "2020-12-02T16:38:29.217474Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:38.930552Z",
     "start_time": "2020-12-02T16:38:38.874982Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        \n",
    "        for i in epochs_save_range:\n",
    "            index = i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:38.955047Z",
     "start_time": "2020-12-02T16:38:38.934004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.480</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.830</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.420  0.820 -0.680 -0.640\n",
       "1 -0.980 -0.820  0.060 -0.300\n",
       "2 -0.480  0.000 -0.370  0.600\n",
       "3 -0.250  0.630 -0.480 -0.960\n",
       "4 -0.830  0.230 -0.170 -0.460"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:38.964456Z",
     "start_time": "2020-12-02T16:38:38.956579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.610</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.800</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.950</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0  0.610  0.220 -0.980  0.170\n",
       "1 -0.230 -0.050 -0.900  0.600\n",
       "2  0.800 -0.540  0.550  0.440\n",
       "3  0.030 -0.600 -0.320 -0.430\n",
       "4  0.950  0.770 -0.250  0.530"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:38.971133Z",
     "start_time": "2020-12-02T16:38:38.966361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:38.977803Z",
     "start_time": "2020-12-02T16:38:38.972830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:39.017904Z",
     "start_time": "2020-12-02T16:38:38.979740Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lambda_net(identifier, \n",
    "                        X_data_real_lambda, \n",
    "                        y_data_real_lambda, \n",
    "                        y_data_pred_lambda, \n",
    "                        y_data_pred_lambda_poly_lstsq, \n",
    "                        y_data_real_lambda_poly_lstsq):\n",
    "    \n",
    "    mae_real_VS_predLambda = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    mae_predLambda_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_realPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    rmse_real_VS_predLambda = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    rmse_predLambda_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_realPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    mape_real_VS_predLambda = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    mape_predLambda_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_realPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)            \n",
    "\n",
    "    r2_real_VS_predLambda = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_predPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    r2_predLambda_VS_predPolyLstsq = np.round(r2_score(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_realPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    raae_real_VS_predLambda = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    raae_predLambda_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_realPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    rmae_real_VS_predLambda = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    rmae_predLambda_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_realPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    fd_real_VS_predLambda = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_predLambda_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_realPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "    dtw_real_VS_predLambda, dtw_complete_real_VS_predLambda = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predLambda = np.round(dtw_real_VS_predLambda, 4)\n",
    "    dtw_real_VS_predPolyLstsq, dtw_complete_real_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predPolyLstsq = np.round(dtw_real_VS_predPolyLstsq, 4)\n",
    "    dtw_predLambda_VS_predPolyLstsq, dtw_complete_predLambda_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_predLambda_VS_predPolyLstsq = np.round(dtw_predLambda_VS_predPolyLstsq, 4)    \n",
    "    dtw_real_VS_realPolyLstsq, dtw_complete_real_VS_realPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_realPolyLstsq = np.round(dtw_real_VS_realPolyLstsq, 4) \n",
    "        \n",
    "    std_data_real_lambda = np.round(np.std(y_data_real_lambda), 4) \n",
    "    std_data_pred_lambda = np.round(np.std(y_data_pred_lambda), 4) \n",
    "    std_data_pred_lambda_poly_lstsq = np.round(np.std(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    std_data_real_lambda_poly_lstsq = np.round(np.std(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    mean_data_real_lambda = np.round(np.mean(y_data_real_lambda), 4) \n",
    "    mean_data_pred_lambda = np.round(np.mean(y_data_pred_lambda), 4) \n",
    "    mean_data_pred_lambda_poly_lstsq = np.round(np.mean(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    mean_data_real_lambda_poly_lstsq = np.round(np.mean(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    return [{\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mae_real_VS_predLambda,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_real_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_predLambda_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mae_real_VS_realPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmse_real_VS_predLambda,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_real_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_predLambda_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmse_real_VS_realPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mape_real_VS_predLambda,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_real_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_predLambda_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mape_real_VS_realPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': r2_real_VS_predLambda,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_real_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_predLambda_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': r2_real_VS_realPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': raae_real_VS_predLambda,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_real_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_predLambda_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': raae_real_VS_realPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmae_real_VS_predLambda,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_real_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_predLambda_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmae_real_VS_realPolyLstsq,\n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': fd_real_VS_predLambda,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_real_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_predLambda_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': fd_real_VS_realPolyLstsq,   \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': dtw_real_VS_predLambda, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_real_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_predLambda_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': dtw_real_VS_realPolyLstsq, \n",
    "            },\n",
    "            {\n",
    "             'STD FV ' + identifier + ' REAL LAMBDA': std_data_real_lambda,\n",
    "             'STD FV ' + identifier + ' PRED LAMBDA': std_data_pred_lambda, \n",
    "             'STD FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': std_data_pred_lambda_poly_lstsq, \n",
    "             'STD FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': std_data_real_lambda_poly_lstsq, \n",
    "            },\n",
    "            {\n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA': mean_data_real_lambda,\n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA': mean_data_pred_lambda, \n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': mean_data_pred_lambda_poly_lstsq, \n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': mean_data_real_lambda_poly_lstsq, \n",
    "            }]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:39.024047Z",
     "start_time": "2020-12-02T16:38:39.019739Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_function_values_from_polynomial(X_data, polynomial):\n",
    "    function_value_list = []\n",
    "    for entry in X_data:\n",
    "        function_value, _ = calcualate_function_value_with_X_data_entry(polynomial, entry)\n",
    "        function_value_list.append(function_value)\n",
    "    function_value_array = np.array(function_value_list).reshape(len(function_value_list), 1)     \n",
    "\n",
    "    return function_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:39.032085Z",
     "start_time": "2020-12-02T16:38:39.025922Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_term_matric_for_lstsq(X_data, polynomial_indices):\n",
    "    term_list_all = []\n",
    "    y = 0\n",
    "    for term in list(polynomial_indices):\n",
    "        term_list = [int(value_mult) for value_mult in term]\n",
    "        term_list_all.append(term_list)\n",
    "    terms_matrix = []\n",
    "    for unknowns in X_data:\n",
    "        terms = []\n",
    "        for term_multipliers in term_list_all:\n",
    "            term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "            terms.append(term_value)\n",
    "        terms_matrix.append(np.array(terms))\n",
    "    terms_matrix = np.array(terms_matrix)\n",
    "    \n",
    "    return terms_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T16:38:39.092477Z",
     "start_time": "2020-12-02T16:38:39.033987Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn(X_data_lambda, y_data_real_lambda, polynomial, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    if fixed_seed_lambda_training:\n",
    "        random.seed(RANDOM_SEED)\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        if int(tf.__version__[0]) >= 2:\n",
    "            tf.random.set_seed(RANDOM_SEED)\n",
    "        else:\n",
    "            tf.set_random_seed(RANDOM_SEED) \n",
    "        \n",
    "    if isinstance(X_data_lambda, pd.DataFrame):\n",
    "        X_data_lambda = X_data_lambda.values\n",
    "    if isinstance(y_data_real_lambda, pd.DataFrame):\n",
    "        y_data_real_lambda = y_data_real_lambda.values\n",
    "                \n",
    "    X_train_lambda_with_valid, X_test_lambda, y_train_real_lambda_with_valid, y_test_real_lambda = train_test_split(X_data_lambda, y_data_real_lambda, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    X_train_lambda, X_valid_lambda, y_train_real_lambda, y_valid_real_lambda = train_test_split(X_train_lambda_with_valid, y_train_real_lambda_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "     \n",
    "        \n",
    "    model = Sequential()\n",
    "\n",
    "    if initialize_network_one:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer='ones', bias_initializer='ones')) #1024\n",
    "    else:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1])) #1024\n",
    "        \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        if initialize_network_one:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer='ones', bias_initializer='ones'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "    \n",
    "    if initialize_network_one:\n",
    "        model.add(Dense(1, kernel_initializer='ones', bias_initializer='ones'))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_lstsq_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_train_pred_lambda = model.predict(X_train_lambda) \n",
    "        y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "        y_test_pred_lambda = model.predict(X_test_lambda)\n",
    "    \n",
    "        terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                \n",
    "        polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "        y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "        y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "        y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)    \n",
    "        y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)\n",
    "        y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)  \n",
    "        \n",
    "        pred_list = (y_train_real_lambda, \n",
    "                     y_train_pred_lambda, \n",
    "                     y_train_pred_lambda_poly_lstsq,\n",
    "                     #y_train_real_lambda_poly_lstsq,\n",
    "                     X_train_lambda, \n",
    "                     y_valid_real_lambda,\n",
    "                     y_valid_pred_lambda, \n",
    "                     y_valid_pred_lambda_poly_lstsq,\n",
    "                     #y_valid_real_lambda_poly_lstsq,\n",
    "                     X_valid_lambda, \n",
    "                     y_test_real_lambda, \n",
    "                     y_test_pred_lambda, \n",
    "                     y_test_pred_lambda_poly_lstsq, \n",
    "                     #y_test_real_lambda_poly_lstsq,\n",
    "                     X_test_lambda)\n",
    "\n",
    "        scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "        scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "        scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "        scores_std = {}\n",
    "        for aDict in (std_train, std_valid, std_test):\n",
    "            scores_std.update(aDict)      \n",
    "        scores_mean = {}\n",
    "        for aDict in (mean_train, mean_valid, mean_test):\n",
    "            scores_mean.update(aDict)\n",
    "        \n",
    "        scores_list =  [scores_train,\n",
    "                             scores_valid,\n",
    "                             scores_test,\n",
    "                             scores_std,\n",
    "                             scores_mean]            \n",
    "                            \n",
    "    else:\n",
    "        scores_list = []\n",
    "        pred_list = []\n",
    "        for i in epochs_save_range:\n",
    "            train_epochs_step = each_epochs_save if i > 1 else max(each_epochs_save-1, 1) if i==1 else 1\n",
    "            \n",
    "            model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=train_epochs_step, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=1,\n",
    "                      use_multiprocessing=False)\n",
    "            \n",
    "            #history adjustment for continuing training\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                history = model_history.history\n",
    "            else:\n",
    "                history = mergeDict(history, model_history.history)\n",
    "                #for key_1 in history.keys():\n",
    "                #    for key_2 in model_history.history.keys():\n",
    "                #        if key_1 == key_2:\n",
    "                #            history[key_1] += model_history.history[key_2]  \n",
    "\n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_train_pred_lambda = model.predict(X_train_lambda)                \n",
    "            y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "            y_test_pred_lambda = model.predict(X_test_lambda)        \n",
    "\n",
    "            terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                        \n",
    "            polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            \n",
    "            y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "            y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "            y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)           \n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "                y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)  \n",
    "                y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)                    \n",
    "                \n",
    "            pred_list.append((y_train_real_lambda, \n",
    "                              y_train_pred_lambda, \n",
    "                              y_train_pred_lambda_poly_lstsq,\n",
    "                              #y_train_real_lambda_poly_lstsq,\n",
    "                              X_train_lambda, \n",
    "                              y_valid_real_lambda,\n",
    "                              y_valid_pred_lambda, \n",
    "                              y_valid_pred_lambda_poly_lstsq,\n",
    "                              #y_valid_real_lambda_poly_lstsq,\n",
    "                              X_valid_lambda, \n",
    "                              y_test_real_lambda, \n",
    "                              y_test_pred_lambda, \n",
    "                              y_test_pred_lambda_poly_lstsq, \n",
    "                              #y_test_real_lambda_poly_lstsq,\n",
    "                              X_test_lambda))\n",
    "    \n",
    "            scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "            scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "            scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "            scores_std = {}\n",
    "            for aDict in (std_train, std_valid, std_test):\n",
    "                scores_std.update(aDict)\n",
    "            scores_mean = {}\n",
    "            for aDict in (mean_train, mean_valid, mean_test):\n",
    "                scores_mean.update(aDict)\n",
    "\n",
    "            scores_list_single_epoch =  [scores_train,\n",
    "                                              scores_valid,\n",
    "                                              scores_test,\n",
    "                                              scores_std,\n",
    "                                              scores_mean]        \n",
    "                  \n",
    "            scores_list.append(scores_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_lstsq_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_lstsq_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save == None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                for i, value in enumerate(polynomial.values):\n",
    "                    if i == 0:\n",
    "                        text_file.write(str(value))  \n",
    "                    else:\n",
    "                        text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_lstsq_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, #polynomial_lstsq_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T19:51:12.352614Z",
     "start_time": "2020-12-02T16:38:39.094385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    7.978, max:  190.176, cur:    7.984)\n",
      "\tvalidation       \t (min:    7.832, max:  174.951, cur:    7.930)\n",
      "mean_absolute_percentage_error_keras\n",
      "\ttraining         \t (min:    1.742, max:   79.548, cur:    2.364)\n",
      "\tvalidation       \t (min:    1.181, max:   47.267, cur:    1.349)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:   10.769, max:  246.160, cur:   10.992)\n",
      "\tvalidation       \t (min:   10.223, max:  222.401, cur:   10.434)\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(X_data[1].values, y_data[1].values, X_data[0], return_history=True, each_epochs_save=each_epochs_save, printing=True) for X_data, y_data in zip(X_data_list_split, y_data_list_split))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T19:51:17.945382Z",
     "start_time": "2020-12-02T19:51:12.354752Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    scores_list_train = [clf[1][0] for clf in clf_list]\n",
    "    scores_list_valid = [clf[1][1] for clf in clf_list]\n",
    "    scores_list_test = [clf[1][2] for clf in clf_list]\n",
    "    scores_list_stds = [clf[1][3] for clf in clf_list]\n",
    "    scores_list_means = [clf[1][4] for clf in clf_list]\n",
    "\n",
    "    scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_list_by_epochs = [[] for i in epochs_save_range]\n",
    "    for scores_list in scores_list:   \n",
    "        for index, scores in enumerate(scores_list):\n",
    "            scores_list_by_epochs[index].append(scores)\n",
    "            \n",
    "        \n",
    "    for i, scores_list_single_epoch in enumerate(scores_list_by_epochs):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "           \n",
    "        scores_list_train = [scores_list[0] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_valid = [scores_list[1] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_test = [scores_list[2] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_stds = [scores_list[3] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_means = [scores_list[4] for scores_list in scores_list_single_epoch]\n",
    "        \n",
    "        scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()  \n",
    "        scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()  \n",
    "        scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T19:51:17.971862Z",
     "start_time": "2020-12-02T19:51:17.948224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E1</th>\n",
       "      <th>TRAIN POLY E1</th>\n",
       "      <th>TRAIN POLY PRED E1</th>\n",
       "      <th>TRAIN LSTSQ E1</th>\n",
       "      <th>TRAIN PRED E20</th>\n",
       "      <th>TRAIN POLY E20</th>\n",
       "      <th>TRAIN POLY PRED E20</th>\n",
       "      <th>TRAIN LSTSQ E20</th>\n",
       "      <th>TRAIN PRED E40</th>\n",
       "      <th>TRAIN POLY E40</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E160</th>\n",
       "      <th>TRAIN LSTSQ E160</th>\n",
       "      <th>TRAIN PRED E180</th>\n",
       "      <th>TRAIN POLY E180</th>\n",
       "      <th>TRAIN POLY PRED E180</th>\n",
       "      <th>TRAIN LSTSQ E180</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>169.814</td>\n",
       "      <td>171.597</td>\n",
       "      <td>7.741</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.914</td>\n",
       "      <td>8.903</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.767</td>\n",
       "      <td>8.757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.097</td>\n",
       "      <td>8.088</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.039</td>\n",
       "      <td>8.029</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>224.678</td>\n",
       "      <td>224.424</td>\n",
       "      <td>10.647</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.673</td>\n",
       "      <td>11.667</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.515</td>\n",
       "      <td>11.509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.786</td>\n",
       "      <td>10.780</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.720</td>\n",
       "      <td>10.714</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>274.556</td>\n",
       "      <td>274.567</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.195</td>\n",
       "      <td>6.071</td>\n",
       "      <td>1.857</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.253</td>\n",
       "      <td>6.084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.070</td>\n",
       "      <td>6.827</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.126</td>\n",
       "      <td>6.879</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-448.167</td>\n",
       "      <td>-447.151</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>15.732</td>\n",
       "      <td>15.900</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>2.483</td>\n",
       "      <td>4.006</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.451</td>\n",
       "      <td>3.457</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.434</td>\n",
       "      <td>3.439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.353</td>\n",
       "      <td>3.360</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.346</td>\n",
       "      <td>3.353</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>406.713</td>\n",
       "      <td>408.867</td>\n",
       "      <td>21.217</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.548</td>\n",
       "      <td>20.491</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.449</td>\n",
       "      <td>20.389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.894</td>\n",
       "      <td>19.815</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.821</td>\n",
       "      <td>19.740</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>1659.979</td>\n",
       "      <td>1680.567</td>\n",
       "      <td>77.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>83.765</td>\n",
       "      <td>83.748</td>\n",
       "      <td>2.249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>82.718</td>\n",
       "      <td>82.698</td>\n",
       "      <td>...</td>\n",
       "      <td>1.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>77.855</td>\n",
       "      <td>77.803</td>\n",
       "      <td>2.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>77.384</td>\n",
       "      <td>77.329</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E1  TRAIN POLY E1  TRAIN POLY PRED E1  TRAIN LSTSQ E1  \\\n",
       "MAE FV         169.814        171.597               7.741           0.000   \n",
       "RMSE FV        224.678        224.424              10.647           0.000   \n",
       "MAPE FV        274.556        274.567               0.422           0.000   \n",
       "R2 FV         -448.167       -447.151               0.995           1.000   \n",
       "RAAE FV         15.732         15.900               0.052           0.000   \n",
       "RMAE FV          2.483          4.006               0.507           0.000   \n",
       "FD FV          406.713        408.867              21.217           0.000   \n",
       "DTW FV        1659.979       1680.567              77.439           0.000   \n",
       "\n",
       "         TRAIN PRED E20  TRAIN POLY E20  TRAIN POLY PRED E20  TRAIN LSTSQ E20  \\\n",
       "MAE FV            8.914           8.903                0.225            0.000   \n",
       "RMSE FV          11.673          11.667                0.304            0.000   \n",
       "MAPE FV           6.195           6.071                1.857            0.000   \n",
       "R2 FV            -0.128          -0.127                0.991            1.000   \n",
       "RAAE FV           0.804           0.803                0.071            0.000   \n",
       "RMAE FV           3.451           3.457                0.460            0.000   \n",
       "FD FV            20.548          20.491                0.585            0.000   \n",
       "DTW FV           83.765          83.748                2.249            0.000   \n",
       "\n",
       "         TRAIN PRED E40  TRAIN POLY E40  ...  TRAIN POLY PRED E160  \\\n",
       "MAE FV            8.767           8.757  ...                 0.201   \n",
       "RMSE FV          11.515          11.509  ...                 0.272   \n",
       "MAPE FV           6.253           6.084  ...                 0.592   \n",
       "R2 FV            -0.094          -0.093  ...                 0.991   \n",
       "RAAE FV           0.790           0.789  ...                 0.070   \n",
       "RMAE FV           3.434           3.439  ...                 0.456   \n",
       "FD FV            20.449          20.389  ...                 0.524   \n",
       "DTW FV           82.718          82.698  ...                 1.998   \n",
       "\n",
       "         TRAIN LSTSQ E160  TRAIN PRED E180  TRAIN POLY E180  \\\n",
       "MAE FV              0.000            8.097            8.088   \n",
       "RMSE FV             0.000           10.786           10.780   \n",
       "MAPE FV             0.000            7.070            6.827   \n",
       "R2 FV               1.000            0.050            0.051   \n",
       "RAAE FV             0.000            0.728            0.727   \n",
       "RMAE FV             0.000            3.353            3.360   \n",
       "FD FV               0.000           19.894           19.815   \n",
       "DTW FV              0.000           77.855           77.803   \n",
       "\n",
       "         TRAIN POLY PRED E180  TRAIN LSTSQ E180  TRAIN PRED E200  \\\n",
       "MAE FV                  0.201             0.000            8.039   \n",
       "RMSE FV                 0.273             0.000           10.720   \n",
       "MAPE FV                 0.640             0.000            7.126   \n",
       "R2 FV                   0.991             1.000            0.062   \n",
       "RAAE FV                 0.070             0.000            0.723   \n",
       "RMAE FV                 0.455             0.000            3.346   \n",
       "FD FV                   0.525             0.000           19.821   \n",
       "DTW FV                  2.001             0.000           77.384   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             8.029                 0.202             0.000  \n",
       "RMSE FV           10.714                 0.274             0.000  \n",
       "MAPE FV            6.879                 0.887             0.000  \n",
       "R2 FV              0.063                 0.991             1.000  \n",
       "RAAE FV            0.722                 0.070             0.000  \n",
       "RMAE FV            3.353                 0.456             0.000  \n",
       "FD FV             19.740                 0.527             0.000  \n",
       "DTW FV            77.329                 2.010             0.000  \n",
       "\n",
       "[8 rows x 44 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T19:51:17.997096Z",
     "start_time": "2020-12-02T19:51:17.976329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E1</th>\n",
       "      <th>VALID POLY E1</th>\n",
       "      <th>VALID POLY PRED E1</th>\n",
       "      <th>VALID LSTSQ E1</th>\n",
       "      <th>VALID PRED E20</th>\n",
       "      <th>VALID POLY E20</th>\n",
       "      <th>VALID POLY PRED E20</th>\n",
       "      <th>VALID LSTSQ E20</th>\n",
       "      <th>VALID PRED E40</th>\n",
       "      <th>VALID POLY E40</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E160</th>\n",
       "      <th>VALID LSTSQ E160</th>\n",
       "      <th>VALID PRED E180</th>\n",
       "      <th>VALID POLY E180</th>\n",
       "      <th>VALID POLY PRED E180</th>\n",
       "      <th>VALID LSTSQ E180</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>169.752</td>\n",
       "      <td>171.780</td>\n",
       "      <td>8.332</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.921</td>\n",
       "      <td>8.912</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.776</td>\n",
       "      <td>8.767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.118</td>\n",
       "      <td>8.109</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.061</td>\n",
       "      <td>8.052</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>224.223</td>\n",
       "      <td>224.002</td>\n",
       "      <td>11.736</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.653</td>\n",
       "      <td>11.649</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.497</td>\n",
       "      <td>11.493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.777</td>\n",
       "      <td>10.773</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.713</td>\n",
       "      <td>10.709</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>143.374</td>\n",
       "      <td>152.371</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.539</td>\n",
       "      <td>2.643</td>\n",
       "      <td>1.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.805</td>\n",
       "      <td>3.763</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.912</td>\n",
       "      <td>3.865</td>\n",
       "      <td>2.568</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-454.977</td>\n",
       "      <td>-454.078</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>15.842</td>\n",
       "      <td>16.034</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>2.049</td>\n",
       "      <td>3.668</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.015</td>\n",
       "      <td>3.020</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.999</td>\n",
       "      <td>3.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.924</td>\n",
       "      <td>2.929</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.917</td>\n",
       "      <td>2.922</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>405.723</td>\n",
       "      <td>408.254</td>\n",
       "      <td>22.946</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.179</td>\n",
       "      <td>20.121</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.089</td>\n",
       "      <td>20.026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.572</td>\n",
       "      <td>19.490</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.502</td>\n",
       "      <td>19.417</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>1663.514</td>\n",
       "      <td>1688.408</td>\n",
       "      <td>83.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>83.013</td>\n",
       "      <td>83.001</td>\n",
       "      <td>2.451</td>\n",
       "      <td>0.000</td>\n",
       "      <td>82.054</td>\n",
       "      <td>82.036</td>\n",
       "      <td>...</td>\n",
       "      <td>2.184</td>\n",
       "      <td>0.000</td>\n",
       "      <td>77.354</td>\n",
       "      <td>77.338</td>\n",
       "      <td>2.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>76.901</td>\n",
       "      <td>76.880</td>\n",
       "      <td>2.198</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E1  VALID POLY E1  VALID POLY PRED E1  VALID LSTSQ E1  \\\n",
       "MAE FV         169.752        171.780               8.332           0.000   \n",
       "RMSE FV        224.223        224.002              11.736           0.000   \n",
       "MAPE FV        143.374        152.371               0.342           0.000   \n",
       "R2 FV         -454.977       -454.078               0.994           1.000   \n",
       "RAAE FV         15.842         16.034               0.056           0.000   \n",
       "RMAE FV          2.049          3.668               0.380           0.000   \n",
       "FD FV          405.723        408.254              22.946           0.000   \n",
       "DTW FV        1663.514       1688.408              83.021           0.000   \n",
       "\n",
       "         VALID PRED E20  VALID POLY E20  VALID POLY PRED E20  VALID LSTSQ E20  \\\n",
       "MAE FV            8.921           8.912                0.242            0.000   \n",
       "RMSE FV          11.653          11.649                0.334            0.000   \n",
       "MAPE FV           2.539           2.643                1.970            0.000   \n",
       "R2 FV            -0.137          -0.136                0.988            1.000   \n",
       "RAAE FV           0.809           0.808                0.077            0.000   \n",
       "RMAE FV           3.015           3.020                0.382            0.000   \n",
       "FD FV            20.179          20.121                0.648            0.000   \n",
       "DTW FV           83.013          83.001                2.451            0.000   \n",
       "\n",
       "         VALID PRED E40  VALID POLY E40  ...  VALID POLY PRED E160  \\\n",
       "MAE FV            8.776           8.767  ...                 0.215   \n",
       "RMSE FV          11.497          11.493  ...                 0.299   \n",
       "MAPE FV           2.720           2.777  ...                 0.535   \n",
       "R2 FV            -0.103          -0.102  ...                 0.989   \n",
       "RAAE FV           0.795           0.794  ...                 0.075   \n",
       "RMAE FV           2.999           3.004  ...                 0.376   \n",
       "FD FV            20.089          20.026  ...                 0.581   \n",
       "DTW FV           82.054          82.036  ...                 2.184   \n",
       "\n",
       "         VALID LSTSQ E160  VALID PRED E180  VALID POLY E180  \\\n",
       "MAE FV              0.000            8.118            8.109   \n",
       "RMSE FV             0.000           10.777           10.773   \n",
       "MAPE FV             0.000            3.805            3.763   \n",
       "R2 FV               1.000            0.042            0.043   \n",
       "RAAE FV             0.000            0.734            0.733   \n",
       "RMAE FV             0.000            2.924            2.929   \n",
       "FD FV               0.000           19.572           19.490   \n",
       "DTW FV              0.000           77.354           77.338   \n",
       "\n",
       "         VALID POLY PRED E180  VALID LSTSQ E180  VALID PRED E200  \\\n",
       "MAE FV                  0.216             0.000            8.061   \n",
       "RMSE FV                 0.299             0.000           10.713   \n",
       "MAPE FV                 0.570             0.000            3.912   \n",
       "R2 FV                   0.989             1.000            0.054   \n",
       "RAAE FV                 0.075             0.000            0.729   \n",
       "RMAE FV                 0.377             0.000            2.917   \n",
       "FD FV                   0.581             0.000           19.502   \n",
       "DTW FV                  2.187             0.000           76.901   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             8.052                 0.217             0.000  \n",
       "RMSE FV           10.709                 0.301             0.000  \n",
       "MAPE FV            3.865                 2.568             0.000  \n",
       "R2 FV              0.054                 0.989             1.000  \n",
       "RAAE FV            0.728                 0.075             0.000  \n",
       "RMAE FV            2.922                 0.377             0.000  \n",
       "FD FV             19.417                 0.584             0.000  \n",
       "DTW FV            76.880                 2.198             0.000  \n",
       "\n",
       "[8 rows x 44 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T19:51:18.019732Z",
     "start_time": "2020-12-02T19:51:17.999311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E1</th>\n",
       "      <th>TEST POLY E1</th>\n",
       "      <th>TEST POLY PRED E1</th>\n",
       "      <th>TEST LSTSQ E1</th>\n",
       "      <th>TEST PRED E20</th>\n",
       "      <th>TEST POLY E20</th>\n",
       "      <th>TEST POLY PRED E20</th>\n",
       "      <th>TEST LSTSQ E20</th>\n",
       "      <th>TEST PRED E40</th>\n",
       "      <th>TEST POLY E40</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E160</th>\n",
       "      <th>TEST LSTSQ E160</th>\n",
       "      <th>TEST PRED E180</th>\n",
       "      <th>TEST POLY E180</th>\n",
       "      <th>TEST POLY PRED E180</th>\n",
       "      <th>TEST LSTSQ E180</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>169.635</td>\n",
       "      <td>171.631</td>\n",
       "      <td>8.323</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.927</td>\n",
       "      <td>8.918</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.782</td>\n",
       "      <td>8.773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.120</td>\n",
       "      <td>8.111</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.063</td>\n",
       "      <td>8.053</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>224.273</td>\n",
       "      <td>224.034</td>\n",
       "      <td>11.679</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.660</td>\n",
       "      <td>11.655</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.503</td>\n",
       "      <td>11.499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.779</td>\n",
       "      <td>10.775</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.715</td>\n",
       "      <td>10.710</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>264.599</td>\n",
       "      <td>274.250</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.455</td>\n",
       "      <td>4.999</td>\n",
       "      <td>1.404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.604</td>\n",
       "      <td>5.136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.880</td>\n",
       "      <td>6.379</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.002</td>\n",
       "      <td>6.503</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-452.690</td>\n",
       "      <td>-451.717</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>15.801</td>\n",
       "      <td>15.990</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>2.173</td>\n",
       "      <td>3.824</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.136</td>\n",
       "      <td>3.142</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.119</td>\n",
       "      <td>3.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.040</td>\n",
       "      <td>3.047</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.032</td>\n",
       "      <td>3.039</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>412.583</td>\n",
       "      <td>414.574</td>\n",
       "      <td>23.445</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.296</td>\n",
       "      <td>20.241</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.210</td>\n",
       "      <td>20.150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.684</td>\n",
       "      <td>19.602</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.615</td>\n",
       "      <td>19.530</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>1677.403</td>\n",
       "      <td>1702.309</td>\n",
       "      <td>83.938</td>\n",
       "      <td>0.000</td>\n",
       "      <td>83.545</td>\n",
       "      <td>83.530</td>\n",
       "      <td>2.426</td>\n",
       "      <td>0.000</td>\n",
       "      <td>82.552</td>\n",
       "      <td>82.531</td>\n",
       "      <td>...</td>\n",
       "      <td>2.155</td>\n",
       "      <td>0.000</td>\n",
       "      <td>77.697</td>\n",
       "      <td>77.648</td>\n",
       "      <td>2.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>77.219</td>\n",
       "      <td>77.168</td>\n",
       "      <td>2.169</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E1  TEST POLY E1  TEST POLY PRED E1  TEST LSTSQ E1  \\\n",
       "MAE FV        169.635       171.631              8.323          0.000   \n",
       "RMSE FV       224.273       224.034             11.679          0.000   \n",
       "MAPE FV       264.599       274.250              0.342          0.000   \n",
       "R2 FV        -452.690      -451.717              0.994          1.000   \n",
       "RAAE FV        15.801        15.990              0.056          0.000   \n",
       "RMAE FV         2.173         3.824              0.413          0.000   \n",
       "FD FV         412.583       414.574             23.445          0.000   \n",
       "DTW FV       1677.403      1702.309             83.938          0.000   \n",
       "\n",
       "         TEST PRED E20  TEST POLY E20  TEST POLY PRED E20  TEST LSTSQ E20  \\\n",
       "MAE FV           8.927          8.918               0.242           0.000   \n",
       "RMSE FV         11.660         11.655               0.333           0.000   \n",
       "MAPE FV          4.455          4.999               1.404           0.000   \n",
       "R2 FV           -0.137         -0.136               0.989           1.000   \n",
       "RAAE FV          0.809          0.808               0.076           0.000   \n",
       "RMAE FV          3.136          3.142               0.411           0.000   \n",
       "FD FV           20.296         20.241               0.648           0.000   \n",
       "DTW FV          83.545         83.530               2.426           0.000   \n",
       "\n",
       "         TEST PRED E40  TEST POLY E40  ...  TEST POLY PRED E160  \\\n",
       "MAE FV           8.782          8.773  ...                0.215   \n",
       "RMSE FV         11.503         11.499  ...                0.298   \n",
       "MAPE FV          4.604          5.136  ...                0.458   \n",
       "R2 FV           -0.102         -0.102  ...                0.989   \n",
       "RAAE FV          0.795          0.794  ...                0.075   \n",
       "RMAE FV          3.119          3.125  ...                0.406   \n",
       "FD FV           20.210         20.150  ...                0.580   \n",
       "DTW FV          82.552         82.531  ...                2.155   \n",
       "\n",
       "         TEST LSTSQ E160  TEST PRED E180  TEST POLY E180  TEST POLY PRED E180  \\\n",
       "MAE FV             0.000           8.120           8.111                0.216   \n",
       "RMSE FV            0.000          10.779          10.775                0.298   \n",
       "MAPE FV            0.000           5.880           6.379                0.570   \n",
       "R2 FV              1.000           0.043           0.044                0.989   \n",
       "RAAE FV            0.000           0.733           0.733                0.075   \n",
       "RMAE FV            0.000           3.040           3.047                0.406   \n",
       "FD FV              0.000          19.684          19.602                0.581   \n",
       "DTW FV             0.000          77.697          77.648                2.159   \n",
       "\n",
       "         TEST LSTSQ E180  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           8.063           8.053                0.217   \n",
       "RMSE FV            0.000          10.715          10.710                0.300   \n",
       "MAPE FV            0.000           6.002           6.503                0.444   \n",
       "R2 FV              1.000           0.055           0.055                0.989   \n",
       "RAAE FV            0.000           0.728           0.727                0.075   \n",
       "RMAE FV            0.000           3.032           3.039                0.406   \n",
       "FD FV              0.000          19.615          19.530                0.584   \n",
       "DTW FV             0.000          77.219          77.168                2.169   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "FD FV              0.000  \n",
       "DTW FV             0.000  \n",
       "\n",
       "[8 rows x 44 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T19:51:18.034825Z",
     "start_time": "2020-12-02T19:51:18.021501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E20</th>\n",
       "      <th>E40</th>\n",
       "      <th>E60</th>\n",
       "      <th>E80</th>\n",
       "      <th>E100</th>\n",
       "      <th>E120</th>\n",
       "      <th>E140</th>\n",
       "      <th>E160</th>\n",
       "      <th>E180</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA</th>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA</th>\n",
       "      <td>149.125</td>\n",
       "      <td>3.198</td>\n",
       "      <td>3.113</td>\n",
       "      <td>3.046</td>\n",
       "      <td>2.994</td>\n",
       "      <td>2.957</td>\n",
       "      <td>2.933</td>\n",
       "      <td>2.921</td>\n",
       "      <td>2.920</td>\n",
       "      <td>2.927</td>\n",
       "      <td>2.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>148.741</td>\n",
       "      <td>3.183</td>\n",
       "      <td>3.099</td>\n",
       "      <td>3.032</td>\n",
       "      <td>2.981</td>\n",
       "      <td>2.944</td>\n",
       "      <td>2.920</td>\n",
       "      <td>2.909</td>\n",
       "      <td>2.907</td>\n",
       "      <td>2.914</td>\n",
       "      <td>2.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA</th>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA</th>\n",
       "      <td>148.410</td>\n",
       "      <td>3.180</td>\n",
       "      <td>3.095</td>\n",
       "      <td>3.028</td>\n",
       "      <td>2.977</td>\n",
       "      <td>2.939</td>\n",
       "      <td>2.915</td>\n",
       "      <td>2.903</td>\n",
       "      <td>2.901</td>\n",
       "      <td>2.908</td>\n",
       "      <td>2.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>148.060</td>\n",
       "      <td>3.167</td>\n",
       "      <td>3.083</td>\n",
       "      <td>3.016</td>\n",
       "      <td>2.965</td>\n",
       "      <td>2.928</td>\n",
       "      <td>2.904</td>\n",
       "      <td>2.892</td>\n",
       "      <td>2.890</td>\n",
       "      <td>2.897</td>\n",
       "      <td>2.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA</th>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA</th>\n",
       "      <td>148.663</td>\n",
       "      <td>3.187</td>\n",
       "      <td>3.102</td>\n",
       "      <td>3.036</td>\n",
       "      <td>2.985</td>\n",
       "      <td>2.947</td>\n",
       "      <td>2.923</td>\n",
       "      <td>2.912</td>\n",
       "      <td>2.910</td>\n",
       "      <td>2.917</td>\n",
       "      <td>2.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>148.325</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.091</td>\n",
       "      <td>3.024</td>\n",
       "      <td>2.973</td>\n",
       "      <td>2.936</td>\n",
       "      <td>2.913</td>\n",
       "      <td>2.901</td>\n",
       "      <td>2.899</td>\n",
       "      <td>2.906</td>\n",
       "      <td>2.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         E1    E20    E40    E60    E80  \\\n",
       "STD FV TRAIN REAL LAMBDA             11.169 11.169 11.169 11.169 11.169   \n",
       "STD FV TRAIN PRED LAMBDA            149.125  3.198  3.113  3.046  2.994   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ 148.741  3.183  3.099  3.032  2.981   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ  11.169 11.169 11.169 11.169 11.169   \n",
       "STD FV VALID REAL LAMBDA             11.114 11.114 11.114 11.114 11.114   \n",
       "STD FV VALID PRED LAMBDA            148.410  3.180  3.095  3.028  2.977   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ 148.060  3.167  3.083  3.016  2.965   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ  11.114 11.114 11.114 11.114 11.114   \n",
       "STD FV TEST REAL LAMBDA              11.124 11.124 11.124 11.124 11.124   \n",
       "STD FV TEST PRED LAMBDA             148.663  3.187  3.102  3.036  2.985   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ  148.325  3.175  3.091  3.024  2.973   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ   11.124 11.124 11.124 11.124 11.124   \n",
       "\n",
       "                                      E100   E120   E140   E160   E180   E200  \n",
       "STD FV TRAIN REAL LAMBDA            11.169 11.169 11.169 11.169 11.169 11.169  \n",
       "STD FV TRAIN PRED LAMBDA             2.957  2.933  2.921  2.920  2.927  2.942  \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  2.944  2.920  2.909  2.907  2.914  2.929  \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.169 11.169 11.169 11.169 11.169 11.169  \n",
       "STD FV VALID REAL LAMBDA            11.114 11.114 11.114 11.114 11.114 11.114  \n",
       "STD FV VALID PRED LAMBDA             2.939  2.915  2.903  2.901  2.908  2.923  \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  2.928  2.904  2.892  2.890  2.897  2.912  \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.114 11.114 11.114 11.114 11.114 11.114  \n",
       "STD FV TEST REAL LAMBDA             11.124 11.124 11.124 11.124 11.124 11.124  \n",
       "STD FV TEST PRED LAMBDA              2.947  2.923  2.912  2.910  2.917  2.933  \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   2.936  2.913  2.901  2.899  2.906  2.922  \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.124 11.124 11.124 11.124 11.124 11.124  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T19:51:18.050046Z",
     "start_time": "2020-12-02T19:51:18.036551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E20</th>\n",
       "      <th>E40</th>\n",
       "      <th>E60</th>\n",
       "      <th>E80</th>\n",
       "      <th>E100</th>\n",
       "      <th>E120</th>\n",
       "      <th>E140</th>\n",
       "      <th>E160</th>\n",
       "      <th>E180</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA</th>\n",
       "      <td>167.366</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>167.366</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA</th>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA</th>\n",
       "      <td>167.302</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>167.317</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA</th>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA</th>\n",
       "      <td>167.185</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>167.164</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          E1    E20    E40    E60    E80  \\\n",
       "MEAN FV TRAIN REAL LAMBDA             -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TRAIN PRED LAMBDA            167.366  0.039  0.029  0.020  0.012   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ 167.366  0.039  0.029  0.020  0.012   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ  -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV VALID REAL LAMBDA             -0.083 -0.083 -0.083 -0.083 -0.083   \n",
       "MEAN FV VALID PRED LAMBDA            167.302  0.035  0.025  0.016  0.008   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ 167.317  0.034  0.025  0.016  0.008   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ  -0.083 -0.083 -0.083 -0.083 -0.083   \n",
       "MEAN FV TEST REAL LAMBDA              -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "MEAN FV TEST PRED LAMBDA             167.185  0.035  0.026  0.017  0.009   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  167.164  0.036  0.026  0.018  0.009   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ   -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "\n",
       "                                       E100   E120   E140   E160   E180   E200  \n",
       "MEAN FV TRAIN REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "MEAN FV TRAIN PRED LAMBDA             0.005 -0.003 -0.009 -0.014 -0.020 -0.026  \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  0.005 -0.003 -0.009 -0.014 -0.020 -0.026  \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "MEAN FV VALID REAL LAMBDA            -0.083 -0.083 -0.083 -0.083 -0.083 -0.083  \n",
       "MEAN FV VALID PRED LAMBDA             0.001 -0.007 -0.013 -0.018 -0.024 -0.030  \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  0.001 -0.007 -0.013 -0.018 -0.025 -0.030  \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.083 -0.083 -0.083 -0.083 -0.083 -0.083  \n",
       "MEAN FV TEST REAL LAMBDA             -0.080 -0.080 -0.080 -0.080 -0.080 -0.080  \n",
       "MEAN FV TEST PRED LAMBDA              0.002 -0.006 -0.012 -0.017 -0.024 -0.029  \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   0.002 -0.005 -0.011 -0.016 -0.023 -0.028  \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.080 -0.080 -0.080 -0.080 -0.080 -0.080  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Net Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T20:33:49.053955Z",
     "start_time": "2020-12-02T19:51:18.052139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b6f545c39f44929835de0725350d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7899e0f45d644153b499b9aa168f72eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list] \n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list] \n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list] \n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "\n",
    "    y_train_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][0])))\n",
    "    y_train_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][1])))\n",
    "    y_train_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][2])))\n",
    "    X_train_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][3].shape)]][0])\n",
    "    y_valid_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][4])))\n",
    "    y_valid_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][5])))\n",
    "    y_valid_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][6])))\n",
    "    X_valid_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][7].shape)]][0])\n",
    "    y_test_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][8])))\n",
    "    y_test_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][9])))\n",
    "    y_test_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][10])))\n",
    "    X_test_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][11].shape)]][0])\n",
    "\n",
    "    for index, (y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate([clf[2] for clf in clf_list]):\n",
    "        y_train_real_lambda_list[index] = y_train_real_lambda.ravel()\n",
    "        y_train_pred_lambda_list[index] = y_train_pred_lambda.ravel()\n",
    "        y_train_pred_lambda_poly_lstsq_list[index] = y_train_pred_lambda_poly_lstsq.ravel()\n",
    "        X_train_lambda_list[index] = X_train_lambda#.ravel()\n",
    "\n",
    "        y_valid_real_lambda_list[index] = y_valid_real_lambda.ravel()\n",
    "        y_valid_pred_lambda_list[index] = y_valid_pred_lambda.ravel()\n",
    "        y_valid_pred_lambda_poly_lstsq_list[index] = y_valid_pred_lambda_poly_lstsq.ravel()\n",
    "        X_valid_lambda_list[index] = X_valid_lambda#.ravel()\n",
    "\n",
    "        y_test_real_lambda_list[index] = y_test_real_lambda.ravel()\n",
    "        y_test_pred_lambda_list[index] = y_test_pred_lambda.ravel()\n",
    "        y_test_pred_lambda_poly_lstsq_list[index] = y_test_pred_lambda_poly_lstsq.ravel()\n",
    "        X_test_lambda_list[index] = X_test_lambda#.ravel()\n",
    "    \n",
    "    #add x_data before each pred\n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda.reshape(len(y_train_real_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_list, y_train_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda.reshape(len(y_valid_real_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_list, y_valid_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda.reshape(len(y_test_real_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_list, y_test_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda.reshape(len(y_train_pred_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_list, y_train_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda.reshape(len(y_valid_pred_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_list, y_valid_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda.reshape(len(y_test_pred_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_list, y_test_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq.reshape(len(y_train_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_list, y_train_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq.reshape(len(y_valid_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_list, y_valid_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq.reshape(len(y_test_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_list, y_test_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())    \n",
    "    \n",
    "    y_train_real_lambda_df = pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)\n",
    "       \n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "    \n",
    "    y_train_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][0]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]), 1)) for i in epochs_save_range]\n",
    "    X_train_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][3].shape)]][0]) for i in epochs_save_range]\n",
    "    y_valid_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][4]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]), 1)) for i in epochs_save_range]\n",
    "    X_valid_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][7].shape)]][0]) for i in epochs_save_range]\n",
    "    y_test_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][8]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]), 1)) for i in epochs_save_range]\n",
    "    X_test_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][11].shape)]][0]) for i in epochs_save_range]\n",
    "    \n",
    "    for i, y_data_list_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate(y_data_list_per_epoch):\n",
    "            y_train_real_lambda_list[index][i] = y_train_real_lambda#.ravel()\n",
    "            y_train_pred_lambda_list[index][i] = y_train_pred_lambda#.ravel()\n",
    "            y_train_pred_lambda_poly_lstsq_list[index][i] = y_train_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_train_lambda_list[index][i] = X_train_lambda#.ravel()\n",
    "            \n",
    "            y_valid_real_lambda_list[index][i] = y_valid_real_lambda#.ravel()\n",
    "            y_valid_pred_lambda_list[index][i] = y_valid_pred_lambda#.ravel()\n",
    "            y_valid_pred_lambda_poly_lstsq_list[index][i] = y_valid_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_valid_lambda_list[index][i] = X_valid_lambda#.ravel()\n",
    "            \n",
    "            y_test_real_lambda_list[index][i] = y_test_real_lambda#.ravel()\n",
    "            y_test_pred_lambda_list[index][i] = y_test_pred_lambda#.ravel()\n",
    "            y_test_pred_lambda_poly_lstsq_list[index][i] = y_test_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_test_lambda_list[index][i] = X_test_lambda#.ravel()\n",
    "    \n",
    "    for i, (y_train_real_lambda_by_epoch, y_train_pred_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch, X_train_lambda_by_epoch, y_valid_real_lambda_by_epoch, y_valid_pred_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch, X_valid_lambda_by_epoch, y_test_real_lambda_by_epoch, y_test_pred_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch, X_test_lambda_by_epoch) in tqdm(enumerate(zip(y_train_real_lambda_list, y_train_pred_lambda_list, y_train_pred_lambda_poly_lstsq_list, X_train_lambda_list, y_valid_real_lambda_list, y_valid_pred_lambda_list, y_valid_pred_lambda_poly_lstsq_list, X_valid_lambda_list, y_test_real_lambda_list, y_test_pred_lambda_list, y_test_pred_lambda_poly_lstsq_list, X_test_lambda_list)), total=len(y_train_pred_lambda_list)):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "        \n",
    "        y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_by_epoch, y_train_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_by_epoch, y_valid_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_by_epoch, y_test_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_by_epoch, y_train_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_by_epoch, y_test_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())    \n",
    "\n",
    "        y_train_real_lambda_df = pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)\n",
    "        y_valid_real_lambda_df = pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)\n",
    "        y_test_real_lambda_df = pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)\n",
    "        y_train_pred_lambda_df = pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)\n",
    "        y_valid_pred_lambda_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)\n",
    "        y_test_pred_lambda_df = pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)\n",
    "        y_train_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)\n",
    "        y_valid_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)\n",
    "        y_test_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)\n",
    "\n",
    "        path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "        y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "        y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)         \n",
    "        y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "        y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "        y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)    \n",
    "        y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "        y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "        y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T20:33:49.079415Z",
     "start_time": "2020-12-02T20:33:49.056172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.530</td>\n",
       "      <td>19.032</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>23.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1.935</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-13.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-34.270</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-7.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>9.774</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>7.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>1.342</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>3.687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  0.380  0.960 -0.820  0.530  19.032  0.480  0.970 -1.000  0.350  23.972  \n",
       "1 -0.890 -0.980 -0.440  0.730   1.935 -0.040  0.630 -0.100  0.170 -13.036  \n",
       "2 -0.930 -0.730 -0.910 -0.980 -34.270  0.330 -0.210  0.120 -0.100  -7.948  \n",
       "3  0.220  0.680  0.710 -0.460   9.774  0.910 -0.130  0.050 -0.910   7.523  \n",
       "4  0.280  0.340 -0.520 -0.690   1.342 -0.280 -0.520  0.070 -0.370   3.687  \n",
       "\n",
       "[5 rows x 1285 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T20:33:49.101606Z",
     "start_time": "2020-12-02T20:33:49.081037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-4.514</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-3.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-6.925</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-9.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-7.581</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-7.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>6.559</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>3.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>2.118</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>2.170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  0.380  0.960 -0.820  0.530  -4.514  0.480  0.970 -1.000  0.350  -3.482  \n",
       "1 -0.890 -0.980 -0.440  0.730  -6.925 -0.040  0.630 -0.100  0.170  -9.351  \n",
       "2 -0.930 -0.730 -0.910 -0.980  -7.581  0.330 -0.210  0.120 -0.100  -7.542  \n",
       "3  0.220  0.680  0.710 -0.460   6.559  0.910 -0.130  0.050 -0.910   3.651  \n",
       "4  0.280  0.340 -0.520 -0.690   2.118 -0.280 -0.520  0.070 -0.370   2.170  \n",
       "\n",
       "[5 rows x 1285 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T20:33:49.121106Z",
     "start_time": "2020-12-02T20:33:49.103093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-4.327</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-3.258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-6.754</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-9.262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-7.538</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-7.541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>6.489</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>3.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>1.839</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>2.175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  0.380  0.960 -0.820  0.530  -4.327  0.480  0.970 -1.000  0.350  -3.258  \n",
       "1 -0.890 -0.980 -0.440  0.730  -6.754 -0.040  0.630 -0.100  0.170  -9.262  \n",
       "2 -0.930 -0.730 -0.910 -0.980  -7.538  0.330 -0.210  0.120 -0.100  -7.541  \n",
       "3  0.220  0.680  0.710 -0.460   6.489  0.910 -0.130  0.050 -0.910   3.558  \n",
       "4  0.280  0.340 -0.520 -0.690   1.839 -0.280 -0.520  0.070 -0.370   2.175  \n",
       "\n",
       "[5 rows x 1285 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_poly_lstsq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T20:33:52.178587Z",
     "start_time": "2020-12-02T20:33:49.122814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c23c1abf8d0471daaa24b60091c1b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T20:34:05.848596Z",
     "start_time": "2020-12-02T20:33:52.180477Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T20:34:06.485929Z",
     "start_time": "2020-12-02T20:34:05.852537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>185.104</td>\n",
       "      <td>159.228</td>\n",
       "      <td>136.759</td>\n",
       "      <td>117.086</td>\n",
       "      <td>99.705</td>\n",
       "      <td>84.203</td>\n",
       "      <td>70.224</td>\n",
       "      <td>57.480</td>\n",
       "      <td>45.742</td>\n",
       "      <td>34.856</td>\n",
       "      <td>...</td>\n",
       "      <td>8.076</td>\n",
       "      <td>8.073</td>\n",
       "      <td>8.071</td>\n",
       "      <td>8.068</td>\n",
       "      <td>8.065</td>\n",
       "      <td>8.062</td>\n",
       "      <td>8.059</td>\n",
       "      <td>8.056</td>\n",
       "      <td>8.054</td>\n",
       "      <td>8.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.640</td>\n",
       "      <td>7.397</td>\n",
       "      <td>6.447</td>\n",
       "      <td>5.762</td>\n",
       "      <td>5.316</td>\n",
       "      <td>5.084</td>\n",
       "      <td>5.038</td>\n",
       "      <td>5.144</td>\n",
       "      <td>5.362</td>\n",
       "      <td>5.616</td>\n",
       "      <td>...</td>\n",
       "      <td>1.610</td>\n",
       "      <td>1.609</td>\n",
       "      <td>1.609</td>\n",
       "      <td>1.608</td>\n",
       "      <td>1.607</td>\n",
       "      <td>1.607</td>\n",
       "      <td>1.606</td>\n",
       "      <td>1.605</td>\n",
       "      <td>1.605</td>\n",
       "      <td>1.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>155.201</td>\n",
       "      <td>133.924</td>\n",
       "      <td>114.971</td>\n",
       "      <td>98.536</td>\n",
       "      <td>84.014</td>\n",
       "      <td>69.244</td>\n",
       "      <td>54.123</td>\n",
       "      <td>38.647</td>\n",
       "      <td>23.955</td>\n",
       "      <td>11.662</td>\n",
       "      <td>...</td>\n",
       "      <td>4.120</td>\n",
       "      <td>4.118</td>\n",
       "      <td>4.124</td>\n",
       "      <td>4.131</td>\n",
       "      <td>4.120</td>\n",
       "      <td>4.126</td>\n",
       "      <td>4.125</td>\n",
       "      <td>4.143</td>\n",
       "      <td>4.133</td>\n",
       "      <td>4.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>179.036</td>\n",
       "      <td>153.923</td>\n",
       "      <td>132.073</td>\n",
       "      <td>112.849</td>\n",
       "      <td>95.802</td>\n",
       "      <td>80.465</td>\n",
       "      <td>66.592</td>\n",
       "      <td>53.858</td>\n",
       "      <td>42.006</td>\n",
       "      <td>31.021</td>\n",
       "      <td>...</td>\n",
       "      <td>6.872</td>\n",
       "      <td>6.865</td>\n",
       "      <td>6.867</td>\n",
       "      <td>6.866</td>\n",
       "      <td>6.860</td>\n",
       "      <td>6.862</td>\n",
       "      <td>6.859</td>\n",
       "      <td>6.856</td>\n",
       "      <td>6.855</td>\n",
       "      <td>6.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>184.956</td>\n",
       "      <td>159.037</td>\n",
       "      <td>136.509</td>\n",
       "      <td>116.827</td>\n",
       "      <td>99.440</td>\n",
       "      <td>83.930</td>\n",
       "      <td>69.985</td>\n",
       "      <td>57.284</td>\n",
       "      <td>45.613</td>\n",
       "      <td>34.800</td>\n",
       "      <td>...</td>\n",
       "      <td>7.899</td>\n",
       "      <td>7.895</td>\n",
       "      <td>7.892</td>\n",
       "      <td>7.888</td>\n",
       "      <td>7.885</td>\n",
       "      <td>7.883</td>\n",
       "      <td>7.878</td>\n",
       "      <td>7.874</td>\n",
       "      <td>7.873</td>\n",
       "      <td>7.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>191.007</td>\n",
       "      <td>164.367</td>\n",
       "      <td>141.293</td>\n",
       "      <td>121.183</td>\n",
       "      <td>103.492</td>\n",
       "      <td>87.804</td>\n",
       "      <td>73.719</td>\n",
       "      <td>61.009</td>\n",
       "      <td>49.411</td>\n",
       "      <td>38.636</td>\n",
       "      <td>...</td>\n",
       "      <td>9.090</td>\n",
       "      <td>9.088</td>\n",
       "      <td>9.087</td>\n",
       "      <td>9.082</td>\n",
       "      <td>9.080</td>\n",
       "      <td>9.076</td>\n",
       "      <td>9.073</td>\n",
       "      <td>9.074</td>\n",
       "      <td>9.065</td>\n",
       "      <td>9.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>217.662</td>\n",
       "      <td>187.617</td>\n",
       "      <td>161.386</td>\n",
       "      <td>138.396</td>\n",
       "      <td>118.305</td>\n",
       "      <td>101.591</td>\n",
       "      <td>87.314</td>\n",
       "      <td>74.653</td>\n",
       "      <td>63.113</td>\n",
       "      <td>54.393</td>\n",
       "      <td>...</td>\n",
       "      <td>15.671</td>\n",
       "      <td>15.658</td>\n",
       "      <td>15.654</td>\n",
       "      <td>15.659</td>\n",
       "      <td>15.658</td>\n",
       "      <td>15.650</td>\n",
       "      <td>15.644</td>\n",
       "      <td>15.649</td>\n",
       "      <td>15.662</td>\n",
       "      <td>15.649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean        185.104       159.228       136.759       117.086        99.705   \n",
       "std           8.640         7.397         6.447         5.762         5.316   \n",
       "min         155.201       133.924       114.971        98.536        84.014   \n",
       "25%         179.036       153.923       132.073       112.849        95.802   \n",
       "50%         184.956       159.037       136.509       116.827        99.440   \n",
       "75%         191.007       164.367       141.293       121.183       103.492   \n",
       "max         217.662       187.617       161.386       138.396       118.305   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000      10000.000   \n",
       "mean         84.203        70.224        57.480        45.742         34.856   \n",
       "std           5.084         5.038         5.144         5.362          5.616   \n",
       "min          69.244        54.123        38.647        23.955         11.662   \n",
       "25%          80.465        66.592        53.858        42.006         31.021   \n",
       "50%          83.930        69.985        57.284        45.613         34.800   \n",
       "75%          87.804        73.719        61.009        49.411         38.636   \n",
       "max         101.591        87.314        74.653        63.113         54.393   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...       10000.000       10000.000       10000.000       10000.000   \n",
       "mean   ...           8.076           8.073           8.071           8.068   \n",
       "std    ...           1.610           1.609           1.609           1.608   \n",
       "min    ...           4.120           4.118           4.124           4.131   \n",
       "25%    ...           6.872           6.865           6.867           6.866   \n",
       "50%    ...           7.899           7.895           7.892           7.888   \n",
       "75%    ...           9.090           9.088           9.087           9.082   \n",
       "max    ...          15.671          15.658          15.654          15.659   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            8.065           8.062           8.059           8.056   \n",
       "std             1.607           1.607           1.606           1.605   \n",
       "min             4.120           4.126           4.125           4.143   \n",
       "25%             6.860           6.862           6.859           6.856   \n",
       "50%             7.885           7.883           7.878           7.874   \n",
       "75%             9.080           9.076           9.073           9.074   \n",
       "max            15.658          15.650          15.644          15.649   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count       10000.000       10000.000  \n",
       "mean            8.054           8.051  \n",
       "std             1.605           1.604  \n",
       "min             4.133           4.123  \n",
       "25%             6.855           6.850  \n",
       "50%             7.873           7.869  \n",
       "75%             9.065           9.065  \n",
       "max            15.662          15.649  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T20:34:07.096343Z",
     "start_time": "2020-12-02T20:34:06.487548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>169.752</td>\n",
       "      <td>145.980</td>\n",
       "      <td>125.235</td>\n",
       "      <td>106.972</td>\n",
       "      <td>90.750</td>\n",
       "      <td>76.186</td>\n",
       "      <td>62.971</td>\n",
       "      <td>50.852</td>\n",
       "      <td>39.632</td>\n",
       "      <td>29.226</td>\n",
       "      <td>...</td>\n",
       "      <td>8.086</td>\n",
       "      <td>8.083</td>\n",
       "      <td>8.080</td>\n",
       "      <td>8.078</td>\n",
       "      <td>8.075</td>\n",
       "      <td>8.072</td>\n",
       "      <td>8.070</td>\n",
       "      <td>8.066</td>\n",
       "      <td>8.064</td>\n",
       "      <td>8.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.126</td>\n",
       "      <td>10.984</td>\n",
       "      <td>10.005</td>\n",
       "      <td>9.179</td>\n",
       "      <td>8.485</td>\n",
       "      <td>7.919</td>\n",
       "      <td>7.476</td>\n",
       "      <td>7.146</td>\n",
       "      <td>6.904</td>\n",
       "      <td>6.655</td>\n",
       "      <td>...</td>\n",
       "      <td>1.663</td>\n",
       "      <td>1.663</td>\n",
       "      <td>1.662</td>\n",
       "      <td>1.661</td>\n",
       "      <td>1.661</td>\n",
       "      <td>1.660</td>\n",
       "      <td>1.659</td>\n",
       "      <td>1.659</td>\n",
       "      <td>1.658</td>\n",
       "      <td>1.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>117.755</td>\n",
       "      <td>98.262</td>\n",
       "      <td>81.782</td>\n",
       "      <td>67.586</td>\n",
       "      <td>55.198</td>\n",
       "      <td>44.266</td>\n",
       "      <td>34.435</td>\n",
       "      <td>24.962</td>\n",
       "      <td>14.072</td>\n",
       "      <td>7.641</td>\n",
       "      <td>...</td>\n",
       "      <td>4.096</td>\n",
       "      <td>4.102</td>\n",
       "      <td>4.116</td>\n",
       "      <td>4.098</td>\n",
       "      <td>4.100</td>\n",
       "      <td>4.098</td>\n",
       "      <td>4.110</td>\n",
       "      <td>4.096</td>\n",
       "      <td>4.096</td>\n",
       "      <td>4.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>161.471</td>\n",
       "      <td>138.521</td>\n",
       "      <td>118.499</td>\n",
       "      <td>100.735</td>\n",
       "      <td>84.906</td>\n",
       "      <td>70.729</td>\n",
       "      <td>57.829</td>\n",
       "      <td>45.965</td>\n",
       "      <td>34.872</td>\n",
       "      <td>24.615</td>\n",
       "      <td>...</td>\n",
       "      <td>6.847</td>\n",
       "      <td>6.844</td>\n",
       "      <td>6.846</td>\n",
       "      <td>6.843</td>\n",
       "      <td>6.838</td>\n",
       "      <td>6.837</td>\n",
       "      <td>6.835</td>\n",
       "      <td>6.835</td>\n",
       "      <td>6.832</td>\n",
       "      <td>6.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>169.797</td>\n",
       "      <td>145.987</td>\n",
       "      <td>125.131</td>\n",
       "      <td>106.889</td>\n",
       "      <td>90.601</td>\n",
       "      <td>76.061</td>\n",
       "      <td>62.829</td>\n",
       "      <td>50.666</td>\n",
       "      <td>39.470</td>\n",
       "      <td>29.046</td>\n",
       "      <td>...</td>\n",
       "      <td>7.908</td>\n",
       "      <td>7.905</td>\n",
       "      <td>7.901</td>\n",
       "      <td>7.897</td>\n",
       "      <td>7.897</td>\n",
       "      <td>7.890</td>\n",
       "      <td>7.889</td>\n",
       "      <td>7.889</td>\n",
       "      <td>7.888</td>\n",
       "      <td>7.879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>178.042</td>\n",
       "      <td>153.376</td>\n",
       "      <td>131.940</td>\n",
       "      <td>113.021</td>\n",
       "      <td>96.358</td>\n",
       "      <td>81.443</td>\n",
       "      <td>67.856</td>\n",
       "      <td>55.531</td>\n",
       "      <td>44.134</td>\n",
       "      <td>33.624</td>\n",
       "      <td>...</td>\n",
       "      <td>9.142</td>\n",
       "      <td>9.141</td>\n",
       "      <td>9.133</td>\n",
       "      <td>9.127</td>\n",
       "      <td>9.124</td>\n",
       "      <td>9.122</td>\n",
       "      <td>9.113</td>\n",
       "      <td>9.107</td>\n",
       "      <td>9.104</td>\n",
       "      <td>9.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>217.000</td>\n",
       "      <td>189.344</td>\n",
       "      <td>165.112</td>\n",
       "      <td>143.673</td>\n",
       "      <td>124.530</td>\n",
       "      <td>107.228</td>\n",
       "      <td>91.558</td>\n",
       "      <td>77.766</td>\n",
       "      <td>65.628</td>\n",
       "      <td>54.371</td>\n",
       "      <td>...</td>\n",
       "      <td>15.356</td>\n",
       "      <td>15.390</td>\n",
       "      <td>15.383</td>\n",
       "      <td>15.371</td>\n",
       "      <td>15.346</td>\n",
       "      <td>15.325</td>\n",
       "      <td>15.339</td>\n",
       "      <td>15.337</td>\n",
       "      <td>15.334</td>\n",
       "      <td>15.338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean            169.752           145.980           125.235           106.972   \n",
       "std              12.126            10.984            10.005             9.179   \n",
       "min             117.755            98.262            81.782            67.586   \n",
       "25%             161.471           138.521           118.499           100.735   \n",
       "50%             169.797           145.987           125.131           106.889   \n",
       "75%             178.042           153.376           131.940           113.021   \n",
       "max             217.000           189.344           165.112           143.673   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             90.750            76.186            62.971            50.852   \n",
       "std               8.485             7.919             7.476             7.146   \n",
       "min              55.198            44.266            34.435            24.962   \n",
       "25%              84.906            70.729            57.829            45.965   \n",
       "50%              90.601            76.061            62.829            50.666   \n",
       "75%              96.358            81.443            67.856            55.531   \n",
       "max             124.530           107.228            91.558            77.766   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count         10000.000          10000.000  ...           10000.000   \n",
       "mean             39.632             29.226  ...               8.086   \n",
       "std               6.904              6.655  ...               1.663   \n",
       "min              14.072              7.641  ...               4.096   \n",
       "25%              34.872             24.615  ...               6.847   \n",
       "50%              39.470             29.046  ...               7.908   \n",
       "75%              44.134             33.624  ...               9.142   \n",
       "max              65.628             54.371  ...              15.356   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                8.083               8.080               8.078   \n",
       "std                 1.663               1.662               1.661   \n",
       "min                 4.102               4.116               4.098   \n",
       "25%                 6.844               6.846               6.843   \n",
       "50%                 7.905               7.901               7.897   \n",
       "75%                 9.141               9.133               9.127   \n",
       "max                15.390              15.383              15.371   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                8.075               8.072               8.070   \n",
       "std                 1.661               1.660               1.659   \n",
       "min                 4.100               4.098               4.110   \n",
       "25%                 6.838               6.837               6.835   \n",
       "50%                 7.897               7.890               7.889   \n",
       "75%                 9.124               9.122               9.113   \n",
       "max                15.346              15.325              15.339   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count           10000.000           10000.000           10000.000  \n",
       "mean                8.066               8.064               8.061  \n",
       "std                 1.659               1.658               1.658  \n",
       "min                 4.096               4.096               4.096  \n",
       "25%                 6.835               6.832               6.827  \n",
       "50%                 7.889               7.888               7.879  \n",
       "75%                 9.107               9.104               9.099  \n",
       "max                15.337              15.334              15.338  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T20:34:07.713893Z",
     "start_time": "2020-12-02T20:34:07.097957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>300.264</td>\n",
       "      <td>256.116</td>\n",
       "      <td>215.641</td>\n",
       "      <td>186.617</td>\n",
       "      <td>151.996</td>\n",
       "      <td>130.116</td>\n",
       "      <td>109.793</td>\n",
       "      <td>81.767</td>\n",
       "      <td>65.466</td>\n",
       "      <td>48.703</td>\n",
       "      <td>...</td>\n",
       "      <td>7.075</td>\n",
       "      <td>7.252</td>\n",
       "      <td>6.994</td>\n",
       "      <td>6.941</td>\n",
       "      <td>7.222</td>\n",
       "      <td>6.865</td>\n",
       "      <td>6.996</td>\n",
       "      <td>7.171</td>\n",
       "      <td>7.090</td>\n",
       "      <td>7.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9409.196</td>\n",
       "      <td>8042.590</td>\n",
       "      <td>6659.830</td>\n",
       "      <td>5951.020</td>\n",
       "      <td>4588.385</td>\n",
       "      <td>3984.053</td>\n",
       "      <td>3617.879</td>\n",
       "      <td>2338.798</td>\n",
       "      <td>1957.473</td>\n",
       "      <td>1453.791</td>\n",
       "      <td>...</td>\n",
       "      <td>284.768</td>\n",
       "      <td>293.622</td>\n",
       "      <td>266.288</td>\n",
       "      <td>267.421</td>\n",
       "      <td>289.424</td>\n",
       "      <td>268.467</td>\n",
       "      <td>268.688</td>\n",
       "      <td>292.710</td>\n",
       "      <td>284.078</td>\n",
       "      <td>275.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.896</td>\n",
       "      <td>8.384</td>\n",
       "      <td>7.005</td>\n",
       "      <td>5.871</td>\n",
       "      <td>4.889</td>\n",
       "      <td>3.971</td>\n",
       "      <td>3.211</td>\n",
       "      <td>2.506</td>\n",
       "      <td>1.906</td>\n",
       "      <td>1.340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>62.158</td>\n",
       "      <td>53.151</td>\n",
       "      <td>45.106</td>\n",
       "      <td>38.216</td>\n",
       "      <td>32.279</td>\n",
       "      <td>26.953</td>\n",
       "      <td>22.220</td>\n",
       "      <td>18.019</td>\n",
       "      <td>14.047</td>\n",
       "      <td>10.347</td>\n",
       "      <td>...</td>\n",
       "      <td>1.584</td>\n",
       "      <td>1.595</td>\n",
       "      <td>1.592</td>\n",
       "      <td>1.593</td>\n",
       "      <td>1.591</td>\n",
       "      <td>1.593</td>\n",
       "      <td>1.601</td>\n",
       "      <td>1.593</td>\n",
       "      <td>1.599</td>\n",
       "      <td>1.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>90.657</td>\n",
       "      <td>77.423</td>\n",
       "      <td>65.857</td>\n",
       "      <td>56.004</td>\n",
       "      <td>47.110</td>\n",
       "      <td>39.524</td>\n",
       "      <td>32.589</td>\n",
       "      <td>26.240</td>\n",
       "      <td>20.559</td>\n",
       "      <td>15.102</td>\n",
       "      <td>...</td>\n",
       "      <td>2.012</td>\n",
       "      <td>2.005</td>\n",
       "      <td>2.016</td>\n",
       "      <td>2.012</td>\n",
       "      <td>2.011</td>\n",
       "      <td>2.020</td>\n",
       "      <td>2.018</td>\n",
       "      <td>2.023</td>\n",
       "      <td>2.023</td>\n",
       "      <td>2.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>135.842</td>\n",
       "      <td>116.271</td>\n",
       "      <td>99.030</td>\n",
       "      <td>84.308</td>\n",
       "      <td>71.448</td>\n",
       "      <td>59.691</td>\n",
       "      <td>49.332</td>\n",
       "      <td>39.842</td>\n",
       "      <td>31.287</td>\n",
       "      <td>23.048</td>\n",
       "      <td>...</td>\n",
       "      <td>2.774</td>\n",
       "      <td>2.764</td>\n",
       "      <td>2.764</td>\n",
       "      <td>2.770</td>\n",
       "      <td>2.777</td>\n",
       "      <td>2.774</td>\n",
       "      <td>2.778</td>\n",
       "      <td>2.798</td>\n",
       "      <td>2.795</td>\n",
       "      <td>2.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>874901.250</td>\n",
       "      <td>748346.625</td>\n",
       "      <td>614854.438</td>\n",
       "      <td>553756.562</td>\n",
       "      <td>422933.219</td>\n",
       "      <td>361754.156</td>\n",
       "      <td>341096.312</td>\n",
       "      <td>215269.922</td>\n",
       "      <td>177930.859</td>\n",
       "      <td>132071.297</td>\n",
       "      <td>...</td>\n",
       "      <td>27578.158</td>\n",
       "      <td>28238.438</td>\n",
       "      <td>25217.801</td>\n",
       "      <td>25613.885</td>\n",
       "      <td>27754.561</td>\n",
       "      <td>26051.287</td>\n",
       "      <td>25647.109</td>\n",
       "      <td>28354.172</td>\n",
       "      <td>27400.014</td>\n",
       "      <td>26446.242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean          300.264         256.116         215.641         186.617   \n",
       "std          9409.196        8042.590        6659.830        5951.020   \n",
       "min             9.896           8.384           7.005           5.871   \n",
       "25%            62.158          53.151          45.106          38.216   \n",
       "50%            90.657          77.423          65.857          56.004   \n",
       "75%           135.842         116.271          99.030          84.308   \n",
       "max        874901.250      748346.625      614854.438      553756.562   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean          151.996         130.116         109.793          81.767   \n",
       "std          4588.385        3984.053        3617.879        2338.798   \n",
       "min             4.889           3.971           3.211           2.506   \n",
       "25%            32.279          26.953          22.220          18.019   \n",
       "50%            47.110          39.524          32.589          26.240   \n",
       "75%            71.448          59.691          49.332          39.842   \n",
       "max        422933.219      361754.156      341096.312      215269.922   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count       10000.000        10000.000  ...         10000.000   \n",
       "mean           65.466           48.703  ...             7.075   \n",
       "std          1957.473         1453.791  ...           284.768   \n",
       "min             1.906            1.340  ...             0.425   \n",
       "25%            14.047           10.347  ...             1.584   \n",
       "50%            20.559           15.102  ...             2.012   \n",
       "75%            31.287           23.048  ...             2.774   \n",
       "max        177930.859       132071.297  ...         27578.158   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              7.252             6.994             6.941             7.222   \n",
       "std             293.622           266.288           267.421           289.424   \n",
       "min               0.425             0.423             0.423             0.421   \n",
       "25%               1.595             1.592             1.593             1.591   \n",
       "50%               2.005             2.016             2.012             2.011   \n",
       "75%               2.764             2.764             2.770             2.777   \n",
       "max           28238.438         25217.801         25613.885         27754.561   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              6.865             6.996             7.171             7.090   \n",
       "std             268.467           268.688           292.710           284.078   \n",
       "min               0.426             0.417             0.420             0.423   \n",
       "25%               1.593             1.601             1.593             1.599   \n",
       "50%               2.020             2.018             2.023             2.023   \n",
       "75%               2.774             2.778             2.798             2.795   \n",
       "max           26051.287         25647.109         28354.172         27400.014   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count         10000.000  \n",
       "mean              7.072  \n",
       "std             275.878  \n",
       "min               0.418  \n",
       "25%               1.596  \n",
       "50%               2.023  \n",
       "75%               2.788  \n",
       "max           26446.242  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T20:34:08.330217Z",
     "start_time": "2020-12-02T20:34:07.715523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>142.977</td>\n",
       "      <td>120.491</td>\n",
       "      <td>101.430</td>\n",
       "      <td>85.059</td>\n",
       "      <td>71.402</td>\n",
       "      <td>59.331</td>\n",
       "      <td>48.441</td>\n",
       "      <td>38.503</td>\n",
       "      <td>29.324</td>\n",
       "      <td>20.751</td>\n",
       "      <td>...</td>\n",
       "      <td>3.862</td>\n",
       "      <td>3.868</td>\n",
       "      <td>3.866</td>\n",
       "      <td>3.892</td>\n",
       "      <td>3.885</td>\n",
       "      <td>3.895</td>\n",
       "      <td>3.891</td>\n",
       "      <td>3.885</td>\n",
       "      <td>3.903</td>\n",
       "      <td>3.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>883.017</td>\n",
       "      <td>675.883</td>\n",
       "      <td>530.604</td>\n",
       "      <td>430.566</td>\n",
       "      <td>357.851</td>\n",
       "      <td>295.819</td>\n",
       "      <td>241.225</td>\n",
       "      <td>192.288</td>\n",
       "      <td>147.558</td>\n",
       "      <td>105.709</td>\n",
       "      <td>...</td>\n",
       "      <td>52.749</td>\n",
       "      <td>52.971</td>\n",
       "      <td>53.058</td>\n",
       "      <td>53.328</td>\n",
       "      <td>53.454</td>\n",
       "      <td>53.652</td>\n",
       "      <td>53.808</td>\n",
       "      <td>53.933</td>\n",
       "      <td>54.182</td>\n",
       "      <td>54.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.722</td>\n",
       "      <td>7.217</td>\n",
       "      <td>5.963</td>\n",
       "      <td>4.943</td>\n",
       "      <td>4.069</td>\n",
       "      <td>3.310</td>\n",
       "      <td>2.637</td>\n",
       "      <td>1.964</td>\n",
       "      <td>1.381</td>\n",
       "      <td>0.829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46.356</td>\n",
       "      <td>39.358</td>\n",
       "      <td>33.579</td>\n",
       "      <td>28.395</td>\n",
       "      <td>23.865</td>\n",
       "      <td>19.795</td>\n",
       "      <td>16.148</td>\n",
       "      <td>12.819</td>\n",
       "      <td>9.716</td>\n",
       "      <td>6.802</td>\n",
       "      <td>...</td>\n",
       "      <td>1.356</td>\n",
       "      <td>1.357</td>\n",
       "      <td>1.356</td>\n",
       "      <td>1.357</td>\n",
       "      <td>1.361</td>\n",
       "      <td>1.361</td>\n",
       "      <td>1.361</td>\n",
       "      <td>1.363</td>\n",
       "      <td>1.365</td>\n",
       "      <td>1.361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>69.451</td>\n",
       "      <td>59.362</td>\n",
       "      <td>50.595</td>\n",
       "      <td>42.842</td>\n",
       "      <td>36.035</td>\n",
       "      <td>29.932</td>\n",
       "      <td>24.413</td>\n",
       "      <td>19.406</td>\n",
       "      <td>14.788</td>\n",
       "      <td>10.390</td>\n",
       "      <td>...</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1.751</td>\n",
       "      <td>1.755</td>\n",
       "      <td>1.754</td>\n",
       "      <td>1.754</td>\n",
       "      <td>1.757</td>\n",
       "      <td>1.763</td>\n",
       "      <td>1.759</td>\n",
       "      <td>1.761</td>\n",
       "      <td>1.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>110.160</td>\n",
       "      <td>94.009</td>\n",
       "      <td>80.013</td>\n",
       "      <td>67.795</td>\n",
       "      <td>57.219</td>\n",
       "      <td>47.611</td>\n",
       "      <td>38.908</td>\n",
       "      <td>30.923</td>\n",
       "      <td>23.534</td>\n",
       "      <td>16.690</td>\n",
       "      <td>...</td>\n",
       "      <td>2.494</td>\n",
       "      <td>2.505</td>\n",
       "      <td>2.514</td>\n",
       "      <td>2.507</td>\n",
       "      <td>2.511</td>\n",
       "      <td>2.509</td>\n",
       "      <td>2.516</td>\n",
       "      <td>2.524</td>\n",
       "      <td>2.515</td>\n",
       "      <td>2.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48084.703</td>\n",
       "      <td>29461.697</td>\n",
       "      <td>24071.650</td>\n",
       "      <td>20604.045</td>\n",
       "      <td>17491.592</td>\n",
       "      <td>14689.195</td>\n",
       "      <td>12141.362</td>\n",
       "      <td>9806.278</td>\n",
       "      <td>7642.206</td>\n",
       "      <td>5574.090</td>\n",
       "      <td>...</td>\n",
       "      <td>4625.464</td>\n",
       "      <td>4642.744</td>\n",
       "      <td>4660.353</td>\n",
       "      <td>4671.705</td>\n",
       "      <td>4691.786</td>\n",
       "      <td>4704.755</td>\n",
       "      <td>4727.914</td>\n",
       "      <td>4738.685</td>\n",
       "      <td>4755.886</td>\n",
       "      <td>4766.676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean              142.977             120.491             101.430   \n",
       "std               883.017             675.883             530.604   \n",
       "min                 8.722               7.217               5.963   \n",
       "25%                46.356              39.358              33.579   \n",
       "50%                69.451              59.362              50.595   \n",
       "75%               110.160              94.009              80.013   \n",
       "max             48084.703           29461.697           24071.650   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               85.059              71.402              59.331   \n",
       "std               430.566             357.851             295.819   \n",
       "min                 4.943               4.069               3.310   \n",
       "25%                28.395              23.865              19.795   \n",
       "50%                42.842              36.035              29.932   \n",
       "75%                67.795              57.219              47.611   \n",
       "max             20604.045           17491.592           14689.195   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               48.441              38.503              29.324   \n",
       "std               241.225             192.288             147.558   \n",
       "min                 2.637               1.964               1.381   \n",
       "25%                16.148              12.819               9.716   \n",
       "50%                24.413              19.406              14.788   \n",
       "75%                38.908              30.923              23.534   \n",
       "max             12141.362            9806.278            7642.206   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count            10000.000  ...             10000.000             10000.000   \n",
       "mean                20.751  ...                 3.862                 3.868   \n",
       "std                105.709  ...                52.749                52.971   \n",
       "min                  0.829  ...                 0.416                 0.424   \n",
       "25%                  6.802  ...                 1.356                 1.357   \n",
       "50%                 10.390  ...                 1.750                 1.751   \n",
       "75%                 16.690  ...                 2.494                 2.505   \n",
       "max               5574.090  ...              4625.464              4642.744   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  3.866                 3.892                 3.885   \n",
       "std                  53.058                53.328                53.454   \n",
       "min                   0.394                 0.401                 0.400   \n",
       "25%                   1.356                 1.357                 1.361   \n",
       "50%                   1.755                 1.754                 1.754   \n",
       "75%                   2.514                 2.507                 2.511   \n",
       "max                4660.353              4671.705              4691.786   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  3.895                 3.891                 3.885   \n",
       "std                  53.652                53.808                53.933   \n",
       "min                   0.398                 0.420                 0.394   \n",
       "25%                   1.361                 1.361                 1.363   \n",
       "50%                   1.757                 1.763                 1.759   \n",
       "75%                   2.509                 2.516                 2.524   \n",
       "max                4704.755              4727.914              4738.685   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count             10000.000             10000.000  \n",
       "mean                  3.903                 3.894  \n",
       "std                  54.182                54.187  \n",
       "min                   0.403                 0.397  \n",
       "25%                   1.365                 1.361  \n",
       "50%                   1.761                 1.760  \n",
       "75%                   2.515                 2.528  \n",
       "max                4755.886              4766.676  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T21:14:43.377492Z",
     "start_time": "2020-12-02T21:14:40.426369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 10#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T21:14:49.831956Z",
     "start_time": "2020-12-02T21:14:47.904761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 10#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
