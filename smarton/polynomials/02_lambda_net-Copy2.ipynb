{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:36:56.386021Z",
     "start_time": "2020-12-03T07:36:56.379296Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:36:56.422138Z",
     "start_time": "2020-12-03T07:36:56.388523Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 10000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 10  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "fixed_seed_lambda_training = False\n",
    "initialize_network_zero = False\n",
    "initialize_network_one = False\n",
    "initialize_network_fixed_random = True\n",
    "\n",
    "\n",
    "n_jobs = -5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:36:56.469261Z",
     "start_time": "2020-12-03T07:36:56.424503Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if each_epochs_save != None:\n",
    "    epochs_save_range = range(1, epochs//each_epochs_save+1) if each_epochs_save == 1 else range(epochs//each_epochs_save+1)\n",
    "else:\n",
    "    epochs_save_range = None\n",
    "    \n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_SeedMethod'\n",
    "elif not fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_NoSeedMethod'\n",
    "    \n",
    "if initialize_network_zero:\n",
    "    seed_shuffle_string = seed_shuffle_string + '_zero_initialize'\n",
    "\n",
    "if initialize_network_one:\n",
    "    seed_shuffle_string = seed_shuffle_string + '_ones_initialize'   \n",
    "\n",
    "if initialize_network_fixed_random:\n",
    "    seed_shuffle_string = seed_shuffle_string + '_fixedRandom_initialize'   \n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:01.761724Z",
     "start_time": "2020-12-03T07:36:56.471682Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:01.772913Z",
     "start_time": "2020-12-03T07:37:01.765469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:01.793221Z",
     "start_time": "2020-12-03T07:37:01.775426Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "    \n",
    "    \n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:01.801250Z",
     "start_time": "2020-12-03T07:37:01.795282Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcualate_function_value_with_X_data_entry(coefficient_list, X_data_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "     \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [X_data_value**int(coefficient_multiplier) for coefficient_multiplier, X_data_value in zip(coefficient_multipliers, X_data_entry)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "        \n",
    "    return result, np.append(X_data_entry, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:01.838640Z",
     "start_time": "2020-12-03T07:37:01.803787Z"
    },
    "code_folding": [
     0,
     20,
     43,
     66,
     88,
     91,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:01.912003Z",
     "start_time": "2020-12-03T07:37:01.840694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9deb41891c984da5aaf255d5af6643ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997918c7b14f4b9991f145912839cc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:12.089165Z",
     "start_time": "2020-12-03T07:37:01.914270Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:12.365411Z",
     "start_time": "2020-12-03T07:37:12.092965Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        \n",
    "        for i in epochs_save_range:\n",
    "            index = i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:12.390605Z",
     "start_time": "2020-12-03T07:37:12.370663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.480</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.830</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.420  0.820 -0.680 -0.640\n",
       "1 -0.980 -0.820  0.060 -0.300\n",
       "2 -0.480  0.000 -0.370  0.600\n",
       "3 -0.250  0.630 -0.480 -0.960\n",
       "4 -0.830  0.230 -0.170 -0.460"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:12.401555Z",
     "start_time": "2020-12-03T07:37:12.392787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.610</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.800</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.950</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0  0.610  0.220 -0.980  0.170\n",
       "1 -0.230 -0.050 -0.900  0.600\n",
       "2  0.800 -0.540  0.550  0.440\n",
       "3  0.030 -0.600 -0.320 -0.430\n",
       "4  0.950  0.770 -0.250  0.530"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:12.409617Z",
     "start_time": "2020-12-03T07:37:12.403858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:12.417851Z",
     "start_time": "2020-12-03T07:37:12.411941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:12.459888Z",
     "start_time": "2020-12-03T07:37:12.420218Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lambda_net(identifier, \n",
    "                        X_data_real_lambda, \n",
    "                        y_data_real_lambda, \n",
    "                        y_data_pred_lambda, \n",
    "                        y_data_pred_lambda_poly_lstsq, \n",
    "                        y_data_real_lambda_poly_lstsq):\n",
    "    \n",
    "    mae_real_VS_predLambda = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    mae_predLambda_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_realPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    rmse_real_VS_predLambda = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    rmse_predLambda_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_realPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    mape_real_VS_predLambda = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    mape_predLambda_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_realPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)            \n",
    "\n",
    "    r2_real_VS_predLambda = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_predPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    r2_predLambda_VS_predPolyLstsq = np.round(r2_score(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_realPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    raae_real_VS_predLambda = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    raae_predLambda_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_realPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    rmae_real_VS_predLambda = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    rmae_predLambda_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_realPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    fd_real_VS_predLambda = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_predLambda_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_realPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "    dtw_real_VS_predLambda, dtw_complete_real_VS_predLambda = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predLambda = np.round(dtw_real_VS_predLambda, 4)\n",
    "    dtw_real_VS_predPolyLstsq, dtw_complete_real_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predPolyLstsq = np.round(dtw_real_VS_predPolyLstsq, 4)\n",
    "    dtw_predLambda_VS_predPolyLstsq, dtw_complete_predLambda_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_predLambda_VS_predPolyLstsq = np.round(dtw_predLambda_VS_predPolyLstsq, 4)    \n",
    "    dtw_real_VS_realPolyLstsq, dtw_complete_real_VS_realPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_realPolyLstsq = np.round(dtw_real_VS_realPolyLstsq, 4) \n",
    "        \n",
    "    std_data_real_lambda = np.round(np.std(y_data_real_lambda), 4) \n",
    "    std_data_pred_lambda = np.round(np.std(y_data_pred_lambda), 4) \n",
    "    std_data_pred_lambda_poly_lstsq = np.round(np.std(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    std_data_real_lambda_poly_lstsq = np.round(np.std(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    mean_data_real_lambda = np.round(np.mean(y_data_real_lambda), 4) \n",
    "    mean_data_pred_lambda = np.round(np.mean(y_data_pred_lambda), 4) \n",
    "    mean_data_pred_lambda_poly_lstsq = np.round(np.mean(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    mean_data_real_lambda_poly_lstsq = np.round(np.mean(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    return [{\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mae_real_VS_predLambda,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_real_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_predLambda_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mae_real_VS_realPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmse_real_VS_predLambda,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_real_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_predLambda_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmse_real_VS_realPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mape_real_VS_predLambda,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_real_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_predLambda_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mape_real_VS_realPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': r2_real_VS_predLambda,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_real_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_predLambda_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': r2_real_VS_realPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': raae_real_VS_predLambda,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_real_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_predLambda_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': raae_real_VS_realPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmae_real_VS_predLambda,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_real_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_predLambda_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmae_real_VS_realPolyLstsq,\n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': fd_real_VS_predLambda,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_real_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_predLambda_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': fd_real_VS_realPolyLstsq,   \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': dtw_real_VS_predLambda, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_real_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_predLambda_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': dtw_real_VS_realPolyLstsq, \n",
    "            },\n",
    "            {\n",
    "             'STD FV ' + identifier + ' REAL LAMBDA': std_data_real_lambda,\n",
    "             'STD FV ' + identifier + ' PRED LAMBDA': std_data_pred_lambda, \n",
    "             'STD FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': std_data_pred_lambda_poly_lstsq, \n",
    "             'STD FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': std_data_real_lambda_poly_lstsq, \n",
    "            },\n",
    "            {\n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA': mean_data_real_lambda,\n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA': mean_data_pred_lambda, \n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': mean_data_pred_lambda_poly_lstsq, \n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': mean_data_real_lambda_poly_lstsq, \n",
    "            }]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:12.466568Z",
     "start_time": "2020-12-03T07:37:12.461923Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_function_values_from_polynomial(X_data, polynomial):\n",
    "    function_value_list = []\n",
    "    for entry in X_data:\n",
    "        function_value, _ = calcualate_function_value_with_X_data_entry(polynomial, entry)\n",
    "        function_value_list.append(function_value)\n",
    "    function_value_array = np.array(function_value_list).reshape(len(function_value_list), 1)     \n",
    "\n",
    "    return function_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:12.475573Z",
     "start_time": "2020-12-03T07:37:12.468727Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_term_matric_for_lstsq(X_data, polynomial_indices):\n",
    "    term_list_all = []\n",
    "    y = 0\n",
    "    for term in list(polynomial_indices):\n",
    "        term_list = [int(value_mult) for value_mult in term]\n",
    "        term_list_all.append(term_list)\n",
    "    terms_matrix = []\n",
    "    for unknowns in X_data:\n",
    "        terms = []\n",
    "        for term_multipliers in term_list_all:\n",
    "            term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "            terms.append(term_value)\n",
    "        terms_matrix.append(np.array(terms))\n",
    "    terms_matrix = np.array(terms_matrix)\n",
    "    \n",
    "    return terms_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T07:37:12.543419Z",
     "start_time": "2020-12-03T07:37:12.477816Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn(X_data_lambda, y_data_real_lambda, polynomial, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    if fixed_seed_lambda_training:\n",
    "        random.seed(RANDOM_SEED)\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        if int(tf.__version__[0]) >= 2:\n",
    "            tf.random.set_seed(RANDOM_SEED)\n",
    "        else:\n",
    "            tf.set_random_seed(RANDOM_SEED) \n",
    "        \n",
    "    if isinstance(X_data_lambda, pd.DataFrame):\n",
    "        X_data_lambda = X_data_lambda.values\n",
    "    if isinstance(y_data_real_lambda, pd.DataFrame):\n",
    "        y_data_real_lambda = y_data_real_lambda.values\n",
    "                \n",
    "    X_train_lambda_with_valid, X_test_lambda, y_train_real_lambda_with_valid, y_test_real_lambda = train_test_split(X_data_lambda, y_data_real_lambda, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    X_train_lambda, X_valid_lambda, y_train_real_lambda, y_valid_real_lambda = train_test_split(X_train_lambda_with_valid, y_train_real_lambda_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "     \n",
    "        \n",
    "    model = Sequential()\n",
    "\n",
    "    if initialize_network_one:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer='ones', bias_initializer='ones')) #1024\n",
    "    elif initialize_network_zero:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer='zeros', bias_initializer='zeros')) #1024\n",
    "    elif initialize_network_fixed_random:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED), bias_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED))) #1024\n",
    "    else:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1])) #1024\n",
    "        \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        if initialize_network_one:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer='ones', bias_initializer='ones'))\n",
    "        elif initialize_network_zero:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer='zeros', bias_initializer='zeros'))\n",
    "        elif initialize_network_fixed_random:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED), bias_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED)))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "    \n",
    "    if initialize_network_one:\n",
    "        model.add(Dense(1, kernel_initializer='ones', bias_initializer='ones'))\n",
    "    if initialize_network_zero:\n",
    "        model.add(Dense(1, kernel_initializer='zeros', bias_initializer='zeros'))\n",
    "    if initialize_network_fixed_random:\n",
    "        model.add(Dense(1, kernel_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED), bias_initializer=tf.keras.initializers.RandomUniform(seed=RANDOM_SEED)))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_lstsq_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_train_pred_lambda = model.predict(X_train_lambda) \n",
    "        y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "        y_test_pred_lambda = model.predict(X_test_lambda)\n",
    "    \n",
    "        terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                \n",
    "        polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "        y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "        y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "        y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)    \n",
    "        y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)\n",
    "        y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)  \n",
    "        \n",
    "        pred_list = (y_train_real_lambda, \n",
    "                     y_train_pred_lambda, \n",
    "                     y_train_pred_lambda_poly_lstsq,\n",
    "                     #y_train_real_lambda_poly_lstsq,\n",
    "                     X_train_lambda, \n",
    "                     y_valid_real_lambda,\n",
    "                     y_valid_pred_lambda, \n",
    "                     y_valid_pred_lambda_poly_lstsq,\n",
    "                     #y_valid_real_lambda_poly_lstsq,\n",
    "                     X_valid_lambda, \n",
    "                     y_test_real_lambda, \n",
    "                     y_test_pred_lambda, \n",
    "                     y_test_pred_lambda_poly_lstsq, \n",
    "                     #y_test_real_lambda_poly_lstsq,\n",
    "                     X_test_lambda)\n",
    "\n",
    "        scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "        scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "        scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "        scores_std = {}\n",
    "        for aDict in (std_train, std_valid, std_test):\n",
    "            scores_std.update(aDict)      \n",
    "        scores_mean = {}\n",
    "        for aDict in (mean_train, mean_valid, mean_test):\n",
    "            scores_mean.update(aDict)\n",
    "        \n",
    "        scores_list =  [scores_train,\n",
    "                             scores_valid,\n",
    "                             scores_test,\n",
    "                             scores_std,\n",
    "                             scores_mean]            \n",
    "                            \n",
    "    else:\n",
    "        scores_list = []\n",
    "        pred_list = []\n",
    "        for i in epochs_save_range:\n",
    "            train_epochs_step = each_epochs_save if i > 1 else max(each_epochs_save-1, 1) if i==1 else 1\n",
    "            \n",
    "            model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=train_epochs_step, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=1,\n",
    "                      use_multiprocessing=False)\n",
    "            \n",
    "            #history adjustment for continuing training\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                history = model_history.history\n",
    "            else:\n",
    "                history = mergeDict(history, model_history.history)\n",
    "                #for key_1 in history.keys():\n",
    "                #    for key_2 in model_history.history.keys():\n",
    "                #        if key_1 == key_2:\n",
    "                #            history[key_1] += model_history.history[key_2]  \n",
    "\n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_train_pred_lambda = model.predict(X_train_lambda)                \n",
    "            y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "            y_test_pred_lambda = model.predict(X_test_lambda)        \n",
    "\n",
    "            terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                        \n",
    "            polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            \n",
    "            y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "            y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "            y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)           \n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "                y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)  \n",
    "                y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)                    \n",
    "                \n",
    "            pred_list.append((y_train_real_lambda, \n",
    "                              y_train_pred_lambda, \n",
    "                              y_train_pred_lambda_poly_lstsq,\n",
    "                              #y_train_real_lambda_poly_lstsq,\n",
    "                              X_train_lambda, \n",
    "                              y_valid_real_lambda,\n",
    "                              y_valid_pred_lambda, \n",
    "                              y_valid_pred_lambda_poly_lstsq,\n",
    "                              #y_valid_real_lambda_poly_lstsq,\n",
    "                              X_valid_lambda, \n",
    "                              y_test_real_lambda, \n",
    "                              y_test_pred_lambda, \n",
    "                              y_test_pred_lambda_poly_lstsq, \n",
    "                              #y_test_real_lambda_poly_lstsq,\n",
    "                              X_test_lambda))\n",
    "    \n",
    "            scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "            scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "            scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "            scores_std = {}\n",
    "            for aDict in (std_train, std_valid, std_test):\n",
    "                scores_std.update(aDict)\n",
    "            scores_mean = {}\n",
    "            for aDict in (mean_train, mean_valid, mean_test):\n",
    "                scores_mean.update(aDict)\n",
    "\n",
    "            scores_list_single_epoch =  [scores_train,\n",
    "                                              scores_valid,\n",
    "                                              scores_test,\n",
    "                                              scores_std,\n",
    "                                              scores_mean]        \n",
    "                  \n",
    "            scores_list.append(scores_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_lstsq_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_lstsq_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save == None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                for i, value in enumerate(polynomial.values):\n",
    "                    if i == 0:\n",
    "                        text_file.write(str(value))  \n",
    "                    else:\n",
    "                        text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_lstsq_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, #polynomial_lstsq_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:26:20.154899Z",
     "start_time": "2020-12-03T07:37:12.545434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    5.018, max:    9.438, cur:    5.018)\n",
      "\tvalidation       \t (min:    4.997, max:    9.846, cur:    4.997)\n",
      "mean_absolute_percentage_error_keras\n",
      "\ttraining         \t (min:    1.002, max:    2.463, cur:    1.315)\n",
      "\tvalidation       \t (min:    0.807, max:    1.095, cur:    1.007)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    6.908, max:   12.554, cur:    6.908)\n",
      "\tvalidation       \t (min:    6.448, max:   12.600, cur:    6.448)\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(X_data[1].values, y_data[1].values, X_data[0], return_history=True, each_epochs_save=each_epochs_save, printing=True) for X_data, y_data in zip(X_data_list_split, y_data_list_split))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:26:31.107735Z",
     "start_time": "2020-12-03T12:26:20.156848Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    scores_list_train = [clf[1][0] for clf in clf_list]\n",
    "    scores_list_valid = [clf[1][1] for clf in clf_list]\n",
    "    scores_list_test = [clf[1][2] for clf in clf_list]\n",
    "    scores_list_stds = [clf[1][3] for clf in clf_list]\n",
    "    scores_list_means = [clf[1][4] for clf in clf_list]\n",
    "\n",
    "    scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_list_by_epochs = [[] for i in epochs_save_range]\n",
    "    for scores_list in scores_list:   \n",
    "        for index, scores in enumerate(scores_list):\n",
    "            scores_list_by_epochs[index].append(scores)\n",
    "            \n",
    "        \n",
    "    for i, scores_list_single_epoch in enumerate(scores_list_by_epochs):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "           \n",
    "        scores_list_train = [scores_list[0] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_valid = [scores_list[1] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_test = [scores_list[2] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_stds = [scores_list[3] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_means = [scores_list[4] for scores_list in scores_list_single_epoch]\n",
    "        \n",
    "        scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()  \n",
    "        scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()  \n",
    "        scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:26:31.131368Z",
     "start_time": "2020-12-03T12:26:31.109579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E1</th>\n",
       "      <th>TRAIN POLY E1</th>\n",
       "      <th>TRAIN POLY PRED E1</th>\n",
       "      <th>TRAIN LSTSQ E1</th>\n",
       "      <th>TRAIN PRED E10</th>\n",
       "      <th>TRAIN POLY E10</th>\n",
       "      <th>TRAIN POLY PRED E10</th>\n",
       "      <th>TRAIN LSTSQ E10</th>\n",
       "      <th>TRAIN PRED E20</th>\n",
       "      <th>TRAIN POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E180</th>\n",
       "      <th>TRAIN LSTSQ E180</th>\n",
       "      <th>TRAIN PRED E190</th>\n",
       "      <th>TRAIN POLY E190</th>\n",
       "      <th>TRAIN POLY PRED E190</th>\n",
       "      <th>TRAIN LSTSQ E190</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.217</td>\n",
       "      <td>10.217</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.996</td>\n",
       "      <td>9.997</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.718</td>\n",
       "      <td>9.718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.225</td>\n",
       "      <td>5.222</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.166</td>\n",
       "      <td>5.165</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>13.016</td>\n",
       "      <td>13.016</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.798</td>\n",
       "      <td>12.798</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.522</td>\n",
       "      <td>12.522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.248</td>\n",
       "      <td>7.235</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.170</td>\n",
       "      <td>7.156</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.049</td>\n",
       "      <td>1.049</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.447</td>\n",
       "      <td>1.447</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.944</td>\n",
       "      <td>1.945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.463</td>\n",
       "      <td>4.249</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.415</td>\n",
       "      <td>4.332</td>\n",
       "      <td>3.960</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.924</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.472</td>\n",
       "      <td>3.472</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.468</td>\n",
       "      <td>3.468</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.453</td>\n",
       "      <td>3.453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.645</td>\n",
       "      <td>2.643</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.627</td>\n",
       "      <td>2.621</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>25.196</td>\n",
       "      <td>25.196</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.890</td>\n",
       "      <td>24.890</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.484</td>\n",
       "      <td>24.484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.521</td>\n",
       "      <td>13.468</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.349</td>\n",
       "      <td>13.293</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.364</td>\n",
       "      <td>102.364</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.149</td>\n",
       "      <td>100.149</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.355</td>\n",
       "      <td>97.356</td>\n",
       "      <td>...</td>\n",
       "      <td>2.717</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.306</td>\n",
       "      <td>50.257</td>\n",
       "      <td>2.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.726</td>\n",
       "      <td>49.693</td>\n",
       "      <td>2.906</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E1  TRAIN POLY E1  TRAIN POLY PRED E1  TRAIN LSTSQ E1  \\\n",
       "MAE FV          10.217         10.217               0.001           0.000   \n",
       "RMSE FV         13.016         13.016               0.001           0.000   \n",
       "MAPE FV          1.049          1.049               0.074           0.000   \n",
       "R2 FV           -0.419         -0.419               1.000           1.000   \n",
       "RAAE FV          0.924          0.924               0.014           0.000   \n",
       "RMAE FV          3.472          3.472               0.065           0.000   \n",
       "FD FV           25.196         25.196               0.001           0.000   \n",
       "DTW FV         102.364        102.364               0.005           0.000   \n",
       "\n",
       "         TRAIN PRED E10  TRAIN POLY E10  TRAIN POLY PRED E10  TRAIN LSTSQ E10  \\\n",
       "MAE FV            9.996           9.997                0.001            0.000   \n",
       "RMSE FV          12.798          12.798                0.001            0.000   \n",
       "MAPE FV           1.447           1.447                0.015            0.000   \n",
       "R2 FV            -0.363          -0.363                0.999            1.000   \n",
       "RAAE FV           0.902           0.902                0.018            0.000   \n",
       "RMAE FV           3.468           3.468                0.085            0.000   \n",
       "FD FV            24.890          24.890                0.002            0.000   \n",
       "DTW FV          100.149         100.149                0.007            0.000   \n",
       "\n",
       "         TRAIN PRED E20  TRAIN POLY E20  ...  TRAIN POLY PRED E180  \\\n",
       "MAE FV            9.718           9.718  ...                 0.271   \n",
       "RMSE FV          12.522          12.522  ...                 0.362   \n",
       "MAPE FV           1.944           1.945  ...                 0.521   \n",
       "R2 FV            -0.293          -0.293  ...                 0.997   \n",
       "RAAE FV           0.876           0.876  ...                 0.038   \n",
       "RMAE FV           3.453           3.453  ...                 0.240   \n",
       "FD FV            24.484          24.484  ...                 0.713   \n",
       "DTW FV           97.355          97.356  ...                 2.717   \n",
       "\n",
       "         TRAIN LSTSQ E180  TRAIN PRED E190  TRAIN POLY E190  \\\n",
       "MAE FV              0.000            5.225            5.222   \n",
       "RMSE FV             0.000            7.248            7.235   \n",
       "MAPE FV             0.000            4.463            4.249   \n",
       "R2 FV               1.000            0.540            0.541   \n",
       "RAAE FV             0.000            0.481            0.481   \n",
       "RMAE FV             0.000            2.645            2.643   \n",
       "FD FV               0.000           13.521           13.468   \n",
       "DTW FV              0.000           50.306           50.257   \n",
       "\n",
       "         TRAIN POLY PRED E190  TRAIN LSTSQ E190  TRAIN PRED E200  \\\n",
       "MAE FV                  0.281             0.000            5.166   \n",
       "RMSE FV                 0.372             0.000            7.170   \n",
       "MAPE FV                 0.883             0.000            4.415   \n",
       "R2 FV                   0.997             1.000            0.549   \n",
       "RAAE FV                 0.039             0.000            0.476   \n",
       "RMAE FV                 0.239             0.000            2.627   \n",
       "FD FV                   0.729             0.000           13.349   \n",
       "DTW FV                  2.810             0.000           49.726   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             5.165                 0.290             0.000  \n",
       "RMSE FV            7.156                 0.382             0.000  \n",
       "MAPE FV            4.332                 3.960             0.000  \n",
       "R2 FV              0.550                 0.997             1.000  \n",
       "RAAE FV            0.476                 0.040             0.000  \n",
       "RMAE FV            2.621                 0.238             0.000  \n",
       "FD FV             13.293                 0.746             0.000  \n",
       "DTW FV            49.693                 2.906             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:26:31.153554Z",
     "start_time": "2020-12-03T12:26:31.132819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E1</th>\n",
       "      <th>VALID POLY E1</th>\n",
       "      <th>VALID POLY PRED E1</th>\n",
       "      <th>VALID LSTSQ E1</th>\n",
       "      <th>VALID PRED E10</th>\n",
       "      <th>VALID POLY E10</th>\n",
       "      <th>VALID POLY PRED E10</th>\n",
       "      <th>VALID LSTSQ E10</th>\n",
       "      <th>VALID PRED E20</th>\n",
       "      <th>VALID POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E180</th>\n",
       "      <th>VALID LSTSQ E180</th>\n",
       "      <th>VALID PRED E190</th>\n",
       "      <th>VALID POLY E190</th>\n",
       "      <th>VALID POLY PRED E190</th>\n",
       "      <th>VALID LSTSQ E190</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.201</td>\n",
       "      <td>10.201</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.982</td>\n",
       "      <td>9.982</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.706</td>\n",
       "      <td>9.706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.315</td>\n",
       "      <td>5.303</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.259</td>\n",
       "      <td>5.249</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.976</td>\n",
       "      <td>12.976</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.760</td>\n",
       "      <td>12.760</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.485</td>\n",
       "      <td>12.485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.307</td>\n",
       "      <td>7.290</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.233</td>\n",
       "      <td>7.214</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.034</td>\n",
       "      <td>1.034</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.599</td>\n",
       "      <td>1.598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.900</td>\n",
       "      <td>4.915</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.885</td>\n",
       "      <td>4.892</td>\n",
       "      <td>1.185</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.928</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.031</td>\n",
       "      <td>3.031</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.027</td>\n",
       "      <td>3.027</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.012</td>\n",
       "      <td>3.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.254</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.238</td>\n",
       "      <td>2.231</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.834</td>\n",
       "      <td>24.834</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.529</td>\n",
       "      <td>24.529</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.123</td>\n",
       "      <td>24.123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.249</td>\n",
       "      <td>13.184</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.093</td>\n",
       "      <td>13.026</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>101.668</td>\n",
       "      <td>101.668</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.465</td>\n",
       "      <td>99.465</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>96.693</td>\n",
       "      <td>96.693</td>\n",
       "      <td>...</td>\n",
       "      <td>2.906</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.343</td>\n",
       "      <td>50.243</td>\n",
       "      <td>3.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.807</td>\n",
       "      <td>49.719</td>\n",
       "      <td>3.101</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E1  VALID POLY E1  VALID POLY PRED E1  VALID LSTSQ E1  \\\n",
       "MAE FV          10.201         10.201               0.001           0.000   \n",
       "RMSE FV         12.976         12.976               0.001           0.000   \n",
       "MAPE FV          1.034          1.034               0.091           0.000   \n",
       "R2 FV           -0.427         -0.427               1.000           1.000   \n",
       "RAAE FV          0.928          0.928               0.015           0.000   \n",
       "RMAE FV          3.031          3.031               0.064           0.000   \n",
       "FD FV           24.834         24.834               0.002           0.000   \n",
       "DTW FV         101.668        101.668               0.006           0.000   \n",
       "\n",
       "         VALID PRED E10  VALID POLY E10  VALID POLY PRED E10  VALID LSTSQ E10  \\\n",
       "MAE FV            9.982           9.982                0.001            0.000   \n",
       "RMSE FV          12.760          12.760                0.001            0.000   \n",
       "MAPE FV           1.280           1.280                0.019            0.000   \n",
       "R2 FV            -0.371          -0.371                0.999            1.000   \n",
       "RAAE FV           0.906           0.906                0.020            0.000   \n",
       "RMAE FV           3.027           3.027                0.081            0.000   \n",
       "FD FV            24.529          24.529                0.002            0.000   \n",
       "DTW FV           99.465          99.465                0.008            0.000   \n",
       "\n",
       "         VALID PRED E20  VALID POLY E20  ...  VALID POLY PRED E180  \\\n",
       "MAE FV            9.706           9.706  ...                 0.292   \n",
       "RMSE FV          12.485          12.485  ...                 0.396   \n",
       "MAPE FV           1.599           1.598  ...                 0.649   \n",
       "R2 FV            -0.300          -0.300  ...                 0.996   \n",
       "RAAE FV           0.880           0.880  ...                 0.041   \n",
       "RMAE FV           3.012           3.012  ...                 0.206   \n",
       "FD FV            24.123          24.123  ...                 0.765   \n",
       "DTW FV           96.693          96.693  ...                 2.906   \n",
       "\n",
       "         VALID LSTSQ E180  VALID PRED E190  VALID POLY E190  \\\n",
       "MAE FV              0.000            5.315            5.303   \n",
       "RMSE FV             0.000            7.307            7.290   \n",
       "MAPE FV             0.000            4.900            4.915   \n",
       "R2 FV               1.000            0.528            0.529   \n",
       "RAAE FV             0.000            0.492            0.491   \n",
       "RMAE FV             0.000            2.254            2.250   \n",
       "FD FV               0.000           13.249           13.184   \n",
       "DTW FV              0.000           50.343           50.243   \n",
       "\n",
       "         VALID POLY PRED E190  VALID LSTSQ E190  VALID PRED E200  \\\n",
       "MAE FV                  0.301             0.000            5.259   \n",
       "RMSE FV                 0.406             0.000            7.233   \n",
       "MAPE FV                 0.485             0.000            4.885   \n",
       "R2 FV                   0.996             1.000            0.536   \n",
       "RAAE FV                 0.042             0.000            0.487   \n",
       "RMAE FV                 0.206             0.000            2.238   \n",
       "FD FV                   0.782             0.000           13.093   \n",
       "DTW FV                  3.002             0.000           49.807   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             5.249                 0.311             0.000  \n",
       "RMSE FV            7.214                 0.417             0.000  \n",
       "MAPE FV            4.892                 1.185             0.000  \n",
       "R2 FV              0.538                 0.996             1.000  \n",
       "RAAE FV            0.486                 0.043             0.000  \n",
       "RMAE FV            2.231                 0.206             0.000  \n",
       "FD FV             13.026                 0.801             0.000  \n",
       "DTW FV            49.719                 3.101             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:26:31.175488Z",
     "start_time": "2020-12-03T12:26:31.155120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E1</th>\n",
       "      <th>TEST POLY E1</th>\n",
       "      <th>TEST POLY PRED E1</th>\n",
       "      <th>TEST LSTSQ E1</th>\n",
       "      <th>TEST PRED E10</th>\n",
       "      <th>TEST POLY E10</th>\n",
       "      <th>TEST POLY PRED E10</th>\n",
       "      <th>TEST LSTSQ E10</th>\n",
       "      <th>TEST PRED E20</th>\n",
       "      <th>TEST POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E180</th>\n",
       "      <th>TEST LSTSQ E180</th>\n",
       "      <th>TEST PRED E190</th>\n",
       "      <th>TEST POLY E190</th>\n",
       "      <th>TEST POLY PRED E190</th>\n",
       "      <th>TEST LSTSQ E190</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.213</td>\n",
       "      <td>10.213</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.994</td>\n",
       "      <td>9.994</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.717</td>\n",
       "      <td>9.717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.313</td>\n",
       "      <td>5.302</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.258</td>\n",
       "      <td>5.247</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.991</td>\n",
       "      <td>12.991</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.774</td>\n",
       "      <td>12.774</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.499</td>\n",
       "      <td>12.499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.308</td>\n",
       "      <td>7.290</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.234</td>\n",
       "      <td>7.214</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.080</td>\n",
       "      <td>1.080</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.630</td>\n",
       "      <td>1.630</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.391</td>\n",
       "      <td>2.389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.725</td>\n",
       "      <td>6.855</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.771</td>\n",
       "      <td>6.922</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.928</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.150</td>\n",
       "      <td>3.150</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.146</td>\n",
       "      <td>3.147</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.132</td>\n",
       "      <td>3.132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.363</td>\n",
       "      <td>2.357</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.347</td>\n",
       "      <td>2.338</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>25.014</td>\n",
       "      <td>25.014</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.708</td>\n",
       "      <td>24.708</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.302</td>\n",
       "      <td>24.302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.394</td>\n",
       "      <td>13.330</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.233</td>\n",
       "      <td>13.168</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.284</td>\n",
       "      <td>102.284</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.075</td>\n",
       "      <td>100.075</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.294</td>\n",
       "      <td>97.294</td>\n",
       "      <td>...</td>\n",
       "      <td>2.906</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.728</td>\n",
       "      <td>50.592</td>\n",
       "      <td>3.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.187</td>\n",
       "      <td>50.059</td>\n",
       "      <td>3.100</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E1  TEST POLY E1  TEST POLY PRED E1  TEST LSTSQ E1  \\\n",
       "MAE FV         10.213        10.213              0.001          0.000   \n",
       "RMSE FV        12.991        12.991              0.001          0.000   \n",
       "MAPE FV         1.080         1.080              0.097          0.000   \n",
       "R2 FV          -0.428        -0.428              1.000          1.000   \n",
       "RAAE FV         0.928         0.928              0.015          0.000   \n",
       "RMAE FV         3.150         3.150              0.066          0.000   \n",
       "FD FV          25.014        25.014              0.002          0.000   \n",
       "DTW FV        102.284       102.284              0.006          0.000   \n",
       "\n",
       "         TEST PRED E10  TEST POLY E10  TEST POLY PRED E10  TEST LSTSQ E10  \\\n",
       "MAE FV           9.994          9.994               0.001           0.000   \n",
       "RMSE FV         12.774         12.774               0.001           0.000   \n",
       "MAPE FV          1.630          1.630               0.015           0.000   \n",
       "R2 FV           -0.372         -0.372               0.999           1.000   \n",
       "RAAE FV          0.907          0.907               0.020           0.000   \n",
       "RMAE FV          3.146          3.147               0.085           0.000   \n",
       "FD FV           24.708         24.708               0.002           0.000   \n",
       "DTW FV         100.075        100.075               0.008           0.000   \n",
       "\n",
       "         TEST PRED E20  TEST POLY E20  ...  TEST POLY PRED E180  \\\n",
       "MAE FV           9.717          9.717  ...                0.292   \n",
       "RMSE FV         12.499         12.499  ...                0.396   \n",
       "MAPE FV          2.391          2.389  ...                0.558   \n",
       "R2 FV           -0.301         -0.301  ...                0.996   \n",
       "RAAE FV          0.880          0.880  ...                0.041   \n",
       "RMAE FV          3.132          3.132  ...                0.222   \n",
       "FD FV           24.302         24.302  ...                0.770   \n",
       "DTW FV          97.294         97.294  ...                2.906   \n",
       "\n",
       "         TEST LSTSQ E180  TEST PRED E190  TEST POLY E190  TEST POLY PRED E190  \\\n",
       "MAE FV             0.000           5.313           5.302                0.301   \n",
       "RMSE FV            0.000           7.308           7.290                0.406   \n",
       "MAPE FV            0.000           6.725           6.855                0.504   \n",
       "R2 FV              1.000           0.529           0.531                0.996   \n",
       "RAAE FV            0.000           0.491           0.490                0.042   \n",
       "RMAE FV            0.000           2.363           2.357                0.222   \n",
       "FD FV              0.000          13.394          13.330                0.787   \n",
       "DTW FV             0.000          50.728          50.592                3.002   \n",
       "\n",
       "         TEST LSTSQ E190  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           5.258           5.247                0.311   \n",
       "RMSE FV            0.000           7.234           7.214                0.417   \n",
       "MAPE FV            0.000           6.771           6.922                0.509   \n",
       "R2 FV              1.000           0.538           0.540                0.996   \n",
       "RAAE FV            0.000           0.486           0.485                0.043   \n",
       "RMAE FV            0.000           2.347           2.338                0.222   \n",
       "FD FV              0.000          13.233          13.168                0.806   \n",
       "DTW FV             0.000          50.187          50.059                3.100   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "FD FV              0.000  \n",
       "DTW FV             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:26:31.199587Z",
     "start_time": "2020-12-03T12:26:31.176936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA</th>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>...</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.826</td>\n",
       "      <td>1.378</td>\n",
       "      <td>2.029</td>\n",
       "      <td>2.733</td>\n",
       "      <td>3.430</td>\n",
       "      <td>...</td>\n",
       "      <td>4.719</td>\n",
       "      <td>5.284</td>\n",
       "      <td>5.773</td>\n",
       "      <td>6.176</td>\n",
       "      <td>6.497</td>\n",
       "      <td>6.749</td>\n",
       "      <td>6.948</td>\n",
       "      <td>7.107</td>\n",
       "      <td>7.235</td>\n",
       "      <td>7.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.826</td>\n",
       "      <td>1.377</td>\n",
       "      <td>2.028</td>\n",
       "      <td>2.730</td>\n",
       "      <td>3.427</td>\n",
       "      <td>...</td>\n",
       "      <td>4.713</td>\n",
       "      <td>5.277</td>\n",
       "      <td>5.765</td>\n",
       "      <td>6.167</td>\n",
       "      <td>6.487</td>\n",
       "      <td>6.739</td>\n",
       "      <td>6.938</td>\n",
       "      <td>7.096</td>\n",
       "      <td>7.224</td>\n",
       "      <td>7.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>...</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "      <td>11.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA</th>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>...</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.820</td>\n",
       "      <td>1.367</td>\n",
       "      <td>2.014</td>\n",
       "      <td>2.713</td>\n",
       "      <td>3.406</td>\n",
       "      <td>...</td>\n",
       "      <td>4.686</td>\n",
       "      <td>5.249</td>\n",
       "      <td>5.735</td>\n",
       "      <td>6.136</td>\n",
       "      <td>6.456</td>\n",
       "      <td>6.707</td>\n",
       "      <td>6.905</td>\n",
       "      <td>7.063</td>\n",
       "      <td>7.190</td>\n",
       "      <td>7.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.820</td>\n",
       "      <td>1.367</td>\n",
       "      <td>2.013</td>\n",
       "      <td>2.710</td>\n",
       "      <td>3.402</td>\n",
       "      <td>...</td>\n",
       "      <td>4.681</td>\n",
       "      <td>5.243</td>\n",
       "      <td>5.728</td>\n",
       "      <td>6.128</td>\n",
       "      <td>6.447</td>\n",
       "      <td>6.698</td>\n",
       "      <td>6.895</td>\n",
       "      <td>7.052</td>\n",
       "      <td>7.180</td>\n",
       "      <td>7.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>...</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "      <td>11.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA</th>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>...</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.821</td>\n",
       "      <td>1.368</td>\n",
       "      <td>2.015</td>\n",
       "      <td>2.714</td>\n",
       "      <td>3.407</td>\n",
       "      <td>...</td>\n",
       "      <td>4.689</td>\n",
       "      <td>5.252</td>\n",
       "      <td>5.739</td>\n",
       "      <td>6.141</td>\n",
       "      <td>6.461</td>\n",
       "      <td>6.713</td>\n",
       "      <td>6.911</td>\n",
       "      <td>7.069</td>\n",
       "      <td>7.196</td>\n",
       "      <td>7.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.820</td>\n",
       "      <td>1.368</td>\n",
       "      <td>2.014</td>\n",
       "      <td>2.712</td>\n",
       "      <td>3.404</td>\n",
       "      <td>...</td>\n",
       "      <td>4.684</td>\n",
       "      <td>5.246</td>\n",
       "      <td>5.732</td>\n",
       "      <td>6.133</td>\n",
       "      <td>6.453</td>\n",
       "      <td>6.704</td>\n",
       "      <td>6.902</td>\n",
       "      <td>7.059</td>\n",
       "      <td>7.186</td>\n",
       "      <td>7.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>...</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        E1    E10    E20    E30    E40    E50  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.169 11.169 11.169 11.169 11.169 11.169   \n",
       "STD FV TRAIN PRED LAMBDA             0.038  0.047  0.080  0.175  0.413  0.826   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  0.038  0.047  0.080  0.175  0.413  0.826   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.169 11.169 11.169 11.169 11.169 11.169   \n",
       "STD FV VALID REAL LAMBDA            11.114 11.114 11.114 11.114 11.114 11.114   \n",
       "STD FV VALID PRED LAMBDA             0.038  0.047  0.080  0.174  0.410  0.820   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  0.038  0.047  0.080  0.174  0.410  0.820   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.114 11.114 11.114 11.114 11.114 11.114   \n",
       "STD FV TEST REAL LAMBDA             11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "STD FV TEST PRED LAMBDA              0.038  0.047  0.080  0.174  0.410  0.821   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   0.038  0.047  0.080  0.174  0.410  0.820   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "\n",
       "                                       E60    E70    E80    E90  ...   E110  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.169 11.169 11.169 11.169  ... 11.169   \n",
       "STD FV TRAIN PRED LAMBDA             1.378  2.029  2.733  3.430  ...  4.719   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  1.377  2.028  2.730  3.427  ...  4.713   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.169 11.169 11.169 11.169  ... 11.169   \n",
       "STD FV VALID REAL LAMBDA            11.114 11.114 11.114 11.114  ... 11.114   \n",
       "STD FV VALID PRED LAMBDA             1.367  2.014  2.713  3.406  ...  4.686   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  1.367  2.013  2.710  3.402  ...  4.681   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.114 11.114 11.114 11.114  ... 11.114   \n",
       "STD FV TEST REAL LAMBDA             11.124 11.124 11.124 11.124  ... 11.124   \n",
       "STD FV TEST PRED LAMBDA              1.368  2.015  2.714  3.407  ...  4.689   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   1.368  2.014  2.712  3.404  ...  4.684   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.124 11.124 11.124 11.124  ... 11.124   \n",
       "\n",
       "                                      E120   E130   E140   E150   E160   E170  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.169 11.169 11.169 11.169 11.169 11.169   \n",
       "STD FV TRAIN PRED LAMBDA             5.284  5.773  6.176  6.497  6.749  6.948   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  5.277  5.765  6.167  6.487  6.739  6.938   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.169 11.169 11.169 11.169 11.169 11.169   \n",
       "STD FV VALID REAL LAMBDA            11.114 11.114 11.114 11.114 11.114 11.114   \n",
       "STD FV VALID PRED LAMBDA             5.249  5.735  6.136  6.456  6.707  6.905   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  5.243  5.728  6.128  6.447  6.698  6.895   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.114 11.114 11.114 11.114 11.114 11.114   \n",
       "STD FV TEST REAL LAMBDA             11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "STD FV TEST PRED LAMBDA              5.252  5.739  6.141  6.461  6.713  6.911   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   5.246  5.732  6.133  6.453  6.704  6.902   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "\n",
       "                                      E180   E190   E200  \n",
       "STD FV TRAIN REAL LAMBDA            11.169 11.169 11.169  \n",
       "STD FV TRAIN PRED LAMBDA             7.107  7.235  7.343  \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.096  7.224  7.332  \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.169 11.169 11.169  \n",
       "STD FV VALID REAL LAMBDA            11.114 11.114 11.114  \n",
       "STD FV VALID PRED LAMBDA             7.063  7.190  7.297  \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.052  7.180  7.286  \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.114 11.114 11.114  \n",
       "STD FV TEST REAL LAMBDA             11.124 11.124 11.124  \n",
       "STD FV TEST PRED LAMBDA              7.069  7.196  7.303  \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.059  7.186  7.293  \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.124 11.124 11.124  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:26:31.226508Z",
     "start_time": "2020-12-03T12:26:31.201108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.208</td>\n",
       "      <td>1.191</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.208</td>\n",
       "      <td>1.191</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA</th>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.188</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.909</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.188</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA</th>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.909</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.188</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.909</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.188</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         E1    E10    E20    E30    E40  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TRAIN PRED LAMBDA             0.039  0.058  0.161  0.458  0.912   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  0.039  0.058  0.161  0.458  0.912   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV VALID REAL LAMBDA            -0.083 -0.083 -0.083 -0.083 -0.083   \n",
       "MEAN FV VALID PRED LAMBDA             0.040  0.058  0.161  0.457  0.910   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  0.040  0.058  0.161  0.457  0.909   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.083 -0.083 -0.083 -0.083 -0.083   \n",
       "MEAN FV TEST REAL LAMBDA             -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "MEAN FV TEST PRED LAMBDA              0.039  0.058  0.161  0.457  0.909   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   0.039  0.058  0.161  0.457  0.909   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "\n",
       "                                        E50    E60    E70    E80    E90  ...  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA             1.208  1.191  0.965  0.704  0.484  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  1.208  1.191  0.965  0.704  0.484  ...   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078  ...   \n",
       "MEAN FV VALID REAL LAMBDA            -0.083 -0.083 -0.083 -0.083 -0.083  ...   \n",
       "MEAN FV VALID PRED LAMBDA             1.204  1.188  0.964  0.705  0.485  ...   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  1.204  1.188  0.964  0.705  0.486  ...   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.083 -0.083 -0.083 -0.083 -0.083  ...   \n",
       "MEAN FV TEST REAL LAMBDA             -0.080 -0.080 -0.080 -0.080 -0.080  ...   \n",
       "MEAN FV TEST PRED LAMBDA              1.204  1.188  0.964  0.705  0.486  ...   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   1.204  1.188  0.964  0.705  0.486  ...   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.080 -0.080 -0.080 -0.080 -0.080  ...   \n",
       "\n",
       "                                       E110   E120   E130   E140   E150  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TRAIN PRED LAMBDA             0.194  0.108  0.051  0.014 -0.011   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  0.194  0.108  0.051  0.014 -0.011   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV VALID REAL LAMBDA            -0.083 -0.083 -0.083 -0.083 -0.083   \n",
       "MEAN FV VALID PRED LAMBDA             0.195  0.110  0.053  0.015 -0.010   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  0.196  0.111  0.054  0.016 -0.009   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.083 -0.083 -0.083 -0.083 -0.083   \n",
       "MEAN FV TEST REAL LAMBDA             -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "MEAN FV TEST PRED LAMBDA              0.197  0.112  0.056  0.018 -0.007   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   0.197  0.113  0.056  0.018 -0.006   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "\n",
       "                                       E160   E170   E180   E190   E200  \n",
       "MEAN FV TRAIN REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.029 -0.042 -0.054 -0.063 -0.069  \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.029 -0.042 -0.054 -0.063 -0.069  \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "MEAN FV VALID REAL LAMBDA            -0.083 -0.083 -0.083 -0.083 -0.083  \n",
       "MEAN FV VALID PRED LAMBDA            -0.027 -0.041 -0.053 -0.062 -0.069  \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.027 -0.040 -0.052 -0.061 -0.068  \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.083 -0.083 -0.083 -0.083 -0.083  \n",
       "MEAN FV TEST REAL LAMBDA             -0.080 -0.080 -0.080 -0.080 -0.080  \n",
       "MEAN FV TEST PRED LAMBDA             -0.024 -0.038 -0.049 -0.058 -0.065  \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.024 -0.037 -0.048 -0.057 -0.064  \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.080 -0.080 -0.080 -0.080 -0.080  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Net Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:48:57.883069Z",
     "start_time": "2020-12-03T12:26:31.228312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9b4f63688440a581c92ad7b7e29b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8dab6d166b447096b53cf3f8d0ed0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list] \n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list] \n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list] \n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "\n",
    "    y_train_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][0])))\n",
    "    y_train_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][1])))\n",
    "    y_train_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][2])))\n",
    "    X_train_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][3].shape)]][0])\n",
    "    y_valid_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][4])))\n",
    "    y_valid_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][5])))\n",
    "    y_valid_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][6])))\n",
    "    X_valid_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][7].shape)]][0])\n",
    "    y_test_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][8])))\n",
    "    y_test_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][9])))\n",
    "    y_test_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][10])))\n",
    "    X_test_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][11].shape)]][0])\n",
    "\n",
    "    for index, (y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate([clf[2] for clf in clf_list]):\n",
    "        y_train_real_lambda_list[index] = y_train_real_lambda.ravel()\n",
    "        y_train_pred_lambda_list[index] = y_train_pred_lambda.ravel()\n",
    "        y_train_pred_lambda_poly_lstsq_list[index] = y_train_pred_lambda_poly_lstsq.ravel()\n",
    "        X_train_lambda_list[index] = X_train_lambda#.ravel()\n",
    "\n",
    "        y_valid_real_lambda_list[index] = y_valid_real_lambda.ravel()\n",
    "        y_valid_pred_lambda_list[index] = y_valid_pred_lambda.ravel()\n",
    "        y_valid_pred_lambda_poly_lstsq_list[index] = y_valid_pred_lambda_poly_lstsq.ravel()\n",
    "        X_valid_lambda_list[index] = X_valid_lambda#.ravel()\n",
    "\n",
    "        y_test_real_lambda_list[index] = y_test_real_lambda.ravel()\n",
    "        y_test_pred_lambda_list[index] = y_test_pred_lambda.ravel()\n",
    "        y_test_pred_lambda_poly_lstsq_list[index] = y_test_pred_lambda_poly_lstsq.ravel()\n",
    "        X_test_lambda_list[index] = X_test_lambda#.ravel()\n",
    "    \n",
    "    #add x_data before each pred\n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda.reshape(len(y_train_real_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_list, y_train_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda.reshape(len(y_valid_real_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_list, y_valid_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda.reshape(len(y_test_real_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_list, y_test_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda.reshape(len(y_train_pred_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_list, y_train_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda.reshape(len(y_valid_pred_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_list, y_valid_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda.reshape(len(y_test_pred_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_list, y_test_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq.reshape(len(y_train_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_list, y_train_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq.reshape(len(y_valid_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_list, y_valid_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq.reshape(len(y_test_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_list, y_test_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())    \n",
    "    \n",
    "    y_train_real_lambda_df = pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)\n",
    "       \n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "    \n",
    "    y_train_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][0]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]), 1)) for i in epochs_save_range]\n",
    "    X_train_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][3].shape)]][0]) for i in epochs_save_range]\n",
    "    y_valid_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][4]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]), 1)) for i in epochs_save_range]\n",
    "    X_valid_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][7].shape)]][0]) for i in epochs_save_range]\n",
    "    y_test_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][8]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]), 1)) for i in epochs_save_range]\n",
    "    X_test_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][11].shape)]][0]) for i in epochs_save_range]\n",
    "    \n",
    "    for i, y_data_list_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate(y_data_list_per_epoch):\n",
    "            y_train_real_lambda_list[index][i] = y_train_real_lambda#.ravel()\n",
    "            y_train_pred_lambda_list[index][i] = y_train_pred_lambda#.ravel()\n",
    "            y_train_pred_lambda_poly_lstsq_list[index][i] = y_train_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_train_lambda_list[index][i] = X_train_lambda#.ravel()\n",
    "            \n",
    "            y_valid_real_lambda_list[index][i] = y_valid_real_lambda#.ravel()\n",
    "            y_valid_pred_lambda_list[index][i] = y_valid_pred_lambda#.ravel()\n",
    "            y_valid_pred_lambda_poly_lstsq_list[index][i] = y_valid_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_valid_lambda_list[index][i] = X_valid_lambda#.ravel()\n",
    "            \n",
    "            y_test_real_lambda_list[index][i] = y_test_real_lambda#.ravel()\n",
    "            y_test_pred_lambda_list[index][i] = y_test_pred_lambda#.ravel()\n",
    "            y_test_pred_lambda_poly_lstsq_list[index][i] = y_test_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_test_lambda_list[index][i] = X_test_lambda#.ravel()\n",
    "    \n",
    "    for i, (y_train_real_lambda_by_epoch, y_train_pred_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch, X_train_lambda_by_epoch, y_valid_real_lambda_by_epoch, y_valid_pred_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch, X_valid_lambda_by_epoch, y_test_real_lambda_by_epoch, y_test_pred_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch, X_test_lambda_by_epoch) in tqdm(enumerate(zip(y_train_real_lambda_list, y_train_pred_lambda_list, y_train_pred_lambda_poly_lstsq_list, X_train_lambda_list, y_valid_real_lambda_list, y_valid_pred_lambda_list, y_valid_pred_lambda_poly_lstsq_list, X_valid_lambda_list, y_test_real_lambda_list, y_test_pred_lambda_list, y_test_pred_lambda_poly_lstsq_list, X_test_lambda_list)), total=len(y_train_pred_lambda_list)):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "        \n",
    "        y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_by_epoch, y_train_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_by_epoch, y_valid_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_by_epoch, y_test_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_by_epoch, y_train_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_by_epoch, y_test_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "        y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())    \n",
    "\n",
    "        y_train_real_lambda_df = pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)\n",
    "        y_valid_real_lambda_df = pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)\n",
    "        y_test_real_lambda_df = pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)\n",
    "        y_train_pred_lambda_df = pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)\n",
    "        y_valid_pred_lambda_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)\n",
    "        y_test_pred_lambda_df = pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)\n",
    "        y_train_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)\n",
    "        y_valid_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)\n",
    "        y_test_pred_lambda_poly_lstsq_df = pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)\n",
    "\n",
    "        path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "        y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "        y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)         \n",
    "        y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "        y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "        y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)    \n",
    "        y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "        y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "        y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:48:57.908975Z",
     "start_time": "2020-12-03T13:48:57.885402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.530</td>\n",
       "      <td>19.032</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>23.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1.935</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-13.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-34.270</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-7.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>9.774</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>7.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>1.342</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>3.687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  0.380  0.960 -0.820  0.530  19.032  0.480  0.970 -1.000  0.350  23.972  \n",
       "1 -0.890 -0.980 -0.440  0.730   1.935 -0.040  0.630 -0.100  0.170 -13.036  \n",
       "2 -0.930 -0.730 -0.910 -0.980 -34.270  0.330 -0.210  0.120 -0.100  -7.948  \n",
       "3  0.220  0.680  0.710 -0.460   9.774  0.910 -0.130  0.050 -0.910   7.523  \n",
       "4  0.280  0.340 -0.520 -0.690   1.342 -0.280 -0.520  0.070 -0.370   3.687  \n",
       "\n",
       "[5 rows x 1285 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:48:57.983047Z",
     "start_time": "2020-12-03T13:48:57.910580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.530</td>\n",
       "      <td>19.395</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>20.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-5.211</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-14.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-28.928</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-15.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>13.601</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>10.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>7.471</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>4.123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  0.380  0.960 -0.820  0.530  19.395  0.480  0.970 -1.000  0.350  20.751  \n",
       "1 -0.890 -0.980 -0.440  0.730  -5.211 -0.040  0.630 -0.100  0.170 -14.319  \n",
       "2 -0.930 -0.730 -0.910 -0.980 -28.928  0.330 -0.210  0.120 -0.100 -15.292  \n",
       "3  0.220  0.680  0.710 -0.460  13.601  0.910 -0.130  0.050 -0.910  10.207  \n",
       "4  0.280  0.340 -0.520 -0.690   7.471 -0.280 -0.520  0.070 -0.370   4.123  \n",
       "\n",
       "[5 rows x 1285 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:48:58.002832Z",
     "start_time": "2020-12-03T13:48:57.984512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.530</td>\n",
       "      <td>19.636</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>21.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-4.880</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-14.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-28.996</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-15.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>13.906</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>10.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>7.566</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>4.044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  0.380  0.960 -0.820  0.530  19.636  0.480  0.970 -1.000  0.350  21.274  \n",
       "1 -0.890 -0.980 -0.440  0.730  -4.880 -0.040  0.630 -0.100  0.170 -14.366  \n",
       "2 -0.930 -0.730 -0.910 -0.980 -28.996  0.330 -0.210  0.120 -0.100 -15.056  \n",
       "3  0.220  0.680  0.710 -0.460  13.906  0.910 -0.130  0.050 -0.910  10.788  \n",
       "4  0.280  0.340 -0.520 -0.690   7.566 -0.280 -0.520  0.070 -0.370   4.044  \n",
       "\n",
       "[5 rows x 1285 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_poly_lstsq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:49:01.304570Z",
     "start_time": "2020-12-03T13:48:58.004304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0ac3afc7dc4648b843bf68381b7ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:49:15.865982Z",
     "start_time": "2020-12-03T13:49:01.309103Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:49:16.514194Z",
     "start_time": "2020-12-03T13:49:15.868337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.231</td>\n",
       "      <td>10.207</td>\n",
       "      <td>10.182</td>\n",
       "      <td>10.158</td>\n",
       "      <td>10.133</td>\n",
       "      <td>10.109</td>\n",
       "      <td>10.084</td>\n",
       "      <td>10.060</td>\n",
       "      <td>10.035</td>\n",
       "      <td>10.011</td>\n",
       "      <td>...</td>\n",
       "      <td>5.225</td>\n",
       "      <td>5.219</td>\n",
       "      <td>5.213</td>\n",
       "      <td>5.207</td>\n",
       "      <td>5.202</td>\n",
       "      <td>5.196</td>\n",
       "      <td>5.190</td>\n",
       "      <td>5.184</td>\n",
       "      <td>5.178</td>\n",
       "      <td>5.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.348</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.321</td>\n",
       "      <td>2.308</td>\n",
       "      <td>2.294</td>\n",
       "      <td>2.281</td>\n",
       "      <td>2.268</td>\n",
       "      <td>2.255</td>\n",
       "      <td>2.242</td>\n",
       "      <td>2.228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.699</td>\n",
       "      <td>4.699</td>\n",
       "      <td>4.699</td>\n",
       "      <td>4.698</td>\n",
       "      <td>4.698</td>\n",
       "      <td>4.698</td>\n",
       "      <td>4.697</td>\n",
       "      <td>4.697</td>\n",
       "      <td>4.696</td>\n",
       "      <td>4.696</td>\n",
       "      <td>...</td>\n",
       "      <td>3.079</td>\n",
       "      <td>3.073</td>\n",
       "      <td>3.067</td>\n",
       "      <td>3.060</td>\n",
       "      <td>3.055</td>\n",
       "      <td>3.048</td>\n",
       "      <td>3.041</td>\n",
       "      <td>3.034</td>\n",
       "      <td>3.028</td>\n",
       "      <td>3.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.500</td>\n",
       "      <td>8.485</td>\n",
       "      <td>8.470</td>\n",
       "      <td>8.451</td>\n",
       "      <td>8.436</td>\n",
       "      <td>8.423</td>\n",
       "      <td>8.411</td>\n",
       "      <td>8.398</td>\n",
       "      <td>8.389</td>\n",
       "      <td>8.376</td>\n",
       "      <td>...</td>\n",
       "      <td>4.790</td>\n",
       "      <td>4.784</td>\n",
       "      <td>4.777</td>\n",
       "      <td>4.772</td>\n",
       "      <td>4.766</td>\n",
       "      <td>4.761</td>\n",
       "      <td>4.755</td>\n",
       "      <td>4.749</td>\n",
       "      <td>4.743</td>\n",
       "      <td>4.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.969</td>\n",
       "      <td>9.952</td>\n",
       "      <td>9.931</td>\n",
       "      <td>9.913</td>\n",
       "      <td>9.889</td>\n",
       "      <td>9.872</td>\n",
       "      <td>9.850</td>\n",
       "      <td>9.826</td>\n",
       "      <td>9.806</td>\n",
       "      <td>9.781</td>\n",
       "      <td>...</td>\n",
       "      <td>5.214</td>\n",
       "      <td>5.210</td>\n",
       "      <td>5.204</td>\n",
       "      <td>5.198</td>\n",
       "      <td>5.192</td>\n",
       "      <td>5.186</td>\n",
       "      <td>5.179</td>\n",
       "      <td>5.174</td>\n",
       "      <td>5.168</td>\n",
       "      <td>5.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.640</td>\n",
       "      <td>11.612</td>\n",
       "      <td>11.582</td>\n",
       "      <td>11.550</td>\n",
       "      <td>11.518</td>\n",
       "      <td>11.490</td>\n",
       "      <td>11.465</td>\n",
       "      <td>11.437</td>\n",
       "      <td>11.403</td>\n",
       "      <td>11.377</td>\n",
       "      <td>...</td>\n",
       "      <td>5.649</td>\n",
       "      <td>5.642</td>\n",
       "      <td>5.635</td>\n",
       "      <td>5.629</td>\n",
       "      <td>5.624</td>\n",
       "      <td>5.619</td>\n",
       "      <td>5.611</td>\n",
       "      <td>5.605</td>\n",
       "      <td>5.600</td>\n",
       "      <td>5.595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.689</td>\n",
       "      <td>20.597</td>\n",
       "      <td>20.507</td>\n",
       "      <td>20.417</td>\n",
       "      <td>20.329</td>\n",
       "      <td>20.241</td>\n",
       "      <td>20.154</td>\n",
       "      <td>20.067</td>\n",
       "      <td>19.981</td>\n",
       "      <td>19.895</td>\n",
       "      <td>...</td>\n",
       "      <td>8.392</td>\n",
       "      <td>8.388</td>\n",
       "      <td>8.384</td>\n",
       "      <td>8.383</td>\n",
       "      <td>8.382</td>\n",
       "      <td>8.376</td>\n",
       "      <td>8.374</td>\n",
       "      <td>8.374</td>\n",
       "      <td>8.371</td>\n",
       "      <td>8.369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean         10.231        10.207        10.182        10.158        10.133   \n",
       "std           2.348         2.334         2.321         2.308         2.294   \n",
       "min           4.699         4.699         4.699         4.698         4.698   \n",
       "25%           8.500         8.485         8.470         8.451         8.436   \n",
       "50%           9.969         9.952         9.931         9.913         9.889   \n",
       "75%          11.640        11.612        11.582        11.550        11.518   \n",
       "max          20.689        20.597        20.507        20.417        20.329   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000      10000.000   \n",
       "mean         10.109        10.084        10.060        10.035         10.011   \n",
       "std           2.281         2.268         2.255         2.242          2.228   \n",
       "min           4.698         4.697         4.697         4.696          4.696   \n",
       "25%           8.423         8.411         8.398         8.389          8.376   \n",
       "50%           9.872         9.850         9.826         9.806          9.781   \n",
       "75%          11.490        11.465        11.437        11.403         11.377   \n",
       "max          20.241        20.154        20.067        19.981         19.895   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...       10000.000       10000.000       10000.000       10000.000   \n",
       "mean   ...           5.225           5.219           5.213           5.207   \n",
       "std    ...           0.645           0.645           0.644           0.644   \n",
       "min    ...           3.079           3.073           3.067           3.060   \n",
       "25%    ...           4.790           4.784           4.777           4.772   \n",
       "50%    ...           5.214           5.210           5.204           5.198   \n",
       "75%    ...           5.649           5.642           5.635           5.629   \n",
       "max    ...           8.392           8.388           8.384           8.383   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            5.202           5.196           5.190           5.184   \n",
       "std             0.643           0.643           0.643           0.642   \n",
       "min             3.055           3.048           3.041           3.034   \n",
       "25%             4.766           4.761           4.755           4.749   \n",
       "50%             5.192           5.186           5.179           5.174   \n",
       "75%             5.624           5.619           5.611           5.605   \n",
       "max             8.382           8.376           8.374           8.374   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count       10000.000       10000.000  \n",
       "mean            5.178           5.173  \n",
       "std             0.642           0.641  \n",
       "min             3.028           3.022  \n",
       "25%             4.743           4.738  \n",
       "50%             5.168           5.163  \n",
       "75%             5.600           5.595  \n",
       "max             8.371           8.369  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:49:17.133396Z",
     "start_time": "2020-12-03T13:49:16.516020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.201</td>\n",
       "      <td>10.176</td>\n",
       "      <td>10.152</td>\n",
       "      <td>10.128</td>\n",
       "      <td>10.104</td>\n",
       "      <td>10.079</td>\n",
       "      <td>10.055</td>\n",
       "      <td>10.031</td>\n",
       "      <td>10.006</td>\n",
       "      <td>9.982</td>\n",
       "      <td>...</td>\n",
       "      <td>5.309</td>\n",
       "      <td>5.303</td>\n",
       "      <td>5.298</td>\n",
       "      <td>5.292</td>\n",
       "      <td>5.287</td>\n",
       "      <td>5.281</td>\n",
       "      <td>5.276</td>\n",
       "      <td>5.270</td>\n",
       "      <td>5.265</td>\n",
       "      <td>5.259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.382</td>\n",
       "      <td>2.368</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.342</td>\n",
       "      <td>2.329</td>\n",
       "      <td>2.316</td>\n",
       "      <td>2.303</td>\n",
       "      <td>2.290</td>\n",
       "      <td>2.277</td>\n",
       "      <td>2.264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.597</td>\n",
       "      <td>4.595</td>\n",
       "      <td>4.594</td>\n",
       "      <td>4.592</td>\n",
       "      <td>4.590</td>\n",
       "      <td>4.589</td>\n",
       "      <td>4.587</td>\n",
       "      <td>4.586</td>\n",
       "      <td>4.584</td>\n",
       "      <td>4.582</td>\n",
       "      <td>...</td>\n",
       "      <td>2.789</td>\n",
       "      <td>2.787</td>\n",
       "      <td>2.785</td>\n",
       "      <td>2.786</td>\n",
       "      <td>2.783</td>\n",
       "      <td>2.783</td>\n",
       "      <td>2.780</td>\n",
       "      <td>2.777</td>\n",
       "      <td>2.780</td>\n",
       "      <td>2.777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.463</td>\n",
       "      <td>8.449</td>\n",
       "      <td>8.433</td>\n",
       "      <td>8.418</td>\n",
       "      <td>8.406</td>\n",
       "      <td>8.387</td>\n",
       "      <td>8.375</td>\n",
       "      <td>8.359</td>\n",
       "      <td>8.343</td>\n",
       "      <td>8.330</td>\n",
       "      <td>...</td>\n",
       "      <td>4.811</td>\n",
       "      <td>4.806</td>\n",
       "      <td>4.801</td>\n",
       "      <td>4.796</td>\n",
       "      <td>4.791</td>\n",
       "      <td>4.787</td>\n",
       "      <td>4.782</td>\n",
       "      <td>4.776</td>\n",
       "      <td>4.770</td>\n",
       "      <td>4.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.929</td>\n",
       "      <td>9.905</td>\n",
       "      <td>9.881</td>\n",
       "      <td>9.859</td>\n",
       "      <td>9.835</td>\n",
       "      <td>9.811</td>\n",
       "      <td>9.792</td>\n",
       "      <td>9.769</td>\n",
       "      <td>9.752</td>\n",
       "      <td>9.732</td>\n",
       "      <td>...</td>\n",
       "      <td>5.275</td>\n",
       "      <td>5.269</td>\n",
       "      <td>5.263</td>\n",
       "      <td>5.257</td>\n",
       "      <td>5.251</td>\n",
       "      <td>5.247</td>\n",
       "      <td>5.242</td>\n",
       "      <td>5.238</td>\n",
       "      <td>5.231</td>\n",
       "      <td>5.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.623</td>\n",
       "      <td>11.597</td>\n",
       "      <td>11.574</td>\n",
       "      <td>11.545</td>\n",
       "      <td>11.509</td>\n",
       "      <td>11.481</td>\n",
       "      <td>11.450</td>\n",
       "      <td>11.421</td>\n",
       "      <td>11.389</td>\n",
       "      <td>11.358</td>\n",
       "      <td>...</td>\n",
       "      <td>5.791</td>\n",
       "      <td>5.783</td>\n",
       "      <td>5.777</td>\n",
       "      <td>5.770</td>\n",
       "      <td>5.764</td>\n",
       "      <td>5.759</td>\n",
       "      <td>5.753</td>\n",
       "      <td>5.746</td>\n",
       "      <td>5.740</td>\n",
       "      <td>5.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.608</td>\n",
       "      <td>20.532</td>\n",
       "      <td>20.456</td>\n",
       "      <td>20.378</td>\n",
       "      <td>20.297</td>\n",
       "      <td>20.215</td>\n",
       "      <td>20.129</td>\n",
       "      <td>20.042</td>\n",
       "      <td>19.950</td>\n",
       "      <td>19.854</td>\n",
       "      <td>...</td>\n",
       "      <td>8.565</td>\n",
       "      <td>8.562</td>\n",
       "      <td>8.559</td>\n",
       "      <td>8.555</td>\n",
       "      <td>8.552</td>\n",
       "      <td>8.549</td>\n",
       "      <td>8.546</td>\n",
       "      <td>8.542</td>\n",
       "      <td>8.539</td>\n",
       "      <td>8.536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             10.201            10.176            10.152            10.128   \n",
       "std               2.382             2.368             2.355             2.342   \n",
       "min               4.597             4.595             4.594             4.592   \n",
       "25%               8.463             8.449             8.433             8.418   \n",
       "50%               9.929             9.905             9.881             9.859   \n",
       "75%              11.623            11.597            11.574            11.545   \n",
       "max              20.608            20.532            20.456            20.378   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             10.104            10.079            10.055            10.031   \n",
       "std               2.329             2.316             2.303             2.290   \n",
       "min               4.590             4.589             4.587             4.586   \n",
       "25%               8.406             8.387             8.375             8.359   \n",
       "50%               9.835             9.811             9.792             9.769   \n",
       "75%              11.509            11.481            11.450            11.421   \n",
       "max              20.297            20.215            20.129            20.042   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count         10000.000          10000.000  ...           10000.000   \n",
       "mean             10.006              9.982  ...               5.309   \n",
       "std               2.277              2.264  ...               0.724   \n",
       "min               4.584              4.582  ...               2.789   \n",
       "25%               8.343              8.330  ...               4.811   \n",
       "50%               9.752              9.732  ...               5.275   \n",
       "75%              11.389             11.358  ...               5.791   \n",
       "max              19.950             19.854  ...               8.565   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                5.303               5.298               5.292   \n",
       "std                 0.724               0.723               0.723   \n",
       "min                 2.787               2.785               2.786   \n",
       "25%                 4.806               4.801               4.796   \n",
       "50%                 5.269               5.263               5.257   \n",
       "75%                 5.783               5.777               5.770   \n",
       "max                 8.562               8.559               8.555   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                5.287               5.281               5.276   \n",
       "std                 0.722               0.722               0.721   \n",
       "min                 2.783               2.783               2.780   \n",
       "25%                 4.791               4.787               4.782   \n",
       "50%                 5.251               5.247               5.242   \n",
       "75%                 5.764               5.759               5.753   \n",
       "max                 8.552               8.549               8.546   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count           10000.000           10000.000           10000.000  \n",
       "mean                5.270               5.265               5.259  \n",
       "std                 0.721               0.720               0.720  \n",
       "min                 2.777               2.780               2.777  \n",
       "25%                 4.776               4.770               4.765  \n",
       "50%                 5.238               5.231               5.226  \n",
       "75%                 5.746               5.740               5.733  \n",
       "max                 8.542               8.539               8.536  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:49:17.741559Z",
     "start_time": "2020-12-03T13:49:17.135095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.058</td>\n",
       "      <td>1.047</td>\n",
       "      <td>1.099</td>\n",
       "      <td>1.159</td>\n",
       "      <td>1.174</td>\n",
       "      <td>1.227</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.326</td>\n",
       "      <td>1.371</td>\n",
       "      <td>1.411</td>\n",
       "      <td>...</td>\n",
       "      <td>4.465</td>\n",
       "      <td>4.427</td>\n",
       "      <td>4.418</td>\n",
       "      <td>4.608</td>\n",
       "      <td>4.519</td>\n",
       "      <td>4.416</td>\n",
       "      <td>4.396</td>\n",
       "      <td>4.387</td>\n",
       "      <td>4.387</td>\n",
       "      <td>4.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.329</td>\n",
       "      <td>1.481</td>\n",
       "      <td>3.777</td>\n",
       "      <td>7.466</td>\n",
       "      <td>6.841</td>\n",
       "      <td>9.307</td>\n",
       "      <td>12.153</td>\n",
       "      <td>14.197</td>\n",
       "      <td>16.044</td>\n",
       "      <td>17.226</td>\n",
       "      <td>...</td>\n",
       "      <td>77.027</td>\n",
       "      <td>76.295</td>\n",
       "      <td>75.867</td>\n",
       "      <td>92.800</td>\n",
       "      <td>79.483</td>\n",
       "      <td>75.748</td>\n",
       "      <td>75.385</td>\n",
       "      <td>75.125</td>\n",
       "      <td>75.032</td>\n",
       "      <td>75.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.002</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.395</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.391</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.391</td>\n",
       "      <td>1.387</td>\n",
       "      <td>1.389</td>\n",
       "      <td>1.385</td>\n",
       "      <td>1.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.004</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.009</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.022</td>\n",
       "      <td>1.028</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.040</td>\n",
       "      <td>...</td>\n",
       "      <td>1.877</td>\n",
       "      <td>1.877</td>\n",
       "      <td>1.878</td>\n",
       "      <td>1.878</td>\n",
       "      <td>1.874</td>\n",
       "      <td>1.873</td>\n",
       "      <td>1.876</td>\n",
       "      <td>1.871</td>\n",
       "      <td>1.873</td>\n",
       "      <td>1.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.014</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1.035</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.058</td>\n",
       "      <td>1.071</td>\n",
       "      <td>1.084</td>\n",
       "      <td>1.098</td>\n",
       "      <td>1.113</td>\n",
       "      <td>...</td>\n",
       "      <td>2.685</td>\n",
       "      <td>2.679</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.684</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.682</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.683</td>\n",
       "      <td>2.681</td>\n",
       "      <td>2.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>217.772</td>\n",
       "      <td>147.125</td>\n",
       "      <td>308.283</td>\n",
       "      <td>710.870</td>\n",
       "      <td>621.350</td>\n",
       "      <td>856.579</td>\n",
       "      <td>1139.659</td>\n",
       "      <td>1346.105</td>\n",
       "      <td>1511.021</td>\n",
       "      <td>1618.645</td>\n",
       "      <td>...</td>\n",
       "      <td>6621.291</td>\n",
       "      <td>6660.474</td>\n",
       "      <td>6640.080</td>\n",
       "      <td>8526.950</td>\n",
       "      <td>6657.537</td>\n",
       "      <td>6629.684</td>\n",
       "      <td>6630.515</td>\n",
       "      <td>6620.024</td>\n",
       "      <td>6609.880</td>\n",
       "      <td>6651.516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            1.058           1.047           1.099           1.159   \n",
       "std             2.329           1.481           3.777           7.466   \n",
       "min             0.981           0.965           0.953           0.943   \n",
       "25%             1.000           0.998           0.998           0.999   \n",
       "50%             1.004           1.003           1.005           1.009   \n",
       "75%             1.014           1.017           1.025           1.035   \n",
       "max           217.772         147.125         308.283         710.870   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            1.174           1.227           1.280           1.326   \n",
       "std             6.841           9.307          12.153          14.197   \n",
       "min             0.929           0.918           0.905           0.893   \n",
       "25%             1.000           1.001           1.002           1.004   \n",
       "50%             1.013           1.017           1.022           1.028   \n",
       "75%             1.046           1.058           1.071           1.084   \n",
       "max           621.350         856.579        1139.659        1346.105   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count       10000.000        10000.000  ...         10000.000   \n",
       "mean            1.371            1.411  ...             4.465   \n",
       "std            16.044           17.226  ...            77.027   \n",
       "min             0.880            0.868  ...             0.255   \n",
       "25%             1.006            1.008  ...             1.395   \n",
       "50%             1.034            1.040  ...             1.877   \n",
       "75%             1.098            1.113  ...             2.685   \n",
       "max          1511.021         1618.645  ...          6621.291   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              4.427             4.418             4.608             4.519   \n",
       "std              76.295            75.867            92.800            79.483   \n",
       "min               0.255             0.254             0.253             0.253   \n",
       "25%               1.396             1.392             1.391             1.392   \n",
       "50%               1.877             1.878             1.878             1.874   \n",
       "75%               2.679             2.680             2.684             2.680   \n",
       "max            6660.474          6640.080          8526.950          6657.537   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              4.416             4.396             4.387             4.387   \n",
       "std              75.748            75.385            75.125            75.032   \n",
       "min               0.253             0.252             0.253             0.252   \n",
       "25%               1.391             1.387             1.389             1.385   \n",
       "50%               1.873             1.876             1.871             1.873   \n",
       "75%               2.682             2.680             2.683             2.681   \n",
       "max            6629.684          6630.515          6620.024          6609.880   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count         10000.000  \n",
       "mean              4.406  \n",
       "std              75.742  \n",
       "min               0.252  \n",
       "25%               1.384  \n",
       "50%               1.871  \n",
       "75%               2.682  \n",
       "max            6651.516  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:49:18.357752Z",
     "start_time": "2020-12-03T13:49:17.743228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.034</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.075</td>\n",
       "      <td>1.103</td>\n",
       "      <td>1.132</td>\n",
       "      <td>1.160</td>\n",
       "      <td>1.189</td>\n",
       "      <td>1.219</td>\n",
       "      <td>1.248</td>\n",
       "      <td>1.278</td>\n",
       "      <td>...</td>\n",
       "      <td>4.887</td>\n",
       "      <td>4.887</td>\n",
       "      <td>4.886</td>\n",
       "      <td>4.882</td>\n",
       "      <td>4.883</td>\n",
       "      <td>4.880</td>\n",
       "      <td>4.876</td>\n",
       "      <td>4.875</td>\n",
       "      <td>4.872</td>\n",
       "      <td>4.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.639</td>\n",
       "      <td>1.090</td>\n",
       "      <td>2.261</td>\n",
       "      <td>3.470</td>\n",
       "      <td>4.664</td>\n",
       "      <td>5.829</td>\n",
       "      <td>6.995</td>\n",
       "      <td>8.146</td>\n",
       "      <td>9.279</td>\n",
       "      <td>10.389</td>\n",
       "      <td>...</td>\n",
       "      <td>109.597</td>\n",
       "      <td>109.550</td>\n",
       "      <td>109.530</td>\n",
       "      <td>109.402</td>\n",
       "      <td>109.451</td>\n",
       "      <td>109.400</td>\n",
       "      <td>109.244</td>\n",
       "      <td>109.233</td>\n",
       "      <td>109.145</td>\n",
       "      <td>109.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.965</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.996</td>\n",
       "      <td>...</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.188</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.186</td>\n",
       "      <td>1.186</td>\n",
       "      <td>1.186</td>\n",
       "      <td>1.186</td>\n",
       "      <td>1.185</td>\n",
       "      <td>1.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.020</td>\n",
       "      <td>...</td>\n",
       "      <td>1.634</td>\n",
       "      <td>1.635</td>\n",
       "      <td>1.634</td>\n",
       "      <td>1.634</td>\n",
       "      <td>1.632</td>\n",
       "      <td>1.632</td>\n",
       "      <td>1.631</td>\n",
       "      <td>1.629</td>\n",
       "      <td>1.629</td>\n",
       "      <td>1.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.009</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.019</td>\n",
       "      <td>1.027</td>\n",
       "      <td>1.036</td>\n",
       "      <td>1.047</td>\n",
       "      <td>1.057</td>\n",
       "      <td>1.067</td>\n",
       "      <td>1.078</td>\n",
       "      <td>1.090</td>\n",
       "      <td>...</td>\n",
       "      <td>2.443</td>\n",
       "      <td>2.445</td>\n",
       "      <td>2.445</td>\n",
       "      <td>2.445</td>\n",
       "      <td>2.446</td>\n",
       "      <td>2.446</td>\n",
       "      <td>2.447</td>\n",
       "      <td>2.445</td>\n",
       "      <td>2.445</td>\n",
       "      <td>2.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>37.708</td>\n",
       "      <td>89.112</td>\n",
       "      <td>212.916</td>\n",
       "      <td>334.749</td>\n",
       "      <td>453.600</td>\n",
       "      <td>569.225</td>\n",
       "      <td>684.689</td>\n",
       "      <td>798.535</td>\n",
       "      <td>910.407</td>\n",
       "      <td>1019.974</td>\n",
       "      <td>...</td>\n",
       "      <td>9681.220</td>\n",
       "      <td>9667.358</td>\n",
       "      <td>9670.751</td>\n",
       "      <td>9666.282</td>\n",
       "      <td>9663.828</td>\n",
       "      <td>9666.908</td>\n",
       "      <td>9661.688</td>\n",
       "      <td>9662.915</td>\n",
       "      <td>9660.175</td>\n",
       "      <td>9660.849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.034               1.049               1.075   \n",
       "std                 0.639               1.090               2.261   \n",
       "min                 0.965               0.954               0.939   \n",
       "25%                 0.996               0.994               0.993   \n",
       "50%                 1.001               1.000               1.001   \n",
       "75%                 1.009               1.013               1.019   \n",
       "max                37.708              89.112             212.916   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.103               1.132               1.160   \n",
       "std                 3.470               4.664               5.829   \n",
       "min                 0.927               0.916               0.906   \n",
       "25%                 0.993               0.993               0.993   \n",
       "50%                 1.003               1.005               1.007   \n",
       "75%                 1.027               1.036               1.047   \n",
       "max               334.749             453.600             569.225   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.189               1.219               1.248   \n",
       "std                 6.995               8.146               9.279   \n",
       "min                 0.893               0.879               0.865   \n",
       "25%                 0.994               0.994               0.995   \n",
       "50%                 1.010               1.013               1.016   \n",
       "75%                 1.057               1.067               1.078   \n",
       "max               684.689             798.535             910.407   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count            10000.000  ...             10000.000             10000.000   \n",
       "mean                 1.278  ...                 4.887                 4.887   \n",
       "std                 10.389  ...               109.597               109.550   \n",
       "min                  0.852  ...                 0.247                 0.246   \n",
       "25%                  0.996  ...                 1.187                 1.188   \n",
       "50%                  1.020  ...                 1.634                 1.635   \n",
       "75%                  1.090  ...                 2.443                 2.445   \n",
       "max               1019.974  ...              9681.220              9667.358   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  4.886                 4.882                 4.883   \n",
       "std                 109.530               109.402               109.451   \n",
       "min                   0.246                 0.246                 0.246   \n",
       "25%                   1.187                 1.187                 1.186   \n",
       "50%                   1.634                 1.634                 1.632   \n",
       "75%                   2.445                 2.445                 2.446   \n",
       "max                9670.751              9666.282              9663.828   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  4.880                 4.876                 4.875   \n",
       "std                 109.400               109.244               109.233   \n",
       "min                   0.245                 0.245                 0.244   \n",
       "25%                   1.186                 1.186                 1.186   \n",
       "50%                   1.632                 1.631                 1.629   \n",
       "75%                   2.446                 2.447                 2.445   \n",
       "max                9666.908              9661.688              9662.915   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count             10000.000             10000.000  \n",
       "mean                  4.872                 4.870  \n",
       "std                 109.145               109.105  \n",
       "min                   0.244                 0.244  \n",
       "25%                   1.185                 1.183  \n",
       "50%                   1.629                 1.629  \n",
       "75%                   2.445                 2.444  \n",
       "max                9660.175              9660.849  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:49:19.861453Z",
     "start_time": "2020-12-03T13:49:18.359367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T13:49:21.625854Z",
     "start_time": "2020-12-03T13:49:19.864127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
