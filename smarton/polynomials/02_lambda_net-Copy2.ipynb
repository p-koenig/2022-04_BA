{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:39:34.932644Z",
     "start_time": "2021-01-17T08:39:34.929215Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:39:34.941968Z",
     "start_time": "2021-01-17T08:39:34.934600Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 6\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.001\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 100000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='adam' \n",
    "\n",
    "each_epochs_save = 5  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "\n",
    "fixed_seed_lambda_training = True\n",
    "fixed_initialization_lambda_training = False\n",
    "number_different_lambda_trainings = 1\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:39:34.951520Z",
     "start_time": "2021-01-17T08:39:34.943916Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if each_epochs_save != None:\n",
    "    epochs_save_range = range(1, epochs//each_epochs_save+1) if each_epochs_save == 1 else range(epochs//each_epochs_save+1)\n",
    "else:\n",
    "    epochs_save_range = None\n",
    "    \n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_' + str(number_different_lambda_trainings) + '-FixedSeed'\n",
    "else:\n",
    "    seed_shuffle_string = '_NoFixedSeed'\n",
    "    \n",
    "if fixed_initialization_lambda_training:\n",
    "    seed_shuffle_string += '_' + str(number_different_lambda_trainings) + '-FixedEvaluation'\n",
    "else:\n",
    "    seed_shuffle_string += '_NoFixedEvaluation'\n",
    "    \n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:40:19.155558Z",
     "start_time": "2021-01-17T08:39:34.953506Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:40:19.178484Z",
     "start_time": "2021-01-17T08:40:19.157605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:40:19.204545Z",
     "start_time": "2021-01-17T08:40:19.180785Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "    \n",
    "    \n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:40:19.210894Z",
     "start_time": "2021-01-17T08:40:19.206631Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcualate_function_value_with_X_data_entry(coefficient_list, X_data_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "     \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [X_data_value**int(coefficient_multiplier) for coefficient_multiplier, X_data_value in zip(coefficient_multipliers, X_data_entry)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "        \n",
    "    return result, np.append(X_data_entry, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:40:19.238039Z",
     "start_time": "2021-01-17T08:40:19.212613Z"
    },
    "code_folding": [
     0,
     20,
     43,
     66,
     88,
     91,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:40:19.337313Z",
     "start_time": "2021-01-17T08:40:19.239621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecd89bffb8843e393bd8ea879cd246f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4096), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 4096\n",
      "Number of monomials in a polynomial with 6 variables and degree 3: 84\n",
      "Sparsity: 84\n",
      "['000000', '000001', '000002', '000003', '000010', '000011', '000012', '000013', '000020', '000021', '000022', '000023', '000030', '000031', '000032', '000033', '000100', '000101', '000102', '000103', '000110', '000111', '000112', '000113', '000120', '000121', '000122', '000123', '000130', '000131', '000132', '000133', '000200', '000201', '000202', '000203', '000210', '000211', '000212', '000213', '000220', '000221', '000222', '000223', '000230', '000231', '000232', '000233', '000300', '000301', '000302', '000303', '000310', '000311', '000312', '000313', '000320', '000321', '000322', '000323', '000330', '000331', '000332', '000333', '001000', '001001', '001002', '001003', '001010', '001011', '001012', '001013', '001020', '001021', '001022', '001023', '001030', '001031', '001032', '001033', '001100', '001101', '001102', '001103', '001110', '001111', '001112', '001113', '001120', '001121', '001122', '001123', '001130', '001131', '001132', '001133', '001200', '001201', '001202', '001203', '001210', '001211', '001212', '001213', '001220', '001221', '001222', '001223', '001230', '001231', '001232', '001233', '001300', '001301', '001302', '001303', '001310', '001311', '001312', '001313', '001320', '001321', '001322', '001323', '001330', '001331', '001332', '001333', '002000', '002001', '002002', '002003', '002010', '002011', '002012', '002013', '002020', '002021', '002022', '002023', '002030', '002031', '002032', '002033', '002100', '002101', '002102', '002103', '002110', '002111', '002112', '002113', '002120', '002121', '002122', '002123', '002130', '002131', '002132', '002133', '002200', '002201', '002202', '002203', '002210', '002211', '002212', '002213', '002220', '002221', '002222', '002223', '002230', '002231', '002232', '002233', '002300', '002301', '002302', '002303', '002310', '002311', '002312', '002313', '002320', '002321', '002322', '002323', '002330', '002331', '002332', '002333', '003000', '003001', '003002', '003003', '003010', '003011', '003012', '003013', '003020', '003021', '003022', '003023', '003030', '003031', '003032', '003033', '003100', '003101', '003102', '003103', '003110', '003111', '003112', '003113', '003120', '003121', '003122', '003123', '003130', '003131', '003132', '003133', '003200', '003201', '003202', '003203', '003210', '003211', '003212', '003213', '003220', '003221', '003222', '003223', '003230', '003231', '003232', '003233', '003300', '003301', '003302', '003303', '003310', '003311', '003312', '003313', '003320', '003321', '003322', '003323', '003330', '003331', '003332', '003333', '010000', '010001', '010002', '010003', '010010', '010011', '010012', '010013', '010020', '010021', '010022', '010023', '010030', '010031', '010032', '010033', '010100', '010101', '010102', '010103', '010110', '010111', '010112', '010113', '010120', '010121', '010122', '010123', '010130', '010131', '010132', '010133', '010200', '010201', '010202', '010203', '010210', '010211', '010212', '010213', '010220', '010221', '010222', '010223', '010230', '010231', '010232', '010233', '010300', '010301', '010302', '010303', '010310', '010311', '010312', '010313', '010320', '010321', '010322', '010323', '010330', '010331', '010332', '010333', '011000', '011001', '011002', '011003', '011010', '011011', '011012', '011013', '011020', '011021', '011022', '011023', '011030', '011031', '011032', '011033', '011100', '011101', '011102', '011103', '011110', '011111', '011112', '011113', '011120', '011121', '011122', '011123', '011130', '011131', '011132', '011133', '011200', '011201', '011202', '011203', '011210', '011211', '011212', '011213', '011220', '011221', '011222', '011223', '011230', '011231', '011232', '011233', '011300', '011301', '011302', '011303', '011310', '011311', '011312', '011313', '011320', '011321', '011322', '011323', '011330', '011331', '011332', '011333', '012000', '012001', '012002', '012003', '012010', '012011', '012012', '012013', '012020', '012021', '012022', '012023', '012030', '012031', '012032', '012033', '012100', '012101', '012102', '012103', '012110', '012111', '012112', '012113', '012120', '012121', '012122', '012123', '012130', '012131', '012132', '012133', '012200', '012201', '012202', '012203', '012210', '012211', '012212', '012213', '012220', '012221', '012222', '012223', '012230', '012231', '012232', '012233', '012300', '012301', '012302', '012303', '012310', '012311', '012312', '012313', '012320', '012321', '012322', '012323', '012330', '012331', '012332', '012333', '013000', '013001', '013002', '013003', '013010', '013011', '013012', '013013', '013020', '013021', '013022', '013023', '013030', '013031', '013032', '013033', '013100', '013101', '013102', '013103', '013110', '013111', '013112', '013113', '013120', '013121', '013122', '013123', '013130', '013131', '013132', '013133', '013200', '013201', '013202', '013203', '013210', '013211', '013212', '013213', '013220', '013221', '013222', '013223', '013230', '013231', '013232', '013233', '013300', '013301', '013302', '013303', '013310', '013311', '013312', '013313', '013320', '013321', '013322', '013323', '013330', '013331', '013332', '013333', '020000', '020001', '020002', '020003', '020010', '020011', '020012', '020013', '020020', '020021', '020022', '020023', '020030', '020031', '020032', '020033', '020100', '020101', '020102', '020103', '020110', '020111', '020112', '020113', '020120', '020121', '020122', '020123', '020130', '020131', '020132', '020133', '020200', '020201', '020202', '020203', '020210', '020211', '020212', '020213', '020220', '020221', '020222', '020223', '020230', '020231', '020232', '020233', '020300', '020301', '020302', '020303', '020310', '020311', '020312', '020313', '020320', '020321', '020322', '020323', '020330', '020331', '020332', '020333', '021000', '021001', '021002', '021003', '021010', '021011', '021012', '021013', '021020', '021021', '021022', '021023', '021030', '021031', '021032', '021033', '021100', '021101', '021102', '021103', '021110', '021111', '021112', '021113', '021120', '021121', '021122', '021123', '021130', '021131', '021132', '021133', '021200', '021201', '021202', '021203', '021210', '021211', '021212', '021213', '021220', '021221', '021222', '021223', '021230', '021231', '021232', '021233', '021300', '021301', '021302', '021303', '021310', '021311', '021312', '021313', '021320', '021321', '021322', '021323', '021330', '021331', '021332', '021333', '022000', '022001', '022002', '022003', '022010', '022011', '022012', '022013', '022020', '022021', '022022', '022023', '022030', '022031', '022032', '022033', '022100', '022101', '022102', '022103', '022110', '022111', '022112', '022113', '022120', '022121', '022122', '022123', '022130', '022131', '022132', '022133', '022200', '022201', '022202', '022203', '022210', '022211', '022212', '022213', '022220', '022221', '022222', '022223', '022230', '022231', '022232', '022233', '022300', '022301', '022302', '022303', '022310', '022311', '022312', '022313', '022320', '022321', '022322', '022323', '022330', '022331', '022332', '022333', '023000', '023001', '023002', '023003', '023010', '023011', '023012', '023013', '023020', '023021', '023022', '023023', '023030', '023031', '023032', '023033', '023100', '023101', '023102', '023103', '023110', '023111', '023112', '023113', '023120', '023121', '023122', '023123', '023130', '023131', '023132', '023133', '023200', '023201', '023202', '023203', '023210', '023211', '023212', '023213', '023220', '023221', '023222', '023223', '023230', '023231', '023232', '023233', '023300', '023301', '023302', '023303', '023310', '023311', '023312', '023313', '023320', '023321', '023322', '023323', '023330', '023331', '023332', '023333', '030000', '030001', '030002', '030003', '030010', '030011', '030012', '030013', '030020', '030021', '030022', '030023', '030030', '030031', '030032', '030033', '030100', '030101', '030102', '030103', '030110', '030111', '030112', '030113', '030120', '030121', '030122', '030123', '030130', '030131', '030132', '030133', '030200', '030201', '030202', '030203', '030210', '030211', '030212', '030213', '030220', '030221', '030222', '030223', '030230', '030231', '030232', '030233', '030300', '030301', '030302', '030303', '030310', '030311', '030312', '030313', '030320', '030321', '030322', '030323', '030330', '030331', '030332', '030333', '031000', '031001', '031002', '031003', '031010', '031011', '031012', '031013', '031020', '031021', '031022', '031023', '031030', '031031', '031032', '031033', '031100', '031101', '031102', '031103', '031110', '031111', '031112', '031113', '031120', '031121', '031122', '031123', '031130', '031131', '031132', '031133', '031200', '031201', '031202', '031203', '031210', '031211', '031212', '031213', '031220', '031221', '031222', '031223', '031230', '031231', '031232', '031233', '031300', '031301', '031302', '031303', '031310', '031311', '031312', '031313', '031320', '031321', '031322', '031323', '031330', '031331', '031332', '031333', '032000', '032001', '032002', '032003', '032010', '032011', '032012', '032013', '032020', '032021', '032022', '032023', '032030', '032031', '032032', '032033', '032100', '032101', '032102', '032103', '032110', '032111', '032112', '032113', '032120', '032121', '032122', '032123', '032130', '032131', '032132', '032133', '032200', '032201', '032202', '032203', '032210', '032211', '032212', '032213', '032220', '032221', '032222', '032223', '032230', '032231', '032232', '032233', '032300', '032301', '032302', '032303', '032310', '032311', '032312', '032313', '032320', '032321', '032322', '032323', '032330', '032331', '032332', '032333', '033000', '033001', '033002', '033003', '033010', '033011', '033012', '033013', '033020', '033021', '033022', '033023', '033030', '033031', '033032', '033033', '033100', '033101', '033102', '033103', '033110', '033111', '033112', '033113', '033120', '033121', '033122', '033123', '033130', '033131', '033132', '033133', '033200', '033201', '033202', '033203', '033210', '033211', '033212', '033213', '033220', '033221', '033222', '033223', '033230', '033231', '033232', '033233', '033300', '033301', '033302', '033303', '033310', '033311', '033312', '033313', '033320', '033321', '033322', '033323', '033330', '033331', '033332', '033333', '100000', '100001', '100002', '100003', '100010', '100011', '100012', '100013', '100020', '100021', '100022', '100023', '100030', '100031', '100032', '100033', '100100', '100101', '100102', '100103', '100110', '100111', '100112', '100113', '100120', '100121', '100122', '100123', '100130', '100131', '100132', '100133', '100200', '100201', '100202', '100203', '100210', '100211', '100212', '100213', '100220', '100221', '100222', '100223', '100230', '100231', '100232', '100233', '100300', '100301', '100302', '100303', '100310', '100311', '100312', '100313', '100320', '100321', '100322', '100323', '100330', '100331', '100332', '100333', '101000', '101001', '101002', '101003', '101010', '101011', '101012', '101013', '101020', '101021', '101022', '101023', '101030', '101031', '101032', '101033', '101100', '101101', '101102', '101103', '101110', '101111', '101112', '101113', '101120', '101121', '101122', '101123', '101130', '101131', '101132', '101133', '101200', '101201', '101202', '101203', '101210', '101211', '101212', '101213', '101220', '101221', '101222', '101223', '101230', '101231', '101232', '101233', '101300', '101301', '101302', '101303', '101310', '101311', '101312', '101313', '101320', '101321', '101322', '101323', '101330', '101331', '101332', '101333', '102000', '102001', '102002', '102003', '102010', '102011', '102012', '102013', '102020', '102021', '102022', '102023', '102030', '102031', '102032', '102033', '102100', '102101', '102102', '102103', '102110', '102111', '102112', '102113', '102120', '102121', '102122', '102123', '102130', '102131', '102132', '102133', '102200', '102201', '102202', '102203', '102210', '102211', '102212', '102213', '102220', '102221', '102222', '102223', '102230', '102231', '102232', '102233', '102300', '102301', '102302', '102303', '102310', '102311', '102312', '102313', '102320', '102321', '102322', '102323', '102330', '102331', '102332', '102333', '103000', '103001', '103002', '103003', '103010', '103011', '103012', '103013', '103020', '103021', '103022', '103023', '103030', '103031', '103032', '103033', '103100', '103101', '103102', '103103', '103110', '103111', '103112', '103113', '103120', '103121', '103122', '103123', '103130', '103131', '103132', '103133', '103200', '103201', '103202', '103203', '103210', '103211', '103212', '103213', '103220', '103221', '103222', '103223', '103230', '103231', '103232', '103233', '103300', '103301', '103302', '103303', '103310', '103311', '103312', '103313', '103320', '103321', '103322', '103323', '103330', '103331', '103332', '103333', '110000', '110001', '110002', '110003', '110010', '110011', '110012', '110013', '110020', '110021', '110022', '110023', '110030', '110031', '110032', '110033', '110100', '110101', '110102', '110103', '110110', '110111', '110112', '110113', '110120', '110121', '110122', '110123', '110130', '110131', '110132', '110133', '110200', '110201', '110202', '110203', '110210', '110211', '110212', '110213', '110220', '110221', '110222', '110223', '110230', '110231', '110232', '110233', '110300', '110301', '110302', '110303', '110310', '110311', '110312', '110313', '110320', '110321', '110322', '110323', '110330', '110331', '110332', '110333', '111000', '111001', '111002', '111003', '111010', '111011', '111012', '111013', '111020', '111021', '111022', '111023', '111030', '111031', '111032', '111033', '111100', '111101', '111102', '111103', '111110', '111111', '111112', '111113', '111120', '111121', '111122', '111123', '111130', '111131', '111132', '111133', '111200', '111201', '111202', '111203', '111210', '111211', '111212', '111213', '111220', '111221', '111222', '111223', '111230', '111231', '111232', '111233', '111300', '111301', '111302', '111303', '111310', '111311', '111312', '111313', '111320', '111321', '111322', '111323', '111330', '111331', '111332', '111333', '112000', '112001', '112002', '112003', '112010', '112011', '112012', '112013', '112020', '112021', '112022', '112023', '112030', '112031', '112032', '112033', '112100', '112101', '112102', '112103', '112110', '112111', '112112', '112113', '112120', '112121', '112122', '112123', '112130', '112131', '112132', '112133', '112200', '112201', '112202', '112203', '112210', '112211', '112212', '112213', '112220', '112221', '112222', '112223', '112230', '112231', '112232', '112233', '112300', '112301', '112302', '112303', '112310', '112311', '112312', '112313', '112320', '112321', '112322', '112323', '112330', '112331', '112332', '112333', '113000', '113001', '113002', '113003', '113010', '113011', '113012', '113013', '113020', '113021', '113022', '113023', '113030', '113031', '113032', '113033', '113100', '113101', '113102', '113103', '113110', '113111', '113112', '113113', '113120', '113121', '113122', '113123', '113130', '113131', '113132', '113133', '113200', '113201', '113202', '113203', '113210', '113211', '113212', '113213', '113220', '113221', '113222', '113223', '113230', '113231', '113232', '113233', '113300', '113301', '113302', '113303', '113310', '113311', '113312', '113313', '113320', '113321', '113322', '113323', '113330', '113331', '113332', '113333', '120000', '120001', '120002', '120003', '120010', '120011', '120012', '120013', '120020', '120021', '120022', '120023', '120030', '120031', '120032', '120033', '120100', '120101', '120102', '120103', '120110', '120111', '120112', '120113', '120120', '120121', '120122', '120123', '120130', '120131', '120132', '120133', '120200', '120201', '120202', '120203', '120210', '120211', '120212', '120213', '120220', '120221', '120222', '120223', '120230', '120231', '120232', '120233', '120300', '120301', '120302', '120303', '120310', '120311', '120312', '120313', '120320', '120321', '120322', '120323', '120330', '120331', '120332', '120333', '121000', '121001', '121002', '121003', '121010', '121011', '121012', '121013', '121020', '121021', '121022', '121023', '121030', '121031', '121032', '121033', '121100', '121101', '121102', '121103', '121110', '121111', '121112', '121113', '121120', '121121', '121122', '121123', '121130', '121131', '121132', '121133', '121200', '121201', '121202', '121203', '121210', '121211', '121212', '121213', '121220', '121221', '121222', '121223', '121230', '121231', '121232', '121233', '121300', '121301', '121302', '121303', '121310', '121311', '121312', '121313', '121320', '121321', '121322', '121323', '121330', '121331', '121332', '121333', '122000', '122001', '122002', '122003', '122010', '122011', '122012', '122013', '122020', '122021', '122022', '122023', '122030', '122031', '122032', '122033', '122100', '122101', '122102', '122103', '122110', '122111', '122112', '122113', '122120', '122121', '122122', '122123', '122130', '122131', '122132', '122133', '122200', '122201', '122202', '122203', '122210', '122211', '122212', '122213', '122220', '122221', '122222', '122223', '122230', '122231', '122232', '122233', '122300', '122301', '122302', '122303', '122310', '122311', '122312', '122313', '122320', '122321', '122322', '122323', '122330', '122331', '122332', '122333', '123000', '123001', '123002', '123003', '123010', '123011', '123012', '123013', '123020', '123021', '123022', '123023', '123030', '123031', '123032', '123033', '123100', '123101', '123102', '123103', '123110', '123111', '123112', '123113', '123120', '123121', '123122', '123123', '123130', '123131', '123132', '123133', '123200', '123201', '123202', '123203', '123210', '123211', '123212', '123213', '123220', '123221', '123222', '123223', '123230', '123231', '123232', '123233', '123300', '123301', '123302', '123303', '123310', '123311', '123312', '123313', '123320', '123321', '123322', '123323', '123330', '123331', '123332', '123333', '130000', '130001', '130002', '130003', '130010', '130011', '130012', '130013', '130020', '130021', '130022', '130023', '130030', '130031', '130032', '130033', '130100', '130101', '130102', '130103', '130110', '130111', '130112', '130113', '130120', '130121', '130122', '130123', '130130', '130131', '130132', '130133', '130200', '130201', '130202', '130203', '130210', '130211', '130212', '130213', '130220', '130221', '130222', '130223', '130230', '130231', '130232', '130233', '130300', '130301', '130302', '130303', '130310', '130311', '130312', '130313', '130320', '130321', '130322', '130323', '130330', '130331', '130332', '130333', '131000', '131001', '131002', '131003', '131010', '131011', '131012', '131013', '131020', '131021', '131022', '131023', '131030', '131031', '131032', '131033', '131100', '131101', '131102', '131103', '131110', '131111', '131112', '131113', '131120', '131121', '131122', '131123', '131130', '131131', '131132', '131133', '131200', '131201', '131202', '131203', '131210', '131211', '131212', '131213', '131220', '131221', '131222', '131223', '131230', '131231', '131232', '131233', '131300', '131301', '131302', '131303', '131310', '131311', '131312', '131313', '131320', '131321', '131322', '131323', '131330', '131331', '131332', '131333', '132000', '132001', '132002', '132003', '132010', '132011', '132012', '132013', '132020', '132021', '132022', '132023', '132030', '132031', '132032', '132033', '132100', '132101', '132102', '132103', '132110', '132111', '132112', '132113', '132120', '132121', '132122', '132123', '132130', '132131', '132132', '132133', '132200', '132201', '132202', '132203', '132210', '132211', '132212', '132213', '132220', '132221', '132222', '132223', '132230', '132231', '132232', '132233', '132300', '132301', '132302', '132303', '132310', '132311', '132312', '132313', '132320', '132321', '132322', '132323', '132330', '132331', '132332', '132333', '133000', '133001', '133002', '133003', '133010', '133011', '133012', '133013', '133020', '133021', '133022', '133023', '133030', '133031', '133032', '133033', '133100', '133101', '133102', '133103', '133110', '133111', '133112', '133113', '133120', '133121', '133122', '133123', '133130', '133131', '133132', '133133', '133200', '133201', '133202', '133203', '133210', '133211', '133212', '133213', '133220', '133221', '133222', '133223', '133230', '133231', '133232', '133233', '133300', '133301', '133302', '133303', '133310', '133311', '133312', '133313', '133320', '133321', '133322', '133323', '133330', '133331', '133332', '133333', '200000', '200001', '200002', '200003', '200010', '200011', '200012', '200013', '200020', '200021', '200022', '200023', '200030', '200031', '200032', '200033', '200100', '200101', '200102', '200103', '200110', '200111', '200112', '200113', '200120', '200121', '200122', '200123', '200130', '200131', '200132', '200133', '200200', '200201', '200202', '200203', '200210', '200211', '200212', '200213', '200220', '200221', '200222', '200223', '200230', '200231', '200232', '200233', '200300', '200301', '200302', '200303', '200310', '200311', '200312', '200313', '200320', '200321', '200322', '200323', '200330', '200331', '200332', '200333', '201000', '201001', '201002', '201003', '201010', '201011', '201012', '201013', '201020', '201021', '201022', '201023', '201030', '201031', '201032', '201033', '201100', '201101', '201102', '201103', '201110', '201111', '201112', '201113', '201120', '201121', '201122', '201123', '201130', '201131', '201132', '201133', '201200', '201201', '201202', '201203', '201210', '201211', '201212', '201213', '201220', '201221', '201222', '201223', '201230', '201231', '201232', '201233', '201300', '201301', '201302', '201303', '201310', '201311', '201312', '201313', '201320', '201321', '201322', '201323', '201330', '201331', '201332', '201333', '202000', '202001', '202002', '202003', '202010', '202011', '202012', '202013', '202020', '202021', '202022', '202023', '202030', '202031', '202032', '202033', '202100', '202101', '202102', '202103', '202110', '202111', '202112', '202113', '202120', '202121', '202122', '202123', '202130', '202131', '202132', '202133', '202200', '202201', '202202', '202203', '202210', '202211', '202212', '202213', '202220', '202221', '202222', '202223', '202230', '202231', '202232', '202233', '202300', '202301', '202302', '202303', '202310', '202311', '202312', '202313', '202320', '202321', '202322', '202323', '202330', '202331', '202332', '202333', '203000', '203001', '203002', '203003', '203010', '203011', '203012', '203013', '203020', '203021', '203022', '203023', '203030', '203031', '203032', '203033', '203100', '203101', '203102', '203103', '203110', '203111', '203112', '203113', '203120', '203121', '203122', '203123', '203130', '203131', '203132', '203133', '203200', '203201', '203202', '203203', '203210', '203211', '203212', '203213', '203220', '203221', '203222', '203223', '203230', '203231', '203232', '203233', '203300', '203301', '203302', '203303', '203310', '203311', '203312', '203313', '203320', '203321', '203322', '203323', '203330', '203331', '203332', '203333', '210000', '210001', '210002', '210003', '210010', '210011', '210012', '210013', '210020', '210021', '210022', '210023', '210030', '210031', '210032', '210033', '210100', '210101', '210102', '210103', '210110', '210111', '210112', '210113', '210120', '210121', '210122', '210123', '210130', '210131', '210132', '210133', '210200', '210201', '210202', '210203', '210210', '210211', '210212', '210213', '210220', '210221', '210222', '210223', '210230', '210231', '210232', '210233', '210300', '210301', '210302', '210303', '210310', '210311', '210312', '210313', '210320', '210321', '210322', '210323', '210330', '210331', '210332', '210333', '211000', '211001', '211002', '211003', '211010', '211011', '211012', '211013', '211020', '211021', '211022', '211023', '211030', '211031', '211032', '211033', '211100', '211101', '211102', '211103', '211110', '211111', '211112', '211113', '211120', '211121', '211122', '211123', '211130', '211131', '211132', '211133', '211200', '211201', '211202', '211203', '211210', '211211', '211212', '211213', '211220', '211221', '211222', '211223', '211230', '211231', '211232', '211233', '211300', '211301', '211302', '211303', '211310', '211311', '211312', '211313', '211320', '211321', '211322', '211323', '211330', '211331', '211332', '211333', '212000', '212001', '212002', '212003', '212010', '212011', '212012', '212013', '212020', '212021', '212022', '212023', '212030', '212031', '212032', '212033', '212100', '212101', '212102', '212103', '212110', '212111', '212112', '212113', '212120', '212121', '212122', '212123', '212130', '212131', '212132', '212133', '212200', '212201', '212202', '212203', '212210', '212211', '212212', '212213', '212220', '212221', '212222', '212223', '212230', '212231', '212232', '212233', '212300', '212301', '212302', '212303', '212310', '212311', '212312', '212313', '212320', '212321', '212322', '212323', '212330', '212331', '212332', '212333', '213000', '213001', '213002', '213003', '213010', '213011', '213012', '213013', '213020', '213021', '213022', '213023', '213030', '213031', '213032', '213033', '213100', '213101', '213102', '213103', '213110', '213111', '213112', '213113', '213120', '213121', '213122', '213123', '213130', '213131', '213132', '213133', '213200', '213201', '213202', '213203', '213210', '213211', '213212', '213213', '213220', '213221', '213222', '213223', '213230', '213231', '213232', '213233', '213300', '213301', '213302', '213303', '213310', '213311', '213312', '213313', '213320', '213321', '213322', '213323', '213330', '213331', '213332', '213333', '220000', '220001', '220002', '220003', '220010', '220011', '220012', '220013', '220020', '220021', '220022', '220023', '220030', '220031', '220032', '220033', '220100', '220101', '220102', '220103', '220110', '220111', '220112', '220113', '220120', '220121', '220122', '220123', '220130', '220131', '220132', '220133', '220200', '220201', '220202', '220203', '220210', '220211', '220212', '220213', '220220', '220221', '220222', '220223', '220230', '220231', '220232', '220233', '220300', '220301', '220302', '220303', '220310', '220311', '220312', '220313', '220320', '220321', '220322', '220323', '220330', '220331', '220332', '220333', '221000', '221001', '221002', '221003', '221010', '221011', '221012', '221013', '221020', '221021', '221022', '221023', '221030', '221031', '221032', '221033', '221100', '221101', '221102', '221103', '221110', '221111', '221112', '221113', '221120', '221121', '221122', '221123', '221130', '221131', '221132', '221133', '221200', '221201', '221202', '221203', '221210', '221211', '221212', '221213', '221220', '221221', '221222', '221223', '221230', '221231', '221232', '221233', '221300', '221301', '221302', '221303', '221310', '221311', '221312', '221313', '221320', '221321', '221322', '221323', '221330', '221331', '221332', '221333', '222000', '222001', '222002', '222003', '222010', '222011', '222012', '222013', '222020', '222021', '222022', '222023', '222030', '222031', '222032', '222033', '222100', '222101', '222102', '222103', '222110', '222111', '222112', '222113', '222120', '222121', '222122', '222123', '222130', '222131', '222132', '222133', '222200', '222201', '222202', '222203', '222210', '222211', '222212', '222213', '222220', '222221', '222222', '222223', '222230', '222231', '222232', '222233', '222300', '222301', '222302', '222303', '222310', '222311', '222312', '222313', '222320', '222321', '222322', '222323', '222330', '222331', '222332', '222333', '223000', '223001', '223002', '223003', '223010', '223011', '223012', '223013', '223020', '223021', '223022', '223023', '223030', '223031', '223032', '223033', '223100', '223101', '223102', '223103', '223110', '223111', '223112', '223113', '223120', '223121', '223122', '223123', '223130', '223131', '223132', '223133', '223200', '223201', '223202', '223203', '223210', '223211', '223212', '223213', '223220', '223221', '223222', '223223', '223230', '223231', '223232', '223233', '223300', '223301', '223302', '223303', '223310', '223311', '223312', '223313', '223320', '223321', '223322', '223323', '223330', '223331', '223332', '223333', '230000', '230001', '230002', '230003', '230010', '230011', '230012', '230013', '230020', '230021', '230022', '230023', '230030', '230031', '230032', '230033', '230100', '230101', '230102', '230103', '230110', '230111', '230112', '230113', '230120', '230121', '230122', '230123', '230130', '230131', '230132', '230133', '230200', '230201', '230202', '230203', '230210', '230211', '230212', '230213', '230220', '230221', '230222', '230223', '230230', '230231', '230232', '230233', '230300', '230301', '230302', '230303', '230310', '230311', '230312', '230313', '230320', '230321', '230322', '230323', '230330', '230331', '230332', '230333', '231000', '231001', '231002', '231003', '231010', '231011', '231012', '231013', '231020', '231021', '231022', '231023', '231030', '231031', '231032', '231033', '231100', '231101', '231102', '231103', '231110', '231111', '231112', '231113', '231120', '231121', '231122', '231123', '231130', '231131', '231132', '231133', '231200', '231201', '231202', '231203', '231210', '231211', '231212', '231213', '231220', '231221', '231222', '231223', '231230', '231231', '231232', '231233', '231300', '231301', '231302', '231303', '231310', '231311', '231312', '231313', '231320', '231321', '231322', '231323', '231330', '231331', '231332', '231333', '232000', '232001', '232002', '232003', '232010', '232011', '232012', '232013', '232020', '232021', '232022', '232023', '232030', '232031', '232032', '232033', '232100', '232101', '232102', '232103', '232110', '232111', '232112', '232113', '232120', '232121', '232122', '232123', '232130', '232131', '232132', '232133', '232200', '232201', '232202', '232203', '232210', '232211', '232212', '232213', '232220', '232221', '232222', '232223', '232230', '232231', '232232', '232233', '232300', '232301', '232302', '232303', '232310', '232311', '232312', '232313', '232320', '232321', '232322', '232323', '232330', '232331', '232332', '232333', '233000', '233001', '233002', '233003', '233010', '233011', '233012', '233013', '233020', '233021', '233022', '233023', '233030', '233031', '233032', '233033', '233100', '233101', '233102', '233103', '233110', '233111', '233112', '233113', '233120', '233121', '233122', '233123', '233130', '233131', '233132', '233133', '233200', '233201', '233202', '233203', '233210', '233211', '233212', '233213', '233220', '233221', '233222', '233223', '233230', '233231', '233232', '233233', '233300', '233301', '233302', '233303', '233310', '233311', '233312', '233313', '233320', '233321', '233322', '233323', '233330', '233331', '233332', '233333', '300000', '300001', '300002', '300003', '300010', '300011', '300012', '300013', '300020', '300021', '300022', '300023', '300030', '300031', '300032', '300033', '300100', '300101', '300102', '300103', '300110', '300111', '300112', '300113', '300120', '300121', '300122', '300123', '300130', '300131', '300132', '300133', '300200', '300201', '300202', '300203', '300210', '300211', '300212', '300213', '300220', '300221', '300222', '300223', '300230', '300231', '300232', '300233', '300300', '300301', '300302', '300303', '300310', '300311', '300312', '300313', '300320', '300321', '300322', '300323', '300330', '300331', '300332', '300333', '301000', '301001', '301002', '301003', '301010', '301011', '301012', '301013', '301020', '301021', '301022', '301023', '301030', '301031', '301032', '301033', '301100', '301101', '301102', '301103', '301110', '301111', '301112', '301113', '301120', '301121', '301122', '301123', '301130', '301131', '301132', '301133', '301200', '301201', '301202', '301203', '301210', '301211', '301212', '301213', '301220', '301221', '301222', '301223', '301230', '301231', '301232', '301233', '301300', '301301', '301302', '301303', '301310', '301311', '301312', '301313', '301320', '301321', '301322', '301323', '301330', '301331', '301332', '301333', '302000', '302001', '302002', '302003', '302010', '302011', '302012', '302013', '302020', '302021', '302022', '302023', '302030', '302031', '302032', '302033', '302100', '302101', '302102', '302103', '302110', '302111', '302112', '302113', '302120', '302121', '302122', '302123', '302130', '302131', '302132', '302133', '302200', '302201', '302202', '302203', '302210', '302211', '302212', '302213', '302220', '302221', '302222', '302223', '302230', '302231', '302232', '302233', '302300', '302301', '302302', '302303', '302310', '302311', '302312', '302313', '302320', '302321', '302322', '302323', '302330', '302331', '302332', '302333', '303000', '303001', '303002', '303003', '303010', '303011', '303012', '303013', '303020', '303021', '303022', '303023', '303030', '303031', '303032', '303033', '303100', '303101', '303102', '303103', '303110', '303111', '303112', '303113', '303120', '303121', '303122', '303123', '303130', '303131', '303132', '303133', '303200', '303201', '303202', '303203', '303210', '303211', '303212', '303213', '303220', '303221', '303222', '303223', '303230', '303231', '303232', '303233', '303300', '303301', '303302', '303303', '303310', '303311', '303312', '303313', '303320', '303321', '303322', '303323', '303330', '303331', '303332', '303333', '310000', '310001', '310002', '310003', '310010', '310011', '310012', '310013', '310020', '310021', '310022', '310023', '310030', '310031', '310032', '310033', '310100', '310101', '310102', '310103', '310110', '310111', '310112', '310113', '310120', '310121', '310122', '310123', '310130', '310131', '310132', '310133', '310200', '310201', '310202', '310203', '310210', '310211', '310212', '310213', '310220', '310221', '310222', '310223', '310230', '310231', '310232', '310233', '310300', '310301', '310302', '310303', '310310', '310311', '310312', '310313', '310320', '310321', '310322', '310323', '310330', '310331', '310332', '310333', '311000', '311001', '311002', '311003', '311010', '311011', '311012', '311013', '311020', '311021', '311022', '311023', '311030', '311031', '311032', '311033', '311100', '311101', '311102', '311103', '311110', '311111', '311112', '311113', '311120', '311121', '311122', '311123', '311130', '311131', '311132', '311133', '311200', '311201', '311202', '311203', '311210', '311211', '311212', '311213', '311220', '311221', '311222', '311223', '311230', '311231', '311232', '311233', '311300', '311301', '311302', '311303', '311310', '311311', '311312', '311313', '311320', '311321', '311322', '311323', '311330', '311331', '311332', '311333', '312000', '312001', '312002', '312003', '312010', '312011', '312012', '312013', '312020', '312021', '312022', '312023', '312030', '312031', '312032', '312033', '312100', '312101', '312102', '312103', '312110', '312111', '312112', '312113', '312120', '312121', '312122', '312123', '312130', '312131', '312132', '312133', '312200', '312201', '312202', '312203', '312210', '312211', '312212', '312213', '312220', '312221', '312222', '312223', '312230', '312231', '312232', '312233', '312300', '312301', '312302', '312303', '312310', '312311', '312312', '312313', '312320', '312321', '312322', '312323', '312330', '312331', '312332', '312333', '313000', '313001', '313002', '313003', '313010', '313011', '313012', '313013', '313020', '313021', '313022', '313023', '313030', '313031', '313032', '313033', '313100', '313101', '313102', '313103', '313110', '313111', '313112', '313113', '313120', '313121', '313122', '313123', '313130', '313131', '313132', '313133', '313200', '313201', '313202', '313203', '313210', '313211', '313212', '313213', '313220', '313221', '313222', '313223', '313230', '313231', '313232', '313233', '313300', '313301', '313302', '313303', '313310', '313311', '313312', '313313', '313320', '313321', '313322', '313323', '313330', '313331', '313332', '313333', '320000', '320001', '320002', '320003', '320010', '320011', '320012', '320013', '320020', '320021', '320022', '320023', '320030', '320031', '320032', '320033', '320100', '320101', '320102', '320103', '320110', '320111', '320112', '320113', '320120', '320121', '320122', '320123', '320130', '320131', '320132', '320133', '320200', '320201', '320202', '320203', '320210', '320211', '320212', '320213', '320220', '320221', '320222', '320223', '320230', '320231', '320232', '320233', '320300', '320301', '320302', '320303', '320310', '320311', '320312', '320313', '320320', '320321', '320322', '320323', '320330', '320331', '320332', '320333', '321000', '321001', '321002', '321003', '321010', '321011', '321012', '321013', '321020', '321021', '321022', '321023', '321030', '321031', '321032', '321033', '321100', '321101', '321102', '321103', '321110', '321111', '321112', '321113', '321120', '321121', '321122', '321123', '321130', '321131', '321132', '321133', '321200', '321201', '321202', '321203', '321210', '321211', '321212', '321213', '321220', '321221', '321222', '321223', '321230', '321231', '321232', '321233', '321300', '321301', '321302', '321303', '321310', '321311', '321312', '321313', '321320', '321321', '321322', '321323', '321330', '321331', '321332', '321333', '322000', '322001', '322002', '322003', '322010', '322011', '322012', '322013', '322020', '322021', '322022', '322023', '322030', '322031', '322032', '322033', '322100', '322101', '322102', '322103', '322110', '322111', '322112', '322113', '322120', '322121', '322122', '322123', '322130', '322131', '322132', '322133', '322200', '322201', '322202', '322203', '322210', '322211', '322212', '322213', '322220', '322221', '322222', '322223', '322230', '322231', '322232', '322233', '322300', '322301', '322302', '322303', '322310', '322311', '322312', '322313', '322320', '322321', '322322', '322323', '322330', '322331', '322332', '322333', '323000', '323001', '323002', '323003', '323010', '323011', '323012', '323013', '323020', '323021', '323022', '323023', '323030', '323031', '323032', '323033', '323100', '323101', '323102', '323103', '323110', '323111', '323112', '323113', '323120', '323121', '323122', '323123', '323130', '323131', '323132', '323133', '323200', '323201', '323202', '323203', '323210', '323211', '323212', '323213', '323220', '323221', '323222', '323223', '323230', '323231', '323232', '323233', '323300', '323301', '323302', '323303', '323310', '323311', '323312', '323313', '323320', '323321', '323322', '323323', '323330', '323331', '323332', '323333', '330000', '330001', '330002', '330003', '330010', '330011', '330012', '330013', '330020', '330021', '330022', '330023', '330030', '330031', '330032', '330033', '330100', '330101', '330102', '330103', '330110', '330111', '330112', '330113', '330120', '330121', '330122', '330123', '330130', '330131', '330132', '330133', '330200', '330201', '330202', '330203', '330210', '330211', '330212', '330213', '330220', '330221', '330222', '330223', '330230', '330231', '330232', '330233', '330300', '330301', '330302', '330303', '330310', '330311', '330312', '330313', '330320', '330321', '330322', '330323', '330330', '330331', '330332', '330333', '331000', '331001', '331002', '331003', '331010', '331011', '331012', '331013', '331020', '331021', '331022', '331023', '331030', '331031', '331032', '331033', '331100', '331101', '331102', '331103', '331110', '331111', '331112', '331113', '331120', '331121', '331122', '331123', '331130', '331131', '331132', '331133', '331200', '331201', '331202', '331203', '331210', '331211', '331212', '331213', '331220', '331221', '331222', '331223', '331230', '331231', '331232', '331233', '331300', '331301', '331302', '331303', '331310', '331311', '331312', '331313', '331320', '331321', '331322', '331323', '331330', '331331', '331332', '331333', '332000', '332001', '332002', '332003', '332010', '332011', '332012', '332013', '332020', '332021', '332022', '332023', '332030', '332031', '332032', '332033', '332100', '332101', '332102', '332103', '332110', '332111', '332112', '332113', '332120', '332121', '332122', '332123', '332130', '332131', '332132', '332133', '332200', '332201', '332202', '332203', '332210', '332211', '332212', '332213', '332220', '332221', '332222', '332223', '332230', '332231', '332232', '332233', '332300', '332301', '332302', '332303', '332310', '332311', '332312', '332313', '332320', '332321', '332322', '332323', '332330', '332331', '332332', '332333', '333000', '333001', '333002', '333003', '333010', '333011', '333012', '333013', '333020', '333021', '333022', '333023', '333030', '333031', '333032', '333033', '333100', '333101', '333102', '333103', '333110', '333111', '333112', '333113', '333120', '333121', '333122', '333123', '333130', '333131', '333132', '333133', '333200', '333201', '333202', '333203', '333210', '333211', '333212', '333213', '333220', '333221', '333222', '333223', '333230', '333231', '333232', '333233', '333300', '333301', '333302', '333303', '333310', '333311', '333312', '333313', '333320', '333321', '333322', '333323', '333330', '333331', '333332', '333333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dff48b516e44934bcde1d13fa6dca57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4096), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 84\n",
      "Number of monomials in a polynomial with 6 variables and degree 3: 84\n",
      "Sparsity: 84\n",
      "['000000', '000001', '000002', '000003', '000010', '000011', '000012', '000020', '000021', '000030', '000100', '000101', '000102', '000110', '000111', '000120', '000200', '000201', '000210', '000300', '001000', '001001', '001002', '001010', '001011', '001020', '001100', '001101', '001110', '001200', '002000', '002001', '002010', '002100', '003000', '010000', '010001', '010002', '010010', '010011', '010020', '010100', '010101', '010110', '010200', '011000', '011001', '011010', '011100', '012000', '020000', '020001', '020010', '020100', '021000', '030000', '100000', '100001', '100002', '100010', '100011', '100020', '100100', '100101', '100110', '100200', '101000', '101001', '101010', '101100', '102000', '110000', '110001', '110010', '110100', '111000', '120000', '200000', '200001', '200010', '200100', '201000', '210000', '300000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:42:14.287869Z",
     "start_time": "2021-01-17T08:40:19.338723Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:42:14.358763Z",
     "start_time": "2021-01-17T08:42:14.326398Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        \n",
    "        for i in epochs_save_range:\n",
    "            index = i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:42:14.389177Z",
     "start_time": "2021-01-17T08:42:14.360418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.280</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.600</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5\n",
       "0 -0.480 -0.320 -0.010  0.420 -0.960  0.740\n",
       "1  0.300  0.380  0.830 -0.470 -0.980 -0.360\n",
       "2  0.280  0.740  0.770 -0.130  0.050 -0.780\n",
       "3  0.600 -0.480 -0.100  0.430  0.390  0.610\n",
       "4 -0.010  0.130 -0.470 -0.060  0.380 -0.180"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:42:14.398459Z",
     "start_time": "2021-01-17T08:42:14.390524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>0.470</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.690</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5\n",
       "0 -0.310  0.680  0.800  0.640 -0.320  0.410\n",
       "1 -0.560  0.370 -0.850 -0.380  0.210  0.270\n",
       "2 -0.400 -0.610  0.470 -0.100  0.330 -0.570\n",
       "3  0.200  0.440 -0.820 -0.030  0.050  0.590\n",
       "4 -0.690 -0.280 -0.170  0.530  0.610  0.710"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:42:14.404189Z",
     "start_time": "2021-01-17T08:42:14.400098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000000   -6.352\n",
       "000001   -9.181\n",
       "000002   -0.988\n",
       "000003   -1.976\n",
       "000010   -2.686\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T08:42:14.410732Z",
     "start_time": "2021-01-17T08:42:14.406847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000000   -8.949\n",
       "000001    0.336\n",
       "000002    3.145\n",
       "000003   -1.227\n",
       "000010   -7.832\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.872Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lambda_net(identifier, \n",
    "                        X_data_real_lambda, \n",
    "                        y_data_real_lambda, \n",
    "                        y_data_pred_lambda, \n",
    "                        y_data_pred_lambda_poly_lstsq, \n",
    "                        y_data_real_lambda_poly_lstsq):\n",
    "    \n",
    "    mae_real_VS_predLambda = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    mae_predLambda_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_realPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    rmse_real_VS_predLambda = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    rmse_predLambda_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_realPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    mape_real_VS_predLambda = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    mape_predLambda_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_realPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)            \n",
    "\n",
    "    r2_real_VS_predLambda = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_predPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    r2_predLambda_VS_predPolyLstsq = np.round(r2_score(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_realPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    raae_real_VS_predLambda = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    raae_predLambda_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_realPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    rmae_real_VS_predLambda = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    rmae_predLambda_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_realPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "        \n",
    "    std_data_real_lambda = np.round(np.std(y_data_real_lambda), 4) \n",
    "    std_data_pred_lambda = np.round(np.std(y_data_pred_lambda), 4) \n",
    "    std_data_pred_lambda_poly_lstsq = np.round(np.std(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    std_data_real_lambda_poly_lstsq = np.round(np.std(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    mean_data_real_lambda = np.round(np.mean(y_data_real_lambda), 4) \n",
    "    mean_data_pred_lambda = np.round(np.mean(y_data_pred_lambda), 4) \n",
    "    mean_data_pred_lambda_poly_lstsq = np.round(np.mean(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    mean_data_real_lambda_poly_lstsq = np.round(np.mean(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    return [{\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mae_real_VS_predLambda,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_real_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_predLambda_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mae_real_VS_realPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmse_real_VS_predLambda,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_real_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_predLambda_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmse_real_VS_realPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mape_real_VS_predLambda,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_real_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_predLambda_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mape_real_VS_realPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': r2_real_VS_predLambda,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_real_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_predLambda_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': r2_real_VS_realPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': raae_real_VS_predLambda,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_real_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_predLambda_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': raae_real_VS_realPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmae_real_VS_predLambda,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_real_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_predLambda_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmae_real_VS_realPolyLstsq,\n",
    "            },\n",
    "            {\n",
    "             'STD FV ' + identifier + ' REAL LAMBDA': std_data_real_lambda,\n",
    "             'STD FV ' + identifier + ' PRED LAMBDA': std_data_pred_lambda, \n",
    "             'STD FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': std_data_pred_lambda_poly_lstsq, \n",
    "             'STD FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': std_data_real_lambda_poly_lstsq, \n",
    "            },\n",
    "            {\n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA': mean_data_real_lambda,\n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA': mean_data_pred_lambda, \n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': mean_data_pred_lambda_poly_lstsq, \n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': mean_data_real_lambda_poly_lstsq, \n",
    "            }]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.876Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_function_values_from_polynomial(X_data, polynomial):\n",
    "    function_value_list = []\n",
    "    for entry in X_data:\n",
    "        function_value, _ = calcualate_function_value_with_X_data_entry(polynomial, entry)\n",
    "        function_value_list.append(function_value)\n",
    "    function_value_array = np.array(function_value_list).reshape(len(function_value_list), 1)     \n",
    "\n",
    "    return function_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.878Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_term_matric_for_lstsq(X_data, polynomial_indices):\n",
    "    term_list_all = []\n",
    "    y = 0\n",
    "    for term in list(polynomial_indices):\n",
    "        term_list = [int(value_mult) for value_mult in term]\n",
    "        term_list_all.append(term_list)\n",
    "    terms_matrix = []\n",
    "    for unknowns in X_data:\n",
    "        terms = []\n",
    "        for term_multipliers in term_list_all:\n",
    "            term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "            terms.append(term_value)\n",
    "        terms_matrix.append(np.array(terms))\n",
    "    terms_matrix = np.array(terms_matrix)\n",
    "    \n",
    "    return terms_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.880Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn(lambda_index, X_data_lambda, y_data_real_lambda, polynomial, seed_list, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    current_seed = None\n",
    "    if fixed_seed_lambda_training or fixed_initialization_lambda_training:\n",
    "        current_seed = seed_list[lambda_index%number_different_lambda_trainings]\n",
    "    \n",
    "    if fixed_seed_lambda_training:\n",
    "        random.seed(current_seed)\n",
    "        np.random.seed(current_seed)\n",
    "        if int(tf.__version__[0]) >= 2:\n",
    "            tf.random.set_seed(current_seed)\n",
    "        else:\n",
    "            tf.set_random_seed(current_seed) \n",
    "        \n",
    "    if isinstance(X_data_lambda, pd.DataFrame):\n",
    "        X_data_lambda = X_data_lambda.values\n",
    "    if isinstance(y_data_real_lambda, pd.DataFrame):\n",
    "        y_data_real_lambda = y_data_real_lambda.values\n",
    "                \n",
    "    X_train_lambda_with_valid, X_test_lambda, y_train_real_lambda_with_valid, y_test_real_lambda = train_test_split(X_data_lambda, y_data_real_lambda, test_size=0.25, random_state=current_seed)           \n",
    "    X_train_lambda, X_valid_lambda, y_train_real_lambda, y_valid_real_lambda = train_test_split(X_train_lambda_with_valid, y_train_real_lambda_with_valid, test_size=0.25, random_state=current_seed)           \n",
    "     \n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    #kerase defaults: kernel_initializer='glorot_uniform', bias_initializer='zeros'               \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros')) #1024\n",
    "    else:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1])) #1024\n",
    "        \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        if fixed_initialization_lambda_training:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "    \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=[r2_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_lstsq_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_train_pred_lambda = model.predict(X_train_lambda) \n",
    "        y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "        y_test_pred_lambda = model.predict(X_test_lambda)\n",
    "    \n",
    "        terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                \n",
    "        polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "        y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "        y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "        y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)    \n",
    "        y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)\n",
    "        y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)  \n",
    "        \n",
    "        pred_list = (lambda_index,\n",
    "                     y_train_real_lambda, \n",
    "                     y_train_pred_lambda, \n",
    "                     y_train_pred_lambda_poly_lstsq,\n",
    "                     #y_train_real_lambda_poly_lstsq,\n",
    "                     X_train_lambda, \n",
    "                     y_valid_real_lambda,\n",
    "                     y_valid_pred_lambda, \n",
    "                     y_valid_pred_lambda_poly_lstsq,\n",
    "                     #y_valid_real_lambda_poly_lstsq,\n",
    "                     X_valid_lambda, \n",
    "                     y_test_real_lambda, \n",
    "                     y_test_pred_lambda, \n",
    "                     y_test_pred_lambda_poly_lstsq, \n",
    "                     #y_test_real_lambda_poly_lstsq,\n",
    "                     X_test_lambda)\n",
    "\n",
    "        scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "        scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "        scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "        scores_std = {}\n",
    "        for aDict in (std_train, std_valid, std_test):\n",
    "            scores_std.update(aDict)      \n",
    "        scores_mean = {}\n",
    "        for aDict in (mean_train, mean_valid, mean_test):\n",
    "            scores_mean.update(aDict)\n",
    "        \n",
    "        scores_list = [lambda_index,\n",
    "                     scores_train,\n",
    "                     scores_valid,\n",
    "                     scores_test,\n",
    "                     scores_std,\n",
    "                     scores_mean]            \n",
    "                            \n",
    "    else:\n",
    "        scores_list = []\n",
    "        pred_list = []\n",
    "        for i in epochs_save_range:\n",
    "            train_epochs_step = each_epochs_save if i > 1 else max(each_epochs_save-1, 1) if i==1 else 1\n",
    "            \n",
    "            model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=train_epochs_step, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=1,\n",
    "                      use_multiprocessing=False)\n",
    "            \n",
    "            #history adjustment for continuing training\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                history = model_history.history\n",
    "            else:\n",
    "                history = mergeDict(history, model_history.history)\n",
    "\n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_train_pred_lambda = model.predict(X_train_lambda)                \n",
    "            y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "            y_test_pred_lambda = model.predict(X_test_lambda)        \n",
    "\n",
    "            terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                        \n",
    "            polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            \n",
    "            y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "            y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "            y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)           \n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "                y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)  \n",
    "                y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)                    \n",
    "                \n",
    "            pred_list.append((lambda_index,\n",
    "                              y_train_real_lambda, \n",
    "                              y_train_pred_lambda, \n",
    "                              y_train_pred_lambda_poly_lstsq,\n",
    "                              #y_train_real_lambda_poly_lstsq,\n",
    "                              X_train_lambda, \n",
    "                              y_valid_real_lambda,\n",
    "                              y_valid_pred_lambda, \n",
    "                              y_valid_pred_lambda_poly_lstsq,\n",
    "                              #y_valid_real_lambda_poly_lstsq,\n",
    "                              X_valid_lambda, \n",
    "                              y_test_real_lambda, \n",
    "                              y_test_pred_lambda, \n",
    "                              y_test_pred_lambda_poly_lstsq, \n",
    "                              #y_test_real_lambda_poly_lstsq,\n",
    "                              X_test_lambda))\n",
    "    \n",
    "            scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "            scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "            scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "            scores_std = {}\n",
    "            for aDict in (std_train, std_valid, std_test):\n",
    "                scores_std.update(aDict)\n",
    "            scores_mean = {}\n",
    "            for aDict in (mean_train, mean_valid, mean_test):\n",
    "                scores_mean.update(aDict)\n",
    "\n",
    "            scores_list_single_epoch =  [lambda_index,\n",
    "                                         scores_train,\n",
    "                                          scores_valid,\n",
    "                                          scores_test,\n",
    "                                          scores_std,\n",
    "                                          scores_mean]        \n",
    "                  \n",
    "            scores_list.append(scores_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_lstsq_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_lstsq_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save == None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                text_file.write(str(lambda_index))\n",
    "                text_file.write(', ' + str(current_seed))\n",
    "                for i, value in enumerate(polynomial.values): \n",
    "                    text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_lstsq_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "        text_file.close() \n",
    "\n",
    "        directory = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "        path = directory + 'lambda_' + str(lambda_index) + '_test_data'\n",
    "        np.save(path, X_test_lambda)\n",
    "            \n",
    "    if return_model:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, #polynomial_lstsq_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368673a15e2846f6bb46e58e83a40c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.9min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.6min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.6min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.4min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.7min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.5min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.5min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.7min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.6min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.6min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.8min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "/home/smarton/anaconda3/envs/masterthesos/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.6min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.4min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.2min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.4min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.1min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 25.8min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.3min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 54.9min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 25.7min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.1min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 54.8min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 25.7min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.0min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 54.7min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 25.8min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.0min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 54.9min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.3min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.1min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.7min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.3min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.4min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.2min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.5min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.3min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.6min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.3min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.6min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.4min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.6min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.4min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.7min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.7min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.6min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.5min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.5min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.3min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.5min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.3min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.3min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.5min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.4min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 41.7min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.5min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 42.9min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 57.1min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 43.2min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 57.4min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.4min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 57.6min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 57.8min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 63.0min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.3min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 42.5min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 56.5min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 42.7min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 57.1min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 43.1min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 57.1min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 26.4min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 42.9min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 57.0min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-3)]: Done 452 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=-3)]: Done 740 tasks      | elapsed: 42.0min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 55.8min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  68 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-3)]: Done 228 tasks      | elapsed: 13.3min\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "max_seed = 2147483646\n",
    "seed_list = random.sample(range(0, max_seed), number_different_lambda_trainings)\n",
    "chunk_multiplier = 0\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(chunksize*chunk_multiplier+index, X_data[1].values, y_data[1].values, X_data[0], seed_list, return_history=True, each_epochs_save=each_epochs_save, printing=True) for index, (X_data, y_data) in enumerate(zip(X_data_list_split, y_data_list_split)))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "    chunk_multiplier +=1\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(rand_index, X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], seed_list, callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.884Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    scores_list_train = [clf[1][1] for clf in clf_list]\n",
    "    scores_list_valid = [clf[1][2] for clf in clf_list]\n",
    "    scores_list_test = [clf[1][3] for clf in clf_list]\n",
    "    scores_list_stds = [clf[1][4] for clf in clf_list]\n",
    "    scores_list_means = [clf[1][5] for clf in clf_list]\n",
    "\n",
    "    scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "\n",
    "    scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "\n",
    "    scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_list_by_epochs = [[] for i in epochs_save_range]\n",
    "    for scores_list in scores_list:   \n",
    "        for index, scores in enumerate(scores_list):\n",
    "            scores_list_by_epochs[index].append(scores)\n",
    "            \n",
    "        \n",
    "    for i, scores_list_single_epoch in enumerate(scores_list_by_epochs):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "           \n",
    "        scores_list_train = [scores_list[1] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_valid = [scores_list[2] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_test = [scores_list[3] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_stds = [scores_list[4] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_means = [scores_list[5] for scores_list in scores_list_single_epoch]\n",
    "        \n",
    "        scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()  \n",
    "        scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()  \n",
    "        scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.887Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.889Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.892Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.894Z"
    }
   },
   "outputs": [],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.896Z"
    }
   },
   "outputs": [],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Net Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.898Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.900Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.903Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.907Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.910Z"
    }
   },
   "outputs": [],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.912Z"
    }
   },
   "outputs": [],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.914Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T08:39:34.916Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['trainover', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
