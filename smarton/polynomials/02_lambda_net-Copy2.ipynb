{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:04.303027Z",
     "start_time": "2020-12-14T08:46:04.294737Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:04.372000Z",
     "start_time": "2020-12-14T08:46:04.306963Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 10000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 10  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "\n",
    "fixed_seed_lambda_training = False\n",
    "fixed_initialization_lambda_training = True\n",
    "number_different_lambda_trainings = 50\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:04.402216Z",
     "start_time": "2020-12-14T08:46:04.375962Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if each_epochs_save != None:\n",
    "    epochs_save_range = range(1, epochs//each_epochs_save+1) if each_epochs_save == 1 else range(epochs//each_epochs_save+1)\n",
    "else:\n",
    "    epochs_save_range = None\n",
    "    \n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_' + str(number_different_lambda_trainings) + '-FixedSeed'\n",
    "else:\n",
    "    seed_shuffle_string = '_NoFixedSeed'\n",
    "    \n",
    "if fixed_initialization_lambda_training:\n",
    "    seed_shuffle_string += '_' + str(number_different_lambda_trainings) + '-FixedEvaluation'\n",
    "else:\n",
    "    seed_shuffle_string += '_NoFixedEvaluation'\n",
    "    \n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:19.241616Z",
     "start_time": "2020-12-14T08:46:04.407019Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:19.255045Z",
     "start_time": "2020-12-14T08:46:19.246806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:19.284932Z",
     "start_time": "2020-12-14T08:46:19.258797Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "    \n",
    "    \n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:19.302650Z",
     "start_time": "2020-12-14T08:46:19.287798Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcualate_function_value_with_X_data_entry(coefficient_list, X_data_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "     \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [X_data_value**int(coefficient_multiplier) for coefficient_multiplier, X_data_value in zip(coefficient_multipliers, X_data_entry)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "        \n",
    "    return result, np.append(X_data_entry, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:19.357076Z",
     "start_time": "2020-12-14T08:46:19.305496Z"
    },
    "code_folding": [
     0,
     20,
     43,
     66,
     88,
     91,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:19.473979Z",
     "start_time": "2020-12-14T08:46:19.360203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85a42f0df8648b9b110c09997dfdc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dee054cd8d438eb49d2b0be5c7d8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:31.908395Z",
     "start_time": "2020-12-14T08:46:19.477309Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:36.568167Z",
     "start_time": "2020-12-14T08:46:31.911047Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        \n",
    "        for i in epochs_save_range:\n",
    "            index = i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:36.600006Z",
     "start_time": "2020-12-14T08:46:36.572481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.620 -0.950  0.340 -0.760\n",
       "1 -0.300  0.950 -0.770  0.310\n",
       "2 -0.300  0.280  0.480  0.080\n",
       "3 -0.520  0.980  0.230  0.170\n",
       "4  0.710 -0.920 -0.310  0.260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:36.624014Z",
     "start_time": "2020-12-14T08:46:36.604773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1      2      3\n",
       "0  0.660 0.100 -0.040 -1.000\n",
       "1 -0.970 0.920  0.460  0.990\n",
       "2 -0.280 0.760  0.910 -0.230\n",
       "3 -0.250 0.210 -0.070 -0.300\n",
       "4 -0.330 0.770 -0.360  0.110"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:36.635385Z",
     "start_time": "2020-12-14T08:46:36.628217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:36.665131Z",
     "start_time": "2020-12-14T08:46:36.637808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:36.728420Z",
     "start_time": "2020-12-14T08:46:36.668553Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lambda_net(identifier, \n",
    "                        X_data_real_lambda, \n",
    "                        y_data_real_lambda, \n",
    "                        y_data_pred_lambda, \n",
    "                        y_data_pred_lambda_poly_lstsq, \n",
    "                        y_data_real_lambda_poly_lstsq):\n",
    "    \n",
    "    mae_real_VS_predLambda = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    mae_predLambda_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_realPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    rmse_real_VS_predLambda = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    rmse_predLambda_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_realPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    mape_real_VS_predLambda = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    mape_predLambda_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_realPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)            \n",
    "\n",
    "    r2_real_VS_predLambda = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_predPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    r2_predLambda_VS_predPolyLstsq = np.round(r2_score(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_realPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    raae_real_VS_predLambda = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    raae_predLambda_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_realPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    rmae_real_VS_predLambda = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    rmae_predLambda_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_realPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    fd_real_VS_predLambda = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_predLambda_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_realPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "    dtw_real_VS_predLambda, dtw_complete_real_VS_predLambda = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predLambda = np.round(dtw_real_VS_predLambda, 4)\n",
    "    dtw_real_VS_predPolyLstsq, dtw_complete_real_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predPolyLstsq = np.round(dtw_real_VS_predPolyLstsq, 4)\n",
    "    dtw_predLambda_VS_predPolyLstsq, dtw_complete_predLambda_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_predLambda_VS_predPolyLstsq = np.round(dtw_predLambda_VS_predPolyLstsq, 4)    \n",
    "    dtw_real_VS_realPolyLstsq, dtw_complete_real_VS_realPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_realPolyLstsq = np.round(dtw_real_VS_realPolyLstsq, 4) \n",
    "        \n",
    "    std_data_real_lambda = np.round(np.std(y_data_real_lambda), 4) \n",
    "    std_data_pred_lambda = np.round(np.std(y_data_pred_lambda), 4) \n",
    "    std_data_pred_lambda_poly_lstsq = np.round(np.std(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    std_data_real_lambda_poly_lstsq = np.round(np.std(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    mean_data_real_lambda = np.round(np.mean(y_data_real_lambda), 4) \n",
    "    mean_data_pred_lambda = np.round(np.mean(y_data_pred_lambda), 4) \n",
    "    mean_data_pred_lambda_poly_lstsq = np.round(np.mean(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    mean_data_real_lambda_poly_lstsq = np.round(np.mean(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    return [{\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mae_real_VS_predLambda,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_real_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_predLambda_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mae_real_VS_realPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmse_real_VS_predLambda,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_real_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_predLambda_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmse_real_VS_realPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mape_real_VS_predLambda,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_real_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_predLambda_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mape_real_VS_realPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': r2_real_VS_predLambda,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_real_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_predLambda_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': r2_real_VS_realPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': raae_real_VS_predLambda,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_real_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_predLambda_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': raae_real_VS_realPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmae_real_VS_predLambda,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_real_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_predLambda_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmae_real_VS_realPolyLstsq,\n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': fd_real_VS_predLambda,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_real_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_predLambda_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': fd_real_VS_realPolyLstsq,   \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': dtw_real_VS_predLambda, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_real_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_predLambda_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': dtw_real_VS_realPolyLstsq, \n",
    "            },\n",
    "            {\n",
    "             'STD FV ' + identifier + ' REAL LAMBDA': std_data_real_lambda,\n",
    "             'STD FV ' + identifier + ' PRED LAMBDA': std_data_pred_lambda, \n",
    "             'STD FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': std_data_pred_lambda_poly_lstsq, \n",
    "             'STD FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': std_data_real_lambda_poly_lstsq, \n",
    "            },\n",
    "            {\n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA': mean_data_real_lambda,\n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA': mean_data_pred_lambda, \n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': mean_data_pred_lambda_poly_lstsq, \n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': mean_data_real_lambda_poly_lstsq, \n",
    "            }]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:36.745963Z",
     "start_time": "2020-12-14T08:46:36.731461Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_function_values_from_polynomial(X_data, polynomial):\n",
    "    function_value_list = []\n",
    "    for entry in X_data:\n",
    "        function_value, _ = calcualate_function_value_with_X_data_entry(polynomial, entry)\n",
    "        function_value_list.append(function_value)\n",
    "    function_value_array = np.array(function_value_list).reshape(len(function_value_list), 1)     \n",
    "\n",
    "    return function_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:36.765474Z",
     "start_time": "2020-12-14T08:46:36.749245Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_term_matric_for_lstsq(X_data, polynomial_indices):\n",
    "    term_list_all = []\n",
    "    y = 0\n",
    "    for term in list(polynomial_indices):\n",
    "        term_list = [int(value_mult) for value_mult in term]\n",
    "        term_list_all.append(term_list)\n",
    "    terms_matrix = []\n",
    "    for unknowns in X_data:\n",
    "        terms = []\n",
    "        for term_multipliers in term_list_all:\n",
    "            term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "            terms.append(term_value)\n",
    "        terms_matrix.append(np.array(terms))\n",
    "    terms_matrix = np.array(terms_matrix)\n",
    "    \n",
    "    return terms_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T08:46:36.863145Z",
     "start_time": "2020-12-14T08:46:36.769195Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn(lambda_index, X_data_lambda, y_data_real_lambda, polynomial, seed_list, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    current_seed = seed_list[lambda_index%number_different_lambda_trainings]\n",
    "    \n",
    "    if fixed_seed_lambda_training:\n",
    "        random.seed(current_seed)\n",
    "        np.random.seed(current_seed)\n",
    "        if int(tf.__version__[0]) >= 2:\n",
    "            tf.random.set_seed(current_seed)\n",
    "        else:\n",
    "            tf.set_random_seed(current_seed) \n",
    "        \n",
    "    if isinstance(X_data_lambda, pd.DataFrame):\n",
    "        X_data_lambda = X_data_lambda.values\n",
    "    if isinstance(y_data_real_lambda, pd.DataFrame):\n",
    "        y_data_real_lambda = y_data_real_lambda.values\n",
    "                \n",
    "    X_train_lambda_with_valid, X_test_lambda, y_train_real_lambda_with_valid, y_test_real_lambda = train_test_split(X_data_lambda, y_data_real_lambda, test_size=0.25, random_state=current_seed)           \n",
    "    X_train_lambda, X_valid_lambda, y_train_real_lambda, y_valid_real_lambda = train_test_split(X_train_lambda_with_valid, y_train_real_lambda_with_valid, test_size=0.25, random_state=current_seed)           \n",
    "     \n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    #kerase defaults: kernel_initializer='glorot_uniform', bias_initializer='zeros'               \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros')) #1024\n",
    "    else:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1])) #1024\n",
    "        \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        if fixed_initialization_lambda_training:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "    \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_lstsq_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_train_pred_lambda = model.predict(X_train_lambda) \n",
    "        y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "        y_test_pred_lambda = model.predict(X_test_lambda)\n",
    "    \n",
    "        terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                \n",
    "        polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "        y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "        y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "        y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)    \n",
    "        y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)\n",
    "        y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)  \n",
    "        \n",
    "        pred_list = (lambda_index,\n",
    "                     y_train_real_lambda, \n",
    "                     y_train_pred_lambda, \n",
    "                     y_train_pred_lambda_poly_lstsq,\n",
    "                     #y_train_real_lambda_poly_lstsq,\n",
    "                     X_train_lambda, \n",
    "                     y_valid_real_lambda,\n",
    "                     y_valid_pred_lambda, \n",
    "                     y_valid_pred_lambda_poly_lstsq,\n",
    "                     #y_valid_real_lambda_poly_lstsq,\n",
    "                     X_valid_lambda, \n",
    "                     y_test_real_lambda, \n",
    "                     y_test_pred_lambda, \n",
    "                     y_test_pred_lambda_poly_lstsq, \n",
    "                     #y_test_real_lambda_poly_lstsq,\n",
    "                     X_test_lambda)\n",
    "\n",
    "        scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "        scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "        scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "        scores_std = {}\n",
    "        for aDict in (std_train, std_valid, std_test):\n",
    "            scores_std.update(aDict)      \n",
    "        scores_mean = {}\n",
    "        for aDict in (mean_train, mean_valid, mean_test):\n",
    "            scores_mean.update(aDict)\n",
    "        \n",
    "        scores_list = [lambda_index,\n",
    "                     scores_train,\n",
    "                     scores_valid,\n",
    "                     scores_test,\n",
    "                     scores_std,\n",
    "                     scores_mean]            \n",
    "                            \n",
    "    else:\n",
    "        scores_list = []\n",
    "        pred_list = []\n",
    "        for i in epochs_save_range:\n",
    "            train_epochs_step = each_epochs_save if i > 1 else max(each_epochs_save-1, 1) if i==1 else 1\n",
    "            \n",
    "            model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=train_epochs_step, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=1,\n",
    "                      use_multiprocessing=False)\n",
    "            \n",
    "            #history adjustment for continuing training\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                history = model_history.history\n",
    "            else:\n",
    "                history = mergeDict(history, model_history.history)\n",
    "                #for key_1 in history.keys():\n",
    "                #    for key_2 in model_history.history.keys():\n",
    "                #        if key_1 == key_2:\n",
    "                #            history[key_1] += model_history.history[key_2]  \n",
    "\n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_train_pred_lambda = model.predict(X_train_lambda)                \n",
    "            y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "            y_test_pred_lambda = model.predict(X_test_lambda)        \n",
    "\n",
    "            terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                        \n",
    "            polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            \n",
    "            y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "            y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "            y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)           \n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "                y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)  \n",
    "                y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)                    \n",
    "                \n",
    "            pred_list.append((lambda_index,\n",
    "                              y_train_real_lambda, \n",
    "                              y_train_pred_lambda, \n",
    "                              y_train_pred_lambda_poly_lstsq,\n",
    "                              #y_train_real_lambda_poly_lstsq,\n",
    "                              X_train_lambda, \n",
    "                              y_valid_real_lambda,\n",
    "                              y_valid_pred_lambda, \n",
    "                              y_valid_pred_lambda_poly_lstsq,\n",
    "                              #y_valid_real_lambda_poly_lstsq,\n",
    "                              X_valid_lambda, \n",
    "                              y_test_real_lambda, \n",
    "                              y_test_pred_lambda, \n",
    "                              y_test_pred_lambda_poly_lstsq, \n",
    "                              #y_test_real_lambda_poly_lstsq,\n",
    "                              X_test_lambda))\n",
    "    \n",
    "            scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "            scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "            scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "            scores_std = {}\n",
    "            for aDict in (std_train, std_valid, std_test):\n",
    "                scores_std.update(aDict)\n",
    "            scores_mean = {}\n",
    "            for aDict in (mean_train, mean_valid, mean_test):\n",
    "                scores_mean.update(aDict)\n",
    "\n",
    "            scores_list_single_epoch =  [lambda_index,\n",
    "                                         scores_train,\n",
    "                                          scores_valid,\n",
    "                                          scores_test,\n",
    "                                          scores_std,\n",
    "                                          scores_mean]        \n",
    "                  \n",
    "            scores_list.append(scores_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_lstsq_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_lstsq_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save == None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                text_file.write(str(lambda_index))\n",
    "                text_file.write(', ' + str(current_seed))\n",
    "                for i, value in enumerate(polynomial.values): \n",
    "                    text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_lstsq_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, #polynomial_lstsq_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:47:34.144066Z",
     "start_time": "2020-12-14T08:46:36.866264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAANYCAYAAADZn0yoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3Xd8XGeV+P/PzJ0+6s2We3/cYsdJnMKmkR4IIYQOoS11ly+QLz+WBZa28IXd7C6w1IWFQAokEHpIgAAhjRTHSWzHduLruMmWLVu9TS/398e9dzQjaaQZaSR5pPN+vfKKNHPn3jMjWZqjc57zOAzDQAghhBBCCCHE5DlnOgAhhBBCCCGEmC0kwRJCCCGEEEKIEpEESwghhBBCCCFKRBIsIYQQQgghhCgRSbCEEEIIIYQQokQkwRJCCCGEEEKIEpEESwghhBBzklJqmVLKUEq5SnzeI0qpK0p5TjE7KaXeqZT620zHIUpLEiwhSkB+mQohhJiIufT7Qyl1qVKqdabjEGKqSYIlhBBCCCEAUEo5lFLy/tCilNIKuW2cc5S0QjpT1xCFky+GEFNIKfVe4J+BOuBvwAd0XT+hlHIAXwXeCviAFuDNuq7vUUq9AvgvYDHQD3xN1/X/mpEnIIQQBVBKHQG+DbwNWAn8FPgUcBtwIbANeL2u6z1KqfMxf/6tx/zZ9xFd1x+2zvMu4OPAIqADuEXX9e9Z910K/Bj4GubP1RTwKV3XfzRObK8E/p8VVx9wq67rnx922N8rpT4POICv2D9zlVLnAt8B1gAR4Ce6rn/Uuu964N+AhcBO4B90XX9xlOvfBrTquv7p7Oeh6/oipdSdwBLgd0qpFPAFXdf/Y6zXaIzn+TDwJHA5sBZ4CHiXruvd1v1jve4PA48DlwJnAWcopbqBrwBXA37gEV3Xb7COv856TZcBL2D+bnveuu8I8C3g7cBS4I/AOwAN+APgVUoNWmGvwfxafx1YZ73GvwQ+qut63DrfVcA3gfnAT4ANwJ26rv/Auv/vgX+y7n8aeJ+u6y3jvFZrrXOejfl99hld1++x7rvNimMpcAnwaqXUTaPctt06x7VAGPg+8GVd19NKqXcC77XieTvwP8Cnx4opK7b/BC4AXqnret9Yz08pZQD/B7gZ8z39cqXU14EbgWrgJeBmXdcfs47P+/0sSkv+QiHEFFFKXYb5y/cNQDPmL7SfWndfBVyM+UOu2jqmy7rvVuD9uq5XAhuBv05j2EIIMVGvBa7E/Ln2Ksw3058CGjHfb3xYKbUQuB/zzXkd8DHgl0qpRusc7cB1QBXwLuBrSqmzsq4xH/Nn5kLg3cC3lVK148QVwnyTWwO8EvgHpdQNw455ObAa82fzP2e17H0d+Lqu61WYCZr9JnwNcDfmG9tG4PeYSZJnnFhy6Lr+NuAo8Cpd1yus5Gq812gsbwf+HvN3ThL4hhVvIed8G/A+oBLz99WdQAAzoWnCTGxRSm0Bfgi8H6gHvgfcq5TyZp3rDcA1wHJgE/BOXddDmMnICeu5Vui6fgIzUf6/QANmYnE58I/WtRqAXwCftK6lAy+zL6KUejXm99iNmF+HxzC/LnkppYLAn4G7rOf1JuA7Sqn1WYe9BfiS9Vr8Lc9t38T8XlyBmXS9HfN71nYecAiYZz1uTEopp1Lq+9brdZWVXBXy/G6wrmXHvx04E/PrfBfwc6WUz7pv1O9nUXpSwRJi6rwV+KGu688BKKU+CfQopZYBCcwf0muBp4f91TMBrFdK7dJ1vQfomd6whRBiQr6p6/opAKXUY0C7rus7rM9/jfnG+Sbg97qu/956zJ+VUs8ArwBu13X9/qzzPaKU+hNwEfCcdVsCs8qTBH5vVUIU8FS+oIZVfp5XSt2N+Yb4N1m3/6uVAOxWSv0IeDPwF+t6q5RSDbqud2Zd543A/bqu/9l6fv8FfATzzX/29SZizNdonMfeqev6HiumzwA7lVLvKPCct+m6vtd6bDNmMlRv/R4CeMT6//uA7+m6vs36/Hal1KeA87OO+YaVPKGU+h3mG/5R6br+bNanR5RS38P8+vy3Fd9eXdd/ZZ3rG5jJoe0DwL/Zv0OVUl8GPqWUWjpGFes64EhW5XOHUuqXwOuBf7Vu+62u649bH0eVUjm3KaUSmInZmbquDwADSqmvYCapt1qPO6Hr+jetj5P5nr/FjZk4uTCT7XgRz+/f7ColgK7rP84671eUUp/G/Deyi/zfz6LEJMESYuosYOhNAbquDyqluoCFuq7/VSn1LcyWmqVKqV8BH9N1vR/zr8CfBv5dKfU88Ald15+cgfiFEKIYp7I+jozyeQVmi9XrlVKvyrrPjdnOhlLqWuBzmFUwJ2YFZXfWsV1WcmULW+fNSyl1HvDvmB0BHsAL/HzYYceyPm4BzrA+fjfwBWCfUuowZiJ2H+bP98wbeKst7BhmZW2yxnyNxjH8ebgxK0OFnDP7sYuB7qzkanh871BKfSjrNg/ma2I7mfVxeNh9Oaxq4FeBczC/3i7ATroWZMel67oxbEjGUuDrVnJjc2B+HfIlWEuB85RSvVm3uTArdrZjjJR9WwPm65d9jRZyv/6jnSOfVcBm4Nys5MqOdbznl3MdpdTHML9vFwAGZjW4wbo73/ezKDFJsISYOicwfzgCmbaEeuA4gK7r3wC+oZRqwizT/xNmH/h2zP5uN2Zv9T2Yv+yEEKLcHcOssrx3+B1Wi9kvMVutfqvrekIp9RvMN5STcRfmmqBrdV2PKqX+m6E3nLbFwD7r4yWYP7/Rdf0l4M3W0IcbgV8opeqt++0kDGtd7WKsn+/DhDATB9v8Yfcbwz7P+xoVIPt3xRLMikVngefMjuMYUKeUqtF1vXfYcceAL+m6Pm7b2zjXsP0PsANzHfKAUupm4HXWfW2Ya7SAzOu8KOuxdiw/KSKGY5jrya4sMs7s2zoxX9ulmGvQwHy9j+c5fjwvYv7B9Q9Kqct0XdezYh3v+WWuo5S6CHMN4+WYlb+0UqoH699Qvu9nq3orSkgSLCFKx53V5wxmuf9updRdmD88vwxs03X9iFJqK+ZfZ5/D/OUbBdJW//7rgfus/ut+ID2tz0IIIabOj4HtSqmrMVvw3JitZQcwB1B4MYcOJK1q1lXAnklesxKzGhO1Fvm/BfjTsGM+o8yhRMsx19HcBGANN3hA1/WOrIpHGvMPX59QSl0OPIrZHhgDnhjl+juB/08p9f8wKz03D7v/FOY6Hlve10jX9fFGnN+klLoDOIJZqfiFrusppVRR59R1vU0p9QfMtUkfBAaBC3RdfxRzmMOvlVJ/wRy6EMAcjvGo1S43llNAvVKqWtf1Puu2SsyBToPW8Il/wPweAHPd2LesNXP3YbbMZSeo3wW+qJTaqev6XqVUNeb6peEVymz3YXaIvI2hddFnAoOjDSkZjfWa3gN8SSn1dsz1Th/FHFA1Ibqu3229B/iLUupSXdcPUvzzq8RsR+wAXEqpT2BWsIAxv59FicmQCyFK5/eYbTD2f5cCn8H8i2wb5oLSN1nHVmH+kurBLPN3Af9p3fc2zD70fsxfJm+dnvCFEGJq6bp+DLAX7ndg/oX+nwCn9eb8w5jJSw9mInRvCS77j8AXlFIDwGcZfWH/I5hJ3oPAf+m6bidg1wB7rbVeXwfepOt6xKow3IQ56KATc6hH9tqZbHdirn85gpnY/WzY/f8GfFop1auU+thYr1EBz/VOzMmNJzEn1H4Yxn7dxzjX2zCrNPswh4/cbJ3rGcwJed/C/DodAN5ZQGzour4P84+Ph6znuwBzTdVbgAHM34s/yzq+E/OPjv+B+XtyPfAMZjKLruu/Bm4Bfmr9ztyDuXZsrBgGMBP3N2FWIk9a5/CO9bhRfAjzD6SHMIde3IU5/GPCdF2/HTMx/qtSatkEnt8DmFMb92O+t4iS20I46vfzZGIWo3MYRjEVTCGEEEIIcbpR5qj1H9vjy2cjq7WtFXirruuFrEkTYkZIi6AQQgghhDgtWW2N2zA7Q/4Jcz2RTL8TpzVJsIQQQghR1pRSe8kaKpTl/UUOQDitqaENeocbsy2uzF2A2X7nwRwoccN4bW3WsIc/jHafrutjTp2cCkqp72Kt6xvmx7quf2C64xFTT1oEhRBCCCGEEKJEZMiFEEIIIYQQQpTItLYIptNpI5WafMVM0xyU4jzTpdziBYl5OpRbvCAxT4dyixcmH7PbrXUCjaWLaPLm6u8rKL+Yyy1ekJinQ7nFC+UXc7nFC9P3+2rcBEsp9UPgOqBd1/WN1m11mGM0l2GOHX1Dnt2+c6RSBr294fEOG1dNTaAk55ku5RYvSMzTodziBYl5OpRbvDD5mBsbK1tKGE5JzNXfV1B+MZdbvCAxT4dyixfKL+Zyixem7/dVIS2Ct2HOzc/2CeBBXddXY+4Z8YmiohNCCCGEEEKIWWjcBMvatbt72M2vBm63Pr4duKHEcQkhhBBCCCFE2ZnoGqx5uq63WR+fBOYV8iBNc1BTE5jgJbPP4yzJeaZLucULEvN0KLd4QWKeDuUWL5RnzEIIIcRUmfSQC13XDaVUQavF5mpPe7nFCxLzdCi3eEFing7lFi+UpKe9hNEIIYQQM2uiY9pPKaWaAaz/t5cuJCGEEEIIIYQoTxNNsO4F3mF9/A7gt6UJRwghhBBCCCHKVyFj2u8GLgUalFKtwOeAfwfuUUq9G2gB3jCVQQohhBBCCCFEORg3wdJ1/c157rq8xLEIIYQQQgghRFmbaIugEEIIIYQQQohhJMESQgghhBBCiBKRBEsIIYQQQgghSkQSLCGEEEIIIYQoEUmwhBBCCCGEEKJEJMESQgghhBBCiBIpuwTLt+fH0Nsy02EIIYQQQohpEk2kuOPpYyTTxkyHIsS4yi7BCjz7TbT7PgSG/AMTQgghhJgLtrX08M3HDvPCyYGZDkWIcZVdghU58304W/6G+9ijMx2KEEIIIYSYBv3RJGBWsoQ43ZVfgrXxJozqxQSfukWqWEIIIYQQc8BAzEywYsn0DEcixPjKLsFC85K66J9xdzyP5+D9Mx2NEEIIIYSYYoNWghWVBEuUgfJLsADjjDeSrF1NcNt/Qjo50+EIIYQQQogpNBAzWwOlRVCUg7JMsHBqhM77J1y9B/Ht+8VMRyOEEEIIIabQQDQBSIugKA/lmWAB8RXXkmjaTGD7VyEVm+lwhBBCCCHEFMlUsCTBEmWgbBMsHA5C530cbfAEvhd/PtPRCCGEEEKIKTI05EJaBMXpr3wTLCCx+GIS87YQePabkIrPdDhCCCGEECJLKJ7ko7/eQ0t3eFLnyQy5SIxfweqNJHipfXBS1xNiMso6wcLhIHzOzWiDx/Hpv5zpaIQQQgghRJbnjvXx2KFudh3vn9R5BqKFTxG8/eljvPO27ZO6nhCTUd4JFhBfehmJxk1WFSsx0+EIIYQQQgjL8yfMxCo0yel/xbQIdofjdIXiGLJfqpghZZ9g4XAQ3nozWv9RvC/9ZqajEUIIIYQQlt1tVoIVm/i2Oqm0QShuj2kfv4IViqVIpQ2ZOChmTPknWEB82ZUk69cTeOYbkJbFj0IIIYQQMy2ZNtjbNgBAOD7x92eDWclZIUmTXS0LTeKaQkzGrEiwcDgIbb0ZV99hvC/9dqajEUIIIYSY8w52hDJrpsKTaBEcyEqwogW0CNrVsskkdUJMxuxIsID4imtI1ikCO74D0nMrhBBCCDGjdlnrr7wu56SqSdkVrEJaBO3E6nRMsHrCcT7z+305z0lMXDSR4v69p0679XazJsHC4SSy+b24uvbhbn1spqMRQgghhJjTdrf10xD0sKTWP6lkx65geV3OwloErWuFEqdfErPreD9/fLE9szZNTM4jB7r4/B91WnoiMx1KjtmTYAFR9RrS/kYCO78306EIIYQQQsxpe9r6OWNBFUGPRig+8WRnIGYmTI0VnoJaBE/nClbEir99IDbDkcwOg9b31WSGqEyFWZVgoXmJbHoXnqOPoHXtm+lohBBCCCHmrI7BOIuqfQQ9rskNubD2wGqs8I5bwUqljcx6r1CsuGsmU2l+/8Ip0lPYbhaxXodTkmCVRMRqGT3dBprMrgQLiGx8G4bLh3/n92c6FCGEEEKIOSmRShNLpqnwugh4tEm9Ae63qhMNQc+4a7AiWcM0it1764kjPXzuD3pm8uFUsBOC9oH4lF1jtnv0YBct3WFg6OsdmeQ+a6U26xIsw1dLdO0b8e3/NY5Q+0yHI4QQQggx59jVowqvRsCjTXoNltMBdQH3uC2C2YlcsdfsGDSrSr2RRMGPuXfPSf6sdxR8vJ0InBqUCtZEfeGPOnc/dxwwh1zA5KZUToVZl2ABhDe/B9IJ/Ltvm+lQhBBCCCHmHHttTNDjmvQarMFokgqvC59bG7dFMPs64SKv2RUyq0r90cIf99PnjvPLXScKPt6uYI3VIphMG7xwcuqqaOXgicPdo05aNAyDwXgqk0jbr2dEWgSnXrpmOfHlV+HfcwckwjMdjhBCCCHEnGK/AQ56NIIejUgiPeG1TQMxK8FyOUmkDFLp/OfJrloV25bYFTIrV/1FDEwIxVMMFJGQ2RWssYZcPHKgk3f8ZAdt/dGCzzub9EUSfORXe7h3z8kR99lf/0g8tzUwXMD4/uk0KxMsgMiZ78MZ68X30r0zHYoQQgghxJxiV5KCXo2AxwVMfKrfQCxJlVXBAsasYmUPtig+wTIrWAPRwlsEw/EUg0Vcx04IQvFU3r2wOgfNOLpDp8c6rYFoMtM+OR3sFs3R1qmFh7UE2i2CUsGaJonmc0nWrsG3986ZDkUIIYQQYk6xE52gxxxyARNPsAZjSSp8Lrwu823rWOuwsgdbFHu9zgm0CIbjyaI2DY5mxdeeJ2mx2ysHi5yCOFW+/bfDfORXe6btevbr3xUemWANH2ohUwSnm8NBdMNbcbfvwtUxfd8UQgghhBBzXU6LoHtyCVZ/NEml1SIIjDlJ0N4PqTbgLvp6w9dgHegIjbkWKplKE08ZDMaSBbc/hhMpHNbH+dZh2YnV4CTWrZVS+0AsU1WbDnaL5mgVvOF7nNnJtkwRnEZR9VoMzYtv749nOhQhhBBCiDljqEXQRdCr5dxWrMFYkkqvlqlgjdUiaL/xbqr0FnU9wzAyFZMB6w3+1x4+yC0PHsj7GDuJTBuFJ4+RRJrmKi+Qfx1WKFPBOj0SrFA8Na1T+vqtFs3u8MhWzXwVLJkiOI0MXw2x1dfj3f9rHPHBmQ5HCCGEEGJOyIxp92iZFsGJtnENxJJUet2ZNVhjtgha15hX5SvqegOxJImUWYXqi5iJTcdgPPNmfzTZb+oLTYaiiRSLa/04KKCCVUSL4N62flp7IwUfX4xQPEUsmSY5xnCRUrKHhnSP0iI4vIIVkTVYMyOy4SaciRDel34z06EIIYQQQswJg/EkmgO8LidB98SHXCRTaSKJNJU+LdMiGBurRTCewqM5qPEX1yJoTxDUHDAQsyso8TEnBGYncIUmQ+F4ikqvi/qgJ+9mw3ayVkwF67N/0PneEy0FH18Me9x9sWPvJ8pu0ewJJ0YkdcMrWLIP1gxJzjuLZP1afHt/MtOhCCGEEELMCaFYiqDXhcPhGBpyMYE3wfZ6nEpvgUMu4klz7y2vq8gEy0x2Ftb46Y8miSfT9EWTDMZTGHnWV2Wff6DAZCiSSOF3azRVevNuNjy0Bqvw+HvCiaI2SC5GaFjVaKrZCZbByE2f7e+heMrIJN/TGVuhZn2ChcNBZMNNuDt242rfNdPRCCGEEELMemaiYyZW9hqsiUzFsxOf+qBnqEVwnApWwKMR9LqKWoNlX2dFfYD+aDLTnpZKG0TzrPmKTCDBiibTZoJV4cnfIljkGqy0YQ7amKo1W5kEa5qqRNn7kA0fdJH9mkcS6ax9sCTBmnaxNTdiuPz49sjIdiGEEEKIqRaKpwha+18FMlMEi08A7MSnIegpeMhF0KNR4XVlqhwFXcdKqJbVBUimDY73DW3ymy9xCU1gDVYkkcLn1phX6c0/5KLIFsFwPIUBRW14XKhkKp15vaetgpVVtRo+qj17Q+FQPJmJTdZgzQDDW0Vs1avwHrgPElOzAFAIIYQQQpgGrUQHzHVYmmNiVYbO7ApWwS2CWtbkwsKu2TkYx6M5WFDtA+BwVzhzX77qVHbCWEhyk0ylSaQM/G4ni2r8hOKpzPPLVmyLoB1foVW0YmS/fqVOsKJ5vh8GYknmVZqTFrtDuS2C2YlU9pRBGdM+Q6JrX4czMYj38AMzHYoQQgghxKwWiiUzSY65DqvwNVH37DjBtx87DJDZf6mhwBbBcNxc+2VXzwpN6rrCceqDHqp95uOOdA8lWPlaG7OfTyF7VtnrhQIejTVNQQD2t+dOuU6ljUzMoQITJju5m8iUxlgyTc8o0/ps4Uls3DyW1t4Il37riVH3GeuPJllW5wdGThLMjse+L+jRpEVwpiQWnE+qYiE+/RczHYoQQogSUUr5lFJPK6V2KaX2KqX+dZRj3qmU6lBK7bT+e89MxCrEXBKKp6iwkhww3wQXUpF5rrWXrzx0gF893wZAVzhB0KPhc2dNERyjRTAUTxFwmy2C9ueF6AqZCVallWAVUsGyz605HQxEx7+OXWXxuTXWNFYAIxOsnKSt0ATLOi6WTI/52ozmh9uO8o6f7Mh7fygruZxoEpM2RrZqHu2JkEobOa+zrT+aZH6lD6/LmZnuaMuuVNnVrbqAm0TKIFFgO+h0mDMJFg4nUfVa3McexRk6NdPRCCGEKI0YcJmu65uBM4FrlFLnj3Lcz3RdP9P67wfTG6IQc08onspUsMCs2oxXAemLJPjM/ftIG+ab7MFYks5BM/EB8BTUImhet8JuESwwSekKJWgIeqjyuYHcCla+c4TjKTSng1q/u6BkyE5Q/G4nFV4XC6p97O8I5RxjV8LcmmNE5WwwliQ1yl5U2dcudtDF8d4Ibf0x4nkSs+xBIRPdx+yOp4/xljuey7mtLzo0Cn+4gViSKp+L+oB75BqsrBjs++zvj9NpkuDcSbCA2NrX4TDSePfLnlhCCDEb6Lpu6Lpu/wnYbf03PbthCiHyCsWSmTY9sNq4xmmju3fPSdoH47x96yIATvRF6QrFaLDeQDsdDrwu59gVrFiSgNsc0w5FtAhaFawqq4LVPhjPrCHLvwbLXGdW6XMVtP7JXnNkD/1Y0xgcUcGyE6R5ld6ctsNU2uDGW7fzi50nRpw3+9rFrsOyN1UeLdGB3KRqooMkDnWFOdIdzkkOeyNDe11liybMTY2rfC7qgp6RUwQTKdyaAxgagFIX8GTuO124xj9k9kjVrCDRtBnvgXuJbHn/TIcjhBCiBJRSGvAssAr4tq7r20Y57LVKqYuB/cD/1XX92Fjn1DQHNTWBScemac6SnGc6lVvM5RYvzP6Yk6k00WSa+ipf5jFVAQ+heHLMcxztizGvysurz1rMHdtb6U8Z9ESTbGiuyjzO79YwnKPHYo9Ur6/2Uek3K1G4XOPGnUil6Y0mWFgXZFFTVeb2lY0VPH+8j6Rj9OslHFDhdVEb9BBNGeNeR+s1JxM21AaoqQmwaXEtjxzswhPwEPC40DQnWJsyL6oN0NobxR/04nVrdA3G6IkkOBGKj7hO0jlUL3F4xn++2QaspCSW5zXF1Zf5MDXsmEK/J8LJNAbg8LqpsZLlmJVrDabSOec41W++RvPqAszvDHOsJ5xzf9yAxgovJ/qiDFpr2pprzfs1n2f8r8E0/dubUwkWQGzlK6l48ss4+4+Rrlo80+EIIYSYJF3XU8CZSqka4NdKqY26ru/JOuR3wN26rseUUu8HbgcuG+ucqZRBb+/ItQHFqqkJlOQ806ncYi63eGH2x9xnjdnWjKF/R16ng7ZwYsxz7GvrZ3ldgCorX9h/oo/2/hgXLNUyj/NoDvpCsVHPYw970NLpTJWoozc8btwn+6MYBgRdDpKRGJoDUgY0BT1oTgedfZFRz9E7GMfncuLXnHTniSlbe495fyqWpLc3zJIqD4YBzx7o5IwFZhJ5sstsGawPmAlia/sA9UEPhzrMSldbjxlL2jAIxVJU+lx09AxNyG7rHGRphWfMOLJ1W5sdt5waGPVxHVnPqXsgmvMcC/2e6LTG0R871Y+zzkxuTlmvRXtv7jlbO83n70obVHqctPfnvq4D4Ti1fjcn+qKc7DOfd9BlVrTauwZp9IzdnDfZf3uNjZUFHTenWgQBYquuAzBHtgshhJg1dF3vBR4Crhl2e5eu6/aGMz8Azp7u2ISYS+y2MrvFDsw1WGOt4UmlDQ53h1lRH6Da78LvdnKoM0w4kcq0CII5ICJfi6C9Xijo0TLXLmTdkL3hb3OVF4fDQaW1DquhwkOlN3/7X9gaCV/h1Qpa+zSiRbDJGnTRMdQmaLcFzrfGlNvntQc62C1zD+xr57r/3cZgLDmsRbC4Njm7RbBrlHHxMPT6+VzOCbcI2uut+rJG2ffmaU3st46p8rmoC3jojSRIZrUWhhMp6qzk0x7Tbn8+0TViU2HOJVjpqiVmm+BBSbCEEKLcKaUarcoVSik/cCWwb9gxzVmfXg+8OH0RCjG79UUSGEbuskc70anISrCC4wy5ON4XJZZMs7IhiMNh7ke1u60fGBpiAOaeWvnGtNtvsAOerDVYBYxPb+s3E6z5leYeWPY6rPqAe8zkKRxPEbA2NS4ksRmaIui0ruel0uvipaxBF/Zgi0yCZT0ne6CDnZAc6DCTz5MDMQZiSTSnWcUpZg1WPJnOrFHLm2BZ8TRUeCacwNhJXF/WBsJ20jV8DVZ2glUf9GAAvVmPi8RTVPlceDRHJtmsPw3XYM25BAvMNkF3+y6c/WO24AshhDj9NQMPKaWeB7YDf9Z1/T6l1BeUUtdbx3zYGuG+C/gw8M4ZilWIWaU/muCV/7uNn+7IHbxgvynPHnIRsIZcDE/GbIes1rCV9WYLWXOVj0PWCO+cCpZrrArWUOXMrTnxaI6CJsu1Wet+5leZSY2dYNUFzQpWvn2wQvEUAY8rU+XK99xs9j5YfvfQ/mCrG4Psb89OsJI5sdif2wmQXbXpsFr7OgZjDMaSmYSs0KmJMJTkZJ93tOfoczmp9LoIJ4rfyDiZNjJJX/b17GSrZ1iC3m/8cTVIAAAgAElEQVQdY08RhNzkL5xI43dr+N1apiJWZ08RPI0SrDm3BgvMNsGKJ7+M9+D9RLZ8YKbDEUIIMUG6rj8PbBnl9s9mffxJ4JPTGZcQc8HBzjCxZJqfPNPK6zc349LMv9tnEh1vdgXLRcow92qyNwzOOZe19mh5vbkB78JqX+a+nAqW25l3THs4q0XQvmahLYLVPlcm8am0ql/1QQ9Br2vcClal10UqbeR9brZIpsI2dMziWj9/O9Sd+XwwZo5+t59zKJNgmYlHKJ4imkgNJVgDcQZiSRorPJlqVqGyK0P5WwSTBL2ugsbsj2YgJ6nKbhE0b48l00QS6cxrYsdf5XXTZCWNpwZiKKudMpIwX/OgJyvBshKxibYwToU5WcFKVy0h0bgJ74HfzXQoQgghhBBl6bC1V9SpgRh/fakzc/vQWqjcCpZ53+hvgg92hllQ7csc11w1lGDlVrDMFsHHD3Xz4P4OwFzb9K9/1PnBk0dzrjveui9bW38053p2BashMM4arESKoFujwjp+vOTGbmHzuobefi+o8tEVimfWZ4XiSSo8Qxsl29Wz7P2geiIJ2gfNzztCMfqjSSq9Lio8WmbQRyHshEdzOvImWPYo+oB7YglWb1ZSlVPBiiap8dtrqYau3R9N4sBMzu2vSVufWWFMWkms363ht75PnA4y5wnnaR2dCXMywQKziiVtgkIIIYQQE3OkK4zX5WRJrZ+7nj2eafUaHGXIRbWVhOTbb+lQV4gV9UPjs5utCpbmdFDlH0rUfC4noXiSL/5pP1/600vEk2keOdDFfXtP0RGKs6YxSHO1WfkIeDRO9EVJj9O6d7I/lmnJAzKbDdcF3VR4Rl+DZRiG1SKoZdaajZ9gpfG5nDgdjqzn6c3EAGZLYIXXRYWVJNpDL7L3g+oOxbNaBOMMWhvzFrofl81OeJbW+sfcByvo0cwK1gRa8HLWXVnJlr3X1bI6P5C7Dqs/aj4Xp8NBXcCNz+XkhNXCmRkSYiV8QKZdEKSCdVrITBM8eP8MRyKEEEIIUX4Od4dZVhfgjVsWsvfkADuPm0Mp7La27BbBdfPM8da72wZGnCeZStPSHWFlQzBz20KrelEfcOckJF63xrHeKF0hszXuySPdPLCvnaYKD79+91Z+8vazMxWsa9c18fyJfr7wwP6cTW6zGYZhJVhDFaymCg8+l5Nav5tK3+hrsOIpg1TaMFsE7QrWONUju70t2wLrunYSEYqnqLBa8iBrDVY4TpM1Rv1obySznqtj0HwdKryuMdeLjcZu01tRH8i0IA4XsiYlTrRFMLdqlci57nIroe7OSbASmdfT4XDQXO3jhFXBsq+fnVT53Bqa09yAerRqZb7K3FSbswnWUJugTBMUQgghxOnjS3/az8NZLXdTKVTAlL18jnSFWVbn5/qN86gLuLn1qRbrnCkcDA1zAFhU46Mu4GbX8b6cczx1pJvP/1EnmTaGVbDMyk5DhTfneJ/VXlcf9FDrd/PznSd48kgPV6jGnEQM4G1bF/P+ly3l/r2nMrEd6Ajx1YcOZhKugViScCKVGRIB8MazFnLnTWfh0pxUeFyEE6kRCVr2eq/KYe18+UQSqRFrtDJtcFaCZVawzKQh6NGGWgRDCVY3muuQ9p0aGut+aiBm7ofldVExxnqx0dgVpRX1QcKJ1KhT+OxBHgG3a2IJlnWNpgpPppplr51aZu2J1RvJbRG0X08wE9BMgpU15t5OQP3WRMagRxsR/4HOENd+9ymePNLNdJuzCRZAbNUrcbfvxDlwfKZDEUIIIYQgbRj8bs9JHj889W8K2/qjXPmdJ3kiz7X+ondk9ogaLhw3R4Qvrw/gc2vcdM4itrX0svtEP4OxJAGPlpPwOBwONi+sZpdV5QJo6Q7z4V/u4akjPVy9tpG/W16Xua/KZ45ItyfJ2ez1S9dtmMcVqpFtLb0k0wZXr20aNc73XLCUK9Y0cuf2VtoHYnzuD/u4+7njPHus13oNhvbAsvndGsusZM9eXzU8EQ1lDawYWi81fgXLTghsDRUeXE4HJ/rsFsFUpj0waLUnJlNp+iIJVjWaFb797WaCtaTWT0t3GAOo9LmscfHFDbnwu52Z9sjRqj12i2DQoxFNpvNWAvOxq1ZLav2ZxKpvjArWQCxJtW/oa76g2pf5GtkJlN8zVMHK/v/wFsY9J/oxgL/un54/VmSb0wlWfLm5F6XnyJ9nOBIhhBBCCPMv+Clj/GpIKTxztJdEymDXif4R9/VHE3zyvhczlZ/hWnrMARfLrSrEazcvoMbv5ruPH6EvmswkHdk2L6jieF+UTmv90E+fO45Lc/Czd57D/3vlOqr9ucnUW89exCvWz8u5zU4+rt84n6vXNgKwuMbHunkVeZ/nP1y4jEQqzfvv2cX+jhBOB/xJNwdknLQqR/OyWgRzrzf6+qpwJsFyZSouhQy5CAyrYDkdDpqrvCMqWIBZkYqnzFHmmElghVdjn5VgrZtXQdQaWW+2CBa24bGtL5qgxu/OjDnvCsXpCcdJpoaGRYRiZougPVSi2L2meiNJXE4HzVW+TGJltwg2VXgJuLXMGizDMGgfiFGdteZuQbWPgViSgWgyq0XQmalg+Vzm/wMebcQarAPW6P8nDnePO0K/1OZ0gpWqXUmyejleSbCEEEIIcRroDdvjuCfeujeWvx3qykzfsxOrw9Z+Ux2DMXZbtx3sNG976kjPqG9O7cfYlZ6AR+Pd5y/h6aO9/PHF9pwBF7YzF1ZlrtsXSfC7vae4dl1Tzhj2bO+5YClXqMac216zaT5fe80GltT62bSginMWV/OmsxbhGNYemG1JrZ9Xn9FMa2+U85fWcqVq5OGXOkmk0pnhEtkVrGz52v/sN/tB91AFK3u90WgiidHHuDdntcENxoeSU7vlr9taH1Uf8FAX8GRiWT+/MifO8SpY3338SE67XF/ErBY1WBv1Hu4K85pbt3PXs0OdXeGE1SI4zhTIfPqiCar9bqr97qEKlvX/ar+b2oA7M2DjxVODtA/GOXdpbebxC6yvy4m+aCa5C7izK1hO6/8jK1h2gtU+GM/ZzHk6lF2CtbO1r6jsfDzx5Vfhbn0SR3xw/IOFEEIIIaZQj/XX/amqYH3jkcN8+c8vkUiled5q17M3+f32Y4f54C+eJ5lKZ96ctvXHONoTGXGeI91hNAcsrvFnbnvTWQv5yg0bmFfpZWldYMRjVFMFXpeTncf7ufu548SSad589qKi4m+o8HLhinrAbDv8nzds5g1bFoz7uPe9bCnXbZjHJ65cxVVrm+iLJnm6pZe2/hhea6DFaPK1/4WyJtp5XU6W1wd45EDXmJUSs0VwlASr2kdbf5R02iAUS2WS0wqrItVpJSB1QU+mZbLK58p57SutIReRRJrkKG18g7EkP3zqKD95pjVzW28kQbXfRX3QPOedz7QSiqfYc9IcRBJPpkmkDCq8GsEJTurriySo9rmo9rmIJdNEE6lMJava56Iu4M5UtB7c34nmdHDJyvrM4xdUDw0ByVSwhk0RBEaMkTcMgwMdIS5aYbadTkfLbbayS7A+fu8LfOa3e0t2vviyK3Ck47iPPVKycwohhBBCTEQmwZqCClZ3OM7h7jD90SR/2d/B4e4wQY9Ga2+EeDLNrhP9RBJmcnWwM4TLaVaFnjrSM+Jch7vCLKrx49Zy30pevLKee997Lv923boRj3FpTjbMr+SeHce59amjXLiijlVZkwOnUn3Qw+euUSys9nP+0loqvS7u3XOSE/1R5lV681bAKvMkWOFhmwa//swFvHhqkN1tAxiGkTOe3BYdZQ0WmIMcusMJusNxDIaSugpro2R7bVR9cKidr7HCQ2PFUOXPrmBlxxpNpGix9irbe3IAA9h5vJ+41VZotwjWBDw4IJNI2wl3OGvcvt0iGCqyRbAvmsxUsMBM6nojCYIeDbfmpDbgoTucwDAMHtzfwdYlNTmtovYQkBEVLLtF0Ho9h4+R7xiM0xdNcv6yOtY2VfD4oTJKsJRSH1FK7VFK7VVK3VyqoMby6jPmc9/utswCv8lKNG8l7a3Ge+QvJTmfEEIIIcRE9VrVilAJu3Vs9hh1B/Dtx44AcPXaJlIG7DjeR2uv2aa2u22Ag50hNjZXsqjGx1MtuQlWIpXmhZMDmSEFwzkdDjTn6AnLqzbOY8P8Sj515epRk7Dp4HE5edXGefz1pU4eeqkzZ4LgcPao+ezWO8MwcqYIArxy/TyCHo07tx/jY799gWu/9xR723LXtoUT6TwVLPP6L1qVo2BWi2BvJJHZ86o+4MlU2hqDXhqzJixW+LQRyeCPth3lLXc8S18kwV5rPH4smWa3FZfdIuhyOjKb9S6t9WcSbjvJD1hDLmBoemKheu0KlnX+vmgyk3QB1Prd9IQT7G8PcbwvyuWrG3IeX+VzEfRotPVHMxsJ+90agazWQDCrWtnVNbsCu6oxwIUr6th7cqDoAR2TMeEESym1EXgvcC6wGbhOKbWqVIHl87ati6jyufifx4+U5oROF/ElL8fT8lcwTp8doIUQQggx90xli+CO1j68LidXrW3k1EAMzengug3mEIl7d58EzORrT1s/BzvDrGwIcv7SWp452kssOfQe6cfPtNI+GOeGTc1Fx3Ddhvn88C1beM2m5lHXI02Xj1yygn9/1TrWz6/kZVnTC4ezk5aXOkJ84J5dXPmdJ7n2e9syyapdwQp4NK7fOJ+HD3Tx2MEuAm6Nf7l/X07lK5qnRdDeC+urf94PwBprWuAFy2rpjya5Z8cJgh4Nn1vLqWDVBtyZRDa7gmUng08e6SGeMnjiSDe72/qZV+nF6YDtR83JiwOxZCaxqgu6qfW7ece5i0kZZjUrlKlgDa3BCseLe6/cF7HWYFnTGPsiiUzbIEBtwE1PJMG3HjuM5oBLV+UmWA6HgwXVPo73RTMJVMCTXcEy/x90a/RFE5nNiA9Ya65WNQR5x7mL+coNG/Im/VNhMhWsdcA2XdfDuq4ngUeAG0sTVn5VPjfvvXA5fzvUPWIvhYmKL70UZ6QLV2fpWg+FEEIIIYplT1Qbbe+lydrR2scZC6q41prMp5oqUE0VaA546EAnLqeD85bV8vihbgZiSVY2BLlgeR3RZJonD3UBcLwvwq1PHeWy1Q05Y9XLjdPh4PI1jdz+1i289Zz868DsatJdzx5nf3uIy6wKy317TwHmFEHbm89eyBnNldxy/Xq+csMG2vqj/MeDBwCz6hVJpDKJQTZ7ndGeE/28ZtN8Njabw0AuWVXPOYur6Q4nMoNA7DVYjZVenA4HDdbtQY+LSt/Q5sR9kURmv6xHD3Sxt22Ac5bUsG5eJc8c7aXfGshhT+x77wVL+ZerVrPWmsZ4qCuU0wZpr3kKJ0avYPVHExzqCuWs/zIMw6xW+dyZ0ev90SS9kUQmsWus8JJKG+w83sd7LlhKTWDkWrgFVT6rgpXC5XTg1pwE3GbcdsvlZWsaiCTSmQEdL3WGmFfppcrnxufWxkyip8LIGZqF2wN8SSlVD0SAVwDPjPUATXNQUzN6ObkY77pwObc/1cL/PnWUH//9uWNOjinIhqvhL1DV8STp1edNOr7hNM1Zkuc9nSTmqVdu8YLEPB3KLV4oz5iFOF31Zq3dCcWTVPlGH75QrMFYkv3tg7z3gqWct6SGpgoP5y+tweNysqjGT0tPhI3NlZy9qDqz5mplQ4CN86torPDwoyeO8N+vXs9//fUgTgd89OUrSxLX6c5lbfhrGPCN125kY3MVV6gGPvjz3Xhczsw6NTDXC/3wLVsyn7/ngqX87xMtnL+slstWN5A2wO8aWduoD3rwaA7qgl4+fPGKzO0Oh4OPvnwlN935HHVW4lFnTfxrqhiqZA3GkmhOR2aE/UAsxbPHejEwKzgPH+gimTbYOL+SxqCHO59ppc2aWmgnOpevMac1xpJpnA441BXOVK0qPFpWBWv0yurn/qDzt0PdeF1O/uWq1Vy7bh6huPlHghq/K5PI9UUT9EWTmSEo122YR3OVl7MWVxP0jJ6WLKj28fTRHvoiiayKYW6L4NmLa7h0VT23PX2Ua9c3sb99kNWN07O+bzQTTrB0XX9RKXUL8CcgBOwExqxnp1IGvb3hiV4yo6YmwLvOXcx//vUgf9x1nAuWTTYrraS2fj3p/X+hb/37Jh3fcDU1gZI87+kkMU+9cosXJObpUG7xwuRjbmysHP8gIeaInqxNVwdjqZIlWLuOm5uubllUjUtz8vN3bcVjvdlf0RCkpSfCGc1VnLGgKvOYlfVBPC4nbz5rId949DDffaKFvx3q5iOXrGDeGOuWZpuPX76KZXWBzFj0rUtq+T8XLR91/7Bs7zpvCU+39HDLXw7w6+fbAFiYNfnP5nQ4+MQVq9myvH7E/mGrGyv458tXZdYsNVvVrkXWeeZVeum2hmBUWm13A9EEL54aJODWeN/LlvLxe18A4IzmKpbUJrjt6WPc9vQxgJxNfcHcyHlRjZ9DXWFWWGvsclsER77VTxsGO1r7OGdJDV2Dce7c3sq16+ZlxtZnV7D6IsmcFsGAR+OirKmBo7lwRR13P3ec+/aeylTy7IpadpvpRy5Zwet/9AzXf/9pAF4+bD3XdJpMBQtd128FbgVQSn0ZaB37EaXzmk3N/OSZVr7z2BHOX1o76SpWfMnF+HfdCokwuOUvsUIIIYSYfj2RBA7AYOTkusl4rrUPl9PBxmYzSQhktaotrw/w0EuwaUEV6+ZV4nSYVRX7Tf1rNjXzw23H+OFTR1nVEORNBYxFn02Gb3YM8PZzF4/7OJfTwRdfsZa33PEcBzvDfOaqNVyxZvQ3/a/aOD/vH6tu3Dz0equmCm5/65bMxsrvvWBpZspgrd9N0KPxsx0nCMVTbFlUzQXLavG5nBjAysYgGAYXrqjj4QNmy2f2pr62FfUBDnWG6F1cA1hrntwjE6yEtSHx4a4woXiKV65voiec4BuPHuZEX5S+iL3flQuPy4nf7eQPL54iFE+xbJQx/vmcu7SWt5y9kLuePT40nt36/s2uCC6q8fPZa9bwUnsI1VTBxavGTtym0mSnCDZZ/1+Cuf7qrlIEVQi35uQ9FyxlX/tgSWbbxxdfgiOdwHP8yRJEJ4QQQghRvN5IgvnW5qqlHNW+o7WP9fMrRx0ssXVxDdU+F1sWVRPwaKyfX8n6eUOV5Qqvi7edtwQH8IkrVuHSym6Xnxkzv8rHXW8/i1+9eyvXnzF/8staMDcYts+zsiGY2ZjX59a45VXrOdwV5kRflK1LavC5Na5e18SFK+pwOR24NCdfvWEDn7xyNX+3vG7U/cpWNARp7Y1w61MtLK7xURf04HQ4zL2mrCESdz3byjlffhC9fTCzOfUZzVVcYg2peOxgV6aCZbchVvvcHOk2W1FvOGN+Uc/5gxcuZ928ikzldEGVjwtX1HHmouqc465dN48PX7KCq9c1jTpQZLpMqoIF/NJag5UAPqjrem8JYirYteua+P6TLfzwqWP83fK6SX3TJpq3Yrh8uI89QnzZ5SWMUgghhBBifIZh0BNOsGVRNW39sZJNEowmUrxwaoCb8gxzOGdJDX/54Msyn391lIlrN1++mqtW12da00Th5ltTAqfDectq+fw1im89dpiLrda7T1+1JucYh8PBjZuauTHPFMgVdQFShrke6zuv35RZZ+b3aBzsDPH9J1v43ydaAPjdnpNEE2mqfS6W1PpxOBwsrwvwyMEuVvaZa6Dstr7agJv+aJIvvmJt0Um6x+Xk+286M7ORs8+t8bXXbCzqHNNpsi2CF5UqkIlwaU7etnUx//HgAZ5r7eNsq5Q5sZP5SCw4H8+xRwmVLkQhhBBCiIKE4imSaYPFNX62H+0lVKIK1u62flJpgy3D/tqfT23AM+I2p9MhyVWZuHpdE1etbZxw4WHzwiqaKjx84orVrMzaCLq5ysu2ll62tfRyycp6PB6NP+3roMrnYmNzVeZ6F6+q546nj7H9aC+v3dyc+b65+dIVOJn495F3lAEhp6vJVrBm3PUb5/ODJ1v40bajk0uwMNsEKx7/V5wDx0lXLixRhEIIIYQQ47MHXCyqMSsepapg7Wjtw+mAzVkDLMTsNpmurvlVPu5///kjbv/O6zdxoi9KMmWwqjHIzlMh/vxiOz2RBNesa8oc9/JV9dz+9DEuWFbLxy4b2iL3rEWTe59eTsonFczD63Jy0zmL2NbSy15rB+yJii++GADPsUdLEZoQQgghRMHsTYYXW3/hL3bIhWEYpI2Re2ftaO1jTWPFiAl1QhTD79ZY2RBEzatAczq4aHVDZhrgGc1DyfuG5iq+98ZN3HL9+pwx9nNJ2SdYADdubqbS6+K2bUcndZ5U3RpSwfm4JcESQgghxDSzK1hNlV7cmqPoCtZXHz7Eu+/emVmnAuakt91tAwW3BwpRKI/LyVVrm9CcDjY05263cdaimhkdMjHTZkWCFfS4eOOWBTx8oIuDnZNYQeVwEF98iVnBSpemLC+EEEIIUYjeiDVuO+CmwuMqeg3Wowc62dM2wP72ofdCu9v6iSXTkmCJKfGPFy7j1jefKdXRYWZFggXwxrMW4nc7uWP7sUmdJ7HkYpyxPlwdz5coMiGEEEKI8dkVrFq/mwqvVlSL4Mn+KCf6YwD8/sVTmdt/ubONoEdj65K5s/5FTJ8Kr4sN82Wz+OFmTYJV43fz6jOaeWBfB6cGYhM+T3zRRRg4ZB2WEEIIIaZVTySBz+XE59ao8LqKahHcedzci2hxjY8/7esglTY42R/lwf0d3HBGs1QYhJhGsybBAnjTWQswDIN7dhyf8DkMfx2p+nW4W58oYWRCCCGEEGPrjSSoDZibsga9rqIqWDuP9xH0aHzg75bRGYqz/WgPdz9nvh9601kLpiReIcToZlWCtbDaz2WrG/jV822T2jsivuhluE8+A6mJV8KEEEIIIYrRE05Q4zcTrAqPxmAR72V2tPaxaUEVl6xqoMKr8aFf7uGuZ49zhWqc1o1uhRCzLMECeOs5ixiMpbh3z6nxD84jsfBlOFIx3Kd2lDAyIYQQQoj8usMJ6oPmJr/BAloEU2mDRw50cqQ7zKGuMFsWVeN1Ofnv12zkHy9cxlvOXsg/XLhsGiIXQmSbdQ25G5urOHNhFT99tpXXn7lgQvP3EwvOw3A4cbc+QWLByI3WhBBCCCFKrTMUZ+28CsCqYI3TInjb00f57uMt2O90zlxoTgrcvLCazQtlaqAQM2XWVbAA3nr2Ik70x3j4pc4JPd7wVpNs2Ij7uKzDEkIIIcTUS6UNesLxTAWrwusiHE+NunEwwL5TA3z/yaNcvLKeV58xn3MWV8s0NyFOE7OuggVw0cp6Ftf4+PEzrVy+pgGHYwJVrIUX4H/+R5CMgMs/BVEKIYQQQph6IwnSBjRkJVgGEI6nRkwADMWTfPYPOrV+N5+9eg3V1rotIcTpYVZWsDSngzefvYi9Jwd4/kT/hM6RWPgyHOk47pPPlTg6IYQQQohcXSFzk+FMBcujAYxoE0ymDf7lvn0c7Q7z+WuVJFdCnIZmZYIF8Mr18wh6NH6+88SEHp9oPgcDB+627SWOTAghhBAiV6edYFlj2u2q1WA8d9DFtx87zOOHu/n4Fas5b2nt9AYphCjIrE2wAh6NV22cz4P7OzN/FSqG4a0mVbcG90lJsIQQQggxtUZUsLxmBSuUVcGKJdP8alcb165r4sZNzdMfpBCiILM2wQJ43eZmkmmD3+xum9DjE81bcZ18DtKF76QuhBBCCFGskQmWVcHKGtX+zNFewokU16xrmv4AhRAFm9UJ1tK6AOcvreVXu9pIpkefwjOWRPNWnPEBtG59CqITQgghhDB1hRMEPRp+t1m5sjccfvHUQOaYRw52EvRonLO4ZkZiFEIUZlYnWACvO3MB7YNxHj1Q/Mj2RPNWANwnnyl1WEIIIYQQGV2hoRHtAItq/Fyysp7bnj7G8b4IacPg0YPdXLCsFo9r1r99E6Kszfp/oReuqKO5yss9Exh2ka5cTCowD/eJp6cgMiGEEEIIU+ewBAvgY5etRHM4+NKfXuKv1pryS1Y1zFCEQohCzfoES3M6eO3mBTx7rI+DnaHiHuxwkGw+RypYQgghhJiUoz0Rdh3vy3t/VyhOfSA3wZpf5eNDFy9n+9FePnnfi2hOB3+3vG6qQxVCTNKsT7AAXr1xPh7NMaGR7YnmrWgDrTgHJzYoQwghhBDif/52mA/9cjfd4dEnG5stgiP3tHrdmQv42TvP5pNXruaLr1hLpc81yqOFEKeTOZFg1QTcXKEa+eOL7UQTxU0ETMw/BwB3m1SxhBBCCDEx7YNxIok0d25vHXFfNJEiFE+NaBG0ragPcuOmZq5UjVMdphCiBOZEggVw/cb5hOIpHtxf3LCLZMMGDJcfl+yHJYQQQogJsjcS/vnOE3QMxHLu6wrnjmgXQpS3OZNgnbWomsU1Pn6752RxD9TcJOZtkQqWEEIIISbEMAxzQMXKepKpNJ/8zR76o4nM/Z2DZoLVIAmWELPCnEmwHA4Hr9o4nx2tfRztiRT12ETzVlydeyFe5JAMIYQQQsx5oXiKWDLN5oVV3HzpSh4/0Mmbb3+WA9bwra6wmWxJBUuI2WHOJFgA122Yh+aA+/YWV8VKzD8Hh5HCfWrHFEUmhBBCiNmqKzTUAvimsxbys/eeTzJt8MUH9pNKGzn3CyHK35xKsBorvGxdWssD+zowDKPgxyXnn42BA7eswxJCCCFEkYavsdq0qJqbL13BCycH+NG2o/xuz0m8Lic1/pFTBIUQ5WdOJVgAV69t5ERflD1tAwU/xvBWkapXsg5LCCGEEEWz11hlV6iuWdvE2Yur+d4TLRzuCvOFV6zF5XTMVIhCiBKacwnWpasa8GgOHtjXXtTjEs3n4jr5LKSLG/MuhBBCiLnNXmOVPcTC4XDwqSvXcNnqBn7w5jO5bHXDTIUnhCixOZdgVXhdXBwgm0wAACAASURBVLiinj/rHSTThbcJJpq34kwMonXtm8LohBBCCDHbdIXiuJwOqoZtEryk1s8t169HNVXMUGRCiKkw5xIsMNsEu8MJnj3WW/BjEvO3AuBue3qqwhJCCCHELNQVilMXcON0SAugEHOBa/xDZp+XLa8j6NF44MV2zltaW9Bj0pULSVU0427bTnTTu6Y4QiGEEIVQSvmARwEv5u+0X+i6/rlhx3iBO4CzgS7gjbquH5nmUMUc1hmKy4RAIeaQOVnB8rk1Ll3dwEMHOokn04U9yOEg0Xwu7rZtUMQEQiGEEFMqBlym6/pm4EzgGqXU+cOOeTfQo+v6KuBrwC3THKOY47pCcdlEWIg5ZE4mWGC2CQ7GUjxxuLvgxySat6KFTuEcaJ3CyIQQQhRK13VD1/VB61O39d/wv4K9Grjd+vgXwOVKKenVEtOmSypYQswpc7JFEGDrklpq/W4e2NfOpQVO7kk0nwuAu207sarFUxmeEEKIAimlNOBZYBXwbV3Xtw07ZCFwDEDX9aRSqg+oBzrznVPTHNTUBCYdm6Y5S3Ke6VRuMZ/u8abSBr2RBAvrg5k4T/eYR1NuMZdbvFB+MZdbvDB9Mc/ZBMvldHCFauTePScJxZMEPeO/FKk6RdpTaSZY6sZpiFIIIcR4dF1PAWcqpWqAXyulNuq6vmcy50ylDHp7w5OOraYmUJLzTKdyi/l0j7dzMEbagKDmyMR5usc8mnKLudzihfKLudzihcnH3NhYWdBxc7ZFEOBK1UgsmebxQwW2CTo1kvPPlkmCQghxGtJ1vRd4CLhm2F3HgcUASikXUI057EKIKdcVMvfAkhZBIeaOOZ1gbVpQRV3AzUMv5e0SGSHRvBVXt44jWviIdyGEEFNDKdVoVa5QSvmBK4HhGxbeC7zD+vh1wF91XZdpRWJadIbjADLkQog5ZE4nWJrTwaWrGnj8cDfRRKqgxySarf2wTj47laEJIYQoTDPwkFLqeWA78Gdd1+9TSn1BKXW9dcytQL1S6gDwUeATMxSryCOZSpOepRN6u0JmglUfdM9wJEKI6TJn12DZLlvdwK+eb2NbSw+XrBp/2EWiaQuG04W77Wniyy6fhgiFEELko+v688CWUW7/bNbHUeD10xmXKM5HfrWH5iofn756Td5jnj3WS33Qw7K68lpUf2ogBkBD0DvDkQghpsucrmABnL24miqfq/A2QbefZOMZuNu2T21gQgghxBzQG06w/Wgvh7pCYx73uT/o/Gjb0WmKqnRaeyPMq/Tidc35t1xCzBlz/l+7S3Ny0Yo6Hj3YTTJV2KbDieZzcbXvglRsiqMTQgghykvnYIxP3/8ioXiyoOO3tfRgAD2RRN5jDMOgKxSnP1rYOU8nx3oiLK7xzXQYQohpNOcTLICLV9YzEEuy60R/QccnmrfiSMVwte+e4siEEEKI8rKtpZcH9nWw58RAQcc/1dIDQE84f4IViqdIpg0GyjHB6o2yqMY/02EIIaaRJFjAectqcWsOHjtY2Lj2zKALGdcuhBBC5LDXHB3vi4x7rGEYbLMSrFA8RSw5eieJnXwNFlgVO10MRJP0RhIslgRLiDlFEiwg6HFx9qIaHjtU2LYohr+eZM0KWYclhBBCDDOUYEXHPfZgZ5iOwTgbm83NO3uskebD2e2Dg7HCJv6eLlqtJHNxrSRYQswlkmBZLlpZx9GeCC3dhe3unGjeaiZYRmHrtoQQQoi5oH2w8ATLbg+8dl0TAL151mFlKlix8qpgHeuxEiypYAkxp0iCZbloZT0Ajx0qtE3wXJyxXrSeA1MZlhBCCFFW7ArWiQISrF3H+1hc40M1VQDQnWcdVm/ErGyF46my2i+rtdd8DRbKkAsh5hRJsCzNVT5WNgR4vMA2waSswxJCCCFGKKZF8IWTA6yfX0ltwAOMX8EyMJOscnGsN0JjhQe/W5vpUIQQ00gSrCwXLKtj14l+Ionxf3inqpeT9jfIOiwhhBDCEkmk6I8mqfG76Y8m6Y/mnwzYORijfTDO+vmV1AXcQP4KVvYI93JqE2ztjcgEQSHmIEmwspy/tJZEyuC51r7xD3Y4htZhCSGEECJTvdqyqBoYu03wxVODAKyfV0nQo+FyOvKOau/LSbDKqYIVlT2whJiDJMHKsnlhFV6Xk21Hego6PtF8Llr/UZyhk1McmRBCCHH6KybBeuHkAE4HqHkVOBwO6gLucacIQvlUsP5/9u47PI7y3P//e7bvStqiXi1Zlj223Bs2vffeWyAQEkISIMkJv4Sck97P+RHOCaQHEnoJHUIPmF5dcR1sy5aLrN63l/n+sbJsg23J0lbpfl2XLvBq9pmPFryzt55n7scbitDhDckMlhDjkBRYe7GZjcytcA12NRrK4H5YTXIflhBCCLG7wJo3UGAd7D6s9S39TCxwDN6f5HFY9imk9tblC+OymYDs2AsrGInx4NIdgHQQFGI8kgLrMxbVeNjS4Ru8SBxMpGgGMXMO5qYPUpBMCCGEyGytA9fOifkOXDbTAQssXddZ39LHtJK8wcc8dvMBlwh2+8ODM0GZvkQwGtO59qEV/O39bRxVm89RtfnpjiSESDEpsD5jcbUHYHBn+YMymIiULcDc9GGSUwkhhBCZr6UvSL7DjMVkoMJtZ2d3gIYOL3d/0Mh3nl7LnW9tobk3QEtfkE5fmPrSvQosh/mgM1iVA/cy9WX4EsHtXX42tnm5+ZiJ/O/5M7BJB0Ehxh1TugNkmkmFDgpyLHy4tYtzZpQOeXy4bDE5H/43ir8T3S6/pRJCCDF+tfQFKcmzAlDutPF2QwdX3LuMmA5VHjvvNHRw38fbMRsVAKaV5A4+13OAe7AC4SiBSGxwqV2m34Oltcabdywa+IWtEGL8kQLrMxRFYVG1m3cbOonpOgZFOejxoYrF5ADmXR8Sqj09NSGFEEKIDNTSF2SCJ14I1RU5+PenbZw7o5RvHF2Dx2FhV2+AVza00e0P4zAbP7dE0B+O0dwb4Pdvb+HGoydS6rQNzmqVOq2YDErGLxHUWvsxGxVqCxzpjiKESBMpsPZjUbWHF9a1orX27/Pmvz+R4lnoRivmJimwhBBCjG+t/UEWVLkBuHJ+JadOLd6ni16Z08YXD6va73M9A3th3fPRdl7e0EYoqvM/59QP3pfltlvItZrwZniTC621n7rCHExGuQtDiPFK/vbvx2ED0/ofDKddu9FKuHSe3IclhBBiXNNa+ukPRgeXCNrMxkNqUe5xWAD419oWTAaFJRvb+Xhb1+AMlttuItdqzMglgrvvMYvpOlprP1OKc4d+khBizBrVDJaqqt8GvgzowGrgWk3TDtyTNUsU5liYXJTDh41dXLtowpDHh8sX41j6O5RgL7rVmYKEQgghRPrd8WYDb27u4LAJbl5Y10pxroUT1cIRjZU/MIMVjMT4zvGTeHjZDm5f0sDl8yuAeAGWazFl3BLB59Y08+d3GwF4fm0LPYEIqhRYQoxrI57BUlW1ArgZWKBp2gzACFyWqGDptrjaw6qdvfhCQ7+Rh8sXo+gxzLs+TkEyIYQQIv22d/l5aNkOIjGdp1Y3M8Fj554r51LhGtm+T257vMCyGBXOrC/hm8fWsqndywMD+0l57OaMm8HSWvv59b83sqDKhctm4s63tgBIgSXEODfaJYImwK6qqglwAE2jj5QZFtV4iMR0VuzoGfLYcMk8dIMZ8y5ZJiiEEGJ8uOuDRkxGA3dfNps3bjyC+74wl6Jc64jHyx9YInjMpELybCaOn1zIgioXWzp8mAwKuVYjuVZTRm00/MSqJkwGhd+cXc+5M0vp8odRgMlFOemOJoRIoxEvEdQ0baeqqrcB2wA/8Iqmaa8c7DlGo4LbPfquOkajISHjHMwxDitm4xrWtnk5c17lEEc70MvnYmv5GPN+cqUib6JJ5uTLtrwgmVMh2/JCdmYWo7O1w8dL61u5fF4lhaMoqvbmsBi59aS6wfbmiqLwH8dP4gv3L8dtN6MoCjnWzFkiGI7GeO3Tdo6tK8RlN3P+rDLu/3gH1fl27LL3lRDj2ogLLFVVPcC5wESgG3hMVdUvaJr2wIGeE43qdHf7RnrKQW63IyHjDKW+JI/3N7fT3b3/jkd7yyleiH3lX+huawfzvh80UpU3kSRz8mVbXpDMqZBteWH0mYuKDt6tVWQWXde5/Y3N2M1GvnjYUL+APDQXzi7f58+Ti3K5bvEE2r3x/bFyLZmzRPD9rV30BiKcNrUYgEq3ncvmVVCYY0lzMiFEuo1mieBJwBZN09o0TQsDTwJHJCZWZphX5WJ9c9+wWsKGyhejxCKYm5enIJkQQgiRHq992s77W7u44ciawc5/yXT9ETX858lTAMizmvCFosR0PennHcrL61tx2UwsqnYPPvYfx0/i6gO0oRdCjB+jKbC2AYtVVXWoqqoAJwLrExMrM8yvdBPV4ZOm3iGPjZQtQFcMmJs+SEEyIYQQIvV8oSi/XbKZqcW5XDSnfOgnJFiu1YQ+kCOdfKEob27u4CS1SPa7EkJ8zojfFTRN+xB4HFhOvEW7AfhrgnJlhFkVTowGhWXbh250oVvyiBTOwNz0fgqSCSGEEKm3dHs37d4QNx49EZNBSfn5c63xe5v60rxM8OUNrQQjMU6fVpzWHEKIzDSqfbA0Tfsx8OMEZck4drOR+pJclg+jwAIIVx6JfdVdEPKCRToICSGEGFs2t3sBmF6Wnvvmcq3xjy3pvg/rqU92ManQwaxy2ftSCPF5Mq89hHlVbta19OEPD70cIVR1DEosjEWWCQohhBiDNrd7KXNaBwudVMu1xM/b3BvkyU92EYzEUp5hXXMf61v6uWBWOYqS+lk8IUTmkwJrCHMrXERjOmt2DX0fVrhsAbrRinnH2ylIJoQQQqTW5nYfkwrTt0Jj9xLBX726kV+/upH/e2NzyjM8+ckubCYDZ9TL8kAhxP5JgTWEWeVOFGDlzqELLEx2wmWHYdkuBZYQQoixJRKNsbXTR21B+gqsnIGZs3ZviKnFuTy+ahcvr21OaYYlG9s5cUph2mbxhBCZTwqsIeTZTNQV5bBq5/DuwwpVHY2pU8PgbUlyMiGEECJ1tnX7icR06orSt6m0x25GAU6aUsTfr5hDfWkeP3x2LZFYatq2e0MRegORtBaZQojMJwXWMMwud7K6qW9Yb+DhqqMBZJmgEEKIMWVTW7zBxaQ0Fhcuu5l7vzCXn56uYjYauGxeOV2+MFs6vCk5f1t/fMPjojzZTFgIcWBSYA3DnAoXvnCUjW39Qx4bKZxOzF6EdcsrKUgmhBBCpMbmDh9GBWry0zeDBTCtJA+LKf7xpb4k3s1wXXNfSs7dvrvAyrGm5HxCiOwkBdYwzKl0AcO8D0sxEKg7C8vW11CCwzheCCGEyCCNnT5+/rJG6DMd+hravUzwOAaLm0xQ5bGTZzOxNkUFVmt/EIDCXJnBEkIcWOa8S2awkjwrZU7rsO/DCk45HyUaxLr5hSQnE0IIIRLrDa2VZ9e0sPoz3XM3tXuZVJje2avPMigKMytcrGseeoVJIgzOYEmBJYQ4CCmwhml2hYsVO3rQ9aHvw4qUzCXqrMb66VMpSCaEEEIkTltffJZmfcueouWZ1bvY0R1gakl6Nhg+mFkVLja1ewkMY7/K0WrzhsixGMmxSAdBIcSBSYE1THMqnHT6wuzoDgx9sKIQUC/AvPM9DP27kh9OCCGESJDdy+B239f0xKomfvHKRo6Y6OHSueXpjLZfswb2q/y0LfmNLtr6gxTmyOyVEOLgpMAapjkVu+/DOoRlguhYNz6bzFhCCCFEQu2ZweojEo3xx3e2snCCm9vOnY7NbExzus+bOXCf9EgaXby3pZMr71v2ufvNDqStP0RRnjS4EEIcnBRYwzSxwIHTZmLVcBpdAFF3LeHi2bJMUAghRFZp74vfZ7SjO8DrG9vpDUS4dG4FZmNmfmQoddoozLHw2qdtvLW5g0h0eMUSwLLtPXza5qXNGxzW8e39QYpkBksIMYTMfLfMQAZFYVa5kxXDnMGC+CyWuX0NtG1IYjIhhBAicVr7g1S5bQD85b1G7GYDi6rdaU51cMdPLmTlzl6+8/Ra/uf1TcN+XnNvfNl/hzc85LG6rtPmDUmDCyHEkKTAOgRzK1xs6/LT6QsN6/hA3TnoigHDmseTnEwIIYQYvUg0Rqc3xFG1BQBs6/Jz5MSCjFwauLfvnljH6984gjOnl/D82ha6/UMXTADNA8shO7xDX9d7/BHCUZ2iXFkiKIQ4OCmwDsHsCifAsJcJ6jnFhCuPxrD2cRhG90EhhBAinTp88cKkJt/OBI8dgBOmFKYz0rDl2UxctaCSUFTn2dXNw3rO7hms4fzidPcyQpnBEkIMRQqsQzCtJA+LURl2owuAwJTzUXq2YWpemsRkQgghxOi1D8zkFORYmV6ah9Vk4MiJ+WlONXyTCnNYUOXi8VVNRGMH/8VmJBob/HmHM4PVOrAHlnQRFEIMRQqsQ2AxGZhemsfKYc5gAYRqT0M32bFJswshhBAZbnehUZhr4etH1fD7C2fisGT28sDPunhuBbt6g7y3pfOgx7X2h9hdg3X6hl5S2D7Qvr5YuggKIYYgBdYhmlPpQmvpwz/MDQ11Sy76lNOxbnoOosNbEy6EEEKkw+AMlsNMqdPGnIEW6NnkmNp8XDYTL29o3edxfzhKcK927M19e/a1HM4MVlv/7tdGZrCEEAcnBdYhml3hIqrDml3Dn8WKzbgYQ6ALy/Y3k5hMCCGEGJ2O3UVEFi+DMxkNHD+5kLc3dxLY65ehNz+xmuseXokvFH+suTc+I1WaZx1WF8G2/hBuuxmLST46CSEOTt4lDtGsMicKsHLH8AssvfYEYjYPVu3J5AUTQgghRug//7Wel9a30u4N4XGYM3bPq+E6WS3CF44OLhPsD0b4pKkXrbWfH72wgZiuDxZY00rz6BhGk4vW/qA0uBBCDEt2v4OmQZ7NRF1RziE1usBoJlh3DtYtL6MEupMXTgghhDhEMV3ntU/beHDpDjq8oTFxj9G8Kjf5DjOvam0ArNzZQ0yHk6YU8ebmDh5b0URzXwCP3Uy500anN4Q+RLffze1eqgc6KwohxMFIgTUCcypcrN7VS2SIDkV7C9RfjhINYv1UZrGEEEJkjr5AhJgOG1r7WdfSR9EYKLBMBoUTJhfydkMn3lCEFTt6MBkUfnzaFOpL83hubQu7eoOUOq0U5JgJRGL4DnJvdZcvxK7eIPWleSn8KYQQ2UoKrBGYU+HEH46xsa1/2M+JFM0gXDQL+7qHZE8sIYRIEFVVq1RVXaKq6jpVVdeqqvrN/RxznKqqPaqqrhz4+lE6smaqnkBk8N/b+kNjosACOGt6CcFIjGdWN7N8Rw/TS/OwmY2cOrUIrbWfNbt6KXXaBu83O9h9WOtb4td7KbCEEMMhBdYIzK6Id1VaseMQlgkCgelXYOrYgKlleTJiCSHEeBQBvqNpWj2wGPiGqqr1+znubU3T5gx8/Sy1ETNbjz9eWBiU+J+Lc8dGgTW9zMncCicPLt3B+pZ+5g50RDxZLUIB+oNRSvOs5DvMAHQepJPguuY+ANTi3KTnFkJkP1O6A2Sjkjwr5U4rq3b2csX84T8vOPk8ct/5GbZ1D9FfeghPFEKkXDQaoaurjUhk6JvfE6mlRRnyXpBMM9zMJpMFj6cIozFxlx5N03YBuwb+vU9V1fVABbAuYScZ43oC8QLr8Jp83t3SOWZmsACuWljFfzy9FoB5VfECqyjXyvwqF0u39wwsERyYwTpIo4v1Lf1Ue+zkWuVjk8hM6bhmyfXqIM8f0bMEsytcfNjYha7rKIoyrOfollwCU87F9unTeI/6CbpFlhoIkam6utqw2Rzk5JQO++94IhiNBqLR2NAHZpDhZNZ1Ha+3l66uNgoLy5KSQ1XVGmAu8OF+vn24qqqrgCbgFk3T1h5sLKNRwe12jDqT0WhIyDjJFFa6ALj6yBo+2tbFlFJnxmfe28Fe4zPn2vnDu1vZ2uHj6GmlgwXSefMqWbq9h0mlTiaWxQsvv84Bx9Fa+1lcW5Cw1yUb/r/4rGzLnG15YXSZGxu34nDkkJtbntJr1lik6zr9/T3093dSXV0zojGkwBqhORVOXlzfyvbuABMOoatQoP4K7Osexvrp0wRmXJXEhEKI0YhEQikvrsYyRVHIyXHS35+cTqqqquYCTwDf0jTts/toLAeqNU3rV1X1DOBpYPLBxotGdbq7faPO5XY7EjJOMu3q9AJQ57LyytcOp6I4L+Mz722o1/j7J9bxaZuXiD9Etz/+2/1jq93ccGQ1s4pzUEIRDApsb/fud5y2/iAtfUEm5dsT9rpkw/8Xn5VtmbMtL4wus9/vx+ksIhbTgdTMKo3VXwgC2O159PZ2fe6/R1HR8CZH5B6sEdq9u/0htWsHIsVziBTUY1v3UDJiCSESSIqrxErW66mqqpl4cfWgpn1+w0FN03o1Tesf+PcXALOqqoVJCZOFevxhDArkWk3kWk1j7v/72RUuLp5Tvs9jNrOR6xZXYzcbMRoU3HbzAe/BGmxwUSL3X4nMNtb+7qbTaF9LKbBGqCbfgctmYtUhFlgoCv7pV2BuW42p9ZPkhBNCiHFCVVUFuBtYr2na7Qc4pnTgOFRVPYz4ta8jdSkzW08ggtNmxjCOP5wV5FjoOECBtbqpF6MiDS6EEMMnBdYIGRSFWeVOVu787EqUoQWnnI9usmNbc18Skgkhxoq+vj6efPKxQ37eLbfcTF9f30GPueuuP/Pxx/u7VSnrHAlcBZywVxv2M1RVvUFV1RsGjrkIWDNwD9YdwGWapmXXndlJ1OMP47KN7zsGChwWOnz7b9P+7pZOZlW4sJmNKU4lRPaQ69W+xvc76ijNqXDxdkMnHd7QYBei4dCtLgJTLsCmPY73iP9Ct3mSmFIIka36+/t46qnHuOCCi/d5PBKJYDId+O37ttvuGHLsL3/5hiGPyQaapr0DHHTqRdO03wO/T02i7NMdiOCym9MdI63qinJ4ePlOdnT7qXTvua+6uTfAxjYvNx8zMY3phMh8cr3alxRYo7C75euy7d2cMrX4kJ7rn3Ut9nUPYlv3MP55X09GPCFElvvzn+9k586dXHPNFZhMJiwWC3l5eTQ2NvLII0/y/e9/h5aWFkKhEBdffBnnnnsBABdddDZ33XU/fr+PW265mVmz5rB69ScUFRXxm9/8FqvVxi9/+ROOOOIojj/+JC666GxOP/0s3n33LSKRCD//+X9TXV1DV1cXP/3pf9He3s6MGTP5+OMPufvuB3C73Wl+ZUQi9fjDlI6h1uwjccX8Ch5b2cTdH2zjx6epg4+/09AJwNG1BemKJkRWkOvVvqTAGoWpJXnkWIx8vO3QC6xowVRCFUdgX30v/jnXg0H+UwiRqZ5f28Kza5oTOuY5M0o5c3rJQY+54YabaGjYzD33PMTy5Uv57ne/xX33PUp5eQUA3//+j3A6XQSDAb785as57rgTcLn2vZjs2LGdn/zkl3zvez/ghz+8lTfeeJ1TTz3jc+dyuVz8/e8P8uSTj/Hww/dz660/5B//+Cvz5y/kqquu5YMP3uNf/3omcS+AyBg9/vC4v7+oKNfKhbPLeGT5Tq45rIrq/Hir7LcbOqh026jOH363YCHSLR3XLLle7UvuwRoFk0FhQZWbj7aNrO2wf9aXMPbvxLr5xQQnE0KMRdOmTR+8WAE89tgjfPGLl3P99dfS2trC9u3bP/ecsrJyJk+O/0ZeVaeya1fTfsc+9tgTBo6Zxq5duwD45JNVnHjiKQAsXnwEeXnOhP48IjP0BCK4bON7iSDA1QursBgN/OOj+N8jfzjK0m3dHF1bIN3ZhDhE4/16JdMmo7Rwgps3N3d8bt32cIRqTibinoRj2Z0E684CeQMXIiOdOb1kyNmmVLDb97zHLF++lKVLP+Ivf/kHNpuNG2+8nlAo+LnnmM17PjgbDEai0c8fEz8ufh9pfI+QSIKTi0wVCEcJRmK47PJxoCDHwjkzSnlq9S5uOnoi72/tJBTVOao2P93RhDgkmXDNGu/XK5nBGqXDquMNKj4eySyWwYhv/k2YOtZhaXwtwcmEENnO4XDg8+1/00mvt5+8PCc2m43Gxq2sW7cm4eefOXM2r7/+KgAfffQBfX2H3jVVZLaeQPzDyXhvcrHbxXPLCUd1Hl6+kz+/28i0klwWTJB7DoUYilyv9iUF1ijV5NspzLGMrMACgpPPJZpXhWPp70CXrsFCiD1cLjczZ87mqqsu4Y9/3LfT0qJFRxCNRrnyyov485/vpL5+RsLP/6UvfYWPP/6Qq666hCVL/k1BQQEOhyPh5xHp0+OPtyZ3j/M27bvV5Ds4YqKHez/aTktfkG8eWzuu9wcTYrjkerUvRU/hh/pwOKp3d++/uj0UbreDRIyTKD96YQMfbO3ipa8t3u8b8VB5bWseIO/NW+k+5xHCVUclM+qwZdprPBzZljnb8sL4ytzc3EhpaXUSEh1cfMlDLOXn3Z9QKITBYMBkMrFmzSfcdttvuOeehz533KFk3t/rWlSUtwxYkIjMiTJWr1ef9fG2Lr7+2Gr+fMks5lfFZ2oyPfNnJTrve1s6+eaTazhmUgG/PW96wsbdW7a9xpB9mbMtL4wuczquWXK9OjD5lVUCHFbt5sX1rWxq8zJlBJ2YAtMuxrH0f3Es+x09GVJgCSFES0szP/rRrcRiOmazme9977/SHUkkWI9/YImgNLkYtLjGwy3HT+L4yYXpjiKEGKZMu15JgZUACyfsuQ9rJAUWRiv+uV8j952fYNr1MZGyhYkNKIQQI1BVNYF//OPzvwEUY0dPIL5EUJpc7GFQFC6dVzH0gUKIjJFp1yu5BysBSvKsVHvsfLSta8Rj+OuvIGYvIGfp/yUwmRBCCHFgMoMlhBCJJwVWgiyc4GbFjh7CI12Lanbgm3MDlm1vYm76ILHhJwZKBwAAIABJREFUhBBCiP3oCYSxmw1YTPJxQAghEkXeURPksGoP/nCMtbv6RjyGf9Y1RHNKyXnvV9JRUAghRNL1+MMyeyWEEAkmBVaCzK9yYVAY1TJBTHZ8h/0H5pblWLa8nLhwQgghxF6CkRgvrGvhk6Ze2QNLCCESTAqsBHHazKjFuSPeD2u3wNRLiHjqyPngvyGWmbtTCyEy08knHw1Ae3sbP/jBd/d7zI03Xs+GDesOOs4///kQgUBg8M+33HIzfX0jn50Xmef+j7fz4xc1vKEoZ08vSXccIcQ4M9avV1JgJdBh1R5W7+rDF4qOfBCDCe+i72Lq2ohtw+OJCyeEGDcKC4v4xS/+Z8TP/+c/H97ngnXbbXeQl5eXiGgiQzT1BCjKtfDiDYulY54QIm3G6vVK+rIm0MIJbu79aDsrdvRwZG3+iMcJ1Z5OuGQujo9/S2DKuWCyJzClECJb/OlPd1JcXMKFF14CwN13/wWj0ciKFcvo6+slEonwla98jaOPPm6f5+3a1cR3v/st7r//nwSDAX71q5+yadNGJkyoIRgMDh53222/Zv36dQSDQY4//kSuu+6rPPbYI7S3t3HzzV/F5XJz551/4aKLzuauu+7H7XbzyCMP8PzzzwJw9tnnccklV7BrVxPf/vaNzJo1h9WrP6GoqIjf/Oa3WK22lL1W4tB0+EIU5lgwKEq6owghxgC5Xu1LCqwEml3uxGJU+Ghb16gKLBQF7+Hfx/30Jdg/+Tv+ed9IXEghxCGzbngc2/pHEjpmYNplBKdedNBjTjzxZO644/bBC9aSJf/mt7+9k4svvoycnFy6u7v56lev4aijjkU5wAflp556HKvVxoMPPs6mTRu57rovDH7v+uu/jtPpIhqN8s1vfo1NmzZy8cWX8eijD3LHHX/B7XbvM9aGDet54YXn+Otf70XXda6//hrmzJmH2+1mx47t/OQnv+R73/sBP/zhrbzxxuuceuoZo3yVRLK094cozrOmO4YQIgnScc2S69W+pMBKIJvZyKwK16jvwwIIVxxBsOZkHEvvJDD1UnSH7CgvxHgzZcpUuro6aW9vo6uri7y8PAoKCrnjjt+yatUKFMVAW1sbnZ0dFBTs/z1i1aoVXHTRZQDU1U1m0qS6we+9/vqrPPvsU0SjUTo62tm6tYG6uskHzPPJJys55pjjsdvjs+rHHns8q1at5Nhjj6OsrJzJk1UAVHUqu3Y1JeplEEnQ4QtTX5r+ZTRCiLFBrlf7kgIrwQ6b4OaP72yl0xci32EZ1VjeI36A55ETyfnoNvqP+02CEgohDlVw6kVDzjYly/HHn8SSJa/R2dnBCSecwiuvvEh3dzd33/0AJpOJiy46m1AodMjjNjXt5OGHH+Bvf7sPp9PJL3/5kxGNs5vZvKcTncFgJBoNHuRokU7RmE6XL0RBzuiuUUKIzJSua5Zcr/aQJhcJdtiE+BTl0gTMYkU9k/DPuBrbuocwdqwf9XhCiOxzwgkn89prr7BkyWscf/xJ9Pf34/F4MJlMLF++lObmXQd9/uzZc3n11ZcAaGjYxObNmwDwer3YbHZyc3Pp7Ozggw/eG3yOw+HA5/Pud6y3336DQCCA3+/nrbeWMHv2nAT+tCIVuvxhYjoUSoElhEgguV7tIQVWgk0tySPPauKDraPYD2svvoXfRrfkkfvuz2XzYSHGodraSfh8XoqKiigsLOSUU05nw4b1XH31pbz00vNUV9cc9Pnnn38Rfr+PK6+8iLvu+gtTpkwFYPLkKUyZonLFFRfx05/+gJkzZw8+55xzzuc737mJm2766j5jqepUTj/9LL7ylau5/vovcvbZ5w2OJ7JHR3/8N78ygyWESCS5Xu2h6Cn80B4OR/Xubt+ox3G7HSRinGT5r3+tZ+n2bl68YTEGRRl1Xvuqu8h95yf0nHkvoZoTExf0IDL9Nd6fbMucbXlhfGVubm6ktLQ6CYkOzmg0EI3GUn7e0TiUzPt7XYuK8pYBC5IQbcTG8vXq3S2dfOvJNfz98jnMLHd+7vuZmPlgsi0vSOZUyLa8MLrM6bhmyfXqwGQGKwmOrM2n0xdmfUt/Qsbzz7iaiGsiOe/9HKLhhIwphBBifJIZLCGESC4psJLgiJp8FODdho7EDGi04D3yh5i6NmFb92BixhRCCDEudfikwBJCiGSSAisJ3A4zM8udvNPQmbAxQzUnE6o4kpyPfosSGH0DDSHE0FK5hHo8kNczM7T3h8izmrCa5COAEGOJvMcmzmhfS3l3TZKjavNZ39JPe3+CWj8qCv1H/ggl0I1j6R2JGVMIcUAmkwWvt1cuWAmi6zpeby8mk8yapFuHL0RBjnnoA4UQWUOuWYmTiOuV7IOVJEfV5vPHd7bydkMndZWehIwZLZpOYNql2Ff/g8CMLxB11yZkXCHE53k8RXR1tdHfn9oZY0VRsu4COdzMJpMFj6coBYnEwXR4Q9KiXYgxJh3XLLleHeT5I32iqqoq8OheD9UCP9I07f9GnGYMqSvMocJl441N7Vx7zKSEjetd9F1sG58l5/1f0Xv6XQkbVwixL6PRRGFhWcrPO946X4nUa/eGmF6al+4YQogESsc1Kxvf+1OVecRLBLW4OZqmzQHmAz7gqYQly3KKonBcXSEfNXbTF0hc5z89pxjf/BuxNryEeed7Qz9BCCGE2EuHNyQNLoQQIokSdQ/WicBmTdMaEzTemHD85AIiMZ03Pm1L6Li+OV8hmltB7ts/gVg0oWMLIYQYu7yhCP5wTJYICiFEEiXqHqzLgIeHOshojG+6O1pGoyEh4yTb0U47RbkbeHV9K2fPKk/gyA70k3+G6anryN/6BLF51yRw7LhseY33lm2Zsy0vSOZUyLa8kJ2Zx6sOb3xFhcxgCSFE8oy6wFJV1QKcA3x/qGOjUT0h6x6zac3nMZPyeWFdK81tfdjMxsQNXHYKrvLFmJb8nJ7yU9Bt7sSNTXa9xrtlW+ZsywuSORWyLS+MPnNRkdwPlCrt3nhnWymwhBAieRKxRPB0YLmmaS0JGGvMOb6uEH84yoeNXYkdWFHoP/pnKMFect77eWLHFkIIMSa19MULrOJca5qTCCHE2JWIAutyhrE8cLyaX+XCaTOxZFNHwseOFtbjn3sD9vWPYt72RsLHF0IIMbbs6A6gAOUuW7qjCCHEmDWqAktV1RzgZODJxMQZe0xGAydOLebtzR1EorGEj+9d+G0insnkLfkuSqgv4eMLIYQYO3Z0+ynJs2I1JarHlRBCiM8a1TuspmleTdMKNE3rSVSgseiU+hJ6AxGW7UjCy2Sy0XfCbRi8zeS898vEjy+EEGLM2N4VoNIts1dCCJFM8iusFDiqrhCbycCSje1JGT9SOh//7K9gX/sA5h3vJuUcQgghst+Obj+Vbnu6YwghxJgmBVYK2MxGjpiYzxubOojG9KScw7voFiKuieS99h8ogQQ31BBCCJH1+oMRuvxhqqTAEkKIpJICK0VOUovo8IZYvqM7OScw2ek7+U4Mvjacr9woGxALIYTYx87uAACVHimwhBAimaTASpGja/NxmI28vKEtaeeIlMyh/5ifY9n+Jjkf/CZp5xFCCJF9tnf7AaiUDoJCCJFUUmCliM1s5Ji6ApZsbCechG6CuwWmX4l/+lU4VvwJ2+p7k3YeIYQQ2WWwwJIlgkIIkVRSYKXQqVOL6A1E+GBrcu+R6j/m5wRrTib3rR9gaXgxqecSQgiRHXZ2ByjIseCwGNMdRQghxjQpsFJocbUHl83ES+tbk3sig4neU/5IpGQOzlduxNT0UXLPJ4QQIuNt7/ZTJS3ahRAi6aTASiGT0cApU4t5Y1M7vYFwck9mttNz5r1E8ypwvXAtxg4tuecTQgiR0XZ0+6mQ5YFCCJF0UmCl2DkzSghFdV5JYrOL3XR7Pj1nP4ButOF69goMPY1JP6cQQojMEwhHae0PyQyWEEKkgBRYKaYW5zK5KIdn1zSn5Hwx5wR6znkIJRrE/cxlGPp3peS8QgghMseOgRbtsgeWEEIknxRYKaYoCufMKGV9Sz8b2/pTcs5ogUrP2Q+gBLpwPXsFir8jJecVQgiRGbZ1+QCo9jjSnEQIIcY+KbDS4LRpxZiNCs+taUnZOSMlc+g98x8Ye7fheu4LKMHelJ1bCCFEejV2xVu0V8kmw0IIkXRSYKWB227m2EkFvLCuJal7Yn1WuOJwek/7K6aO9bievwbC/pSdWwghRPps6/JTlCst2oUQIhVM6Q4wXp09o5R/f9rO25s7OGFKUcrOG6o5kb6T7iDvlW/gev4aes74O1hyUnZ+IYRIJFVVq4D7gBJAB/6qadrvPnOMAvwOOAPwAddomrY81VnTaVuXnwkyeyWEECkhM1hpsqjaQ3GuhWdTuExwt+Dkc+g76X8xN72P+7krUII9Kc8ghBAJEgG+o2laPbAY+IaqqvWfOeZ0YPLA1/XAn1IbMf2kwBJCiNSRAitNjAaFs6aX8P7WTlr7gik/f1C9iN5T/4Sp9RPcT12M4mtPeQYhhBgtTdN27Z6N0jStD1gPVHzmsHOB+zRN0zVN+wBwq6paluKoadPjD9PtDzNBGlwIIURKyBLBNDpreil//3A7z69r4dpFE1J+/tCkM+k5MwfXi1/G/dSF9JzzMLG88pTnEEKIRFBVtQaYC3z4mW9VANv3+vOOgccOuG+F0ajgdo++IDEaDQkZZzS29nUDMK3SPawsmZD5UGRbXpDMqZBteSH7MmdbXkhdZimw0qjKY2depYvn1jRzzWFVKIqS8gzhCcfRc/aDOJ+/BvdTF9B9zsPE3BNTnkMIIUZDVdVc4AngW5qmjbpNajSq093tG3Uut9uRkHFGY932LgAKLIZhZcmEzIci2/KCZE6FbMsL2Zc52/LC6DMXFeUN6zhZIphm58woZXt3gJU709c2PVy+iJ5zH0UJe/E8eQGm1lVpyyKEEIdKVVUz8eLqQU3TntzPITuBqr3+XDnw2LjQ2OXHqECFy5buKEIIMS5IgZVmJ0wpJMdi5Nk1zWnNESmeRff5T6KbrLifugjl0xfTmkcIIYZjoEPg3cB6TdNuP8BhzwJXq6qqqKq6GOjRNO2AywPHmm2dfspdNsxGueQLIUQqyLttmtnNRk5Wi/i31kZ/MJLWLNH8yXRd+CyR/CkYH/sC9lV3pzWPEEIMw5HAVcAJqqquHPg6Q1XVG1RVvWHgmBeABmAT8Dfg62nKmhaNXT5pcCGEECkk92BlgPNmlfH06mZeWNfKJXPT22RCzymm+7zHKXjjW+S+82OM3Q30H/UTMJrTmksIIfZH07R3gIPewKppmg58IzWJMksgHKWhw8fhNfnpjiKEEOOGzGBlgOmleUwryeWJVU3oup7uOGC2E73wHnxzvop9zb24nr0cxdua7lRCCCEO0drmPqIxnbmVznRHEUKIcUMKrAxx4ewyGjp8aW12sQ+DEe+RP6T3pDswt6wg/5GTsDS8nO5UQgghDsHKnfGN5GeVS4ElhBCpIgVWhjhlajG5ViNPrGpKd5R9BNUL6LrkRaK5ZbhevI7c12+BkDfdsYQQQgzDyp29TCp04LTJMm8hhEgVKbAyhN1s5Mz6El77tJ1OXyjdcfYRzZ9C90XP4Zv3DWzrH43PZm19Ld2xhBBCHEQ0prO6qZc5Fa50RxFCiHFFCqwMcsHsMiIxnefWtKQ7yucZLXgP/z7d5z+BbrLhev6LOF+4DkPfuNlKRgghssqmNi/eUFQKLCGESDEpsDJIbUEO8ypdPPnJLmKZ0OxiPyLlh9F16cv0L74Vy/Y3yX/oOOzL/wDRzJp1E0KI8W73/VdzKuT+KyGESCUpsDLMhbPLaOoJ8P7WrnRHOTCjBf/8G+m8/A1CVceQ+/6v8Tx6KlbtSYiG051OCCEEsHpXL8W5FkqdtnRHEUKIcUUKrAxz/ORC8h1mHl+ZWc0u9ifmrKT3jLvpOfMeQMf575vJv38x9mW/RwlkcIEohBDjQEOHj8lFuemOIYQQ444UWBnGbDRw4ewy3mnoZFuXP91xhiVUcxJdl79Oz5n3EvVMIfeD31BwzwLyXv46li2vQCQ7fg4hhBgrYrrOti4/1fn2dEcRQohxRwqsDHTB7HJMBoV/rsiiBhKKgVDNifSc+zCdl72Kv/4KLNvfwvXClyi8eybOF67Duv5RFF97upMKIcSY19wbJBiJUZ3vSHcUIYQYd0zpDiA+rzDHwslqEc+taeGGI2vItWbXf6ZowTS8x/wc75E/xNz0AdYtr2DZ8grWLS+joxApW0Cw5hRCE08h6pmU7rhCCDHmNHb5AKiRGSwhhEg5mcHKUJfNq8AXjvL06uZ0Rxk5o4Vw1TH0H/MLOq/+kK5LXsK38NsQ9pP7/i/Jf+hYPA8eS877v8bUugpi0XQnFkKIMWFrZ3xpdo3MYAkhRMpl19TIOFJfmsf8KhcPLdvBJXPKsZiyvBZWFCJFM4gUzcB32H9g6NuJZeurWBtexr7izziW/wHdZCNSMI1Q5VGEJxxLuGQ+GM3pTi6EEFmnsdNHntWExy7voUIIkWpSYGWwaw6r4qYn1vDCuhbOm1WW7jgJFcurIDDzGgIzr0EJdGFpfB1T2xrMLStwLP8jyrI7iZlzCVceSWjCsYRLFxD1TJaCSwghhqGx00dNvh1FUdIdRQghxh0psDLYomoPU4tzuX/pDs6eUYrRMDYvlLrNQ1C9kKB6IQBKsAfzzvewbHsTy7Y3sW55OX6cwULUVU3UNZGoq4aop5ZIQT2RgqlglmUwQgix29ZOP4trPOmOIYQQ45IUWBlMURSuWVTFrc+tZ8nGdk5Si9IdKSV0q4tQ7emEak8HXcfQsxVz6ypM7Wsx9mzB2LMVy463UCKB+PEoRN0TMZTNxOFUiRTWEymoJ5ZbBvLbWyHEONMfjNDuDcn9V0IIkSZSYGW44+oKmeCxc89H2zlxSuH4W+6hKMTcEwm6JxKcct6ex/UYhr4dmNrXxb861mHctYqc9c8MHhKzugaLrUhhPdHCeiKeyWCypeEHEUKI1Ggc2EOx2iMdBIUQIh2kwMpwRoPC1Qsr+cUrG/mgsYvDa/LTHSkzKAZizgmEnBMI1Z4GgNvtoKe1BWPHhn0KL/u6h1AGNjvWFSNRdy2xvAqizglEPJOJ5k8hkj8F3V4oM15CiKzX2Lm7RbvMYAkhRDpIgZUFzqgv4a/vNXLPh9ulwBqCbskjUraQSNnCPQ/Gohh7GzHuLro6NQz9u7A2L8ce6t1zmNUdL7Y8dUQ9dUTdk4h46ojlVYLBmIafRgghDl1Dhw+jQaHSLbP1QgiRDlJgZQGz0cCVCyr53zca+KSpl1nlznRHyi6G+KxV1F1LqO6sPY/rOgZfC8bOjZg6P8XY+Smmro1YG17CEOjcc5jRStQ9kYi7jqhnUnwGLKeUWG4Z0bxKMFrS8EMJIcT+NbR7qfbYMRmzfHsPIYTIUlJgZYnzZpbx9w+2cc+H27j9/BnpjjM2KEq8UMopJVx19L7f8ndi7N6MqWszxu5NGLs2Y2pfi7XhBRQ9NnicrhiJ5VUScdcSdU+MF3KuGqKumoGZL/krJoRIrc0dPqaX5qU7hhBCjFvy6S9LOCxGLp1bwV/fb2RTu5e6wpx0RxrTdHs+EXv+vksNAaJBjL3bMfhaMfQ1xbsadm/B2N2ApelDlIhvzxgDxVfUXUPUWTNYeEVdNUSdVYDcHyGESCxfKEpTT4BzZpSkO4oQQoxbUmBlkUvmlnP/0u3c+9F2fn7G1HTHGZ+M1vj9WZ66z39v95LDnkYMPVsx9jRi7NmKsWcr1uYVGPa630tHAVclzvx6IsWziBTOIJKvxlvLy/1eQogR2jLQ4KK2QH4JJ4QQ6SIFVhZx2c2cP6uMR5fv5IYjq6lwSQvejLLXkkPKF+37PV1HCXbHZ7t6tmLsbcTe34CxadXgRsoAusFELLeCqKuGSP4UovkqkYKpRPKnyGbKQoghbW73AjBJVjkIIUTaSIGVZa6cX8k/VzRx/8c7uPWkyemOI4ZLUdBtHiKlHiKl8wCwuB10d/tQQn3xTZS7GzD2bMPQuw1jzxbsa+5DiQaB+IxXzDmByEDBFS2YSiRfJequBaM5nT+ZECKDNLT7sJoMVLikg6AQQqSLFFhZpjjPypnTS3huTTNfObyaghzpYJftdEse4fLFhMsX7/uN3e3lOzVMHVp8f69ODUvjayh6NP5cg5moZzLhopnxpYZFM4l66tCt0mlSiPFoc4eXmnwHRoPs6SeEEOkiBVYWunphFc+taebh5Tu58eiJ6Y4jkmXv9vK1p+95PBLA2N2AqWM9pk4t3t1w66vYNzw6eEjM6iLqnEDMOYGosyreWMNTR8QzGd1ekPqfRQiREg3tXhZMcKc7hhBCjGtSYGWhCR47J0wu4vGVTVxzWBW5VvnPOK6YbEQL64kW1hPc/ZiuY+hvwtS2euAer23xr44NWLa8ihILDT49ZvMMFFt1RF0T4+3lXbVE3TVgtKbjJxJCJEBfIEJrf0gaXAghRJrJJ/Msdc1hVfz70zYeW9nEtYsmpDuOSDdFIZZXQSiv4vPf02PxlvLdmzB1bcLYtQlj10asW17F4G/fc5jBQqR4JpGC+sGZs6h7ItG8KrnPS4gs0NCxu8GFNMQRQoh0kgIrS6kluRxe4+GR5Tu5fF4FNrO09hYHoBiIOSuJOSsJTzhu328Fewf28mrA1LYGc8sKrJuexRDsGTxGN5iI5lURdU/EUDwZm70K3epGt+TGizBntbSWFyIDbO6QFu1CCJEJpMDKYtcsquKrj37CU6ubuXzefmYuhBiCbnUSKZ5NpHg2wSnnDz6uBLriXQ0HvkzdDRi7t2Bo+oC8sG/fMUz2eEfD/ClEnROIuqrj/3RWx+/3UuRmeyFSoaHdi8NspNQpS32FECKdpMDKYvMq3SyocnHPh9s4b2YpdpnFEgkSbyk/n0jp/H0ed7vs9DZtRQn1owR7MHZtjDfbaF+Hefub2Lwt+xwfM+cQzZ8SL8AGWstHCqai2wul8BIiwTZ3+JhY4MAgf7eEECKtpMDKcjccWcOXH1nFYyuauPqwqnTHEWOdohDLKYGcEgAipfP2NNoACPsx9m2PN9joaYwvP+z8FOuWVzCsf2TwsJgtn0iBGt9IOX9qfG+v/CnSXl6IUWho93JUbX66Y+xDCfZi3v4WhlAfEU8dutUFihFdMYBiAMVIzOYBiyxrFEKMHVJgZbnZFS4Or/Fw38fbuWB2mXQUFOlltseXCuZP+dy3FF87po4NmDo3DO7tZd3wGPawd/CYaG450fzJRNx1RD11RD2TiLjr0B1FMuMlxEF0+UJ0+sJMKsyMQsXQt5OcD/9/rBufRolFDnqsbrIRmHIB/lnXEi2YlqKEQgiRPPJpfAy44cgavvjgCh5ZvpMvH16d7jhC7JfuKCTsOIpw1VF7PRjvcGjq3BDfSLljA8buzdibHkKJ+AcPi1mc8WLLM4Vo4TQihdOJFNbHfxsuhKBhsMFFejsIKsFeHMt/j33V3QD4Z1xNsO5sYo5iTF2bUMJe0KPxr1gMRY9halmGTXsC+7qHCFUcgX/WlwjVnCzNc4QQWUsKrDGgvjSP4+oKeHDZDi6ZW47TJi21RZYY6HAYclZCzUl7HtdjGPqbMXbH28qbujbHW8s3/hvDXhsqR/MqiRTUEymcRqRgWnx/MFdNfOmREOPI5vbdLdrTNIMVDWFfcx+Opb9DCXQTVC/Au+i7xPbaOiLkOsAvAOsvw3v4f2Jb9zD21ffgevHLRJ0T6D3tL0SKZqboBxBCiMQZVYGlqqobuAuYAejAlzRNez8RwcShuf6Iat64r4MHl+7ga0dNTHccIUZHMRDLKyeWV0646pg9j+s6Bl8rxvZ1mNrXDjbYsDT+G0WPxQ8xOYgUTCVSWE+kYBpKaS1GpSC+bNEgv1MSY1NDh488q4nCHEtKz6sEurGtexD76vsw9u8kVHkU3iN+QKRoxiGNo9s8+Od9Hf+c67FseYXcd36K69kr6b7gSaKeuiSlF0KI5Bjtp43fAS9pmnaRqqoWQHY3TJPJRbmcrBbx8PKdXDavAo8jtRdZIVJioMlGLKeEcPXxex6P+DF1foqpfV28+OpYj3XTc9jXPgBAPhAz5xIpnk3UWUk0fyrh8kVECqbJJspiTNjc7mVSoQMlhfcqGrwtuJ6+GFN3A6GKI+g77jfxvfZGk8FgIjTpDHoKpuJ+8kJcz1xG9wVPEXNKEychRPYYcYGlqqoLOAa4BkDTtBAQSkwsMRLXH17Na5+2cd/HO/jmsbXpjiNE6pjsg/t5DdJ1DN5duJQefDs3YG76CFP7GiyNSzCujy8z1A1mop7Jg0sMIwXx+7t0R2GafhAhDl1M19nc7uNktShl51R8bbieuRRjfzPd5z1GuOLwhI4fddfSfc6DuJ++GNezl9N9/pPoOcUJPYcQQiTLaGawJgJtwD9UVZ0NLAO+qWma90BPMBoV3O7RT3IZjYaEjJMqqco7x+3g3NnlPLayia+dUEdxnm3EY2XbawzZlznb8kIWZvbUYTAasFfMB64EIAbEeptQtr+P0rIaQ8tarDvfw6Y9Mfg03VWFXjYXvXxe/Kt0NljzUhI5615jsjPzWPKvtS30BSMsnOBOzQnDPlzPX4Oxbyc9Z99PuHxxUk4TLayn56z7cD9zGe5nLqXnrPtkJksIkRVGU2CZgHnATZqmfaiq6u+AW4EfHugJ0ahOd7dvFKeMc7sdCRknVVKZ9+r5FTy7qonbX9a49aTJIx4n215jyL7M2ZYXxlJmN1ScHv8aoPg74/d0ta3B1LoKc9NKjBueBUBHic90lcwhXDw7vglzwbSkdDkbO6/x8BUVpaZ4HYv6gxH+8PYWZpY5OXFKCmZeY1Gcr96EqW01vaffnbTiardI6Xx6zroP54tfxvP42fSe8gfClUcm9ZxCCDFaoymwdgA7NE37cOAcBNgdAAAgAElEQVTPjxMvsEQaVbrtXDgwi3X29BKml8nGrUIMh27PJ1x55D4f3hR/B+bWVZhaVmJqXYWl8TVsG/4JDLSOL1CJuiYSLpq5p+iSe7pECnT6Qry5qYN3Gjrp9IW5/fwZKbn/yvHRb7FueZm+o39GaOLJST8fQLjicLovfAbn89fgfuZSgrWn0X/ML+ObngshRAYacYGlaVqzqqrbVVVVNU3TgBOBdYmLJkbqa0fVsGRTO798dSP3XTkXk1FaVgsxErq9gFD1CYSqTxh4QMfQtwNz81LMTR9i7NqIpXHJYNGlm2yEi2YTKZ1HpGgmEU8dUfdEMNnT+FOIseiv7zXyxKpdmAwKVy+sYnpp8mcBLQ0vk7PsDvzTLiMw60tJP9/eop46ui59Fceqv+FYdieuZy6n+4In0G2elOYQQojhGG0XwZuABwc6CDYA144+khitXKuJW06o43vPruPh5Tu5aqGsWRciIRSFmLOKoLOK4JTz44/pOoa+nZhblmNqXoa5eRn2VXehxMLxb6MQdVUTrjqW0IRjieZWEHNWoVtldlmM3M7uAFOLc7nnyrkYDcmfuTL07yLvtW8TLp5N/zG/SPr59stsx7fgZsKl83H962pcz32B7vMeQxoYCyEyzagKLE3TVgILEpRFJNDxdQUcM6mAv7zXyAlTCqlwyW/QhUgKRSHmrCTorCQ4+Zz4Y5EAxu4GTF0DGyW3r8W24Z/Y19wLgK4YiBTOIFxxOOGKIwiXzEG3F6TxhxDZpqUvSE2BIyXFFUDu2z9CiQbpPeUPYBp5A6VECFceSe+pf8L5wnXkvvNjOP8Pac0jhBCfJbtujlGKovD/nTCJS+9Zxn//exO/uyA16/OFEIDJRrSwnmhh/Z7HIn5MbWsx+FowdWzAvPM97J/8A8fKvwAQzSkhUlCPoXohprITiBROH91+QmLM0nWd5r4Ai2pSszzO0vAS1oYX6T/8+8RcNSk551BCE0/BP+/rOJb/gcjUU6AsNfeDCSHEcEiBNYaVOm3ccFQNty/ZzIvrWzmjXm4IFiJtTHYiZfEJ/9CkM4HvQMSPuXk5pva1mNrXYWpfi+Gd2/Do/0PUUUykaCZRdy2xvArCpQuIFM1MSudCkV36ghH84RgledbknyzsJ/ftHxEpmIZ/9vXJP98h8B52C+Yd72J6/psYLnqemHtiuiMJIQQgBdaYd8mccl7d0MZvl2xm4QQ3RbkpuCALIYbHZP9c50K32Udg1dPxjZHbVmPZ+R5KxA9AzOoiXHkUofJFAxskT0e356crvUiTlr4gQEoKLMeKP2Hsb6L75Dszr0Om0UzvqX8k//Ezcb30FboufBbMcj+WECL9pMAa44wGhR+fNoUr71/Or17dyO3nTZelgkJkspxCAvVXEKi/Iv5nXcfga8W88z0s29/GvP0trJufHzw8UlBPqPLI+L1cxbPRc4rTFFykyu4CqzTJBZahbyeOFX8kUHcO4fJFST3XSMWcE4ie9zeMj1xC3pvfp++k36U7khBCSIE1HlTnO/jG0RO5fclm/rmiiUvnVaQ7khBiuBSFWE4JwSnnxzsX6joGXwvGrs2Ym5dj3vku9jX34Vj1N4CBpYUziBTOIFI0nUjxXGJ55Wn+IZJHVdW/A2cBrZqmzdjP948DngG2DDz0pKZpP0tdwsRL1QxWzvu/Al3He8R/JfU8o6XXnoBv/s3kLP0/AlPOJzzhuHRHEkKMc1JgjROXzi3no8YufvdWA7MqnEwrSf6eKUKIJFAUYjmlxHJK40sLF9wEkQDm1pWY2tYMfK3Gsu1NFD0KQDS3LH4PV+n8+D8Lp2fecq+Ruwf4PXDfQY55W9O0s1ITJ/mae4MYDQoFOZakncPU9BG2jc/gXfAtYnmZ/0s534KbsG56jrw3/5POy14Ds3TOFUKkjxRY44RBUfjxaSpfuH85339uPQ9cNY9cq/znF2JMMNkIly8mXL54z2MRP6aODZhaVmBuXoZ511Jsm54DQDeYiOVWEMlXCZctIFy2MN5AI83tt0dC07S3VFWtSXeOVGrpC1KUY0lei3Y9Ru47PyaaW4Zv3teTc45EM1rpP+7XuJ++hJwPfoP36J+mO5EQYhyTT9jjiNtu5pdnTuWrj67iF698yq/Pmib3YwkxVpnsRErmEimZS2DWlwAw9Ddhal6OuW0Nht5GTO1rsW59BQDdYCFSPCtecJUuJFy2YCztzXW4qqqrgCbgFk3T1g71BKNRwe0efcMEo9GQkHH21uEPU+GxJ3zc3Yxr/4mhbTWR8/6Gu6gwKedIpMHX2H0S0R3X41j6Vyy1i9GnX5juaAeUjP8vki3bMmdbXsi+zNmWF1KXWQqscWZ2hYtvHD2RO97awmMrm7hkbuYv/RBCJEYst5xQXTmhuj2r5RRfe3yGq/ljzLuWYl/1dxwr/gxAxF1LuHQhkYFZrqh7UjbuzbUcqNY0rV9V1TOAp4HJQz0pGtXp7vaN+uRutyMh4+xtZ5ef6aV5CR8XgFiUwrduI1JQT1f5aZCMcyTYPq/xgltx71yB6V8302WrIVowLb3hDiAZ/18kW7Zlzra8kH2Zsy0vjD5zUdHwbrGRAmscunJBJSt29HD7ks1MLHCwcEJqNqsUQmQe3VFIqPZUQrWnxh+IBDC1rca8K15wWbe+gn3DowDEbB7CpQsIl84nUraQcMkcMGb21g+apvXu9e8vqKr6R1VVCzVNa09nrpGK/T/27jtMkqre//i7Ovf0hJ60aTans5klZyRJRlCQpARRkSvg9eq9KiaQq/enchVRrxERESQKiuQgEpScNrB7Nuc4Ozn09HR3/f7o3mU2p5ru6ZnP63n22emq6lPfqamZms+cU6dcl/WtXZw0sXd6lsKLH8NpWET7qb8qxjAN/hAtp/6K+P1nUPH4Z2i84HHccEWhqxKRAUYBawDyOQ43nTGJT9/zLl/72zzuuORARlTqhmARAQIRUkMPJTX0UDoBXBd/0xKCa98gsG5z6HoGgK6xp9Fy+m2FrHa3jDFDgPXWWtcYcxjgAzYVuKx91tDRTSrj9s4Mgm6Gkjdvxa2ZSHLcGd63nyeZ2GBaTvs18b98nLJnvpA9R/vPpC7Fy3XxNy7C37KCQP1cgmteww1E6BpzKgTCOJ0NdA89jHTNFMgk8bWvx9+2FtwMmVA5brgMN5T953S3E1z3Fk5XC5nYYNIVo8nEhhTnHwWkX1LAGqBKwwF+dO5Urrj7Hb70lzn8/pIDNemFiGzPcUhXjiNdOQ6mXJRd1NlAcN2bZEqHFrg4MMbcAxwP1BhjVgE3AEEAa+2vgPOBfzPGpIBO4CJrrVugcvdbb07RHlz1MoEGS+rsX4Dj87z9fEoNPYS2Y2+i7IXrqXjsClpO/SVuuLzQZQ0ITkc9gU3vE9g4l8CmeeDz4wZKCC3/O/7WlVu2S1VPwulqJrz0qa3enwmV4Uu27vV+M6Fy8IdxfX4ysSH4B00gGp9KcsSHSFftdlRwYaUShFa9DEPHQnjs3r/fdXG6mvC3rsbXtgZf2xqcZBvJcWeQjo/F17YGyA4Tl/zQb9QD2PB4lB98ZArXPDibrz86j1s+Oq33ZqUSkX7DjVaRHHNKocsAwFp78W7W/5zsNO79Qm8GrMi8+8iEK3CnnAttGc/bz7fEtEvBF6T0ha8Rf/g8mj76oIYLesDXugZ/89IPnsnXvAynuwMn0URozav4W5Zv2TYdGwKOD1+ike5hh9NxyBdIVU0kHR+LG6nM9mptmgeOHzdUSmjlCwQ2zCYTG0QmNpR06VBw/DjdrThdrfiSLTjJVnD8dA85mEy0Bl/HBvxNiwk0LoJ0N06mG1/bWpxlL1Pa9gAAyWFHkI6PBTeNG60lXTaMTOkw0qXDyJQOxQ3He7f3y3Xxta4m0GBJlw4lHR9DcMMsAuveIlA/l9CKf+DragYgPugAnHQXvrZ1ZEqHkQmX42tfjy/ZBpluMpE4mZLBOJkkTrIdp7sNX6IRJ9W5/W5f/QGZ8pFbvibJoYeTrpoIbopA42Kcznq6JpxDYvJF2fClHkDPKGANcAePiPPVk8bzP88s5GcvLuWLx+/DX05ERCQvFte345D9A5mXnEQj4SVPkphyMYFABCiuG9d3JjHlItKlQ6l47ArKn/gMzWff1efvGywIN4OTaMSNVIHj4HQ1g5vBDcfxta4kuPYNfA3vUrn0pWyQ6flWHAhEcIMldA8+iM5pl5KqnU6qZko2RO2K42SHBOYkplwCU3ax/Q6kqw3dI47dbnk8XkLLqkWEFzxMZP4D+JuWggO+zk04mdTWn0Mgmg1bZXWkS4dmw1fFKFKDDyITqcLXvhZfVzNOsh1f21r8ravwta7CSSdIl4/CSTQRXP8W4COT+5ydTDdkUjjJVnwdG/B1t++4/tI6kqNOpGviRylNrIBZD5IuraN7yKHZ3qhkK6naabihcvAHcDob8XWsxw2VZ8NhsJRMOE6mbHNgzP7v4BKZcyeB+rl0Tr8cJ9VFeNFfCSx5HHCywypLhxF74xZib9xCJlhKaughJMx5OF3NBDbOITn2dJKjTsyeEx0bCW6cTTo2ZKuvGW4GMmkNw92GApbw0RlDWbSxnbvfWsWE2hhnTh1c6JJERGQH5q5tZXR1iedDusML/4qT7iIx+SJKPW258LpHfojWE/+X8mf/nfJnrqPlwz8Hf+89pLlPc1187esI1L+Pf9M8/C0rsvdEbZiFL9mCGyjBDUbxdWZvU3R9wWxQANxwGd2DD6ZtyidI1UwBx0empJZ0+Yg+G1ozpcPoPOgaOg+6psfCNL7Ojfhas0Pp/G1r8bWtxt+2Bl/rGkKbLL6ODTjsfCRx9lmCw3D9YULLn8+GyyGHgC+Ak2gEx4cbCOP6ArgVo7LHKT6WdPVkfK2r8DctIVU7ne5hh28VQjPxEprMFZ587i7Qcfh/bbWs45DrttvO37SE4Ip/EGhaTGjpM5Q/k93GDZQQnXcv6dKhON0dW3rYALprp9M9/Gh8FTVUvnM3/rY1dBz0+exz8wK6px8UsCTnP44fy5JN7Xz36QXUxEKcOrO4nmsgItLfua7LnLUtfGi8988ni7x/L90100jVTvO87b6gy5xHW2cDpf/8DhXdV9J82m8g2P+vc762tTipTpyOeqLz7iW07Bl8icYt6zO54XJd488mXTkue+9OdzvpijHgC+LrWE+6bDjdQw+jbOyBNLd0FfCz8UjuHq1MbAhw0I63SSezwWPdWzjJtuxQvUglbjBKJjaUTGww+PzZbd0M4OzF8LrDPfgkvJOOj80OnwQ45iYC698hU1JDprSO8IKHsgGypJp0+ShStdPwb5pPxD5E9L3bcTJJugcfRLJ6ErE3biG84C+0nP4b3Egl4fkPZocmZjJ0HPrvZMpHFvYTzTMFLAEg4Pfxg49M4er7Z/Gff53LoKoSxpT3zb9IiYgMRKuaEjQnUkwd6u1kDYGNcwjWz6H12P/2tN2+pnPmZ3FDMUqf/yrxhz5Gy+m3kSkfXuiyvJXqzN7bs3E24UWPElz35pZVmWCM5NjT6R50AOmaKaSqJ+/dxB+bA8VA4A+Rrp5EunrS7rct8glhtuLzkxp6yJaXXZMvpGvyhVtt0l13VPbh9elu4uEETansc6GCK1+m7NkvUPnAWdkhp5lu0iWD8CXbCC95gvYjv05q0AxSVRMGRC+XApZsUR4J8tPzpnPVve/ymT++xa8umMGE2v42WEREpDjNWZd9pNe0IXv2oMs9FZ53H64vRNfEcz1tty9KTLmETMlgyp65lsoHzqDl1F/SPfzoQpe1X5xEI8E1rxNa/izhRY9umYEvVTmBtiOvzw5l8wXpHnk8bkjXdPGIPwilFVseRt494hgaL3iSspe+Raakho4DriJTMQpfywrKn/o8ZS98DQA3ECE5/FiSo08mOfrkbG9gP6SAJVupiYX4+fkzuOr+97j2wdncdtFMPSNLRGQfvL2qicEd3dSVeHPz99y1rUSDPsbWxDxpD4BUgsiCh+gae9ruJyToJ5KjT6Lp449R/vinqXjkEtqP/hadB3ym0GXtscDG2YSWPUdw3ZvZ+6ja1wPZiRq6xp9F19gzSA2aTqZksGaFk7xyY4NoOe3XWy3LlI+k6by/4m+w+JuXElz9KuFlzxJe9gwuDl0TPkLHof9BunJ8QWruLQpYsp1hFRHuuPxQLvrtq1zz4Cxuu2gmg3rjoZYiIv3Y7a+uoNuFX398hiftzV7byuTBZQQ8fJxGeOnT+LqaSeSecTZQpONjaTr/b5Q990VKX74RJ9FEx2Ff7ruBxM0QWv480Xd/RWj1K7g4pKsm0j38WDqrJpAacgjdgw6AQKTQlYpsz+cnXTOFdM0UkuPOpP3Ym/A35O7lmv0HIgv/SvfgA0mY80lMugCCxf+H/X40cFS8NH5QKT89bzotiRTXPjibpo7uQpckIlJUxlbHeH9tC6nM/j/XuCuVYcGGNqZ5fP9VZN592Smh64p7mNy+cEOltJz2GzonX0jszZ8Qe/UHuQkL+pDuDiJz7qTyTydQ8djl+JuW0nb0t9n06Vk0XvwcrSf/hM6DrqF72OEKV1I8HId09WTaj/oGmy57hbYjvoaT6qLsxW9QfefhlLx5K06iqdBV7hcFLNmpKUPK+NG5U1nTkuDzD86iqVMhS0RkT00ZUkaiO8OS+h0//2ZvPGs3ksq4HDTcuwfl+lpXE1z5IolJHx9YExj05PhoO+FmOqd8gpK3f075U1dDcv+/Xl4IL/wb1XceTtkLX8+GwQ//jIZLX6Fz5lUDZjin9H9utJrOg6+l8aKnafzoQ3QPPpDYazdTdefhlD95FZHZd+B01Be6zL2mIYKySwePiPO/50zhy3+Zy7UPzubWj02jOjZAnx8iIrIXpuYmo5i7rpWJg/Z9coGuVIZf/nMZkweXcuQY736xjsx/AAeXxDazhA04jo+2479PunIcsX99l8qmpTSfcTuZ8hF5LyW46p+Elj1LoGEBoZUv0D34QJpP/x2poYf23eGLIh5JDTuMlmGH4d80j+is2wmtfInw4scpffk7JEedSHL4MSRHn1QUU76rB0t264jRVdx8zlSWNXRw5T3vsmxTR6FLEhHp84bHI1REg7y/rnW/2rn/ndWsb+3iC8eNxefVL9luhsj8+0kOP6YgQaLPcRw6Z15F81l34mtbQ+UDZxC2D0ImnZ/9uxlK3riF+F8vJDr3j/haVtB+2H/S9LGHSQ07TOFKBpR09WTaTriZhstepeHiv9M5/QoCG+dQ9tK3qLrrWEqf/y+Cy5/HX/8+ZFKFLneHFLBkjxw1popfXzCDRHeaK+95l7dWFvfYWBGR3uY4DtOGle9XwEqmMvzh9ZUcNaaSQ0bGPastuPoV/C0r1Hu1je6Rx9N0/t9Il9ZR/uwXqbz3ZHxta3pvh65LaPFjxB84k9jrPyJhzqf+07Np/ORLdBz6RfBpoJEMbOmqibQfcwMNl7/Gpk/+k87plxOZ/2fij15K1X2nUH3HwZT+43r8m+YXutStKGDJHps6tJzbL5lJTSzEtQ/O5rG56wtdkohInzajroLF9e0kuvetJ+SVZY00J1JccGCdp3VF5t1LJlRO19jTPG23P0jHx9J0weM0n/Zr/K2rKXvuS70y+YWTaMpOFf/k53CSbbSc/BNaT7plQDyEVWRfZCpG0X7sTWy64g0aP/YXWj78M5J1RxOxD1B178lU/O0TBFe8AO7+Tyy0v/SnEdkrdRVRbrv4AL76yPvc+KRlUX071x47Br+H0waLiPQXM4ZXkHZhwcZ2Zgzb+xkAn7EbqIgEONzD3iunq5nw4sdJTL5Iv8zvjOMjOe5M2hJNlP3jq0Rn3e7ps7Kc1W9S+ecr8bWvp+3oG+icceXAnWhEZC+50WpS0WpSQw+ha+JHaUs0Ep1zF5HZvyf+t0+QLh9F17gzSEz6OOmqiQWpUT1YstfKI0F+dt50Pj5zGHe9uYp/u/891rUkCl2WiEifM70uO+vf7DUte/3eRHeaFxdv4sSJNQT83l2uwwsfwUl3aXjgHkhMuYSu0R8m9q/vEnn/3v1v0HWJvncb/jvPBKDpYw/ROfOzClci+8GNVNJxyHU0XPYKLSf9hHR8NNH3fkvVPSdS8ZcLCC1+PO/3ailgyT4J+H185aTx3HiawW5o5+I73+Lp+RsKXZaISJ8yuDzCuJoSnl+499MMv7ykgc7uDKeYQZ7WFJl3L6nqyaRqp3vabr/kOLSefCvddUdT9vx/UvL6j/a9qUQT5U98htKXb8Qd/2EaL3iS1OADvatVZKDzh+madD7NZ9/Npiveou2Ir+FvXk7Fk1dRef9p+Zu0BgUs2U9nTh3M3ZcdxJiqEr7x2HxufGI+bV19c0YXEZFCOHXSIN5b08Ka5r3r6X9uwUaqYyEO9PDZV/769wlueC/be6WZ6faIGy6n+cw76Jx0IbE3biEy5869bMAlbB+i6k8nEFr+HG3H3Ej6/D/iRrwb9ikiW9v8fK2GS/9F8+m/o3PKJXntKVbAkv02PB7lNxfN5LNHjuSJeRv4xB/f5r3VzYUuS0SkTzhlUi3AXvXyu67LWyubOWJU3NN7XCPz78f1hUiY8zxrc0DwB2k74Qd0jT6Z0he/SclrNxNY/85ub6Z3uloof/KzlD/7BdJlw2g675HsvVwKtyL54fOTHHsqiRlX5ne3ed2b9FsBn8NVR43mNxceAK7LZ+59j5uetDR0JAtdmohIQdVVRJkxrJyn5m/c4/esakrQ2NnNjDrveq9IJ4nYP9M15hTciHcPLB4wfAFaTvkF3SOOo+TNn1L54NnEXvr2jkOWmyG05EniD5xBaNmztB31LZrOe4TUoBn5r1tE8k4BSzx1QF0Ff7r8YC47dDhPzNvAebe/wb1vryaV9n6KWxGRYnHqpEEsqm9n0cb2Pdp+Vm5SjH2ZeXBnQkufxpdo1OQW+yNYQvPZd7HpyvfomHElJbN/T+mL39wqZPk3zSP+wJlUPJGddbDp3AfoPPBzmshCZABRwBLPxUIBrjtuLPdcdjDThpTzo+cXc8Edb/LUvA1k+sCzCURE8u1kU4Pfgaf2cJjgrDUtxEJ+xlaXeFZDZN59pEuH0j3iOM/aHKjcaBXtx3yHjpmfIzrnD5S8cQu4GaLv3UblA2fhb1tHy8m30njJP0gNPbTQ5YpInilgSa8ZXV3CT8+bxo/PnUok6Oebj8/nE3e+zd8X1pPKKGiJyMBRVRLisFGVPDV/A+4e/KFp1poWpg8rx+fRvTq+tjWEVr5AYtIF6knxiuPQftQ3SUz6OLE3fkz8gbMofflGkiOOo+HiZ+ky54FPjxsVGYgUsKRXOY7DseOquevSg/jemZNIpjN89ZH3Oee3r/G7V5fTkugudIkiInlx2uRBrG3p2jL8b2faulIsrm/nAA+HB0bm/xnHzZCY9HHP2hSy07gf/wOSdUcSaFxA64f+Hy1n3I4brS50ZSJSQPrTiuSFz3E4ZdIgTpxYy0uLN/HQe2v51T+Xc+frqzh1ci0fNrUcNNzb2bJERPqSD42vJhzw8dT8jRywi8kr5qxtwcXD+6/cDJF595KsO5JMxWhv2pQP+EM0n/0nnO42TR4iIoACluRZwOdwwoQaTphQw8KNbdz15iqenLeBh2eto6okyAkTajh6TBWHjIwTDWoYi4j0H7FQgGPHVvGs3ciXjh9LwL/9IJKM6/LX2evxOzB1aJkn+w2ueQ1/y3LaD/sPT9qTHfAHcf0KVyKSpYAlBTOhtpTvnD6J609O86+lDTxj63ls7nr+/N5agn6HmXUVHDWmiiNHVzK2ugRHzw0RkSJ31tQhPLugnucW1HPq5EFbrXNdl1tfWMKzCzZy9dGjiIW8uURH5t1HJlRG19gzPWlPRER2TQFLCi4S9HPixFpOnFhLMpXh3dXNvLKskVeWNXDrC0u49QWoLQ1xyIg4EweVMqEmxtShZZSGdfqKSHE5ckwlIyuj3PvO6u0C1hsrmvjTW6u5YOYwrjx8pCf7cxJNhBc/SsKcD8GoJ22KiMiu6TdU6VNCAR+HjarksFGV/PuHxrK+tYtXlzXw+vImXl/RxBPzslMcO8C4mhjTh5UxaXAZ46pLOCgSLGzxIiK74XMcLpg5jP99fjFz1rYwbegH91m9ubIJv8/huuPGeNZjH7F/xkklSEz9pCftiYjI7ilgSZ82uCzMOdOHcs70oQA0dXRjN7Yxe00Ls9a08IzdyMOz1m3ZflBpiLE1McZVxxhbU8K4mhhjq0t0P5eI9BlnTRvML/+5jHveWs33zvogYM1e08LE2hgRr35euS6RuXfRPfhAUrXTvGlTRER2SwFLikq8JMjhoyo5fFT2ZuKM67K2JcGS+g7WtHczd1UTSzZ18MDK1STTHzxrZlhFhHHVJdnwVVPC2OoYo6tKCAf0pAIRya9YKMB5Bwzlj2+s4orDRzChtpRUxmXuulbOnjrEs/0E175GoHEhLSf+2LM2RURk9xSwpKj5HIe6iih1FVHi8RKamjoASGdcVjcnWFzfzuL6dpZs6mBxfTv/WtZIOveQY58Dw+NRRlZGGVoeYUhZmCHl4ezH5WGqYyHPHvIpItLT5YeN4K+z1/HTF5bys/Ons7i+nc7uDNO9fPbVnD+SCVfQNf5sz9oUEZHdU8CSfsnvcxhZmQ1PJ0yo2bK8O51hRWPnlsC1ZFMHq5o6eW91C61dqa3aCPodhpSFqYtHGRGPMjweyYa5eIThFRHvhvGIyIBTHgny6SNH8ePnF/OvpQ2saU4AMH2YN1OzOx31hBc/Tue0yzS5hYhInilgyYAS9PsYVxNjXE2MD5varda1daVY19LFutYEa1u6WNeSYE1zglVNCWavaaE9md5q+5pYiLqKSDZ4xaO5j7NBrDIa1LTyIrJL5x8wlAffXcP/PLOQCbUxqkqCDCuPeNJ2ZP59OJluTW4hIlIAClgiOaXhADw8daAAACAASURBVONrA4yvjW23znVdmjtTrG7uZFVTglXNnaxuSrCqOcEbK5p47P0NW21fEvRTF49QVxFhSHmEoeXhLf8PLYtQEdW3nshAF/T7+O6Zk/j0Pe/y8pIujh9f7c0fZtwM0bl3kxx2BOmqCfvfnoiI7BX9lieyBxzHIV4SJF4SZOrQ7e+R6EplWNOc+CCANXWyujnBisZOXl/eREf31r1fkYCPYfEog2KhLfd91VVEGFUVpaokRHkkoCGIIgPA5MFlfOn4cfzguUXM8Oj+q+DKF/G3rKD9iK960p6IiOwdBSwRD4QDPsZUlzCmumS7da7r0pxIsa4lwbqWLta2ZocfbupMsbKhgwWL22jo6N7ufbWloS33kY2qLNkyGUdNaYiKSEBDEEX6ifMOGMqQ8jAHDq/wpL3oe78jE62ha+xpnrQnIiJ7RwFLpJc5jkM8GiQeDTJp8Ac3sPec9TDRnWZVrserqbObxo4kK5sSrGjo5O8L6mlObD8BR20sRE1pmEGlYYbHI7l/UYbHo9SWagZEkWLhOA7HjK32pC1/w0LCK56n/dAvgT/sSZsiIrJ3FLBE+oBI0M/4mhjja7a//wugqbOblY2drG/tYmN7kvq2Lja2JdnYnsRuaOX5RfVbpp8HCPkdqmMhamLZXrBRVSWMqoxSF48ypCxMuXrARPql6Kzbcf3h7OyBIiJSEApYIkVgcw/Y9J2sT2Vc1rUkWN2UYGVTJ2uaE2zqSLKhtWunk3AMLg9v9eyvoeW5XrCKKBVRBTCRYuMkGonYB0hMPBe3pGb3bxARkV6hgCXSDwR8zpbhgYdTud369mSKFY2drG1OsK61K3svWEuC9a1dzF/fRmPn1veAlYb9DK+Ibhl2WFsaJuR3GDGojNqwn2EVEQI+BTCRviQy926cVILOAz5T6FJERAY0BSyRASAWCjB5cBmTB+/4IaaJ7jRrW7pY2dSZnQEx1xO2YGM7zy/atNXwQ8gGuhGVUUbnhh6OqopSE8ve9zWoLMyIeBS/AphI/qSTRGf/nuTwY0lXTy50NSIiA5oClogQCfp3OgtiKuPSkugmmcqQcHzMWdHAsoZOlm3qYEl9Oy8u3j6ARQI+xtfGmFAbY0TuIcx1FVHq4hFKw/qxIwND7MVv4QsH4PAben1f4UWP4m9fT9vxP+z1fYmIyK7pNx0R2aWAz6GqJARkZz4cXRbaan0qnWF1c4Kmzm5SGZc1zQkWbmxnwca2Hc6AWBEJMKwie8/XsIrcvx4fhwO+vH1uIr3JLanB/9rNBIedQPeI43pxRy7R924jFR9HctQJvbcfERHZIwpYIrJfAn5fdpbC3OuDR2y9vq0rxermRPZf7gHMq5sTLK5v5+Ulm0imt+79GlQaoi7X6zU8HmFCbSnja2LUxEKEFL6kiHTM/BwlC/5M6QvfoPHiZ3tt2vTQ8r8T3DiL1uO/D46+R0RECk0BS0R6VWk4gBlUihlUut26jOuyqT3Jms0BLPdvTVMnry1v5NG5ya22r4mFmDy4lDHVJQwuCzO4LMKQ8uxzwGIh/TiTPiYQIX3aDwnccz4l7/yKjkP+3ft9uBlKXvsh6fJRJCZd6H37IiKy1/brNxJjzDKgFUgDKWvtIR7UJCIDhM9xqC0NU1sa5oC6iu3WdyTTLNzYxpJNHTR0JFnR2Mm8dW28uryR7m16vqpzz/waGY8ysjLKiMrs/9NietiqFI479kQS486i5M2fkphwLpmKUbt/014IL3qMYP1cWk6+FfxBT9sWEZF948WffE+w1tZ70I6IyFZKQn4OqKvYLnxlXJeGjm7Wt3axriXBysZOVjR2srKpk5eWbKKh44Np5x0HBpeGtwpdY6pLmFATozoW0vO+pNe1H3MDoRXPU/rSt2g58w/Zk9IL6SQlr/2QVJWha8K53rQpIiL7TWNqRKTo+ByHmliImliIqUO2n3q+rSv73K+VjZ1s6EyxYF0LKxs7eXr+Rlq7Pph0Ix4NMr42xrjqEkZXZWdRHFtdQmVJaLs2RfZVpnQoHYd9mdJ/3kRo8WMkx5/lSbvRWbcTaF5K01l/BJ/fkzZFRGT/7W/AcoGnjTEu8Gtr7W92tbHf7xCPbz8N9N7y+32etJMvxVYvqOZ8KLZ6oXhqjgPDB5cD2ZrT6QwAbq7na9GGNuava8Gub8Oua+XRuetpT6a3vL+yJMi42lLG1cYYVR3LPsS5MsrIqhIqor07DKtYjnFPxVhzvnXOuJLwwr9S9uI3aKg7CjdatV/tOe0bKHnjJ3SNPpluzRwoItKn7G/AOsZau9oYMwh4xhgz31r74s42Tqddmpo69nOX2amivWgnX4qtXlDN+VBs9UL/qNkPmMoIpjICkwcB2eC1vrWLpQ0dLN3UwZJNHSzb1METc9bRss0084NKQ0yoLWVcTfY5X+NrY4yqjBL0ezN7W384xnurtnbHD8DeE8aY24GzgA3W2mk7WO8AtwJnAB3AFdbat/d5h/vKF6D1xB9Ref/plL70LVpP+b/9ai722g9w0l20H/1tjwoUERGv7FfAstauzv2/wRjzMHAYsNOAJSLSFzmOw5DyCEPKIxw5euuehdZEijUtCdY2J1jR2Mmi+nYW1bfz2vJGUrkHLPt9DiPj0S1DDMdUlzC2JsbIeFRTy/e+O4CfA3fuZP3pwITcv8OBX+b+z7t09SQ6Dv0isddupmv8WSTHnr5P7QQ2vEdk3v10Hvg50vGxHlcpIiL7a58DljEmBvista25j08BbvKsMhGRPqAsEsBEtp9mvjudYXljJ4s2trN0UztLNnWwqL6dfyyqJ5e78DswojLKmOoYY3uEr1GVJQpeHrHWvmiMGb2LTc4B7rTWusCrxpi4MWaotXZtfircWseBnye0+AnK/vF1GoYdgRup3LsGXJfSl76NG63pnWnfRURkv+1PD9Zg4GFjzOZ2/mStfdKTqkRE+rig38f4mhjja2JbLe9KZVi+ZahhNngtrm/nhW2CV108uiV0Tcw9J6yuIlKAz6TfqwNW9ni9KresIAELf5DWk35M5QNnUPrSt2n98M/26u3h+Q8QXPcWLSf+CDe070MrRUSk9+xzwLLWLgEO8LAWEZGiFw74mDiolInb9HglUxmWN2aD1+JNuQBW385Lizex+ZFeZeEAZkgZg0tDjKqMMrG2FDNI08kXQq9OyhQ/hMzRXyby0g8ITDsbd/IeTrHesobAP28kM+IIokdcTtTpnV7QYpu0pNjqBdWcD8VWLxRfzcVWL+SvZk3TLiKSB6GAjwm1pUyo3Tp4daUyLK5vx25ow25oY3lTgjeWN/LY3PVbtqkqCTJm81TyVdn/R1VFGVQWxqfgtTurgRE9Xg/PLdulXp+UaerVxO1T+B/7Eo3lM8jEhuy6Idel/NHrIJ2k8bibyTQn9ru2nSm2iVaKrV5QzflQbPVC8dVcbPVC/iZlUsASESmgcMDHlCFlTMk9z2vzD/+2rhQLNraxYEM7Cza0sbShgyfnbdhqOvnSsJ+pQ8oYX1PKqKrolgAWL+ndqeSLzCPAtcaYe8lObtFcqPuvtuIP0vrhn1J53ymUP/k5mj5yLwSjO908PP9+wiuep+2Y75CJj8ljoSIisrcUsERE+qDScICDhsc5aHh8yzLXddnUnmRZQycrGjuwG9qZu66VB99bQ1cqs2W7YRURJg8u3arHa2RVlGiw/z2M1hhzD3A8UGOMWQXcAAQBrLW/Ah4nO0X7IrLTtH+qMJVuLx0fS8vJt1L+5NWUP/U5Wk7/Hfi3D8e+tjWUvnwjyWGH0zmjz5QvIiI7oYAlIlIkHMehpjRMTWmYQ0Z+ELwyrsu6li6WNWQn1JiztpWFG9t5fuEHE2sADC0PMy43MceE2hjjarLP8Ap49AyvQrDWXryb9S5wTZ7K2WvJcWfSdvz/o+wfX6Ps71+m9eSfQM97q1yXsuf/CyeTovXEH2+9TkRE+iQFLBGRIudzHIZVRBhWEeGoMR88xyuZyrCyqZNlDR0sy81suKi+nVeWNZLOJa+g32F0VQkTamNMzQ1VHFIeoaokqPu78iQx9ZP4OhuIvfZDMtEq2o++AXLHPjLvXkIrXqD12P8mUzGqwJWKiMieUMASEemnQgEf42qyPVU9daczLGvoYOHGdhbXt7NwYzuvLW/i8fc3bNkmGvQxeXAZwyoihAM+Zgwr58QJNUT64TDDvqDj4OtwOjdR8t5tZKI1dB58Lb7W1cRe/g7JuiNJTL+80CWKiMgeUsASERlggv7tZzR0XZf1rV3YDW2sb02yvKGDuetaeWNFEx3JNH9+by3ff3YhI+JR6uJRpg4p46SJNQyP73xiBtkLjkP7MTfgSzRQ+ur38TctJbT6nzhuhtYTf6ShgSIiRUQBS0REcByHIeURhpRv/7DjjOvyzqpmnl9Yz+rmBIs2tvH8wnreWNHIz8+fUYBq+ynHR+uJP8bpaiY6/z6SdUfSeuKPyJSPLHRlIiKyFxSwRERkl3yOw8Ej4hw84oOJNZo6u4kE1KviOX+QljNux9dZv/tnY4mISJ+kgCUiInstHtWztnqNL6BwJSJSxPTnRxEREREREY8oYImIiIiIiHhEAUtERERERMQjClgiIiIiIiIeUcASERERERHxiAKWiIiIiIiIRxSwREREREREPKKAJSIiIiIi4hEFLBEREREREY8oYImIiIiIiHhEAUtERERERMQjClgiIiIiIiIeUcASERERERHxiAKWiIiIiIiIRxzXdfO5v43A8nzuUERE+rxRQG2hi9iGrlciIrKtPbpe5TtgiYiIiIiI9FsaIigiIiIiIuIRBSwRERERERGPKGCJiIiIiIh4RAFLRERERETEIwpYIiIiIiIiHlHAEhERERER8Uig0AXsDWPMacCtgB+4zVr7/QKXtB1jzAjgTmAw4AK/sdbeaoy5Efgs2WerAHzdWvt4YarcnjFmGdAKpIGUtfYQY0wVcB8wGlgGXGCtbSxQiVsYYwzZujYbC3wbiNOHjrEx5nbgLGCDtXZabtkOj6kxxiF7bp8BdABXWGvf7iM13wycDSSBxcCnrLVNxpjRwDzA5t7+qrX26j5Q743s5DwwxlwPfJrsef4Fa+1T+ax3FzXfB5jcJnGgyVo7s48c4539TOvT53Kh6XrVe3S98p6uVwWr90Z0vfKy3j5zvSqaHixjjB/4P+B0YApwsTFmSmGr2qEU8GVr7RTgCOCaHnXeYq2dmfvXZy5WPZyQq+2Q3OuvAc9ZaycAz+VeF5zNmmmtnQkcTPab4uHc6r50jO8ATttm2c6O6enAhNy/q4Bf5qnGbd3B9jU/A0yz1s4AFgDX91i3uMfxzusP0pw72L5e2MF5kPs+vAiYmnvPL3I/V/LtDrap2Vp7YY9z+s/AQz1WF/oY7+xnWl8/lwtG16u80PXKW3eg61VvuwNdr3pbn7leFU3AAg4DFllrl1hrk8C9wDkFrmk71tq1m9OvtbaVbJqvK2xV++wc4A+5j/8AnFvAWnbmJLLf0MsLXci2rLUvAg3bLN7ZMT0HuNNa61prXwXixpih+an0Azuq2Vr7tLU2lXv5KjA833XtzE6O8c6cA9xrre2y1i4FFpH9uZJXu6o599e0C4B78lrULuziZ1qfPpcLTNer/NP1aj/oetX7dL3qfX3pelVMAasOWNnj9Sr6+IUg1116IPBabtG1xphZxpjbjTGVhatsh1zgaWPMW8aYq3LLBltr1+Y+Xke2y7WvuYitv7n78jGGnR/TYjm/rwSe6PF6jDHmHWPMC8aYYwtV1A7s6DwohmN8LLDeWruwx7I+c4y3+ZlW7Odybyq6Y6DrVV7oepVful71Ll2vdqGYAlZRMcaUku06/aK1toVst+M4YCawFvhRAcvbkWOstQeR7S69xhhzXM+V1lqX7EWtzzDGhICPAA/kFvX1Y7yVvnhMd8UY8w2y3e935xatBUZaaw8EvgT8yRhTXqj6eiiq82AbF7P1L2B95hjv4GfaFsV2LsvWdL3qfbpe5ZeuV3mh69UuFFPAWg2M6PF6eG5Zn2OMCZL9wt5trX0IwFq73lqbttZmgN9SgK7eXbHWrs79v4Hs+PDDgPWbu0pz/28oXIU7dDrwtrV2PfT9Y5yzs2Pap89vY8wVZG90/UTuhxO5oQubch+/RfaG4okFKzJnF+dBXz/GAeBj9Lghvq8c4x39TKNIz+U8KZpjoOtV3uh6lSe6XvU+Xa92r5gC1hvABGPMmNxfgi4CHilwTdvJjUn9HTDPWvvjHst7jun8KDAn37XtjDEmZowp2/wxcArZ+h4BLs9tdjnw18JUuFNb/fWkLx/jHnZ2TB8BLjPGOMaYI4DmHt3ZBZWbDe0rwEestR09ltduvunWGDOW7E2iSwpT5Qd2cR48AlxkjAkbY8aQrff1fNe3CycD8621qzYv6AvHeGc/0yjCczmPdL3qJbpe5VXRfY/repU3ul7tRtFM026tTRljrgWeIjvt7e3W2rkFLmtHjgYuBWYbY97NLfs62VmkZpLtllwGfK4w5e3QYOBhYwxkz4k/WWufNMa8AdxvjPk0sJzszYx9Qu7C+mG2Po4/7EvH2BhzD3A8UGOMWQXcAHyfHR/Tx8lOE7qI7CxTn8p7wey05uuBMPBM7hzZPPXqccBNxphuIANcba3d0xt4e7Pe43d0Hlhr5xpj7gfeJzt05BprbTqf9e6sZmvt79j+/gzoA8eYnf9M69PnciHpetWrdL3qBbpeFaxeXa+81WeuV47rFs2QWhERERERkT6tmIYIioiIiIiI9GkKWCIiIiIiIh5RwBIREREREfGIApaIiIiIiIhHFLBEREREREQ8ooAl0kcZY443xjxa6DpERER2Rdcrka0pYImIiIiIiHhEz8ES2U/GmE8CXwBCwGvA54Fm4LfAKcA64CJr7cbcAwV/BZQAi4ErrbWNxpjxueW1QBr4ODACuBGoB6YBbwGftNbqm1ZERPaarlci+aEeLJH9YIyZDFwIHG2tnUn2YvMJIAa8aa2dCrxA9ontAHcCX7XWzgBm91h+N/B/1toDgKOAtbnlBwJfBKYAY8k+pVxERGSv6Holkj+BQhcgUuROAg4G3jDGAESBDUAGuC+3zV3AQ8aYCiBurX0ht/wPwAPGmDKgzlr7MIC1NgGQa+91a+2q3Ot3gdHAy73/aYmISD+j65VInihgiewfB/iDtfb6nguNMd/aZrt9HSbR1ePjNPqeFRGRfaPrlUieaIigyP55DjjfGDMIwBhTZYwZRfZ76/zcNpcAL1trm4FGY8yxueWXAi9Ya1uBVcaYc3NthI0xJXn9LEREpL/T9UokTxSwRPaDtfZ94JvA08aYWcAzwFCgHTjMGDMHOBG4KfeWy4Gbc9vO7LH8UuALueX/Aobk77MQEZH+TtcrkfzRLIIivcAY02atLS10HSIiIrui65WI99SDJSIiIiIi4hH1YImIiIiIiHhEPVgiIiIiIiIeUcASERERERHxiAKWiIiIiIiIRxSwREREREREPKKAJSIiIiIi4hEFLBEREREREY8oYImIiIiIiHhEAUtERERERMQjClgiIiIiIiIeUcASERERERHxiAKWiGzFGHOHMea7ha5DREREpBgpYEmfYoy5whjzcqHrEBERERHZFwpY4jljTKDQNUhWX/ha7KiGfanLGOP3piIRERGR3uO4rlvoGqQfMMYsA34JfAIwwCHAz4CZwGrgemvtI7ltK3LrTgc6gN8C/5N73ztAEOgEUtba+C72eUfu/WOAY4H3gPOArwGXA+uBi6217+S2H5bb73FAG3CLtfanuXWHAbcCk3P7/jPwJWttMrfeBf4N+DJQC9wNXGut3ek3kDFmPPC73DHoBp6z1l6YW/fhXC1DgT8C04E/WmtvM8bcCIy31n4yt+1oYCkQtNamjDGfAr4CDAc2Aj+w1v46t+3xwF25tv8DeMZae6kx5izgu8Bo4H3gamvtrNx7DszVOQF4HHCBRdbab+7sc8u9b1dtLmPr8yEGLNrBsgm5ZTs6T+7IfS1GAR8CzrHWPrurmkREREQKTT1Y4qWLgTOBGuBh4GlgEHAdcLcxxuS2+xlQAYwl+4vzZcCnrLXzgKuBV6y1pbsKVz1cAHwzt88u4BXg7dzrB4EfAxhjfMDfyIawOuAk4IvGmFNz7aTJBpIa4Mjc+s9vs6+zgEOBGbn9nsqu/XfuGFSSDUM/y9VSAzzUo+7FwNF78LlutiFXSznwKeAWY8xBPdYPAarIBpOrcgHqduBzQDXwa+ARY0zYGBMC/kI25FUBD5ANqbu0qzZ7bLb5fIhba1PbLgMcsl+TnZ0nAJcA3wPKAA0dFRERkT6v4MOHpF/5qbV2pTHmWKAU+L61NgP83RjzKHCxMea/gYuAmdbaVqDVGPMj4FKyvSh762Fr7VsAxpiHgc9ba+/Mvb4PuDa33aFArbX2ptzrJcaY3+ZqeWpzGznLjDG/Jhv+ftJj+fettU1AkzHmebK9Lk/uorZusiFnmLV2FR8EhDOAudbaB3N1/oRsz9gesdY+1uPlC8aYp8n24L2dW5YBbrDWduXavwr4tbX2tdz6Pxhjvg4cQba3Kgj8JNcb96Ax5kt7UMau2nwht+yn1tqV27xvy7JdnSfAjbnt/2qt/Wfu48Qe1CUiIiJSUApY4qXNv0wPA1bmfmnebDnZnqMasr/QL9/Bun2xvsfHnTt4XZr7eBQwzBjT1GO9H3gJwBgzkWxv1yFACdnvjZ6hC2Bdj487erS9M18h24v1ujGmEfiRtfZ2csdn80bWWtcYs20Q2SljzOnADcBEsr3QJcDsHptstNb2DCOjgMuNMdf1WBbK1eECq7cZ6tjza7Mzu2pzsx19Tj2X7eo82VUbIiIiIn2WApZ4afMv6WuAEcYYX49fnkcCC4B6PujZeb/HutXbtOG1lcBSa+2Enaz/Jdn7vy621rYaY74InL8/O7TWrgM+C2CMOQZ41hjzIrAWGLF5O2OM0/M10E42NG02pMe2YbL3h11Gtnen2xjzF7LD7Tbb9hiuBL5nrf3etjUaYz4E1BljnB4hayTZYYu7stM2d1HHtst2dZ7sqg0RERGRPksBS3rDa2R7eL6SG/53NHA2cKi1Nm2MuR/4njHmMrL3/XwJ+N/ce9cDw40xoc0TTHjkdbLDEb8K/BRIkp3QImqtfYPsPT4tQJsxZhLZCS027s8OjTEfJ3s/2SqgkWxYyACPAT83xnwMeAS4hh4hCngX+KoxZiTQDFzfY10ICOdqS+V6s04B5uyilN8CDxtjniV7HEqA44EXyd6zlgK+YIz5Bdmv02HA87v59HbaZm7o557Y6Xmyh+8XERER6XM0yYV4LheMziY7S2A98AvgMmvt/Nwm15HtpVlC9r6kP5GdMAHg78BcYJ0xpt7DmtJkJ4aYSXZGvnrgNrKTbQD8J9kJFVrJhof7PNjtocBrxpg2skHq3621S6y19cDHge8Dm8jOpLf5PiOstc/k9j+L7DDFR3usawW+ANxPNrRdkmt7p6y1b5LtSft57j2LgCty65LAx3KvG4ALyU7AsUu7anNP7cF5IiIiIlJ0NE27SB9gjPkHcJe19rZC1yIiIiIi+049WCIiIiIiIh7RPVjSpxlj5pKdEGNbn7PW3p3verZljPkV8MkdrLrLWnt1vuvxUm7a9a/vYNVL1trT812PiIiISDHQEEERERERERGPaIigiIiIiIiIR/I6RDCTybjp9P73mPn9Dl60ky/FVi+o5nwotnpBNedDsdUL+19zMOivB2q9q0hERKRw8hqw0mmXpqaO/W4nHi/xpJ18KbZ6QTXnQ7HVC6o5H4qtXtj/mmtry5Z7WI6IiEhB7TZgGWNuJ/v8oA3W2mm5Zf8NnEP2oakbgCustWt6s1AREREREZG+bk/uwboDOG2bZTdba2dYa2eSfQjqt70uTEREREREpNjsNmBZa18EGrZZ1tLjZQworhsGREREREREesE+34NljPkecBnQDJywJ+/x+x3i8ZJ93WWPdnyetJMvxVYvqOZ8KLZ6QTXnQ7HVC8VZs4iISG/Z54Blrf0G8A1jzPXAtcANu3uPJrkoHqq59xVbvaCa86HY6gVPJrnwsBoREZHC8uI5WHcD53nQjoiIiIiISFHbp4BljJnQ4+U5wHxvyhERERERESleezJN+z3A8UCNMWYV2aGAZxhjDNlp2pcDV/dmkSIiIiIiIsVgtwHLWnvxDhb/rhdqERERERERKWpe3IMlIiIiIiIiKGCJiIiIiIh4RgFLRERERETEIwpYIiIiIiIiHlHAEhERERER8UjRBaz4n8/BefeuQpchIiIiIiKynaILWJlwHP8zX8fXtrbQpYiIiIiIiGyl6AJW27E3QSZF6cs3FroUERERERGRrRRdwMpUjCJz9JcJL36M0PK/F7ocERERERGRLYouYAFkjriWVHwcpS9+C1KdhS5HREREREQEKNKARSBM24f+B3/Lckre+nmhqxEREREREQGKNWAB3cOPJjHxY5S8/Qt8TUsLXY6IiIiIiEjxBiyA9qO+Ab4gpa98r9CliIiIiIiIFHfAysQG03HwNYSXPElw9b8KXY6IiIiIiAxwRR2wADpmXkW6dBixl2+CTLrQ5YiIiIiIyABW9AGLQJT2I68nWD+HsH2w0NWIiIiIiMgAVvwBC+iacC7dgw8k9uoPINle6HJERERERGSA6hcBC8eh7Zgb8XdsoOSdXxS6GhERERERGaD6R8ACUkMOJjH+bEre/S1OR32hyxERERERkQGo3wQsgI7D/hPSXZS89bNClyIiIiIiIgNQvwpY6cpxJCZ9nOicP+JrXV3ockREREREZIDpVwELoOPQ/wCg5I1bClyJiIiIiIgMNP0uYGXK6uicdimR+ffjb1xc6HJERERERGQA6XcBC6Dj4OvAH6Hk9f8tdCkiIiIiIjKA9MuA5ZbU0DHzs0QW/Q1/A8oGRQAAIABJREFU/fuFLkdERERERAaIfhmwADoP+CyZYIySt/+v0KWIiIiIiMgA0W8DlhuJk5h2GeFFf8PftKTQ5YiIiIiIyADQbwMWQMcBnwVfkOg7vyx0KSIiIiIiMgD064DlxgaRmHwhkfkP4mtbU+hyRERERESkn+vXAQug48Crwc0Qffe3hS5FRERERET6uX4fsDLlI+maeC7RuXfhdDYUuhwREREREenH+n3AAug46BqcVCfRWb8rdCkiIiIiItKPDYiAla6aSNeYU4nOuRNSnYUuR0RERERE+qkBEbAAOmdciS/RSHjhI4UuRURERERE+qkBE7C6644iVWWIzvo9uG6hyxERERERkX5owAQsHIfO6VcQrJ9DYN1bha5GRERERET6oYETsIDExI+RCZUTnf37QpciIiIiIiL90IAKWIRiJCZfSHjxY/ja1xW6GhERERER6WcGVsACOqddBpk0kTl3FboUERERERHpZwZcwMrEx5AcdQLRuXdDOlnockREREREpB8ZcAELoHP6p/B1biS85IlClyIiIiIiIv3IgAxY3SM/RLpsOJH37y10KSIiIiIi0o8MyICF4yMx6QKCq17G17Ky0NWIiIiIiEg/MTADFpCYfCEAkfn3F7gSERERERHpLwZswMqU1dE94jgi8+6HTLrQ5YiIiIiISD8wYAMWZHux/G2rCa7+Z6FLERERERGRfmBAB6yusaeSCcc12YWIiIiIiHgisLsNjDG3A2cBG6y103LLbgbOBpLAYuBT1tqm3iy0V/jDJCZ+lOjcu2lLNOJGKgtdkYiIiIiIFLE96cG6Azhtm2XPANOstTOABcD1HteVN4kpF+NkkoQXPFzoUkREREREpMjtNmBZa18EGrZZ9rS1NpV7+SowvBdqy4t0zRS6a2cQmXdfoUsREREREZEit9shgnvgSmCP0onf7xCPl+z3Dv1+nyftbOY78GL8T19PPLUSaoxn7W7mdb35oJp7X7HVC6o5H4qtXijOmkVERHrLfgUsY8w3gBRw955sn067NDV17M8uAYjHSzxpZzPfsA9TxddJvn0/HYd92bN2N/O63nxQzb2v2OoF1ZwPxVYv7H/NtbVlHlYjIiJSWPs8i6Ax5gqyk198wlrrelZRAWRiQ+iuO4Lwor+BW9SfioiIiIiIFNA+BSxjzGnAV4CPWGuL60+tO9E1/iMEGhfh3zSv0KWIiIiIiEiR2m3AMsbcA7yS/dCsMsZ8Gvg5UAY8Y4x51xjzq16us9d1jTsD1/Fne7FERERERET2wW7vwbLWXryDxb/rhVoKyo1W0z38GCILH6Hj8K+A4xS6JBERERERKTL7fA9Wf9Q1/mz8LcsJbJxV6FJERERERKQIKWD10DX2NFxfkPDCRwpdioiIiIiIFKGiCliu63Ldn2dzxyvLeqf9SJzkyA/lZhPM9Mo+RERERESk/yqqgOU4DtUlQb73+Hz+9NaqXtlH1/iz8betIbDu7V5pX0RERERE+q+iClgA3zxlIqdOGcwt/1jCrDUtnrefHHMKrj9MePFjnrctIiIiIiL9W9EFrIDfx/c/Np2g3+G5BRs9b98NlZEcfjThpU/rocMiIiIiIrJXii5gAZSGAxwyIs5Lizfh9kIISo4+BX/LcvwN1vO2RURERESk/yrKgAVw3LhqVjYlWN7Q6XnbyTEnAxBe+oznbYuIiIiISP9VtAHr2HHVALy4eJPnbWdiQ+geNJPQ0qc8b1tERETk/7N33/FxlQe6x39n+ox6r5ZkW/JIcgVjbEzAxhgCoddkIZ1ANgVCbthN203C7iZLWHJvLuxNgAQCiwGHGiCUgG0MhGLcbWx53OUq2epdM3Nm7h8yso0LlnSk0UjP9/PhY+nMOe95ZuLYevye8x4RGbnitmDlJLkpz04clIIFEBz7WZwH1mBrrxmU8UVEREREZOSJ24IFcG5pBuv2tbCxptXysbvHXgCAa8ciy8cWEREREZGRKa4L1hdOKyAz0cWdrwUIhq19MLCZ7sdMLtZlgiIiIiIicsriumAleRz89MIJbK/v4A/vV1s7uGHQPfZCXHvehWC7tWOLiIiIiMiIFNcFC+Dssel8rjKbJ1ftpbEjaOnYwbEXYkSCuHYvtXRcEREREREZmeK+YAF87cwiusMRnlmz39JxQ3kziLhTex46LCIiIiIi8ilGRMEqyfDxmXHpPLVmH10h07qBbQ6CJefj2rkYImHrxhURERERkRFpRBQsgC+eUUhTZ4i7Fm3hra31hE1rFr3oLrkAW3cTjppVlownIiIiIiIj14gpWKcXpvDZ8ixeqzrAHS9s4K8bai0ZNzTmXKI2B+7qxZaMJyIiIiIiI9eIKViGYfAfl1Sw9NazSXDZ2XLQmpX/ou5kQnkzcFUvsWQ8EREREREZuUZMwfqYx2mnJN3H9oYOy8YMFs3DUV+FrXWfZWOKiIiIiMjIM+IKFsDYDB876i0sWCXnA+DapVksERERERE5sRFZsMZl+KhvD9LcGbJkPDOtDDOpENdOFSwRERERETmxEVmwxmb4ANhp1WWChkGw+Hxce94Bs9uaMUVEREREZMQZ0QXL0ssEi+dhhDtx7v3AsjFFRERERGRkccQ6wGDIS/bgdtjY0dDBnqZOnl6zj/agyeyx6cwry+zXmMGC2UTtblzVSwgVzbE4sYiIiIiIjAQjsmDZDIOx6T621bXz45eq2FrXjs2ANXua+12wcHoJFszGVb2Y9nPutDawiIiIiIiMCCPyEkHouUzww+omNh1o42cXTeBrM4vY1dhJezDc7zGDJefjaN6JvWm7hUlFRERERGSkGNEFKwpU5ibx2fJsKnKSiAKBA239HjNYPA9ADx0WEREREZHjGrEFa0p+MnabwffnjMNmGJTnJAKwqbb/BSuSXEQ4rUwFS0REREREjmvEFqzpY1JZ/J2zmFaYAkBGgovsRBdVAyhY0DOL5dz7AQTbrYgpIiIiIiIjyIgtWAAJrqPX8CjPSWJTbeuAxgwWz8OIBHHt+fuAxhERERERkZFnRBesTyrPSaS6YWALXYTyZhBxJuKqXmxhMhERERERGQlGVcGqyEkkCmw+MIDL++wuQkXn9tyHFY1alk1EREREROLfqCpY5TlJAGyoGeBlgkXzsLfXYK+vsiKWiIiIiIiMEKOqYGUmuKjISeSx5btp7Aj2e5xg8XmAlmsXEREREZGjjaqCBfCvn51Aa3eY/1y0lWg/L/GLJOQQypqMW/dhiYiIiIjIEUZdwSrLSuQfZ5fw5pY63ggc7Pc4weJ5OGpWYnQ1WphORERERETi2agrWAA3nlFIRU4iv31re79XFAwWz8OIRnDtesvidCIiIiIiEq9GZcGy2wx+eH4pdW1BHnyvul9jhLOnEfGka7l2ERERERHpNSoLFsDEvGSumJzLn1ftpba1u+8D2OwEi+bi2rUUIqbl+UREREREJP6M2oIFPZcKmlFYuqWuX8cHS87H1tWI48Aai5OJiIiIiEg8GtUFqyTdx7gMH29u7WfBGnMuUcOm5dpFRERERAQY5QUL4LyyTFbvae7Xc7GinjTCuWfg2qn7sERERERERAWL88oyiUTh7W31/Tq+u3gezrqPsLXXWJxMRERERETizagvWBOyEshP8bCkv/dhFc8DwFX9ppWxREREREQkDo36gmUYBueXZfJhdRNNHaE+H29mVGAm5uk+LBERERER+fSC5ff7H/b7/Qf8fv9HR2y7zu/3b/D7/RG/33/G4EYcfBdXZhOORHk9cLDvBxsGwaJ5OHe/A2bf7+MSEREREZGR41RmsB4BLvrEto+Aq4G3rQ4UC2VZiZRlJfDKxtp+HR8sOR9bqA3n/uUWJxMRERERkXjyqQUrEAi8DTR8YltVIBAIDFqqGLikMocNNa3sbOjo87HBgrOJ2lxaTVBEREREZJRzDOXJ7HaD1FSfBePYLBnnSNfNLOLet7ezZHsD/2tcZh+P9hEtORvvnjdxpt51zKuDkXewKfPgi7e8oMxDId7yQnxmFhERGSxDWrBMM0pTU99niD4pNdVnyThHcgEzi9N4ftVevjq9AJth9Ol4b/4cErf/gpbqKiIpxUe9Nhh5B5syD754ywvKPBTiLS8MPHNWVpKFaURERGJr1K8ieKRLKnOoae1m9Z7mPh/bXXw+gFYTFBEREREZxVSwjjCnNIMEl71fi11EUscSThmLu1r3YYmIiIiIjFanskz7k8D7PV/69/j9/pv8fv9Vfr9/D3AW8LLf7//bYAcdCh6nnXllmSzeXEdXyOzz8cGS83HufR9CnYOQTkREREREhrtPvQcrEAj8wwleet7iLMPCJRNzeGlDLW8EDnLZpNw+HRssnodv7R9x7X2XYMn8QUooIiIiIiLDlS4R/ITTClMoz07k3rd3UN/etwcHh/JnEnX4dB+WiIiIiMgopYL1CTbD4M7P+ekIhvnl65uJRqOnfrDdTXDMOT3Pw+rLcSIiIiIiMiKoYB3HuIwEvnPOWN7Z3sCHu5r6dGyweB72tr3YGzYPUjoRERERERmuVLBO4OopebjsBu9ub+jTccHi8wBwVS8ajFgiIiIiIjKMqWCdgMdp5/Qxqby3o28FK5KYTyhrCu5trw5SMhERERERGa5UsE5i9th0qhs72dvct2XXu0svwXlgDbaWPYOUTEREREREhiMVrJOYXZIGwHs7Gvt0XPf4SwFwb3vZ8kwiIiIiIjJ8qWCdRFGal4IUD+/39TLBlGJCWZNxb/vrICUTEREREZHhSAXrJAzD4KySNJbvaiIYjvTp2O7xl+CsXY2tde8gpRMRERERkeFGBetTzCpJpyscYf3+lj4d112qywRFREREREYbFaxPMX1MCnYDPqzu231YkZQSQpmTcG/VZYIiIiIiIqOFCtanSHQ7mJiX3OcHDkPPLJazdhVoNUERERERkVFBBesUnFmUysaaVlq7wn06rnv8JQDYql4cjFgiIiIiIjLMqGCdgjOL04hEYeXuvs1iRVLHEsqciFH1wiAlExERERGR4UQF6xRMykvC67T16zLB4PhLse1djq113yAkExERERGR4UQF6xQ47TZOK0xhRR9nsAC6S3suE3Rvf8XqWCIiIiIiMsyoYJ2iSbnJ7KzvoDNk9uk4M3Uc0WytJigiIiIiMhqoYJ0if04iUWDzgbY+HxupuAJnzQpsbbpMUERERERkJFPBOkUVOYkAbKrtX8ECcG/TZYIiIiIiIiOZCtYpykxwke5zUtWPGSwySglnVODe9rL1wUREREREZNhQwTpFhmFQnpNIoB8zWHDoocP7l2Nr229xMhERERERGS5UsPqgPCeJHfXtdPVxoQuA7vGXArpMUERERERkJFPB6oPy7ETMKGyta+/zsWbaeMIZ5bpMUERERERkBFPB6oPyASx0AT2zWI79y7G111gZS0REREREhgkVrD7ITXKT4nFQVdvar+O7Sy/FIIpLlwmKiIiIiIxIKlh9YBgGk/KSWb+vfwXLTCslnFGOZ8uLFicTEREREZHhQAWrj6bkJ7OjoYOWrlC/ju+acFXPQ4ebd1obTEREREREYk4Fq48m5ycBsH5/Py8TnHAVUQw8gWetjCUiIiIiIsOAClYfTcxNxmbA+n0t/To+kphPqPBsPIHnIBq1OJ2IiIiIiMSSClYf+Vx2SjMTWNfPggXQ5b8Ge0s1jpqVFiYTEREREZFYU8Hqhyn5yWzY34oZ6d8MVHDcxUQdHl0mKCIiIiIywqhg9cPk/GQ6Qibb6/v+wGGAqCuR7rEX4d76IpjdFqcTEREREZFYUcHqh2kFKQA8u3Z/v8foKr8WW3czrp2LrYolIiIiIiIxpoLVD/kpHm6YXsCza/fz0kc1/RojVPgZTF+2LhMUERERERlBVLD66dZzx3FGUSp3LdpCTUtX3wewOeguuxJX9RKMrkbrA4qIiIiIyJBTweonh83gn+eVEjSjvL+zfwWpq/xajEioZ8l2ERERERGJeypYA1CS7iUjwcXK3U39Ot7MrCSUPRXPxif0TCwRERERkRFABWsADMPgjDEprNjdTLSfBamr8h9wNARw1K6yOJ2IiIiIiAw1FawBmj4mlfr2IDsbOvt1fHfZlUQdPjwbnrA4mYiIiIiIDDUVrAE6Y0wqACv6eZlg1JVI14Qr8Gx9ESPYamU0EREREREZYipYA1SY6iE7sf/3YQF0Vd6AEe7EvfkvFiYTEREREZGhpoI1QIZhMKMolRW7mjAj/bsPK5w9jXBGRc9iFyIiIiIiErdUsCwwe2w6zV1h1u9r6d8AhkFn5Q04D67HcXC9teFERERERGTIqGBZYPbYdBw2g6Vb6/s9RveEq4ja3VrsQkREREQkjqlgWSDR7WBGUSpLt9b1e7n2qCeV7tJLcW9+HkIdFicUEREREZGhoIJlkbllmext7mJrXXu/x+iqvAFbqA331pcsTCYiIiIiIkPFEesAI8Wc8Rnc9cYWlm6ppywrsV9jhPLOJJxWinfjE3RXfN7ihCLSF6YZprHxIOFwcEjPW1tr9HsmPFZONbPD4SItLQu7XX/1iIjIyPWpf8v5/f6HgUuBA4FAYNKhbenAn4ESYCdwfSAQaBy8mMNfRoKLSXnJvL+zkZtnF/dvEMOgq+IfSHzv37HXBzAz/NaGFJFT1th4EI/HR0JCLoZhDNl57XYbphkZsvNZ4VQyR6NR2ttbaGw8SGZm3hAlExERGXqncongI8BFn9j2I2BxIBAoAxYf+n7Uq8hJZFtd+4D+9bmr/FqiNieeDQssTCYifRUOB0lISB7ScjWSGYZBQkLykM8IioiIDLVPLViBQOBtoOETm68AHj309aPAlRbnikvjsxLoCJnsb+nu9xhRbwbdpZfh2fQURnezhelEpK9Urqylz1NEREaD/l4InxMIBPYf+roGyDmVg+x2g9RUXz9PeeQ4NkvGsdq0knQAarrCVBYfztfnvOfchm3zc6TteIbIrFutjnlKhutnfDLxljne8sLoylxba2C3x2YdoFiddyBONbNhWPP3gIiIyHA14DuNA4FA1O/3n9I1caYZpalp4EuQp6b6LBnHatluOwDrdjZwes7hhS76nNdTSkrBbOzL7qep7Mtgd1od9VMN18/4ZOItc7zlhdGVORqNxuReqCPvZ2ptbeWNN17j6quv69MYd9xxGz//+S9JSko64T5//OP9TJ16GjNmzBxQXujbfWPR6LF/D2RlnTiniIhIvOnvP5PW+v3+PIBDvx6wLlL8SnQ7yE1yD2ip9o91TrsFe9t+3NtetiCZiMSjtrZWnn/+6WO2h8Phkx53zz33nrRcAXzjG/9oSbkSERGRo/V3ButF4CvAXYd+fcGyRHFufGYC2+sH/i/8weJ5hFPH413zIN1lV4DuXRCJmZc31PLiRzWWjnn5pFwumXjyq6vvv/8+9u7dy1e/egMOhwOXy0VSUhLV1dUsXPgcP/7xD6itrSUYDHLddV/giiuuBuDaay/jj398jM7ODu644zamTJnG+vXryMrK4q67foPb7eGXv/wFs2d/hvPOm8+1117GxRdfyrvvvk04HObf//3XFBeX0NjYyJ13/pS6ujomTZrM8uXLeOihBaSmplr6WYiIiIwknzqD5ff7nwTe7/nSv8fv999ET7G6wO/3bwHmH/pegPGZPnY2dBAe6KVFho3OqTfjPLgO5/5l1oQTkbjyj/94KwUFBTzyyBN8+9u3sXnzJr73vTtYuPA5AH7845/x8MMLeOih/+GZZxbS3Nx0zBh79uzm6quvY8GCp0hMTGLp0iXHPVdKSgoPP/w4V155LU8++RgAf/rTg0yfPoMFC55i7tzzqa21tmSKiIiMRJ86gxUIBP7hBC+db3GWEWF8ZgIhM8qupk7GZSQMaKwu/zUkLPs13tUPEsqfZVFCEemrSybmfOps01CoqJhIfn5B7/dPP72Qt99eCsCBA7Xs3r2blJSjZ5fy8vIpK+t5pp7fX87+/fuOO/acOfMO7VPBW2+9CcC6dWv51a/+C4BZs2aTlJRs6fsREREZieJvqaphbnxmT6naVmfBQgBOL52Tvoxr5xvYm7YPfDwRiWter7f361WrVrBixYc88MCfePTRJykr8xMMHvuICKfz8CI5Npsd0zSPO7bT6QI+XrDi5Pd4iYiIyImpYFmsJN2H3YCtB9ssGa9z0lfA5sS79iFLxhOR+OHz+ejoOP4/1rS3t5GUlIzH46G6eicbN35k+fknT57KkiVvAPDhhx/Q2tpi+TlERERGGhUsi7kdNsqyElmz15ofRKIJ2XRNuArPpj9jdDVaMqaIxIeUlFQmT57Kl750Pb/73b1HvTZz5mxM0+TGG6/l/vvvo7JykuXn//rXb2b58mV86UvX8+abi8jIyMDn0zOsRERETsaIRk/pEVaWCIXM6Eh+DtbH/s/SbTyzZh9Lvns2bodtwHnt9ZtIXzif9pk/pOOMoXnw8HD/jI8n3jLHW14YXZlraqrJzS0ehEQn15dnSg22YDCIzWbD4XDw0UfruOeeu3jkkSeO2a8vmY/3uWZlJa0EzrAis4iISKwN+EHDcqzTC1N5YuVeNtS0cHrhwJczNjPKCY6Zg2f9n+g47Rawuy1IKSJycrW1NfzsZz8iEonidDr54Q9/GutIIiIiw54K1iA4rTAZA1i1u5nxGQks3bmXOcWpGAN4llXHtFtIfelG3FtepLv8OuvCioicwJgxRfzpT8fOWImIiMiJ6R6sQZDscVKalcDKPc38298280/PrmfzgfYBjRkacy7hdD++NQ/CEF7WKSIiIiIip04Fa5CcXpjCyl1NvL2tHoA1e5sHNqBh0Dn1Zhz1VTj3/N2ChCIiIiIiYjUVrEEyfUwqUWBibhK5yZ6BFyygy38VEW8W3jUPDjygiIiIiIhYTgVrkMwsTuNzldn84mI/M0rSWLO3hQGv2Gh30zn5K7h3vYm9YbM1QUVERERExDIqWIPE57Jz58XllKT7mF6cRl17kL3NXQMet3PSl4na3XjX/sGClCIyklxwwTkA1NUd5F/+5Z+Pu893v3sLmzZtPOk4Tz31BF1dh/+8uuOO22htbbUuqIiIyAimgjUEZhSnARbchwVEvel0lV+HJ/AcRsfBAY8nIiNPZmYW//Efd/f7+KeeevKognXPPfeSlJRkRTQREZERT8u0D4HSrESSPQ7W7G3h0om5Ax6vc+o38G5YgHf9I3TM/CcLEorIybg3PYOnaqGlY3ZVfIHu8mtPus/vf38f2dk5XHPN9QA89NAD2O12Vq9eSWtrC+FwmJtv/hbnnDP3qOP279/HP//z7Tz22FN0d3fxq1/dydatWygqKqG7u7t3v3vu+U+qqjbS3d3Neeedz003fZOnn15IXd1Bbrvtm6SkpHLffQ9w7bWX8cc/PkZqaioLFy7g5ZdfBOCyy67k+utvYP/+fXz/+99lypRprF+/jqysLO666ze43R5LPzMREZF4oBmsIWCzGUzJT2bFriYiFiyxbqaV0j3uIrzr/oTRPfBZMREZns4//wLefHNR7/dvvrmIiy++lF/96r94+OHHuffeB/jv//7tSe/vfP75Z3C7PTz++DPcdNM32bx5U+9rt9zybR566DEeffRJVq9eydatW7juui+QmZnFvfc+wH33PXDUWJs2VfHKKy/x4IOP8sADj/Dii3/pHW/Pnt1cffV1LFjwFImJSSxdusTiT0NERCQ+aAZriFxckc1PX97E3zYd4OKKnAGP13HG7aRtfw3vuj/RMeN2CxKKyIl0l1/7qbNNg2HChHIaGxuoqztIY2MjSUlJZGRkcu+9v2Ht2tUYho2DBw/S0FBPRkbmccdYu3Y11177BQBKS8sYP76097UlS97gxRefxzRN6uvr2LlzO6WlZSfMs27dGs499zy8Xi8Ac+acx9q1a5gzZy55efmUlfkB8PvL2b9/n1Ufg4iISFzRDNYQme/PYkJWAve/W03IjAx4vHDWJLpLLsC79g8YQd18LjJSnXfefN58czFLlrzBvHkX8vrrr9LU1MRDDy3gkUeeID09nWAw2Odx9+3by5NPLuC3v/09jz66kLPO+ky/xvmY0+ns/dpms2OaZr/HEhERiWcqWEPEZhh855yx7Gvu4vl1+y0Zs2PG7di6m/Gsf9SS8URk+Jk37wIWL36dN99czHnnzaetrY20tDQcDgerVq2gpubkf55MnXoab7zxGgDbt29l27atALS3t+PxeElMTKShoZ4PPniv9xifz0dHR/txx3rnnaV0dXXR2dnJ22+/ydSp0yx8tyIiIvFPBWsInVWSRkVOIq9sPGDJeOHsqXQXz8O35gEIHvvDkIjEv3HjxtPR0U5WVhaZmZlceOHFbNpUxZe//Hlee+1liotLTnr8VVddS2dnBzfeeC1//OMDTJhQDkBZ2QQmTPBzww3Xcued/8LkyVN7j7n88qv4wQ9u5dZbv3nUWH5/ORdffCk33/xlbrnlK1x22ZW944mIiEgPY8APv+2DUMiMNjV1DHic1FQfVowzVI7M+//e2cFjK/bw5ndn43XaBzy2o2Ylac9eQdtZP6Xz9G8NeLyPxdtnDPGXOd7ywujKXFNTTW5u8SAkOjm73YZpwWXEQ6kvmY/3uWZlJa0EzhiEaCIiIkNOM1hDbFphCmYkyvp9LZaMF86dTnDMHHxr7odQfP3gKyIiIiIy0qhgDbGp+cnYDFi9x7rl1dtn3I6tsx7vhgWWjSkiIiIiIn2ngjXEEt0OyrISWbPXuoIVzptBsPAz+Fb9HsKdlo0rMtoN5SXUo4E+TxERGQ1UsGJgWkEy6/e3WrJc+8c6ZtyOrfMg3g1PWDamyGjmcLhob29RKbBINBqlvb0Fh8MV6ygiIiKDSg8ajoHTClP48+p9VNW2MSU/2ZIxQ/mzCObPwrvqd3RW3gBOryXjioxWaWlZNDYepK2taUjPaxhG3JW6U83scLhIS8vOEe/vAAAgAElEQVQagkQiIiKxo4IVA9MKUgB4dWOtZQULoH3mP5P2/NV41z1E5/TvWjauyGhktzvIzMwb8vOOppUaRURERiJdIhgDGQkurp+WzzNr97N0S51l44bzz6S7ZD6+Vb/D6Gq0bFwRERERETk1Klgx8r0546jISeTOvwWoaw9aNm77rB9hBFvxrbjPsjFFREREROTUqGDFiMth44fzy2jrNi1dst3MKKer8gt41z+MvX6TZeOKiIiIiMinU8GKofEZPgB2NVp770L7WT8h6koi6a2fQNS6lQpFREREROTkVLBiyOO0k5PkZlejtc+uinrSaD/rpzj3f4in6ilLxxYRERERkRNTwYqxojSv5QULoKviekJ5Z5Lw/i8xOhssH19ERERERI6lghVjg1WwMGy0zvkVRrCVhPd/af34IiIiIiJyDBWsGCtK89LSFaapM2T52GZGOZ1Tb8Zb9Wec+5ZZPr6IiIiIiBxNBSvGitK8AIMziwW0z/g+ZlIhiUt/CGb3oJxDRERERER6qGDFWFHa4Kwk2Mvpo3XOf+Jo3Ipv5X8PzjlERERERARQwYq5/GQ3dpsxaDNYAKHi8+gquxLfyv/GfnDDoJ1HRERERGS0U8GKMYfdRkGKh+qGTn72yibu+MvgFKC2c/+diCed5De+C6HBK3MiIiIiIqOZCtYwUJTm5a1t9bxadYC/72igK2Rafo6oJ43W+f8HR+MWEt//D8vHFxERERERFaxhoSjNixmJkpvkxoxECRxoG5TzhMacS8fUW/CufxTXzsWDcg4RERERkdFMBWsYOHd8BrNK0rjv2skArNvXMmjnaj/rh4QzKkha8gOMjoODdh4RERERkdFIBWsYmD4mlfuumUxJuo/8FA8f7W8dvJPZ3bRc8N8YwVaSF98O0cjgnUtEREREZJRRwRpmJucl8dH+wZvBAjAz/LR95k5cu97Cu+p3g3ouEREREZHRRAVrmJmUl8yBtiC1rYP7UOCuiTfSVXYFCcvuxrnvg0E9l4iIiIjIaKGCNcxMzk8GGPRZLAyDtrm/xkwuJulv38HoqBvc84mIiIiIjAIqWMPMhKwEXHaDhz7YxcJVewmGB+8eqagrkZaLHsDW3UTyott0P5aIiIiIyACpYA0zTruNO+aVEjIj/ObNbTzwXvWgns/MrKTtnDtx7X4b38r7BvVcIiIiIiIjnQrWMHTVlDye/toM5k/I4tm1+2jrDg/q+boqb6Sr7Ep8H/4G5973BvVcIiIiIiIj2YAKlt/v/57f7//I7/dv8Pv9t1sVSnp85cxC2oMmz63dP7gnMgza5t6FmVJC8t++g6117+CeT0RERERkhOp3wfL7/ZOAm4EzganApX6/v9SqYALlOUmcWZTKk4N8LxZ8fD/WH8DsIuXlr0D3IC+yISIiIiIyAg1kBqsCWBYIBDoCgUAYeAu42ppY8rEvnzmGuvYgr2ysHfRzmRl+Wi56AHvDFuzP3QSRwb00UURERERkpDGi0Wi/DvT7/RXAC8BZQCewGFgRCARuPdExkUgkapr9O9+R7HYbphk/K94NJG80GuWq+9+nvTvMa7edg91mWJzuWMbqR3G88n3M079G5KJ7wBj8c1phNP2+iBVlHnzxlhcGntnptK8EzrAukYiISOw4+ntgIBCo8vv9vwZeB9qBNYB5smNMM0pTU0d/T9krNdVnyThDZaB5bzy9gJ/8tYoXVuxi3oQsC5OdwNjrSD9rB/b376XTnUfn6d8e/HNaYLT9vogFZR588ZYXBp45KyvJwjQiIiKxNaBFLgKBwEOBQGB6IBA4F2gENlsTS440ryyTwlQPj3y4m0g/Zxz7KnLez+gqvZzE93+FZ8PjQ3JOEREREZF4N9BVBLMP/VpEz/1XT1gRSo5mtxncfFYxVbVtLFi+Z2hOathonf9buovOI3Hpj3BveXFozisiIiIiEscG+hysZ/1+/0bgJeA7gUCgyYJMchwXV2QzryyT3727k401rUNzUruLloseJJR3JkmLbsO1c/HQnFdEREREJE4N9BLBcwKBQGUgEJgaCAT00/cgMgyDn1xQRobPyW+Xbhu6Ezu9tFzyJ8IZFSS/dotKloiIiIjISQx0BkuGUIrXycWVOazb30pn6KTriVgq6k6m+bLHCadPIPnVm3S5oIiIiIjICahgxZnpY1IwI1HW7R3aBwFHvek0X/FnQrnTSXr9O3g+WjCk5xcRERERiQcqWHFman4KdgNW7hn62916ZrIWECyeR9JbP8K34l4YolUNRURERETigQpWnPG57FTmJrFyd3NsAji8tFz8R7omXE3CsrtJWnw7mN2xySIiIiIiMsyoYMWh08eksqFmaO/DOordSev8/0v7zH/CE3iW1L98HqOjLjZZRERERESGERWsOBSr+7COYhh0nPE9mj97P466j0h75lIcB9fHLo+IiIiIyDCgghWHpuanYLcZfLgr9o8dC5ZeStNVz0LUJPWZK/B89D+6L0tERERERi0VrDjkc9k5vTCFpVvriA6DMhPOnkrj9X8jVDibpLd+QtLr38EItsU6loiIiIjIkFPBilPnT8hkV2Mn2+o7Yh0FOLSM+6X/Q9usH+He9ldSn7oYe93GWMcSERERERlSKlhxak5pJgbw5uZhtLiEYaNz+ndpvvIpjFAHac9chnftQxCJ0WIcIiIiIiJDTAUrTmUmuJhWkMziLQdjHeUYofxZNH7+b4QKziLx7z8n9ZnLcNSuiXUsEREREZFBp4IVx+ZNyGJbXQc7G4bHZYJHivoyab70MVou/B229lpSn7mMxLd+gtEdo+d3iYiIiIgMARWsODavLBObAa9srI11lOMzDLrLLqfxhjfpnPI1PBsWkP74HNyBZ7XSoIiIiIiMSCpYcSw7yc3ZY9N5YX0NYTMS6zgnFHUn037Ov9F03cuYSYUkL/oeKS9cj71hc6yjiYiIiIhYSgUrzl09NY+GjhBvbauPdZRPFc6aTNM1L9A65z9x1G0kbeF8khZ9D1vTjlhHExERERGxhApWnDurJJ3cJDfPrt0f6yinxmana9KXaLjxbTqn3ox728ukPzGXxMU/wF4fiHU6EREREZEBUcGKc3abwdVT81i+q4mvPr6ad+JgJgsg6s2g/ex/pf6L79E5+at4tvyF9IXnk/LcNbg3Pw9md6wjioiIiIj0mQrWCPDFMwr5/txxNHaG+NdXNhGNowUkognZtJ9zJ/Vf+ZC2s36Kvb2G5DduJeORGSS890tszTtjHVFERERE5JSpYI0ATruNG6YX8uUZhbQHTWpa42/2J+rNoPP0b9HwxXdouuxxQnkz8K55kIwFnyHlL5/HvfkvEO6KdUwRERERkZNyxDqAWGdcRgIA2+s6yElyc/firVxSmcPk/OQYJ+sDw0aoaA6hojnY2vbj2fQUno0LSX7ju0TcKQRL5tNdcgGhMecSdcfR+xIRERGRUUEFawQZl+EDYHt9O3kpPQtfdIUj8VWwjhBJzKPjjO/RMf1WnHvexbPpaVw7F+EJPEvU5iCUP6u3cEVSimMdV0REREREBWskSfE6yUhwsa2+g2RPz/+0q3c3xTiVBQwboTHnEBpzDkTCOGtW4tr5Bq6di0j8+y9I/PsvCKdNwCi/CEfuXMK508Fmj3VqERERERmFVLBGmHEZPrbXtWM3er7f19LN/pYu8pI9sQ1mFZuDUP5MQvkzaZ/9L9iaduCuXoxr5yKcy35HWuReIp40gsXzCBbPJ1g0R5cSioiIiMiQUcEaYcZl+HhhfQ3tQZP8FA/7mrtYtbuZSyaOkIL1CZHUsXSmfoPOqd8g1ROmc/2rh2a3Fh91KWGo4GxCuacTzp5K1JUY69giIiIiMkKpYI0w4zIT6ApH2NXYybfOLuHxlXtYtaeJSybmxDra4PMk0112Gd1ll0HExFGzEvfON3BVLyFh2a8BiGJgZpQTLDyHUOFswlmTiPhywDBim11ERERERgQVrBFm/KGFLgCmFiRTVZvCyt3NMUwUIzY74fwzCeefSfvsn2J0NeE4sAZnzSqc+z/E+9Gj+NY+CEDEm0E4cyLhzErCmZMIZU8lklKi0iUiIiIifaaCNcJ8vFS73YDK3CROK0xh6dZ6/rRsFxNzkzizOC3GCWMj6kklVDSXUNHcng2hThwH1+Oo29D7n3ftwxiRIAARdwrhnGmEsqZgpk8gnO7HTBsPdnfs3oSIiIiIDHsqWCNMksdBVqKLzAQXXqedc8Zl8PAHu/jd33diAC/efCa5I2XBi4FwentnuHqZIeyNW3AeWIOjdg3O2tX49vweIxIGIGrYMVPH9cx0ZVRgZlQQzqwgkpCn2S4RERERAVSwRqTvnTuOxEPLtI9J8/LGt89ia107N/zPKt7aWs/nTy+IccJhyu7EzKzEzKyEyht6tplB7E3bcTRsxt4QwFFXhbNmJZ4tL/QeFnGnEs6sIJxR2VO60ssw00qJulNi9EZEREREJFZUsEagz1ZkH/W9YRiUZSUyNt3HW9t6CtZf1u1nbIaPqQUqASdld2FmlGNmlB+12ehuwVFfhb2+CkfdRhz1VXg3PoER7uzdJ+LNIpw2HjOtFDOtlHBaKWZqKZGkfDBsQ/1ORERERGQIqGCNIueWZrBg+W7+vr2eX76xhdMLU3jg81NjHSsuRd3Jvc/j6hUxsbdUY2/chr1xK/amrTgat+He+hK27sMLjUQdHsJpZdhzK/EmjsNM9xNOn0AkqVDFS0RERCTOqWCNInNLM3j0w938+KUqANbubaatO0yiW78NLGHruUfLTB0HYy84vD0axehqwNG4FXvjFuyNW3E0bMHY8TaJbX8+vJvD13N54aHC9fHiGpHEfN3jJSIiIhIn9JP1KFKZm0Rmgou69iCfPy2fP6/ex7LqRs6fkBXraCObYRD1ZhDyZhw145Wa6qO5dj/2hs04GgKHft2Mc9dSPJue6t0v4kzETC87VLoOly8triEiIiIy/KhgjSI2w+CG6QVsq2vn9rnjeWXjAd7b0aCCFUNRdwrhvBmE82Yctd3oajy0sMbh8uXeuRhb1eEZr4gr+fjFSw9OFhEREYkZFaxR5kszxvR+PbM4jfd2NBKNRjH0A/mwEvWkHXuPF2B0Nhwx2xXA3hDAvf01bBuf7N0n4k45XLjSSjFTSjCTizGTC8HhHeq3IiIiIjKqqGCNYrPHprFo80E2H2zHn50Y6zhyCqLedEIFZxEqOOuIjVGMzrrDS8kfutTwk4trAJi+HCLJhZhJhUSSxmAm5RNJLMBMzCOSXETUpd8HIiIiIgOhgjWKnTU2HQN4a2udClY8MwyivixCvixChWcf3h6NYnTW96xs2FyNvaUaW+se7C17cNauwbbt5d6HKH/MTMjFTCvDTBt/aNZrDGbSGCLJhXqul4iIiMgpUMEaxTITXEwrTGHR5jpumV0S6zhiNcMg6ssk7MsknDv92NcjJrbOg9ha92Jv3Yetpbp3pUP3pmewhdqO3t2dgpFaRLIvHzN5TM8MWPKYntkwzX6JiIiIACpYo94F/izuXryVrXXtlGYmxDqODCWbnUhCLpGE3GMLWDSK0d2EvWU3ttbd2Fv2YG/dhbtzP/aGnbh2v33UQ5UBIu7UnuKVmE8kMQ8zIZdIYj5mchFmSjFRb6YW3xAREZERTwVrlJtXlsk9S7ayKHCQ9u4w2+s7uGpKXqxjSawZBlFPGmFPGmRP6d3sSPXR1NTR+2yvngK2B3vLLuwf/9q8E+e+D465/yvq8B4qWyU9vyYXEUkpPrwAh9091O9SRERExHIqWKNcRoKL08ek8syafTzy4W7MSJQZRamkpvpiHU2Gs0PP9gp7MyBn2vH3CXVgb9uHvbkaW8uuQ+WrGnvzsTNgUYyeWa/kIszk4kPFq2fmK5w2AVyaXRUREZH4oIIlXODPYsWuJk4rSGb13haWbK5jUklGrGNJvHP6epaJTys99rVoFKPjYE/p6l2Eo6eEuXYtxd5Re9TuZnIx4YxywhkVhDPKMTMrMZOLwWYfojcjIiIicmpUsIQrJuWSn+zmjKI0bnpyDYs2H+S2C/2xjiUjmWEQTcgmnJBNOO+MY18PdWJv3Y29aQeOhk3Y6zfhqK/CtfMNjGgEgKjDQzjd3/twZTMxD/NQCYu6k4f4DYmIiIj0UMES7DaDWSXpAJxflsl97+xgd2MHSQYcbOvm9uc+4mef9ePP0SpxMkScXsz0CZjpEwiO++zh7eFOHA1beguXo74K5+63sXXUYUTN3t3MpMKe2a7MSoyiqdg94zFTSjTjJSIiIoNOBUuOcr6/p2C9tqGW6ybl8Oza/Ww+2M6rVQdUsCT2HF7C2VMIZ0+h+8jt0Qi29hocdVXYDxUvR10VruolGCtM0jk84xXOqMA8VL7CGeVEPWkxejMiIiIyEqlgyVEKUrxU5CSycPluLi7L4IX1NQB8UN0AjIttOJETMWxEEvMJJuZDyfmHt4e7SA3voXPnahx1G3HUV+He8Tq2qoW9u5iJeYdKVyXhzIqer1PHgU1/PIqIiEjfDegnCL/f/33gG0AUWA98LRAIdFkRTGLn1nPH8u2n1/PNP6+lrj3IGWNSWLG7mQOt3WQnaSltiSMOD2ROodtTenjGKxrF1nGgZ6ar7tBsV/3GnpUNI+GeXexuwukTema6eme7Koh602P2VkRERCQ+9Ltg+f3+AuA2oDIQCHT6/f6ngC8Aj1iUTWJkRlEaX5pVxGMf7CIv2c3tc8fzxcdW8cHORi6fnBvreCIDYxhEEnKIJOQQKpp7eLsZxN64FUf9xkPFaxPOXUvxbHrq8C6+HMzMcsIZlYeKVwVmainYnUP/PkRERGRYGug1MA7A6/f7Q4AP2DfwSDIc/NMFfrbUtHJxRTYTshLISnTxvgqWjGR2V8/y75mVdB+xiKbRUde7oIajbiP2+iq8ex7CiAQBiNqcmGllhDMrCeVMI5xzGuGMCrC7YvRGREREJJb6XbACgcBev99/D7AL6AReDwQCr1uWTGLK67Jz3zWTe7+fVZzGW9vqMSNR7DYjhslEhlbUl0nIdw6hMecc3miGsDdt6y1e9roqXLvewhN4pucYu5twWhmR5DGHH5icNZlw5kQVLxERkRHOiEaj/TrQ7/enAc8CnweagKeBZwKBwIITHROJRKKm2b/zHclut2GakQGPM1TiLS8cm3lRVS3femI1v756MlefVhDDZCcWb59zvOUFZT6paBRa9mDsXYGxbyVG3WaMpmpo3o0R7rk1NWpzQvpYolkVRPOnEy04g2juFHD6hj6vhQaa2em0rwSO80A0ERGR+DOQSwTnAzsCgcBBAL/f/xwwGzhhwTLNKE1NHQM4ZY/UVJ8l4wyVeMsLx2Y+PTeRiblJ3PN6gLMKk/E6e54nFI1GMYzhMaMVb59zvOUFZf50GZD/2Z7/PhaNYmvbj6N2Fc6D67A3bsOxdzX2qhd6XrY5MFPHEz70kGRP8Wm0eMYSSciDYfL/rU8z0M84KyvJwjQiIiKxNZCCtQuY5ff7ffRcIng+sMKSVDLs2AyD788dxzcWruWx5bu5ZXYJXSGTax5ezlfOHMP1w3RWSyTmDINIUj7BpHyCpZce3txxEGftahy1q3semLx/BZ4tL8AHkAFE3CmEM8oPr2SYUUE43Q+uhNi9FxEREflUA7kHa5nf738GWAWEgdXAg1YFk+FnakEK8ydk8viKvXx5xhiWVTdyoC3Ig+9Vc8nEHBJcem6QyKmK+rIIjr2Q4NgLe7cZ3c2kBqt7nttV37OSoXvT03hD7T3HYGCmjiWcOZFw5kTMzErCmROJJOTE6m2IiIjIJwzoJ+JAIPBz4OcWZZE4cM3UfBZtruPtbfW8v7MRt8NGc1eYhav2ctOs4ljHE4lrUXcK0ZxZdCVNOWJjBFvrHhz1m3oelly3AeeBdXi2vtS7S8SbRTirp2yFMyp6VjVMHXfUvV0iIiIyNDTlIH1y+pgUshNdvLyxlg37W5lXlklbd5jHV+zlAn82RWneWEcUGVkMG5HkIoLJRZ+Y7WrpmeU6+BH2Q8XLu+YPGJFQ7z5mYj5m6njMtHGEU0sx08Zjpo4nkpgHhi0W70ZERGTEU8GSPrEZBheWZ7NgxR4A5pZmUJTm4+tPrub6R1bwuYpsZhSnclZxOqk+PXxVZLBE3cmE8mcSyp95eKMZxN60vWcJ+cZt2Ju2YW/cijvwHN5g6+FjHV7CqeMw00oxU8dhpk0gnD4BM3WcHposIiIyQCpY0mcXVfQULJfdYFZJOj6Xnee+PoM/frCLlzfU8tKGWgpTPTz9tRk49MwskaFjd2FmlGNmlBM8cns0iq3jwKHCtR1701bsjdtw1q7GveVFDHoenxG1OTFTx/WUrXQ/4fQyzLQyzJQSPb9LRETkFKlgSZ9NyEqgIieR/BQPPlfPcu2ZiW5+NL+MO+aV8lpVLXe+tpklmw9yYXl2jNOKCIZBJCGHSEIOoYLZR78W7sTeuB1HQwBHw2bsDZtxHliHe+tfjyheDsyUsZjpZYTTyjDTJ/T8mlaq4iUiIvIJKljSZ4ZhcP/1U7EfZ3bKYTP4XGUOf1q2mwUr9nCBP2vYPCdLRI7D4cXMmoiZNZHuI7eHOnE0bcPesBl745ae8lVXhWv7axjRnocKd4+7mJaL/xCT2CIiIsOVCpb0y8czV8djMwxumF7AXYu2snpvM6cXpg5hMhGxhNNLOGsS4axJR28Pd2Fv3oGjYTNmytjYZBMRERnGtIyUDIpLKnNI8Th4fMXeo7a/vukA//jUWpo6Qyc4UkSGNYcHM6OC7rIrCGdP+fT9RURERhkVLBkUHqed66bl8862enY2dAAQiUb5/bs7Wbm7mR+/tJGwGYlxShERERERa6lgyaC57rR8nHaDJ1f2zGItq25kT1MX8ydksmJ3M/e+vSPGCUVERERErKWCJYMm3efi4socXt5YS11bN8+s2U+6z8m/fa6cq6bk8tSafexr7op1TBERERERy6hgyaC6cXoh4UiUKx9azjvb6rlyci5Ou42bZhVjM+DRD3f37vthdSPffWYdHUHzqDF2NnQQDOtyQhEREREZ/lSwZFCNzfDx6A2ncdnEHCpyk7hmaj4AOUlurpiUy4sf1VDT0kV1Qwc/fGkjy6qbWL6rqff4tu4wN/7PSu5/d2eM3oGIiIiIyKnTMu0y6Pw5ifwwp+yY7V85cwx/WV/D9Y+swOu0YzcMPA4by6obmVOaAUDgQBtBM8pfN9Tyrc+U4LTr3wREREREZPjST6sSM7nJHu6/fgqfq8whJ8nNry+v5IyiVJZVN/buU1XbBkBjZ4h3tjfEKqqIiIiIyCnRDJbE1NSCFKYWpPR+v/lgO3/f3sDe5k4KUrxU1bSSnegiCrz0UQ3zyjIBqG7ooLU7zKS85BglFxERERE5lmawZFiZVZwGwLLqnvuwqmpbmZSXzKUTc3hvRwO1rd0A/Osrm/jO0+tp6tADi0VERERk+FDBkmGlJN1LdqKLD6sbaekKsbupi4qcRK6YnIsBPLFyDxtqWqmqbaMjZPLo8p5VCHfUd2BGogC8WlXL7c99RPjQ9yIiIiIiQ0WXCMqwYhgGs8em81rVAZZurQegIjeJghQvF1Vk8+za/exu7MTrtDGzOI2n1+yjvj3Iq1UH+OLMIm6aUchvlmyjuSvMO9vqOa8sk6117eQkuknyfPpv9+bOECle52C/TREREREZoTSDJcPO12cVYRhw9+KtAJRnJwLwtZlFhMwI72xv4LPl2XxvzjjCkSivVR2gIieRBct28aOXNtLcFSbV62Thqr0Eatv40mOr+L9vbz/qHK9VHeB/Pf8Rz6/bT1t3GICVu5u48Pfv8+ERi2yIiIiIiPSFCpYMO3nJHr71mbF0hyMUpHh6Z5SK031cWJ4NwDVT8yhM9fJfl1fyhy9M5cHPT2VcZgLLqpv4XGU2X55RyKo9zfzghQ2EI1GWbqnrvWSwrTvMPUu2sqy6kV+9sYXbnv2ISDTKY8v3EInC8+tqjsnU1h3mmoeX896Oo1cyDEeiRKO6FFFEREREeqhgybB0/bR8zixK7X0e1sd+MHc8/3V5JeU5SQCcMz6DqQUpeJx2/vd1Uzh7bDrf/sxYrpici8dho7a1m6um5NLcFWbNnmYAFq7aS3NXmAe/MI0fzy9l/f4WHnh3J+/uaCDF4+CtbXU0dx69eMbafS3sauzktaoDR23/xaubuPGxVTR0BAfx0xARERGReKGCJcOS3Wbw/66bwvfnjj9qe6rPydxDS7V/0sT8FH579SRyktwke5z807xSfnDeeL4/dzxuh40lW3qK04IVe5hbmsHE3CSunJJHZW4SDy/bjctu8B+XlBMyo/xt08Gjxl67t6ecfbirqXfGqiNosmRLHVsOth+1omF1Qwe/fH0zXSHT6o9FRERERIY5FSwZsS6fnMsXTi/A67Qze2w6S7bUceuz6+kKmXxzdgkANsPgB+f1lLiLKrKZVZLOhKwE/rrh6MsE1+xtAaC+Pci2ug4APqxuJGRGuWlWEbsaO7h7Sc89Y/e/W81f1tfw1w21Q/RORURERGS4UMGSUeG8sgzq24Nsr+/g7ismUpqV0PvalPxk7r9+Ct+bMw6AK6fkUVXbxqJAzyxWMBxhY00r8yf0zJwtO7QIxt93NJDgsvONWUXceEYhbwQO8tbWepZsOdi7pHzIjHDnawF+9som6toPX0bYHgyz5UDbURmj0SjLdjYec3miiIiIiMQPLdMuo8Lc0kyumdrCpRNzmJSXfMzr08ek9n591ZQ8Xvqohl8v3srpY1LY09RFdzjCheXZbDnYzrLqRm6YXsC72xs4qyQNh93GjdMLeWr1Pn78140YhsHtc8fxv9/cxjf/vJb1+1tx2Aze2V7P3NJMkj0O/rqhltbuMPddM5mZhx6u/NJHtfz765vJSXLzy0vKmVqQMmSfj4iIiIhYQzNYMip4nepgWTEAABr4SURBVHZ+NL/suOXqkxw2g59f5Kc9GOYXrwZ4/9DKgVMLkplZnMaqPc28EThIXXuQz4zrWYQjxevkhukFhMwoF5Vncf20fPJTPKzf38oN0wtY+JXpzChK470dDTyxci/TClIYm5HAz18NUN8eZOvBdu5espUp+ck4bAbffGoduxs7gZ57vT6e/WrsCPJa1QFerapl1Z6mQfq0RERERKS/NIMlchzjMxO4Y14pv160hfd3NlKU5iXd5+Kc8ek8tWYfP315EwYwe2xa7zE3TC9kX0s335hVhN1m8JMLyli1u4lbZpdgtxncfXklAF0hE4/TTm2XyTUPvM81Dy+nKxwh1evk7ssr6QiaXP3wcj6obmRMmpcf/3Uj7+1oJDvRRX17EPOIVeEf/odpTM7/9NLYV7Wt3WQnujAMw/KxRUREREYyFSyRE7h6Sh4FKR7+5eVNfGZcOgCzStJ5+mtnsOVgOz6nnTSfq3f/RLeDX1zk7/1+ZnFa7+V/R/I47QD4c5O4+/JKFm8+SJrPxcUV2WQkuEj3RclKdLFmTzOXTcxh+a4mzhiTQrrPRV6Kh/MnZOKy2/jGwjX8efXeowrW65sO4HXamT4mFZ/Lfsy5GzqC/OLVAF+fWcS0wuNfglhV28pXH1/ND+eXcfWUvOPu0xE0eeiDXXz1zDEkefTHiIiIiMjH9JORyEnMLE7j1X+cddS2knQfJek+S8afPTad2WPTj9pmGAbTClJYs7eZVXua/397dx4eZXnvf/w92fd9IwthCzeEAAkI4gKCLBUtoKAiVo+t7dG2WlvtOa1bPeqv56enni4ute1xp8X1IP6oWi1FxaKgrMrmzRIgJIGEkJ3sk/n9MUNMSIIoQ2Yin9d1cTFzzz3PfJ/7evJc8537fr4PrU4X103MYtKgrv3m5qXx0qZSfnxBM8lRobyxrYx737IAhAcH8MRV+ZiUqI7+7S4X971lWbOvivrmNp5alN8xQ/X46r0cOdrC3bOG8+LGEtpd8NSa/VySm8ozHxXxzq4K5oxK5duT3YVAVu48zOJ1BxgQE8rl+eleGQsRERGRrwNdgyXyBYICHAQF9O1SufyMWMrrW3htyyFCAt0J1/GuyE+nvd3FX9YX82lpLQ/+Yxfjs2J5bMFoAF7ZXNql/wsbSvhwbxUFmbFsOVjXUXoe4I1tZSzfWsarnx5khT3MqLRoyutbuP2v23lqbRFNrU4eeX8vP37pEwBWF7qvS1uzr+p0DYGIiIhIv6QES8QPFWS6l/29u6uC/IzYjmWFnWXGhTN5aCLPbyjhuy9sJjw4kP9z8QjOHhTP9OHJrPjsMI2emx23tbt4cu1+zhucwCPz84gLD2bxugMAHK5vpry+hQAHPPiP3bQ6Xdw72zAuM5bVhZXkZ8Sw9PoJ3HBONv/cXcHeIw2s3VeFA1hXVEVLW3tHTKU1TdQ3t53+ARIRERHxU0qwRPzQ0KRIokPdK3h7uo7rmLtm5XDfbMMdM3N4clE+yVGhgHv5YEOrk5U73ffy2lpaS32zk7l5qYQFB3JlQTqrCyvZX9nA9kN1ANwyZQgOYNKgeAYlRHDbtKFMy0nigTm5BAcGMHd0GgEO+OXfd9LQ6mTe6DQaW9v5pLQGgOLqRhY9t4F/X74dgE3FNcx74iNVOxQREZEzihIsET8U4HAwNsM9i3WiBCshIoSLc1OZP2YAA+PDO9rzM2LIigvjr1vLAFizr5JAB0wY6N7WvLw0AN7ZVcH2Q3UEOmDB2AH85rJR3D5jGAAmJYpfzc0lKdJdyCM1OpTJOcl8WlpLaFAAPzh/EEEBDtbsraKt3cU9b35GQ6uT9UXVbD1YyyPvF1Ja28zPl+/gYG2T9wdJRERExA8pwRLxU7NHpjBhYBw5KZFf+r0Oh4O5eWlsLK5h26E61uyrYnR6TEfFv5ToUEYPiObdXRVsO1TH0KRIwoIDOX9IIhmx4b1u98rxmQCclRVHQkQI+ZmxrLCHuXXZVrYcrOPuWTnEhAVx1xufsfVgHdeelUlbezvffWEzP/rfLSzfcuik4q9vbsPlcvX42pGjLTR5lj6KiIiI+BslWCJ+ataIFB6/YgwBX/FeVFcUpJMQEcwDK3axo6yec46rQjgtJ4kdZfVsLqll1IDok9rmNJPMpOx45o91l2+fOjSRQ3XN7Kk4yo3nZjNv9ACuyE+ntKaJjNgwfnj+IH596ShMShSltU3854qdbPMsSezJtoO13Pn6Dmb8/kP+8MG+bq83tDi5evEGrluyidqm1pMfDBEREZE+ojLtIl9TkSFB3HBuNg/+YzfQ9abIAFOHJfHI+3tpbmtnVNrJJVjBgQE8evnojueX56dzwbBEUqNDO0q+X1WQwQp7mJsmDyYoMIBxmXGMy4yjvrmNhc+u55dv72TxNQUEB37++44tr+ehlbv5pLSWyJBAhiRF8ud1xcwZlUZWp6WPSz8ppbKhlZqmNv7ttW08evkYQoP0O5GIiIj4D30zEfkamzd6AIMSwkmICGZ4p3tiAWTFhzMsyb38MPckE6zjBQY4SIsJ60iuAOIigll6/QQuzEnq0jcqNIifz8hhd8VRrnx2PT9bvp2SmsaO+3MdqG7ktmlDeePGs3lkfh4hgQE8vKqw4/2NrU7+vK6YSdnx3D/bsKmklqc/KvrCGP+5u4KfvraNmkbNeImIiMjppwRL5GssKMDB7+bn8cj80T0uNZyTl0pKVAiDE7/8dV5fxZShidw+YxjDkiL5aF8V9/7N8t7uI+w6fJQfXzCEReMyiAwJIikqlG+fncWqPUd4c7u7UMfijw9Q1djK984ZyKwRKcwYnsyLG0qobnAnTi6Xi53l9RRVNXZ83oYD1fxgyUbe33OEFzaW9Mk+ioiIyJlNSwRFvuYyYsOh+32KAVg0LoOFBRkE9uGNlBeMTWfB2HRe33aI+97ayc5yS1ZcGLNGpHTp963xmXy8v4r733InYe/uqmCmSWas56bLN5ybzcqdh3nm4yKy4sJ5cWMJ+6saCXDA5WPTcbpcvL6tjKz4CJIig3lxYwnfGp/ZUehDRERE5HTQDJbIGczhcPRpctXZJbmpnDMonoZWJ9dPGkjQcXGEBAXw60vzyE2L4d1dFSwsSOf+i0d0vD44MYJvjEzh+Q0l/NfK3USHBXHHjGHMHzOAVzaX8teth5g+PInF35nAzZMHc7TFyYueWayDtU1874XN/M+H+3C2u2hztnPkaEuf7r+IiIh8PemnXBHxCYfDwb2zDe/uquCikak99okICeSxy0ezs7ye/Mzu03A3nT8Il8vF7NxUzh0U33Et2HUTswgPDiQ2PJi46FCCnVFMHZbIU2v3c6ShhdWFlVQ2tPBJaS2rCys5VNtMfUsbzywqwKRGdfscERERkZOlGSwR8ZmEiBAWjE3vNnvVWURIYI/JFUBaTBi/vGQk5w1O6FJoIy0mjNjw4C597541nEvHDGDZpwdpbmvnmasLuHtWDlUNrRRkxhITFsx9b1tane3e2TkRERE5I2kGS0TOCLHhwdw+I4erx2cSGhRAanQoJiWKeaPd9/R6f88RfvraNh5eVcjNkwcTFhzY67ac7S4+Lqri9a1lrN1fxUPzchmXGXdScRRVNVLT2Mro9Biv7JeIiIj4FyVYInJGGdjpvlqdTRmayGVj0nhpUylv7SjntmlDuTg3laKqRh5eVUhdcxsO3DNqO8vrKa9vISYsiKZWJyttxUklWK3Odm5dtpXGVidv3jjJy3smIiIi/kAJloiIxx0zcpg9MpXH/rmX+9/eSUJEML95r5DD9c0MT47CBRyub8GkRHHbtFQmD0nkZ8u389H+qh6398HeSkamRpEQEQLAS5tKO8rIHznaQmJkSJf+zW3tvL3tEOsLj5CTHNmtsqKIiIj4PyVYIiIeDoeDgsxYfndZHv+yZCM/WrqVAAc8umA0E7Pje3zPxOw4PthbyaHaJtJiwjraqxpauPXVrZw3JIHfXpZHZUMLT67ZT2p0KGV1zew6XE9iZEJH/6ZWJ7cs3cKmkloAwoICmJgdT9xx15KJiIiIf1ORCxGR40SHBfHQ3FEkRARzy5QhvSZXABMGupcGriuqBqDd5QJgU0ktLmB1YSXri6q592+W5rZ2HvjmSABs+dGObbQ62/nZ8u1sLqnlgcvyWHxNAU1t7byyqfSEcbpcLh5fvZcdZXWnsrsiIiLiRV95BssYY4CXOjUNAe6x1v7ulKMSEfGxYcmR/O37kwhwnPg+YUOTIkmICObDvZV8sLeSfZUNLLl2PBsPVBMaFEBsWBC3vbaVxtZ27piZw+j0GAbEhLKzvL5jG0s/OciafVXcOTOHy8dlUl3dwJShiby0qYRrJmQS3kvBjcIjDTzz0QHK6pq5b/aIHvuIiIhI3/rKM1jWLd9amw+MBxqAZV6LTETEx74ouTrWZ8LAOP6xs4KVOyvYU9HA+gPVbCquYXR6DDecm01jazsLxg5g/hh3xcLhyVHsPOxOsFra2vnzugMUZMZymed1cN/Lq6aprePmyD35cG8lABsO1OByuSipaeSPH+zjaEtbt75t7S7W7qtUGXoREZHTzFtLBKcDe6y1+720PRGRfuOCYUkA3Dp1CJEhgby8qZRdh48yLjOWuXlpPLFwLP924bCO/sNTItlf2Uhjq5M3tpdRXt/C9WdnddnmmPQYpg5L5E8f7OPjXopoHEuwyuqaKalp4sk1RTy1toh/ffETyuqau/R9fn0xP1q6leuf38zeIw3e3H0RERHpxFtFLq4CXviiToGBDuLiIk75wwIDA7yynb7S3+IFxdwX+lu8oJh7c8XZ2UzPG0BCZAjFdS28sqEYgCkjUomPj2RqfGSX/gWDEnGtKaKwtpnF64sZkxHLN8Zm4HA4usT7m4UFLHxiLXe+8RnLfnAOWfGf70ddUxuflNZyoUnmHXuYTYfqeXd3BflZsewuP8pN/7uFN24+j9DgQKobWnhu3QFyB8RwsKaR65Zs4rFF+UzJSfbK/vfH40JEROR0OeUEyxgTAswF7viivk6ni+rqU//lNC4uwivb6Sv9LV5QzH2hv8ULivlEAoDq6jamD03glQ3FBAc6yI4K7vGzMyLdlQFven4TTa1Obp8+jJqaxh7j/dWckVzz543c9eoWHp6fx4f7qth4oIbBieG0Ol1cOXYAmw9U89i7uzna7OR7Zw8EF9y8dAuPv7OL75w9kEdWFVLX1MYvZuYQFx7Ej1/dyg1/2ch/XjKC6cNPPck61TFOTo4+5RhERET8hTdmsGYDG621ZV7YlohIv1aQGUt6TChpMWGE9VKcYkBMKNGhQTS1OXlwTi5nn6BKYWZcODecm81v3yvk4VV7eXlzCa1Od6XCyJBAxqbHMD4rjhX2MEmRIZyVFUdggINpOUk8vbaI6sZWXt5UysWjUhmW7J5J+9PCsfzwlU/53XuFXJiThOMLrjVrbHUSGhRwUtekiYiInOm8cQ3WIk5ieaCIyJkgwOHgkQWjuXe26bWPw+Hgl5eM4I9XjmVqTtIXbvPKggyGJ0eyZEMxA+PDeXRBHiYliotzUwkKDOCsrFgAZo1IJjDAnQT95IIhuIDnN5QwwyTzkwuGdGwvKjSIOXlpHKprpri66YSffaCqkYv/tJbFHx84ib0XERGRU5rBMsZEAjOBG70TjohI/5ed8MXXI507OOEL+xwTFODgnosMz35UxK1Th5ISHcqkQZ+/f8rQRN7YXt5RpRAgPTaMP1wxhtCgAIanRHXb5sSO+3dVkRUf3uPntjrbueuNHdQ3O/nrtjKum5jFh3ur+NOH+3hgzkgyYnt+n4iIyJnslGawrLVHrbWJ1toabwUkIiLdmZQoHpiTS0p0aLfXkqJCeWpRfrfEbnR6TI/JFcDA+HBSokL42HOD5L1HGmhrd3Xp88Sa/ewoq2fqsESKqhrZXlbPo/8sZEdZPbcu20Z9c/dy8CIiImc6b5VpFxGRfsThcDAxO571RdW8vaOcK59dzz1vfobTk2S1u1ws+/QQF+Yk8YtvDCc40MEDK3axp6KBy8akUVTVyH+/s9vHeyEiIuJ/lGCJiJyhJmbHUdPUxn1vWxIjQ1hhD/NfK3fhcrkorGigurGVyUMTiAkL5rzBCdjyelKiQvj3C4fx3/NyGZcV5+tdEBER8Tveug+WiIj0MxMGuqsXBgU4eGLhWF7bcpDF64q5aGQKtvwoAGd5kqiLRqbw3u4jXD0+k+DAAM4fkuizuEVERPyZEiwRkTNUUmQI156VyZj0GLLiw/neOdks/eQgy7eWUdfURmZcGGkxYQBMy0ni/35zJFOHKbESERE5ESVYIiJnsFs6lW8PDw5kpknmrR3lBAY4mGk+vwlxgKPrcxEREemZrsESEZEOc/PSaGpr52iLkwkDdY2ViIjIl6UES0REOuQNiGZworvc+3gVsRAREfnStERQREQ6OBwOfnjeIDYW15AYGeLrcERERPodJVgiItLF1JwkpuYk+ToMERGRfklLBEVERERERLxECZaIiIiIiIiXKMESERERERHxEiVYIiIiIiIiXqIES0RERERExEuUYImIiIiIiHiJEiwREREREREvUYIlIiIiIiLiJUqwREREREREvEQJloiIiIiIiJcowRIREREREfESJVgiIiIiIiJeogRLRERERETES5RgiYiIiIiIeIkSLBERERERES9xuFyuvvy8w8D+vvxAERHxe9lAsq+DEBER8Ya+TrBERERERES+trREUERERERExEuUYImIiIiIiHiJEiwREREREREvUYIlIiIiIiLiJUqwREREREREvEQJloiIiIiIiJcE+TqAL8MYcxHwMBAIPGmtfdDHIXVjjMkCFgOpgAv4H2vtw8aYe4F/xX0vMIA7rbVv+ibK7owx+4A6wAm0WWvPMsYkAC8Bg4B9wJXW2iofhdjBGGNwx3XMEOAeIA4/GmNjzNPAN4Fya22ep63HMTXGOHAf2xcDDcC3rbUb/STmh4A5QAuwB/iOtbbaGDMI2AFYz9vXWmu/7wfx3ksvx4Ex5g7gu7iP81ustW/3ZbwniPklwHi6xAHV1tp8Pxnj3s5pfn0si4iI+Eq/mcEyxgQCvwdmA7nAImNMrm+j6lEb8FNrbS4wCbipU5y/tdbme/75TXLVyTRPbGd5nt8OrLTW5gArPc99zrrlW2vzgfG4v8Qt87zsT2P8LHDRcW29jelsIMfz7wbgD30U4/GepXvMK4A8a+0YYCdwR6fX9nQa7z794u/xLN3jhR6OA8/f4VXAKM97HvecV/rasxwXs7V2YadjeinwaqeXfT3GvZ3T/P1YFhER8Yl+k2ABE4Hd1tpCa20L8CIwz8cxdWOtPXjs11prbR3uX58zfBvVVzYPeM7z+DngUh/G0pvpuL+A7vd1IMez1r4PVB7X3NuYzgMWW2td1tq1QJwxZkDfRPq5nmK21v7dWtvmeboWyOzruHrTyxj3Zh7worW22Vq7F9iN+7zSp04Us2f250rghT4N6gROcE7z62NZRETEV/pTgpUBHOj0vBg/T1w8y3sKgI88TTcbYz41xjxtjIn3XWQ9cgF/N8ZsMMbc4GlLtdYe9Dw+hHuJkL+5iq5fRv15jKH3Me0vx/f1wN86PR9sjNlkjFlljJnsq6B60NNx0B/GeDJQZq3d1anNb8b4uHNafz+WRURETov+lGD1K8aYKNxLfX5ira3FvUxmKJAPHAR+7cPwenK+tXYc7uU9NxljpnR+0Vrrwp2E+Q1jTAgwF3jF0+TvY9yFP47piRhj7sK9XGyJp+kgMNBaWwDcBjxvjInxVXyd9Kvj4DiL6PqDgd+McQ/ntA797VgWERE5nfpTglUCZHV6nulp8zvGmGDcX0SWWGtfBbDWlllrndbaduAJfLA06USstSWe/8txX880ESg7trTH83+57yLs0Wxgo7W2DPx/jD16G1O/Pr6NMd/GXZjhW54v03iW2h3xPN6AuwDGcJ8F6XGC48DfxzgImE+nAi7+MsY9ndPop8eyiIjI6dafEqx1QI4xZrBn5uIqYLmPY+rGcw3FU8AOa+1vOrV3vgbhMmBrX8fWG2NMpDEm+thjYBbu+JYD13m6XQf8P99E2Ksuv/b78xh30tuYLgf+xRjjMMZMAmo6Lb/yKU/1zp8Bc621DZ3ak48ViTDGDMFd1KDQN1F+7gTHwXLgKmNMqDFmMO54P+7r+E5gBvCZtbb4WIM/jHFv5zT64bEsIiLSF/pNmXZrbZsx5mbgbdxl2p+21m7zcVg9OQ+4FthijNnsabsTd9XDfNzLaPYBN/omvB6lAsvc1c8JAp631r5ljFkHvGyM+S6wH/fF937BkwjOpOs4/sqfxtgY8wIwFUgyxhQD/wE8SM9j+ibusta7cVdF/E6fB0yvMd8BhAIrPMfIsVLhU4D7jTGtQDvwfWvtyRacOJ3xTu3pOLDWbjPGvAxsx73U8SZrrbMv4+0tZmvtU3S/nhD8YIzp/Zzm18eyiIiIrzhcLi2bFxERERER8Yb+tERQRERERETErynBEhERERER8RIlWCIiIiIiIl6iBEtERERERMRLlGCJiIiIiIh4iRIsET9ljJlqjHnd13GIiIiIyMlTgiUiIiIiIuIlug+WyCkyxlwD3AKEAB8BPwRqgCeAWcAh4Cpr7WHPDXD/CEQAe4DrrbVVxphhnvZkwAlcAWQB9wIVQB6wAbjGWqs/WhERERE/pRkskVNgjBkJLATOs9bm406OvgVEAuuttaOAVcB/eN6yGPi5tXYMsKVT+xLg99bascC5wEFPewHwEyAXGAKcd9p3SkRERES+siBfByDSz00HxgPrjDEA4UA50A685OnzF+BVY0wsEGetXeVpfw54xRgTDWRYa5cBWGubADzb+9haW+x5vhkYBKw+/bslIiIiIl+FEiyRU+MAnrPW3tG50Rjzi+P6fdVlfc2dHjvR36yIiIiIX9MSQZFTsxK43BiTAmCMSTDGZOP+27rc0+dqYLW1tgaoMsZM9rRfC6yy1tYBxcaYSz3bCDXGRPTpXoiIiIiIVyjBEjkF1trtwN3A340xnwIrgAHAUWCiMWYrcCFwv+ct1wEPefrmd2q/FrjF0/4hkNZ3eyEiIiIi3qIqgiKngTGm3lob5es4RERERKRvaQZLRERERETESzSDJSIiIiIi4iWawRIREREREfESJVgiIiIiIiJeogRLRERERETES5RgiYiIiIiIeIkSLBERERERES/5//i35hlkA7c1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    4.800, max:    9.052, cur:    4.800)\n",
      "\tvalidation       \t (min:    5.751, max:    9.800, cur:    5.751)\n",
      "mean_absolute_percentage_error_keras\n",
      "\ttraining         \t (min:    1.154, max:    3.810, cur:    3.367)\n",
      "\tvalidation       \t (min:    0.924, max:    1.665, cur:    1.316)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    6.773, max:   11.837, cur:    6.883)\n",
      "\tvalidation       \t (min:    7.820, max:   12.789, cur:    7.820)\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "max_seed = 2**32 - 1\n",
    "seed_list = random.sample(range(0, max_seed), number_different_lambda_trainings)\n",
    "chunk_multiplier = 0\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(chunksize*chunk_multiplier+index, X_data[1].values, y_data[1].values, X_data[0], seed_list, return_history=True, each_epochs_save=each_epochs_save, printing=True) for index, (X_data, y_data) in enumerate(zip(X_data_list_split, y_data_list_split)))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "    chunk_multiplier +=1\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(rand_index, X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], seed_list, callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:47:47.094061Z",
     "start_time": "2020-12-14T14:47:34.147228Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    scores_list_train = [clf[1][1] for clf in clf_list]\n",
    "    scores_list_valid = [clf[1][2] for clf in clf_list]\n",
    "    scores_list_test = [clf[1][3] for clf in clf_list]\n",
    "    scores_list_stds = [clf[1][4] for clf in clf_list]\n",
    "    scores_list_means = [clf[1][5] for clf in clf_list]\n",
    "\n",
    "    scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_list_by_epochs = [[] for i in epochs_save_range]\n",
    "    for scores_list in scores_list:   \n",
    "        for index, scores in enumerate(scores_list):\n",
    "            scores_list_by_epochs[index].append(scores)\n",
    "            \n",
    "        \n",
    "    for i, scores_list_single_epoch in enumerate(scores_list_by_epochs):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "           \n",
    "        scores_list_train = [scores_list[1] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_valid = [scores_list[2] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_test = [scores_list[3] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_stds = [scores_list[4] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_means = [scores_list[5] for scores_list in scores_list_single_epoch]\n",
    "        \n",
    "        scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()  \n",
    "        scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()  \n",
    "        scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:47:47.127846Z",
     "start_time": "2020-12-14T14:47:47.097592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E1</th>\n",
       "      <th>TRAIN POLY E1</th>\n",
       "      <th>TRAIN POLY PRED E1</th>\n",
       "      <th>TRAIN LSTSQ E1</th>\n",
       "      <th>TRAIN PRED E10</th>\n",
       "      <th>TRAIN POLY E10</th>\n",
       "      <th>TRAIN POLY PRED E10</th>\n",
       "      <th>TRAIN LSTSQ E10</th>\n",
       "      <th>TRAIN PRED E20</th>\n",
       "      <th>TRAIN POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E180</th>\n",
       "      <th>TRAIN LSTSQ E180</th>\n",
       "      <th>TRAIN PRED E190</th>\n",
       "      <th>TRAIN POLY E190</th>\n",
       "      <th>TRAIN POLY PRED E190</th>\n",
       "      <th>TRAIN LSTSQ E190</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.192</td>\n",
       "      <td>10.192</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.697</td>\n",
       "      <td>9.698</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.933</td>\n",
       "      <td>8.935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.399</td>\n",
       "      <td>4.434</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.312</td>\n",
       "      <td>4.348</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.986</td>\n",
       "      <td>12.986</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.477</td>\n",
       "      <td>12.477</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.686</td>\n",
       "      <td>11.686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.199</td>\n",
       "      <td>6.175</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.089</td>\n",
       "      <td>6.062</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.416</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.474</td>\n",
       "      <td>3.474</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.447</td>\n",
       "      <td>3.447</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.398</td>\n",
       "      <td>3.398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.388</td>\n",
       "      <td>2.336</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.359</td>\n",
       "      <td>2.303</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.425</td>\n",
       "      <td>24.425</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.697</td>\n",
       "      <td>23.696</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.378</td>\n",
       "      <td>22.376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.618</td>\n",
       "      <td>11.526</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.442</td>\n",
       "      <td>11.345</td>\n",
       "      <td>1.007</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>101.318</td>\n",
       "      <td>101.318</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>96.420</td>\n",
       "      <td>96.425</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.609</td>\n",
       "      <td>88.626</td>\n",
       "      <td>...</td>\n",
       "      <td>3.645</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.383</td>\n",
       "      <td>42.701</td>\n",
       "      <td>3.874</td>\n",
       "      <td>0.000</td>\n",
       "      <td>41.595</td>\n",
       "      <td>41.937</td>\n",
       "      <td>4.103</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E1  TRAIN POLY E1  TRAIN POLY PRED E1  TRAIN LSTSQ E1  \\\n",
       "MAE FV          10.192         10.192               0.013           0.000   \n",
       "RMSE FV         12.986         12.986               0.016           0.000   \n",
       "MAPE FV            inf            inf               0.258           0.000   \n",
       "R2 FV           -0.416         -0.416               0.999           1.000   \n",
       "RAAE FV          0.923          0.923               0.022           0.000   \n",
       "RMAE FV          3.474          3.474               0.095           0.000   \n",
       "FD FV           24.425         24.425               0.031           0.000   \n",
       "DTW FV         101.318        101.318               0.128           0.000   \n",
       "\n",
       "         TRAIN PRED E10  TRAIN POLY E10  TRAIN POLY PRED E10  TRAIN LSTSQ E10  \\\n",
       "MAE FV            9.697           9.698                0.015            0.000   \n",
       "RMSE FV          12.477          12.477                0.020            0.000   \n",
       "MAPE FV             inf             inf                0.219            0.000   \n",
       "R2 FV            -0.293          -0.293                0.999            1.000   \n",
       "RAAE FV           0.876           0.876                0.023            0.000   \n",
       "RMAE FV           3.447           3.447                0.117            0.000   \n",
       "FD FV            23.697          23.696                0.040            0.000   \n",
       "DTW FV           96.420          96.425                0.155            0.000   \n",
       "\n",
       "         TRAIN PRED E20  TRAIN POLY E20  ...  TRAIN POLY PRED E180  \\\n",
       "MAE FV            8.933           8.935  ...                 0.365   \n",
       "RMSE FV          11.686          11.686  ...                 0.469   \n",
       "MAPE FV             inf             inf  ...                 0.594   \n",
       "R2 FV            -0.117          -0.117  ...                 0.996   \n",
       "RAAE FV           0.804           0.804  ...                 0.049   \n",
       "RMAE FV           3.398           3.398  ...                 0.245   \n",
       "FD FV            22.378          22.376  ...                 0.901   \n",
       "DTW FV           88.609          88.626  ...                 3.645   \n",
       "\n",
       "         TRAIN LSTSQ E180  TRAIN PRED E190  TRAIN POLY E190  \\\n",
       "MAE FV              0.000            4.399            4.434   \n",
       "RMSE FV             0.000            6.199            6.175   \n",
       "MAPE FV             0.000              inf              inf   \n",
       "R2 FV               1.000            0.659            0.661   \n",
       "RAAE FV             0.000            0.406            0.410   \n",
       "RMAE FV             0.000            2.388            2.336   \n",
       "FD FV               0.000           11.618           11.526   \n",
       "DTW FV              0.000           42.383           42.701   \n",
       "\n",
       "         TRAIN POLY PRED E190  TRAIN LSTSQ E190  TRAIN PRED E200  \\\n",
       "MAE FV                  0.388             0.000            4.312   \n",
       "RMSE FV                 0.497             0.000            6.089   \n",
       "MAPE FV                 0.540             0.000              inf   \n",
       "R2 FV                   0.995             1.000            0.670   \n",
       "RAAE FV                 0.051             0.000            0.398   \n",
       "RMAE FV                 0.253             0.000            2.359   \n",
       "FD FV                   0.954             0.000           11.442   \n",
       "DTW FV                  3.874             0.000           41.595   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             4.348                 0.411             0.000  \n",
       "RMSE FV            6.062                 0.525             0.000  \n",
       "MAPE FV              inf                 0.730             0.000  \n",
       "R2 FV              0.673                 0.995             1.000  \n",
       "RAAE FV            0.402                 0.054             0.000  \n",
       "RMAE FV            2.303                 0.262             0.000  \n",
       "FD FV             11.345                 1.007             0.000  \n",
       "DTW FV            41.937                 4.103             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:47:47.162113Z",
     "start_time": "2020-12-14T14:47:47.131483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E1</th>\n",
       "      <th>VALID POLY E1</th>\n",
       "      <th>VALID POLY PRED E1</th>\n",
       "      <th>VALID LSTSQ E1</th>\n",
       "      <th>VALID PRED E10</th>\n",
       "      <th>VALID POLY E10</th>\n",
       "      <th>VALID POLY PRED E10</th>\n",
       "      <th>VALID LSTSQ E10</th>\n",
       "      <th>VALID PRED E20</th>\n",
       "      <th>VALID POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E180</th>\n",
       "      <th>VALID LSTSQ E180</th>\n",
       "      <th>VALID PRED E190</th>\n",
       "      <th>VALID POLY E190</th>\n",
       "      <th>VALID POLY PRED E190</th>\n",
       "      <th>VALID LSTSQ E190</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.196</td>\n",
       "      <td>10.196</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.708</td>\n",
       "      <td>9.708</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.954</td>\n",
       "      <td>8.955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.530</td>\n",
       "      <td>4.544</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.447</td>\n",
       "      <td>4.461</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.979</td>\n",
       "      <td>12.979</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.475</td>\n",
       "      <td>12.474</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.691</td>\n",
       "      <td>11.691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.314</td>\n",
       "      <td>6.271</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.208</td>\n",
       "      <td>6.161</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.442</td>\n",
       "      <td>1.429</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.919</td>\n",
       "      <td>1.926</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.289</td>\n",
       "      <td>3.265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.505</td>\n",
       "      <td>5.411</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.545</td>\n",
       "      <td>5.453</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.926</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.042</td>\n",
       "      <td>3.042</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.016</td>\n",
       "      <td>3.015</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.968</td>\n",
       "      <td>2.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.022</td>\n",
       "      <td>1.977</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.996</td>\n",
       "      <td>1.947</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.399</td>\n",
       "      <td>24.399</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.669</td>\n",
       "      <td>23.668</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.344</td>\n",
       "      <td>22.342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.664</td>\n",
       "      <td>11.544</td>\n",
       "      <td>1.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.493</td>\n",
       "      <td>11.364</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>101.996</td>\n",
       "      <td>101.997</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.100</td>\n",
       "      <td>97.103</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>89.271</td>\n",
       "      <td>89.285</td>\n",
       "      <td>...</td>\n",
       "      <td>3.860</td>\n",
       "      <td>0.000</td>\n",
       "      <td>43.543</td>\n",
       "      <td>43.677</td>\n",
       "      <td>4.103</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.799</td>\n",
       "      <td>42.931</td>\n",
       "      <td>4.347</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E1  VALID POLY E1  VALID POLY PRED E1  VALID LSTSQ E1  \\\n",
       "MAE FV          10.196         10.196               0.014           0.000   \n",
       "RMSE FV         12.979         12.979               0.018           0.000   \n",
       "MAPE FV          1.442          1.429               0.326           0.000   \n",
       "R2 FV           -0.425         -0.425               0.999           1.000   \n",
       "RAAE FV          0.926          0.926               0.024           0.000   \n",
       "RMAE FV          3.042          3.042               0.096           0.000   \n",
       "FD FV           24.399         24.399               0.033           0.000   \n",
       "DTW FV         101.996        101.997               0.138           0.000   \n",
       "\n",
       "         VALID PRED E10  VALID POLY E10  VALID POLY PRED E10  VALID LSTSQ E10  \\\n",
       "MAE FV            9.708           9.708                0.017            0.000   \n",
       "RMSE FV          12.475          12.474                0.022            0.000   \n",
       "MAPE FV           1.919           1.926                0.174            0.000   \n",
       "R2 FV            -0.302          -0.302                0.999            1.000   \n",
       "RAAE FV           0.880           0.880                0.025            0.000   \n",
       "RMAE FV           3.016           3.015                0.111            0.000   \n",
       "FD FV            23.669          23.668                0.043            0.000   \n",
       "DTW FV           97.100          97.103                0.164            0.000   \n",
       "\n",
       "         VALID PRED E20  VALID POLY E20  ...  VALID POLY PRED E180  \\\n",
       "MAE FV            8.954           8.955  ...                 0.389   \n",
       "RMSE FV          11.691          11.691  ...                 0.507   \n",
       "MAPE FV           3.289           3.265  ...                 0.541   \n",
       "R2 FV            -0.125          -0.125  ...                 0.995   \n",
       "RAAE FV           0.808           0.808  ...                 0.052   \n",
       "RMAE FV           2.968           2.968  ...                 0.228   \n",
       "FD FV            22.344          22.342  ...                 0.967   \n",
       "DTW FV           89.271          89.285  ...                 3.860   \n",
       "\n",
       "         VALID LSTSQ E180  VALID PRED E190  VALID POLY E190  \\\n",
       "MAE FV              0.000            4.530            4.544   \n",
       "RMSE FV             0.000            6.314            6.271   \n",
       "MAPE FV             0.000            5.505            5.411   \n",
       "R2 FV               1.000            0.644            0.648   \n",
       "RAAE FV             0.000            0.420            0.421   \n",
       "RMAE FV             0.000            2.022            1.977   \n",
       "FD FV               0.000           11.664           11.544   \n",
       "DTW FV              0.000           43.543           43.677   \n",
       "\n",
       "         VALID POLY PRED E190  VALID LSTSQ E190  VALID PRED E200  \\\n",
       "MAE FV                  0.413             0.000            4.447   \n",
       "RMSE FV                 0.537             0.000            6.208   \n",
       "MAPE FV                 0.424             0.000            5.545   \n",
       "R2 FV                   0.994             1.000            0.655   \n",
       "RAAE FV                 0.055             0.000            0.412   \n",
       "RMAE FV                 0.237             0.000            1.996   \n",
       "FD FV                   1.024             0.000           11.493   \n",
       "DTW FV                  4.103             0.000           42.799   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             4.461                 0.438             0.000  \n",
       "RMSE FV            6.161                 0.568             0.000  \n",
       "MAPE FV            5.453                 0.698             0.000  \n",
       "R2 FV              0.660                 0.994             1.000  \n",
       "RAAE FV            0.413                 0.058             0.000  \n",
       "RMAE FV            1.947                 0.247             0.000  \n",
       "FD FV             11.364                 1.082             0.000  \n",
       "DTW FV            42.931                 4.347             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:47:47.191640Z",
     "start_time": "2020-12-14T14:47:47.164304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E1</th>\n",
       "      <th>TEST POLY E1</th>\n",
       "      <th>TEST POLY PRED E1</th>\n",
       "      <th>TEST LSTSQ E1</th>\n",
       "      <th>TEST PRED E10</th>\n",
       "      <th>TEST POLY E10</th>\n",
       "      <th>TEST POLY PRED E10</th>\n",
       "      <th>TEST LSTSQ E10</th>\n",
       "      <th>TEST PRED E20</th>\n",
       "      <th>TEST POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E180</th>\n",
       "      <th>TEST LSTSQ E180</th>\n",
       "      <th>TEST PRED E190</th>\n",
       "      <th>TEST POLY E190</th>\n",
       "      <th>TEST POLY PRED E190</th>\n",
       "      <th>TEST LSTSQ E190</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.189</td>\n",
       "      <td>10.189</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.701</td>\n",
       "      <td>9.701</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.947</td>\n",
       "      <td>8.948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.529</td>\n",
       "      <td>4.543</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.446</td>\n",
       "      <td>4.461</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.975</td>\n",
       "      <td>12.975</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.470</td>\n",
       "      <td>12.470</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.687</td>\n",
       "      <td>11.687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.314</td>\n",
       "      <td>6.272</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.209</td>\n",
       "      <td>6.162</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.350</td>\n",
       "      <td>1.348</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.496</td>\n",
       "      <td>1.497</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.002</td>\n",
       "      <td>2.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.253</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.232</td>\n",
       "      <td>3.240</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.925</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.167</td>\n",
       "      <td>3.166</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.140</td>\n",
       "      <td>3.140</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.092</td>\n",
       "      <td>3.092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.134</td>\n",
       "      <td>2.084</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.054</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.136</td>\n",
       "      <td>24.136</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.419</td>\n",
       "      <td>23.418</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.104</td>\n",
       "      <td>22.102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.587</td>\n",
       "      <td>11.485</td>\n",
       "      <td>1.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.422</td>\n",
       "      <td>11.310</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>101.554</td>\n",
       "      <td>101.554</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.000</td>\n",
       "      <td>96.682</td>\n",
       "      <td>96.685</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>88.892</td>\n",
       "      <td>88.905</td>\n",
       "      <td>...</td>\n",
       "      <td>3.867</td>\n",
       "      <td>0.000</td>\n",
       "      <td>43.340</td>\n",
       "      <td>43.472</td>\n",
       "      <td>4.108</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.592</td>\n",
       "      <td>42.725</td>\n",
       "      <td>4.349</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E1  TEST POLY E1  TEST POLY PRED E1  TEST LSTSQ E1  \\\n",
       "MAE FV         10.189        10.189              0.014          0.000   \n",
       "RMSE FV        12.975        12.975              0.018          0.000   \n",
       "MAPE FV         1.350         1.348              0.268          0.000   \n",
       "R2 FV          -0.423        -0.423              0.999          1.000   \n",
       "RAAE FV         0.925         0.925              0.024          0.000   \n",
       "RMAE FV         3.167         3.166              0.100          0.000   \n",
       "FD FV          24.136        24.136              0.033          0.000   \n",
       "DTW FV        101.554       101.554              0.138          0.000   \n",
       "\n",
       "         TEST PRED E10  TEST POLY E10  TEST POLY PRED E10  TEST LSTSQ E10  \\\n",
       "MAE FV           9.701          9.701               0.017           0.000   \n",
       "RMSE FV         12.470         12.470               0.022           0.000   \n",
       "MAPE FV          1.496          1.497               0.186           0.000   \n",
       "R2 FV           -0.300         -0.300               0.999           1.000   \n",
       "RAAE FV          0.879          0.879               0.025           0.000   \n",
       "RMAE FV          3.140          3.140               0.117           0.000   \n",
       "FD FV           23.419         23.418               0.042           0.000   \n",
       "DTW FV          96.682         96.685               0.164           0.000   \n",
       "\n",
       "         TEST PRED E20  TEST POLY E20  ...  TEST POLY PRED E180  \\\n",
       "MAE FV           8.947          8.948  ...                0.389   \n",
       "RMSE FV         11.687         11.687  ...                0.507   \n",
       "MAPE FV          2.002          2.002  ...                0.463   \n",
       "R2 FV           -0.123         -0.123  ...                0.995   \n",
       "RAAE FV          0.807          0.807  ...                0.052   \n",
       "RMAE FV          3.092          3.092  ...                0.243   \n",
       "FD FV           22.104         22.102  ...                0.964   \n",
       "DTW FV          88.892         88.905  ...                3.867   \n",
       "\n",
       "         TEST LSTSQ E180  TEST PRED E190  TEST POLY E190  TEST POLY PRED E190  \\\n",
       "MAE FV             0.000           4.529           4.543                0.413   \n",
       "RMSE FV            0.000           6.314           6.272                0.537   \n",
       "MAPE FV            0.000           3.240           3.253                0.636   \n",
       "R2 FV              1.000           0.645           0.649                0.994   \n",
       "RAAE FV            0.000           0.419           0.421                0.055   \n",
       "RMAE FV            0.000           2.134           2.084                0.252   \n",
       "FD FV              0.000          11.587          11.485                1.019   \n",
       "DTW FV             0.000          43.340          43.472                4.108   \n",
       "\n",
       "         TEST LSTSQ E190  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           4.446           4.461                0.437   \n",
       "RMSE FV            0.000           6.209           6.162                0.567   \n",
       "MAPE FV            0.000           3.232           3.240                0.842   \n",
       "R2 FV              1.000           0.656           0.660                0.994   \n",
       "RAAE FV            0.000           0.412           0.413                0.058   \n",
       "RMAE FV            0.000           2.107           2.054                0.262   \n",
       "FD FV              0.000          11.422          11.310                1.075   \n",
       "DTW FV             0.000          42.592          42.725                4.349   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "FD FV              0.000  \n",
       "DTW FV             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:47:47.223785Z",
     "start_time": "2020-12-14T14:47:47.194148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA</th>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>...</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.673</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.706</td>\n",
       "      <td>2.614</td>\n",
       "      <td>3.584</td>\n",
       "      <td>4.482</td>\n",
       "      <td>5.244</td>\n",
       "      <td>5.854</td>\n",
       "      <td>6.328</td>\n",
       "      <td>...</td>\n",
       "      <td>6.966</td>\n",
       "      <td>7.178</td>\n",
       "      <td>7.346</td>\n",
       "      <td>7.482</td>\n",
       "      <td>7.598</td>\n",
       "      <td>7.699</td>\n",
       "      <td>7.790</td>\n",
       "      <td>7.875</td>\n",
       "      <td>7.956</td>\n",
       "      <td>8.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.672</td>\n",
       "      <td>1.033</td>\n",
       "      <td>1.706</td>\n",
       "      <td>2.613</td>\n",
       "      <td>3.582</td>\n",
       "      <td>4.480</td>\n",
       "      <td>5.241</td>\n",
       "      <td>5.850</td>\n",
       "      <td>6.323</td>\n",
       "      <td>...</td>\n",
       "      <td>6.960</td>\n",
       "      <td>7.171</td>\n",
       "      <td>7.337</td>\n",
       "      <td>7.473</td>\n",
       "      <td>7.587</td>\n",
       "      <td>7.686</td>\n",
       "      <td>7.776</td>\n",
       "      <td>7.860</td>\n",
       "      <td>7.938</td>\n",
       "      <td>8.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>...</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "      <td>11.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA</th>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>...</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.671</td>\n",
       "      <td>1.029</td>\n",
       "      <td>1.696</td>\n",
       "      <td>2.596</td>\n",
       "      <td>3.560</td>\n",
       "      <td>4.453</td>\n",
       "      <td>5.211</td>\n",
       "      <td>5.818</td>\n",
       "      <td>6.290</td>\n",
       "      <td>...</td>\n",
       "      <td>6.926</td>\n",
       "      <td>7.137</td>\n",
       "      <td>7.303</td>\n",
       "      <td>7.438</td>\n",
       "      <td>7.552</td>\n",
       "      <td>7.651</td>\n",
       "      <td>7.741</td>\n",
       "      <td>7.824</td>\n",
       "      <td>7.903</td>\n",
       "      <td>7.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.671</td>\n",
       "      <td>1.028</td>\n",
       "      <td>1.695</td>\n",
       "      <td>2.595</td>\n",
       "      <td>3.558</td>\n",
       "      <td>4.451</td>\n",
       "      <td>5.208</td>\n",
       "      <td>5.815</td>\n",
       "      <td>6.287</td>\n",
       "      <td>...</td>\n",
       "      <td>6.921</td>\n",
       "      <td>7.131</td>\n",
       "      <td>7.296</td>\n",
       "      <td>7.431</td>\n",
       "      <td>7.544</td>\n",
       "      <td>7.642</td>\n",
       "      <td>7.731</td>\n",
       "      <td>7.814</td>\n",
       "      <td>7.891</td>\n",
       "      <td>7.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>...</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA</th>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>...</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.671</td>\n",
       "      <td>1.030</td>\n",
       "      <td>1.697</td>\n",
       "      <td>2.597</td>\n",
       "      <td>3.560</td>\n",
       "      <td>4.454</td>\n",
       "      <td>5.212</td>\n",
       "      <td>5.820</td>\n",
       "      <td>6.292</td>\n",
       "      <td>...</td>\n",
       "      <td>6.928</td>\n",
       "      <td>7.138</td>\n",
       "      <td>7.305</td>\n",
       "      <td>7.440</td>\n",
       "      <td>7.554</td>\n",
       "      <td>7.653</td>\n",
       "      <td>7.743</td>\n",
       "      <td>7.826</td>\n",
       "      <td>7.905</td>\n",
       "      <td>7.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.671</td>\n",
       "      <td>1.029</td>\n",
       "      <td>1.696</td>\n",
       "      <td>2.596</td>\n",
       "      <td>3.558</td>\n",
       "      <td>4.451</td>\n",
       "      <td>5.209</td>\n",
       "      <td>5.816</td>\n",
       "      <td>6.288</td>\n",
       "      <td>...</td>\n",
       "      <td>6.922</td>\n",
       "      <td>7.132</td>\n",
       "      <td>7.298</td>\n",
       "      <td>7.432</td>\n",
       "      <td>7.545</td>\n",
       "      <td>7.644</td>\n",
       "      <td>7.733</td>\n",
       "      <td>7.815</td>\n",
       "      <td>7.893</td>\n",
       "      <td>7.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>...</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "      <td>11.133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        E1    E10    E20    E30    E40    E50  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.156 11.156 11.156 11.156 11.156 11.156   \n",
       "STD FV TRAIN PRED LAMBDA             0.574  0.673  1.034  1.706  2.614  3.584   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  0.574  0.672  1.033  1.706  2.613  3.582   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.156 11.156 11.156 11.156 11.156 11.156   \n",
       "STD FV VALID REAL LAMBDA            11.129 11.129 11.129 11.129 11.129 11.129   \n",
       "STD FV VALID PRED LAMBDA             0.574  0.671  1.029  1.696  2.596  3.560   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  0.574  0.671  1.028  1.695  2.595  3.558   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.129 11.129 11.129 11.129 11.129 11.129   \n",
       "STD FV TEST REAL LAMBDA             11.133 11.133 11.133 11.133 11.133 11.133   \n",
       "STD FV TEST PRED LAMBDA              0.574  0.671  1.030  1.697  2.597  3.560   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   0.574  0.671  1.029  1.696  2.596  3.558   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.133 11.133 11.133 11.133 11.133 11.133   \n",
       "\n",
       "                                       E60    E70    E80    E90  ...   E110  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.156 11.156 11.156 11.156  ... 11.156   \n",
       "STD FV TRAIN PRED LAMBDA             4.482  5.244  5.854  6.328  ...  6.966   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  4.480  5.241  5.850  6.323  ...  6.960   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.156 11.156 11.156 11.156  ... 11.156   \n",
       "STD FV VALID REAL LAMBDA            11.129 11.129 11.129 11.129  ... 11.129   \n",
       "STD FV VALID PRED LAMBDA             4.453  5.211  5.818  6.290  ...  6.926   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  4.451  5.208  5.815  6.287  ...  6.921   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.129 11.129 11.129 11.129  ... 11.129   \n",
       "STD FV TEST REAL LAMBDA             11.133 11.133 11.133 11.133  ... 11.133   \n",
       "STD FV TEST PRED LAMBDA              4.454  5.212  5.820  6.292  ...  6.928   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   4.451  5.209  5.816  6.288  ...  6.922   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.133 11.133 11.133 11.133  ... 11.133   \n",
       "\n",
       "                                      E120   E130   E140   E150   E160   E170  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.156 11.156 11.156 11.156 11.156 11.156   \n",
       "STD FV TRAIN PRED LAMBDA             7.178  7.346  7.482  7.598  7.699  7.790   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.171  7.337  7.473  7.587  7.686  7.776   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.156 11.156 11.156 11.156 11.156 11.156   \n",
       "STD FV VALID REAL LAMBDA            11.129 11.129 11.129 11.129 11.129 11.129   \n",
       "STD FV VALID PRED LAMBDA             7.137  7.303  7.438  7.552  7.651  7.741   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.131  7.296  7.431  7.544  7.642  7.731   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.129 11.129 11.129 11.129 11.129 11.129   \n",
       "STD FV TEST REAL LAMBDA             11.133 11.133 11.133 11.133 11.133 11.133   \n",
       "STD FV TEST PRED LAMBDA              7.138  7.305  7.440  7.554  7.653  7.743   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.132  7.298  7.432  7.545  7.644  7.733   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.133 11.133 11.133 11.133 11.133 11.133   \n",
       "\n",
       "                                      E180   E190   E200  \n",
       "STD FV TRAIN REAL LAMBDA            11.156 11.156 11.156  \n",
       "STD FV TRAIN PRED LAMBDA             7.875  7.956  8.034  \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.860  7.938  8.015  \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.156 11.156 11.156  \n",
       "STD FV VALID REAL LAMBDA            11.129 11.129 11.129  \n",
       "STD FV VALID PRED LAMBDA             7.824  7.903  7.979  \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.814  7.891  7.966  \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.129 11.129 11.129  \n",
       "STD FV TEST REAL LAMBDA             11.133 11.133 11.133  \n",
       "STD FV TEST PRED LAMBDA              7.826  7.905  7.981  \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.815  7.893  7.968  \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.133 11.133 11.133  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:47:47.257999Z",
     "start_time": "2020-12-14T14:47:47.226236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA</th>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA</th>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA</th>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         E1    E10    E20    E30    E40  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.002 -0.011 -0.029 -0.051 -0.066   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.002 -0.011 -0.029 -0.051 -0.066   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "MEAN FV VALID REAL LAMBDA            -0.075 -0.075 -0.075 -0.075 -0.075   \n",
       "MEAN FV VALID PRED LAMBDA            -0.001 -0.010 -0.028 -0.050 -0.064   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.001 -0.010 -0.028 -0.050 -0.064   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.075 -0.075 -0.075 -0.075 -0.075   \n",
       "MEAN FV TEST REAL LAMBDA             -0.077 -0.077 -0.077 -0.077 -0.077   \n",
       "MEAN FV TEST PRED LAMBDA             -0.002 -0.012 -0.030 -0.053 -0.069   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.002 -0.012 -0.030 -0.053 -0.069   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.077 -0.077 -0.077 -0.077 -0.077   \n",
       "\n",
       "                                        E50    E60    E70    E80    E90  ...  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.080 -0.080 -0.080 -0.080 -0.080  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.071 -0.067 -0.065 -0.064 -0.065  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.071 -0.067 -0.065 -0.064 -0.065  ...   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.080 -0.080 -0.080 -0.080 -0.080  ...   \n",
       "MEAN FV VALID REAL LAMBDA            -0.075 -0.075 -0.075 -0.075 -0.075  ...   \n",
       "MEAN FV VALID PRED LAMBDA            -0.068 -0.063 -0.060 -0.059 -0.059  ...   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.068 -0.063 -0.060 -0.059 -0.058  ...   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.075 -0.075 -0.075 -0.075 -0.075  ...   \n",
       "MEAN FV TEST REAL LAMBDA             -0.077 -0.077 -0.077 -0.077 -0.077  ...   \n",
       "MEAN FV TEST PRED LAMBDA             -0.073 -0.068 -0.065 -0.064 -0.064  ...   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.073 -0.068 -0.065 -0.064 -0.064  ...   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.077 -0.077 -0.077 -0.077 -0.077  ...   \n",
       "\n",
       "                                       E110   E120   E130   E140   E150  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.063 -0.062 -0.062 -0.062 -0.063   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.063 -0.062 -0.062 -0.062 -0.063   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.080 -0.080 -0.080 -0.080 -0.080   \n",
       "MEAN FV VALID REAL LAMBDA            -0.075 -0.075 -0.075 -0.075 -0.075   \n",
       "MEAN FV VALID PRED LAMBDA            -0.056 -0.056 -0.055 -0.056 -0.056   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.056 -0.056 -0.056 -0.056 -0.056   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.075 -0.075 -0.075 -0.075 -0.075   \n",
       "MEAN FV TEST REAL LAMBDA             -0.077 -0.077 -0.077 -0.077 -0.077   \n",
       "MEAN FV TEST PRED LAMBDA             -0.061 -0.060 -0.060 -0.060 -0.060   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.061 -0.060 -0.060 -0.060 -0.060   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.077 -0.077 -0.077 -0.077 -0.077   \n",
       "\n",
       "                                       E160   E170   E180   E190   E200  \n",
       "MEAN FV TRAIN REAL LAMBDA            -0.080 -0.080 -0.080 -0.080 -0.080  \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.063 -0.064 -0.064 -0.064 -0.064  \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.063 -0.064 -0.064 -0.064 -0.064  \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.080 -0.080 -0.080 -0.080 -0.080  \n",
       "MEAN FV VALID REAL LAMBDA            -0.075 -0.075 -0.075 -0.075 -0.075  \n",
       "MEAN FV VALID PRED LAMBDA            -0.057 -0.057 -0.057 -0.057 -0.057  \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.057 -0.057 -0.057 -0.057 -0.058  \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.075 -0.075 -0.075 -0.075 -0.075  \n",
       "MEAN FV TEST REAL LAMBDA             -0.077 -0.077 -0.077 -0.077 -0.077  \n",
       "MEAN FV TEST PRED LAMBDA             -0.061 -0.061 -0.061 -0.061 -0.061  \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.060 -0.061 -0.061 -0.060 -0.061  \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.077 -0.077 -0.077 -0.077 -0.077  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Net Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:47:47.383121Z",
     "start_time": "2020-12-14T14:47:47.260567Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_lambda_preds(i, \n",
    "                      lambda_indices,\n",
    "                      y_train_real_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_train_lambda_by_epoch, \n",
    "                      y_valid_real_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_valid_lambda_by_epoch, \n",
    "                      y_test_real_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_test_lambda_by_epoch):\n",
    "    \n",
    "    \n",
    "    index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "        \n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_by_epoch, y_train_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_by_epoch, y_valid_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_by_epoch, y_test_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_by_epoch, y_train_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_by_epoch, y_test_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())    \n",
    "\n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "\n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)         \n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)    \n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False) \n",
    "\n",
    "    return y_train_real_lambda_df, y_valid_real_lambda_df, y_test_real_lambda_df, y_train_pred_lambda_df, y_valid_pred_lambda_df, y_test_pred_lambda_df, y_train_pred_lambda_poly_lstsq_df, y_valid_pred_lambda_poly_lstsq_df, y_test_pred_lambda_poly_lstsq_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:57:44.356114Z",
     "start_time": "2020-12-14T14:47:47.386116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3613576825e54641a404bb0789412702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of  21 | elapsed:  8.4min remaining: 79.9min\n",
      "[Parallel(n_jobs=-3)]: Done  10 out of  21 | elapsed:  9.1min remaining: 10.0min\n",
      "[Parallel(n_jobs=-3)]: Done  18 out of  21 | elapsed:  9.5min remaining:  1.6min\n",
      "[Parallel(n_jobs=-3)]: Done  21 out of  21 | elapsed:  9.7min finished\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    lambda_index_list = [clf[0][0] for clf in clf_list]\n",
    "    lambda_seed_list = [clf[0][1] for clf in clf_list] \n",
    "    polynomial_real_list = [clf[0][2] for clf in clf_list] \n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][3] for clf in clf_list] \n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][4] for clf in clf_list] \n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "\n",
    "    lambda_indices_list = np.zeros((len(clf_list), 1))\n",
    "    y_train_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][1])))\n",
    "    y_train_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][2])))\n",
    "    y_train_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][3])))\n",
    "    X_train_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][4].shape)]][0])\n",
    "    y_valid_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][5])))\n",
    "    y_valid_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][6])))\n",
    "    y_valid_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][7])))\n",
    "    X_valid_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][8].shape)]][0])\n",
    "    y_test_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][9])))\n",
    "    y_test_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][10])))\n",
    "    y_test_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][11])))\n",
    "    X_test_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][12].shape)]][0])\n",
    "\n",
    "    for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate([clf[2] for clf in clf_list]):\n",
    "        lambda_indices_list[index] = lambda_indices\n",
    "        y_train_real_lambda_list[index] = y_train_real_lambda.ravel()\n",
    "        y_train_pred_lambda_list[index] = y_train_pred_lambda.ravel()\n",
    "        y_train_pred_lambda_poly_lstsq_list[index] = y_train_pred_lambda_poly_lstsq.ravel()\n",
    "        X_train_lambda_list[index] = X_train_lambda#.ravel()\n",
    "\n",
    "        y_valid_real_lambda_list[index] = y_valid_real_lambda.ravel()\n",
    "        y_valid_pred_lambda_list[index] = y_valid_pred_lambda.ravel()\n",
    "        y_valid_pred_lambda_poly_lstsq_list[index] = y_valid_pred_lambda_poly_lstsq.ravel()\n",
    "        X_valid_lambda_list[index] = X_valid_lambda#.ravel()\n",
    "\n",
    "        y_test_real_lambda_list[index] = y_test_real_lambda.ravel()\n",
    "        y_test_pred_lambda_list[index] = y_test_pred_lambda.ravel()\n",
    "        y_test_pred_lambda_poly_lstsq_list[index] = y_test_pred_lambda_poly_lstsq.ravel()\n",
    "        X_test_lambda_list[index] = X_test_lambda#.ravel()\n",
    "    \n",
    "    #add x_data before each pred\n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda.reshape(len(y_train_real_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_list, y_train_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda.reshape(len(y_valid_real_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_list, y_valid_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda.reshape(len(y_test_real_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_list, y_test_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda.reshape(len(y_train_pred_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_list, y_train_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda.reshape(len(y_valid_pred_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_list, y_valid_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda.reshape(len(y_test_pred_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_list, y_test_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq.reshape(len(y_train_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_list, y_train_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq.reshape(len(y_valid_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_list, y_valid_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq.reshape(len(y_test_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_list, y_test_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())    \n",
    "    \n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "       \n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "    \n",
    "    lambda_indices_list = [np.zeros((len(clf_list), 1)) for i in epochs_save_range]\n",
    "    y_train_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][3]), 1)) for i in epochs_save_range]\n",
    "    X_train_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][4].shape)]][0]) for i in epochs_save_range]\n",
    "    y_valid_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][7]), 1)) for i in epochs_save_range]\n",
    "    X_valid_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][8].shape)]][0]) for i in epochs_save_range]\n",
    "    y_test_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][12]), 1)) for i in epochs_save_range]\n",
    "    X_test_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][12].shape)]][0]) for i in epochs_save_range]\n",
    "    \n",
    "    for i, y_data_list_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate(y_data_list_per_epoch):\n",
    "            lambda_indices_list[index][i] = lambda_indices\n",
    "            y_train_real_lambda_list[index][i] = y_train_real_lambda#.ravel()\n",
    "            y_train_pred_lambda_list[index][i] = y_train_pred_lambda#.ravel()\n",
    "            y_train_pred_lambda_poly_lstsq_list[index][i] = y_train_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_train_lambda_list[index][i] = X_train_lambda#.ravel()\n",
    "            \n",
    "            y_valid_real_lambda_list[index][i] = y_valid_real_lambda#.ravel()\n",
    "            y_valid_pred_lambda_list[index][i] = y_valid_pred_lambda#.ravel()\n",
    "            y_valid_pred_lambda_poly_lstsq_list[index][i] = y_valid_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_valid_lambda_list[index][i] = X_valid_lambda#.ravel()\n",
    "            \n",
    "            y_test_real_lambda_list[index][i] = y_test_real_lambda#.ravel()\n",
    "            y_test_pred_lambda_list[index][i] = y_test_pred_lambda#.ravel()\n",
    "            y_test_pred_lambda_poly_lstsq_list[index][i] = y_test_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_test_lambda_list[index][i] = X_test_lambda#.ravel()\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    y_data_lambda_list = parallel(delayed(save_lambda_preds)(i, \n",
    "                                                           lambda_indices,\n",
    "                                                           y_train_real_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_train_lambda_by_epoch, \n",
    "                                                           y_valid_real_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_valid_lambda_by_epoch, \n",
    "                                                           y_test_real_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_test_lambda_by_epoch) for i, \n",
    "                                                                                        (lambda_indices,\n",
    "                                                                                         y_train_real_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_train_lambda_by_epoch, \n",
    "                                                                                         y_valid_real_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_valid_lambda_by_epoch, \n",
    "                                                                                         y_test_real_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_test_lambda_by_epoch) in enumerate(zip(lambda_indices_list,\n",
    "                                                                                                                                  y_train_real_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_train_lambda_list, \n",
    "                                                                                                                                  y_valid_real_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_valid_lambda_list, \n",
    "                                                                                                                                  y_test_real_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_test_lambda_list)))  \n",
    "    y_test_real_lambda_df = y_data_lambda_list[-1][2]\n",
    "    y_test_pred_lambda_df = y_data_lambda_list[-1][5]\n",
    "    y_test_pred_lambda_poly_lstsq_df = y_data_lambda_list[-1][8]\n",
    "    del parallel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:57:44.401007Z",
     "start_time": "2020-12-14T14:57:44.365126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-7.580</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>10.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-11.955</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-19.621</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-11.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-2.020</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>20.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>25.836</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-7.972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -7.580 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -11.955  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -19.621  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940  -2.020  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920  25.836 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0  10.155  \n",
       "1  -0.622  \n",
       "2 -11.502  \n",
       "3  20.981  \n",
       "4  -7.972  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:57:44.427963Z",
     "start_time": "2020-12-14T14:57:44.404220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-4.449</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>3.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-17.060</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-1.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-24.264</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-8.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>9.710</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>6.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>4.960</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-7.052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -4.449 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -17.060  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -24.264  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940   9.710  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920   4.960 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0   3.053  \n",
       "1  -1.837  \n",
       "2  -8.320  \n",
       "3   6.120  \n",
       "4  -7.052  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:57:44.454218Z",
     "start_time": "2020-12-14T14:57:44.430947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-3.985</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>4.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-17.342</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-1.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-24.986</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-7.879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>8.865</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>6.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>3.877</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-6.408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -3.985 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -17.342  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -24.986  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940   8.865  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920   3.877 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0   4.084  \n",
       "1  -1.934  \n",
       "2  -7.879  \n",
       "3   6.410  \n",
       "4  -6.408  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_poly_lstsq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:57:48.738141Z",
     "start_time": "2020-12-14T14:57:44.457227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fc991a6fd549128cb8952ccb54cea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:58:07.476626Z",
     "start_time": "2020-12-14T14:57:48.742773Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:58:08.396754Z",
     "start_time": "2020-12-14T14:58:07.482887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.222</td>\n",
       "      <td>10.169</td>\n",
       "      <td>10.116</td>\n",
       "      <td>10.063</td>\n",
       "      <td>10.010</td>\n",
       "      <td>9.957</td>\n",
       "      <td>9.903</td>\n",
       "      <td>9.847</td>\n",
       "      <td>9.790</td>\n",
       "      <td>9.732</td>\n",
       "      <td>...</td>\n",
       "      <td>4.398</td>\n",
       "      <td>4.390</td>\n",
       "      <td>4.381</td>\n",
       "      <td>4.372</td>\n",
       "      <td>4.363</td>\n",
       "      <td>4.355</td>\n",
       "      <td>4.346</td>\n",
       "      <td>4.337</td>\n",
       "      <td>4.329</td>\n",
       "      <td>4.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.342</td>\n",
       "      <td>2.319</td>\n",
       "      <td>2.296</td>\n",
       "      <td>2.273</td>\n",
       "      <td>2.249</td>\n",
       "      <td>2.226</td>\n",
       "      <td>2.202</td>\n",
       "      <td>2.178</td>\n",
       "      <td>2.153</td>\n",
       "      <td>2.127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.812</td>\n",
       "      <td>4.805</td>\n",
       "      <td>4.798</td>\n",
       "      <td>4.790</td>\n",
       "      <td>4.784</td>\n",
       "      <td>4.777</td>\n",
       "      <td>4.770</td>\n",
       "      <td>4.764</td>\n",
       "      <td>4.757</td>\n",
       "      <td>4.750</td>\n",
       "      <td>...</td>\n",
       "      <td>2.411</td>\n",
       "      <td>2.408</td>\n",
       "      <td>2.403</td>\n",
       "      <td>2.400</td>\n",
       "      <td>2.396</td>\n",
       "      <td>2.391</td>\n",
       "      <td>2.389</td>\n",
       "      <td>2.386</td>\n",
       "      <td>2.384</td>\n",
       "      <td>2.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.494</td>\n",
       "      <td>8.452</td>\n",
       "      <td>8.417</td>\n",
       "      <td>8.382</td>\n",
       "      <td>8.341</td>\n",
       "      <td>8.304</td>\n",
       "      <td>8.267</td>\n",
       "      <td>8.231</td>\n",
       "      <td>8.191</td>\n",
       "      <td>8.153</td>\n",
       "      <td>...</td>\n",
       "      <td>3.999</td>\n",
       "      <td>3.990</td>\n",
       "      <td>3.982</td>\n",
       "      <td>3.973</td>\n",
       "      <td>3.964</td>\n",
       "      <td>3.956</td>\n",
       "      <td>3.948</td>\n",
       "      <td>3.939</td>\n",
       "      <td>3.931</td>\n",
       "      <td>3.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.953</td>\n",
       "      <td>9.906</td>\n",
       "      <td>9.858</td>\n",
       "      <td>9.817</td>\n",
       "      <td>9.767</td>\n",
       "      <td>9.716</td>\n",
       "      <td>9.672</td>\n",
       "      <td>9.628</td>\n",
       "      <td>9.582</td>\n",
       "      <td>9.526</td>\n",
       "      <td>...</td>\n",
       "      <td>4.371</td>\n",
       "      <td>4.362</td>\n",
       "      <td>4.353</td>\n",
       "      <td>4.343</td>\n",
       "      <td>4.335</td>\n",
       "      <td>4.325</td>\n",
       "      <td>4.316</td>\n",
       "      <td>4.307</td>\n",
       "      <td>4.297</td>\n",
       "      <td>4.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.632</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.500</td>\n",
       "      <td>11.428</td>\n",
       "      <td>11.366</td>\n",
       "      <td>11.309</td>\n",
       "      <td>11.247</td>\n",
       "      <td>11.174</td>\n",
       "      <td>11.104</td>\n",
       "      <td>11.034</td>\n",
       "      <td>...</td>\n",
       "      <td>4.764</td>\n",
       "      <td>4.754</td>\n",
       "      <td>4.745</td>\n",
       "      <td>4.736</td>\n",
       "      <td>4.727</td>\n",
       "      <td>4.718</td>\n",
       "      <td>4.708</td>\n",
       "      <td>4.700</td>\n",
       "      <td>4.691</td>\n",
       "      <td>4.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.716</td>\n",
       "      <td>20.558</td>\n",
       "      <td>20.401</td>\n",
       "      <td>20.246</td>\n",
       "      <td>20.090</td>\n",
       "      <td>19.931</td>\n",
       "      <td>19.768</td>\n",
       "      <td>19.598</td>\n",
       "      <td>19.421</td>\n",
       "      <td>19.235</td>\n",
       "      <td>...</td>\n",
       "      <td>7.295</td>\n",
       "      <td>7.280</td>\n",
       "      <td>7.266</td>\n",
       "      <td>7.249</td>\n",
       "      <td>7.234</td>\n",
       "      <td>7.220</td>\n",
       "      <td>7.202</td>\n",
       "      <td>7.187</td>\n",
       "      <td>7.171</td>\n",
       "      <td>7.157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean         10.222        10.169        10.116        10.063        10.010   \n",
       "std           2.342         2.319         2.296         2.273         2.249   \n",
       "min           4.812         4.805         4.798         4.790         4.784   \n",
       "25%           8.494         8.452         8.417         8.382         8.341   \n",
       "50%           9.953         9.906         9.858         9.817         9.767   \n",
       "75%          11.632        11.567        11.500        11.428        11.366   \n",
       "max          20.716        20.558        20.401        20.246        20.090   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000      10000.000   \n",
       "mean          9.957         9.903         9.847         9.790          9.732   \n",
       "std           2.226         2.202         2.178         2.153          2.127   \n",
       "min           4.777         4.770         4.764         4.757          4.750   \n",
       "25%           8.304         8.267         8.231         8.191          8.153   \n",
       "50%           9.716         9.672         9.628         9.582          9.526   \n",
       "75%          11.309        11.247        11.174        11.104         11.034   \n",
       "max          19.931        19.768        19.598        19.421         19.235   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...       10000.000       10000.000       10000.000       10000.000   \n",
       "mean   ...           4.398           4.390           4.381           4.372   \n",
       "std    ...           0.583           0.583           0.582           0.581   \n",
       "min    ...           2.411           2.408           2.403           2.400   \n",
       "25%    ...           3.999           3.990           3.982           3.973   \n",
       "50%    ...           4.371           4.362           4.353           4.343   \n",
       "75%    ...           4.764           4.754           4.745           4.736   \n",
       "max    ...           7.295           7.280           7.266           7.249   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            4.363           4.355           4.346           4.337   \n",
       "std             0.581           0.580           0.579           0.579   \n",
       "min             2.396           2.391           2.389           2.386   \n",
       "25%             3.964           3.956           3.948           3.939   \n",
       "50%             4.335           4.325           4.316           4.307   \n",
       "75%             4.727           4.718           4.708           4.700   \n",
       "max             7.234           7.220           7.202           7.187   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count       10000.000       10000.000  \n",
       "mean            4.329           4.320  \n",
       "std             0.578           0.577  \n",
       "min             2.384           2.379  \n",
       "25%             3.931           3.922  \n",
       "50%             4.297           4.288  \n",
       "75%             4.691           4.682  \n",
       "max             7.171           7.157  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:58:09.181827Z",
     "start_time": "2020-12-14T14:58:08.399191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.196</td>\n",
       "      <td>10.144</td>\n",
       "      <td>10.092</td>\n",
       "      <td>10.040</td>\n",
       "      <td>9.987</td>\n",
       "      <td>9.934</td>\n",
       "      <td>9.880</td>\n",
       "      <td>9.824</td>\n",
       "      <td>9.767</td>\n",
       "      <td>9.708</td>\n",
       "      <td>...</td>\n",
       "      <td>4.522</td>\n",
       "      <td>4.514</td>\n",
       "      <td>4.505</td>\n",
       "      <td>4.497</td>\n",
       "      <td>4.489</td>\n",
       "      <td>4.480</td>\n",
       "      <td>4.472</td>\n",
       "      <td>4.464</td>\n",
       "      <td>4.456</td>\n",
       "      <td>4.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.378</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.332</td>\n",
       "      <td>2.310</td>\n",
       "      <td>2.287</td>\n",
       "      <td>2.263</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.215</td>\n",
       "      <td>2.190</td>\n",
       "      <td>2.164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.149</td>\n",
       "      <td>4.144</td>\n",
       "      <td>4.139</td>\n",
       "      <td>4.134</td>\n",
       "      <td>4.129</td>\n",
       "      <td>4.125</td>\n",
       "      <td>4.121</td>\n",
       "      <td>4.117</td>\n",
       "      <td>4.113</td>\n",
       "      <td>4.110</td>\n",
       "      <td>...</td>\n",
       "      <td>2.322</td>\n",
       "      <td>2.320</td>\n",
       "      <td>2.312</td>\n",
       "      <td>2.307</td>\n",
       "      <td>2.304</td>\n",
       "      <td>2.300</td>\n",
       "      <td>2.298</td>\n",
       "      <td>2.296</td>\n",
       "      <td>2.292</td>\n",
       "      <td>2.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.442</td>\n",
       "      <td>8.413</td>\n",
       "      <td>8.378</td>\n",
       "      <td>8.342</td>\n",
       "      <td>8.304</td>\n",
       "      <td>8.265</td>\n",
       "      <td>8.223</td>\n",
       "      <td>8.182</td>\n",
       "      <td>8.142</td>\n",
       "      <td>8.102</td>\n",
       "      <td>...</td>\n",
       "      <td>4.073</td>\n",
       "      <td>4.066</td>\n",
       "      <td>4.057</td>\n",
       "      <td>4.049</td>\n",
       "      <td>4.042</td>\n",
       "      <td>4.033</td>\n",
       "      <td>4.025</td>\n",
       "      <td>4.017</td>\n",
       "      <td>4.008</td>\n",
       "      <td>4.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.945</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.856</td>\n",
       "      <td>9.811</td>\n",
       "      <td>9.755</td>\n",
       "      <td>9.705</td>\n",
       "      <td>9.654</td>\n",
       "      <td>9.609</td>\n",
       "      <td>9.557</td>\n",
       "      <td>9.504</td>\n",
       "      <td>...</td>\n",
       "      <td>4.496</td>\n",
       "      <td>4.489</td>\n",
       "      <td>4.481</td>\n",
       "      <td>4.472</td>\n",
       "      <td>4.464</td>\n",
       "      <td>4.456</td>\n",
       "      <td>4.448</td>\n",
       "      <td>4.440</td>\n",
       "      <td>4.431</td>\n",
       "      <td>4.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.668</td>\n",
       "      <td>11.611</td>\n",
       "      <td>11.551</td>\n",
       "      <td>11.483</td>\n",
       "      <td>11.412</td>\n",
       "      <td>11.343</td>\n",
       "      <td>11.274</td>\n",
       "      <td>11.207</td>\n",
       "      <td>11.136</td>\n",
       "      <td>11.061</td>\n",
       "      <td>...</td>\n",
       "      <td>4.938</td>\n",
       "      <td>4.929</td>\n",
       "      <td>4.921</td>\n",
       "      <td>4.910</td>\n",
       "      <td>4.902</td>\n",
       "      <td>4.893</td>\n",
       "      <td>4.884</td>\n",
       "      <td>4.876</td>\n",
       "      <td>4.867</td>\n",
       "      <td>4.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23.030</td>\n",
       "      <td>22.886</td>\n",
       "      <td>22.742</td>\n",
       "      <td>22.597</td>\n",
       "      <td>22.451</td>\n",
       "      <td>22.300</td>\n",
       "      <td>22.143</td>\n",
       "      <td>21.979</td>\n",
       "      <td>21.807</td>\n",
       "      <td>21.622</td>\n",
       "      <td>...</td>\n",
       "      <td>7.861</td>\n",
       "      <td>7.844</td>\n",
       "      <td>7.826</td>\n",
       "      <td>7.806</td>\n",
       "      <td>7.786</td>\n",
       "      <td>7.764</td>\n",
       "      <td>7.745</td>\n",
       "      <td>7.723</td>\n",
       "      <td>7.700</td>\n",
       "      <td>7.679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             10.196            10.144            10.092            10.040   \n",
       "std               2.378             2.355             2.332             2.310   \n",
       "min               4.149             4.144             4.139             4.134   \n",
       "25%               8.442             8.413             8.378             8.342   \n",
       "50%               9.945             9.900             9.856             9.811   \n",
       "75%              11.668            11.611            11.551            11.483   \n",
       "max              23.030            22.886            22.742            22.597   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              9.987             9.934             9.880             9.824   \n",
       "std               2.287             2.263             2.240             2.215   \n",
       "min               4.129             4.125             4.121             4.117   \n",
       "25%               8.304             8.265             8.223             8.182   \n",
       "50%               9.755             9.705             9.654             9.609   \n",
       "75%              11.412            11.343            11.274            11.207   \n",
       "max              22.451            22.300            22.143            21.979   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count         10000.000          10000.000  ...           10000.000   \n",
       "mean              9.767              9.708  ...               4.522   \n",
       "std               2.190              2.164  ...               0.655   \n",
       "min               4.113              4.110  ...               2.322   \n",
       "25%               8.142              8.102  ...               4.073   \n",
       "50%               9.557              9.504  ...               4.496   \n",
       "75%              11.136             11.061  ...               4.938   \n",
       "max              21.807             21.622  ...               7.861   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                4.514               4.505               4.497   \n",
       "std                 0.655               0.654               0.653   \n",
       "min                 2.320               2.312               2.307   \n",
       "25%                 4.066               4.057               4.049   \n",
       "50%                 4.489               4.481               4.472   \n",
       "75%                 4.929               4.921               4.910   \n",
       "max                 7.844               7.826               7.806   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                4.489               4.480               4.472   \n",
       "std                 0.652               0.651               0.651   \n",
       "min                 2.304               2.300               2.298   \n",
       "25%                 4.042               4.033               4.025   \n",
       "50%                 4.464               4.456               4.448   \n",
       "75%                 4.902               4.893               4.884   \n",
       "max                 7.786               7.764               7.745   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count           10000.000           10000.000           10000.000  \n",
       "mean                4.464               4.456               4.447  \n",
       "std                 0.650               0.649               0.648  \n",
       "min                 2.296               2.292               2.288  \n",
       "25%                 4.017               4.008               4.001  \n",
       "50%                 4.440               4.431               4.424  \n",
       "75%                 4.876               4.867               4.859  \n",
       "max                 7.723               7.700               7.679  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:58:10.007266Z",
     "start_time": "2020-12-14T14:58:09.186197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.964</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.052</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.041</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.036</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.041</td>\n",
       "      <td>1.044</td>\n",
       "      <td>1.047</td>\n",
       "      <td>...</td>\n",
       "      <td>1.216</td>\n",
       "      <td>1.214</td>\n",
       "      <td>1.213</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.105</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.097</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.098</td>\n",
       "      <td>1.101</td>\n",
       "      <td>1.106</td>\n",
       "      <td>1.112</td>\n",
       "      <td>1.119</td>\n",
       "      <td>...</td>\n",
       "      <td>1.636</td>\n",
       "      <td>1.637</td>\n",
       "      <td>1.633</td>\n",
       "      <td>1.630</td>\n",
       "      <td>1.626</td>\n",
       "      <td>1.626</td>\n",
       "      <td>1.621</td>\n",
       "      <td>1.619</td>\n",
       "      <td>1.616</td>\n",
       "      <td>1.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.207</td>\n",
       "      <td>1.203</td>\n",
       "      <td>1.201</td>\n",
       "      <td>1.202</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.211</td>\n",
       "      <td>1.220</td>\n",
       "      <td>1.232</td>\n",
       "      <td>1.247</td>\n",
       "      <td>1.263</td>\n",
       "      <td>...</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.333</td>\n",
       "      <td>2.328</td>\n",
       "      <td>2.331</td>\n",
       "      <td>2.320</td>\n",
       "      <td>2.315</td>\n",
       "      <td>2.321</td>\n",
       "      <td>2.306</td>\n",
       "      <td>2.311</td>\n",
       "      <td>2.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean              inf             inf             inf             inf   \n",
       "std               nan             nan             nan             nan   \n",
       "min             0.964           0.955           0.945           0.924   \n",
       "25%             1.052           1.046           1.041           1.039   \n",
       "50%             1.105           1.100           1.097           1.096   \n",
       "75%             1.207           1.203           1.201           1.202   \n",
       "max               inf             inf             inf             inf   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean              inf             inf             inf             inf   \n",
       "std               nan             nan             nan             nan   \n",
       "min             0.902           0.884           0.868           0.849   \n",
       "25%             1.037           1.036           1.037           1.041   \n",
       "50%             1.096           1.098           1.101           1.106   \n",
       "75%             1.206           1.211           1.220           1.232   \n",
       "max               inf             inf             inf             inf   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count       10000.000        10000.000  ...         10000.000   \n",
       "mean              inf              inf  ...               inf   \n",
       "std               nan              nan  ...               nan   \n",
       "min             0.828            0.806  ...             0.212   \n",
       "25%             1.044            1.047  ...             1.216   \n",
       "50%             1.112            1.119  ...             1.636   \n",
       "75%             1.247            1.263  ...             2.334   \n",
       "max               inf              inf  ...               inf   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean                inf               inf               inf               inf   \n",
       "std                 nan               nan               nan               nan   \n",
       "min               0.211             0.210             0.210             0.209   \n",
       "25%               1.214             1.213             1.209             1.205   \n",
       "50%               1.637             1.633             1.630             1.626   \n",
       "75%               2.333             2.328             2.331             2.320   \n",
       "max                 inf               inf               inf               inf   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean                inf               inf               inf               inf   \n",
       "std                 nan               nan               nan               nan   \n",
       "min               0.210             0.208             0.207             0.208   \n",
       "25%               1.206             1.206             1.200             1.199   \n",
       "50%               1.626             1.621             1.619             1.616   \n",
       "75%               2.315             2.321             2.306             2.311   \n",
       "max                 inf               inf               inf               inf   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count         10000.000  \n",
       "mean                inf  \n",
       "std                 nan  \n",
       "min               0.207  \n",
       "25%               1.199  \n",
       "50%               1.612  \n",
       "75%               2.301  \n",
       "max                 inf  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:58:10.794600Z",
     "start_time": "2020-12-14T14:58:10.009576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.438</td>\n",
       "      <td>1.386</td>\n",
       "      <td>1.341</td>\n",
       "      <td>1.389</td>\n",
       "      <td>1.460</td>\n",
       "      <td>1.537</td>\n",
       "      <td>1.621</td>\n",
       "      <td>1.711</td>\n",
       "      <td>1.805</td>\n",
       "      <td>1.907</td>\n",
       "      <td>...</td>\n",
       "      <td>5.449</td>\n",
       "      <td>5.460</td>\n",
       "      <td>5.481</td>\n",
       "      <td>5.500</td>\n",
       "      <td>5.470</td>\n",
       "      <td>5.480</td>\n",
       "      <td>5.465</td>\n",
       "      <td>5.468</td>\n",
       "      <td>5.469</td>\n",
       "      <td>5.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.738</td>\n",
       "      <td>8.409</td>\n",
       "      <td>6.290</td>\n",
       "      <td>8.154</td>\n",
       "      <td>12.433</td>\n",
       "      <td>17.565</td>\n",
       "      <td>23.237</td>\n",
       "      <td>29.342</td>\n",
       "      <td>35.661</td>\n",
       "      <td>42.366</td>\n",
       "      <td>...</td>\n",
       "      <td>234.268</td>\n",
       "      <td>235.796</td>\n",
       "      <td>237.559</td>\n",
       "      <td>239.409</td>\n",
       "      <td>236.782</td>\n",
       "      <td>237.922</td>\n",
       "      <td>236.712</td>\n",
       "      <td>237.226</td>\n",
       "      <td>237.697</td>\n",
       "      <td>239.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.022</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.007</td>\n",
       "      <td>...</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.057</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.054</td>\n",
       "      <td>1.054</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.048</td>\n",
       "      <td>1.047</td>\n",
       "      <td>1.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.068</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.063</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.063</td>\n",
       "      <td>1.066</td>\n",
       "      <td>1.069</td>\n",
       "      <td>1.074</td>\n",
       "      <td>1.079</td>\n",
       "      <td>...</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1.447</td>\n",
       "      <td>1.445</td>\n",
       "      <td>1.443</td>\n",
       "      <td>1.441</td>\n",
       "      <td>1.440</td>\n",
       "      <td>1.437</td>\n",
       "      <td>1.435</td>\n",
       "      <td>1.434</td>\n",
       "      <td>1.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.162</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.161</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.167</td>\n",
       "      <td>1.173</td>\n",
       "      <td>1.182</td>\n",
       "      <td>1.193</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.224</td>\n",
       "      <td>...</td>\n",
       "      <td>2.169</td>\n",
       "      <td>2.164</td>\n",
       "      <td>2.162</td>\n",
       "      <td>2.159</td>\n",
       "      <td>2.159</td>\n",
       "      <td>2.156</td>\n",
       "      <td>2.152</td>\n",
       "      <td>2.148</td>\n",
       "      <td>2.143</td>\n",
       "      <td>2.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1168.587</td>\n",
       "      <td>624.077</td>\n",
       "      <td>580.461</td>\n",
       "      <td>639.239</td>\n",
       "      <td>998.509</td>\n",
       "      <td>1562.583</td>\n",
       "      <td>2155.970</td>\n",
       "      <td>2781.807</td>\n",
       "      <td>3422.119</td>\n",
       "      <td>4098.042</td>\n",
       "      <td>...</td>\n",
       "      <td>23302.398</td>\n",
       "      <td>23457.070</td>\n",
       "      <td>23629.078</td>\n",
       "      <td>23812.221</td>\n",
       "      <td>23547.904</td>\n",
       "      <td>23661.814</td>\n",
       "      <td>23539.492</td>\n",
       "      <td>23590.789</td>\n",
       "      <td>23638.453</td>\n",
       "      <td>23781.338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.438               1.386               1.341   \n",
       "std                12.738               8.409               6.290   \n",
       "min                 0.934               0.923               0.911   \n",
       "25%                 1.022               1.016               1.012   \n",
       "50%                 1.068               1.065               1.063   \n",
       "75%                 1.162               1.162               1.161   \n",
       "max              1168.587             624.077             580.461   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.389               1.460               1.537   \n",
       "std                 8.154              12.433              17.565   \n",
       "min                 0.895               0.880               0.861   \n",
       "25%                 1.008               1.007               1.005   \n",
       "50%                 1.062               1.062               1.063   \n",
       "75%                 1.162               1.167               1.173   \n",
       "max               639.239             998.509            1562.583   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.621               1.711               1.805   \n",
       "std                23.237              29.342              35.661   \n",
       "min                 0.839               0.824               0.806   \n",
       "25%                 1.005               1.005               1.006   \n",
       "50%                 1.066               1.069               1.074   \n",
       "75%                 1.182               1.193               1.209   \n",
       "max              2155.970            2781.807            3422.119   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count            10000.000  ...             10000.000             10000.000   \n",
       "mean                 1.907  ...                 5.449                 5.460   \n",
       "std                 42.366  ...               234.268               235.796   \n",
       "min                  0.784  ...                 0.201                 0.201   \n",
       "25%                  1.007  ...                 1.060                 1.057   \n",
       "50%                  1.079  ...                 1.450                 1.447   \n",
       "75%                  1.224  ...                 2.169                 2.164   \n",
       "max               4098.042  ...             23302.398             23457.070   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  5.481                 5.500                 5.470   \n",
       "std                 237.559               239.409               236.782   \n",
       "min                   0.200                 0.200                 0.199   \n",
       "25%                   1.056                 1.054                 1.054   \n",
       "50%                   1.445                 1.443                 1.441   \n",
       "75%                   2.162                 2.159                 2.159   \n",
       "max               23629.078             23812.221             23547.904   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  5.480                 5.465                 5.468   \n",
       "std                 237.922               236.712               237.226   \n",
       "min                   0.198                 0.198                 0.197   \n",
       "25%                   1.051                 1.049                 1.048   \n",
       "50%                   1.440                 1.437                 1.435   \n",
       "75%                   2.156                 2.152                 2.148   \n",
       "max               23661.814             23539.492             23590.789   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count             10000.000             10000.000  \n",
       "mean                  5.469                 5.482  \n",
       "std                 237.697               239.140  \n",
       "min                   0.197                 0.197  \n",
       "25%                   1.047                 1.046  \n",
       "50%                   1.434                 1.433  \n",
       "75%                   2.143                 2.142  \n",
       "max               23638.453             23781.338  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:58:12.710150Z",
     "start_time": "2020-12-14T14:58:10.797072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:58:14.543953Z",
     "start_time": "2020-12-14T14:58:12.712602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xd4HOW99vHvbJdWvVuWe3ksuTdMM8E1mBrANsUhoSRAAiHkJIcWCHkTCJwAKUASSOjVlBhCYjAG0wMYsI27H9yruiVbxZK2vX/MSpZBtiVZu7PS/j7X5cvS7uzOvaPV3pr2jBEKhRBCCBG/bFYHEEIIYS0pAiGEiHNSBEIIEeekCIQQIs5JEQghRJyTIhBCiDgnRSDEESilnlBK3dHOabcppaYf6/MIEW1SBEIIEeekCIQQIs45rA4gxLFSSm0D/gJcAgwC5gO3AE8AJwNLgTla66rw9GcDdwG9gS+BH2mt14fvGws8CgwBXgcOOfVeKXUmcAfQH1gHXK21XtWJzD8EbgQygI/Cz7NHKWUAfwDmAR5gO3CR1nqNUup04F6gD7Af+KPW+t6OzluIr5M1AtFTnA/MAIYCZwFvYJZBNub7/DoApdRQ4Hng+vB9rwP/Vkq5lFIu4FXgacwP6JfCz0v4sWOBx4CrgEzgYeA1pZS7I0GVUlMxi2gu0Avzw35++O6ZwCnh15EanqYyfN+jwFVa62RgBPBOR+YrxOHIGoHoKR7QWpcCKKU+BMq01ivC378CTAtPdwGwUGv9Vvi+e4GfAicCQcAJ/ElrHQJeVkr9T6t5XAk8rLVeGv7+SaXULcDxwPsdyDoPeExrvTyc4WagSinVH/ABycAw4LPmNZUwH1CklFoZXrup6sA8hTgsWSMQPUVpq68PtPF9UvjrfMy/wAHQWgeBnZibifKB3eESaLa91df9gJ8rpaqb/2FupsnvYNavZ6jF/Ku/t9b6HeBBzE1dZUqpvyulUsKTng+cDmxXSr2vlDqhg/MVok2yRiDizR5gZPM34W3yfYDdmPsDeiuljFZl0BfYHP56J3Cn1vrOLsjQr1UGL+ampt0AWuv7gfuVUjnAi8D/ArdprT8HzlFKOYFrw/f1OcYsQkgRiLjzInCTUmoa8AHmZqFG4OPw/X7gOqXUXzH3NRwHvBu+7x/AK0qpt4HPgETgVOADrXVNBzI8DzyvlHoOWA/8Dliqtd6mlJqIuaa+HKgDGoBgeP/FHOA/Wut9Sqn9mJuyhDhmsmlIxBWttQa+CzwAVGB+2J+ltW7SWjcB5wGXAnsx9ycsaPXYL4AfYm66qQI2haftaIa3gduAfwLFmEc6XRi+OwWzcKowNx9VAveE77sE2BYugasx9zUIccwMuTCNEELEN1kjEEKIOCdFIIQQcU6KQAgh4pwUgRBCxLlucfhoMBgMBQKd26lttxt09rGRFKu5IHazSa6OkVwdF6vZOpvL6bRXYA6lckTdoggCgRDV1fWdemxaWmKnHxtJsZoLYjeb5OoYydVxsZqts7mys5O3H30q2TQkhBBxT4pACCHinBSBEELEuW6xj6AtgYCfqqpy/P6mI05XWmoQi2dPW53L4XCRnp6N3d5t3wJCiC7SbT8FqqrK8XgS8XrzMAzjsNPZ7TYCgdgbm8vKXKFQiLq6/VRVlZOV1cuSDEKI2NFtNw35/U14vSlHLAHRNsMw8HpTjro2JYSID922CAApgWMgy04I0azbbhpqjwZfAPxBPI5u3XdCCBFRPfoTsqFuH1WVpdQ2+rv8uWtqaliw4KUOP+4Xv7iOmpqOXMNECCEiq0cXQZqjiT5GGXX7ymn0B7r0uWtra3jllW8Wgd9/5NK59977SU5O7tIs7XG0XEKI+NWjNw3hzSHorye/qZydVQ5yMjJx2Lum+x566AF2797NpZdejMPhwOVykZyczPbt25k/fwE33/xzSktLaWpqYs6cCznnnPMAmD37LB555Gmamhr42c+uZdSoMaxevYrs7Gzuvvs+3G4PGzdq7rnnLhobG8jPL+Dmm39FVdVe7rjjV/zjH08BUFy8hxtv/BlPPfUCGzas58EH/0h9fT1paWnccsuvycrK4tprr2TIEMWqVV8yffq3ueii73bJaxdC9Cw9oggWri3ltTUlbd5nACFfPbCRJnbgcrbvJZ89Io8zhuce9v6rr/4JW7Zs5oknnmP58i+44YbreeqpF8jP7w3AzTf/ipSUVBobG/jBD77HqadOJTU17ZDn2LVrJ7/+9Z3ceOOt3HbbTbz33jt8+9unc8cdt3P99f/L2LHjeeSRh3j88X/w05/+HJ/Pz549u8nP782SJYuZOnUGfr+fP/3pHu666z7S09NZsmQxf//7X7jlltsB8Pl8PPro0+16zUKI+NQjiuCIDMDhwfAfwEUjTX4Dl8Pe5bMpLBzeUgIAL700nw8+eA+AsrJSdu7c+Y0i6NUrnyFDFABKDaO4eA+1tbXU1NQwdux4AGbNOpPbbrsRgKlTp7NkyVtccsmlvPPOW/y//3cXO3ZsY8uWzfzsZ9cAEAwGyMzMapnHtGkzuvy1CiF6logVgVLqMeBMoExrPSJ8WwbwAtAf2AbM1VpXHeu8zhiee9i/3ltO3PLVY6/azIGQk32J/chOTjjW2R4iIeHg8y1f/gVffPEZDz/8OB6Ph2uvvZKmpsZvPMbpdLZ8bbPZCQS+OU1r06bN5LbbbuRb35oCGPTp05fNmzcxYMBAHn748aPmEkKItkRyZ/ETwGlfu+0mYInWegiwJPx9dDgTCab2I8Fowlu/m30Hju1kqsTEROrr2x4Wtq6uluTkFDweD9u3b2PdujXtft6kpCSSk1NYuXIFAIsWLWTMmHEA9O5dgM1m58knH2n5S79v335UV1exZs0qwNwpvGXL5mN5aUKIOBOxNQKt9QdKqf5fu/kc4NTw108C7wE3RirD14XcKQST8kmp3U3F/j3U2Qvwujq3CFJT0xg5cjSXXDIXt9tDRkZGy32TJp3Iq68uYN682fTt24+iohEdeu5bb/11q53Fvbn55ttb7ps6dQZ//eufeeml1wBzreKOO/6PP/3pXmprawkEAsydexEDBw7q1OsSQsQfI5IDn4WL4D+tNg1Va63Twl8bQFXz90fS1hXKtN5Afn7/TuUK7duFrb6cErJIy87HHYF9Bt3Bnj3bUGrYN26X8Zk6RnJ1TKzmgtjN1tlcTqd9GTDhaNNZtrNYax1SSrWrhdq6QlkoFGrXgmlzAXp7EfI3kdtUyZ4KJ5mZWThs0T2lIhbecKFQ21d+62lXaYo0ydUxsZoLYjfbMVyhrF3TRfuEslKlVC+A8P9lUZ6/yTAIpfQlaPfQK1RKRVU1wRgcqloIIaIh2kXwGvD98NffB/4V5fkfZLMRShtAyHCQF9hD+b7amLxugRBCRFrEikAp9Tzwifml2qWUugK4G5ihlNoITA9/bx27k1D6AGyESG/czd66Ix++KYQQPVEkjxq66DB3TYvUPDvF4SGU0pfE/dtoqNtDrbMvSe6ef56dEEI069GDzrVXyJNKIDGHDKOGA/vKaPLH3lEDQggRKVIEYSFvHgFnMnlUUFldRTDYtfsLZsyYDEBFRTm33npDm9Nce+2VbNiwrkvnK4QQRyNF0MwwCKX2JWhz0itYTPn+2ojMJisrmzvu+H1EnlsIITpDNoa3ZnMQShuAbe9G0hr3sO/AAFITXG1O+re/PUBOTi7nnz8XgEcffRi73c6KFcuoqdmP3+/nhz/8EZMnn3rI44qL93DDDdfz3HMv09jYwO9+9//YtGkjffv2p7FRdlYLIaKvRxSBe8PLeNbPb/M+wzA6flho0I+nzyns6jebRmfvNs88njZtBvff/4eWInj33be5774HmDPnQrzeJKqrq7nqqks5+eRvHfb6wK+88jJut4dnn32ZTZs2csUVcr0AIUT09Ygi6HI2BwGnl2yjml3VieRmZmL72of50KHDqKraS0VFOVVVVSQnJ5OZmcX999/HypUrMAwb5eXl7N1beciw0K2tXLmC2bMvBGDw4CEMGjQ44i9NCCG+rkcUQeOw2TQOm93mfZ0eyiEYwNi7kbxgKeX7E8hJ9X5jkilTpvPuu0vYu7eSqVNnsnjxG1RXV/Poo8/gcDiYPfssmpqObZRTIYSINNlZfDg2O6HUfjgI4G0opq7xm9f8nTp1BkuWLObdd5cwZcp0amtrSU9Px+FwsHz5F5SUFB9xFqNHj+WttxYBsGXLJjZv3hSRlyKEEEciRXAkzgSC3jxSjTrq9lUQ+NohpQMHDqK+vo7s7GyysrKYOXMWGzas53vfu4BFixbSr1//Iz79uefO5sCBeubNm80jjzzM0KHfHAlUCCEiLaLDUHcVny8Q+vrIeyUl28nL63fUxx7zKJ+hEMbeTRBopMTVn9y0pM4/V1fm6gKHW4Y9bQTGSJNcHROruSB2sx3D6KPtGoZa1giOJnx+gY0QyY3F1DT4rE4khBBdSoqgPRxugt48Uox6Duz/5iYiIYTozrp1EURzs1YoMYuAPZEcKtlbG3urjh3VHTYJCiGio9sWgcPhoq5uf/Q+0AyDUGoBdoK4D5TS4AtEZ74REAqFqKvbj8PR9lnTQoj40m3PI0hPz6aqqpza2uojTtepM4uPpAFsTcUU1zSQ4vVymJOGj6rLc3WQw+EiPT3bsvkLIWJHty0Cu91BVlavo07X5UcBNNWR+NQp2A64eO/kF5g9tm+nniZWj04QQsSfbrtpyDIuL/5Tf0uhbSe1Hz/EvgNyFJEQonuTIuiEpkGzqMo7hatDL/H8R6usjiOEEMdEiqAzDIPQlF/jNRrpu/6v7Kg6YHUiIYToNCmCTgpkDGXf0Au42PY2Ly75wOo4QgjRaVIExyBw0g2E7G6m7HmIZTuPfPSSEELEKimCYxBKzKZ+3I85zf45by/5t5ykJYTolqQIjpFv3FXUuXKYV/Mo72+ssDqOEEJ0mBTBsXIm4DvhF4y1beLLDxcQlLUCIUQ3Y0kRKKV+qpRao5Raq5S63ooMXclXOIcaTwEX1j/N2xvKrI4jhBAdEvUiUEqNAH4IHAeMBs5USnXvi/XanQRP/B9G2rax7qOX8MvopEKIbsSKNYJCYKnWul5r7QfeB86zIEeXalLnUZPYl+82PMeidUe+RKUQQsQSK8YaWgPcqZTKBA4ApwNfHOkBdrtBWlpip2Zmt9s6/diOMqbfQuFrV/PcJy+SdMJNOOyH79lo5uqoWM0muTpGcnVcrGaLdK6oF4HWer1S6v+AxUAd8CVwxDGdA4FQpwdoi+rgbr1n4fIO5Ls183l56YWcVpQXG7k6KFazSa6OkVwdF6vZjuFSle2azpKdxVrrR7XW47XWpwBVwFdW5OhyNjuccD3KtouNnyyQ8wqEEN2CVUcN5YT/74u5f+A5K3JEQtOQs9nv6c159S/w0eZKq+MIIcRRWXUewT+VUuuAfwPXaK17zvgMNgeh465hjG0zy/77H1krEELEPEsuTKO1nmzFfKOlqXAudZ/cx6x9z7N813cY3yfN6khCCHFYcmZxJDg8+MZdxcn2tXz40dtWpxFCiCOSIogQ/6jvccCezCnlz7ChtMbqOEIIcVhSBBESciXRMOoyvm3/grc++tDqOEIIcVhSBBEUGPdDmgwPY3Y/xba9sXdsshBCgBRBRIU86dQWXczZto95/ePPrY4jhBBtkiKIsNCEH4FhY9CWJyivbbQ6jhBCfIMUQYQFk3pRPehcZtve499LV1sdRwghvkGKIApsk67FZfhJW/8ktY1+q+MIIcQhpAiiIJA+iL29Z3ARb/Lv5ZusjiOEEIeQIogS+wnXkWLUE1zxJI3+oNVxhBCihRRBlPhzx1CeNYmLg/9m8ZodVscRQogWUgRR5Djhp+QY1VR89iwBuZylECJGSBFEkb/PZCqTC5nTuIC31+2xOo4QQgBSBNFlGNhOuJ4BtlLWvvOsDFEthIgJUgRR5h90GtWevpyxbz7Ld/acyzAIIbovKYJos9kJHncNI2zbWPHff1mdRgghpAisECiaTa0rm2+VP8fG8lqr4wgh4pwUgRXsbozjr+FE+zo++PAtq9MIIeKcFIFFXMddRr09mQm7HmfPvgar4wgh4pgUgVXcydSMuILp9uW899E7VqcRQsQxKQIL2SdeSb0tiRFbHqaqvsnqOEKIOCVFYKGQO4W9RVcwzbaMtz981+o4Qog4JUVgMc/xV1FnS2Loxr9RXe+zOo4QIg5JEVgs5E6hquhyphtfsET2FQghLCBFEAOa1woGf/UQ1QdkrUAIEV2WFIFS6mdKqbVKqTVKqeeVUh4rcsSKkDuVvUWXMd34nCUfyb4CIUR0Rb0IlFK9geuACVrrEYAduDDaOWJNwvFXU294GaT/xj5ZKxBCRJFVm4YcQIJSygEkAnE/JnPInUpl0eXMMD7nHdlXIISIIsOKoZCVUj8F7gQOAIu11vOONH0wGAwFAp3LabfbCARi79KQbeZq2E/jH0exMtCfof+zmLREV+xkiwGSq2MkV8fFarbO5nI67cuACUebztGZUMdCKZUOnAMMAKqBl5RS39VaP3O4xwQCIaqr6zs1v7S0xE4/NpLazuWgbuSPOWnl3Tz12gvMOu38GMpmPcnVMZKr42I1W2dzZWcnt2s6KzYNTQe2aq3LtdY+YAFwogU5YpL3+B9Qac9mzKb72VvXaHUcIUQcsKIIdgDHK6USlVIGMA1Yb0GO2OTwsG/CzxllbOaztw67kiSEEF0m6kWgtV4KvAwsB1aHM/w92jliWfK4iyh29eOkXQ+xp0quVyCEiKyo7yMA0FrfDtxuxby7BZudphNvZuB7VzN/8V/Jv+AGqxMJIXowObM4RiUWncHWxDHMKH+crXtKrI4jhOjBpAhilWHgmP5b0qmlbPHdVqcRQvRgUgQxzNNnLOuyT2dG7ausXrfK6jhCiB5KiiDGZZx2O0HDjuPDOwgEo3/ynxCi55MiiHHO1Hw2DryMU/wf88V/F1odRwjRA0kRdAN50/6HUlsOatWd1DUcsDqOEKKHkSLoBgxXIqUTb2UIO9nwxv1WxxFC9DBSBN1Er/HnsiZhIiftfoSS4h1WxxFC9CBSBN2FYeD+9l248VH9xm1WpxFC9CBSBN1IWu9hLO/9XSYfWMK6z9+0Oo4QooeQIuhm+s26mT1GLv0//xUNB+qsjiOE6AGkCLoZh8fLrkm/oV9oN9tfv8vqOEKIHqBdg86Fryj2OFADPAKMBW7SWi+OYDZxGAPGn8Enq6dzXPHT6K1zyBsw2upIQohurL1rBJdrrfcDM4F04BJABsCxUOZZd1NHIs7FPycYDFgdRwjRjbW3CIzw/6cDT2ut17a6TVggPTOPVcN+wTD/BjYt+rPVcYQQ3Vh7i2CZUmoxZhG8qZRKBmLvCs9xpnDqZSxzTWT8lgeo2rXO6jhCiG6qvUVwBXATMFFrXQ84gcsilkq0i2Gz4TrzTxzAjf31nxAK+KyOJITohtpbBCcAWmtdrZT6LnArsC9ysUR75fXqx8dDbmKQT1O8+B6r4wghuqH2FsHfgHql1Gjg58Bm4KmIpRIdMm7G93nPOZmiLQ9zYNeXVscRQnQz7S0Cv9Y6BJwDPKi1/guQHLlYoiMcNoOE0+9hbygZ++vXgr/B6khCiG6kvUVQo5S6GfOw0YVKKRvmfgIRIwYWFPDBkFvp7dtG2Ru/sTqOEKIbaW8RXAA0Yp5PUAIUALJBOsacPOMCFrlPo3D709ToJVbHEUJ0E+0qgvCH/7NAqlLqTKBBay37CGKMw2bQ6zv/xxZ6k77kp1BXanUkIUQ30K4iUErNBT4D5gBzgaVKqdmRDCY6Jz8rk5UT7sMTrKd+wVUgZx0LIY6iXWMNAb/EPIegDEAplQ28Dbzc0RkqpRTwQqubBgK/0lr/qaPPJdo2edKJPL/lGi6v+iM7PvwDCd/6X6sjCSFiWHv3EdiaSyCssgOPPYQ2jdFajwHGA/XAK515LtE2wzCYfM5PeMM4md5rHiC042OrIwkhYlh71wgWKaXeBJ4Pf38B8HoXzH8asFlrvb0Lnku0kuZ1EZx5D9vfOJeM13+E//vvEErItDqWECIGGaFQqF0TKqXOB04Kf/uh1vqY/4pXSj0GLNdaP3ik6YLBYCgQaF/Or7PbbQQCsTcsUrRyPfOv/3Dx6iuoyhpP5pWvge3o3R/vy6yjJFfHxGouiN1snc3ldNqXAROONl27i6CrKaVcwB5guNb6iIe3+HyBUHV1fafmk5aWSGcfG0nRyuUPhvjnM/dxbc2f2KMuxzn96OcYxPsy6yjJ1TGxmgtiN1tnc2VnJ7erCI7456FSqgZoqykMIKS1TulwsoNmYa4NyDGOEeSwGUw77yfMf3IDF+rHqMgfRahIDvgSQhx0xCLQWkdyGImLOLjPQURQVpKbtDPu4tN/b2fcezdQmzmEQK5c1UwIYbLkmsVKKS8wA1hgxfzj0fj+2Xw25l7Kgik4X7sco77C6khCiBhhSRForeu01plaaxnKOoouOHkUf8u+HUdjFbbXfgCBJqsjCSFigCVFIKxhMwx+8J2zuMd9LRmVX2Bf/HOw6GABIUTskCKIM0luB2fM/hEPhuaSseUVnJ/I2IFCxDspgjjUNz2B/mfcyguBU0lbcT+utbLPXoh4JkUQpyYNyKDixDt5PzCK5PduwrnjPasjCSEsIkUQx+aM78vCob9jfbCAxNevxFG+xupIQggLSBHEMcMwuH7GKO7PvoMKfwIJr87DXr3F6lhCiCiTIohzTruNm86dzK+Sfktdo4/EBXOheofVsYQQUSRFIEhyO/j5nNP4met2mg7UEHr6HGx1JVbHEkJEiRSBACDL6+K62WfxY36Jf38pSa9ehHGg0upYQogokCIQLfplJHLFed/h6sCNUL2d5FcvwmiotjqWECLCpAjEIUb0SuHSiy7mR/6fYdv7FcmvzpU1AyF6OCkC8Q2Th2Rz5pkXc6XvF1C5iZQFs7HVyWjhQvRUUgSiTZMHZTLrjAu5rOkGAtU7SVlwPraa3VbHEkJEgBSBOKwpQ7I48/Tzmdd0M0015aQuOA9b9VarYwkhupgUgTiiGSqbc759Jhc03EJ9XQ2pC87DUb7a6lhCiC4kRSCO6vSiXC46fRZzGm+jsgFSF5yPa9sSq2MJIbqIFIFol5nDcrj6nJmc7/sNmwJ5pLx+OZ61z1gdSwjRBaQIRLudPDCT286fzLzA7XyCOWqp95O7IRS0OpoQ4hhIEYgOGVeQxn1zj+Pa0A0sYBqJyx8k5c0fQVOd1dGEEJ0kRSA6rDA3mYcvHM+9rh/z+8A8nJvfIP2fZ8sRRUJ0U1IEolP6Zyby+LyxfJh1Ed9ruhHfvmLSXzpDdiIL0Q1JEYhOy0h08dc5I0kYMpUZ9b9hDzmkLLyUxM//JPsNhOhGpAjEMfE47dxxxjBmTprA9P2/5H3XqXg/u5fU1y6WoayF6CakCMQxsxkGPzqpP7ecPoqr6q/kt8bV2IuXkT5/Jq6tb1kdTwhxFA4rZqqUSgMeAUYAIeByrfUnVmQRXWdWYS5DspK48d8ePtg3iGfd/yDn9cs4MPJSak+8FRweqyMKIdpg1RrBn4FFWuthwGhgvUU5RBcbnO3lyXlj6T1wFCfv/SWLk88jYfUTpL84C0fJcqvjCSHaEPUiUEqlAqcAjwJorZu01nL1kx4kye3g92cXcfUpQ/lRxWx+ar8V34H9pC34Dt7//hb8B6yOKIRoxYo1ggFAOfC4UmqFUuoRpZTXghwiggzD4JKJfXjkojEsc47juOo7+SL9LBK/fJj0+TNx7llqdUQhRJgRCoWiOkOl1ATgU+AkrfVSpdSfgf1a69sO95hgMBgKBDqX0263EQjE3qGMsZoLuj5bXaOfO9/YwEvLdjEvewu38zCump0Ex1xCYMqvIDHTklxdRXJ1TKzmgtjN1tlcTqd9GTDhaNNZsbN4F7BLa938J+HLwE1HekAgEKK6ur5TM0tLS+z0YyMpVnNBZLLdcOpAJvZO4c7FDhb6fsPfCxYzcdXz2Ne/Rt3xN9FQdDHY7FHP1RUkV8fEai6I3WydzZWdndyu6aK+aUhrXQLsVEqp8E3TgHXRziGib8qQLOZfOoHxg3ozd/tZ/MDzR/alDCP5/ZtJe/lMHCXLrI4oRFyy6qihnwDPKqVWAWOA31mUQ0RZltfF3WcV8fuzi1jZ2Ivxu65jfsHtGHVlpP/zHFIWXYW9eovVMYWIK5acR6C1/pJ2bLcSPdeUIVlM6JPG/R9s4abVBn/z3ssDAz5ixPanSd/6Jg3D51E34WeEErOsjipEjydnFgvLJHsc/HLmUB65cDQebypnr/8WVyQ/TMmAOXjWPEPGMyeRuPQejAY5uliISJIiEJYb3TuVJ+eN5eYZQ1he5eakdWfxf/0fozZ/Mt4v/kzG0yeQuPQeOFBldVQheiQpAhET7DaD80b1YsEVE5kzJp9/aBeTtlzKw+oJDuSfjPeLP+N4cDSJn/4eo77c6rhC9ChSBCKmpHic/GLqYF68dAInDcjkrpUuTtp+Oc+OeobAgKkkLnuAzKeOJ+nd/8W+d6PVcYXoEaQIREzqm57AXWcV8uS8sQzO9vLLz2yctPX7PDPmBeqGzsajF5Dx/BRSFl6Kc/cnEOUTI4XoSaQIREwrykvmr7NH8uDskfTNSOS2T/2cor/D30a+QtW463GWLCft1TmkvXQGnnXzwRd7JwMJEeukCETMMwyDSf3See6KSTx8wSiGZifx+0+rOXX5idw37GXKTroTw99A8ru/IPOJ8SR9cCv2yg1Wxxai27DkPAIhOmtcQRrjZqexpng/j326g798WsJjzkGcUfgQV4wvYcCOl/CsfY6E1U/g6zWRA8Pn0TjwDHAmWB1diJglRSC6pRG9UvjDuSP4qqyW55fv5l9rS3l5FZw04Cq+N+16TqxdTMK6Z0l5+3qCzltpGnQ6Dep8fL1PAENWhIVoTYpAdGtDc5K4/TTFtZMHsGBVMS9/uYertu5lQOZE5ow+i3PTtpK25VXcmxfi2fAigaR8GoeeR4M6n0DGEKvjCxEToj4MdWf4fIGQjD4aPbGarT25mvxB3v6qnPnLd7MbI+SoAAAV/klEQVS+tBa3w8Z0lc35hamMb/wEj16Aa+f7GKEg/sxCGgefSeOgMwikD45oLitIro6L1WzHMPpozA5DLUTEuBw2Ti/KZVZhDhvKanl1VQlvbihj4dpSBmT04ZyRd3HWCQa5u9/AvXkh3qX34F16D/4MReOg02kcdCaBjKFgGFa/FCGiRtYILBKruSB2s3U2V31TgLe/KufVVSWsLt6P3WZwQv90ZhXmMCXPR/KON3FvXohzz2cYhPCnD6Zx4Ok09Z+OP3fMUfcp9LTlFWmxmgtiN5usEQhxjBJdds4ekcfZI/LYXFHH6+tKWbS+jI+27CXRaWfKkBOZNfZcjpvRRMK2N3Fv+g+Jyx/Eu+x+gglZNPWbSmP/afj6nELI1b4LfQjRnUgRiLgyKMvLT04ZyDWTB7B85z4WrS/j7a/KWbiujIxEJ1OGnMCUsWcxYWaIxF3v49r2Nq6tb+LZ8CIhmxNf/vE09Z9GU98pBNIGyiYk0SPIpiGLxGouiN1skcrV6A/y0ZZK3tLl/HfLXhr8QVI9Dr41OJOpQ7KZWJCEt2KFWQrbluCo+gqAQFI+TQWTcQ6bTnX6xJi7dkK8/Ry7Qqxmk01DQkSY22Fj2tBspg3NpsEX4JNtVbyzsYIlX1Xw2ppSvC47kwdlcurgK5k0/kZSGvfg2vEBrl0f4t66CNuGF8gC/JmFNPU5haaCk/HlTwJnotUvTYh2kSIQohWP086UIVlMGZJFkz/IZzuqeOerCj7YXMmi9WXYbQZjC1I5ecCpnHzcefSb6Sa9YRMN69/CtfNDElY9TuKXDxOyufDljcWXfzy+/En4cseDy2v1yxOiTbJpyCKxmgtiN5uVufzBEKv37OejLXv5aEslWyrNHH3SPEwtzGVifgpjClJxhxpxlnyOa+cHOHd/gqN8DUYoQMiw488eaZZC/vH4ek0k5EmLaGb5OXZcrGaL9KYhKQKLxGouiN1ssZRrz74GPtqyl/9ureSLnfto8gdxO2yM6Z3CpH7pHNcvnSHZXuy+Ohwly3DuWWr+K12BEWwihEEgc5hZDL0m4es1gWBSry7NGEvLq7VYzQWxm032EQgRg/JTPcwdm8/csfm4El28s6aYpdurWbq9ivs/2ApsJT3BycS+aUzqpziuaBJ5x3vA34Cz7MuWYvCsf5GE1U8A5s5nX954/Lnj8OWNw589AuxuS1+niA9SBEIco0SXg5MHZnLywEwAymoa+XyHWQqf7ahmsTYvrdk3PYEJfdIYVzCQccPGkj3hpxDw4ahYg7NkGY6S5ThLluHZ9G8AQjYX/uwRZinkjseXN95ca5BDVkUXkyIQoovlJLs5Y3guZwzPJRQKsbmyns+2V/HZ9mre3FDGglXFgLl/YWxBKuMK8hk3oJBeo38AgK2uxCyF0uU4S5aTsOZpjJWPABDw5uLPG48vd5y59pA1QobYFsdMikCICDIMg8FZXgZnebl4fAH+YIiN5bUs37mP5bv28d6mSl5bUwpArxQ34wpSGVeQxtiCKRQMnIVhGBBowlG53tzXULIMZ+kK3JtfByBk2AlkDMWXMxp/zhj8OaPwZw4Du8vKly26GSkCIaLIYTMozE2mMDeZeRMKCIZCbCqvY/mufazYtY//bq1i4boyADISnYzKTwn/68+wopG4R10OgFFXhrN0BY6ylTjLV+LesoiE9fOB8CalrCJsfcbjTh2OP2e0ObqqzW7Z6xaxzZIiUEptA2qAAODXWh91r7YQPZHNMBiak8TQnCQuHNebUCjE1r31rNi1j9V79rNqz37e21QJgNNuMCwniZH5KYzOT2FU/qlkDfy2+UShELaanThLV+Io+xJH+Spsq18gpanWvNuRiC97JP6c0fhzRuHLGU0wtb/sbxCAtWsEU7TWFRbOX4iYYxgGAzO9DMz0cv7ofAD21je1lMKqPft5+cs9PLdsNwD5KW5G5qcwKj+V0fkZDBp0Jo4hZwGQluqhZtsasxhKV+IsX0XCmicxAo0ABN2p+LNH4c8ZjS9nFP7skQSTC6Qc4pBsGhIixmUkuvjW4Cy+Ndgcy8gXCKLLaluKYdnOfby5wTwyKcFpY1huMiPykpk4KIsBKQXkDh2EoWabTxbwYa/aiDNcDo7yVSR8+RCJQT/QXA4j8WePwJ81An/2SAJpA+Tynj2cJSeUKaW2AlVACHhYa/33I00fDAZDgUDnctrtNgKBYKceG0mxmgtiN5vkalsoFGLPvgaW76hixc5qVu3ax7ri/fjCvzPZSW5GFaQyuiCVUQWpjOqdSrLHefAJ/A0YZeuhZCVGySrzX9naljWHkCuJUO4IQnmjCOWNJpQ3GrKGgq1zf0davbyOJFazdTaX02mP3TOLlVK9tda7lVI5wFvAT7TWHxxuejmzOLpiNZvkar8mf5CShgCfbixjbUkNa4tr2F51oOX+/hkJDO+VwvC8ZEb0SmZwlhenvdVf/eE1B0f5Ghzlq3FWrMFRvhbDb77OkN2NP7Pw4NpD9kj8mapdJ8DF4vJqFqvZeuSZxVrr3eH/y5RSrwDHAYctAiFEx7gcNkYVJNE36eBf/vsbfKwvqWVNyX7WFtfwyda9LFxrHrrqshuonGSG90puKYfemYUEsopoLJxrPkEwgH3fVhzlq1sKwr3xXySsfRqAkM2BP0MdLIbskfgzC2UU1m4g6kWglPICNq11TfjrmcBvop1DiHiT4nEyqX86k/qnA+YmpZKaRtYUm2sMa0v288qqYuYv3x2e3kFhbhLDcpMpyk2iMC+ZvLRBBNIH0zj0XMJPgm3/DnOtoXwNjorVuLe+RcL6F8y7DRuBtMGtymE4uCcAcp5DLLFijSAXeEUp1Tz/57TWiyzIIURcMwyDXikeeqV4mKGyAXOU1c0VdawtqWFDaQ3rS2p55otdBILmJuS0BCeF4VIoyk2iMDeZ7JS+BFP70TT4TPOJQyFsdcUtaw2O8jU4d3+M56sFLfPOSO6DP6sIf9bw8I7p4QST8uWIJYtEvQi01luA0dGerxDi6Bw2A5WThMpJAszRUBv9QTZV1LG+pIb1pTWsL63lyaU7aD5+I9ProjA3iaLcZArzzHLITMqnKSmfpgEzW57bqC/HUbGW5NqN+HZ9iaN8Da6tizEwnyjoTjOLIWs4/myzJAJpg8Hu/HpM0cXk8FEhxBG5HTaG55n7Dpo1+AJ8VX6wHNaV1vLfLXtpPvQkJ8lFUZ55BnVhXhKFOcmkJWbj63sqwbTTqWne8emrN4fPqFiLo3wtjoq1h5zrELK7zf0OWUUtaw6BzEJCrqQoL4WeTYpACNFhHqe9ZfiLZvVNAXRZbctaw/qSmpazosE8+W1YbjLj+mfQP9VNYW4SKZ5E/Hnj8eeNP/jkQT/26i3hcliDo2Id7q1vHhxCA4NAan+zFLKG48821yKCiTmyaamTpAiEEF0i0WVnbEEqYwtSW26rbfSjy2pZVxIuh9Ia3tl4cECBgjRPeOylJIblmpukUjxOAhlDCWQMPXSndF0xjop1LQXhLF+NZ/N/Wp4rmJAV3qw0vGUTUyB1gIyx1A5SBEKIiElyOxjfJ43xfVpdltPlYOnG8vBmpVrWFO/nrfA1GwB6p3paSqEwN4lhOcmkJToJNu936D+9ZVqjcT+OynU4ytdir1iHo2INCV/+AyPoAyDkSDDPd2hdEBnDZOjur5EiEEJEVVqii0n90pnUL73ltup6X8tmJV1Wy4ayWpZ8dXDNIS/Z3aocklG5SWR5XYTcKeY1oPOPPziDQBP2qk3hzUrmfodDzndoPqT1a0cthRIyorYMYo0UgRDCcmmJh57jAFDT4G8phQ2lNWworeX9TZUtO6SzvC6G5SYxLMc812FYbhI5SS4Mu4tAVpF5Mlzzk4VHZ22938FZ/Bmeja+2zC/gzcPIG4k3NbxzOqsobjYtSREIIWJSssfBhL5pTOh7cLNSXZOfr8rqDimHj7fuJXyaA+kJTrMcWhVErxQ3hmEQTOlLU0pfmgbOank+48Deg/sdKtbgrtpAwpZ3MEIBAEIOz8GjljLNcvFnFhJyp9CTSBEIIboNr8vxjR3SB3wBNpbXsaE0XA5ltTz1+cGT4FI9DlRO0iGblnqnebAZBqGEDHx9TsbX52QA7GmJVFdW4di70dznULnOPGpp8xskrHu+ZZ6B5hPiMgtb1h6CKX277SitUgRCiG4toY1DWZtPgtPhQ1l1WS3PL9/dMiKr12VvKYdhueZ5Dn3SwzuQ7e7wkBgjDt20VFeMo2I9jop12MMFccgJcc4kApnDWtYe/FmF3WasJSkCIUSP09ZJcL5AkC0V9WwoO1gO/1xZTKPfHN45wWmjqFcKgzMTw5uWkumfmYjDZoBhtDpqadrBGfkO4Ni7wdy8VLkOR8V63F+9QkLTU8DBcx4C4bUGsyCKYm44DSkCIURccNptqNwkVG4S54w0b/MHQ2yrNMthQ2ktmyrr+dfqEl5YYZaD22FjSLa3ZdgNlZPEoCwvbkd4E5AzAX/uWPy5Yw/OqGXH9LqDBVG+BvfmhS2TBN2pLZuVAuFy8GcMBYcnWovjEFIEQoi45bAZDM72Mjjby5nDzXH/K/fWsaPqQEs5bCitZdH6Mv65shgAu81gQEYiKsfL0FYFkeQOf5wesmP6tJZ5GU012Cs3HFIQCeuex/Cb14kIGXYCaYPMTUrNaw85owglZEZ+OUR8DkII0Y3YbQYDMhMZkJnIrMJcAILhq8DpstqWf59ur2bhurKWxxWkeVpKobkgsrwHh9sOuZLx95qIv9fEgzMLBrDv3x4+Gc4sB2fx53g2/su82+ml8rIVQGT3M0gRCCHEUdgMg4K0BArSEpg2NLvl9oq6JnRZLV+1KojWJ8Jlel2onEM3LfVO9WA07x+w2QmkDSSQNvDgMN6A0VCFo3I9hu9AVHY2SxEIIUQnZXldZA3I4KQBB89Kbh5f6WBB1LF0286WYbuT3HaGZicdUg4tO6XDQp50fL1PjNrrkCIQQogu1Nb4So3+IJsr6g7ZtLRg1cEjllx2g0FZB9cchuUmMTjLi8cZnbOapQiEECLC3A4bRXnJFLU6nDUQDLG9qt4shtI6dHkt72ys4NXVJQDYDBiVn8ID54+MeD4pAiGEsIDdZjAw08vATC+zCs3bmq8jrcPnOdT7ArgckT9bWYpACCFiROvrSJ86JCtq8+2eA2MIIYToMlIEQggR56QIhBAizkkRCCFEnJMiEEKIOCdFIIQQcU6KQAgh4pwUgRBCxDkjFApZnaE9yoHtVocQQohuph+QfbSJuksRCCGEiBDZNCSEEHFOikAIIeKcFIEQQsQ5KQIhhIhzUgRCCBHnpAiEECLO9egL0yilTgP+DNiBR7TWd1uUow/wFJALhIC/a63/rJT6NfBDzPMkAG7RWr8e5WzbgBogAPi11hOUUhnAC0B/YBswV2tdFcVMKjz/ZgOBXwFpWLC8lFKPAWcCZVrrEeHb2lxGSikD8z13OlAPXKq1Xh7FXPcAZwFNwGbgMq11tVKqP7Ae0OGHf6q1vjqKuX7NYX52SqmbgSsw34PXaa3fjGKuFwAVniQNqNZaj4ny8jrc50PU3mM9do1AKWUH/gLMAoqAi5RSRRbF8QM/11oXAccD17TK8ket9Zjwv6iWQCtTwvOfEP7+JmCJ1noIsCT8fdRo0xit9RhgPOab/ZXw3VYsryeA07522+GW0SxgSPjflcDfopzrLWCE1noU8BVwc6v7NrdadhH5UDtCLmjjZxf+PbgQGB5+zF/Dv7tRyaW1vqDVe+2fwIJWd0dreR3u8yFq77EeWwTAccAmrfUWrXUTMB84x4ogWuvi5sbWWtdg/qXR24os7XQO8GT46yeB71iYZRrmL6RlZ5ZrrT8A9n7t5sMto3OAp7TWIa31p0CaUqpXtHJprRdrrf3hbz8FCiIx747mOoJzgPla60at9VZgE+bvblRzhf/Kngs8H4l5H8kRPh+i9h7ryUXQG9jZ6vtdxMCHb3iVcyywNHzTtUqpVUqpx5RS6RZECgGLlVLLlFJXhm/L1VoXh78uwVxltcqFHPrLafXyana4ZRRL77vLgTdafT9AKbVCKfW+UmqyBXna+tnFyvKaDJRqrTe2ui3qy+trnw9Re4/15CKIOUqpJMzVz+u11vsxV+kGAWOAYuA+C2KdrLUeh7m6eY1S6pTWd2qtQ5hlEXVKKRdwNvBS+KZYWF7fYOUyOhyl1C8xNzk8G76pGOirtR4L/A/wnFIqJYqRYvJn18pFHPoHR9SXVxufDy0i/R7ryUWwG+jT6vuC8G2WUEo5MX/Iz2qtFwBorUu11gGtdRD4BxFaJT4SrfXu8P9lmNvhjwNKm1c1w/+XRTtX2Cxguda6NJzR8uXVyuGWkeXvO6XUpZg7ReeFP0AIb3qpDH+9DHNH8tBoZTrCzy4WlpcDOI9WByhEe3m19flAFN9jPbkIPgeGKKUGhP+yvBB4zYog4e2PjwLrtdZ/aHV76+165wJropzLq5RKbv4amBnO8Brw/fBk3wf+Fc1crRzyV5rVy+trDreMXgO+p5QylFLHA/tard5HXPhIuRuAs7XW9a1uz27eCauUGoi5o3FLFHMd7mf3GnChUsqtlBoQzvVZtHKFTQc2aK13Nd8QzeV1uM8Hovge67GHj2qt/Uqpa4E3MQ8ffUxrvdaiOCcBlwCrlVJfhm+7BfNIpjGYq3zbgKuinCsXeMU8WhMH8JzWepFS6nPgRaXUFZjDf8+Ncq7mYprBocvk91YsL6XU88CpQJZSahdwO3A3bS+j1zEP69uEebTTZVHOdTPgBt4K/1ybD3s8BfiNUsoHBIGrtdbt3aHbFblObetnp7Veq5R6EViHuSnrGq11IFq5tNaP8s39UBDF5cXhPx+i9h6TYaiFECLO9eRNQ0IIIdpBikAIIeKcFIEQQsQ5KQIhhIhzUgRCCBHnpAiEiDCl1KlKqf9YnUOIw5EiEEKIOCfnEQgRppT6LnAd4MIc9OvHwD7MIRFmYg78daHWujx8ctRDQCLm8AOXh8eKHxy+PRtzfP05mMMB/BqoAEYAy4DvNg//IITVZI1ACEApVQhcAJwUHps+AMwDvMAXWuvhwPuYZ8mCeSGRG8Pj/q9udfuzwF+01qOBEzEHLwNzRMnrMa+NMRDzbFIhYkKPHWJCiA6ahnkRnM/DQzMkYA7yFeTgYGTPAAuUUqlAmtb6/fDtTwIvhcdt6q21fgVAa90AEH6+z5rHsgkPI9Af+CjyL0uIo5MiEMJkAE9qrVtf0Qul1G1fm66zm3MaW30dQH73RAyRTUNCmJYAs5VSOWBek1gp1Q/zd2R2eJqLgY+01vuAqlYXK7kEeD98daldSqnvhJ/DrZRKjOqrEKITpAiEALTW64BbMa/Wtgrz2r+9gDrgOKXUGmAq8JvwQ74P3BOedkyr2y8Brgvf/jGQF71XIUTnyFFDQhyBUqpWa51kdQ4hIknWCIQQIs7JGoEQQsQ5WSMQQog4J0UghBBxTopACCHinBSBEELEOSkCIYSIc/8fOYMluMg1178AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['trainover', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
