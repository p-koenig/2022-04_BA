{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 3, #degree\n",
    "        'n': 15, #number of variables\n",
    "        'monomial_vars': None, #int or None\n",
    "        'laurent': False, #use Laurent polynomials (negative degree with up to -d)\n",
    "        'neg_d': 0,#int or None\n",
    "        'neg_d_prob': 0,\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': 15,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 1,\n",
    "        'a_min': -1,\n",
    "        'polynomial_data_size': 50000,  #number of generated polynomials (for loading)\n",
    "        'lambda_nets_total': 50000, #number of lambda-nets to train\n",
    "        'noise': 0,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'border_min': 0.2, #needs to be between 0 and (x_max-x_min)/2\n",
    "        'border_max': 0.4,\n",
    "        'lower_degree_prob': 0.5,\n",
    "        'a_zero_prob': 0.25,\n",
    "        'a_random_prob': 0.1,         \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sample_sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000, #lambda-net training dataset size\n",
    "    },    \n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': False,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "    \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "    },    \n",
    "    'computation':{\n",
    "        'n_jobs': 20,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n] \n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "    \n",
    "    \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 816\n",
      "[[3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import flatten, rec_gen, gen_monomial_identifier_list\n",
    "\n",
    "list_of_monomial_identifiers_extended = []\n",
    "\n",
    "if laurent:\n",
    "    variable_sets = [list(flatten([[_d for _d in range(d+1)], [-_d for _d in range(1, neg_d+1)]])) for _ in range(n)]\n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)    \n",
    "        \n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity:' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)     \n",
    "        \n",
    "    list_of_monomial_identifiers = []\n",
    "    for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "        if np.sum(monomial_identifier) <= d:\n",
    "            if monomial_vars == None or len(list(filter(lambda x: x != 0, monomial_identifier))) <= monomial_vars:\n",
    "                list_of_monomial_identifiers.append(monomial_identifier)        \n",
    "else:\n",
    "    variable_list = ['x'+ str(i) for i in range(n)]\n",
    "    list_of_monomial_identifiers = gen_monomial_identifier_list(variable_list, d, n)\n",
    "            \n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "#print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "#print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'KeyError'>\n",
      "<class 'KeyError'>\n",
      "<class 'KeyError'>\n"
     ]
    }
   ],
   "source": [
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d']) if not laurent else len(list_of_monomial_identifiers)\n",
    "\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "    \n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sample_sparsity')*config['data']['sample_sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='lambda_net'))\n",
    "generate_directory_structure()\n",
    "generate_lambda_net_directory()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lnets_50000_75-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_15_d_3_negd_0_prob_0_spars_15_amin_-1_amax_1_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n",
      "poly_50000_train_5000_var_15_d_3_negd_0_prob_0_spars_15_amin_-1_amax_1_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1_diffX\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_lambda_net_data)\n",
    "\n",
    "print(path_identifier_polynomial_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_network_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:11.182937Z",
     "start_time": "2021-01-17T09:44:31.797522Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample_' + path_identifier_polynomial_data + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if lambda_nets_total < polynomial_data_size:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=lambda_nets_total, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, lambda_nets_total)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, lambda_nets_total)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:12.626401Z",
     "start_time": "2021-01-17T09:46:12.608200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.183</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.608</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.663</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.389</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.729</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.120</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.249</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.323</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a     b     c     d     e     f     g     h     i     j     k     l  \\\n",
       "0 0.375 0.951 0.732 0.599 0.156 0.156 0.058 0.866 0.601 0.708 0.021 0.970   \n",
       "1 0.183 0.304 0.525 0.432 0.291 0.612 0.139 0.292 0.366 0.456 0.785 0.200   \n",
       "2 0.608 0.171 0.065 0.949 0.966 0.808 0.305 0.098 0.684 0.440 0.122 0.495   \n",
       "3 0.663 0.312 0.520 0.547 0.185 0.970 0.775 0.939 0.895 0.598 0.922 0.088   \n",
       "4 0.389 0.271 0.829 0.357 0.281 0.543 0.141 0.802 0.075 0.987 0.772 0.199   \n",
       "5 0.729 0.771 0.074 0.358 0.116 0.863 0.623 0.331 0.064 0.311 0.325 0.730   \n",
       "6 0.120 0.713 0.761 0.561 0.771 0.494 0.523 0.428 0.025 0.108 0.031 0.636   \n",
       "7 0.249 0.410 0.756 0.229 0.077 0.290 0.161 0.930 0.808 0.633 0.871 0.804   \n",
       "8 0.807 0.896 0.318 0.110 0.228 0.427 0.818 0.861 0.007 0.511 0.417 0.222   \n",
       "9 0.323 0.519 0.703 0.364 0.972 0.962 0.252 0.497 0.301 0.285 0.037 0.610   \n",
       "\n",
       "      m     n     o  \n",
       "0 0.832 0.212 0.182  \n",
       "1 0.514 0.592 0.046  \n",
       "2 0.034 0.909 0.259  \n",
       "3 0.196 0.045 0.325  \n",
       "4 0.006 0.815 0.707  \n",
       "5 0.638 0.887 0.472  \n",
       "6 0.314 0.509 0.908  \n",
       "7 0.187 0.893 0.539  \n",
       "8 0.120 0.338 0.943  \n",
       "9 0.503 0.051 0.279  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:12.636995Z",
     "start_time": "2021-01-17T09:46:12.629349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   result\n",
       "0   1.204\n",
       "1   0.277\n",
       "2   0.354\n",
       "3   0.851\n",
       "4   0.009\n",
       "5   0.954\n",
       "6   0.520\n",
       "7   0.792\n",
       "8   0.489\n",
       "9   0.371"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000000000000   0.000\n",
       "210000000000000   0.000\n",
       "201000000000000   0.000\n",
       "200100000000000   0.000\n",
       "200010000000000   0.000\n",
       "200001000000000   0.000\n",
       "200000100000000   0.000\n",
       "200000010000000   0.732\n",
       "200000001000000   0.000\n",
       "200000000100000   0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000000000000   0.000\n",
       "210000000000000   0.000\n",
       "201000000000000   0.000\n",
       "200100000000000   0.000\n",
       "200010000000000   0.000\n",
       "200001000000000   0.000\n",
       "200000100000000   0.000\n",
       "200000010000000   0.732\n",
       "200000001000000   0.000\n",
       "200000000100000   0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8108149d2c714102ba1512606e7060da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 73.9min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 116.5min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 166.2min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 224.2min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 291.2min\n",
      "[Parallel(n_jobs=20)]: Done 2552 tasks      | elapsed: 366.6min\n",
      "[Parallel(n_jobs=20)]: Done 3160 tasks      | elapsed: 449.4min\n",
      "[Parallel(n_jobs=20)]: Done 3832 tasks      | elapsed: 541.0min\n",
      "[Parallel(n_jobs=20)]: Done 4568 tasks      | elapsed: 640.8min\n",
      "[Parallel(n_jobs=20)]: Done 5000 out of 5000 | elapsed: 698.7min finished\n",
      "[Parallel(n_jobs=20)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 39.0min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 71.0min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 111.9min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 160.1min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 217.4min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 283.6min\n",
      "[Parallel(n_jobs=20)]: Done 2552 tasks      | elapsed: 360.1min\n",
      "[Parallel(n_jobs=20)]: Done 3160 tasks      | elapsed: 445.6min\n",
      "[Parallel(n_jobs=20)]: Done 3832 tasks      | elapsed: 540.3min\n",
      "[Parallel(n_jobs=20)]: Done 4568 tasks      | elapsed: 641.6min\n",
      "[Parallel(n_jobs=20)]: Done 5000 out of 5000 | elapsed: 699.4min finished\n",
      "[Parallel(n_jobs=20)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643b07c70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643a98f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643b07c70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643afe490>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643b07130>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643aa8eb0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643a91f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643a91cd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643afee50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643a91cd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643b07be0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643aa1c70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643afea90>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643aa8580>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643a98b50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643a91f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643afeac0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643b07c70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643af7e80>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff643b07bb0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 40.4min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 74.7min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 116.6min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 167.3min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 227.4min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 294.1min\n",
      "[Parallel(n_jobs=20)]: Done 2552 tasks      | elapsed: 370.0min\n",
      "[Parallel(n_jobs=20)]: Done 3160 tasks      | elapsed: 452.7min\n",
      "[Parallel(n_jobs=20)]: Done 3832 tasks      | elapsed: 545.4min\n",
      "[Parallel(n_jobs=20)]: Done 4568 tasks      | elapsed: 644.2min\n",
      "[Parallel(n_jobs=20)]: Done 5000 out of 5000 | elapsed: 701.6min finished\n",
      "[Parallel(n_jobs=20)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bb1d00>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff645c95d60>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff645c95d30>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bb1f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff645c95df0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bbcc70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bc3a60>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bc3ee0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bccc70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff645c95fd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff645c9cf70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff645c9ce20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff645ca6490>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff645c95400>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bcc3d0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bccc10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff645c95df0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bbcb50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bccca0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff640bc3ee0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 41.1min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 79.5min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 125.0min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 178.5min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 242.9min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 313.1min\n",
      "[Parallel(n_jobs=20)]: Done 2552 tasks      | elapsed: 391.0min\n",
      "[Parallel(n_jobs=20)]: Done 3160 tasks      | elapsed: 477.1min\n",
      "[Parallel(n_jobs=20)]: Done 3832 tasks      | elapsed: 572.0min\n",
      "[Parallel(n_jobs=20)]: Done 4568 tasks      | elapsed: 673.4min\n",
      "[Parallel(n_jobs=20)]: Done 5000 out of 5000 | elapsed: 733.4min finished\n",
      "[Parallel(n_jobs=20)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d420f70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d485e20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d417f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d485910>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d417dc0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d485f40>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d417f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d417dc0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d485fa0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d420f70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d40eaf0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d47dd60>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d475e50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d47dd00>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d429ca0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d429ca0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d485df0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d47de50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d40ef10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63d47dd60>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 44.4min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 82.7min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 126.4min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 175.8min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 235.1min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 304.0min\n",
      "[Parallel(n_jobs=20)]: Done 2552 tasks      | elapsed: 378.8min\n",
      "[Parallel(n_jobs=20)]: Done 3160 tasks      | elapsed: 465.8min\n",
      "[Parallel(n_jobs=20)]: Done 3832 tasks      | elapsed: 558.0min\n",
      "[Parallel(n_jobs=20)]: Done 4568 tasks      | elapsed: 659.6min\n",
      "[Parallel(n_jobs=20)]: Done 5000 out of 5000 | elapsed: 719.2min finished\n",
      "[Parallel(n_jobs=20)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a538d90>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4e4f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4d4040>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4ccc10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a543ca0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4db130>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4ccfd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a538f70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4cce80>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4edf10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4d4af0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4dbc70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4d4520>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4d4e50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4dbe80>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4d4e50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4d4af0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a543b20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a543ca0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63a4cc610>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 78.6min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 122.8min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 175.2min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 238.9min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 309.9min\n",
      "[Parallel(n_jobs=20)]: Done 2552 tasks      | elapsed: 389.2min\n",
      "[Parallel(n_jobs=20)]: Done 3160 tasks      | elapsed: 476.3min\n",
      "[Parallel(n_jobs=20)]: Done 3832 tasks      | elapsed: 572.8min\n",
      "[Parallel(n_jobs=20)]: Done 4568 tasks      | elapsed: 676.8min\n",
      "[Parallel(n_jobs=20)]: Done 5000 out of 5000 | elapsed: 735.7min finished\n",
      "[Parallel(n_jobs=20)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6375f6bb0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff637598a60>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6375ffcd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff637598ee0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6375a9df0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6375ffcd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff637598ee0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63758ec70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6375a0d90>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff637606f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6375a0160>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6375a0c70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff637606b80>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff637606b50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63758ee20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff637598eb0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6375a0190>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6375fff10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff637598580>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6375f6be0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 43.0min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 78.4min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 124.1min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 178.1min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 241.9min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 314.2min\n",
      "[Parallel(n_jobs=20)]: Done 2552 tasks      | elapsed: 394.8min\n",
      "[Parallel(n_jobs=20)]: Done 3160 tasks      | elapsed: 482.4min\n",
      "[Parallel(n_jobs=20)]: Done 3832 tasks      | elapsed: 575.3min\n",
      "[Parallel(n_jobs=20)]: Done 4568 tasks      | elapsed: 678.5min\n",
      "[Parallel(n_jobs=20)]: Done 5000 out of 5000 | elapsed: 739.0min finished\n",
      "[Parallel(n_jobs=20)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63365bdf0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff633651e50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6336c9b20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6336c1d30>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6336c1df0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6336c1fd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6336c1df0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6336c9f70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff633651e50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63365bdf0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6336c9e20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6336c1d60>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63365be20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff633663af0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63365be20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff633651490>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6336b93d0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63366bf40>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63366b220>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff6336b0a60>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 42.8min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 79.8min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 128.4min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 184.2min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 249.4min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 321.9min\n",
      "[Parallel(n_jobs=20)]: Done 2552 tasks      | elapsed: 401.4min\n",
      "[Parallel(n_jobs=20)]: Done 3160 tasks      | elapsed: 488.4min\n",
      "[Parallel(n_jobs=20)]: Done 3832 tasks      | elapsed: 585.4min\n",
      "[Parallel(n_jobs=20)]: Done 4568 tasks      | elapsed: 689.1min\n",
      "[Parallel(n_jobs=20)]: Done 5000 out of 5000 | elapsed: 749.6min finished\n",
      "[Parallel(n_jobs=20)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630717b20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630717b20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630717340>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630774df0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630727f70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630717ca0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63070ef70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630774910>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630717ca0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630717ca0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63071ffd0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630785dc0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63071f250>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63070ef70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63070ef70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630785f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63070e790>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff63077daf0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630785f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff630717ca0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 43.7min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 80.9min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 125.2min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 178.5min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 238.1min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 305.4min\n",
      "[Parallel(n_jobs=20)]: Done 2552 tasks      | elapsed: 386.4min\n",
      "[Parallel(n_jobs=20)]: Done 3160 tasks      | elapsed: 470.6min\n",
      "[Parallel(n_jobs=20)]: Done 3832 tasks      | elapsed: 565.2min\n",
      "[Parallel(n_jobs=20)]: Done 4568 tasks      | elapsed: 668.3min\n",
      "[Parallel(n_jobs=20)]: Done 5000 out of 5000 | elapsed: 727.2min finished\n",
      "[Parallel(n_jobs=20)]: Using backend MultiprocessingBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d840e50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7dbb50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d848c70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7dbb50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d840a90>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7d2f70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7e9ee0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7e2d30>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d840a90>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7db880>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d840b20>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d839ee0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7d2f10>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d830f40>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7d2f70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d848e80>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7d2f70>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7dbb50>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7db2e0>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2_keras_loss at 0x7ff66459a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unknown node type <gast.gast.Assign object at 0x7ff62d7dbb80>\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed: 15.0min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 43.9min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 80.3min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 126.0min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 178.3min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 239.3min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 310.2min\n",
      "[Parallel(n_jobs=20)]: Done 2552 tasks      | elapsed: 391.3min\n",
      "[Parallel(n_jobs=20)]: Done 3160 tasks      | elapsed: 480.1min\n",
      "[Parallel(n_jobs=20)]: Done 3832 tasks      | elapsed: 577.3min\n",
      "[Parallel(n_jobs=20)]: Done 4568 tasks      | elapsed: 682.4min\n",
      "[Parallel(n_jobs=20)]: Done 5000 out of 5000 | elapsed: 740.5min finished\n"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "clf_list = []\n",
    "chunksize = 5000 if lambda_nets_total > 50000 else max(lambda_nets_total//10, min(50, lambda_nets_total))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "max_seed = 2147483646\n",
    "seed_list = random.sample(range(0, max_seed), number_different_lambda_trainings)\n",
    "chunk_multiplier = 0\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    clf_sublist = parallel(delayed(train_nn)(chunksize*chunk_multiplier+index, X_data[1].values, y_data[1].values, X_data[0], seed_list, return_history=True, each_epochs_save=each_epochs_save_lambda, printing=True) for index, (X_data, y_data) in enumerate(zip(X_data_list_split, y_data_list_split)))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "    chunk_multiplier +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    0.033, max:    0.409, cur:    0.034)\n",
      "\tvalidation       \t (min:    0.041, max:    0.237, cur:    0.041)\n",
      "mae\n",
      "\ttraining         \t (min:    0.033, max:    0.409, cur:    0.034)\n",
      "\tvalidation       \t (min:    0.041, max:    0.237, cur:    0.041)\n",
      "r2_keras_loss\n",
      "\ttraining         \t (min:   -0.973, max:    2.841, cur:   -0.972)\n",
      "\tvalidation       \t (min:   -0.967, max:    0.082, cur:   -0.965)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    0.047, max:    0.544, cur:    0.047)\n",
      "\tvalidation       \t (min:    0.054, max:    0.312, cur:    0.056)\n"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "rand_index = np.random.randint(lambda_nets_total)\n",
    "\n",
    "random_network = train_nn(rand_index, X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], seed_list, callbacks=[PlotLossesKerasTF()], return_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Plot Lambda-Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b050c65123e454884d1668413f031e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8847edfaaa614cbcaf3838aa9debf3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "index_list = []\n",
    "\n",
    "\n",
    "max_training_epochs = 0\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    history = entry[3]\n",
    "    \n",
    "    current_training_epochs = len(history[list(history.keys())[0]])\n",
    "    max_training_epochs = max(max_training_epochs, current_training_epochs)\n",
    "\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    history = entry[3]\n",
    "    index = entry[0][0]\n",
    "    \n",
    "    current_training_epochs = len(history[list(history.keys())[0]])\n",
    "    \n",
    "    loss_list = np.full(max_training_epochs, np.nan)\n",
    "    metric_list = np.full(max_training_epochs, np.nan)\n",
    "    val_loss_list = np.full(max_training_epochs, np.nan)\n",
    "    val_metric_list = np.full(max_training_epochs, np.nan) \n",
    "\n",
    "    for i in range(current_training_epochs):  \n",
    "        loss_list[i] = history[list(history.keys())[0]][i]\n",
    "        metric_list[i] = history[list(history.keys())[1]][i]\n",
    "        val_loss_list[i] = history[list(history.keys())[len(history.keys())//2]][i]\n",
    "        val_metric_list[i] = history[list(history.keys())[len(history.keys())//2+1]][i]\n",
    "    \n",
    "    index_list.append([index])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=np.hstack([index_list, loss_list_total]), columns=list(flatten(['index', [list(history.keys())[0] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]])))\n",
    "#loss_df['index'] = loss_df['index'].astype(int)\n",
    "metric_df = pd.DataFrame(data=np.hstack([index_list, metric_list_total]), columns=list(flatten(['index', [list(history.keys())[1] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]]))) \n",
    "#metric_df['index'] = metric_df['index'].astype(int)\n",
    "val_loss_df = pd.DataFrame(data=np.hstack([index_list, val_loss_list_total]), columns=list(flatten(['index', [list(history.keys())[len(history.keys())//2] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]])))\n",
    "#val_loss_df['index'] = val_loss_df['index'].astype(int)\n",
    "val_metric_df = pd.DataFrame(data=np.hstack([index_list, val_metric_list_total]), columns=list(flatten(['index', [list(history.keys())[len(history.keys())//2+1] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]]))) \n",
    "#val_metric_df['index'] = val_metric_df['index'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.875Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[0] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "path_metric = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[1] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[len(history.keys())//2] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[len(history.keys())//2+1] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>loss_epoch_11</th>\n",
       "      <th>loss_epoch_12</th>\n",
       "      <th>loss_epoch_13</th>\n",
       "      <th>loss_epoch_14</th>\n",
       "      <th>loss_epoch_15</th>\n",
       "      <th>loss_epoch_16</th>\n",
       "      <th>loss_epoch_17</th>\n",
       "      <th>loss_epoch_18</th>\n",
       "      <th>loss_epoch_19</th>\n",
       "      <th>loss_epoch_20</th>\n",
       "      <th>loss_epoch_21</th>\n",
       "      <th>loss_epoch_22</th>\n",
       "      <th>loss_epoch_23</th>\n",
       "      <th>loss_epoch_24</th>\n",
       "      <th>loss_epoch_25</th>\n",
       "      <th>loss_epoch_26</th>\n",
       "      <th>loss_epoch_27</th>\n",
       "      <th>loss_epoch_28</th>\n",
       "      <th>loss_epoch_29</th>\n",
       "      <th>loss_epoch_30</th>\n",
       "      <th>loss_epoch_31</th>\n",
       "      <th>loss_epoch_32</th>\n",
       "      <th>loss_epoch_33</th>\n",
       "      <th>loss_epoch_34</th>\n",
       "      <th>loss_epoch_35</th>\n",
       "      <th>loss_epoch_36</th>\n",
       "      <th>loss_epoch_37</th>\n",
       "      <th>loss_epoch_38</th>\n",
       "      <th>loss_epoch_39</th>\n",
       "      <th>loss_epoch_40</th>\n",
       "      <th>loss_epoch_41</th>\n",
       "      <th>loss_epoch_42</th>\n",
       "      <th>loss_epoch_43</th>\n",
       "      <th>loss_epoch_44</th>\n",
       "      <th>loss_epoch_45</th>\n",
       "      <th>loss_epoch_46</th>\n",
       "      <th>loss_epoch_47</th>\n",
       "      <th>loss_epoch_48</th>\n",
       "      <th>loss_epoch_49</th>\n",
       "      <th>loss_epoch_50</th>\n",
       "      <th>loss_epoch_51</th>\n",
       "      <th>loss_epoch_52</th>\n",
       "      <th>loss_epoch_53</th>\n",
       "      <th>loss_epoch_54</th>\n",
       "      <th>loss_epoch_55</th>\n",
       "      <th>loss_epoch_56</th>\n",
       "      <th>loss_epoch_57</th>\n",
       "      <th>loss_epoch_58</th>\n",
       "      <th>loss_epoch_59</th>\n",
       "      <th>loss_epoch_60</th>\n",
       "      <th>loss_epoch_61</th>\n",
       "      <th>loss_epoch_62</th>\n",
       "      <th>loss_epoch_63</th>\n",
       "      <th>loss_epoch_64</th>\n",
       "      <th>loss_epoch_65</th>\n",
       "      <th>loss_epoch_66</th>\n",
       "      <th>loss_epoch_67</th>\n",
       "      <th>loss_epoch_68</th>\n",
       "      <th>loss_epoch_69</th>\n",
       "      <th>loss_epoch_70</th>\n",
       "      <th>loss_epoch_71</th>\n",
       "      <th>loss_epoch_72</th>\n",
       "      <th>loss_epoch_73</th>\n",
       "      <th>loss_epoch_74</th>\n",
       "      <th>loss_epoch_75</th>\n",
       "      <th>loss_epoch_76</th>\n",
       "      <th>loss_epoch_77</th>\n",
       "      <th>loss_epoch_78</th>\n",
       "      <th>loss_epoch_79</th>\n",
       "      <th>loss_epoch_80</th>\n",
       "      <th>loss_epoch_81</th>\n",
       "      <th>loss_epoch_82</th>\n",
       "      <th>loss_epoch_83</th>\n",
       "      <th>loss_epoch_84</th>\n",
       "      <th>loss_epoch_85</th>\n",
       "      <th>loss_epoch_86</th>\n",
       "      <th>loss_epoch_87</th>\n",
       "      <th>loss_epoch_88</th>\n",
       "      <th>loss_epoch_89</th>\n",
       "      <th>loss_epoch_90</th>\n",
       "      <th>loss_epoch_91</th>\n",
       "      <th>loss_epoch_92</th>\n",
       "      <th>loss_epoch_93</th>\n",
       "      <th>loss_epoch_94</th>\n",
       "      <th>loss_epoch_95</th>\n",
       "      <th>loss_epoch_96</th>\n",
       "      <th>loss_epoch_97</th>\n",
       "      <th>loss_epoch_98</th>\n",
       "      <th>loss_epoch_99</th>\n",
       "      <th>loss_epoch_100</th>\n",
       "      <th>loss_epoch_101</th>\n",
       "      <th>loss_epoch_102</th>\n",
       "      <th>loss_epoch_103</th>\n",
       "      <th>loss_epoch_104</th>\n",
       "      <th>loss_epoch_105</th>\n",
       "      <th>loss_epoch_106</th>\n",
       "      <th>loss_epoch_107</th>\n",
       "      <th>loss_epoch_108</th>\n",
       "      <th>loss_epoch_109</th>\n",
       "      <th>loss_epoch_110</th>\n",
       "      <th>loss_epoch_111</th>\n",
       "      <th>loss_epoch_112</th>\n",
       "      <th>loss_epoch_113</th>\n",
       "      <th>loss_epoch_114</th>\n",
       "      <th>loss_epoch_115</th>\n",
       "      <th>loss_epoch_116</th>\n",
       "      <th>loss_epoch_117</th>\n",
       "      <th>loss_epoch_118</th>\n",
       "      <th>loss_epoch_119</th>\n",
       "      <th>loss_epoch_120</th>\n",
       "      <th>loss_epoch_121</th>\n",
       "      <th>loss_epoch_122</th>\n",
       "      <th>loss_epoch_123</th>\n",
       "      <th>loss_epoch_124</th>\n",
       "      <th>loss_epoch_125</th>\n",
       "      <th>loss_epoch_126</th>\n",
       "      <th>loss_epoch_127</th>\n",
       "      <th>loss_epoch_128</th>\n",
       "      <th>loss_epoch_129</th>\n",
       "      <th>loss_epoch_130</th>\n",
       "      <th>loss_epoch_131</th>\n",
       "      <th>loss_epoch_132</th>\n",
       "      <th>loss_epoch_133</th>\n",
       "      <th>loss_epoch_134</th>\n",
       "      <th>loss_epoch_135</th>\n",
       "      <th>loss_epoch_136</th>\n",
       "      <th>loss_epoch_137</th>\n",
       "      <th>loss_epoch_138</th>\n",
       "      <th>loss_epoch_139</th>\n",
       "      <th>loss_epoch_140</th>\n",
       "      <th>loss_epoch_141</th>\n",
       "      <th>loss_epoch_142</th>\n",
       "      <th>loss_epoch_143</th>\n",
       "      <th>loss_epoch_144</th>\n",
       "      <th>loss_epoch_145</th>\n",
       "      <th>loss_epoch_146</th>\n",
       "      <th>loss_epoch_147</th>\n",
       "      <th>loss_epoch_148</th>\n",
       "      <th>loss_epoch_149</th>\n",
       "      <th>loss_epoch_150</th>\n",
       "      <th>loss_epoch_151</th>\n",
       "      <th>loss_epoch_152</th>\n",
       "      <th>loss_epoch_153</th>\n",
       "      <th>loss_epoch_154</th>\n",
       "      <th>loss_epoch_155</th>\n",
       "      <th>loss_epoch_156</th>\n",
       "      <th>loss_epoch_157</th>\n",
       "      <th>loss_epoch_158</th>\n",
       "      <th>loss_epoch_159</th>\n",
       "      <th>loss_epoch_160</th>\n",
       "      <th>loss_epoch_161</th>\n",
       "      <th>loss_epoch_162</th>\n",
       "      <th>loss_epoch_163</th>\n",
       "      <th>loss_epoch_164</th>\n",
       "      <th>loss_epoch_165</th>\n",
       "      <th>loss_epoch_166</th>\n",
       "      <th>loss_epoch_167</th>\n",
       "      <th>loss_epoch_168</th>\n",
       "      <th>loss_epoch_169</th>\n",
       "      <th>loss_epoch_170</th>\n",
       "      <th>loss_epoch_171</th>\n",
       "      <th>loss_epoch_172</th>\n",
       "      <th>loss_epoch_173</th>\n",
       "      <th>loss_epoch_174</th>\n",
       "      <th>loss_epoch_175</th>\n",
       "      <th>loss_epoch_176</th>\n",
       "      <th>loss_epoch_177</th>\n",
       "      <th>loss_epoch_178</th>\n",
       "      <th>loss_epoch_179</th>\n",
       "      <th>loss_epoch_180</th>\n",
       "      <th>loss_epoch_181</th>\n",
       "      <th>loss_epoch_182</th>\n",
       "      <th>loss_epoch_183</th>\n",
       "      <th>loss_epoch_184</th>\n",
       "      <th>loss_epoch_185</th>\n",
       "      <th>loss_epoch_186</th>\n",
       "      <th>loss_epoch_187</th>\n",
       "      <th>loss_epoch_188</th>\n",
       "      <th>loss_epoch_189</th>\n",
       "      <th>loss_epoch_190</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "      <th>loss_epoch_201</th>\n",
       "      <th>loss_epoch_202</th>\n",
       "      <th>loss_epoch_203</th>\n",
       "      <th>loss_epoch_204</th>\n",
       "      <th>loss_epoch_205</th>\n",
       "      <th>loss_epoch_206</th>\n",
       "      <th>loss_epoch_207</th>\n",
       "      <th>loss_epoch_208</th>\n",
       "      <th>loss_epoch_209</th>\n",
       "      <th>loss_epoch_210</th>\n",
       "      <th>loss_epoch_211</th>\n",
       "      <th>loss_epoch_212</th>\n",
       "      <th>loss_epoch_213</th>\n",
       "      <th>loss_epoch_214</th>\n",
       "      <th>loss_epoch_215</th>\n",
       "      <th>loss_epoch_216</th>\n",
       "      <th>loss_epoch_217</th>\n",
       "      <th>loss_epoch_218</th>\n",
       "      <th>loss_epoch_219</th>\n",
       "      <th>loss_epoch_220</th>\n",
       "      <th>loss_epoch_221</th>\n",
       "      <th>loss_epoch_222</th>\n",
       "      <th>loss_epoch_223</th>\n",
       "      <th>loss_epoch_224</th>\n",
       "      <th>loss_epoch_225</th>\n",
       "      <th>loss_epoch_226</th>\n",
       "      <th>loss_epoch_227</th>\n",
       "      <th>loss_epoch_228</th>\n",
       "      <th>loss_epoch_229</th>\n",
       "      <th>loss_epoch_230</th>\n",
       "      <th>loss_epoch_231</th>\n",
       "      <th>loss_epoch_232</th>\n",
       "      <th>loss_epoch_233</th>\n",
       "      <th>loss_epoch_234</th>\n",
       "      <th>loss_epoch_235</th>\n",
       "      <th>loss_epoch_236</th>\n",
       "      <th>loss_epoch_237</th>\n",
       "      <th>loss_epoch_238</th>\n",
       "      <th>loss_epoch_239</th>\n",
       "      <th>loss_epoch_240</th>\n",
       "      <th>loss_epoch_241</th>\n",
       "      <th>loss_epoch_242</th>\n",
       "      <th>loss_epoch_243</th>\n",
       "      <th>loss_epoch_244</th>\n",
       "      <th>loss_epoch_245</th>\n",
       "      <th>loss_epoch_246</th>\n",
       "      <th>loss_epoch_247</th>\n",
       "      <th>loss_epoch_248</th>\n",
       "      <th>loss_epoch_249</th>\n",
       "      <th>loss_epoch_250</th>\n",
       "      <th>loss_epoch_251</th>\n",
       "      <th>loss_epoch_252</th>\n",
       "      <th>loss_epoch_253</th>\n",
       "      <th>loss_epoch_254</th>\n",
       "      <th>loss_epoch_255</th>\n",
       "      <th>loss_epoch_256</th>\n",
       "      <th>loss_epoch_257</th>\n",
       "      <th>loss_epoch_258</th>\n",
       "      <th>loss_epoch_259</th>\n",
       "      <th>loss_epoch_260</th>\n",
       "      <th>loss_epoch_261</th>\n",
       "      <th>loss_epoch_262</th>\n",
       "      <th>loss_epoch_263</th>\n",
       "      <th>loss_epoch_264</th>\n",
       "      <th>loss_epoch_265</th>\n",
       "      <th>loss_epoch_266</th>\n",
       "      <th>loss_epoch_267</th>\n",
       "      <th>loss_epoch_268</th>\n",
       "      <th>loss_epoch_269</th>\n",
       "      <th>loss_epoch_270</th>\n",
       "      <th>loss_epoch_271</th>\n",
       "      <th>loss_epoch_272</th>\n",
       "      <th>loss_epoch_273</th>\n",
       "      <th>loss_epoch_274</th>\n",
       "      <th>loss_epoch_275</th>\n",
       "      <th>loss_epoch_276</th>\n",
       "      <th>loss_epoch_277</th>\n",
       "      <th>loss_epoch_278</th>\n",
       "      <th>loss_epoch_279</th>\n",
       "      <th>loss_epoch_280</th>\n",
       "      <th>loss_epoch_281</th>\n",
       "      <th>loss_epoch_282</th>\n",
       "      <th>loss_epoch_283</th>\n",
       "      <th>loss_epoch_284</th>\n",
       "      <th>loss_epoch_285</th>\n",
       "      <th>loss_epoch_286</th>\n",
       "      <th>loss_epoch_287</th>\n",
       "      <th>loss_epoch_288</th>\n",
       "      <th>loss_epoch_289</th>\n",
       "      <th>loss_epoch_290</th>\n",
       "      <th>loss_epoch_291</th>\n",
       "      <th>loss_epoch_292</th>\n",
       "      <th>loss_epoch_293</th>\n",
       "      <th>loss_epoch_294</th>\n",
       "      <th>loss_epoch_295</th>\n",
       "      <th>loss_epoch_296</th>\n",
       "      <th>loss_epoch_297</th>\n",
       "      <th>loss_epoch_298</th>\n",
       "      <th>loss_epoch_299</th>\n",
       "      <th>loss_epoch_300</th>\n",
       "      <th>loss_epoch_301</th>\n",
       "      <th>loss_epoch_302</th>\n",
       "      <th>loss_epoch_303</th>\n",
       "      <th>loss_epoch_304</th>\n",
       "      <th>loss_epoch_305</th>\n",
       "      <th>loss_epoch_306</th>\n",
       "      <th>loss_epoch_307</th>\n",
       "      <th>loss_epoch_308</th>\n",
       "      <th>loss_epoch_309</th>\n",
       "      <th>loss_epoch_310</th>\n",
       "      <th>loss_epoch_311</th>\n",
       "      <th>loss_epoch_312</th>\n",
       "      <th>loss_epoch_313</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49998.000</td>\n",
       "      <td>49997.000</td>\n",
       "      <td>49995.000</td>\n",
       "      <td>49992.000</td>\n",
       "      <td>49988.000</td>\n",
       "      <td>49987.000</td>\n",
       "      <td>49985.000</td>\n",
       "      <td>49977.000</td>\n",
       "      <td>49972.000</td>\n",
       "      <td>49963.000</td>\n",
       "      <td>49954.000</td>\n",
       "      <td>49947.000</td>\n",
       "      <td>49928.000</td>\n",
       "      <td>49915.000</td>\n",
       "      <td>49892.000</td>\n",
       "      <td>49877.000</td>\n",
       "      <td>49863.000</td>\n",
       "      <td>49844.000</td>\n",
       "      <td>49815.000</td>\n",
       "      <td>49786.000</td>\n",
       "      <td>49745.000</td>\n",
       "      <td>49715.000</td>\n",
       "      <td>49684.000</td>\n",
       "      <td>49639.000</td>\n",
       "      <td>49590.000</td>\n",
       "      <td>49533.000</td>\n",
       "      <td>49489.000</td>\n",
       "      <td>49419.000</td>\n",
       "      <td>49345.000</td>\n",
       "      <td>49250.000</td>\n",
       "      <td>49171.000</td>\n",
       "      <td>49086.000</td>\n",
       "      <td>48990.000</td>\n",
       "      <td>48876.000</td>\n",
       "      <td>48734.000</td>\n",
       "      <td>48604.000</td>\n",
       "      <td>48450.000</td>\n",
       "      <td>48292.000</td>\n",
       "      <td>48129.000</td>\n",
       "      <td>47962.000</td>\n",
       "      <td>47784.000</td>\n",
       "      <td>47623.000</td>\n",
       "      <td>47424.000</td>\n",
       "      <td>47195.000</td>\n",
       "      <td>46965.000</td>\n",
       "      <td>46709.000</td>\n",
       "      <td>46440.000</td>\n",
       "      <td>46166.000</td>\n",
       "      <td>45899.000</td>\n",
       "      <td>45611.000</td>\n",
       "      <td>45339.000</td>\n",
       "      <td>45037.000</td>\n",
       "      <td>44714.000</td>\n",
       "      <td>44353.000</td>\n",
       "      <td>44009.000</td>\n",
       "      <td>43657.000</td>\n",
       "      <td>43314.000</td>\n",
       "      <td>42929.000</td>\n",
       "      <td>42508.000</td>\n",
       "      <td>42093.000</td>\n",
       "      <td>41659.000</td>\n",
       "      <td>41237.000</td>\n",
       "      <td>40826.000</td>\n",
       "      <td>40392.000</td>\n",
       "      <td>39922.000</td>\n",
       "      <td>39426.000</td>\n",
       "      <td>38899.000</td>\n",
       "      <td>38437.000</td>\n",
       "      <td>37935.000</td>\n",
       "      <td>37418.000</td>\n",
       "      <td>36939.000</td>\n",
       "      <td>36417.000</td>\n",
       "      <td>35913.000</td>\n",
       "      <td>35372.000</td>\n",
       "      <td>34824.000</td>\n",
       "      <td>34290.000</td>\n",
       "      <td>33739.000</td>\n",
       "      <td>33162.000</td>\n",
       "      <td>32587.000</td>\n",
       "      <td>32037.000</td>\n",
       "      <td>31435.000</td>\n",
       "      <td>30861.000</td>\n",
       "      <td>30298.000</td>\n",
       "      <td>29751.000</td>\n",
       "      <td>29196.000</td>\n",
       "      <td>28593.000</td>\n",
       "      <td>28022.000</td>\n",
       "      <td>27441.000</td>\n",
       "      <td>26875.000</td>\n",
       "      <td>26276.000</td>\n",
       "      <td>25731.000</td>\n",
       "      <td>25168.000</td>\n",
       "      <td>24625.000</td>\n",
       "      <td>24089.000</td>\n",
       "      <td>23560.000</td>\n",
       "      <td>22983.000</td>\n",
       "      <td>22420.000</td>\n",
       "      <td>21915.000</td>\n",
       "      <td>21414.000</td>\n",
       "      <td>20861.000</td>\n",
       "      <td>20337.000</td>\n",
       "      <td>19866.000</td>\n",
       "      <td>19357.000</td>\n",
       "      <td>18812.000</td>\n",
       "      <td>18312.000</td>\n",
       "      <td>17824.000</td>\n",
       "      <td>17372.000</td>\n",
       "      <td>16916.000</td>\n",
       "      <td>16427.000</td>\n",
       "      <td>15973.000</td>\n",
       "      <td>15529.000</td>\n",
       "      <td>15086.000</td>\n",
       "      <td>14656.000</td>\n",
       "      <td>14242.000</td>\n",
       "      <td>13836.000</td>\n",
       "      <td>13441.000</td>\n",
       "      <td>13031.000</td>\n",
       "      <td>12650.000</td>\n",
       "      <td>12264.000</td>\n",
       "      <td>11884.000</td>\n",
       "      <td>11508.000</td>\n",
       "      <td>11163.000</td>\n",
       "      <td>10813.000</td>\n",
       "      <td>10465.000</td>\n",
       "      <td>10126.000</td>\n",
       "      <td>9791.000</td>\n",
       "      <td>9460.000</td>\n",
       "      <td>9137.000</td>\n",
       "      <td>8842.000</td>\n",
       "      <td>8512.000</td>\n",
       "      <td>8217.000</td>\n",
       "      <td>7929.000</td>\n",
       "      <td>7644.000</td>\n",
       "      <td>7362.000</td>\n",
       "      <td>7087.000</td>\n",
       "      <td>6863.000</td>\n",
       "      <td>6614.000</td>\n",
       "      <td>6328.000</td>\n",
       "      <td>6076.000</td>\n",
       "      <td>5826.000</td>\n",
       "      <td>5606.000</td>\n",
       "      <td>5383.000</td>\n",
       "      <td>5167.000</td>\n",
       "      <td>4971.000</td>\n",
       "      <td>4765.000</td>\n",
       "      <td>4597.000</td>\n",
       "      <td>4430.000</td>\n",
       "      <td>4264.000</td>\n",
       "      <td>4085.000</td>\n",
       "      <td>3910.000</td>\n",
       "      <td>3758.000</td>\n",
       "      <td>3607.000</td>\n",
       "      <td>3443.000</td>\n",
       "      <td>3313.000</td>\n",
       "      <td>3155.000</td>\n",
       "      <td>3020.000</td>\n",
       "      <td>2892.000</td>\n",
       "      <td>2746.000</td>\n",
       "      <td>2641.000</td>\n",
       "      <td>2532.000</td>\n",
       "      <td>2412.000</td>\n",
       "      <td>2290.000</td>\n",
       "      <td>2169.000</td>\n",
       "      <td>2054.000</td>\n",
       "      <td>1954.000</td>\n",
       "      <td>1872.000</td>\n",
       "      <td>1772.000</td>\n",
       "      <td>1675.000</td>\n",
       "      <td>1599.000</td>\n",
       "      <td>1532.000</td>\n",
       "      <td>1449.000</td>\n",
       "      <td>1385.000</td>\n",
       "      <td>1327.000</td>\n",
       "      <td>1262.000</td>\n",
       "      <td>1198.000</td>\n",
       "      <td>1123.000</td>\n",
       "      <td>1062.000</td>\n",
       "      <td>1009.000</td>\n",
       "      <td>955.000</td>\n",
       "      <td>913.000</td>\n",
       "      <td>871.000</td>\n",
       "      <td>818.000</td>\n",
       "      <td>762.000</td>\n",
       "      <td>729.000</td>\n",
       "      <td>687.000</td>\n",
       "      <td>653.000</td>\n",
       "      <td>621.000</td>\n",
       "      <td>588.000</td>\n",
       "      <td>556.000</td>\n",
       "      <td>526.000</td>\n",
       "      <td>501.000</td>\n",
       "      <td>476.000</td>\n",
       "      <td>448.000</td>\n",
       "      <td>424.000</td>\n",
       "      <td>392.000</td>\n",
       "      <td>374.000</td>\n",
       "      <td>357.000</td>\n",
       "      <td>333.000</td>\n",
       "      <td>307.000</td>\n",
       "      <td>290.000</td>\n",
       "      <td>271.000</td>\n",
       "      <td>252.000</td>\n",
       "      <td>234.000</td>\n",
       "      <td>220.000</td>\n",
       "      <td>208.000</td>\n",
       "      <td>197.000</td>\n",
       "      <td>184.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>169.000</td>\n",
       "      <td>158.000</td>\n",
       "      <td>146.000</td>\n",
       "      <td>139.000</td>\n",
       "      <td>129.000</td>\n",
       "      <td>122.000</td>\n",
       "      <td>113.000</td>\n",
       "      <td>107.000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>78.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>54.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>45.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>22.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24999.500</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14433.901</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12499.750</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24999.500</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37499.250</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49999.000</td>\n",
       "      <td>1.624</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  \\\n",
       "count 50000.000     50000.000     50000.000     50000.000     50000.000   \n",
       "mean  24999.500         0.356         0.214         0.161         0.143   \n",
       "std   14433.901         0.109         0.054         0.036         0.026   \n",
       "min       0.000         0.132         0.080         0.070         0.063   \n",
       "25%   12499.750         0.280         0.175         0.137         0.126   \n",
       "50%   24999.500         0.334         0.205         0.155         0.140   \n",
       "75%   37499.250         0.408         0.244         0.178         0.157   \n",
       "max   49999.000         1.624         0.591         0.496         0.378   \n",
       "\n",
       "       loss_epoch_5  loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  \\\n",
       "count     50000.000     50000.000     50000.000     50000.000     50000.000   \n",
       "mean          0.135         0.130         0.124         0.119         0.114   \n",
       "std           0.023         0.022         0.021         0.020         0.020   \n",
       "min           0.059         0.056         0.054         0.050         0.047   \n",
       "25%           0.120         0.115         0.110         0.105         0.100   \n",
       "50%           0.134         0.129         0.124         0.118         0.113   \n",
       "75%           0.149         0.143         0.138         0.132         0.127   \n",
       "max           0.334         0.310         0.294         0.278         0.264   \n",
       "\n",
       "       loss_epoch_10  loss_epoch_11  loss_epoch_12  loss_epoch_13  \\\n",
       "count      50000.000      50000.000      50000.000      50000.000   \n",
       "mean           0.109          0.104          0.099          0.095   \n",
       "std            0.020          0.019          0.019          0.018   \n",
       "min            0.044          0.042          0.040          0.038   \n",
       "25%            0.095          0.091          0.086          0.082   \n",
       "50%            0.108          0.103          0.098          0.094   \n",
       "75%            0.122          0.116          0.111          0.106   \n",
       "max            0.251          0.238          0.227          0.218   \n",
       "\n",
       "       loss_epoch_14  loss_epoch_15  loss_epoch_16  loss_epoch_17  \\\n",
       "count      50000.000      50000.000      50000.000      50000.000   \n",
       "mean           0.091          0.087          0.083          0.080   \n",
       "std            0.018          0.017          0.017          0.016   \n",
       "min            0.035          0.035          0.033          0.033   \n",
       "25%            0.078          0.075          0.072          0.069   \n",
       "50%            0.089          0.085          0.082          0.079   \n",
       "75%            0.102          0.097          0.093          0.090   \n",
       "max            0.212          0.203          0.197          0.190   \n",
       "\n",
       "       loss_epoch_18  loss_epoch_19  loss_epoch_20  loss_epoch_21  \\\n",
       "count      50000.000      50000.000      50000.000      50000.000   \n",
       "mean           0.077          0.075          0.073          0.071   \n",
       "std            0.015          0.015          0.014          0.014   \n",
       "min            0.032          0.032          0.031          0.030   \n",
       "25%            0.067          0.065          0.063          0.061   \n",
       "50%            0.076          0.073          0.071          0.069   \n",
       "75%            0.086          0.084          0.081          0.079   \n",
       "max            0.184          0.179          0.176          0.171   \n",
       "\n",
       "       loss_epoch_22  loss_epoch_23  loss_epoch_24  loss_epoch_25  \\\n",
       "count      50000.000      50000.000      50000.000      50000.000   \n",
       "mean           0.069          0.067          0.066          0.065   \n",
       "std            0.013          0.013          0.012          0.012   \n",
       "min            0.029          0.029          0.028          0.029   \n",
       "25%            0.060          0.059          0.058          0.057   \n",
       "50%            0.068          0.066          0.065          0.064   \n",
       "75%            0.077          0.075          0.073          0.072   \n",
       "max            0.164          0.163          0.156          0.156   \n",
       "\n",
       "       loss_epoch_26  loss_epoch_27  loss_epoch_28  loss_epoch_29  \\\n",
       "count      50000.000      50000.000      50000.000      50000.000   \n",
       "mean           0.063          0.062          0.061          0.060   \n",
       "std            0.011          0.011          0.011          0.010   \n",
       "min            0.028          0.027          0.027          0.026   \n",
       "25%            0.056          0.055          0.054          0.053   \n",
       "50%            0.063          0.062          0.061          0.060   \n",
       "75%            0.070          0.069          0.068          0.067   \n",
       "max            0.151          0.149          0.146          0.147   \n",
       "\n",
       "       loss_epoch_30  loss_epoch_31  loss_epoch_32  loss_epoch_33  \\\n",
       "count      50000.000      50000.000      50000.000      50000.000   \n",
       "mean           0.060          0.059          0.058          0.057   \n",
       "std            0.010          0.010          0.010          0.010   \n",
       "min            0.027          0.027          0.027          0.026   \n",
       "25%            0.053          0.052          0.051          0.051   \n",
       "50%            0.059          0.058          0.057          0.057   \n",
       "75%            0.066          0.065          0.064          0.063   \n",
       "max            0.143          0.140          0.136          0.134   \n",
       "\n",
       "       loss_epoch_34  loss_epoch_35  loss_epoch_36  loss_epoch_37  \\\n",
       "count      50000.000      50000.000      50000.000      50000.000   \n",
       "mean           0.056          0.056          0.055          0.055   \n",
       "std            0.009          0.009          0.009          0.009   \n",
       "min            0.026          0.026          0.025          0.025   \n",
       "25%            0.050          0.049          0.049          0.048   \n",
       "50%            0.056          0.055          0.055          0.054   \n",
       "75%            0.062          0.062          0.061          0.060   \n",
       "max            0.134          0.130          0.126          0.128   \n",
       "\n",
       "       loss_epoch_38  loss_epoch_39  loss_epoch_40  loss_epoch_41  \\\n",
       "count      50000.000      50000.000      50000.000      49999.000   \n",
       "mean           0.054          0.053          0.053          0.052   \n",
       "std            0.009          0.009          0.009          0.008   \n",
       "min            0.024          0.025          0.024          0.024   \n",
       "25%            0.048          0.047          0.047          0.047   \n",
       "50%            0.054          0.053          0.053          0.052   \n",
       "75%            0.060          0.059          0.058          0.058   \n",
       "max            0.125          0.123          0.126          0.121   \n",
       "\n",
       "       loss_epoch_42  loss_epoch_43  loss_epoch_44  loss_epoch_45  \\\n",
       "count      49999.000      49999.000      49998.000      49997.000   \n",
       "mean           0.052          0.051          0.051          0.051   \n",
       "std            0.008          0.008          0.008          0.008   \n",
       "min            0.023          0.023          0.024          0.024   \n",
       "25%            0.046          0.046          0.045          0.045   \n",
       "50%            0.052          0.051          0.051          0.050   \n",
       "75%            0.057          0.057          0.056          0.056   \n",
       "max            0.118          0.115          0.115          0.116   \n",
       "\n",
       "       loss_epoch_46  loss_epoch_47  loss_epoch_48  loss_epoch_49  \\\n",
       "count      49995.000      49992.000      49988.000      49987.000   \n",
       "mean           0.050          0.050          0.049          0.049   \n",
       "std            0.008          0.008          0.008          0.008   \n",
       "min            0.023          0.023          0.023          0.023   \n",
       "25%            0.045          0.044          0.044          0.044   \n",
       "50%            0.050          0.049          0.049          0.049   \n",
       "75%            0.055          0.055          0.054          0.054   \n",
       "max            0.112          0.112          0.108          0.109   \n",
       "\n",
       "       loss_epoch_50  loss_epoch_51  loss_epoch_52  loss_epoch_53  \\\n",
       "count      49985.000      49977.000      49972.000      49963.000   \n",
       "mean           0.049          0.048          0.048          0.047   \n",
       "std            0.008          0.008          0.007          0.007   \n",
       "min            0.022          0.023          0.022          0.022   \n",
       "25%            0.043          0.043          0.043          0.042   \n",
       "50%            0.048          0.048          0.047          0.047   \n",
       "75%            0.053          0.053          0.053          0.052   \n",
       "max            0.106          0.104          0.104          0.104   \n",
       "\n",
       "       loss_epoch_54  loss_epoch_55  loss_epoch_56  loss_epoch_57  \\\n",
       "count      49954.000      49947.000      49928.000      49915.000   \n",
       "mean           0.047          0.047          0.046          0.046   \n",
       "std            0.007          0.007          0.007          0.007   \n",
       "min            0.022          0.022          0.021          0.021   \n",
       "25%            0.042          0.042          0.041          0.041   \n",
       "50%            0.047          0.046          0.046          0.046   \n",
       "75%            0.052          0.051          0.051          0.051   \n",
       "max            0.104          0.101          0.100          0.101   \n",
       "\n",
       "       loss_epoch_58  loss_epoch_59  loss_epoch_60  loss_epoch_61  \\\n",
       "count      49892.000      49877.000      49863.000      49844.000   \n",
       "mean           0.046          0.046          0.045          0.045   \n",
       "std            0.007          0.007          0.007          0.007   \n",
       "min            0.021          0.021          0.021          0.020   \n",
       "25%            0.041          0.041          0.040          0.040   \n",
       "50%            0.046          0.045          0.045          0.045   \n",
       "75%            0.050          0.050          0.050          0.049   \n",
       "max            0.098          0.098          0.096          0.095   \n",
       "\n",
       "       loss_epoch_62  loss_epoch_63  loss_epoch_64  loss_epoch_65  \\\n",
       "count      49815.000      49786.000      49745.000      49715.000   \n",
       "mean           0.045          0.044          0.044          0.044   \n",
       "std            0.007          0.007          0.007          0.007   \n",
       "min            0.021          0.020          0.021          0.020   \n",
       "25%            0.040          0.040          0.039          0.039   \n",
       "50%            0.044          0.044          0.044          0.044   \n",
       "75%            0.049          0.049          0.048          0.048   \n",
       "max            0.096          0.093          0.093          0.090   \n",
       "\n",
       "       loss_epoch_66  loss_epoch_67  loss_epoch_68  loss_epoch_69  \\\n",
       "count      49684.000      49639.000      49590.000      49533.000   \n",
       "mean           0.044          0.043          0.043          0.043   \n",
       "std            0.007          0.007          0.007          0.007   \n",
       "min            0.020          0.020          0.020          0.021   \n",
       "25%            0.039          0.039          0.039          0.038   \n",
       "50%            0.043          0.043          0.043          0.043   \n",
       "75%            0.048          0.048          0.047          0.047   \n",
       "max            0.092          0.091          0.095          0.091   \n",
       "\n",
       "       loss_epoch_70  loss_epoch_71  loss_epoch_72  loss_epoch_73  \\\n",
       "count      49489.000      49419.000      49345.000      49250.000   \n",
       "mean           0.043          0.042          0.042          0.042   \n",
       "std            0.007          0.006          0.006          0.006   \n",
       "min            0.021          0.020          0.020          0.020   \n",
       "25%            0.038          0.038          0.038          0.038   \n",
       "50%            0.042          0.042          0.042          0.042   \n",
       "75%            0.047          0.047          0.046          0.046   \n",
       "max            0.095          0.089          0.089          0.088   \n",
       "\n",
       "       loss_epoch_74  loss_epoch_75  loss_epoch_76  loss_epoch_77  \\\n",
       "count      49171.000      49086.000      48990.000      48876.000   \n",
       "mean           0.042          0.042          0.041          0.041   \n",
       "std            0.006          0.006          0.006          0.006   \n",
       "min            0.020          0.020          0.020          0.020   \n",
       "25%            0.037          0.037          0.037          0.037   \n",
       "50%            0.042          0.041          0.041          0.041   \n",
       "75%            0.046          0.046          0.045          0.045   \n",
       "max            0.088          0.086          0.085          0.089   \n",
       "\n",
       "       loss_epoch_78  loss_epoch_79  loss_epoch_80  loss_epoch_81  \\\n",
       "count      48734.000      48604.000      48450.000      48292.000   \n",
       "mean           0.041          0.041          0.041          0.040   \n",
       "std            0.006          0.006          0.006          0.006   \n",
       "min            0.020          0.019          0.019          0.019   \n",
       "25%            0.037          0.037          0.036          0.036   \n",
       "50%            0.041          0.041          0.040          0.040   \n",
       "75%            0.045          0.045          0.045          0.044   \n",
       "max            0.084          0.083          0.084          0.083   \n",
       "\n",
       "       loss_epoch_82  loss_epoch_83  loss_epoch_84  loss_epoch_85  \\\n",
       "count      48129.000      47962.000      47784.000      47623.000   \n",
       "mean           0.040          0.040          0.040          0.040   \n",
       "std            0.006          0.006          0.006          0.006   \n",
       "min            0.019          0.019          0.019          0.019   \n",
       "25%            0.036          0.036          0.036          0.036   \n",
       "50%            0.040          0.040          0.040          0.040   \n",
       "75%            0.044          0.044          0.044          0.044   \n",
       "max            0.083          0.086          0.082          0.083   \n",
       "\n",
       "       loss_epoch_86  loss_epoch_87  loss_epoch_88  loss_epoch_89  \\\n",
       "count      47424.000      47195.000      46965.000      46709.000   \n",
       "mean           0.040          0.039          0.039          0.039   \n",
       "std            0.006          0.006          0.006          0.006   \n",
       "min            0.020          0.018          0.019          0.019   \n",
       "25%            0.035          0.035          0.035          0.035   \n",
       "50%            0.039          0.039          0.039          0.039   \n",
       "75%            0.043          0.043          0.043          0.043   \n",
       "max            0.083          0.080          0.081          0.079   \n",
       "\n",
       "       loss_epoch_90  loss_epoch_91  loss_epoch_92  loss_epoch_93  \\\n",
       "count      46440.000      46166.000      45899.000      45611.000   \n",
       "mean           0.039          0.039          0.039          0.038   \n",
       "std            0.006          0.006          0.006          0.006   \n",
       "min            0.019          0.019          0.019          0.018   \n",
       "25%            0.035          0.035          0.035          0.034   \n",
       "50%            0.039          0.039          0.038          0.038   \n",
       "75%            0.043          0.043          0.042          0.042   \n",
       "max            0.080          0.082          0.079          0.081   \n",
       "\n",
       "       loss_epoch_94  loss_epoch_95  loss_epoch_96  loss_epoch_97  \\\n",
       "count      45339.000      45037.000      44714.000      44353.000   \n",
       "mean           0.038          0.038          0.038          0.038   \n",
       "std            0.006          0.006          0.006          0.006   \n",
       "min            0.019          0.018          0.019          0.019   \n",
       "25%            0.034          0.034          0.034          0.034   \n",
       "50%            0.038          0.038          0.038          0.038   \n",
       "75%            0.042          0.042          0.042          0.042   \n",
       "max            0.078          0.078          0.077          0.077   \n",
       "\n",
       "       loss_epoch_98  loss_epoch_99  loss_epoch_100  loss_epoch_101  \\\n",
       "count      44009.000      43657.000       43314.000       42929.000   \n",
       "mean           0.038          0.038           0.037           0.037   \n",
       "std            0.006          0.006           0.006           0.006   \n",
       "min            0.018          0.018           0.018           0.017   \n",
       "25%            0.034          0.034           0.034           0.033   \n",
       "50%            0.038          0.037           0.037           0.037   \n",
       "75%            0.042          0.041           0.041           0.041   \n",
       "max            0.079          0.077           0.076           0.076   \n",
       "\n",
       "       loss_epoch_102  loss_epoch_103  loss_epoch_104  loss_epoch_105  \\\n",
       "count       42508.000       42093.000       41659.000       41237.000   \n",
       "mean            0.037           0.037           0.037           0.037   \n",
       "std             0.006           0.006           0.006           0.006   \n",
       "min             0.018           0.018           0.018           0.018   \n",
       "25%             0.033           0.033           0.033           0.033   \n",
       "50%             0.037           0.037           0.037           0.037   \n",
       "75%             0.041           0.041           0.041           0.041   \n",
       "max             0.076           0.083           0.075           0.077   \n",
       "\n",
       "       loss_epoch_106  loss_epoch_107  loss_epoch_108  loss_epoch_109  \\\n",
       "count       40826.000       40392.000       39922.000       39426.000   \n",
       "mean            0.037           0.037           0.036           0.036   \n",
       "std             0.006           0.006           0.006           0.006   \n",
       "min             0.018           0.018           0.018           0.017   \n",
       "25%             0.033           0.033           0.033           0.033   \n",
       "50%             0.037           0.036           0.036           0.036   \n",
       "75%             0.040           0.040           0.040           0.040   \n",
       "max             0.078           0.074           0.076           0.073   \n",
       "\n",
       "       loss_epoch_110  loss_epoch_111  loss_epoch_112  loss_epoch_113  \\\n",
       "count       38899.000       38437.000       37935.000       37418.000   \n",
       "mean            0.036           0.036           0.036           0.036   \n",
       "std             0.006           0.006           0.006           0.006   \n",
       "min             0.018           0.018           0.018           0.018   \n",
       "25%             0.032           0.032           0.032           0.032   \n",
       "50%             0.036           0.036           0.036           0.036   \n",
       "75%             0.040           0.040           0.040           0.039   \n",
       "max             0.074           0.072           0.072           0.076   \n",
       "\n",
       "       loss_epoch_114  loss_epoch_115  loss_epoch_116  loss_epoch_117  \\\n",
       "count       36939.000       36417.000       35913.000       35372.000   \n",
       "mean            0.036           0.036           0.036           0.035   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.017           0.017           0.017           0.017   \n",
       "25%             0.032           0.032           0.032           0.032   \n",
       "50%             0.036           0.036           0.035           0.035   \n",
       "75%             0.039           0.039           0.039           0.039   \n",
       "max             0.073           0.072           0.076           0.072   \n",
       "\n",
       "       loss_epoch_118  loss_epoch_119  loss_epoch_120  loss_epoch_121  \\\n",
       "count       34824.000       34290.000       33739.000       33162.000   \n",
       "mean            0.035           0.035           0.035           0.035   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.017           0.017           0.018           0.017   \n",
       "25%             0.032           0.032           0.031           0.031   \n",
       "50%             0.035           0.035           0.035           0.035   \n",
       "75%             0.039           0.039           0.039           0.039   \n",
       "max             0.069           0.070           0.071           0.072   \n",
       "\n",
       "       loss_epoch_122  loss_epoch_123  loss_epoch_124  loss_epoch_125  \\\n",
       "count       32587.000       32037.000       31435.000       30861.000   \n",
       "mean            0.035           0.035           0.035           0.035   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.017           0.018           0.017           0.017   \n",
       "25%             0.031           0.031           0.031           0.031   \n",
       "50%             0.035           0.035           0.035           0.035   \n",
       "75%             0.038           0.038           0.038           0.038   \n",
       "max             0.071           0.070           0.073           0.073   \n",
       "\n",
       "       loss_epoch_126  loss_epoch_127  loss_epoch_128  loss_epoch_129  \\\n",
       "count       30298.000       29751.000       29196.000       28593.000   \n",
       "mean            0.035           0.034           0.034           0.034   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.017           0.017           0.016           0.017   \n",
       "25%             0.031           0.031           0.031           0.031   \n",
       "50%             0.034           0.034           0.034           0.034   \n",
       "75%             0.038           0.038           0.038           0.038   \n",
       "max             0.070           0.069           0.070           0.070   \n",
       "\n",
       "       loss_epoch_130  loss_epoch_131  loss_epoch_132  loss_epoch_133  \\\n",
       "count       28022.000       27441.000       26875.000       26276.000   \n",
       "mean            0.034           0.034           0.034           0.034   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.017           0.017           0.017           0.017   \n",
       "25%             0.031           0.030           0.030           0.030   \n",
       "50%             0.034           0.034           0.034           0.034   \n",
       "75%             0.038           0.038           0.037           0.037   \n",
       "max             0.068           0.067           0.069           0.067   \n",
       "\n",
       "       loss_epoch_134  loss_epoch_135  loss_epoch_136  loss_epoch_137  \\\n",
       "count       25731.000       25168.000       24625.000       24089.000   \n",
       "mean            0.034           0.034           0.034           0.034   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.017           0.017           0.016   \n",
       "25%             0.030           0.030           0.030           0.030   \n",
       "50%             0.034           0.034           0.034           0.033   \n",
       "75%             0.037           0.037           0.037           0.037   \n",
       "max             0.067           0.068           0.070           0.066   \n",
       "\n",
       "       loss_epoch_138  loss_epoch_139  loss_epoch_140  loss_epoch_141  \\\n",
       "count       23560.000       22983.000       22420.000       21915.000   \n",
       "mean            0.033           0.033           0.033           0.033   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.017           0.016           0.016           0.016   \n",
       "25%             0.030           0.030           0.030           0.030   \n",
       "50%             0.033           0.033           0.033           0.033   \n",
       "75%             0.037           0.037           0.037           0.037   \n",
       "max             0.067           0.067           0.066           0.065   \n",
       "\n",
       "       loss_epoch_142  loss_epoch_143  loss_epoch_144  loss_epoch_145  \\\n",
       "count       21414.000       20861.000       20337.000       19866.000   \n",
       "mean            0.033           0.033           0.033           0.033   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.016           0.016           0.016   \n",
       "25%             0.030           0.030           0.029           0.029   \n",
       "50%             0.033           0.033           0.033           0.033   \n",
       "75%             0.037           0.037           0.036           0.036   \n",
       "max             0.067           0.066           0.065           0.068   \n",
       "\n",
       "       loss_epoch_146  loss_epoch_147  loss_epoch_148  loss_epoch_149  \\\n",
       "count       19357.000       18812.000       18312.000       17824.000   \n",
       "mean            0.033           0.033           0.033           0.033   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.016           0.016           0.016   \n",
       "25%             0.029           0.029           0.029           0.029   \n",
       "50%             0.033           0.033           0.033           0.032   \n",
       "75%             0.036           0.036           0.036           0.036   \n",
       "max             0.066           0.066           0.065           0.064   \n",
       "\n",
       "       loss_epoch_150  loss_epoch_151  loss_epoch_152  loss_epoch_153  \\\n",
       "count       17372.000       16916.000       16427.000       15973.000   \n",
       "mean            0.033           0.032           0.032           0.032   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.016           0.015           0.016   \n",
       "25%             0.029           0.029           0.029           0.029   \n",
       "50%             0.032           0.032           0.032           0.032   \n",
       "75%             0.036           0.036           0.036           0.036   \n",
       "max             0.065           0.063           0.064           0.065   \n",
       "\n",
       "       loss_epoch_154  loss_epoch_155  loss_epoch_156  loss_epoch_157  \\\n",
       "count       15529.000       15086.000       14656.000       14242.000   \n",
       "mean            0.032           0.032           0.032           0.032   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.016           0.017           0.016   \n",
       "25%             0.029           0.029           0.029           0.029   \n",
       "50%             0.032           0.032           0.032           0.032   \n",
       "75%             0.036           0.035           0.035           0.035   \n",
       "max             0.063           0.062           0.065           0.064   \n",
       "\n",
       "       loss_epoch_158  loss_epoch_159  loss_epoch_160  loss_epoch_161  \\\n",
       "count       13836.000       13441.000       13031.000       12650.000   \n",
       "mean            0.032           0.032           0.032           0.032   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.015           0.015   \n",
       "25%             0.028           0.028           0.028           0.028   \n",
       "50%             0.032           0.032           0.032           0.032   \n",
       "75%             0.035           0.035           0.035           0.035   \n",
       "max             0.062           0.061           0.062           0.063   \n",
       "\n",
       "       loss_epoch_162  loss_epoch_163  loss_epoch_164  loss_epoch_165  \\\n",
       "count       12264.000       11884.000       11508.000       11163.000   \n",
       "mean            0.032           0.032           0.032           0.031   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.015           0.015           0.016   \n",
       "25%             0.028           0.028           0.028           0.028   \n",
       "50%             0.032           0.032           0.031           0.031   \n",
       "75%             0.035           0.035           0.035           0.035   \n",
       "max             0.062           0.064           0.052           0.051   \n",
       "\n",
       "       loss_epoch_166  loss_epoch_167  loss_epoch_168  loss_epoch_169  \\\n",
       "count       10813.000       10465.000       10126.000        9791.000   \n",
       "mean            0.031           0.031           0.031           0.031   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.015           0.015   \n",
       "25%             0.028           0.028           0.028           0.028   \n",
       "50%             0.031           0.031           0.031           0.031   \n",
       "75%             0.035           0.035           0.035           0.034   \n",
       "max             0.051           0.051           0.051           0.051   \n",
       "\n",
       "       loss_epoch_170  loss_epoch_171  loss_epoch_172  loss_epoch_173  \\\n",
       "count        9460.000        9137.000        8842.000        8512.000   \n",
       "mean            0.031           0.031           0.031           0.031   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.015           0.015   \n",
       "25%             0.028           0.028           0.028           0.028   \n",
       "50%             0.031           0.031           0.031           0.031   \n",
       "75%             0.034           0.034           0.034           0.034   \n",
       "max             0.051           0.050           0.053           0.050   \n",
       "\n",
       "       loss_epoch_174  loss_epoch_175  loss_epoch_176  loss_epoch_177  \\\n",
       "count        8217.000        7929.000        7644.000        7362.000   \n",
       "mean            0.031           0.031           0.031           0.031   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.014           0.015           0.015   \n",
       "25%             0.028           0.027           0.027           0.027   \n",
       "50%             0.031           0.031           0.031           0.031   \n",
       "75%             0.034           0.034           0.034           0.034   \n",
       "max             0.051           0.054           0.051           0.051   \n",
       "\n",
       "       loss_epoch_178  loss_epoch_179  loss_epoch_180  loss_epoch_181  \\\n",
       "count        7087.000        6863.000        6614.000        6328.000   \n",
       "mean            0.031           0.031           0.031           0.030   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.015           0.015   \n",
       "25%             0.027           0.027           0.027           0.027   \n",
       "50%             0.030           0.031           0.030           0.030   \n",
       "75%             0.034           0.034           0.034           0.034   \n",
       "max             0.051           0.051           0.050           0.049   \n",
       "\n",
       "       loss_epoch_182  loss_epoch_183  loss_epoch_184  loss_epoch_185  \\\n",
       "count        6076.000        5826.000        5606.000        5383.000   \n",
       "mean            0.030           0.030           0.030           0.030   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.016           0.016           0.016   \n",
       "25%             0.027           0.027           0.027           0.027   \n",
       "50%             0.030           0.030           0.030           0.030   \n",
       "75%             0.034           0.034           0.033           0.034   \n",
       "max             0.052           0.052           0.050           0.050   \n",
       "\n",
       "       loss_epoch_186  loss_epoch_187  loss_epoch_188  loss_epoch_189  \\\n",
       "count        5167.000        4971.000        4765.000        4597.000   \n",
       "mean            0.030           0.030           0.030           0.030   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.016           0.016           0.016   \n",
       "25%             0.027           0.027           0.027           0.027   \n",
       "50%             0.030           0.030           0.030           0.030   \n",
       "75%             0.033           0.033           0.033           0.033   \n",
       "max             0.049           0.052           0.050           0.048   \n",
       "\n",
       "       loss_epoch_190  loss_epoch_191  loss_epoch_192  loss_epoch_193  \\\n",
       "count        4430.000        4264.000        4085.000        3910.000   \n",
       "mean            0.030           0.030           0.030           0.030   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.015           0.015           0.015   \n",
       "25%             0.027           0.027           0.027           0.026   \n",
       "50%             0.030           0.030           0.030           0.030   \n",
       "75%             0.033           0.033           0.033           0.033   \n",
       "max             0.049           0.048           0.049           0.050   \n",
       "\n",
       "       loss_epoch_194  loss_epoch_195  loss_epoch_196  loss_epoch_197  \\\n",
       "count        3758.000        3607.000        3443.000        3313.000   \n",
       "mean            0.030           0.030           0.030           0.030   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.016           0.015   \n",
       "25%             0.026           0.026           0.026           0.026   \n",
       "50%             0.030           0.030           0.030           0.030   \n",
       "75%             0.033           0.033           0.033           0.033   \n",
       "max             0.047           0.048           0.048           0.047   \n",
       "\n",
       "       loss_epoch_198  loss_epoch_199  loss_epoch_200  loss_epoch_201  \\\n",
       "count        3155.000        3020.000        2892.000        2746.000   \n",
       "mean            0.030           0.030           0.029           0.029   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.016           0.015           0.015   \n",
       "25%             0.026           0.026           0.026           0.026   \n",
       "50%             0.029           0.030           0.029           0.029   \n",
       "75%             0.033           0.033           0.033           0.033   \n",
       "max             0.049           0.047           0.047           0.047   \n",
       "\n",
       "       loss_epoch_202  loss_epoch_203  loss_epoch_204  loss_epoch_205  \\\n",
       "count        2641.000        2532.000        2412.000        2290.000   \n",
       "mean            0.029           0.029           0.029           0.029   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.015           0.016   \n",
       "25%             0.026           0.026           0.026           0.026   \n",
       "50%             0.029           0.029           0.029           0.029   \n",
       "75%             0.032           0.032           0.033           0.032   \n",
       "max             0.049           0.046           0.050           0.048   \n",
       "\n",
       "       loss_epoch_206  loss_epoch_207  loss_epoch_208  loss_epoch_209  \\\n",
       "count        2169.000        2054.000        1954.000        1872.000   \n",
       "mean            0.029           0.029           0.029           0.029   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.016           0.016   \n",
       "25%             0.026           0.026           0.026           0.026   \n",
       "50%             0.029           0.029           0.029           0.029   \n",
       "75%             0.032           0.032           0.032           0.032   \n",
       "max             0.047           0.049           0.048           0.047   \n",
       "\n",
       "       loss_epoch_210  loss_epoch_211  loss_epoch_212  loss_epoch_213  \\\n",
       "count        1772.000        1675.000        1599.000        1532.000   \n",
       "mean            0.029           0.029           0.029           0.029   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.016           0.015   \n",
       "25%             0.026           0.026           0.026           0.026   \n",
       "50%             0.029           0.029           0.029           0.029   \n",
       "75%             0.032           0.032           0.032           0.032   \n",
       "max             0.046           0.047           0.046           0.047   \n",
       "\n",
       "       loss_epoch_214  loss_epoch_215  loss_epoch_216  loss_epoch_217  \\\n",
       "count        1449.000        1385.000        1327.000        1262.000   \n",
       "mean            0.029           0.029           0.029           0.029   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.016           0.015   \n",
       "25%             0.026           0.025           0.025           0.025   \n",
       "50%             0.029           0.029           0.029           0.029   \n",
       "75%             0.032           0.032           0.032           0.032   \n",
       "max             0.047           0.045           0.048           0.045   \n",
       "\n",
       "       loss_epoch_218  loss_epoch_219  loss_epoch_220  loss_epoch_221  \\\n",
       "count        1198.000        1123.000        1062.000        1009.000   \n",
       "mean            0.029           0.029           0.029           0.029   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.015           0.016   \n",
       "25%             0.025           0.025           0.025           0.025   \n",
       "50%             0.029           0.029           0.029           0.028   \n",
       "75%             0.032           0.032           0.032           0.032   \n",
       "max             0.046           0.045           0.048           0.045   \n",
       "\n",
       "       loss_epoch_222  loss_epoch_223  loss_epoch_224  loss_epoch_225  \\\n",
       "count         955.000         913.000         871.000         818.000   \n",
       "mean            0.029           0.028           0.028           0.028   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.015           0.016           0.016   \n",
       "25%             0.025           0.025           0.025           0.025   \n",
       "50%             0.028           0.028           0.028           0.028   \n",
       "75%             0.032           0.032           0.032           0.032   \n",
       "max             0.050           0.045           0.045           0.048   \n",
       "\n",
       "       loss_epoch_226  loss_epoch_227  loss_epoch_228  loss_epoch_229  \\\n",
       "count         762.000         729.000         687.000         653.000   \n",
       "mean            0.028           0.028           0.028           0.028   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.016           0.016           0.016   \n",
       "25%             0.025           0.025           0.025           0.025   \n",
       "50%             0.028           0.028           0.028           0.028   \n",
       "75%             0.031           0.031           0.031           0.032   \n",
       "max             0.045           0.047           0.047           0.043   \n",
       "\n",
       "       loss_epoch_230  loss_epoch_231  loss_epoch_232  loss_epoch_233  \\\n",
       "count         621.000         588.000         556.000         526.000   \n",
       "mean            0.028           0.028           0.028           0.028   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.016           0.015   \n",
       "25%             0.024           0.025           0.024           0.024   \n",
       "50%             0.028           0.028           0.028           0.028   \n",
       "75%             0.032           0.031           0.031           0.031   \n",
       "max             0.042           0.042           0.041           0.044   \n",
       "\n",
       "       loss_epoch_234  loss_epoch_235  loss_epoch_236  loss_epoch_237  \\\n",
       "count         501.000         476.000         448.000         424.000   \n",
       "mean            0.028           0.028           0.028           0.028   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.015           0.015   \n",
       "25%             0.024           0.024           0.024           0.024   \n",
       "50%             0.028           0.028           0.027           0.027   \n",
       "75%             0.031           0.031           0.031           0.031   \n",
       "max             0.042           0.041           0.040           0.042   \n",
       "\n",
       "       loss_epoch_238  loss_epoch_239  loss_epoch_240  loss_epoch_241  \\\n",
       "count         392.000         374.000         357.000         333.000   \n",
       "mean            0.028           0.028           0.028           0.028   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.016           0.015           0.015           0.015   \n",
       "25%             0.024           0.024           0.024           0.025   \n",
       "50%             0.028           0.028           0.027           0.028   \n",
       "75%             0.031           0.031           0.031           0.031   \n",
       "max             0.040           0.041           0.041           0.040   \n",
       "\n",
       "       loss_epoch_242  loss_epoch_243  loss_epoch_244  loss_epoch_245  \\\n",
       "count         307.000         290.000         271.000         252.000   \n",
       "mean            0.028           0.028           0.028           0.028   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.016           0.015   \n",
       "25%             0.024           0.024           0.024           0.024   \n",
       "50%             0.027           0.027           0.027           0.028   \n",
       "75%             0.031           0.032           0.031           0.031   \n",
       "max             0.041           0.041           0.042           0.041   \n",
       "\n",
       "       loss_epoch_246  loss_epoch_247  loss_epoch_248  loss_epoch_249  \\\n",
       "count         234.000         220.000         208.000         197.000   \n",
       "mean            0.028           0.028           0.028           0.028   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.015           0.016   \n",
       "25%             0.024           0.024           0.024           0.024   \n",
       "50%             0.027           0.027           0.027           0.027   \n",
       "75%             0.032           0.031           0.031           0.031   \n",
       "max             0.042           0.041           0.043           0.041   \n",
       "\n",
       "       loss_epoch_250  loss_epoch_251  loss_epoch_252  loss_epoch_253  \\\n",
       "count         184.000         178.000         169.000         158.000   \n",
       "mean            0.028           0.028           0.027           0.027   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.014           0.015   \n",
       "25%             0.024           0.024           0.024           0.024   \n",
       "50%             0.027           0.027           0.027           0.027   \n",
       "75%             0.031           0.031           0.031           0.031   \n",
       "max             0.040           0.041           0.040           0.040   \n",
       "\n",
       "       loss_epoch_254  loss_epoch_255  loss_epoch_256  loss_epoch_257  \\\n",
       "count         146.000         139.000         129.000         122.000   \n",
       "mean            0.027           0.028           0.027           0.027   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.015           0.015           0.018           0.017   \n",
       "25%             0.024           0.023           0.023           0.024   \n",
       "50%             0.027           0.027           0.027           0.027   \n",
       "75%             0.031           0.031           0.030           0.031   \n",
       "max             0.042           0.040           0.040           0.041   \n",
       "\n",
       "       loss_epoch_258  loss_epoch_259  loss_epoch_260  loss_epoch_261  \\\n",
       "count         113.000         107.000          98.000          95.000   \n",
       "mean            0.028           0.028           0.028           0.028   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.018           0.017           0.017           0.018   \n",
       "25%             0.024           0.023           0.024           0.025   \n",
       "50%             0.027           0.028           0.027           0.028   \n",
       "75%             0.031           0.031           0.031           0.031   \n",
       "max             0.040           0.041           0.040           0.041   \n",
       "\n",
       "       loss_epoch_262  loss_epoch_263  loss_epoch_264  loss_epoch_265  \\\n",
       "count          88.000          78.000          72.000          69.000   \n",
       "mean            0.028           0.028           0.028           0.028   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.018           0.018           0.017           0.017   \n",
       "25%             0.025           0.025           0.024           0.025   \n",
       "50%             0.028           0.027           0.028           0.028   \n",
       "75%             0.031           0.031           0.031           0.032   \n",
       "max             0.040           0.042           0.041           0.041   \n",
       "\n",
       "       loss_epoch_266  loss_epoch_267  loss_epoch_268  loss_epoch_269  \\\n",
       "count          66.000          62.000          61.000          54.000   \n",
       "mean            0.028           0.028           0.028           0.027   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.017           0.017           0.018           0.018   \n",
       "25%             0.024           0.024           0.025           0.024   \n",
       "50%             0.027           0.028           0.028           0.027   \n",
       "75%             0.031           0.031           0.031           0.030   \n",
       "max             0.040           0.039           0.038           0.038   \n",
       "\n",
       "       loss_epoch_270  loss_epoch_271  loss_epoch_272  loss_epoch_273  \\\n",
       "count          50.000          47.000          45.000          38.000   \n",
       "mean            0.027           0.027           0.027           0.028   \n",
       "std             0.006           0.005           0.005           0.004   \n",
       "min             0.018           0.018           0.017           0.018   \n",
       "25%             0.024           0.024           0.025           0.025   \n",
       "50%             0.027           0.027           0.027           0.027   \n",
       "75%             0.031           0.030           0.030           0.031   \n",
       "max             0.041           0.037           0.039           0.038   \n",
       "\n",
       "       loss_epoch_274  loss_epoch_275  loss_epoch_276  loss_epoch_277  \\\n",
       "count          34.000          33.000          30.000          26.000   \n",
       "mean            0.028           0.028           0.027           0.027   \n",
       "std             0.004           0.005           0.004           0.005   \n",
       "min             0.020           0.020           0.020           0.020   \n",
       "25%             0.025           0.025           0.024           0.024   \n",
       "50%             0.027           0.028           0.027           0.026   \n",
       "75%             0.031           0.031           0.030           0.031   \n",
       "max             0.038           0.037           0.037           0.037   \n",
       "\n",
       "       loss_epoch_278  loss_epoch_279  loss_epoch_280  loss_epoch_281  \\\n",
       "count          26.000          25.000          25.000          22.000   \n",
       "mean            0.027           0.028           0.027           0.028   \n",
       "std             0.005           0.004           0.005           0.005   \n",
       "min             0.021           0.021           0.021           0.021   \n",
       "25%             0.025           0.024           0.024           0.025   \n",
       "50%             0.027           0.027           0.026           0.027   \n",
       "75%             0.030           0.030           0.031           0.029   \n",
       "max             0.038           0.039           0.039           0.038   \n",
       "\n",
       "       loss_epoch_282  loss_epoch_283  loss_epoch_284  loss_epoch_285  \\\n",
       "count          20.000          18.000          17.000          17.000   \n",
       "mean            0.028           0.028           0.027           0.027   \n",
       "std             0.005           0.004           0.005           0.005   \n",
       "min             0.021           0.021           0.022           0.021   \n",
       "25%             0.024           0.024           0.024           0.024   \n",
       "50%             0.027           0.026           0.026           0.027   \n",
       "75%             0.030           0.030           0.028           0.030   \n",
       "max             0.039           0.037           0.036           0.038   \n",
       "\n",
       "       loss_epoch_286  loss_epoch_287  loss_epoch_288  loss_epoch_289  \\\n",
       "count          16.000          14.000          14.000          13.000   \n",
       "mean            0.028           0.028           0.028           0.028   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.020           0.021           0.020           0.021   \n",
       "25%             0.024           0.025           0.024           0.025   \n",
       "50%             0.026           0.027           0.027           0.027   \n",
       "75%             0.031           0.029           0.030           0.030   \n",
       "max             0.039           0.038           0.039           0.040   \n",
       "\n",
       "       loss_epoch_290  loss_epoch_291  loss_epoch_292  loss_epoch_293  \\\n",
       "count          13.000          11.000          11.000           9.000   \n",
       "mean            0.027           0.027           0.027           0.027   \n",
       "std             0.005           0.006           0.005           0.005   \n",
       "min             0.021           0.020           0.020           0.021   \n",
       "25%             0.024           0.024           0.023           0.024   \n",
       "50%             0.027           0.025           0.025           0.025   \n",
       "75%             0.031           0.028           0.029           0.027   \n",
       "max             0.038           0.040           0.037           0.037   \n",
       "\n",
       "       loss_epoch_294  loss_epoch_295  loss_epoch_296  loss_epoch_297  \\\n",
       "count           8.000           8.000           8.000           7.000   \n",
       "mean            0.027           0.026           0.026           0.028   \n",
       "std             0.006           0.006           0.006           0.006   \n",
       "min             0.021           0.020           0.021           0.020   \n",
       "25%             0.024           0.023           0.023           0.024   \n",
       "50%             0.024           0.024           0.024           0.026   \n",
       "75%             0.028           0.027           0.028           0.031   \n",
       "max             0.037           0.037           0.036           0.037   \n",
       "\n",
       "       loss_epoch_298  loss_epoch_299  loss_epoch_300  loss_epoch_301  \\\n",
       "count           7.000           7.000           7.000           7.000   \n",
       "mean            0.027           0.027           0.027           0.027   \n",
       "std             0.007           0.007           0.006           0.006   \n",
       "min             0.021           0.020           0.020           0.021   \n",
       "25%             0.024           0.024           0.023           0.023   \n",
       "50%             0.024           0.024           0.024           0.024   \n",
       "75%             0.030           0.030           0.030           0.030   \n",
       "max             0.039           0.039           0.037           0.036   \n",
       "\n",
       "       loss_epoch_302  loss_epoch_303  loss_epoch_304  loss_epoch_305  \\\n",
       "count           6.000           4.000           4.000           3.000   \n",
       "mean            0.027           0.029           0.030           0.031   \n",
       "std             0.006           0.007           0.007           0.008   \n",
       "min             0.022           0.023           0.023           0.022   \n",
       "25%             0.023           0.023           0.024           0.028   \n",
       "50%             0.023           0.028           0.029           0.033   \n",
       "75%             0.031           0.034           0.036           0.035   \n",
       "max             0.036           0.036           0.037           0.037   \n",
       "\n",
       "       loss_epoch_306  loss_epoch_307  loss_epoch_308  loss_epoch_309  \\\n",
       "count           2.000           2.000           2.000           2.000   \n",
       "mean            0.035           0.035           0.034           0.034   \n",
       "std             0.004           0.002           0.003           0.002   \n",
       "min             0.033           0.034           0.032           0.032   \n",
       "25%             0.034           0.035           0.033           0.033   \n",
       "50%             0.035           0.035           0.034           0.034   \n",
       "75%             0.037           0.036           0.035           0.035   \n",
       "max             0.038           0.037           0.036           0.036   \n",
       "\n",
       "       loss_epoch_310  loss_epoch_311  loss_epoch_312  loss_epoch_313  \n",
       "count           2.000           2.000           1.000           1.000  \n",
       "mean            0.035           0.037           0.033           0.032  \n",
       "std             0.003           0.001             NaN             NaN  \n",
       "min             0.033           0.036           0.033           0.032  \n",
       "25%             0.034           0.037           0.033           0.032  \n",
       "50%             0.035           0.037           0.033           0.032  \n",
       "75%             0.036           0.037           0.033           0.032  \n",
       "max             0.037           0.038           0.033           0.032  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>val_loss_epoch_11</th>\n",
       "      <th>val_loss_epoch_12</th>\n",
       "      <th>val_loss_epoch_13</th>\n",
       "      <th>val_loss_epoch_14</th>\n",
       "      <th>val_loss_epoch_15</th>\n",
       "      <th>val_loss_epoch_16</th>\n",
       "      <th>val_loss_epoch_17</th>\n",
       "      <th>val_loss_epoch_18</th>\n",
       "      <th>val_loss_epoch_19</th>\n",
       "      <th>val_loss_epoch_20</th>\n",
       "      <th>val_loss_epoch_21</th>\n",
       "      <th>val_loss_epoch_22</th>\n",
       "      <th>val_loss_epoch_23</th>\n",
       "      <th>val_loss_epoch_24</th>\n",
       "      <th>val_loss_epoch_25</th>\n",
       "      <th>val_loss_epoch_26</th>\n",
       "      <th>val_loss_epoch_27</th>\n",
       "      <th>val_loss_epoch_28</th>\n",
       "      <th>val_loss_epoch_29</th>\n",
       "      <th>val_loss_epoch_30</th>\n",
       "      <th>val_loss_epoch_31</th>\n",
       "      <th>val_loss_epoch_32</th>\n",
       "      <th>val_loss_epoch_33</th>\n",
       "      <th>val_loss_epoch_34</th>\n",
       "      <th>val_loss_epoch_35</th>\n",
       "      <th>val_loss_epoch_36</th>\n",
       "      <th>val_loss_epoch_37</th>\n",
       "      <th>val_loss_epoch_38</th>\n",
       "      <th>val_loss_epoch_39</th>\n",
       "      <th>val_loss_epoch_40</th>\n",
       "      <th>val_loss_epoch_41</th>\n",
       "      <th>val_loss_epoch_42</th>\n",
       "      <th>val_loss_epoch_43</th>\n",
       "      <th>val_loss_epoch_44</th>\n",
       "      <th>val_loss_epoch_45</th>\n",
       "      <th>val_loss_epoch_46</th>\n",
       "      <th>val_loss_epoch_47</th>\n",
       "      <th>val_loss_epoch_48</th>\n",
       "      <th>val_loss_epoch_49</th>\n",
       "      <th>val_loss_epoch_50</th>\n",
       "      <th>val_loss_epoch_51</th>\n",
       "      <th>val_loss_epoch_52</th>\n",
       "      <th>val_loss_epoch_53</th>\n",
       "      <th>val_loss_epoch_54</th>\n",
       "      <th>val_loss_epoch_55</th>\n",
       "      <th>val_loss_epoch_56</th>\n",
       "      <th>val_loss_epoch_57</th>\n",
       "      <th>val_loss_epoch_58</th>\n",
       "      <th>val_loss_epoch_59</th>\n",
       "      <th>val_loss_epoch_60</th>\n",
       "      <th>val_loss_epoch_61</th>\n",
       "      <th>val_loss_epoch_62</th>\n",
       "      <th>val_loss_epoch_63</th>\n",
       "      <th>val_loss_epoch_64</th>\n",
       "      <th>val_loss_epoch_65</th>\n",
       "      <th>val_loss_epoch_66</th>\n",
       "      <th>val_loss_epoch_67</th>\n",
       "      <th>val_loss_epoch_68</th>\n",
       "      <th>val_loss_epoch_69</th>\n",
       "      <th>val_loss_epoch_70</th>\n",
       "      <th>val_loss_epoch_71</th>\n",
       "      <th>val_loss_epoch_72</th>\n",
       "      <th>val_loss_epoch_73</th>\n",
       "      <th>val_loss_epoch_74</th>\n",
       "      <th>val_loss_epoch_75</th>\n",
       "      <th>val_loss_epoch_76</th>\n",
       "      <th>val_loss_epoch_77</th>\n",
       "      <th>val_loss_epoch_78</th>\n",
       "      <th>val_loss_epoch_79</th>\n",
       "      <th>val_loss_epoch_80</th>\n",
       "      <th>val_loss_epoch_81</th>\n",
       "      <th>val_loss_epoch_82</th>\n",
       "      <th>val_loss_epoch_83</th>\n",
       "      <th>val_loss_epoch_84</th>\n",
       "      <th>val_loss_epoch_85</th>\n",
       "      <th>val_loss_epoch_86</th>\n",
       "      <th>val_loss_epoch_87</th>\n",
       "      <th>val_loss_epoch_88</th>\n",
       "      <th>val_loss_epoch_89</th>\n",
       "      <th>val_loss_epoch_90</th>\n",
       "      <th>val_loss_epoch_91</th>\n",
       "      <th>val_loss_epoch_92</th>\n",
       "      <th>val_loss_epoch_93</th>\n",
       "      <th>val_loss_epoch_94</th>\n",
       "      <th>val_loss_epoch_95</th>\n",
       "      <th>val_loss_epoch_96</th>\n",
       "      <th>val_loss_epoch_97</th>\n",
       "      <th>val_loss_epoch_98</th>\n",
       "      <th>val_loss_epoch_99</th>\n",
       "      <th>val_loss_epoch_100</th>\n",
       "      <th>val_loss_epoch_101</th>\n",
       "      <th>val_loss_epoch_102</th>\n",
       "      <th>val_loss_epoch_103</th>\n",
       "      <th>val_loss_epoch_104</th>\n",
       "      <th>val_loss_epoch_105</th>\n",
       "      <th>val_loss_epoch_106</th>\n",
       "      <th>val_loss_epoch_107</th>\n",
       "      <th>val_loss_epoch_108</th>\n",
       "      <th>val_loss_epoch_109</th>\n",
       "      <th>val_loss_epoch_110</th>\n",
       "      <th>val_loss_epoch_111</th>\n",
       "      <th>val_loss_epoch_112</th>\n",
       "      <th>val_loss_epoch_113</th>\n",
       "      <th>val_loss_epoch_114</th>\n",
       "      <th>val_loss_epoch_115</th>\n",
       "      <th>val_loss_epoch_116</th>\n",
       "      <th>val_loss_epoch_117</th>\n",
       "      <th>val_loss_epoch_118</th>\n",
       "      <th>val_loss_epoch_119</th>\n",
       "      <th>val_loss_epoch_120</th>\n",
       "      <th>val_loss_epoch_121</th>\n",
       "      <th>val_loss_epoch_122</th>\n",
       "      <th>val_loss_epoch_123</th>\n",
       "      <th>val_loss_epoch_124</th>\n",
       "      <th>val_loss_epoch_125</th>\n",
       "      <th>val_loss_epoch_126</th>\n",
       "      <th>val_loss_epoch_127</th>\n",
       "      <th>val_loss_epoch_128</th>\n",
       "      <th>val_loss_epoch_129</th>\n",
       "      <th>val_loss_epoch_130</th>\n",
       "      <th>val_loss_epoch_131</th>\n",
       "      <th>val_loss_epoch_132</th>\n",
       "      <th>val_loss_epoch_133</th>\n",
       "      <th>val_loss_epoch_134</th>\n",
       "      <th>val_loss_epoch_135</th>\n",
       "      <th>val_loss_epoch_136</th>\n",
       "      <th>val_loss_epoch_137</th>\n",
       "      <th>val_loss_epoch_138</th>\n",
       "      <th>val_loss_epoch_139</th>\n",
       "      <th>val_loss_epoch_140</th>\n",
       "      <th>val_loss_epoch_141</th>\n",
       "      <th>val_loss_epoch_142</th>\n",
       "      <th>val_loss_epoch_143</th>\n",
       "      <th>val_loss_epoch_144</th>\n",
       "      <th>val_loss_epoch_145</th>\n",
       "      <th>val_loss_epoch_146</th>\n",
       "      <th>val_loss_epoch_147</th>\n",
       "      <th>val_loss_epoch_148</th>\n",
       "      <th>val_loss_epoch_149</th>\n",
       "      <th>val_loss_epoch_150</th>\n",
       "      <th>val_loss_epoch_151</th>\n",
       "      <th>val_loss_epoch_152</th>\n",
       "      <th>val_loss_epoch_153</th>\n",
       "      <th>val_loss_epoch_154</th>\n",
       "      <th>val_loss_epoch_155</th>\n",
       "      <th>val_loss_epoch_156</th>\n",
       "      <th>val_loss_epoch_157</th>\n",
       "      <th>val_loss_epoch_158</th>\n",
       "      <th>val_loss_epoch_159</th>\n",
       "      <th>val_loss_epoch_160</th>\n",
       "      <th>val_loss_epoch_161</th>\n",
       "      <th>val_loss_epoch_162</th>\n",
       "      <th>val_loss_epoch_163</th>\n",
       "      <th>val_loss_epoch_164</th>\n",
       "      <th>val_loss_epoch_165</th>\n",
       "      <th>val_loss_epoch_166</th>\n",
       "      <th>val_loss_epoch_167</th>\n",
       "      <th>val_loss_epoch_168</th>\n",
       "      <th>val_loss_epoch_169</th>\n",
       "      <th>val_loss_epoch_170</th>\n",
       "      <th>val_loss_epoch_171</th>\n",
       "      <th>val_loss_epoch_172</th>\n",
       "      <th>val_loss_epoch_173</th>\n",
       "      <th>val_loss_epoch_174</th>\n",
       "      <th>val_loss_epoch_175</th>\n",
       "      <th>val_loss_epoch_176</th>\n",
       "      <th>val_loss_epoch_177</th>\n",
       "      <th>val_loss_epoch_178</th>\n",
       "      <th>val_loss_epoch_179</th>\n",
       "      <th>val_loss_epoch_180</th>\n",
       "      <th>val_loss_epoch_181</th>\n",
       "      <th>val_loss_epoch_182</th>\n",
       "      <th>val_loss_epoch_183</th>\n",
       "      <th>val_loss_epoch_184</th>\n",
       "      <th>val_loss_epoch_185</th>\n",
       "      <th>val_loss_epoch_186</th>\n",
       "      <th>val_loss_epoch_187</th>\n",
       "      <th>val_loss_epoch_188</th>\n",
       "      <th>val_loss_epoch_189</th>\n",
       "      <th>val_loss_epoch_190</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "      <th>val_loss_epoch_201</th>\n",
       "      <th>val_loss_epoch_202</th>\n",
       "      <th>val_loss_epoch_203</th>\n",
       "      <th>val_loss_epoch_204</th>\n",
       "      <th>val_loss_epoch_205</th>\n",
       "      <th>val_loss_epoch_206</th>\n",
       "      <th>val_loss_epoch_207</th>\n",
       "      <th>val_loss_epoch_208</th>\n",
       "      <th>val_loss_epoch_209</th>\n",
       "      <th>val_loss_epoch_210</th>\n",
       "      <th>val_loss_epoch_211</th>\n",
       "      <th>val_loss_epoch_212</th>\n",
       "      <th>val_loss_epoch_213</th>\n",
       "      <th>val_loss_epoch_214</th>\n",
       "      <th>val_loss_epoch_215</th>\n",
       "      <th>val_loss_epoch_216</th>\n",
       "      <th>val_loss_epoch_217</th>\n",
       "      <th>val_loss_epoch_218</th>\n",
       "      <th>val_loss_epoch_219</th>\n",
       "      <th>val_loss_epoch_220</th>\n",
       "      <th>val_loss_epoch_221</th>\n",
       "      <th>val_loss_epoch_222</th>\n",
       "      <th>val_loss_epoch_223</th>\n",
       "      <th>val_loss_epoch_224</th>\n",
       "      <th>val_loss_epoch_225</th>\n",
       "      <th>val_loss_epoch_226</th>\n",
       "      <th>val_loss_epoch_227</th>\n",
       "      <th>val_loss_epoch_228</th>\n",
       "      <th>val_loss_epoch_229</th>\n",
       "      <th>val_loss_epoch_230</th>\n",
       "      <th>val_loss_epoch_231</th>\n",
       "      <th>val_loss_epoch_232</th>\n",
       "      <th>val_loss_epoch_233</th>\n",
       "      <th>val_loss_epoch_234</th>\n",
       "      <th>val_loss_epoch_235</th>\n",
       "      <th>val_loss_epoch_236</th>\n",
       "      <th>val_loss_epoch_237</th>\n",
       "      <th>val_loss_epoch_238</th>\n",
       "      <th>val_loss_epoch_239</th>\n",
       "      <th>val_loss_epoch_240</th>\n",
       "      <th>val_loss_epoch_241</th>\n",
       "      <th>val_loss_epoch_242</th>\n",
       "      <th>val_loss_epoch_243</th>\n",
       "      <th>val_loss_epoch_244</th>\n",
       "      <th>val_loss_epoch_245</th>\n",
       "      <th>val_loss_epoch_246</th>\n",
       "      <th>val_loss_epoch_247</th>\n",
       "      <th>val_loss_epoch_248</th>\n",
       "      <th>val_loss_epoch_249</th>\n",
       "      <th>val_loss_epoch_250</th>\n",
       "      <th>val_loss_epoch_251</th>\n",
       "      <th>val_loss_epoch_252</th>\n",
       "      <th>val_loss_epoch_253</th>\n",
       "      <th>val_loss_epoch_254</th>\n",
       "      <th>val_loss_epoch_255</th>\n",
       "      <th>val_loss_epoch_256</th>\n",
       "      <th>val_loss_epoch_257</th>\n",
       "      <th>val_loss_epoch_258</th>\n",
       "      <th>val_loss_epoch_259</th>\n",
       "      <th>val_loss_epoch_260</th>\n",
       "      <th>val_loss_epoch_261</th>\n",
       "      <th>val_loss_epoch_262</th>\n",
       "      <th>val_loss_epoch_263</th>\n",
       "      <th>val_loss_epoch_264</th>\n",
       "      <th>val_loss_epoch_265</th>\n",
       "      <th>val_loss_epoch_266</th>\n",
       "      <th>val_loss_epoch_267</th>\n",
       "      <th>val_loss_epoch_268</th>\n",
       "      <th>val_loss_epoch_269</th>\n",
       "      <th>val_loss_epoch_270</th>\n",
       "      <th>val_loss_epoch_271</th>\n",
       "      <th>val_loss_epoch_272</th>\n",
       "      <th>val_loss_epoch_273</th>\n",
       "      <th>val_loss_epoch_274</th>\n",
       "      <th>val_loss_epoch_275</th>\n",
       "      <th>val_loss_epoch_276</th>\n",
       "      <th>val_loss_epoch_277</th>\n",
       "      <th>val_loss_epoch_278</th>\n",
       "      <th>val_loss_epoch_279</th>\n",
       "      <th>val_loss_epoch_280</th>\n",
       "      <th>val_loss_epoch_281</th>\n",
       "      <th>val_loss_epoch_282</th>\n",
       "      <th>val_loss_epoch_283</th>\n",
       "      <th>val_loss_epoch_284</th>\n",
       "      <th>val_loss_epoch_285</th>\n",
       "      <th>val_loss_epoch_286</th>\n",
       "      <th>val_loss_epoch_287</th>\n",
       "      <th>val_loss_epoch_288</th>\n",
       "      <th>val_loss_epoch_289</th>\n",
       "      <th>val_loss_epoch_290</th>\n",
       "      <th>val_loss_epoch_291</th>\n",
       "      <th>val_loss_epoch_292</th>\n",
       "      <th>val_loss_epoch_293</th>\n",
       "      <th>val_loss_epoch_294</th>\n",
       "      <th>val_loss_epoch_295</th>\n",
       "      <th>val_loss_epoch_296</th>\n",
       "      <th>val_loss_epoch_297</th>\n",
       "      <th>val_loss_epoch_298</th>\n",
       "      <th>val_loss_epoch_299</th>\n",
       "      <th>val_loss_epoch_300</th>\n",
       "      <th>val_loss_epoch_301</th>\n",
       "      <th>val_loss_epoch_302</th>\n",
       "      <th>val_loss_epoch_303</th>\n",
       "      <th>val_loss_epoch_304</th>\n",
       "      <th>val_loss_epoch_305</th>\n",
       "      <th>val_loss_epoch_306</th>\n",
       "      <th>val_loss_epoch_307</th>\n",
       "      <th>val_loss_epoch_308</th>\n",
       "      <th>val_loss_epoch_309</th>\n",
       "      <th>val_loss_epoch_310</th>\n",
       "      <th>val_loss_epoch_311</th>\n",
       "      <th>val_loss_epoch_312</th>\n",
       "      <th>val_loss_epoch_313</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49998.000</td>\n",
       "      <td>49997.000</td>\n",
       "      <td>49995.000</td>\n",
       "      <td>49992.000</td>\n",
       "      <td>49988.000</td>\n",
       "      <td>49987.000</td>\n",
       "      <td>49985.000</td>\n",
       "      <td>49977.000</td>\n",
       "      <td>49972.000</td>\n",
       "      <td>49963.000</td>\n",
       "      <td>49954.000</td>\n",
       "      <td>49947.000</td>\n",
       "      <td>49928.000</td>\n",
       "      <td>49915.000</td>\n",
       "      <td>49892.000</td>\n",
       "      <td>49877.000</td>\n",
       "      <td>49863.000</td>\n",
       "      <td>49844.000</td>\n",
       "      <td>49815.000</td>\n",
       "      <td>49786.000</td>\n",
       "      <td>49745.000</td>\n",
       "      <td>49715.000</td>\n",
       "      <td>49684.000</td>\n",
       "      <td>49639.000</td>\n",
       "      <td>49590.000</td>\n",
       "      <td>49533.000</td>\n",
       "      <td>49489.000</td>\n",
       "      <td>49419.000</td>\n",
       "      <td>49345.000</td>\n",
       "      <td>49250.000</td>\n",
       "      <td>49171.000</td>\n",
       "      <td>49086.000</td>\n",
       "      <td>48990.000</td>\n",
       "      <td>48876.000</td>\n",
       "      <td>48734.000</td>\n",
       "      <td>48604.000</td>\n",
       "      <td>48450.000</td>\n",
       "      <td>48292.000</td>\n",
       "      <td>48129.000</td>\n",
       "      <td>47962.000</td>\n",
       "      <td>47784.000</td>\n",
       "      <td>47623.000</td>\n",
       "      <td>47424.000</td>\n",
       "      <td>47195.000</td>\n",
       "      <td>46965.000</td>\n",
       "      <td>46709.000</td>\n",
       "      <td>46440.000</td>\n",
       "      <td>46166.000</td>\n",
       "      <td>45899.000</td>\n",
       "      <td>45611.000</td>\n",
       "      <td>45339.000</td>\n",
       "      <td>45037.000</td>\n",
       "      <td>44714.000</td>\n",
       "      <td>44353.000</td>\n",
       "      <td>44009.000</td>\n",
       "      <td>43657.000</td>\n",
       "      <td>43314.000</td>\n",
       "      <td>42929.000</td>\n",
       "      <td>42508.000</td>\n",
       "      <td>42093.000</td>\n",
       "      <td>41659.000</td>\n",
       "      <td>41237.000</td>\n",
       "      <td>40826.000</td>\n",
       "      <td>40392.000</td>\n",
       "      <td>39922.000</td>\n",
       "      <td>39426.000</td>\n",
       "      <td>38899.000</td>\n",
       "      <td>38437.000</td>\n",
       "      <td>37935.000</td>\n",
       "      <td>37418.000</td>\n",
       "      <td>36939.000</td>\n",
       "      <td>36417.000</td>\n",
       "      <td>35913.000</td>\n",
       "      <td>35372.000</td>\n",
       "      <td>34824.000</td>\n",
       "      <td>34290.000</td>\n",
       "      <td>33739.000</td>\n",
       "      <td>33162.000</td>\n",
       "      <td>32587.000</td>\n",
       "      <td>32037.000</td>\n",
       "      <td>31435.000</td>\n",
       "      <td>30861.000</td>\n",
       "      <td>30298.000</td>\n",
       "      <td>29751.000</td>\n",
       "      <td>29196.000</td>\n",
       "      <td>28593.000</td>\n",
       "      <td>28022.000</td>\n",
       "      <td>27441.000</td>\n",
       "      <td>26875.000</td>\n",
       "      <td>26276.000</td>\n",
       "      <td>25731.000</td>\n",
       "      <td>25168.000</td>\n",
       "      <td>24625.000</td>\n",
       "      <td>24089.000</td>\n",
       "      <td>23560.000</td>\n",
       "      <td>22983.000</td>\n",
       "      <td>22420.000</td>\n",
       "      <td>21915.000</td>\n",
       "      <td>21414.000</td>\n",
       "      <td>20861.000</td>\n",
       "      <td>20337.000</td>\n",
       "      <td>19866.000</td>\n",
       "      <td>19357.000</td>\n",
       "      <td>18812.000</td>\n",
       "      <td>18312.000</td>\n",
       "      <td>17824.000</td>\n",
       "      <td>17372.000</td>\n",
       "      <td>16916.000</td>\n",
       "      <td>16427.000</td>\n",
       "      <td>15973.000</td>\n",
       "      <td>15529.000</td>\n",
       "      <td>15086.000</td>\n",
       "      <td>14656.000</td>\n",
       "      <td>14242.000</td>\n",
       "      <td>13836.000</td>\n",
       "      <td>13441.000</td>\n",
       "      <td>13031.000</td>\n",
       "      <td>12650.000</td>\n",
       "      <td>12264.000</td>\n",
       "      <td>11884.000</td>\n",
       "      <td>11508.000</td>\n",
       "      <td>11163.000</td>\n",
       "      <td>10813.000</td>\n",
       "      <td>10465.000</td>\n",
       "      <td>10126.000</td>\n",
       "      <td>9791.000</td>\n",
       "      <td>9460.000</td>\n",
       "      <td>9137.000</td>\n",
       "      <td>8842.000</td>\n",
       "      <td>8512.000</td>\n",
       "      <td>8217.000</td>\n",
       "      <td>7929.000</td>\n",
       "      <td>7644.000</td>\n",
       "      <td>7362.000</td>\n",
       "      <td>7087.000</td>\n",
       "      <td>6863.000</td>\n",
       "      <td>6614.000</td>\n",
       "      <td>6328.000</td>\n",
       "      <td>6076.000</td>\n",
       "      <td>5826.000</td>\n",
       "      <td>5606.000</td>\n",
       "      <td>5383.000</td>\n",
       "      <td>5167.000</td>\n",
       "      <td>4971.000</td>\n",
       "      <td>4765.000</td>\n",
       "      <td>4597.000</td>\n",
       "      <td>4430.000</td>\n",
       "      <td>4264.000</td>\n",
       "      <td>4085.000</td>\n",
       "      <td>3910.000</td>\n",
       "      <td>3758.000</td>\n",
       "      <td>3607.000</td>\n",
       "      <td>3443.000</td>\n",
       "      <td>3313.000</td>\n",
       "      <td>3155.000</td>\n",
       "      <td>3020.000</td>\n",
       "      <td>2892.000</td>\n",
       "      <td>2746.000</td>\n",
       "      <td>2641.000</td>\n",
       "      <td>2532.000</td>\n",
       "      <td>2412.000</td>\n",
       "      <td>2290.000</td>\n",
       "      <td>2169.000</td>\n",
       "      <td>2054.000</td>\n",
       "      <td>1954.000</td>\n",
       "      <td>1872.000</td>\n",
       "      <td>1772.000</td>\n",
       "      <td>1675.000</td>\n",
       "      <td>1599.000</td>\n",
       "      <td>1532.000</td>\n",
       "      <td>1449.000</td>\n",
       "      <td>1385.000</td>\n",
       "      <td>1327.000</td>\n",
       "      <td>1262.000</td>\n",
       "      <td>1198.000</td>\n",
       "      <td>1123.000</td>\n",
       "      <td>1062.000</td>\n",
       "      <td>1009.000</td>\n",
       "      <td>955.000</td>\n",
       "      <td>913.000</td>\n",
       "      <td>871.000</td>\n",
       "      <td>818.000</td>\n",
       "      <td>762.000</td>\n",
       "      <td>729.000</td>\n",
       "      <td>687.000</td>\n",
       "      <td>653.000</td>\n",
       "      <td>621.000</td>\n",
       "      <td>588.000</td>\n",
       "      <td>556.000</td>\n",
       "      <td>526.000</td>\n",
       "      <td>501.000</td>\n",
       "      <td>476.000</td>\n",
       "      <td>448.000</td>\n",
       "      <td>424.000</td>\n",
       "      <td>392.000</td>\n",
       "      <td>374.000</td>\n",
       "      <td>357.000</td>\n",
       "      <td>333.000</td>\n",
       "      <td>307.000</td>\n",
       "      <td>290.000</td>\n",
       "      <td>271.000</td>\n",
       "      <td>252.000</td>\n",
       "      <td>234.000</td>\n",
       "      <td>220.000</td>\n",
       "      <td>208.000</td>\n",
       "      <td>197.000</td>\n",
       "      <td>184.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>169.000</td>\n",
       "      <td>158.000</td>\n",
       "      <td>146.000</td>\n",
       "      <td>139.000</td>\n",
       "      <td>129.000</td>\n",
       "      <td>122.000</td>\n",
       "      <td>113.000</td>\n",
       "      <td>107.000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>78.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>54.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>45.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>22.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24999.500</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14433.901</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12499.750</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24999.500</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37499.250</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49999.000</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  \\\n",
       "count 50000.000         50000.000         50000.000         50000.000   \n",
       "mean  24999.500             0.253             0.180             0.149   \n",
       "std   14433.901             0.060             0.046             0.029   \n",
       "min       0.000             0.090             0.073             0.062   \n",
       "25%   12499.750             0.209             0.148             0.130   \n",
       "50%   24999.500             0.246             0.171             0.146   \n",
       "75%   37499.250             0.288             0.203             0.163   \n",
       "max   49999.000             0.626             0.537             0.420   \n",
       "\n",
       "       val_loss_epoch_4  val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  \\\n",
       "count         50000.000         50000.000         50000.000         50000.000   \n",
       "mean              0.140             0.134             0.129             0.124   \n",
       "std               0.024             0.022             0.022             0.021   \n",
       "min               0.063             0.056             0.056             0.056   \n",
       "25%               0.124             0.119             0.114             0.110   \n",
       "50%               0.138             0.133             0.128             0.123   \n",
       "75%               0.154             0.148             0.143             0.138   \n",
       "max               0.354             0.326             0.310             0.294   \n",
       "\n",
       "       val_loss_epoch_8  val_loss_epoch_9  val_loss_epoch_10  \\\n",
       "count         50000.000         50000.000          50000.000   \n",
       "mean              0.119             0.115              0.110   \n",
       "std               0.021             0.020              0.020   \n",
       "min               0.051             0.048              0.048   \n",
       "25%               0.105             0.100              0.096   \n",
       "50%               0.118             0.114              0.109   \n",
       "75%               0.133             0.128              0.123   \n",
       "max               0.280             0.264              0.258   \n",
       "\n",
       "       val_loss_epoch_11  val_loss_epoch_12  val_loss_epoch_13  \\\n",
       "count          50000.000          50000.000          50000.000   \n",
       "mean               0.105              0.101              0.097   \n",
       "std                0.020              0.019              0.019   \n",
       "min                0.044              0.043              0.040   \n",
       "25%                0.092              0.087              0.084   \n",
       "50%                0.104              0.100              0.096   \n",
       "75%                0.118              0.113              0.108   \n",
       "max                0.243              0.233              0.226   \n",
       "\n",
       "       val_loss_epoch_14  val_loss_epoch_15  val_loss_epoch_16  \\\n",
       "count          50000.000          50000.000          50000.000   \n",
       "mean               0.093              0.090              0.087   \n",
       "std                0.018              0.018              0.017   \n",
       "min                0.040              0.038              0.036   \n",
       "25%                0.080              0.077              0.074   \n",
       "50%                0.092              0.088              0.085   \n",
       "75%                0.104              0.100              0.097   \n",
       "max                0.218              0.211              0.208   \n",
       "\n",
       "       val_loss_epoch_17  val_loss_epoch_18  val_loss_epoch_19  \\\n",
       "count          50000.000          50000.000          50000.000   \n",
       "mean               0.084              0.081              0.079   \n",
       "std                0.016              0.016              0.015   \n",
       "min                0.035              0.034              0.034   \n",
       "25%                0.072              0.070              0.068   \n",
       "50%                0.082              0.080              0.078   \n",
       "75%                0.093              0.091              0.088   \n",
       "max                0.199              0.194              0.193   \n",
       "\n",
       "       val_loss_epoch_20  val_loss_epoch_21  val_loss_epoch_22  \\\n",
       "count          50000.000          50000.000          50000.000   \n",
       "mean               0.077              0.075              0.074   \n",
       "std                0.015              0.014              0.014   \n",
       "min                0.033              0.034              0.032   \n",
       "25%                0.067              0.065              0.064   \n",
       "50%                0.076              0.074              0.073   \n",
       "75%                0.086              0.084              0.082   \n",
       "max                0.187              0.180              0.184   \n",
       "\n",
       "       val_loss_epoch_23  val_loss_epoch_24  val_loss_epoch_25  \\\n",
       "count          50000.000          50000.000          50000.000   \n",
       "mean               0.072              0.071              0.070   \n",
       "std                0.013              0.013              0.012   \n",
       "min                0.030              0.032              0.030   \n",
       "25%                0.063              0.062              0.061   \n",
       "50%                0.071              0.070              0.069   \n",
       "75%                0.080              0.079              0.077   \n",
       "max                0.179              0.168              0.168   \n",
       "\n",
       "       val_loss_epoch_26  val_loss_epoch_27  val_loss_epoch_28  \\\n",
       "count          50000.000          50000.000          50000.000   \n",
       "mean               0.069              0.068              0.067   \n",
       "std                0.012              0.012              0.011   \n",
       "min                0.031              0.028              0.028   \n",
       "25%                0.061              0.060              0.059   \n",
       "50%                0.068              0.067              0.066   \n",
       "75%                0.076              0.075              0.074   \n",
       "max                0.174              0.161              0.156   \n",
       "\n",
       "       val_loss_epoch_29  val_loss_epoch_30  val_loss_epoch_31  \\\n",
       "count          50000.000          50000.000          50000.000   \n",
       "mean               0.066              0.066              0.065   \n",
       "std                0.011              0.011              0.011   \n",
       "min                0.028              0.030              0.027   \n",
       "25%                0.059              0.058              0.057   \n",
       "50%                0.066              0.065              0.064   \n",
       "75%                0.073              0.072              0.072   \n",
       "max                0.156              0.173              0.152   \n",
       "\n",
       "       val_loss_epoch_32  val_loss_epoch_33  val_loss_epoch_34  \\\n",
       "count          50000.000          50000.000          50000.000   \n",
       "mean               0.064              0.064              0.063   \n",
       "std                0.011              0.010              0.010   \n",
       "min                0.027              0.026              0.029   \n",
       "25%                0.057              0.057              0.056   \n",
       "50%                0.064              0.063              0.063   \n",
       "75%                0.071              0.070              0.070   \n",
       "max                0.150              0.147              0.146   \n",
       "\n",
       "       val_loss_epoch_35  val_loss_epoch_36  val_loss_epoch_37  \\\n",
       "count          50000.000          50000.000          50000.000   \n",
       "mean               0.063              0.062              0.062   \n",
       "std                0.010              0.010              0.010   \n",
       "min                0.029              0.026              0.027   \n",
       "25%                0.056              0.055              0.055   \n",
       "50%                0.062              0.062              0.061   \n",
       "75%                0.069              0.068              0.068   \n",
       "max                0.140              0.140              0.136   \n",
       "\n",
       "       val_loss_epoch_38  val_loss_epoch_39  val_loss_epoch_40  \\\n",
       "count          50000.000          50000.000          50000.000   \n",
       "mean               0.061              0.061              0.060   \n",
       "std                0.010              0.010              0.010   \n",
       "min                0.027              0.028              0.026   \n",
       "25%                0.054              0.054              0.054   \n",
       "50%                0.061              0.060              0.060   \n",
       "75%                0.067              0.067              0.066   \n",
       "max                0.135              0.134              0.133   \n",
       "\n",
       "       val_loss_epoch_41  val_loss_epoch_42  val_loss_epoch_43  \\\n",
       "count          49999.000          49999.000          49999.000   \n",
       "mean               0.060              0.059              0.059   \n",
       "std                0.009              0.009              0.009   \n",
       "min                0.025              0.025              0.026   \n",
       "25%                0.053              0.053              0.053   \n",
       "50%                0.059              0.059              0.059   \n",
       "75%                0.066              0.065              0.065   \n",
       "max                0.132              0.130              0.126   \n",
       "\n",
       "       val_loss_epoch_44  val_loss_epoch_45  val_loss_epoch_46  \\\n",
       "count          49998.000          49997.000          49995.000   \n",
       "mean               0.059              0.058              0.058   \n",
       "std                0.009              0.009              0.009   \n",
       "min                0.025              0.027              0.024   \n",
       "25%                0.052              0.052              0.052   \n",
       "50%                0.058              0.058              0.058   \n",
       "75%                0.065              0.064              0.064   \n",
       "max                0.125              0.128              0.127   \n",
       "\n",
       "       val_loss_epoch_47  val_loss_epoch_48  val_loss_epoch_49  \\\n",
       "count          49992.000          49988.000          49987.000   \n",
       "mean               0.058              0.057              0.057   \n",
       "std                0.009              0.009              0.009   \n",
       "min                0.024              0.024              0.024   \n",
       "25%                0.052              0.051              0.051   \n",
       "50%                0.057              0.057              0.057   \n",
       "75%                0.063              0.063              0.063   \n",
       "max                0.120              0.120              0.119   \n",
       "\n",
       "       val_loss_epoch_50  val_loss_epoch_51  val_loss_epoch_52  \\\n",
       "count          49985.000          49977.000          49972.000   \n",
       "mean               0.057              0.056              0.056   \n",
       "std                0.009              0.009              0.009   \n",
       "min                0.024              0.023              0.024   \n",
       "25%                0.051              0.050              0.050   \n",
       "50%                0.056              0.056              0.056   \n",
       "75%                0.062              0.062              0.062   \n",
       "max                0.117              0.115              0.119   \n",
       "\n",
       "       val_loss_epoch_53  val_loss_epoch_54  val_loss_epoch_55  \\\n",
       "count          49963.000          49954.000          49947.000   \n",
       "mean               0.056              0.056              0.055   \n",
       "std                0.009              0.008              0.008   \n",
       "min                0.026              0.024              0.024   \n",
       "25%                0.050              0.050              0.050   \n",
       "50%                0.056              0.055              0.055   \n",
       "75%                0.061              0.061              0.061   \n",
       "max                0.125              0.112              0.113   \n",
       "\n",
       "       val_loss_epoch_56  val_loss_epoch_57  val_loss_epoch_58  \\\n",
       "count          49928.000          49915.000          49892.000   \n",
       "mean               0.055              0.055              0.055   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.023              0.023              0.023   \n",
       "25%                0.049              0.049              0.049   \n",
       "50%                0.055              0.055              0.054   \n",
       "75%                0.061              0.060              0.060   \n",
       "max                0.112              0.119              0.110   \n",
       "\n",
       "       val_loss_epoch_59  val_loss_epoch_60  val_loss_epoch_61  \\\n",
       "count          49877.000          49863.000          49844.000   \n",
       "mean               0.054              0.054              0.054   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.024              0.023              0.023   \n",
       "25%                0.049              0.048              0.048   \n",
       "50%                0.054              0.054              0.054   \n",
       "75%                0.060              0.059              0.059   \n",
       "max                0.109              0.106              0.111   \n",
       "\n",
       "       val_loss_epoch_62  val_loss_epoch_63  val_loss_epoch_64  \\\n",
       "count          49815.000          49786.000          49745.000   \n",
       "mean               0.054              0.053              0.053   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.024              0.024              0.023   \n",
       "25%                0.048              0.048              0.048   \n",
       "50%                0.053              0.053              0.053   \n",
       "75%                0.059              0.059              0.059   \n",
       "max                0.104              0.105              0.104   \n",
       "\n",
       "       val_loss_epoch_65  val_loss_epoch_66  val_loss_epoch_67  \\\n",
       "count          49715.000          49684.000          49639.000   \n",
       "mean               0.053              0.053              0.053   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.023              0.023              0.024   \n",
       "25%                0.048              0.047              0.047   \n",
       "50%                0.053              0.053              0.052   \n",
       "75%                0.058              0.058              0.058   \n",
       "max                0.101              0.113              0.109   \n",
       "\n",
       "       val_loss_epoch_68  val_loss_epoch_69  val_loss_epoch_70  \\\n",
       "count          49590.000          49533.000          49489.000   \n",
       "mean               0.053              0.052              0.052   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.025              0.023              0.023   \n",
       "25%                0.047              0.047              0.047   \n",
       "50%                0.052              0.052              0.052   \n",
       "75%                0.058              0.057              0.057   \n",
       "max                0.101              0.110              0.100   \n",
       "\n",
       "       val_loss_epoch_71  val_loss_epoch_72  val_loss_epoch_73  \\\n",
       "count          49419.000          49345.000          49250.000   \n",
       "mean               0.052              0.052              0.052   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.024              0.022              0.022   \n",
       "25%                0.047              0.046              0.046   \n",
       "50%                0.052              0.052              0.051   \n",
       "75%                0.057              0.057              0.057   \n",
       "max                0.099              0.098              0.100   \n",
       "\n",
       "       val_loss_epoch_74  val_loss_epoch_75  val_loss_epoch_76  \\\n",
       "count          49171.000          49086.000          48990.000   \n",
       "mean               0.051              0.051              0.051   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.023              0.023              0.024   \n",
       "25%                0.046              0.046              0.046   \n",
       "50%                0.051              0.051              0.051   \n",
       "75%                0.057              0.056              0.056   \n",
       "max                0.098              0.096              0.099   \n",
       "\n",
       "       val_loss_epoch_77  val_loss_epoch_78  val_loss_epoch_79  \\\n",
       "count          48876.000          48734.000          48604.000   \n",
       "mean               0.051              0.051              0.051   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.023              0.023              0.022   \n",
       "25%                0.046              0.046              0.045   \n",
       "50%                0.051              0.051              0.050   \n",
       "75%                0.056              0.056              0.056   \n",
       "max                0.096              0.095              0.097   \n",
       "\n",
       "       val_loss_epoch_80  val_loss_epoch_81  val_loss_epoch_82  \\\n",
       "count          48450.000          48292.000          48129.000   \n",
       "mean               0.051              0.050              0.050   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.023              0.024              0.023   \n",
       "25%                0.045              0.045              0.045   \n",
       "50%                0.050              0.050              0.050   \n",
       "75%                0.056              0.055              0.055   \n",
       "max                0.102              0.093              0.102   \n",
       "\n",
       "       val_loss_epoch_83  val_loss_epoch_84  val_loss_epoch_85  \\\n",
       "count          47962.000          47784.000          47623.000   \n",
       "mean               0.050              0.050              0.050   \n",
       "std                0.008              0.008              0.007   \n",
       "min                0.024              0.023              0.024   \n",
       "25%                0.045              0.045              0.045   \n",
       "50%                0.050              0.050              0.050   \n",
       "75%                0.055              0.055              0.055   \n",
       "max                0.096              0.097              0.107   \n",
       "\n",
       "       val_loss_epoch_86  val_loss_epoch_87  val_loss_epoch_88  \\\n",
       "count          47424.000          47195.000          46965.000   \n",
       "mean               0.050              0.050              0.049   \n",
       "std                0.007              0.007              0.007   \n",
       "min                0.023              0.023              0.022   \n",
       "25%                0.045              0.044              0.044   \n",
       "50%                0.050              0.049              0.049   \n",
       "75%                0.055              0.055              0.054   \n",
       "max                0.092              0.092              0.093   \n",
       "\n",
       "       val_loss_epoch_89  val_loss_epoch_90  val_loss_epoch_91  \\\n",
       "count          46709.000          46440.000          46166.000   \n",
       "mean               0.049              0.049              0.049   \n",
       "std                0.007              0.007              0.007   \n",
       "min                0.023              0.024              0.024   \n",
       "25%                0.044              0.044              0.044   \n",
       "50%                0.049              0.049              0.049   \n",
       "75%                0.054              0.054              0.054   \n",
       "max                0.091              0.092              0.095   \n",
       "\n",
       "       val_loss_epoch_92  val_loss_epoch_93  val_loss_epoch_94  \\\n",
       "count          45899.000          45611.000          45339.000   \n",
       "mean               0.049              0.049              0.049   \n",
       "std                0.007              0.007              0.007   \n",
       "min                0.023              0.023              0.022   \n",
       "25%                0.044              0.044              0.044   \n",
       "50%                0.049              0.049              0.049   \n",
       "75%                0.054              0.054              0.054   \n",
       "max                0.091              0.091              0.090   \n",
       "\n",
       "       val_loss_epoch_95  val_loss_epoch_96  val_loss_epoch_97  \\\n",
       "count          45037.000          44714.000          44353.000   \n",
       "mean               0.049              0.049              0.048   \n",
       "std                0.007              0.007              0.007   \n",
       "min                0.024              0.022              0.023   \n",
       "25%                0.044              0.044              0.043   \n",
       "50%                0.049              0.048              0.048   \n",
       "75%                0.053              0.053              0.053   \n",
       "max                0.089              0.090              0.099   \n",
       "\n",
       "       val_loss_epoch_98  val_loss_epoch_99  val_loss_epoch_100  \\\n",
       "count          44009.000          43657.000           43314.000   \n",
       "mean               0.048              0.048               0.048   \n",
       "std                0.007              0.007               0.007   \n",
       "min                0.023              0.022               0.023   \n",
       "25%                0.043              0.043               0.043   \n",
       "50%                0.048              0.048               0.048   \n",
       "75%                0.053              0.053               0.053   \n",
       "max                0.091              0.091               0.090   \n",
       "\n",
       "       val_loss_epoch_101  val_loss_epoch_102  val_loss_epoch_103  \\\n",
       "count           42929.000           42508.000           42093.000   \n",
       "mean                0.048               0.048               0.048   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.023               0.023   \n",
       "25%                 0.043               0.043               0.043   \n",
       "50%                 0.048               0.048               0.048   \n",
       "75%                 0.053               0.053               0.053   \n",
       "max                 0.090               0.092               0.098   \n",
       "\n",
       "       val_loss_epoch_104  val_loss_epoch_105  val_loss_epoch_106  \\\n",
       "count           41659.000           41237.000           40826.000   \n",
       "mean                0.048               0.048               0.048   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.023               0.022               0.022   \n",
       "25%                 0.043               0.043               0.043   \n",
       "50%                 0.048               0.048               0.047   \n",
       "75%                 0.053               0.052               0.052   \n",
       "max                 0.088               0.094               0.095   \n",
       "\n",
       "       val_loss_epoch_107  val_loss_epoch_108  val_loss_epoch_109  \\\n",
       "count           40392.000           39922.000           39426.000   \n",
       "mean                0.047               0.047               0.047   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.023               0.023   \n",
       "25%                 0.043               0.042               0.042   \n",
       "50%                 0.047               0.047               0.047   \n",
       "75%                 0.052               0.052               0.052   \n",
       "max                 0.088               0.091               0.090   \n",
       "\n",
       "       val_loss_epoch_110  val_loss_epoch_111  val_loss_epoch_112  \\\n",
       "count           38899.000           38437.000           37935.000   \n",
       "mean                0.047               0.047               0.047   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.023               0.022               0.023   \n",
       "25%                 0.042               0.042               0.042   \n",
       "50%                 0.047               0.047               0.047   \n",
       "75%                 0.052               0.052               0.052   \n",
       "max                 0.087               0.086               0.085   \n",
       "\n",
       "       val_loss_epoch_113  val_loss_epoch_114  val_loss_epoch_115  \\\n",
       "count           37418.000           36939.000           36417.000   \n",
       "mean                0.047               0.047               0.047   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.023               0.022               0.022   \n",
       "25%                 0.042               0.042               0.042   \n",
       "50%                 0.047               0.047               0.047   \n",
       "75%                 0.052               0.052               0.051   \n",
       "max                 0.090               0.090               0.085   \n",
       "\n",
       "       val_loss_epoch_116  val_loss_epoch_117  val_loss_epoch_118  \\\n",
       "count           35913.000           35372.000           34824.000   \n",
       "mean                0.047               0.047               0.046   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.042               0.042               0.042   \n",
       "50%                 0.046               0.046               0.046   \n",
       "75%                 0.051               0.051               0.051   \n",
       "max                 0.088               0.085               0.084   \n",
       "\n",
       "       val_loss_epoch_119  val_loss_epoch_120  val_loss_epoch_121  \\\n",
       "count           34290.000           33739.000           33162.000   \n",
       "mean                0.046               0.046               0.046   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.042               0.041               0.041   \n",
       "50%                 0.046               0.046               0.046   \n",
       "75%                 0.051               0.051               0.051   \n",
       "max                 0.086               0.083               0.092   \n",
       "\n",
       "       val_loss_epoch_122  val_loss_epoch_123  val_loss_epoch_124  \\\n",
       "count           32587.000           32037.000           31435.000   \n",
       "mean                0.046               0.046               0.046   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.041               0.041               0.041   \n",
       "50%                 0.046               0.046               0.046   \n",
       "75%                 0.051               0.051               0.051   \n",
       "max                 0.086               0.087               0.094   \n",
       "\n",
       "       val_loss_epoch_125  val_loss_epoch_126  val_loss_epoch_127  \\\n",
       "count           30861.000           30298.000           29751.000   \n",
       "mean                0.046               0.046               0.046   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.021               0.021   \n",
       "25%                 0.041               0.041               0.041   \n",
       "50%                 0.046               0.046               0.046   \n",
       "75%                 0.051               0.050               0.050   \n",
       "max                 0.086               0.083               0.083   \n",
       "\n",
       "       val_loss_epoch_128  val_loss_epoch_129  val_loss_epoch_130  \\\n",
       "count           29196.000           28593.000           28022.000   \n",
       "mean                0.046               0.046               0.046   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.021               0.021   \n",
       "25%                 0.041               0.041               0.041   \n",
       "50%                 0.046               0.045               0.045   \n",
       "75%                 0.050               0.050               0.050   \n",
       "max                 0.086               0.083               0.082   \n",
       "\n",
       "       val_loss_epoch_131  val_loss_epoch_132  val_loss_epoch_133  \\\n",
       "count           27441.000           26875.000           26276.000   \n",
       "mean                0.045               0.045               0.045   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.022               0.021   \n",
       "25%                 0.041               0.040               0.040   \n",
       "50%                 0.045               0.045               0.045   \n",
       "75%                 0.050               0.050               0.050   \n",
       "max                 0.081               0.082               0.082   \n",
       "\n",
       "       val_loss_epoch_134  val_loss_epoch_135  val_loss_epoch_136  \\\n",
       "count           25731.000           25168.000           24625.000   \n",
       "mean                0.045               0.045               0.045   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.021               0.020   \n",
       "25%                 0.040               0.040               0.040   \n",
       "50%                 0.045               0.045               0.045   \n",
       "75%                 0.050               0.050               0.050   \n",
       "max                 0.082               0.081               0.081   \n",
       "\n",
       "       val_loss_epoch_137  val_loss_epoch_138  val_loss_epoch_139  \\\n",
       "count           24089.000           23560.000           22983.000   \n",
       "mean                0.045               0.045               0.045   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.020               0.021               0.021   \n",
       "25%                 0.040               0.040               0.040   \n",
       "50%                 0.045               0.045               0.045   \n",
       "75%                 0.050               0.050               0.049   \n",
       "max                 0.082               0.081               0.081   \n",
       "\n",
       "       val_loss_epoch_140  val_loss_epoch_141  val_loss_epoch_142  \\\n",
       "count           22420.000           21915.000           21414.000   \n",
       "mean                0.045               0.045               0.045   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.021               0.022   \n",
       "25%                 0.040               0.040               0.040   \n",
       "50%                 0.045               0.045               0.045   \n",
       "75%                 0.049               0.049               0.049   \n",
       "max                 0.083               0.080               0.080   \n",
       "\n",
       "       val_loss_epoch_143  val_loss_epoch_144  val_loss_epoch_145  \\\n",
       "count           20861.000           20337.000           19866.000   \n",
       "mean                0.045               0.045               0.044   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.020               0.021               0.020   \n",
       "25%                 0.040               0.040               0.040   \n",
       "50%                 0.044               0.044               0.044   \n",
       "75%                 0.049               0.049               0.049   \n",
       "max                 0.082               0.080               0.083   \n",
       "\n",
       "       val_loss_epoch_146  val_loss_epoch_147  val_loss_epoch_148  \\\n",
       "count           19357.000           18812.000           18312.000   \n",
       "mean                0.044               0.044               0.044   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.020               0.021               0.020   \n",
       "25%                 0.040               0.039               0.039   \n",
       "50%                 0.044               0.044               0.044   \n",
       "75%                 0.049               0.049               0.049   \n",
       "max                 0.083               0.088               0.080   \n",
       "\n",
       "       val_loss_epoch_149  val_loss_epoch_150  val_loss_epoch_151  \\\n",
       "count           17824.000           17372.000           16916.000   \n",
       "mean                0.044               0.044               0.044   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.020               0.020               0.020   \n",
       "25%                 0.039               0.039               0.039   \n",
       "50%                 0.044               0.044               0.044   \n",
       "75%                 0.049               0.049               0.049   \n",
       "max                 0.091               0.083               0.080   \n",
       "\n",
       "       val_loss_epoch_152  val_loss_epoch_153  val_loss_epoch_154  \\\n",
       "count           16427.000           15973.000           15529.000   \n",
       "mean                0.044               0.044               0.044   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.021               0.019   \n",
       "25%                 0.039               0.039               0.039   \n",
       "50%                 0.044               0.044               0.044   \n",
       "75%                 0.049               0.048               0.048   \n",
       "max                 0.080               0.078               0.079   \n",
       "\n",
       "       val_loss_epoch_155  val_loss_epoch_156  val_loss_epoch_157  \\\n",
       "count           15086.000           14656.000           14242.000   \n",
       "mean                0.044               0.044               0.044   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.019               0.020               0.020   \n",
       "25%                 0.039               0.039               0.039   \n",
       "50%                 0.044               0.044               0.043   \n",
       "75%                 0.048               0.048               0.048   \n",
       "max                 0.082               0.092               0.079   \n",
       "\n",
       "       val_loss_epoch_158  val_loss_epoch_159  val_loss_epoch_160  \\\n",
       "count           13836.000           13441.000           13031.000   \n",
       "mean                0.044               0.044               0.043   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.020               0.019               0.020   \n",
       "25%                 0.039               0.039               0.039   \n",
       "50%                 0.043               0.043               0.043   \n",
       "75%                 0.048               0.048               0.048   \n",
       "max                 0.079               0.078               0.079   \n",
       "\n",
       "       val_loss_epoch_161  val_loss_epoch_162  val_loss_epoch_163  \\\n",
       "count           12650.000           12264.000           11884.000   \n",
       "mean                0.043               0.043               0.043   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.020               0.019               0.019   \n",
       "25%                 0.039               0.039               0.039   \n",
       "50%                 0.043               0.043               0.043   \n",
       "75%                 0.048               0.048               0.048   \n",
       "max                 0.081               0.083               0.078   \n",
       "\n",
       "       val_loss_epoch_164  val_loss_epoch_165  val_loss_epoch_166  \\\n",
       "count           11508.000           11163.000           10813.000   \n",
       "mean                0.043               0.043               0.043   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.019               0.020               0.020   \n",
       "25%                 0.038               0.038               0.038   \n",
       "50%                 0.043               0.043               0.043   \n",
       "75%                 0.048               0.048               0.048   \n",
       "max                 0.070               0.073               0.071   \n",
       "\n",
       "       val_loss_epoch_167  val_loss_epoch_168  val_loss_epoch_169  \\\n",
       "count           10465.000           10126.000            9791.000   \n",
       "mean                0.043               0.043               0.043   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.020               0.019               0.019   \n",
       "25%                 0.038               0.038               0.038   \n",
       "50%                 0.043               0.043               0.043   \n",
       "75%                 0.048               0.048               0.047   \n",
       "max                 0.071               0.071               0.072   \n",
       "\n",
       "       val_loss_epoch_170  val_loss_epoch_171  val_loss_epoch_172  \\\n",
       "count            9460.000            9137.000            8842.000   \n",
       "mean                0.043               0.043               0.043   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.019               0.019               0.019   \n",
       "25%                 0.038               0.038               0.038   \n",
       "50%                 0.043               0.043               0.043   \n",
       "75%                 0.047               0.047               0.047   \n",
       "max                 0.070               0.068               0.073   \n",
       "\n",
       "       val_loss_epoch_173  val_loss_epoch_174  val_loss_epoch_175  \\\n",
       "count            8512.000            8217.000            7929.000   \n",
       "mean                0.043               0.043               0.043   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.019               0.020               0.019   \n",
       "25%                 0.038               0.038               0.038   \n",
       "50%                 0.043               0.043               0.043   \n",
       "75%                 0.047               0.047               0.047   \n",
       "max                 0.072               0.067               0.066   \n",
       "\n",
       "       val_loss_epoch_176  val_loss_epoch_177  val_loss_epoch_178  \\\n",
       "count            7644.000            7362.000            7087.000   \n",
       "mean                0.043               0.043               0.042   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.020               0.020               0.019   \n",
       "25%                 0.038               0.038               0.038   \n",
       "50%                 0.042               0.042               0.042   \n",
       "75%                 0.047               0.047               0.047   \n",
       "max                 0.071               0.068               0.067   \n",
       "\n",
       "       val_loss_epoch_179  val_loss_epoch_180  val_loss_epoch_181  \\\n",
       "count            6863.000            6614.000            6328.000   \n",
       "mean                0.042               0.042               0.042   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.020               0.020   \n",
       "25%                 0.038               0.038               0.038   \n",
       "50%                 0.042               0.042               0.042   \n",
       "75%                 0.047               0.047               0.047   \n",
       "max                 0.068               0.070               0.072   \n",
       "\n",
       "       val_loss_epoch_182  val_loss_epoch_183  val_loss_epoch_184  \\\n",
       "count            6076.000            5826.000            5606.000   \n",
       "mean                0.042               0.042               0.042   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.020               0.020               0.020   \n",
       "25%                 0.038               0.038               0.038   \n",
       "50%                 0.042               0.042               0.042   \n",
       "75%                 0.047               0.047               0.047   \n",
       "max                 0.068               0.073               0.068   \n",
       "\n",
       "       val_loss_epoch_185  val_loss_epoch_186  val_loss_epoch_187  \\\n",
       "count            5383.000            5167.000            4971.000   \n",
       "mean                0.042               0.042               0.042   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.020               0.021   \n",
       "25%                 0.037               0.038               0.037   \n",
       "50%                 0.042               0.042               0.042   \n",
       "75%                 0.047               0.047               0.047   \n",
       "max                 0.066               0.065               0.067   \n",
       "\n",
       "       val_loss_epoch_188  val_loss_epoch_189  val_loss_epoch_190  \\\n",
       "count            4765.000            4597.000            4430.000   \n",
       "mean                0.042               0.042               0.042   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.021               0.021   \n",
       "25%                 0.037               0.037               0.037   \n",
       "50%                 0.042               0.042               0.042   \n",
       "75%                 0.047               0.046               0.046   \n",
       "max                 0.069               0.065               0.069   \n",
       "\n",
       "       val_loss_epoch_191  val_loss_epoch_192  val_loss_epoch_193  \\\n",
       "count            4264.000            4085.000            3910.000   \n",
       "mean                0.042               0.042               0.042   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.021               0.022   \n",
       "25%                 0.037               0.037               0.037   \n",
       "50%                 0.042               0.042               0.042   \n",
       "75%                 0.046               0.046               0.046   \n",
       "max                 0.065               0.065               0.067   \n",
       "\n",
       "       val_loss_epoch_194  val_loss_epoch_195  val_loss_epoch_196  \\\n",
       "count            3758.000            3607.000            3443.000   \n",
       "mean                0.042               0.042               0.042   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.021               0.022   \n",
       "25%                 0.037               0.037               0.037   \n",
       "50%                 0.042               0.041               0.041   \n",
       "75%                 0.046               0.046               0.046   \n",
       "max                 0.063               0.066               0.063   \n",
       "\n",
       "       val_loss_epoch_197  val_loss_epoch_198  val_loss_epoch_199  \\\n",
       "count            3313.000            3155.000            3020.000   \n",
       "mean                0.042               0.042               0.042   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.037               0.037               0.037   \n",
       "50%                 0.041               0.041               0.041   \n",
       "75%                 0.046               0.046               0.046   \n",
       "max                 0.067               0.064               0.063   \n",
       "\n",
       "       val_loss_epoch_200  val_loss_epoch_201  val_loss_epoch_202  \\\n",
       "count            2892.000            2746.000            2641.000   \n",
       "mean                0.041               0.041               0.041   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.037               0.037               0.037   \n",
       "50%                 0.041               0.041               0.041   \n",
       "75%                 0.046               0.046               0.046   \n",
       "max                 0.067               0.069               0.066   \n",
       "\n",
       "       val_loss_epoch_203  val_loss_epoch_204  val_loss_epoch_205  \\\n",
       "count            2532.000            2412.000            2290.000   \n",
       "mean                0.041               0.041               0.041   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.023   \n",
       "25%                 0.037               0.037               0.037   \n",
       "50%                 0.041               0.041               0.041   \n",
       "75%                 0.046               0.046               0.046   \n",
       "max                 0.066               0.067               0.068   \n",
       "\n",
       "       val_loss_epoch_206  val_loss_epoch_207  val_loss_epoch_208  \\\n",
       "count            2169.000            2054.000            1954.000   \n",
       "mean                0.041               0.041               0.041   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.022               0.022   \n",
       "25%                 0.037               0.036               0.036   \n",
       "50%                 0.041               0.041               0.041   \n",
       "75%                 0.046               0.046               0.046   \n",
       "max                 0.067               0.068               0.061   \n",
       "\n",
       "       val_loss_epoch_209  val_loss_epoch_210  val_loss_epoch_211  \\\n",
       "count            1872.000            1772.000            1675.000   \n",
       "mean                0.041               0.041               0.041   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.037               0.036               0.036   \n",
       "50%                 0.041               0.041               0.041   \n",
       "75%                 0.046               0.046               0.045   \n",
       "max                 0.067               0.066               0.067   \n",
       "\n",
       "       val_loss_epoch_212  val_loss_epoch_213  val_loss_epoch_214  \\\n",
       "count            1599.000            1532.000            1449.000   \n",
       "mean                0.041               0.041               0.041   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.036               0.036               0.036   \n",
       "50%                 0.041               0.041               0.041   \n",
       "75%                 0.046               0.045               0.045   \n",
       "max                 0.060               0.061               0.064   \n",
       "\n",
       "       val_loss_epoch_215  val_loss_epoch_216  val_loss_epoch_217  \\\n",
       "count            1385.000            1327.000            1262.000   \n",
       "mean                0.041               0.041               0.041   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.036               0.036               0.036   \n",
       "50%                 0.041               0.041               0.040   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.062               0.063               0.063   \n",
       "\n",
       "       val_loss_epoch_218  val_loss_epoch_219  val_loss_epoch_220  \\\n",
       "count            1198.000            1123.000            1062.000   \n",
       "mean                0.041               0.041               0.041   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.036               0.036               0.036   \n",
       "50%                 0.041               0.040               0.041   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.060               0.064               0.060   \n",
       "\n",
       "       val_loss_epoch_221  val_loss_epoch_222  val_loss_epoch_223  \\\n",
       "count            1009.000             955.000             913.000   \n",
       "mean                0.041               0.041               0.041   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.023   \n",
       "25%                 0.036               0.036               0.036   \n",
       "50%                 0.040               0.040               0.040   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.058               0.059               0.062   \n",
       "\n",
       "       val_loss_epoch_224  val_loss_epoch_225  val_loss_epoch_226  \\\n",
       "count             871.000             818.000             762.000   \n",
       "mean                0.040               0.041               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.023               0.022   \n",
       "25%                 0.036               0.036               0.036   \n",
       "50%                 0.040               0.040               0.040   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.060               0.059               0.061   \n",
       "\n",
       "       val_loss_epoch_227  val_loss_epoch_228  val_loss_epoch_229  \\\n",
       "count             729.000             687.000             653.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.023               0.023               0.022   \n",
       "25%                 0.035               0.036               0.035   \n",
       "50%                 0.040               0.040               0.040   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.061               0.059               0.058   \n",
       "\n",
       "       val_loss_epoch_230  val_loss_epoch_231  val_loss_epoch_232  \\\n",
       "count             621.000             588.000             556.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.023               0.022   \n",
       "25%                 0.035               0.035               0.035   \n",
       "50%                 0.040               0.040               0.040   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.057               0.062               0.060   \n",
       "\n",
       "       val_loss_epoch_233  val_loss_epoch_234  val_loss_epoch_235  \\\n",
       "count             526.000             501.000             476.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.021   \n",
       "25%                 0.035               0.035               0.035   \n",
       "50%                 0.040               0.040               0.040   \n",
       "75%                 0.045               0.044               0.045   \n",
       "max                 0.057               0.059               0.061   \n",
       "\n",
       "       val_loss_epoch_236  val_loss_epoch_237  val_loss_epoch_238  \\\n",
       "count             448.000             424.000             392.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.023               0.022   \n",
       "25%                 0.035               0.035               0.035   \n",
       "50%                 0.040               0.039               0.040   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.059               0.059               0.057   \n",
       "\n",
       "       val_loss_epoch_239  val_loss_epoch_240  val_loss_epoch_241  \\\n",
       "count             374.000             357.000             333.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.035               0.035               0.035   \n",
       "50%                 0.040               0.040               0.040   \n",
       "75%                 0.044               0.045               0.045   \n",
       "max                 0.058               0.057               0.057   \n",
       "\n",
       "       val_loss_epoch_242  val_loss_epoch_243  val_loss_epoch_244  \\\n",
       "count             307.000             290.000             271.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.022   \n",
       "25%                 0.035               0.035               0.035   \n",
       "50%                 0.039               0.039               0.039   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.056               0.058               0.062   \n",
       "\n",
       "       val_loss_epoch_245  val_loss_epoch_246  val_loss_epoch_247  \\\n",
       "count             252.000             234.000             220.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.023               0.023   \n",
       "25%                 0.035               0.035               0.034   \n",
       "50%                 0.039               0.040               0.040   \n",
       "75%                 0.044               0.045               0.045   \n",
       "max                 0.061               0.057               0.057   \n",
       "\n",
       "       val_loss_epoch_248  val_loss_epoch_249  val_loss_epoch_250  \\\n",
       "count             208.000             197.000             184.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.021               0.021   \n",
       "25%                 0.035               0.035               0.035   \n",
       "50%                 0.040               0.039               0.039   \n",
       "75%                 0.045               0.044               0.045   \n",
       "max                 0.059               0.056               0.057   \n",
       "\n",
       "       val_loss_epoch_251  val_loss_epoch_252  val_loss_epoch_253  \\\n",
       "count             178.000             169.000             158.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.021               0.021               0.021   \n",
       "25%                 0.035               0.035               0.035   \n",
       "50%                 0.039               0.040               0.039   \n",
       "75%                 0.044               0.045               0.044   \n",
       "max                 0.058               0.056               0.059   \n",
       "\n",
       "       val_loss_epoch_254  val_loss_epoch_255  val_loss_epoch_256  \\\n",
       "count             146.000             139.000             129.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.022               0.022               0.026   \n",
       "25%                 0.035               0.035               0.035   \n",
       "50%                 0.040               0.039               0.039   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.059               0.056               0.059   \n",
       "\n",
       "       val_loss_epoch_257  val_loss_epoch_258  val_loss_epoch_259  \\\n",
       "count             122.000             113.000             107.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.026               0.026               0.026   \n",
       "25%                 0.034               0.035               0.035   \n",
       "50%                 0.040               0.040               0.039   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.057               0.055               0.057   \n",
       "\n",
       "       val_loss_epoch_260  val_loss_epoch_261  val_loss_epoch_262  \\\n",
       "count              98.000              95.000              88.000   \n",
       "mean                0.041               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.027               0.025               0.026   \n",
       "25%                 0.036               0.035               0.036   \n",
       "50%                 0.041               0.040               0.040   \n",
       "75%                 0.046               0.045               0.045   \n",
       "max                 0.061               0.055               0.054   \n",
       "\n",
       "       val_loss_epoch_263  val_loss_epoch_264  val_loss_epoch_265  \\\n",
       "count              78.000              72.000              69.000   \n",
       "mean                0.041               0.041               0.041   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.026               0.027               0.027   \n",
       "25%                 0.036               0.036               0.036   \n",
       "50%                 0.041               0.040               0.041   \n",
       "75%                 0.045               0.046               0.046   \n",
       "max                 0.057               0.055               0.056   \n",
       "\n",
       "       val_loss_epoch_266  val_loss_epoch_267  val_loss_epoch_268  \\\n",
       "count              66.000              62.000              61.000   \n",
       "mean                0.040               0.041               0.041   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.026               0.028               0.027   \n",
       "25%                 0.036               0.036               0.036   \n",
       "50%                 0.040               0.040               0.041   \n",
       "75%                 0.045               0.046               0.045   \n",
       "max                 0.055               0.055               0.055   \n",
       "\n",
       "       val_loss_epoch_269  val_loss_epoch_270  val_loss_epoch_271  \\\n",
       "count              54.000              50.000              47.000   \n",
       "mean                0.040               0.040               0.040   \n",
       "std                 0.007               0.007               0.007   \n",
       "min                 0.026               0.026               0.026   \n",
       "25%                 0.036               0.036               0.036   \n",
       "50%                 0.039               0.040               0.040   \n",
       "75%                 0.044               0.044               0.044   \n",
       "max                 0.054               0.057               0.056   \n",
       "\n",
       "       val_loss_epoch_272  val_loss_epoch_273  val_loss_epoch_274  \\\n",
       "count              45.000              38.000              34.000   \n",
       "mean                0.040               0.041               0.041   \n",
       "std                 0.007               0.007               0.006   \n",
       "min                 0.026               0.026               0.030   \n",
       "25%                 0.036               0.037               0.036   \n",
       "50%                 0.041               0.040               0.041   \n",
       "75%                 0.045               0.045               0.045   \n",
       "max                 0.054               0.059               0.053   \n",
       "\n",
       "       val_loss_epoch_275  val_loss_epoch_276  val_loss_epoch_277  \\\n",
       "count              33.000              30.000              26.000   \n",
       "mean                0.041               0.040               0.041   \n",
       "std                 0.006               0.006               0.006   \n",
       "min                 0.030               0.031               0.031   \n",
       "25%                 0.036               0.036               0.036   \n",
       "50%                 0.040               0.039               0.040   \n",
       "75%                 0.045               0.044               0.047   \n",
       "max                 0.053               0.054               0.054   \n",
       "\n",
       "       val_loss_epoch_278  val_loss_epoch_279  val_loss_epoch_280  \\\n",
       "count              26.000              25.000              25.000   \n",
       "mean                0.041               0.040               0.041   \n",
       "std                 0.007               0.006               0.008   \n",
       "min                 0.031               0.031               0.031   \n",
       "25%                 0.036               0.036               0.036   \n",
       "50%                 0.040               0.039               0.040   \n",
       "75%                 0.045               0.044               0.043   \n",
       "max                 0.057               0.052               0.063   \n",
       "\n",
       "       val_loss_epoch_281  val_loss_epoch_282  val_loss_epoch_283  \\\n",
       "count              22.000              20.000              18.000   \n",
       "mean                0.041               0.040               0.041   \n",
       "std                 0.006               0.006               0.006   \n",
       "min                 0.032               0.033               0.031   \n",
       "25%                 0.036               0.036               0.037   \n",
       "50%                 0.039               0.039               0.039   \n",
       "75%                 0.045               0.043               0.044   \n",
       "max                 0.056               0.052               0.053   \n",
       "\n",
       "       val_loss_epoch_284  val_loss_epoch_285  val_loss_epoch_286  \\\n",
       "count              17.000              17.000              16.000   \n",
       "mean                0.040               0.041               0.041   \n",
       "std                 0.006               0.006               0.006   \n",
       "min                 0.031               0.031               0.030   \n",
       "25%                 0.037               0.036               0.036   \n",
       "50%                 0.039               0.040               0.040   \n",
       "75%                 0.043               0.044               0.045   \n",
       "max                 0.053               0.053               0.052   \n",
       "\n",
       "       val_loss_epoch_287  val_loss_epoch_288  val_loss_epoch_289  \\\n",
       "count              14.000              14.000              13.000   \n",
       "mean                0.041               0.040               0.041   \n",
       "std                 0.007               0.006               0.007   \n",
       "min                 0.031               0.031               0.032   \n",
       "25%                 0.036               0.035               0.036   \n",
       "50%                 0.041               0.039               0.038   \n",
       "75%                 0.044               0.044               0.045   \n",
       "max                 0.057               0.052               0.053   \n",
       "\n",
       "       val_loss_epoch_290  val_loss_epoch_291  val_loss_epoch_292  \\\n",
       "count              13.000              11.000              11.000   \n",
       "mean                0.040               0.039               0.040   \n",
       "std                 0.006               0.007               0.006   \n",
       "min                 0.031               0.030               0.030   \n",
       "25%                 0.036               0.035               0.036   \n",
       "50%                 0.040               0.036               0.040   \n",
       "75%                 0.045               0.042               0.041   \n",
       "max                 0.054               0.052               0.052   \n",
       "\n",
       "       val_loss_epoch_293  val_loss_epoch_294  val_loss_epoch_295  \\\n",
       "count               9.000               8.000               8.000   \n",
       "mean                0.039               0.039               0.038   \n",
       "std                 0.007               0.008               0.007   \n",
       "min                 0.030               0.031               0.030   \n",
       "25%                 0.035               0.035               0.034   \n",
       "50%                 0.037               0.036               0.036   \n",
       "75%                 0.040               0.039               0.040   \n",
       "max                 0.052               0.054               0.051   \n",
       "\n",
       "       val_loss_epoch_296  val_loss_epoch_297  val_loss_epoch_298  \\\n",
       "count               8.000               7.000               7.000   \n",
       "mean                0.039               0.039               0.040   \n",
       "std                 0.007               0.008               0.009   \n",
       "min                 0.031               0.031               0.030   \n",
       "25%                 0.035               0.035               0.035   \n",
       "50%                 0.036               0.037               0.037   \n",
       "75%                 0.040               0.043               0.043   \n",
       "max                 0.052               0.052               0.055   \n",
       "\n",
       "       val_loss_epoch_299  val_loss_epoch_300  val_loss_epoch_301  \\\n",
       "count               7.000               7.000               7.000   \n",
       "mean                0.040               0.039               0.039   \n",
       "std                 0.009               0.008               0.007   \n",
       "min                 0.030               0.030               0.031   \n",
       "25%                 0.035               0.034               0.035   \n",
       "50%                 0.037               0.036               0.036   \n",
       "75%                 0.043               0.042               0.042   \n",
       "max                 0.055               0.051               0.050   \n",
       "\n",
       "       val_loss_epoch_302  val_loss_epoch_303  val_loss_epoch_304  \\\n",
       "count               6.000               4.000               4.000   \n",
       "mean                0.040               0.042               0.042   \n",
       "std                 0.009               0.009               0.009   \n",
       "min                 0.031               0.033               0.033   \n",
       "25%                 0.035               0.036               0.036   \n",
       "50%                 0.036               0.042               0.042   \n",
       "75%                 0.047               0.048               0.048   \n",
       "max                 0.051               0.052               0.051   \n",
       "\n",
       "       val_loss_epoch_305  val_loss_epoch_306  val_loss_epoch_307  \\\n",
       "count               3.000               2.000               2.000   \n",
       "mean                0.047               0.051               0.050   \n",
       "std                 0.013               0.006               0.001   \n",
       "min                 0.033               0.047               0.050   \n",
       "25%                 0.041               0.049               0.050   \n",
       "50%                 0.049               0.051               0.050   \n",
       "75%                 0.054               0.054               0.050   \n",
       "max                 0.059               0.056               0.051   \n",
       "\n",
       "       val_loss_epoch_308  val_loss_epoch_309  val_loss_epoch_310  \\\n",
       "count               2.000               2.000               2.000   \n",
       "mean                0.051               0.049               0.049   \n",
       "std                 0.006               0.003               0.004   \n",
       "min                 0.047               0.046               0.047   \n",
       "25%                 0.049               0.048               0.048   \n",
       "50%                 0.051               0.049               0.049   \n",
       "75%                 0.053               0.050               0.051   \n",
       "max                 0.055               0.051               0.052   \n",
       "\n",
       "       val_loss_epoch_311  val_loss_epoch_312  val_loss_epoch_313  \n",
       "count               2.000               1.000               1.000  \n",
       "mean                0.051               0.047               0.047  \n",
       "std                 0.004                 NaN                 NaN  \n",
       "min                 0.049               0.047               0.047  \n",
       "25%                 0.050               0.047               0.047  \n",
       "50%                 0.051               0.047               0.047  \n",
       "75%                 0.053               0.047               0.047  \n",
       "max                 0.054               0.047               0.047  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>r2_keras_loss_epoch_1</th>\n",
       "      <th>r2_keras_loss_epoch_2</th>\n",
       "      <th>r2_keras_loss_epoch_3</th>\n",
       "      <th>r2_keras_loss_epoch_4</th>\n",
       "      <th>r2_keras_loss_epoch_5</th>\n",
       "      <th>r2_keras_loss_epoch_6</th>\n",
       "      <th>r2_keras_loss_epoch_7</th>\n",
       "      <th>r2_keras_loss_epoch_8</th>\n",
       "      <th>r2_keras_loss_epoch_9</th>\n",
       "      <th>r2_keras_loss_epoch_10</th>\n",
       "      <th>r2_keras_loss_epoch_11</th>\n",
       "      <th>r2_keras_loss_epoch_12</th>\n",
       "      <th>r2_keras_loss_epoch_13</th>\n",
       "      <th>r2_keras_loss_epoch_14</th>\n",
       "      <th>r2_keras_loss_epoch_15</th>\n",
       "      <th>r2_keras_loss_epoch_16</th>\n",
       "      <th>r2_keras_loss_epoch_17</th>\n",
       "      <th>r2_keras_loss_epoch_18</th>\n",
       "      <th>r2_keras_loss_epoch_19</th>\n",
       "      <th>r2_keras_loss_epoch_20</th>\n",
       "      <th>r2_keras_loss_epoch_21</th>\n",
       "      <th>r2_keras_loss_epoch_22</th>\n",
       "      <th>r2_keras_loss_epoch_23</th>\n",
       "      <th>r2_keras_loss_epoch_24</th>\n",
       "      <th>r2_keras_loss_epoch_25</th>\n",
       "      <th>r2_keras_loss_epoch_26</th>\n",
       "      <th>r2_keras_loss_epoch_27</th>\n",
       "      <th>r2_keras_loss_epoch_28</th>\n",
       "      <th>r2_keras_loss_epoch_29</th>\n",
       "      <th>r2_keras_loss_epoch_30</th>\n",
       "      <th>r2_keras_loss_epoch_31</th>\n",
       "      <th>r2_keras_loss_epoch_32</th>\n",
       "      <th>r2_keras_loss_epoch_33</th>\n",
       "      <th>r2_keras_loss_epoch_34</th>\n",
       "      <th>r2_keras_loss_epoch_35</th>\n",
       "      <th>r2_keras_loss_epoch_36</th>\n",
       "      <th>r2_keras_loss_epoch_37</th>\n",
       "      <th>r2_keras_loss_epoch_38</th>\n",
       "      <th>r2_keras_loss_epoch_39</th>\n",
       "      <th>r2_keras_loss_epoch_40</th>\n",
       "      <th>r2_keras_loss_epoch_41</th>\n",
       "      <th>r2_keras_loss_epoch_42</th>\n",
       "      <th>r2_keras_loss_epoch_43</th>\n",
       "      <th>r2_keras_loss_epoch_44</th>\n",
       "      <th>r2_keras_loss_epoch_45</th>\n",
       "      <th>r2_keras_loss_epoch_46</th>\n",
       "      <th>r2_keras_loss_epoch_47</th>\n",
       "      <th>r2_keras_loss_epoch_48</th>\n",
       "      <th>r2_keras_loss_epoch_49</th>\n",
       "      <th>r2_keras_loss_epoch_50</th>\n",
       "      <th>r2_keras_loss_epoch_51</th>\n",
       "      <th>r2_keras_loss_epoch_52</th>\n",
       "      <th>r2_keras_loss_epoch_53</th>\n",
       "      <th>r2_keras_loss_epoch_54</th>\n",
       "      <th>r2_keras_loss_epoch_55</th>\n",
       "      <th>r2_keras_loss_epoch_56</th>\n",
       "      <th>r2_keras_loss_epoch_57</th>\n",
       "      <th>r2_keras_loss_epoch_58</th>\n",
       "      <th>r2_keras_loss_epoch_59</th>\n",
       "      <th>r2_keras_loss_epoch_60</th>\n",
       "      <th>r2_keras_loss_epoch_61</th>\n",
       "      <th>r2_keras_loss_epoch_62</th>\n",
       "      <th>r2_keras_loss_epoch_63</th>\n",
       "      <th>r2_keras_loss_epoch_64</th>\n",
       "      <th>r2_keras_loss_epoch_65</th>\n",
       "      <th>r2_keras_loss_epoch_66</th>\n",
       "      <th>r2_keras_loss_epoch_67</th>\n",
       "      <th>r2_keras_loss_epoch_68</th>\n",
       "      <th>r2_keras_loss_epoch_69</th>\n",
       "      <th>r2_keras_loss_epoch_70</th>\n",
       "      <th>r2_keras_loss_epoch_71</th>\n",
       "      <th>r2_keras_loss_epoch_72</th>\n",
       "      <th>r2_keras_loss_epoch_73</th>\n",
       "      <th>r2_keras_loss_epoch_74</th>\n",
       "      <th>r2_keras_loss_epoch_75</th>\n",
       "      <th>r2_keras_loss_epoch_76</th>\n",
       "      <th>r2_keras_loss_epoch_77</th>\n",
       "      <th>r2_keras_loss_epoch_78</th>\n",
       "      <th>r2_keras_loss_epoch_79</th>\n",
       "      <th>r2_keras_loss_epoch_80</th>\n",
       "      <th>r2_keras_loss_epoch_81</th>\n",
       "      <th>r2_keras_loss_epoch_82</th>\n",
       "      <th>r2_keras_loss_epoch_83</th>\n",
       "      <th>r2_keras_loss_epoch_84</th>\n",
       "      <th>r2_keras_loss_epoch_85</th>\n",
       "      <th>r2_keras_loss_epoch_86</th>\n",
       "      <th>r2_keras_loss_epoch_87</th>\n",
       "      <th>r2_keras_loss_epoch_88</th>\n",
       "      <th>r2_keras_loss_epoch_89</th>\n",
       "      <th>r2_keras_loss_epoch_90</th>\n",
       "      <th>r2_keras_loss_epoch_91</th>\n",
       "      <th>r2_keras_loss_epoch_92</th>\n",
       "      <th>r2_keras_loss_epoch_93</th>\n",
       "      <th>r2_keras_loss_epoch_94</th>\n",
       "      <th>r2_keras_loss_epoch_95</th>\n",
       "      <th>r2_keras_loss_epoch_96</th>\n",
       "      <th>r2_keras_loss_epoch_97</th>\n",
       "      <th>r2_keras_loss_epoch_98</th>\n",
       "      <th>r2_keras_loss_epoch_99</th>\n",
       "      <th>r2_keras_loss_epoch_100</th>\n",
       "      <th>r2_keras_loss_epoch_101</th>\n",
       "      <th>r2_keras_loss_epoch_102</th>\n",
       "      <th>r2_keras_loss_epoch_103</th>\n",
       "      <th>r2_keras_loss_epoch_104</th>\n",
       "      <th>r2_keras_loss_epoch_105</th>\n",
       "      <th>r2_keras_loss_epoch_106</th>\n",
       "      <th>r2_keras_loss_epoch_107</th>\n",
       "      <th>r2_keras_loss_epoch_108</th>\n",
       "      <th>r2_keras_loss_epoch_109</th>\n",
       "      <th>r2_keras_loss_epoch_110</th>\n",
       "      <th>r2_keras_loss_epoch_111</th>\n",
       "      <th>r2_keras_loss_epoch_112</th>\n",
       "      <th>r2_keras_loss_epoch_113</th>\n",
       "      <th>r2_keras_loss_epoch_114</th>\n",
       "      <th>r2_keras_loss_epoch_115</th>\n",
       "      <th>r2_keras_loss_epoch_116</th>\n",
       "      <th>r2_keras_loss_epoch_117</th>\n",
       "      <th>r2_keras_loss_epoch_118</th>\n",
       "      <th>r2_keras_loss_epoch_119</th>\n",
       "      <th>r2_keras_loss_epoch_120</th>\n",
       "      <th>r2_keras_loss_epoch_121</th>\n",
       "      <th>r2_keras_loss_epoch_122</th>\n",
       "      <th>r2_keras_loss_epoch_123</th>\n",
       "      <th>r2_keras_loss_epoch_124</th>\n",
       "      <th>r2_keras_loss_epoch_125</th>\n",
       "      <th>r2_keras_loss_epoch_126</th>\n",
       "      <th>r2_keras_loss_epoch_127</th>\n",
       "      <th>r2_keras_loss_epoch_128</th>\n",
       "      <th>r2_keras_loss_epoch_129</th>\n",
       "      <th>r2_keras_loss_epoch_130</th>\n",
       "      <th>r2_keras_loss_epoch_131</th>\n",
       "      <th>r2_keras_loss_epoch_132</th>\n",
       "      <th>r2_keras_loss_epoch_133</th>\n",
       "      <th>r2_keras_loss_epoch_134</th>\n",
       "      <th>r2_keras_loss_epoch_135</th>\n",
       "      <th>r2_keras_loss_epoch_136</th>\n",
       "      <th>r2_keras_loss_epoch_137</th>\n",
       "      <th>r2_keras_loss_epoch_138</th>\n",
       "      <th>r2_keras_loss_epoch_139</th>\n",
       "      <th>r2_keras_loss_epoch_140</th>\n",
       "      <th>r2_keras_loss_epoch_141</th>\n",
       "      <th>r2_keras_loss_epoch_142</th>\n",
       "      <th>r2_keras_loss_epoch_143</th>\n",
       "      <th>r2_keras_loss_epoch_144</th>\n",
       "      <th>r2_keras_loss_epoch_145</th>\n",
       "      <th>r2_keras_loss_epoch_146</th>\n",
       "      <th>r2_keras_loss_epoch_147</th>\n",
       "      <th>r2_keras_loss_epoch_148</th>\n",
       "      <th>r2_keras_loss_epoch_149</th>\n",
       "      <th>r2_keras_loss_epoch_150</th>\n",
       "      <th>r2_keras_loss_epoch_151</th>\n",
       "      <th>r2_keras_loss_epoch_152</th>\n",
       "      <th>r2_keras_loss_epoch_153</th>\n",
       "      <th>r2_keras_loss_epoch_154</th>\n",
       "      <th>r2_keras_loss_epoch_155</th>\n",
       "      <th>r2_keras_loss_epoch_156</th>\n",
       "      <th>r2_keras_loss_epoch_157</th>\n",
       "      <th>r2_keras_loss_epoch_158</th>\n",
       "      <th>r2_keras_loss_epoch_159</th>\n",
       "      <th>r2_keras_loss_epoch_160</th>\n",
       "      <th>r2_keras_loss_epoch_161</th>\n",
       "      <th>r2_keras_loss_epoch_162</th>\n",
       "      <th>r2_keras_loss_epoch_163</th>\n",
       "      <th>r2_keras_loss_epoch_164</th>\n",
       "      <th>r2_keras_loss_epoch_165</th>\n",
       "      <th>r2_keras_loss_epoch_166</th>\n",
       "      <th>r2_keras_loss_epoch_167</th>\n",
       "      <th>r2_keras_loss_epoch_168</th>\n",
       "      <th>r2_keras_loss_epoch_169</th>\n",
       "      <th>r2_keras_loss_epoch_170</th>\n",
       "      <th>r2_keras_loss_epoch_171</th>\n",
       "      <th>r2_keras_loss_epoch_172</th>\n",
       "      <th>r2_keras_loss_epoch_173</th>\n",
       "      <th>r2_keras_loss_epoch_174</th>\n",
       "      <th>r2_keras_loss_epoch_175</th>\n",
       "      <th>r2_keras_loss_epoch_176</th>\n",
       "      <th>r2_keras_loss_epoch_177</th>\n",
       "      <th>r2_keras_loss_epoch_178</th>\n",
       "      <th>r2_keras_loss_epoch_179</th>\n",
       "      <th>r2_keras_loss_epoch_180</th>\n",
       "      <th>r2_keras_loss_epoch_181</th>\n",
       "      <th>r2_keras_loss_epoch_182</th>\n",
       "      <th>r2_keras_loss_epoch_183</th>\n",
       "      <th>r2_keras_loss_epoch_184</th>\n",
       "      <th>r2_keras_loss_epoch_185</th>\n",
       "      <th>r2_keras_loss_epoch_186</th>\n",
       "      <th>r2_keras_loss_epoch_187</th>\n",
       "      <th>r2_keras_loss_epoch_188</th>\n",
       "      <th>r2_keras_loss_epoch_189</th>\n",
       "      <th>r2_keras_loss_epoch_190</th>\n",
       "      <th>r2_keras_loss_epoch_191</th>\n",
       "      <th>r2_keras_loss_epoch_192</th>\n",
       "      <th>r2_keras_loss_epoch_193</th>\n",
       "      <th>r2_keras_loss_epoch_194</th>\n",
       "      <th>r2_keras_loss_epoch_195</th>\n",
       "      <th>r2_keras_loss_epoch_196</th>\n",
       "      <th>r2_keras_loss_epoch_197</th>\n",
       "      <th>r2_keras_loss_epoch_198</th>\n",
       "      <th>r2_keras_loss_epoch_199</th>\n",
       "      <th>r2_keras_loss_epoch_200</th>\n",
       "      <th>r2_keras_loss_epoch_201</th>\n",
       "      <th>r2_keras_loss_epoch_202</th>\n",
       "      <th>r2_keras_loss_epoch_203</th>\n",
       "      <th>r2_keras_loss_epoch_204</th>\n",
       "      <th>r2_keras_loss_epoch_205</th>\n",
       "      <th>r2_keras_loss_epoch_206</th>\n",
       "      <th>r2_keras_loss_epoch_207</th>\n",
       "      <th>r2_keras_loss_epoch_208</th>\n",
       "      <th>r2_keras_loss_epoch_209</th>\n",
       "      <th>r2_keras_loss_epoch_210</th>\n",
       "      <th>r2_keras_loss_epoch_211</th>\n",
       "      <th>r2_keras_loss_epoch_212</th>\n",
       "      <th>r2_keras_loss_epoch_213</th>\n",
       "      <th>r2_keras_loss_epoch_214</th>\n",
       "      <th>r2_keras_loss_epoch_215</th>\n",
       "      <th>r2_keras_loss_epoch_216</th>\n",
       "      <th>r2_keras_loss_epoch_217</th>\n",
       "      <th>r2_keras_loss_epoch_218</th>\n",
       "      <th>r2_keras_loss_epoch_219</th>\n",
       "      <th>r2_keras_loss_epoch_220</th>\n",
       "      <th>r2_keras_loss_epoch_221</th>\n",
       "      <th>r2_keras_loss_epoch_222</th>\n",
       "      <th>r2_keras_loss_epoch_223</th>\n",
       "      <th>r2_keras_loss_epoch_224</th>\n",
       "      <th>r2_keras_loss_epoch_225</th>\n",
       "      <th>r2_keras_loss_epoch_226</th>\n",
       "      <th>r2_keras_loss_epoch_227</th>\n",
       "      <th>r2_keras_loss_epoch_228</th>\n",
       "      <th>r2_keras_loss_epoch_229</th>\n",
       "      <th>r2_keras_loss_epoch_230</th>\n",
       "      <th>r2_keras_loss_epoch_231</th>\n",
       "      <th>r2_keras_loss_epoch_232</th>\n",
       "      <th>r2_keras_loss_epoch_233</th>\n",
       "      <th>r2_keras_loss_epoch_234</th>\n",
       "      <th>r2_keras_loss_epoch_235</th>\n",
       "      <th>r2_keras_loss_epoch_236</th>\n",
       "      <th>r2_keras_loss_epoch_237</th>\n",
       "      <th>r2_keras_loss_epoch_238</th>\n",
       "      <th>r2_keras_loss_epoch_239</th>\n",
       "      <th>r2_keras_loss_epoch_240</th>\n",
       "      <th>r2_keras_loss_epoch_241</th>\n",
       "      <th>r2_keras_loss_epoch_242</th>\n",
       "      <th>r2_keras_loss_epoch_243</th>\n",
       "      <th>r2_keras_loss_epoch_244</th>\n",
       "      <th>r2_keras_loss_epoch_245</th>\n",
       "      <th>r2_keras_loss_epoch_246</th>\n",
       "      <th>r2_keras_loss_epoch_247</th>\n",
       "      <th>r2_keras_loss_epoch_248</th>\n",
       "      <th>r2_keras_loss_epoch_249</th>\n",
       "      <th>r2_keras_loss_epoch_250</th>\n",
       "      <th>r2_keras_loss_epoch_251</th>\n",
       "      <th>r2_keras_loss_epoch_252</th>\n",
       "      <th>r2_keras_loss_epoch_253</th>\n",
       "      <th>r2_keras_loss_epoch_254</th>\n",
       "      <th>r2_keras_loss_epoch_255</th>\n",
       "      <th>r2_keras_loss_epoch_256</th>\n",
       "      <th>r2_keras_loss_epoch_257</th>\n",
       "      <th>r2_keras_loss_epoch_258</th>\n",
       "      <th>r2_keras_loss_epoch_259</th>\n",
       "      <th>r2_keras_loss_epoch_260</th>\n",
       "      <th>r2_keras_loss_epoch_261</th>\n",
       "      <th>r2_keras_loss_epoch_262</th>\n",
       "      <th>r2_keras_loss_epoch_263</th>\n",
       "      <th>r2_keras_loss_epoch_264</th>\n",
       "      <th>r2_keras_loss_epoch_265</th>\n",
       "      <th>r2_keras_loss_epoch_266</th>\n",
       "      <th>r2_keras_loss_epoch_267</th>\n",
       "      <th>r2_keras_loss_epoch_268</th>\n",
       "      <th>r2_keras_loss_epoch_269</th>\n",
       "      <th>r2_keras_loss_epoch_270</th>\n",
       "      <th>r2_keras_loss_epoch_271</th>\n",
       "      <th>r2_keras_loss_epoch_272</th>\n",
       "      <th>r2_keras_loss_epoch_273</th>\n",
       "      <th>r2_keras_loss_epoch_274</th>\n",
       "      <th>r2_keras_loss_epoch_275</th>\n",
       "      <th>r2_keras_loss_epoch_276</th>\n",
       "      <th>r2_keras_loss_epoch_277</th>\n",
       "      <th>r2_keras_loss_epoch_278</th>\n",
       "      <th>r2_keras_loss_epoch_279</th>\n",
       "      <th>r2_keras_loss_epoch_280</th>\n",
       "      <th>r2_keras_loss_epoch_281</th>\n",
       "      <th>r2_keras_loss_epoch_282</th>\n",
       "      <th>r2_keras_loss_epoch_283</th>\n",
       "      <th>r2_keras_loss_epoch_284</th>\n",
       "      <th>r2_keras_loss_epoch_285</th>\n",
       "      <th>r2_keras_loss_epoch_286</th>\n",
       "      <th>r2_keras_loss_epoch_287</th>\n",
       "      <th>r2_keras_loss_epoch_288</th>\n",
       "      <th>r2_keras_loss_epoch_289</th>\n",
       "      <th>r2_keras_loss_epoch_290</th>\n",
       "      <th>r2_keras_loss_epoch_291</th>\n",
       "      <th>r2_keras_loss_epoch_292</th>\n",
       "      <th>r2_keras_loss_epoch_293</th>\n",
       "      <th>r2_keras_loss_epoch_294</th>\n",
       "      <th>r2_keras_loss_epoch_295</th>\n",
       "      <th>r2_keras_loss_epoch_296</th>\n",
       "      <th>r2_keras_loss_epoch_297</th>\n",
       "      <th>r2_keras_loss_epoch_298</th>\n",
       "      <th>r2_keras_loss_epoch_299</th>\n",
       "      <th>r2_keras_loss_epoch_300</th>\n",
       "      <th>r2_keras_loss_epoch_301</th>\n",
       "      <th>r2_keras_loss_epoch_302</th>\n",
       "      <th>r2_keras_loss_epoch_303</th>\n",
       "      <th>r2_keras_loss_epoch_304</th>\n",
       "      <th>r2_keras_loss_epoch_305</th>\n",
       "      <th>r2_keras_loss_epoch_306</th>\n",
       "      <th>r2_keras_loss_epoch_307</th>\n",
       "      <th>r2_keras_loss_epoch_308</th>\n",
       "      <th>r2_keras_loss_epoch_309</th>\n",
       "      <th>r2_keras_loss_epoch_310</th>\n",
       "      <th>r2_keras_loss_epoch_311</th>\n",
       "      <th>r2_keras_loss_epoch_312</th>\n",
       "      <th>r2_keras_loss_epoch_313</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49998.000</td>\n",
       "      <td>49997.000</td>\n",
       "      <td>49995.000</td>\n",
       "      <td>49992.000</td>\n",
       "      <td>49988.000</td>\n",
       "      <td>49987.000</td>\n",
       "      <td>49985.000</td>\n",
       "      <td>49977.000</td>\n",
       "      <td>49972.000</td>\n",
       "      <td>49963.000</td>\n",
       "      <td>49954.000</td>\n",
       "      <td>49947.000</td>\n",
       "      <td>49928.000</td>\n",
       "      <td>49915.000</td>\n",
       "      <td>49892.000</td>\n",
       "      <td>49877.000</td>\n",
       "      <td>49863.000</td>\n",
       "      <td>49844.000</td>\n",
       "      <td>49815.000</td>\n",
       "      <td>49786.000</td>\n",
       "      <td>49745.000</td>\n",
       "      <td>49715.000</td>\n",
       "      <td>49684.000</td>\n",
       "      <td>49639.000</td>\n",
       "      <td>49590.000</td>\n",
       "      <td>49533.000</td>\n",
       "      <td>49489.000</td>\n",
       "      <td>49419.000</td>\n",
       "      <td>49345.000</td>\n",
       "      <td>49250.000</td>\n",
       "      <td>49171.000</td>\n",
       "      <td>49086.000</td>\n",
       "      <td>48990.000</td>\n",
       "      <td>48876.000</td>\n",
       "      <td>48734.000</td>\n",
       "      <td>48604.000</td>\n",
       "      <td>48450.000</td>\n",
       "      <td>48292.000</td>\n",
       "      <td>48129.000</td>\n",
       "      <td>47962.000</td>\n",
       "      <td>47784.000</td>\n",
       "      <td>47623.000</td>\n",
       "      <td>47424.000</td>\n",
       "      <td>47195.000</td>\n",
       "      <td>46965.000</td>\n",
       "      <td>46709.000</td>\n",
       "      <td>46440.000</td>\n",
       "      <td>46166.000</td>\n",
       "      <td>45899.000</td>\n",
       "      <td>45611.000</td>\n",
       "      <td>45339.000</td>\n",
       "      <td>45037.000</td>\n",
       "      <td>44714.000</td>\n",
       "      <td>44353.000</td>\n",
       "      <td>44009.000</td>\n",
       "      <td>43657.000</td>\n",
       "      <td>43314.000</td>\n",
       "      <td>42929.000</td>\n",
       "      <td>42508.000</td>\n",
       "      <td>42093.000</td>\n",
       "      <td>41659.000</td>\n",
       "      <td>41237.000</td>\n",
       "      <td>40826.000</td>\n",
       "      <td>40392.000</td>\n",
       "      <td>39922.000</td>\n",
       "      <td>39426.000</td>\n",
       "      <td>38899.000</td>\n",
       "      <td>38437.000</td>\n",
       "      <td>37935.000</td>\n",
       "      <td>37418.000</td>\n",
       "      <td>36939.000</td>\n",
       "      <td>36417.000</td>\n",
       "      <td>35913.000</td>\n",
       "      <td>35372.000</td>\n",
       "      <td>34824.000</td>\n",
       "      <td>34290.000</td>\n",
       "      <td>33739.000</td>\n",
       "      <td>33162.000</td>\n",
       "      <td>32587.000</td>\n",
       "      <td>32037.000</td>\n",
       "      <td>31435.000</td>\n",
       "      <td>30861.000</td>\n",
       "      <td>30298.000</td>\n",
       "      <td>29751.000</td>\n",
       "      <td>29196.000</td>\n",
       "      <td>28593.000</td>\n",
       "      <td>28022.000</td>\n",
       "      <td>27441.000</td>\n",
       "      <td>26875.000</td>\n",
       "      <td>26276.000</td>\n",
       "      <td>25731.000</td>\n",
       "      <td>25168.000</td>\n",
       "      <td>24625.000</td>\n",
       "      <td>24089.000</td>\n",
       "      <td>23560.000</td>\n",
       "      <td>22983.000</td>\n",
       "      <td>22420.000</td>\n",
       "      <td>21915.000</td>\n",
       "      <td>21414.000</td>\n",
       "      <td>20861.000</td>\n",
       "      <td>20337.000</td>\n",
       "      <td>19866.000</td>\n",
       "      <td>19357.000</td>\n",
       "      <td>18812.000</td>\n",
       "      <td>18312.000</td>\n",
       "      <td>17824.000</td>\n",
       "      <td>17372.000</td>\n",
       "      <td>16916.000</td>\n",
       "      <td>16427.000</td>\n",
       "      <td>15973.000</td>\n",
       "      <td>15529.000</td>\n",
       "      <td>15086.000</td>\n",
       "      <td>14656.000</td>\n",
       "      <td>14242.000</td>\n",
       "      <td>13836.000</td>\n",
       "      <td>13441.000</td>\n",
       "      <td>13031.000</td>\n",
       "      <td>12650.000</td>\n",
       "      <td>12264.000</td>\n",
       "      <td>11884.000</td>\n",
       "      <td>11508.000</td>\n",
       "      <td>11163.000</td>\n",
       "      <td>10813.000</td>\n",
       "      <td>10465.000</td>\n",
       "      <td>10126.000</td>\n",
       "      <td>9791.000</td>\n",
       "      <td>9460.000</td>\n",
       "      <td>9137.000</td>\n",
       "      <td>8842.000</td>\n",
       "      <td>8512.000</td>\n",
       "      <td>8217.000</td>\n",
       "      <td>7929.000</td>\n",
       "      <td>7644.000</td>\n",
       "      <td>7362.000</td>\n",
       "      <td>7087.000</td>\n",
       "      <td>6863.000</td>\n",
       "      <td>6614.000</td>\n",
       "      <td>6328.000</td>\n",
       "      <td>6076.000</td>\n",
       "      <td>5826.000</td>\n",
       "      <td>5606.000</td>\n",
       "      <td>5383.000</td>\n",
       "      <td>5167.000</td>\n",
       "      <td>4971.000</td>\n",
       "      <td>4765.000</td>\n",
       "      <td>4597.000</td>\n",
       "      <td>4430.000</td>\n",
       "      <td>4264.000</td>\n",
       "      <td>4085.000</td>\n",
       "      <td>3910.000</td>\n",
       "      <td>3758.000</td>\n",
       "      <td>3607.000</td>\n",
       "      <td>3443.000</td>\n",
       "      <td>3313.000</td>\n",
       "      <td>3155.000</td>\n",
       "      <td>3020.000</td>\n",
       "      <td>2892.000</td>\n",
       "      <td>2746.000</td>\n",
       "      <td>2641.000</td>\n",
       "      <td>2532.000</td>\n",
       "      <td>2412.000</td>\n",
       "      <td>2290.000</td>\n",
       "      <td>2169.000</td>\n",
       "      <td>2054.000</td>\n",
       "      <td>1954.000</td>\n",
       "      <td>1872.000</td>\n",
       "      <td>1772.000</td>\n",
       "      <td>1675.000</td>\n",
       "      <td>1599.000</td>\n",
       "      <td>1532.000</td>\n",
       "      <td>1449.000</td>\n",
       "      <td>1385.000</td>\n",
       "      <td>1327.000</td>\n",
       "      <td>1262.000</td>\n",
       "      <td>1198.000</td>\n",
       "      <td>1123.000</td>\n",
       "      <td>1062.000</td>\n",
       "      <td>1009.000</td>\n",
       "      <td>955.000</td>\n",
       "      <td>913.000</td>\n",
       "      <td>871.000</td>\n",
       "      <td>818.000</td>\n",
       "      <td>762.000</td>\n",
       "      <td>729.000</td>\n",
       "      <td>687.000</td>\n",
       "      <td>653.000</td>\n",
       "      <td>621.000</td>\n",
       "      <td>588.000</td>\n",
       "      <td>556.000</td>\n",
       "      <td>526.000</td>\n",
       "      <td>501.000</td>\n",
       "      <td>476.000</td>\n",
       "      <td>448.000</td>\n",
       "      <td>424.000</td>\n",
       "      <td>392.000</td>\n",
       "      <td>374.000</td>\n",
       "      <td>357.000</td>\n",
       "      <td>333.000</td>\n",
       "      <td>307.000</td>\n",
       "      <td>290.000</td>\n",
       "      <td>271.000</td>\n",
       "      <td>252.000</td>\n",
       "      <td>234.000</td>\n",
       "      <td>220.000</td>\n",
       "      <td>208.000</td>\n",
       "      <td>197.000</td>\n",
       "      <td>184.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>169.000</td>\n",
       "      <td>158.000</td>\n",
       "      <td>146.000</td>\n",
       "      <td>139.000</td>\n",
       "      <td>129.000</td>\n",
       "      <td>122.000</td>\n",
       "      <td>113.000</td>\n",
       "      <td>107.000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>78.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>54.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>45.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>22.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24999.500</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.815</td>\n",
       "      <td>-0.830</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>-0.857</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-0.915</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14433.901</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.632</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12499.750</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>-0.807</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24999.500</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>-0.706</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>-0.789</td>\n",
       "      <td>-0.806</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>-0.873</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-0.922</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37499.250</td>\n",
       "      <td>0.927</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>-0.786</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-0.819</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>-0.873</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-0.914</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49999.000</td>\n",
       "      <td>11.214</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>-0.769</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>-0.878</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.888</td>\n",
       "      <td>-0.883</td>\n",
       "      <td>-0.888</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>-0.888</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-0.914</td>\n",
       "      <td>-0.915</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>-0.915</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.922</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  r2_keras_loss_epoch_1  r2_keras_loss_epoch_2  \\\n",
       "count 50000.000              50000.000              50000.000   \n",
       "mean  24999.500                  0.518                 -0.472   \n",
       "std   14433.901                  0.783                  0.151   \n",
       "min       0.000                 -0.632                 -0.890   \n",
       "25%   12499.750                 -0.069                 -0.596   \n",
       "50%   24999.500                  0.340                 -0.464   \n",
       "75%   37499.250                  0.927                 -0.352   \n",
       "max   49999.000                 11.214                  0.200   \n",
       "\n",
       "       r2_keras_loss_epoch_3  r2_keras_loss_epoch_4  r2_keras_loss_epoch_5  \\\n",
       "count              50000.000              50000.000              50000.000   \n",
       "mean                  -0.696                 -0.760                 -0.783   \n",
       "std                    0.089                  0.067                  0.060   \n",
       "min                   -0.929                 -0.944                 -0.955   \n",
       "25%                   -0.762                 -0.807                 -0.825   \n",
       "50%                   -0.706                 -0.767                 -0.789   \n",
       "75%                   -0.637                 -0.720                 -0.747   \n",
       "max                   -0.193                 -0.362                 -0.455   \n",
       "\n",
       "       r2_keras_loss_epoch_6  r2_keras_loss_epoch_7  r2_keras_loss_epoch_8  \\\n",
       "count              50000.000              50000.000              50000.000   \n",
       "mean                  -0.800                 -0.815                 -0.830   \n",
       "std                    0.055                  0.051                  0.047   \n",
       "min                   -0.955                 -0.959                 -0.962   \n",
       "25%                   -0.838                 -0.851                 -0.862   \n",
       "50%                   -0.806                 -0.821                 -0.835   \n",
       "75%                   -0.768                 -0.786                 -0.803   \n",
       "max                   -0.499                 -0.520                 -0.544   \n",
       "\n",
       "       r2_keras_loss_epoch_9  r2_keras_loss_epoch_10  r2_keras_loss_epoch_11  \\\n",
       "count              50000.000               50000.000               50000.000   \n",
       "mean                  -0.844                  -0.857                  -0.869   \n",
       "std                    0.043                   0.039                   0.036   \n",
       "min                   -0.964                  -0.967                  -0.968   \n",
       "25%                   -0.874                  -0.884                  -0.894   \n",
       "50%                   -0.848                  -0.861                  -0.873   \n",
       "75%                   -0.819                  -0.834                  -0.848   \n",
       "max                   -0.572                  -0.594                  -0.614   \n",
       "\n",
       "       r2_keras_loss_epoch_12  r2_keras_loss_epoch_13  r2_keras_loss_epoch_14  \\\n",
       "count               50000.000               50000.000               50000.000   \n",
       "mean                   -0.881                  -0.891                  -0.900   \n",
       "std                     0.032                   0.030                   0.027   \n",
       "min                    -0.972                  -0.974                  -0.976   \n",
       "25%                    -0.903                  -0.912                  -0.919   \n",
       "50%                    -0.884                  -0.893                  -0.902   \n",
       "75%                    -0.861                  -0.873                  -0.884   \n",
       "max                    -0.647                  -0.684                  -0.714   \n",
       "\n",
       "       r2_keras_loss_epoch_15  r2_keras_loss_epoch_16  r2_keras_loss_epoch_17  \\\n",
       "count               50000.000               50000.000               50000.000   \n",
       "mean                   -0.908                  -0.915                  -0.921   \n",
       "std                     0.025                   0.023                   0.021   \n",
       "min                    -0.980                  -0.981                  -0.982   \n",
       "25%                    -0.925                  -0.931                  -0.936   \n",
       "50%                    -0.910                  -0.916                  -0.922   \n",
       "75%                    -0.893                  -0.901                  -0.908   \n",
       "max                    -0.740                  -0.769                  -0.785   \n",
       "\n",
       "       r2_keras_loss_epoch_18  r2_keras_loss_epoch_19  r2_keras_loss_epoch_20  \\\n",
       "count               50000.000               50000.000               50000.000   \n",
       "mean                   -0.926                  -0.931                  -0.935   \n",
       "std                     0.019                   0.018                   0.017   \n",
       "min                    -0.984                  -0.984                  -0.985   \n",
       "25%                    -0.940                  -0.943                  -0.946   \n",
       "50%                    -0.927                  -0.932                  -0.935   \n",
       "75%                    -0.914                  -0.919                  -0.924   \n",
       "max                    -0.798                  -0.812                  -0.826   \n",
       "\n",
       "       r2_keras_loss_epoch_21  r2_keras_loss_epoch_22  r2_keras_loss_epoch_23  \\\n",
       "count               50000.000               50000.000               50000.000   \n",
       "mean                   -0.938                  -0.941                  -0.943   \n",
       "std                     0.016                   0.015                   0.014   \n",
       "min                    -0.985                  -0.987                  -0.987   \n",
       "25%                    -0.949                  -0.951                  -0.953   \n",
       "50%                    -0.939                  -0.942                  -0.944   \n",
       "75%                    -0.928                  -0.931                  -0.934   \n",
       "max                    -0.840                  -0.846                  -0.859   \n",
       "\n",
       "       r2_keras_loss_epoch_24  r2_keras_loss_epoch_25  r2_keras_loss_epoch_26  \\\n",
       "count               50000.000               50000.000               50000.000   \n",
       "mean                   -0.946                  -0.948                  -0.949   \n",
       "std                     0.014                   0.013                   0.013   \n",
       "min                    -0.988                  -0.988                  -0.988   \n",
       "25%                    -0.955                  -0.957                  -0.958   \n",
       "50%                    -0.946                  -0.948                  -0.950   \n",
       "75%                    -0.937                  -0.939                  -0.941   \n",
       "max                    -0.866                  -0.875                  -0.878   \n",
       "\n",
       "       r2_keras_loss_epoch_27  r2_keras_loss_epoch_28  r2_keras_loss_epoch_29  \\\n",
       "count               50000.000               50000.000               50000.000   \n",
       "mean                   -0.951                  -0.953                  -0.954   \n",
       "std                     0.012                   0.012                   0.012   \n",
       "min                    -0.989                  -0.989                  -0.990   \n",
       "25%                    -0.960                  -0.961                  -0.962   \n",
       "50%                    -0.952                  -0.953                  -0.954   \n",
       "75%                    -0.943                  -0.945                  -0.946   \n",
       "max                    -0.884                  -0.888                  -0.883   \n",
       "\n",
       "       r2_keras_loss_epoch_30  r2_keras_loss_epoch_31  r2_keras_loss_epoch_32  \\\n",
       "count               50000.000               50000.000               50000.000   \n",
       "mean                   -0.955                  -0.956                  -0.957   \n",
       "std                     0.012                   0.011                   0.011   \n",
       "min                    -0.990                  -0.990                  -0.990   \n",
       "25%                    -0.963                  -0.964                  -0.965   \n",
       "50%                    -0.956                  -0.957                  -0.958   \n",
       "75%                    -0.948                  -0.949                  -0.950   \n",
       "max                    -0.888                  -0.892                  -0.890   \n",
       "\n",
       "       r2_keras_loss_epoch_33  r2_keras_loss_epoch_34  r2_keras_loss_epoch_35  \\\n",
       "count               50000.000               50000.000               50000.000   \n",
       "mean                   -0.958                  -0.959                  -0.960   \n",
       "std                     0.011                   0.011                   0.011   \n",
       "min                    -0.991                  -0.991                  -0.991   \n",
       "25%                    -0.966                  -0.967                  -0.968   \n",
       "50%                    -0.959                  -0.960                  -0.961   \n",
       "75%                    -0.951                  -0.952                  -0.953   \n",
       "max                    -0.895                  -0.888                  -0.898   \n",
       "\n",
       "       r2_keras_loss_epoch_36  r2_keras_loss_epoch_37  r2_keras_loss_epoch_38  \\\n",
       "count               50000.000               50000.000               50000.000   \n",
       "mean                   -0.961                  -0.962                  -0.962   \n",
       "std                     0.010                   0.010                   0.010   \n",
       "min                    -0.991                  -0.991                  -0.991   \n",
       "25%                    -0.968                  -0.969                  -0.970   \n",
       "50%                    -0.962                  -0.962                  -0.963   \n",
       "75%                    -0.954                  -0.955                  -0.956   \n",
       "max                    -0.902                  -0.899                  -0.905   \n",
       "\n",
       "       r2_keras_loss_epoch_39  r2_keras_loss_epoch_40  r2_keras_loss_epoch_41  \\\n",
       "count               50000.000               50000.000               49999.000   \n",
       "mean                   -0.963                  -0.964                  -0.965   \n",
       "std                     0.010                   0.010                   0.010   \n",
       "min                    -0.992                  -0.992                  -0.992   \n",
       "25%                    -0.970                  -0.971                  -0.971   \n",
       "50%                    -0.964                  -0.965                  -0.965   \n",
       "75%                    -0.957                  -0.958                  -0.958   \n",
       "max                    -0.905                  -0.902                  -0.907   \n",
       "\n",
       "       r2_keras_loss_epoch_42  r2_keras_loss_epoch_43  r2_keras_loss_epoch_44  \\\n",
       "count               49999.000               49999.000               49998.000   \n",
       "mean                   -0.965                  -0.966                  -0.966   \n",
       "std                     0.010                   0.009                   0.009   \n",
       "min                    -0.992                  -0.992                  -0.993   \n",
       "25%                    -0.972                  -0.972                  -0.973   \n",
       "50%                    -0.966                  -0.967                  -0.967   \n",
       "75%                    -0.959                  -0.960                  -0.961   \n",
       "max                    -0.907                  -0.905                  -0.910   \n",
       "\n",
       "       r2_keras_loss_epoch_45  r2_keras_loss_epoch_46  r2_keras_loss_epoch_47  \\\n",
       "count               49997.000               49995.000               49992.000   \n",
       "mean                   -0.967                  -0.967                  -0.968   \n",
       "std                     0.009                   0.009                   0.009   \n",
       "min                    -0.993                  -0.993                  -0.993   \n",
       "25%                    -0.973                  -0.974                  -0.974   \n",
       "50%                    -0.968                  -0.968                  -0.969   \n",
       "75%                    -0.961                  -0.962                  -0.962   \n",
       "max                    -0.905                  -0.912                  -0.905   \n",
       "\n",
       "       r2_keras_loss_epoch_48  r2_keras_loss_epoch_49  r2_keras_loss_epoch_50  \\\n",
       "count               49988.000               49987.000               49985.000   \n",
       "mean                   -0.968                  -0.969                  -0.969   \n",
       "std                     0.009                   0.009                   0.009   \n",
       "min                    -0.993                  -0.993                  -0.993   \n",
       "25%                    -0.975                  -0.975                  -0.976   \n",
       "50%                    -0.969                  -0.970                  -0.970   \n",
       "75%                    -0.963                  -0.963                  -0.964   \n",
       "max                    -0.906                  -0.909                  -0.914   \n",
       "\n",
       "       r2_keras_loss_epoch_51  r2_keras_loss_epoch_52  r2_keras_loss_epoch_53  \\\n",
       "count               49977.000               49972.000               49963.000   \n",
       "mean                   -0.970                  -0.970                  -0.971   \n",
       "std                     0.009                   0.009                   0.009   \n",
       "min                    -0.993                  -0.994                  -0.994   \n",
       "25%                    -0.976                  -0.976                  -0.977   \n",
       "50%                    -0.971                  -0.971                  -0.971   \n",
       "75%                    -0.964                  -0.965                  -0.965   \n",
       "max                    -0.915                  -0.916                  -0.911   \n",
       "\n",
       "       r2_keras_loss_epoch_54  r2_keras_loss_epoch_55  r2_keras_loss_epoch_56  \\\n",
       "count               49954.000               49947.000               49928.000   \n",
       "mean                   -0.971                  -0.971                  -0.972   \n",
       "std                     0.008                   0.008                   0.008   \n",
       "min                    -0.993                  -0.994                  -0.994   \n",
       "25%                    -0.977                  -0.977                  -0.978   \n",
       "50%                    -0.972                  -0.972                  -0.973   \n",
       "75%                    -0.966                  -0.966                  -0.967   \n",
       "max                    -0.915                  -0.917                  -0.918   \n",
       "\n",
       "       r2_keras_loss_epoch_57  r2_keras_loss_epoch_58  r2_keras_loss_epoch_59  \\\n",
       "count               49915.000               49892.000               49877.000   \n",
       "mean                   -0.972                  -0.972                  -0.973   \n",
       "std                     0.008                   0.008                   0.008   \n",
       "min                    -0.994                  -0.994                  -0.994   \n",
       "25%                    -0.978                  -0.978                  -0.979   \n",
       "50%                    -0.973                  -0.973                  -0.974   \n",
       "75%                    -0.967                  -0.968                  -0.968   \n",
       "max                    -0.918                  -0.917                  -0.916   \n",
       "\n",
       "       r2_keras_loss_epoch_60  r2_keras_loss_epoch_61  r2_keras_loss_epoch_62  \\\n",
       "count               49863.000               49844.000               49815.000   \n",
       "mean                   -0.973                  -0.973                  -0.974   \n",
       "std                     0.008                   0.008                   0.008   \n",
       "min                    -0.994                  -0.994                  -0.995   \n",
       "25%                    -0.979                  -0.979                  -0.979   \n",
       "50%                    -0.974                  -0.974                  -0.975   \n",
       "75%                    -0.968                  -0.969                  -0.969   \n",
       "max                    -0.921                  -0.921                  -0.920   \n",
       "\n",
       "       r2_keras_loss_epoch_63  r2_keras_loss_epoch_64  r2_keras_loss_epoch_65  \\\n",
       "count               49786.000               49745.000               49715.000   \n",
       "mean                   -0.974                  -0.974                  -0.975   \n",
       "std                     0.008                   0.008                   0.008   \n",
       "min                    -0.994                  -0.994                  -0.994   \n",
       "25%                    -0.980                  -0.980                  -0.980   \n",
       "50%                    -0.975                  -0.975                  -0.975   \n",
       "75%                    -0.969                  -0.970                  -0.970   \n",
       "max                    -0.922                  -0.921                  -0.924   \n",
       "\n",
       "       r2_keras_loss_epoch_66  r2_keras_loss_epoch_67  r2_keras_loss_epoch_68  \\\n",
       "count               49684.000               49639.000               49590.000   \n",
       "mean                   -0.975                  -0.975                  -0.975   \n",
       "std                     0.008                   0.008                   0.007   \n",
       "min                    -0.995                  -0.995                  -0.995   \n",
       "25%                    -0.980                  -0.981                  -0.981   \n",
       "50%                    -0.976                  -0.976                  -0.976   \n",
       "75%                    -0.970                  -0.971                  -0.971   \n",
       "max                    -0.920                  -0.926                  -0.926   \n",
       "\n",
       "       r2_keras_loss_epoch_69  r2_keras_loss_epoch_70  r2_keras_loss_epoch_71  \\\n",
       "count               49533.000               49489.000               49419.000   \n",
       "mean                   -0.976                  -0.976                  -0.976   \n",
       "std                     0.007                   0.007                   0.007   \n",
       "min                    -0.995                  -0.995                  -0.995   \n",
       "25%                    -0.981                  -0.981                  -0.981   \n",
       "50%                    -0.976                  -0.977                  -0.977   \n",
       "75%                    -0.971                  -0.972                  -0.972   \n",
       "max                    -0.924                  -0.923                  -0.923   \n",
       "\n",
       "       r2_keras_loss_epoch_72  r2_keras_loss_epoch_73  r2_keras_loss_epoch_74  \\\n",
       "count               49345.000               49250.000               49171.000   \n",
       "mean                   -0.976                  -0.977                  -0.977   \n",
       "std                     0.007                   0.007                   0.007   \n",
       "min                    -0.995                  -0.995                  -0.996   \n",
       "25%                    -0.982                  -0.982                  -0.982   \n",
       "50%                    -0.977                  -0.977                  -0.978   \n",
       "75%                    -0.972                  -0.972                  -0.973   \n",
       "max                    -0.921                  -0.927                  -0.926   \n",
       "\n",
       "       r2_keras_loss_epoch_75  r2_keras_loss_epoch_76  r2_keras_loss_epoch_77  \\\n",
       "count               49086.000               48990.000               48876.000   \n",
       "mean                   -0.977                  -0.977                  -0.978   \n",
       "std                     0.007                   0.007                   0.007   \n",
       "min                    -0.995                  -0.995                  -0.996   \n",
       "25%                    -0.982                  -0.982                  -0.983   \n",
       "50%                    -0.978                  -0.978                  -0.978   \n",
       "75%                    -0.973                  -0.973                  -0.973   \n",
       "max                    -0.927                  -0.928                  -0.928   \n",
       "\n",
       "       r2_keras_loss_epoch_78  r2_keras_loss_epoch_79  r2_keras_loss_epoch_80  \\\n",
       "count               48734.000               48604.000               48450.000   \n",
       "mean                   -0.978                  -0.978                  -0.978   \n",
       "std                     0.007                   0.007                   0.007   \n",
       "min                    -0.995                  -0.996                  -0.996   \n",
       "25%                    -0.983                  -0.983                  -0.983   \n",
       "50%                    -0.979                  -0.979                  -0.979   \n",
       "75%                    -0.974                  -0.974                  -0.974   \n",
       "max                    -0.932                  -0.927                  -0.929   \n",
       "\n",
       "       r2_keras_loss_epoch_81  r2_keras_loss_epoch_82  r2_keras_loss_epoch_83  \\\n",
       "count               48292.000               48129.000               47962.000   \n",
       "mean                   -0.978                  -0.979                  -0.979   \n",
       "std                     0.007                   0.007                   0.007   \n",
       "min                    -0.996                  -0.995                  -0.995   \n",
       "25%                    -0.983                  -0.983                  -0.984   \n",
       "50%                    -0.979                  -0.979                  -0.980   \n",
       "75%                    -0.974                  -0.975                  -0.975   \n",
       "max                    -0.930                  -0.927                  -0.931   \n",
       "\n",
       "       r2_keras_loss_epoch_84  r2_keras_loss_epoch_85  r2_keras_loss_epoch_86  \\\n",
       "count               47784.000               47623.000               47424.000   \n",
       "mean                   -0.979                  -0.979                  -0.979   \n",
       "std                     0.007                   0.007                   0.007   \n",
       "min                    -0.996                  -0.996                  -0.996   \n",
       "25%                    -0.984                  -0.984                  -0.984   \n",
       "50%                    -0.980                  -0.980                  -0.980   \n",
       "75%                    -0.975                  -0.975                  -0.975   \n",
       "max                    -0.932                  -0.929                  -0.928   \n",
       "\n",
       "       r2_keras_loss_epoch_87  r2_keras_loss_epoch_88  r2_keras_loss_epoch_89  \\\n",
       "count               47195.000               46965.000               46709.000   \n",
       "mean                   -0.979                  -0.980                  -0.980   \n",
       "std                     0.007                   0.006                   0.006   \n",
       "min                    -0.996                  -0.995                  -0.996   \n",
       "25%                    -0.984                  -0.984                  -0.984   \n",
       "50%                    -0.980                  -0.980                  -0.981   \n",
       "75%                    -0.976                  -0.976                  -0.976   \n",
       "max                    -0.933                  -0.932                  -0.933   \n",
       "\n",
       "       r2_keras_loss_epoch_90  r2_keras_loss_epoch_91  r2_keras_loss_epoch_92  \\\n",
       "count               46440.000               46166.000               45899.000   \n",
       "mean                   -0.980                  -0.980                  -0.980   \n",
       "std                     0.006                   0.006                   0.006   \n",
       "min                    -0.996                  -0.996                  -0.996   \n",
       "25%                    -0.985                  -0.985                  -0.985   \n",
       "50%                    -0.981                  -0.981                  -0.981   \n",
       "75%                    -0.976                  -0.976                  -0.977   \n",
       "max                    -0.935                  -0.936                  -0.936   \n",
       "\n",
       "       r2_keras_loss_epoch_93  r2_keras_loss_epoch_94  r2_keras_loss_epoch_95  \\\n",
       "count               45611.000               45339.000               45037.000   \n",
       "mean                   -0.980                  -0.981                  -0.981   \n",
       "std                     0.006                   0.006                   0.006   \n",
       "min                    -0.996                  -0.996                  -0.996   \n",
       "25%                    -0.985                  -0.985                  -0.985   \n",
       "50%                    -0.981                  -0.981                  -0.981   \n",
       "75%                    -0.977                  -0.977                  -0.977   \n",
       "max                    -0.932                  -0.939                  -0.934   \n",
       "\n",
       "       r2_keras_loss_epoch_96  r2_keras_loss_epoch_97  r2_keras_loss_epoch_98  \\\n",
       "count               44714.000               44353.000               44009.000   \n",
       "mean                   -0.981                  -0.981                  -0.981   \n",
       "std                     0.006                   0.006                   0.006   \n",
       "min                    -0.996                  -0.996                  -0.996   \n",
       "25%                    -0.985                  -0.985                  -0.986   \n",
       "50%                    -0.982                  -0.982                  -0.982   \n",
       "75%                    -0.977                  -0.977                  -0.978   \n",
       "max                    -0.936                  -0.937                  -0.940   \n",
       "\n",
       "       r2_keras_loss_epoch_99  r2_keras_loss_epoch_100  \\\n",
       "count               43657.000                43314.000   \n",
       "mean                   -0.981                   -0.981   \n",
       "std                     0.006                    0.006   \n",
       "min                    -0.996                   -0.996   \n",
       "25%                    -0.986                   -0.986   \n",
       "50%                    -0.982                   -0.982   \n",
       "75%                    -0.978                   -0.978   \n",
       "max                    -0.939                   -0.940   \n",
       "\n",
       "       r2_keras_loss_epoch_101  r2_keras_loss_epoch_102  \\\n",
       "count                42929.000                42508.000   \n",
       "mean                    -0.982                   -0.982   \n",
       "std                      0.006                    0.006   \n",
       "min                     -0.996                   -0.996   \n",
       "25%                     -0.986                   -0.986   \n",
       "50%                     -0.982                   -0.982   \n",
       "75%                     -0.978                   -0.978   \n",
       "max                     -0.941                   -0.941   \n",
       "\n",
       "       r2_keras_loss_epoch_103  r2_keras_loss_epoch_104  \\\n",
       "count                42093.000                41659.000   \n",
       "mean                    -0.982                   -0.982   \n",
       "std                      0.006                    0.006   \n",
       "min                     -0.996                   -0.996   \n",
       "25%                     -0.986                   -0.986   \n",
       "50%                     -0.983                   -0.983   \n",
       "75%                     -0.978                   -0.979   \n",
       "max                     -0.939                   -0.938   \n",
       "\n",
       "       r2_keras_loss_epoch_105  r2_keras_loss_epoch_106  \\\n",
       "count                41237.000                40826.000   \n",
       "mean                    -0.982                   -0.982   \n",
       "std                      0.006                    0.006   \n",
       "min                     -0.996                   -0.996   \n",
       "25%                     -0.986                   -0.986   \n",
       "50%                     -0.983                   -0.983   \n",
       "75%                     -0.979                   -0.979   \n",
       "max                     -0.942                   -0.940   \n",
       "\n",
       "       r2_keras_loss_epoch_107  r2_keras_loss_epoch_108  \\\n",
       "count                40392.000                39922.000   \n",
       "mean                    -0.982                   -0.983   \n",
       "std                      0.006                    0.006   \n",
       "min                     -0.996                   -0.997   \n",
       "25%                     -0.987                   -0.987   \n",
       "50%                     -0.983                   -0.983   \n",
       "75%                     -0.979                   -0.979   \n",
       "max                     -0.941                   -0.940   \n",
       "\n",
       "       r2_keras_loss_epoch_109  r2_keras_loss_epoch_110  \\\n",
       "count                39426.000                38899.000   \n",
       "mean                    -0.983                   -0.983   \n",
       "std                      0.006                    0.006   \n",
       "min                     -0.996                   -0.997   \n",
       "25%                     -0.987                   -0.987   \n",
       "50%                     -0.983                   -0.983   \n",
       "75%                     -0.979                   -0.980   \n",
       "max                     -0.942                   -0.942   \n",
       "\n",
       "       r2_keras_loss_epoch_111  r2_keras_loss_epoch_112  \\\n",
       "count                38437.000                37935.000   \n",
       "mean                    -0.983                   -0.983   \n",
       "std                      0.006                    0.006   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.987                   -0.987   \n",
       "50%                     -0.984                   -0.984   \n",
       "75%                     -0.980                   -0.980   \n",
       "max                     -0.941                   -0.941   \n",
       "\n",
       "       r2_keras_loss_epoch_113  r2_keras_loss_epoch_114  \\\n",
       "count                37418.000                36939.000   \n",
       "mean                    -0.983                   -0.983   \n",
       "std                      0.006                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.987                   -0.987   \n",
       "50%                     -0.984                   -0.984   \n",
       "75%                     -0.980                   -0.980   \n",
       "max                     -0.940                   -0.944   \n",
       "\n",
       "       r2_keras_loss_epoch_115  r2_keras_loss_epoch_116  \\\n",
       "count                36417.000                35913.000   \n",
       "mean                    -0.983                   -0.983   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.987                   -0.987   \n",
       "50%                     -0.984                   -0.984   \n",
       "75%                     -0.980                   -0.980   \n",
       "max                     -0.944                   -0.943   \n",
       "\n",
       "       r2_keras_loss_epoch_117  r2_keras_loss_epoch_118  \\\n",
       "count                35372.000                34824.000   \n",
       "mean                    -0.984                   -0.984   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.987                   -0.988   \n",
       "50%                     -0.984                   -0.984   \n",
       "75%                     -0.980                   -0.981   \n",
       "max                     -0.945                   -0.944   \n",
       "\n",
       "       r2_keras_loss_epoch_119  r2_keras_loss_epoch_120  \\\n",
       "count                34290.000                33739.000   \n",
       "mean                    -0.984                   -0.984   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.988                   -0.988   \n",
       "50%                     -0.984                   -0.985   \n",
       "75%                     -0.981                   -0.981   \n",
       "max                     -0.945                   -0.946   \n",
       "\n",
       "       r2_keras_loss_epoch_121  r2_keras_loss_epoch_122  \\\n",
       "count                33162.000                32587.000   \n",
       "mean                    -0.984                   -0.984   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.988                   -0.988   \n",
       "50%                     -0.985                   -0.985   \n",
       "75%                     -0.981                   -0.981   \n",
       "max                     -0.945                   -0.947   \n",
       "\n",
       "       r2_keras_loss_epoch_123  r2_keras_loss_epoch_124  \\\n",
       "count                32037.000                31435.000   \n",
       "mean                    -0.984                   -0.984   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.988                   -0.988   \n",
       "50%                     -0.985                   -0.985   \n",
       "75%                     -0.981                   -0.981   \n",
       "max                     -0.946                   -0.946   \n",
       "\n",
       "       r2_keras_loss_epoch_125  r2_keras_loss_epoch_126  \\\n",
       "count                30861.000                30298.000   \n",
       "mean                    -0.984                   -0.985   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.988                   -0.988   \n",
       "50%                     -0.985                   -0.985   \n",
       "75%                     -0.981                   -0.982   \n",
       "max                     -0.947                   -0.948   \n",
       "\n",
       "       r2_keras_loss_epoch_127  r2_keras_loss_epoch_128  \\\n",
       "count                29751.000                29196.000   \n",
       "mean                    -0.985                   -0.985   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.988                   -0.988   \n",
       "50%                     -0.985                   -0.985   \n",
       "75%                     -0.982                   -0.982   \n",
       "max                     -0.946                   -0.946   \n",
       "\n",
       "       r2_keras_loss_epoch_129  r2_keras_loss_epoch_130  \\\n",
       "count                28593.000                28022.000   \n",
       "mean                    -0.985                   -0.985   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.988                   -0.989   \n",
       "50%                     -0.985                   -0.985   \n",
       "75%                     -0.982                   -0.982   \n",
       "max                     -0.949                   -0.947   \n",
       "\n",
       "       r2_keras_loss_epoch_131  r2_keras_loss_epoch_132  \\\n",
       "count                27441.000                26875.000   \n",
       "mean                    -0.985                   -0.985   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.989                   -0.989   \n",
       "50%                     -0.986                   -0.986   \n",
       "75%                     -0.982                   -0.982   \n",
       "max                     -0.946                   -0.946   \n",
       "\n",
       "       r2_keras_loss_epoch_133  r2_keras_loss_epoch_134  \\\n",
       "count                26276.000                25731.000   \n",
       "mean                    -0.985                   -0.985   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.989                   -0.989   \n",
       "50%                     -0.986                   -0.986   \n",
       "75%                     -0.982                   -0.982   \n",
       "max                     -0.947                   -0.951   \n",
       "\n",
       "       r2_keras_loss_epoch_135  r2_keras_loss_epoch_136  \\\n",
       "count                25168.000                24625.000   \n",
       "mean                    -0.985                   -0.985   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.989                   -0.989   \n",
       "50%                     -0.986                   -0.986   \n",
       "75%                     -0.982                   -0.983   \n",
       "max                     -0.951                   -0.950   \n",
       "\n",
       "       r2_keras_loss_epoch_137  r2_keras_loss_epoch_138  \\\n",
       "count                24089.000                23560.000   \n",
       "mean                    -0.986                   -0.986   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.989                   -0.989   \n",
       "50%                     -0.986                   -0.986   \n",
       "75%                     -0.983                   -0.983   \n",
       "max                     -0.949                   -0.949   \n",
       "\n",
       "       r2_keras_loss_epoch_139  r2_keras_loss_epoch_140  \\\n",
       "count                22983.000                22420.000   \n",
       "mean                    -0.986                   -0.986   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.989                   -0.989   \n",
       "50%                     -0.986                   -0.986   \n",
       "75%                     -0.983                   -0.983   \n",
       "max                     -0.951                   -0.952   \n",
       "\n",
       "       r2_keras_loss_epoch_141  r2_keras_loss_epoch_142  \\\n",
       "count                21915.000                21414.000   \n",
       "mean                    -0.986                   -0.986   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.989                   -0.989   \n",
       "50%                     -0.986                   -0.987   \n",
       "75%                     -0.983                   -0.983   \n",
       "max                     -0.948                   -0.952   \n",
       "\n",
       "       r2_keras_loss_epoch_143  r2_keras_loss_epoch_144  \\\n",
       "count                20861.000                20337.000   \n",
       "mean                    -0.986                   -0.986   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.989                   -0.989   \n",
       "50%                     -0.987                   -0.987   \n",
       "75%                     -0.983                   -0.983   \n",
       "max                     -0.951                   -0.951   \n",
       "\n",
       "       r2_keras_loss_epoch_145  r2_keras_loss_epoch_146  \\\n",
       "count                19866.000                19357.000   \n",
       "mean                    -0.986                   -0.986   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.990                   -0.990   \n",
       "50%                     -0.987                   -0.987   \n",
       "75%                     -0.983                   -0.983   \n",
       "max                     -0.949                   -0.952   \n",
       "\n",
       "       r2_keras_loss_epoch_147  r2_keras_loss_epoch_148  \\\n",
       "count                18812.000                18312.000   \n",
       "mean                    -0.986                   -0.986   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.990                   -0.990   \n",
       "50%                     -0.987                   -0.987   \n",
       "75%                     -0.984                   -0.984   \n",
       "max                     -0.957                   -0.958   \n",
       "\n",
       "       r2_keras_loss_epoch_149  r2_keras_loss_epoch_150  \\\n",
       "count                17824.000                17372.000   \n",
       "mean                    -0.986                   -0.987   \n",
       "std                      0.005                    0.005   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.990                   -0.990   \n",
       "50%                     -0.987                   -0.987   \n",
       "75%                     -0.984                   -0.984   \n",
       "max                     -0.956                   -0.956   \n",
       "\n",
       "       r2_keras_loss_epoch_151  r2_keras_loss_epoch_152  \\\n",
       "count                16916.000                16427.000   \n",
       "mean                    -0.987                   -0.987   \n",
       "std                      0.005                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.990                   -0.990   \n",
       "50%                     -0.987                   -0.987   \n",
       "75%                     -0.984                   -0.984   \n",
       "max                     -0.953                   -0.956   \n",
       "\n",
       "       r2_keras_loss_epoch_153  r2_keras_loss_epoch_154  \\\n",
       "count                15973.000                15529.000   \n",
       "mean                    -0.987                   -0.987   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.990                   -0.990   \n",
       "50%                     -0.987                   -0.987   \n",
       "75%                     -0.984                   -0.984   \n",
       "max                     -0.956                   -0.957   \n",
       "\n",
       "       r2_keras_loss_epoch_155  r2_keras_loss_epoch_156  \\\n",
       "count                15086.000                14656.000   \n",
       "mean                    -0.987                   -0.987   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.997   \n",
       "25%                     -0.990                   -0.990   \n",
       "50%                     -0.987                   -0.988   \n",
       "75%                     -0.984                   -0.984   \n",
       "max                     -0.959                   -0.960   \n",
       "\n",
       "       r2_keras_loss_epoch_157  r2_keras_loss_epoch_158  \\\n",
       "count                14242.000                13836.000   \n",
       "mean                    -0.987                   -0.987   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.990                   -0.990   \n",
       "50%                     -0.988                   -0.988   \n",
       "75%                     -0.984                   -0.985   \n",
       "max                     -0.960                   -0.960   \n",
       "\n",
       "       r2_keras_loss_epoch_159  r2_keras_loss_epoch_160  \\\n",
       "count                13441.000                13031.000   \n",
       "mean                    -0.987                   -0.987   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.990                   -0.990   \n",
       "50%                     -0.988                   -0.988   \n",
       "75%                     -0.985                   -0.985   \n",
       "max                     -0.959                   -0.956   \n",
       "\n",
       "       r2_keras_loss_epoch_161  r2_keras_loss_epoch_162  \\\n",
       "count                12650.000                12264.000   \n",
       "mean                    -0.987                   -0.987   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.998   \n",
       "25%                     -0.990                   -0.990   \n",
       "50%                     -0.988                   -0.988   \n",
       "75%                     -0.985                   -0.985   \n",
       "max                     -0.960                   -0.959   \n",
       "\n",
       "       r2_keras_loss_epoch_163  r2_keras_loss_epoch_164  \\\n",
       "count                11884.000                11508.000   \n",
       "mean                    -0.987                   -0.987   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.988                   -0.988   \n",
       "75%                     -0.985                   -0.985   \n",
       "max                     -0.960                   -0.958   \n",
       "\n",
       "       r2_keras_loss_epoch_165  r2_keras_loss_epoch_166  \\\n",
       "count                11163.000                10813.000   \n",
       "mean                    -0.988                   -0.988   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.988                   -0.988   \n",
       "75%                     -0.985                   -0.985   \n",
       "max                     -0.961                   -0.961   \n",
       "\n",
       "       r2_keras_loss_epoch_167  r2_keras_loss_epoch_168  \\\n",
       "count                10465.000                10126.000   \n",
       "mean                    -0.988                   -0.988   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.988                   -0.988   \n",
       "75%                     -0.985                   -0.985   \n",
       "max                     -0.960                   -0.960   \n",
       "\n",
       "       r2_keras_loss_epoch_169  r2_keras_loss_epoch_170  \\\n",
       "count                 9791.000                 9460.000   \n",
       "mean                    -0.988                   -0.988   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.988                   -0.988   \n",
       "75%                     -0.985                   -0.985   \n",
       "max                     -0.957                   -0.959   \n",
       "\n",
       "       r2_keras_loss_epoch_171  r2_keras_loss_epoch_172  \\\n",
       "count                 9137.000                 8842.000   \n",
       "mean                    -0.988                   -0.988   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.988                   -0.988   \n",
       "75%                     -0.986                   -0.986   \n",
       "max                     -0.961                   -0.961   \n",
       "\n",
       "       r2_keras_loss_epoch_173  r2_keras_loss_epoch_174  \\\n",
       "count                 8512.000                 8217.000   \n",
       "mean                    -0.988                   -0.988   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.989                   -0.989   \n",
       "75%                     -0.986                   -0.986   \n",
       "max                     -0.963                   -0.964   \n",
       "\n",
       "       r2_keras_loss_epoch_175  r2_keras_loss_epoch_176  \\\n",
       "count                 7929.000                 7644.000   \n",
       "mean                    -0.988                   -0.988   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.989                   -0.989   \n",
       "75%                     -0.986                   -0.986   \n",
       "max                     -0.964                   -0.963   \n",
       "\n",
       "       r2_keras_loss_epoch_177  r2_keras_loss_epoch_178  \\\n",
       "count                 7362.000                 7087.000   \n",
       "mean                    -0.988                   -0.988   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.989                   -0.989   \n",
       "75%                     -0.986                   -0.986   \n",
       "max                     -0.963                   -0.964   \n",
       "\n",
       "       r2_keras_loss_epoch_179  r2_keras_loss_epoch_180  \\\n",
       "count                 6863.000                 6614.000   \n",
       "mean                    -0.988                   -0.988   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.989                   -0.989   \n",
       "75%                     -0.986                   -0.986   \n",
       "max                     -0.964                   -0.964   \n",
       "\n",
       "       r2_keras_loss_epoch_181  r2_keras_loss_epoch_182  \\\n",
       "count                 6328.000                 6076.000   \n",
       "mean                    -0.988                   -0.988   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.989                   -0.989   \n",
       "75%                     -0.986                   -0.986   \n",
       "max                     -0.964                   -0.965   \n",
       "\n",
       "       r2_keras_loss_epoch_183  r2_keras_loss_epoch_184  \\\n",
       "count                 5826.000                 5606.000   \n",
       "mean                    -0.988                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.991                   -0.991   \n",
       "50%                     -0.989                   -0.989   \n",
       "75%                     -0.986                   -0.986   \n",
       "max                     -0.963                   -0.965   \n",
       "\n",
       "       r2_keras_loss_epoch_185  r2_keras_loss_epoch_186  \\\n",
       "count                 5383.000                 5167.000   \n",
       "mean                    -0.989                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.991                   -0.992   \n",
       "50%                     -0.989                   -0.989   \n",
       "75%                     -0.986                   -0.986   \n",
       "max                     -0.964                   -0.964   \n",
       "\n",
       "       r2_keras_loss_epoch_187  r2_keras_loss_epoch_188  \\\n",
       "count                 4971.000                 4765.000   \n",
       "mean                    -0.989                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.989                   -0.989   \n",
       "75%                     -0.986                   -0.986   \n",
       "max                     -0.965                   -0.962   \n",
       "\n",
       "       r2_keras_loss_epoch_189  r2_keras_loss_epoch_190  \\\n",
       "count                 4597.000                 4430.000   \n",
       "mean                    -0.989                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.989                   -0.989   \n",
       "75%                     -0.986                   -0.987   \n",
       "max                     -0.963                   -0.966   \n",
       "\n",
       "       r2_keras_loss_epoch_191  r2_keras_loss_epoch_192  \\\n",
       "count                 4264.000                 4085.000   \n",
       "mean                    -0.989                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.989                   -0.989   \n",
       "75%                     -0.987                   -0.987   \n",
       "max                     -0.964                   -0.967   \n",
       "\n",
       "       r2_keras_loss_epoch_193  r2_keras_loss_epoch_194  \\\n",
       "count                 3910.000                 3758.000   \n",
       "mean                    -0.989                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.987                   -0.987   \n",
       "max                     -0.966                   -0.968   \n",
       "\n",
       "       r2_keras_loss_epoch_195  r2_keras_loss_epoch_196  \\\n",
       "count                 3607.000                 3443.000   \n",
       "mean                    -0.989                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.987                   -0.987   \n",
       "max                     -0.971                   -0.972   \n",
       "\n",
       "       r2_keras_loss_epoch_197  r2_keras_loss_epoch_198  \\\n",
       "count                 3313.000                 3155.000   \n",
       "mean                    -0.989                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.987                   -0.987   \n",
       "max                     -0.972                   -0.972   \n",
       "\n",
       "       r2_keras_loss_epoch_199  r2_keras_loss_epoch_200  \\\n",
       "count                 3020.000                 2892.000   \n",
       "mean                    -0.989                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.987                   -0.987   \n",
       "max                     -0.972                   -0.972   \n",
       "\n",
       "       r2_keras_loss_epoch_201  r2_keras_loss_epoch_202  \\\n",
       "count                 2746.000                 2641.000   \n",
       "mean                    -0.989                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.987                   -0.987   \n",
       "max                     -0.971                   -0.973   \n",
       "\n",
       "       r2_keras_loss_epoch_203  r2_keras_loss_epoch_204  \\\n",
       "count                 2532.000                 2412.000   \n",
       "mean                    -0.989                   -0.989   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.987                   -0.987   \n",
       "max                     -0.973                   -0.971   \n",
       "\n",
       "       r2_keras_loss_epoch_205  r2_keras_loss_epoch_206  \\\n",
       "count                 2290.000                 2169.000   \n",
       "mean                    -0.989                   -0.990   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.987                   -0.987   \n",
       "max                     -0.972                   -0.973   \n",
       "\n",
       "       r2_keras_loss_epoch_207  r2_keras_loss_epoch_208  \\\n",
       "count                 2054.000                 1954.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.987                   -0.988   \n",
       "max                     -0.973                   -0.972   \n",
       "\n",
       "       r2_keras_loss_epoch_209  r2_keras_loss_epoch_210  \\\n",
       "count                 1872.000                 1772.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.974                   -0.973   \n",
       "\n",
       "       r2_keras_loss_epoch_211  r2_keras_loss_epoch_212  \\\n",
       "count                 1675.000                 1599.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.004                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.974                   -0.974   \n",
       "\n",
       "       r2_keras_loss_epoch_213  r2_keras_loss_epoch_214  \\\n",
       "count                 1532.000                 1449.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.975                   -0.974   \n",
       "\n",
       "       r2_keras_loss_epoch_215  r2_keras_loss_epoch_216  \\\n",
       "count                 1385.000                 1327.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.990                   -0.990   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.974                   -0.975   \n",
       "\n",
       "       r2_keras_loss_epoch_217  r2_keras_loss_epoch_218  \\\n",
       "count                 1262.000                 1198.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.992                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.974                   -0.975   \n",
       "\n",
       "       r2_keras_loss_epoch_219  r2_keras_loss_epoch_220  \\\n",
       "count                 1123.000                 1062.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.973                   -0.975   \n",
       "\n",
       "       r2_keras_loss_epoch_221  r2_keras_loss_epoch_222  \\\n",
       "count                 1009.000                  955.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.975                   -0.978   \n",
       "\n",
       "       r2_keras_loss_epoch_223  r2_keras_loss_epoch_224  \\\n",
       "count                  913.000                  871.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.977                   -0.977   \n",
       "\n",
       "       r2_keras_loss_epoch_225  r2_keras_loss_epoch_226  \\\n",
       "count                  818.000                  762.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.977                   -0.977   \n",
       "\n",
       "       r2_keras_loss_epoch_227  r2_keras_loss_epoch_228  \\\n",
       "count                  729.000                  687.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.977                   -0.978   \n",
       "\n",
       "       r2_keras_loss_epoch_229  r2_keras_loss_epoch_230  \\\n",
       "count                  653.000                  621.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.988   \n",
       "max                     -0.980                   -0.980   \n",
       "\n",
       "       r2_keras_loss_epoch_231  r2_keras_loss_epoch_232  \\\n",
       "count                  588.000                  556.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.989   \n",
       "max                     -0.980                   -0.979   \n",
       "\n",
       "       r2_keras_loss_epoch_233  r2_keras_loss_epoch_234  \\\n",
       "count                  526.000                  501.000   \n",
       "mean                    -0.990                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.989   \n",
       "max                     -0.979                   -0.980   \n",
       "\n",
       "       r2_keras_loss_epoch_235  r2_keras_loss_epoch_236  \\\n",
       "count                  476.000                  448.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.980                   -0.979   \n",
       "\n",
       "       r2_keras_loss_epoch_237  r2_keras_loss_epoch_238  \\\n",
       "count                  424.000                  392.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.979                   -0.980   \n",
       "\n",
       "       r2_keras_loss_epoch_239  r2_keras_loss_epoch_240  \\\n",
       "count                  374.000                  357.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.980                   -0.980   \n",
       "\n",
       "       r2_keras_loss_epoch_241  r2_keras_loss_epoch_242  \\\n",
       "count                  333.000                  307.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.980                   -0.981   \n",
       "\n",
       "       r2_keras_loss_epoch_243  r2_keras_loss_epoch_244  \\\n",
       "count                  290.000                  271.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.980                   -0.981   \n",
       "\n",
       "       r2_keras_loss_epoch_245  r2_keras_loss_epoch_246  \\\n",
       "count                  252.000                  234.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.989   \n",
       "max                     -0.981                   -0.981   \n",
       "\n",
       "       r2_keras_loss_epoch_247  r2_keras_loss_epoch_248  \\\n",
       "count                  220.000                  208.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.988   \n",
       "max                     -0.982                   -0.981   \n",
       "\n",
       "       r2_keras_loss_epoch_249  r2_keras_loss_epoch_250  \\\n",
       "count                  197.000                  184.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.982                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_251  r2_keras_loss_epoch_252  \\\n",
       "count                  178.000                  169.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.988   \n",
       "max                     -0.980                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_253  r2_keras_loss_epoch_254  \\\n",
       "count                  158.000                  146.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.988   \n",
       "max                     -0.982                   -0.979   \n",
       "\n",
       "       r2_keras_loss_epoch_255  r2_keras_loss_epoch_256  \\\n",
       "count                  139.000                  129.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.982                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_257  r2_keras_loss_epoch_258  \\\n",
       "count                  122.000                  113.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.997   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.982                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_259  r2_keras_loss_epoch_260  \\\n",
       "count                  107.000                   98.000   \n",
       "mean                    -0.991                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.997                   -0.995   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.982                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_261  r2_keras_loss_epoch_262  \\\n",
       "count                   95.000                   88.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.996                   -0.996   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.982                   -0.983   \n",
       "\n",
       "       r2_keras_loss_epoch_263  r2_keras_loss_epoch_264  \\\n",
       "count                   78.000                   72.000   \n",
       "mean                    -0.990                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.996                   -0.995   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.988   \n",
       "max                     -0.982                   -0.983   \n",
       "\n",
       "       r2_keras_loss_epoch_265  r2_keras_loss_epoch_266  \\\n",
       "count                   69.000                   66.000   \n",
       "mean                    -0.990                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.995                   -0.996   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.988                   -0.989   \n",
       "max                     -0.981                   -0.981   \n",
       "\n",
       "       r2_keras_loss_epoch_267  r2_keras_loss_epoch_268  \\\n",
       "count                   62.000                   61.000   \n",
       "mean                    -0.991                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.995                   -0.995   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.981                   -0.981   \n",
       "\n",
       "       r2_keras_loss_epoch_269  r2_keras_loss_epoch_270  \\\n",
       "count                   54.000                   50.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.995                   -0.995   \n",
       "25%                     -0.993                   -0.992   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.983                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_271  r2_keras_loss_epoch_272  \\\n",
       "count                   47.000                   45.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.996                   -0.995   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.992   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.982                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_273  r2_keras_loss_epoch_274  \\\n",
       "count                   38.000                   34.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.995                   -0.995   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.990                   -0.990   \n",
       "max                     -0.983                   -0.983   \n",
       "\n",
       "       r2_keras_loss_epoch_275  r2_keras_loss_epoch_276  \\\n",
       "count                   33.000                   30.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.995                   -0.995   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.992   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.983                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_277  r2_keras_loss_epoch_278  \\\n",
       "count                   26.000                   26.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.995                   -0.995   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.983                   -0.983   \n",
       "\n",
       "       r2_keras_loss_epoch_279  r2_keras_loss_epoch_280  \\\n",
       "count                   25.000                   25.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.995                   -0.995   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.982                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_281  r2_keras_loss_epoch_282  \\\n",
       "count                   22.000                   20.000   \n",
       "mean                    -0.991                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.995                   -0.994   \n",
       "25%                     -0.993                   -0.992   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.988   \n",
       "max                     -0.982                   -0.983   \n",
       "\n",
       "       r2_keras_loss_epoch_283  r2_keras_loss_epoch_284  \\\n",
       "count                   18.000                   17.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.994                   -0.994   \n",
       "25%                     -0.993                   -0.992   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.981                   -0.981   \n",
       "\n",
       "       r2_keras_loss_epoch_285  r2_keras_loss_epoch_286  \\\n",
       "count                   17.000                   16.000   \n",
       "mean                    -0.991                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.994                   -0.994   \n",
       "25%                     -0.993                   -0.992   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.983                   -0.983   \n",
       "\n",
       "       r2_keras_loss_epoch_287  r2_keras_loss_epoch_288  \\\n",
       "count                   14.000                   14.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.994                   -0.994   \n",
       "25%                     -0.992                   -0.993   \n",
       "50%                     -0.990                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.983                   -0.984   \n",
       "\n",
       "       r2_keras_loss_epoch_289  r2_keras_loss_epoch_290  \\\n",
       "count                   13.000                   13.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.994                   -0.994   \n",
       "25%                     -0.992                   -0.992   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.983                   -0.984   \n",
       "\n",
       "       r2_keras_loss_epoch_291  r2_keras_loss_epoch_292  \\\n",
       "count                   11.000                   11.000   \n",
       "mean                    -0.990                   -0.991   \n",
       "std                      0.003                    0.003   \n",
       "min                     -0.994                   -0.994   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.992   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.984                   -0.983   \n",
       "\n",
       "       r2_keras_loss_epoch_293  r2_keras_loss_epoch_294  \\\n",
       "count                    9.000                    8.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.004                    0.004   \n",
       "min                     -0.993                   -0.993   \n",
       "25%                     -0.992                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.988   \n",
       "max                     -0.982                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_295  r2_keras_loss_epoch_296  \\\n",
       "count                    8.000                    8.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.003                    0.004   \n",
       "min                     -0.993                   -0.993   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.991                   -0.991   \n",
       "75%                     -0.989                   -0.989   \n",
       "max                     -0.984                   -0.982   \n",
       "\n",
       "       r2_keras_loss_epoch_297  r2_keras_loss_epoch_298  \\\n",
       "count                    7.000                    7.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.003                    0.002   \n",
       "min                     -0.993                   -0.993   \n",
       "25%                     -0.993                   -0.992   \n",
       "50%                     -0.991                   -0.992   \n",
       "75%                     -0.989                   -0.990   \n",
       "max                     -0.987                   -0.987   \n",
       "\n",
       "       r2_keras_loss_epoch_299  r2_keras_loss_epoch_300  \\\n",
       "count                    7.000                    7.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.002                    0.002   \n",
       "min                     -0.993                   -0.993   \n",
       "25%                     -0.992                   -0.993   \n",
       "50%                     -0.991                   -0.992   \n",
       "75%                     -0.989                   -0.990   \n",
       "max                     -0.988                   -0.988   \n",
       "\n",
       "       r2_keras_loss_epoch_301  r2_keras_loss_epoch_302  \\\n",
       "count                    7.000                    6.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.002                    0.003   \n",
       "min                     -0.993                   -0.994   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.992                   -0.992   \n",
       "75%                     -0.990                   -0.989   \n",
       "max                     -0.987                   -0.987   \n",
       "\n",
       "       r2_keras_loss_epoch_303  r2_keras_loss_epoch_304  \\\n",
       "count                    4.000                    4.000   \n",
       "mean                    -0.992                   -0.992   \n",
       "std                      0.002                    0.002   \n",
       "min                     -0.993                   -0.993   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.993                   -0.993   \n",
       "75%                     -0.992                   -0.991   \n",
       "max                     -0.989                   -0.988   \n",
       "\n",
       "       r2_keras_loss_epoch_305  r2_keras_loss_epoch_306  \\\n",
       "count                    3.000                    2.000   \n",
       "mean                    -0.993                   -0.993   \n",
       "std                      0.000                    0.001   \n",
       "min                     -0.993                   -0.993   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.993                   -0.993   \n",
       "75%                     -0.993                   -0.993   \n",
       "max                     -0.993                   -0.992   \n",
       "\n",
       "       r2_keras_loss_epoch_307  r2_keras_loss_epoch_308  \\\n",
       "count                    2.000                    2.000   \n",
       "mean                    -0.993                   -0.993   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.993                   -0.993   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.993                   -0.993   \n",
       "75%                     -0.993                   -0.993   \n",
       "max                     -0.993                   -0.993   \n",
       "\n",
       "       r2_keras_loss_epoch_309  r2_keras_loss_epoch_310  \\\n",
       "count                    2.000                    2.000   \n",
       "mean                    -0.993                   -0.993   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.993                   -0.993   \n",
       "25%                     -0.993                   -0.993   \n",
       "50%                     -0.993                   -0.993   \n",
       "75%                     -0.993                   -0.993   \n",
       "max                     -0.993                   -0.993   \n",
       "\n",
       "       r2_keras_loss_epoch_311  r2_keras_loss_epoch_312  \\\n",
       "count                    2.000                    1.000   \n",
       "mean                    -0.992                   -0.993   \n",
       "std                      0.000                      NaN   \n",
       "min                     -0.993                   -0.993   \n",
       "25%                     -0.992                   -0.993   \n",
       "50%                     -0.992                   -0.993   \n",
       "75%                     -0.992                   -0.993   \n",
       "max                     -0.992                   -0.993   \n",
       "\n",
       "       r2_keras_loss_epoch_313  \n",
       "count                    1.000  \n",
       "mean                    -0.993  \n",
       "std                        NaN  \n",
       "min                     -0.993  \n",
       "25%                     -0.993  \n",
       "50%                     -0.993  \n",
       "75%                     -0.993  \n",
       "max                     -0.993  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>val_r2_keras_loss_epoch_1</th>\n",
       "      <th>val_r2_keras_loss_epoch_2</th>\n",
       "      <th>val_r2_keras_loss_epoch_3</th>\n",
       "      <th>val_r2_keras_loss_epoch_4</th>\n",
       "      <th>val_r2_keras_loss_epoch_5</th>\n",
       "      <th>val_r2_keras_loss_epoch_6</th>\n",
       "      <th>val_r2_keras_loss_epoch_7</th>\n",
       "      <th>val_r2_keras_loss_epoch_8</th>\n",
       "      <th>val_r2_keras_loss_epoch_9</th>\n",
       "      <th>val_r2_keras_loss_epoch_10</th>\n",
       "      <th>val_r2_keras_loss_epoch_11</th>\n",
       "      <th>val_r2_keras_loss_epoch_12</th>\n",
       "      <th>val_r2_keras_loss_epoch_13</th>\n",
       "      <th>val_r2_keras_loss_epoch_14</th>\n",
       "      <th>val_r2_keras_loss_epoch_15</th>\n",
       "      <th>val_r2_keras_loss_epoch_16</th>\n",
       "      <th>val_r2_keras_loss_epoch_17</th>\n",
       "      <th>val_r2_keras_loss_epoch_18</th>\n",
       "      <th>val_r2_keras_loss_epoch_19</th>\n",
       "      <th>val_r2_keras_loss_epoch_20</th>\n",
       "      <th>val_r2_keras_loss_epoch_21</th>\n",
       "      <th>val_r2_keras_loss_epoch_22</th>\n",
       "      <th>val_r2_keras_loss_epoch_23</th>\n",
       "      <th>val_r2_keras_loss_epoch_24</th>\n",
       "      <th>val_r2_keras_loss_epoch_25</th>\n",
       "      <th>val_r2_keras_loss_epoch_26</th>\n",
       "      <th>val_r2_keras_loss_epoch_27</th>\n",
       "      <th>val_r2_keras_loss_epoch_28</th>\n",
       "      <th>val_r2_keras_loss_epoch_29</th>\n",
       "      <th>val_r2_keras_loss_epoch_30</th>\n",
       "      <th>val_r2_keras_loss_epoch_31</th>\n",
       "      <th>val_r2_keras_loss_epoch_32</th>\n",
       "      <th>val_r2_keras_loss_epoch_33</th>\n",
       "      <th>val_r2_keras_loss_epoch_34</th>\n",
       "      <th>val_r2_keras_loss_epoch_35</th>\n",
       "      <th>val_r2_keras_loss_epoch_36</th>\n",
       "      <th>val_r2_keras_loss_epoch_37</th>\n",
       "      <th>val_r2_keras_loss_epoch_38</th>\n",
       "      <th>val_r2_keras_loss_epoch_39</th>\n",
       "      <th>val_r2_keras_loss_epoch_40</th>\n",
       "      <th>val_r2_keras_loss_epoch_41</th>\n",
       "      <th>val_r2_keras_loss_epoch_42</th>\n",
       "      <th>val_r2_keras_loss_epoch_43</th>\n",
       "      <th>val_r2_keras_loss_epoch_44</th>\n",
       "      <th>val_r2_keras_loss_epoch_45</th>\n",
       "      <th>val_r2_keras_loss_epoch_46</th>\n",
       "      <th>val_r2_keras_loss_epoch_47</th>\n",
       "      <th>val_r2_keras_loss_epoch_48</th>\n",
       "      <th>val_r2_keras_loss_epoch_49</th>\n",
       "      <th>val_r2_keras_loss_epoch_50</th>\n",
       "      <th>val_r2_keras_loss_epoch_51</th>\n",
       "      <th>val_r2_keras_loss_epoch_52</th>\n",
       "      <th>val_r2_keras_loss_epoch_53</th>\n",
       "      <th>val_r2_keras_loss_epoch_54</th>\n",
       "      <th>val_r2_keras_loss_epoch_55</th>\n",
       "      <th>val_r2_keras_loss_epoch_56</th>\n",
       "      <th>val_r2_keras_loss_epoch_57</th>\n",
       "      <th>val_r2_keras_loss_epoch_58</th>\n",
       "      <th>val_r2_keras_loss_epoch_59</th>\n",
       "      <th>val_r2_keras_loss_epoch_60</th>\n",
       "      <th>val_r2_keras_loss_epoch_61</th>\n",
       "      <th>val_r2_keras_loss_epoch_62</th>\n",
       "      <th>val_r2_keras_loss_epoch_63</th>\n",
       "      <th>val_r2_keras_loss_epoch_64</th>\n",
       "      <th>val_r2_keras_loss_epoch_65</th>\n",
       "      <th>val_r2_keras_loss_epoch_66</th>\n",
       "      <th>val_r2_keras_loss_epoch_67</th>\n",
       "      <th>val_r2_keras_loss_epoch_68</th>\n",
       "      <th>val_r2_keras_loss_epoch_69</th>\n",
       "      <th>val_r2_keras_loss_epoch_70</th>\n",
       "      <th>val_r2_keras_loss_epoch_71</th>\n",
       "      <th>val_r2_keras_loss_epoch_72</th>\n",
       "      <th>val_r2_keras_loss_epoch_73</th>\n",
       "      <th>val_r2_keras_loss_epoch_74</th>\n",
       "      <th>val_r2_keras_loss_epoch_75</th>\n",
       "      <th>val_r2_keras_loss_epoch_76</th>\n",
       "      <th>val_r2_keras_loss_epoch_77</th>\n",
       "      <th>val_r2_keras_loss_epoch_78</th>\n",
       "      <th>val_r2_keras_loss_epoch_79</th>\n",
       "      <th>val_r2_keras_loss_epoch_80</th>\n",
       "      <th>val_r2_keras_loss_epoch_81</th>\n",
       "      <th>val_r2_keras_loss_epoch_82</th>\n",
       "      <th>val_r2_keras_loss_epoch_83</th>\n",
       "      <th>val_r2_keras_loss_epoch_84</th>\n",
       "      <th>val_r2_keras_loss_epoch_85</th>\n",
       "      <th>val_r2_keras_loss_epoch_86</th>\n",
       "      <th>val_r2_keras_loss_epoch_87</th>\n",
       "      <th>val_r2_keras_loss_epoch_88</th>\n",
       "      <th>val_r2_keras_loss_epoch_89</th>\n",
       "      <th>val_r2_keras_loss_epoch_90</th>\n",
       "      <th>val_r2_keras_loss_epoch_91</th>\n",
       "      <th>val_r2_keras_loss_epoch_92</th>\n",
       "      <th>val_r2_keras_loss_epoch_93</th>\n",
       "      <th>val_r2_keras_loss_epoch_94</th>\n",
       "      <th>val_r2_keras_loss_epoch_95</th>\n",
       "      <th>val_r2_keras_loss_epoch_96</th>\n",
       "      <th>val_r2_keras_loss_epoch_97</th>\n",
       "      <th>val_r2_keras_loss_epoch_98</th>\n",
       "      <th>val_r2_keras_loss_epoch_99</th>\n",
       "      <th>val_r2_keras_loss_epoch_100</th>\n",
       "      <th>val_r2_keras_loss_epoch_101</th>\n",
       "      <th>val_r2_keras_loss_epoch_102</th>\n",
       "      <th>val_r2_keras_loss_epoch_103</th>\n",
       "      <th>val_r2_keras_loss_epoch_104</th>\n",
       "      <th>val_r2_keras_loss_epoch_105</th>\n",
       "      <th>val_r2_keras_loss_epoch_106</th>\n",
       "      <th>val_r2_keras_loss_epoch_107</th>\n",
       "      <th>val_r2_keras_loss_epoch_108</th>\n",
       "      <th>val_r2_keras_loss_epoch_109</th>\n",
       "      <th>val_r2_keras_loss_epoch_110</th>\n",
       "      <th>val_r2_keras_loss_epoch_111</th>\n",
       "      <th>val_r2_keras_loss_epoch_112</th>\n",
       "      <th>val_r2_keras_loss_epoch_113</th>\n",
       "      <th>val_r2_keras_loss_epoch_114</th>\n",
       "      <th>val_r2_keras_loss_epoch_115</th>\n",
       "      <th>val_r2_keras_loss_epoch_116</th>\n",
       "      <th>val_r2_keras_loss_epoch_117</th>\n",
       "      <th>val_r2_keras_loss_epoch_118</th>\n",
       "      <th>val_r2_keras_loss_epoch_119</th>\n",
       "      <th>val_r2_keras_loss_epoch_120</th>\n",
       "      <th>val_r2_keras_loss_epoch_121</th>\n",
       "      <th>val_r2_keras_loss_epoch_122</th>\n",
       "      <th>val_r2_keras_loss_epoch_123</th>\n",
       "      <th>val_r2_keras_loss_epoch_124</th>\n",
       "      <th>val_r2_keras_loss_epoch_125</th>\n",
       "      <th>val_r2_keras_loss_epoch_126</th>\n",
       "      <th>val_r2_keras_loss_epoch_127</th>\n",
       "      <th>val_r2_keras_loss_epoch_128</th>\n",
       "      <th>val_r2_keras_loss_epoch_129</th>\n",
       "      <th>val_r2_keras_loss_epoch_130</th>\n",
       "      <th>val_r2_keras_loss_epoch_131</th>\n",
       "      <th>val_r2_keras_loss_epoch_132</th>\n",
       "      <th>val_r2_keras_loss_epoch_133</th>\n",
       "      <th>val_r2_keras_loss_epoch_134</th>\n",
       "      <th>val_r2_keras_loss_epoch_135</th>\n",
       "      <th>val_r2_keras_loss_epoch_136</th>\n",
       "      <th>val_r2_keras_loss_epoch_137</th>\n",
       "      <th>val_r2_keras_loss_epoch_138</th>\n",
       "      <th>val_r2_keras_loss_epoch_139</th>\n",
       "      <th>val_r2_keras_loss_epoch_140</th>\n",
       "      <th>val_r2_keras_loss_epoch_141</th>\n",
       "      <th>val_r2_keras_loss_epoch_142</th>\n",
       "      <th>val_r2_keras_loss_epoch_143</th>\n",
       "      <th>val_r2_keras_loss_epoch_144</th>\n",
       "      <th>val_r2_keras_loss_epoch_145</th>\n",
       "      <th>val_r2_keras_loss_epoch_146</th>\n",
       "      <th>val_r2_keras_loss_epoch_147</th>\n",
       "      <th>val_r2_keras_loss_epoch_148</th>\n",
       "      <th>val_r2_keras_loss_epoch_149</th>\n",
       "      <th>val_r2_keras_loss_epoch_150</th>\n",
       "      <th>val_r2_keras_loss_epoch_151</th>\n",
       "      <th>val_r2_keras_loss_epoch_152</th>\n",
       "      <th>val_r2_keras_loss_epoch_153</th>\n",
       "      <th>val_r2_keras_loss_epoch_154</th>\n",
       "      <th>val_r2_keras_loss_epoch_155</th>\n",
       "      <th>val_r2_keras_loss_epoch_156</th>\n",
       "      <th>val_r2_keras_loss_epoch_157</th>\n",
       "      <th>val_r2_keras_loss_epoch_158</th>\n",
       "      <th>val_r2_keras_loss_epoch_159</th>\n",
       "      <th>val_r2_keras_loss_epoch_160</th>\n",
       "      <th>val_r2_keras_loss_epoch_161</th>\n",
       "      <th>val_r2_keras_loss_epoch_162</th>\n",
       "      <th>val_r2_keras_loss_epoch_163</th>\n",
       "      <th>val_r2_keras_loss_epoch_164</th>\n",
       "      <th>val_r2_keras_loss_epoch_165</th>\n",
       "      <th>val_r2_keras_loss_epoch_166</th>\n",
       "      <th>val_r2_keras_loss_epoch_167</th>\n",
       "      <th>val_r2_keras_loss_epoch_168</th>\n",
       "      <th>val_r2_keras_loss_epoch_169</th>\n",
       "      <th>val_r2_keras_loss_epoch_170</th>\n",
       "      <th>val_r2_keras_loss_epoch_171</th>\n",
       "      <th>val_r2_keras_loss_epoch_172</th>\n",
       "      <th>val_r2_keras_loss_epoch_173</th>\n",
       "      <th>val_r2_keras_loss_epoch_174</th>\n",
       "      <th>val_r2_keras_loss_epoch_175</th>\n",
       "      <th>val_r2_keras_loss_epoch_176</th>\n",
       "      <th>val_r2_keras_loss_epoch_177</th>\n",
       "      <th>val_r2_keras_loss_epoch_178</th>\n",
       "      <th>val_r2_keras_loss_epoch_179</th>\n",
       "      <th>val_r2_keras_loss_epoch_180</th>\n",
       "      <th>val_r2_keras_loss_epoch_181</th>\n",
       "      <th>val_r2_keras_loss_epoch_182</th>\n",
       "      <th>val_r2_keras_loss_epoch_183</th>\n",
       "      <th>val_r2_keras_loss_epoch_184</th>\n",
       "      <th>val_r2_keras_loss_epoch_185</th>\n",
       "      <th>val_r2_keras_loss_epoch_186</th>\n",
       "      <th>val_r2_keras_loss_epoch_187</th>\n",
       "      <th>val_r2_keras_loss_epoch_188</th>\n",
       "      <th>val_r2_keras_loss_epoch_189</th>\n",
       "      <th>val_r2_keras_loss_epoch_190</th>\n",
       "      <th>val_r2_keras_loss_epoch_191</th>\n",
       "      <th>val_r2_keras_loss_epoch_192</th>\n",
       "      <th>val_r2_keras_loss_epoch_193</th>\n",
       "      <th>val_r2_keras_loss_epoch_194</th>\n",
       "      <th>val_r2_keras_loss_epoch_195</th>\n",
       "      <th>val_r2_keras_loss_epoch_196</th>\n",
       "      <th>val_r2_keras_loss_epoch_197</th>\n",
       "      <th>val_r2_keras_loss_epoch_198</th>\n",
       "      <th>val_r2_keras_loss_epoch_199</th>\n",
       "      <th>val_r2_keras_loss_epoch_200</th>\n",
       "      <th>val_r2_keras_loss_epoch_201</th>\n",
       "      <th>val_r2_keras_loss_epoch_202</th>\n",
       "      <th>val_r2_keras_loss_epoch_203</th>\n",
       "      <th>val_r2_keras_loss_epoch_204</th>\n",
       "      <th>val_r2_keras_loss_epoch_205</th>\n",
       "      <th>val_r2_keras_loss_epoch_206</th>\n",
       "      <th>val_r2_keras_loss_epoch_207</th>\n",
       "      <th>val_r2_keras_loss_epoch_208</th>\n",
       "      <th>val_r2_keras_loss_epoch_209</th>\n",
       "      <th>val_r2_keras_loss_epoch_210</th>\n",
       "      <th>val_r2_keras_loss_epoch_211</th>\n",
       "      <th>val_r2_keras_loss_epoch_212</th>\n",
       "      <th>val_r2_keras_loss_epoch_213</th>\n",
       "      <th>val_r2_keras_loss_epoch_214</th>\n",
       "      <th>val_r2_keras_loss_epoch_215</th>\n",
       "      <th>val_r2_keras_loss_epoch_216</th>\n",
       "      <th>val_r2_keras_loss_epoch_217</th>\n",
       "      <th>val_r2_keras_loss_epoch_218</th>\n",
       "      <th>val_r2_keras_loss_epoch_219</th>\n",
       "      <th>val_r2_keras_loss_epoch_220</th>\n",
       "      <th>val_r2_keras_loss_epoch_221</th>\n",
       "      <th>val_r2_keras_loss_epoch_222</th>\n",
       "      <th>val_r2_keras_loss_epoch_223</th>\n",
       "      <th>val_r2_keras_loss_epoch_224</th>\n",
       "      <th>val_r2_keras_loss_epoch_225</th>\n",
       "      <th>val_r2_keras_loss_epoch_226</th>\n",
       "      <th>val_r2_keras_loss_epoch_227</th>\n",
       "      <th>val_r2_keras_loss_epoch_228</th>\n",
       "      <th>val_r2_keras_loss_epoch_229</th>\n",
       "      <th>val_r2_keras_loss_epoch_230</th>\n",
       "      <th>val_r2_keras_loss_epoch_231</th>\n",
       "      <th>val_r2_keras_loss_epoch_232</th>\n",
       "      <th>val_r2_keras_loss_epoch_233</th>\n",
       "      <th>val_r2_keras_loss_epoch_234</th>\n",
       "      <th>val_r2_keras_loss_epoch_235</th>\n",
       "      <th>val_r2_keras_loss_epoch_236</th>\n",
       "      <th>val_r2_keras_loss_epoch_237</th>\n",
       "      <th>val_r2_keras_loss_epoch_238</th>\n",
       "      <th>val_r2_keras_loss_epoch_239</th>\n",
       "      <th>val_r2_keras_loss_epoch_240</th>\n",
       "      <th>val_r2_keras_loss_epoch_241</th>\n",
       "      <th>val_r2_keras_loss_epoch_242</th>\n",
       "      <th>val_r2_keras_loss_epoch_243</th>\n",
       "      <th>val_r2_keras_loss_epoch_244</th>\n",
       "      <th>val_r2_keras_loss_epoch_245</th>\n",
       "      <th>val_r2_keras_loss_epoch_246</th>\n",
       "      <th>val_r2_keras_loss_epoch_247</th>\n",
       "      <th>val_r2_keras_loss_epoch_248</th>\n",
       "      <th>val_r2_keras_loss_epoch_249</th>\n",
       "      <th>val_r2_keras_loss_epoch_250</th>\n",
       "      <th>val_r2_keras_loss_epoch_251</th>\n",
       "      <th>val_r2_keras_loss_epoch_252</th>\n",
       "      <th>val_r2_keras_loss_epoch_253</th>\n",
       "      <th>val_r2_keras_loss_epoch_254</th>\n",
       "      <th>val_r2_keras_loss_epoch_255</th>\n",
       "      <th>val_r2_keras_loss_epoch_256</th>\n",
       "      <th>val_r2_keras_loss_epoch_257</th>\n",
       "      <th>val_r2_keras_loss_epoch_258</th>\n",
       "      <th>val_r2_keras_loss_epoch_259</th>\n",
       "      <th>val_r2_keras_loss_epoch_260</th>\n",
       "      <th>val_r2_keras_loss_epoch_261</th>\n",
       "      <th>val_r2_keras_loss_epoch_262</th>\n",
       "      <th>val_r2_keras_loss_epoch_263</th>\n",
       "      <th>val_r2_keras_loss_epoch_264</th>\n",
       "      <th>val_r2_keras_loss_epoch_265</th>\n",
       "      <th>val_r2_keras_loss_epoch_266</th>\n",
       "      <th>val_r2_keras_loss_epoch_267</th>\n",
       "      <th>val_r2_keras_loss_epoch_268</th>\n",
       "      <th>val_r2_keras_loss_epoch_269</th>\n",
       "      <th>val_r2_keras_loss_epoch_270</th>\n",
       "      <th>val_r2_keras_loss_epoch_271</th>\n",
       "      <th>val_r2_keras_loss_epoch_272</th>\n",
       "      <th>val_r2_keras_loss_epoch_273</th>\n",
       "      <th>val_r2_keras_loss_epoch_274</th>\n",
       "      <th>val_r2_keras_loss_epoch_275</th>\n",
       "      <th>val_r2_keras_loss_epoch_276</th>\n",
       "      <th>val_r2_keras_loss_epoch_277</th>\n",
       "      <th>val_r2_keras_loss_epoch_278</th>\n",
       "      <th>val_r2_keras_loss_epoch_279</th>\n",
       "      <th>val_r2_keras_loss_epoch_280</th>\n",
       "      <th>val_r2_keras_loss_epoch_281</th>\n",
       "      <th>val_r2_keras_loss_epoch_282</th>\n",
       "      <th>val_r2_keras_loss_epoch_283</th>\n",
       "      <th>val_r2_keras_loss_epoch_284</th>\n",
       "      <th>val_r2_keras_loss_epoch_285</th>\n",
       "      <th>val_r2_keras_loss_epoch_286</th>\n",
       "      <th>val_r2_keras_loss_epoch_287</th>\n",
       "      <th>val_r2_keras_loss_epoch_288</th>\n",
       "      <th>val_r2_keras_loss_epoch_289</th>\n",
       "      <th>val_r2_keras_loss_epoch_290</th>\n",
       "      <th>val_r2_keras_loss_epoch_291</th>\n",
       "      <th>val_r2_keras_loss_epoch_292</th>\n",
       "      <th>val_r2_keras_loss_epoch_293</th>\n",
       "      <th>val_r2_keras_loss_epoch_294</th>\n",
       "      <th>val_r2_keras_loss_epoch_295</th>\n",
       "      <th>val_r2_keras_loss_epoch_296</th>\n",
       "      <th>val_r2_keras_loss_epoch_297</th>\n",
       "      <th>val_r2_keras_loss_epoch_298</th>\n",
       "      <th>val_r2_keras_loss_epoch_299</th>\n",
       "      <th>val_r2_keras_loss_epoch_300</th>\n",
       "      <th>val_r2_keras_loss_epoch_301</th>\n",
       "      <th>val_r2_keras_loss_epoch_302</th>\n",
       "      <th>val_r2_keras_loss_epoch_303</th>\n",
       "      <th>val_r2_keras_loss_epoch_304</th>\n",
       "      <th>val_r2_keras_loss_epoch_305</th>\n",
       "      <th>val_r2_keras_loss_epoch_306</th>\n",
       "      <th>val_r2_keras_loss_epoch_307</th>\n",
       "      <th>val_r2_keras_loss_epoch_308</th>\n",
       "      <th>val_r2_keras_loss_epoch_309</th>\n",
       "      <th>val_r2_keras_loss_epoch_310</th>\n",
       "      <th>val_r2_keras_loss_epoch_311</th>\n",
       "      <th>val_r2_keras_loss_epoch_312</th>\n",
       "      <th>val_r2_keras_loss_epoch_313</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49999.000</td>\n",
       "      <td>49998.000</td>\n",
       "      <td>49997.000</td>\n",
       "      <td>49995.000</td>\n",
       "      <td>49992.000</td>\n",
       "      <td>49988.000</td>\n",
       "      <td>49987.000</td>\n",
       "      <td>49985.000</td>\n",
       "      <td>49977.000</td>\n",
       "      <td>49972.000</td>\n",
       "      <td>49963.000</td>\n",
       "      <td>49954.000</td>\n",
       "      <td>49947.000</td>\n",
       "      <td>49928.000</td>\n",
       "      <td>49915.000</td>\n",
       "      <td>49892.000</td>\n",
       "      <td>49877.000</td>\n",
       "      <td>49863.000</td>\n",
       "      <td>49844.000</td>\n",
       "      <td>49815.000</td>\n",
       "      <td>49786.000</td>\n",
       "      <td>49745.000</td>\n",
       "      <td>49715.000</td>\n",
       "      <td>49684.000</td>\n",
       "      <td>49639.000</td>\n",
       "      <td>49590.000</td>\n",
       "      <td>49533.000</td>\n",
       "      <td>49489.000</td>\n",
       "      <td>49419.000</td>\n",
       "      <td>49345.000</td>\n",
       "      <td>49250.000</td>\n",
       "      <td>49171.000</td>\n",
       "      <td>49086.000</td>\n",
       "      <td>48990.000</td>\n",
       "      <td>48876.000</td>\n",
       "      <td>48734.000</td>\n",
       "      <td>48604.000</td>\n",
       "      <td>48450.000</td>\n",
       "      <td>48292.000</td>\n",
       "      <td>48129.000</td>\n",
       "      <td>47962.000</td>\n",
       "      <td>47784.000</td>\n",
       "      <td>47623.000</td>\n",
       "      <td>47424.000</td>\n",
       "      <td>47195.000</td>\n",
       "      <td>46965.000</td>\n",
       "      <td>46709.000</td>\n",
       "      <td>46440.000</td>\n",
       "      <td>46166.000</td>\n",
       "      <td>45899.000</td>\n",
       "      <td>45611.000</td>\n",
       "      <td>45339.000</td>\n",
       "      <td>45037.000</td>\n",
       "      <td>44714.000</td>\n",
       "      <td>44353.000</td>\n",
       "      <td>44009.000</td>\n",
       "      <td>43657.000</td>\n",
       "      <td>43314.000</td>\n",
       "      <td>42929.000</td>\n",
       "      <td>42508.000</td>\n",
       "      <td>42093.000</td>\n",
       "      <td>41659.000</td>\n",
       "      <td>41237.000</td>\n",
       "      <td>40826.000</td>\n",
       "      <td>40392.000</td>\n",
       "      <td>39922.000</td>\n",
       "      <td>39426.000</td>\n",
       "      <td>38899.000</td>\n",
       "      <td>38437.000</td>\n",
       "      <td>37935.000</td>\n",
       "      <td>37418.000</td>\n",
       "      <td>36939.000</td>\n",
       "      <td>36417.000</td>\n",
       "      <td>35913.000</td>\n",
       "      <td>35372.000</td>\n",
       "      <td>34824.000</td>\n",
       "      <td>34290.000</td>\n",
       "      <td>33739.000</td>\n",
       "      <td>33162.000</td>\n",
       "      <td>32587.000</td>\n",
       "      <td>32037.000</td>\n",
       "      <td>31435.000</td>\n",
       "      <td>30861.000</td>\n",
       "      <td>30298.000</td>\n",
       "      <td>29751.000</td>\n",
       "      <td>29196.000</td>\n",
       "      <td>28593.000</td>\n",
       "      <td>28022.000</td>\n",
       "      <td>27441.000</td>\n",
       "      <td>26875.000</td>\n",
       "      <td>26276.000</td>\n",
       "      <td>25731.000</td>\n",
       "      <td>25168.000</td>\n",
       "      <td>24625.000</td>\n",
       "      <td>24089.000</td>\n",
       "      <td>23560.000</td>\n",
       "      <td>22983.000</td>\n",
       "      <td>22420.000</td>\n",
       "      <td>21915.000</td>\n",
       "      <td>21414.000</td>\n",
       "      <td>20861.000</td>\n",
       "      <td>20337.000</td>\n",
       "      <td>19866.000</td>\n",
       "      <td>19357.000</td>\n",
       "      <td>18812.000</td>\n",
       "      <td>18312.000</td>\n",
       "      <td>17824.000</td>\n",
       "      <td>17372.000</td>\n",
       "      <td>16916.000</td>\n",
       "      <td>16427.000</td>\n",
       "      <td>15973.000</td>\n",
       "      <td>15529.000</td>\n",
       "      <td>15086.000</td>\n",
       "      <td>14656.000</td>\n",
       "      <td>14242.000</td>\n",
       "      <td>13836.000</td>\n",
       "      <td>13441.000</td>\n",
       "      <td>13031.000</td>\n",
       "      <td>12650.000</td>\n",
       "      <td>12264.000</td>\n",
       "      <td>11884.000</td>\n",
       "      <td>11508.000</td>\n",
       "      <td>11163.000</td>\n",
       "      <td>10813.000</td>\n",
       "      <td>10465.000</td>\n",
       "      <td>10126.000</td>\n",
       "      <td>9791.000</td>\n",
       "      <td>9460.000</td>\n",
       "      <td>9137.000</td>\n",
       "      <td>8842.000</td>\n",
       "      <td>8512.000</td>\n",
       "      <td>8217.000</td>\n",
       "      <td>7929.000</td>\n",
       "      <td>7644.000</td>\n",
       "      <td>7362.000</td>\n",
       "      <td>7087.000</td>\n",
       "      <td>6863.000</td>\n",
       "      <td>6614.000</td>\n",
       "      <td>6328.000</td>\n",
       "      <td>6076.000</td>\n",
       "      <td>5826.000</td>\n",
       "      <td>5606.000</td>\n",
       "      <td>5383.000</td>\n",
       "      <td>5167.000</td>\n",
       "      <td>4971.000</td>\n",
       "      <td>4765.000</td>\n",
       "      <td>4597.000</td>\n",
       "      <td>4430.000</td>\n",
       "      <td>4264.000</td>\n",
       "      <td>4085.000</td>\n",
       "      <td>3910.000</td>\n",
       "      <td>3758.000</td>\n",
       "      <td>3607.000</td>\n",
       "      <td>3443.000</td>\n",
       "      <td>3313.000</td>\n",
       "      <td>3155.000</td>\n",
       "      <td>3020.000</td>\n",
       "      <td>2892.000</td>\n",
       "      <td>2746.000</td>\n",
       "      <td>2641.000</td>\n",
       "      <td>2532.000</td>\n",
       "      <td>2412.000</td>\n",
       "      <td>2290.000</td>\n",
       "      <td>2169.000</td>\n",
       "      <td>2054.000</td>\n",
       "      <td>1954.000</td>\n",
       "      <td>1872.000</td>\n",
       "      <td>1772.000</td>\n",
       "      <td>1675.000</td>\n",
       "      <td>1599.000</td>\n",
       "      <td>1532.000</td>\n",
       "      <td>1449.000</td>\n",
       "      <td>1385.000</td>\n",
       "      <td>1327.000</td>\n",
       "      <td>1262.000</td>\n",
       "      <td>1198.000</td>\n",
       "      <td>1123.000</td>\n",
       "      <td>1062.000</td>\n",
       "      <td>1009.000</td>\n",
       "      <td>955.000</td>\n",
       "      <td>913.000</td>\n",
       "      <td>871.000</td>\n",
       "      <td>818.000</td>\n",
       "      <td>762.000</td>\n",
       "      <td>729.000</td>\n",
       "      <td>687.000</td>\n",
       "      <td>653.000</td>\n",
       "      <td>621.000</td>\n",
       "      <td>588.000</td>\n",
       "      <td>556.000</td>\n",
       "      <td>526.000</td>\n",
       "      <td>501.000</td>\n",
       "      <td>476.000</td>\n",
       "      <td>448.000</td>\n",
       "      <td>424.000</td>\n",
       "      <td>392.000</td>\n",
       "      <td>374.000</td>\n",
       "      <td>357.000</td>\n",
       "      <td>333.000</td>\n",
       "      <td>307.000</td>\n",
       "      <td>290.000</td>\n",
       "      <td>271.000</td>\n",
       "      <td>252.000</td>\n",
       "      <td>234.000</td>\n",
       "      <td>220.000</td>\n",
       "      <td>208.000</td>\n",
       "      <td>197.000</td>\n",
       "      <td>184.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>169.000</td>\n",
       "      <td>158.000</td>\n",
       "      <td>146.000</td>\n",
       "      <td>139.000</td>\n",
       "      <td>129.000</td>\n",
       "      <td>122.000</td>\n",
       "      <td>113.000</td>\n",
       "      <td>107.000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>78.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>54.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>45.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>22.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24999.500</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>-0.739</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-0.888</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14433.901</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>-0.922</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12499.750</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>-0.815</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.864</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-0.922</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24999.500</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.634</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>-0.777</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>-0.809</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>-0.922</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37499.250</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-0.752</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-0.819</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.888</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49999.000</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>-0.727</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>-0.789</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-0.827</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>-0.830</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-0.863</td>\n",
       "      <td>-0.863</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>-0.873</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.883</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.878</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.886</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>-0.886</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  val_r2_keras_loss_epoch_1  val_r2_keras_loss_epoch_2  \\\n",
       "count 50000.000                  50000.000                  50000.000   \n",
       "mean  24999.500                     -0.284                     -0.622   \n",
       "std   14433.901                      0.180                      0.119   \n",
       "min       0.000                     -0.816                     -0.922   \n",
       "25%   12499.750                     -0.417                     -0.716   \n",
       "50%   24999.500                     -0.259                     -0.634   \n",
       "75%   37499.250                     -0.150                     -0.535   \n",
       "max   49999.000                      0.633                     -0.001   \n",
       "\n",
       "       val_r2_keras_loss_epoch_3  val_r2_keras_loss_epoch_4  \\\n",
       "count                  50000.000                  50000.000   \n",
       "mean                      -0.739                     -0.770   \n",
       "std                        0.075                      0.065   \n",
       "min                       -0.941                     -0.949   \n",
       "25%                       -0.793                     -0.815   \n",
       "50%                       -0.748                     -0.777   \n",
       "75%                       -0.694                     -0.732   \n",
       "max                       -0.261                     -0.368   \n",
       "\n",
       "       val_r2_keras_loss_epoch_5  val_r2_keras_loss_epoch_6  \\\n",
       "count                  50000.000                  50000.000   \n",
       "mean                      -0.787                     -0.803   \n",
       "std                        0.060                      0.055   \n",
       "min                       -0.952                     -0.954   \n",
       "25%                       -0.829                     -0.842   \n",
       "50%                       -0.794                     -0.809   \n",
       "75%                       -0.752                     -0.771   \n",
       "max                       -0.406                     -0.467   \n",
       "\n",
       "       val_r2_keras_loss_epoch_7  val_r2_keras_loss_epoch_8  \\\n",
       "count                  50000.000                  50000.000   \n",
       "mean                      -0.817                     -0.831   \n",
       "std                        0.051                      0.047   \n",
       "min                       -0.954                     -0.959   \n",
       "25%                       -0.853                     -0.864   \n",
       "50%                       -0.823                     -0.836   \n",
       "75%                       -0.787                     -0.803   \n",
       "max                       -0.500                     -0.536   \n",
       "\n",
       "       val_r2_keras_loss_epoch_9  val_r2_keras_loss_epoch_10  \\\n",
       "count                  50000.000                   50000.000   \n",
       "mean                      -0.844                      -0.856   \n",
       "std                        0.044                       0.040   \n",
       "min                       -0.963                      -0.966   \n",
       "25%                       -0.875                      -0.884   \n",
       "50%                       -0.848                      -0.860   \n",
       "75%                       -0.819                      -0.833   \n",
       "max                       -0.558                      -0.535   \n",
       "\n",
       "       val_r2_keras_loss_epoch_11  val_r2_keras_loss_epoch_12  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.868                      -0.879   \n",
       "std                         0.037                       0.034   \n",
       "min                        -0.972                      -0.975   \n",
       "25%                        -0.894                      -0.902   \n",
       "50%                        -0.871                      -0.882   \n",
       "75%                        -0.846                      -0.859   \n",
       "max                        -0.600                      -0.614   \n",
       "\n",
       "       val_r2_keras_loss_epoch_13  val_r2_keras_loss_epoch_14  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.888                      -0.897   \n",
       "std                         0.031                       0.028   \n",
       "min                        -0.974                      -0.980   \n",
       "25%                        -0.910                      -0.916   \n",
       "50%                        -0.891                      -0.899   \n",
       "75%                        -0.870                      -0.880   \n",
       "max                        -0.664                      -0.689   \n",
       "\n",
       "       val_r2_keras_loss_epoch_15  val_r2_keras_loss_epoch_16  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.904                      -0.910   \n",
       "std                         0.026                       0.024   \n",
       "min                        -0.980                      -0.982   \n",
       "25%                        -0.922                      -0.927   \n",
       "50%                        -0.906                      -0.912   \n",
       "75%                        -0.888                      -0.896   \n",
       "max                        -0.727                      -0.749   \n",
       "\n",
       "       val_r2_keras_loss_epoch_17  val_r2_keras_loss_epoch_18  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.916                      -0.921   \n",
       "std                         0.023                       0.021   \n",
       "min                        -0.984                      -0.984   \n",
       "25%                        -0.932                      -0.936   \n",
       "50%                        -0.918                      -0.922   \n",
       "75%                        -0.902                      -0.908   \n",
       "max                        -0.772                      -0.789   \n",
       "\n",
       "       val_r2_keras_loss_epoch_19  val_r2_keras_loss_epoch_20  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.925                      -0.929   \n",
       "std                         0.020                       0.019   \n",
       "min                        -0.985                      -0.985   \n",
       "25%                        -0.939                      -0.942   \n",
       "50%                        -0.926                      -0.930   \n",
       "75%                        -0.913                      -0.917   \n",
       "max                        -0.781                      -0.812   \n",
       "\n",
       "       val_r2_keras_loss_epoch_21  val_r2_keras_loss_epoch_22  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.932                      -0.934   \n",
       "std                         0.018                       0.017   \n",
       "min                        -0.986                      -0.986   \n",
       "25%                        -0.944                      -0.947   \n",
       "50%                        -0.933                      -0.935   \n",
       "75%                        -0.920                      -0.923   \n",
       "max                        -0.808                      -0.827   \n",
       "\n",
       "       val_r2_keras_loss_epoch_23  val_r2_keras_loss_epoch_24  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.937                      -0.939   \n",
       "std                         0.017                       0.016   \n",
       "min                        -0.987                      -0.988   \n",
       "25%                        -0.948                      -0.950   \n",
       "50%                        -0.938                      -0.940   \n",
       "75%                        -0.926                      -0.929   \n",
       "max                        -0.838                      -0.830   \n",
       "\n",
       "       val_r2_keras_loss_epoch_25  val_r2_keras_loss_epoch_26  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.941                      -0.942   \n",
       "std                         0.016                       0.015   \n",
       "min                        -0.988                      -0.988   \n",
       "25%                        -0.952                      -0.953   \n",
       "50%                        -0.942                      -0.943   \n",
       "75%                        -0.931                      -0.933   \n",
       "max                        -0.848                      -0.832   \n",
       "\n",
       "       val_r2_keras_loss_epoch_27  val_r2_keras_loss_epoch_28  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.944                      -0.945   \n",
       "std                         0.015                       0.015   \n",
       "min                        -0.988                      -0.988   \n",
       "25%                        -0.954                      -0.956   \n",
       "50%                        -0.945                      -0.946   \n",
       "75%                        -0.934                      -0.936   \n",
       "max                        -0.849                      -0.860   \n",
       "\n",
       "       val_r2_keras_loss_epoch_29  val_r2_keras_loss_epoch_30  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.946                      -0.948   \n",
       "std                         0.014                       0.014   \n",
       "min                        -0.989                      -0.989   \n",
       "25%                        -0.957                      -0.958   \n",
       "50%                        -0.947                      -0.948   \n",
       "75%                        -0.937                      -0.939   \n",
       "max                        -0.837                      -0.859   \n",
       "\n",
       "       val_r2_keras_loss_epoch_31  val_r2_keras_loss_epoch_32  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.949                      -0.950   \n",
       "std                         0.014                       0.014   \n",
       "min                        -0.989                      -0.989   \n",
       "25%                        -0.959                      -0.959   \n",
       "50%                        -0.950                      -0.951   \n",
       "75%                        -0.940                      -0.941   \n",
       "max                        -0.863                      -0.863   \n",
       "\n",
       "       val_r2_keras_loss_epoch_33  val_r2_keras_loss_epoch_34  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.950                      -0.951   \n",
       "std                         0.014                       0.013   \n",
       "min                        -0.989                      -0.990   \n",
       "25%                        -0.960                      -0.961   \n",
       "50%                        -0.952                      -0.952   \n",
       "75%                        -0.942                      -0.943   \n",
       "max                        -0.861                      -0.868   \n",
       "\n",
       "       val_r2_keras_loss_epoch_35  val_r2_keras_loss_epoch_36  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.952                      -0.953   \n",
       "std                         0.013                       0.013   \n",
       "min                        -0.989                      -0.990   \n",
       "25%                        -0.962                      -0.962   \n",
       "50%                        -0.953                      -0.954   \n",
       "75%                        -0.944                      -0.945   \n",
       "max                        -0.869                      -0.869   \n",
       "\n",
       "       val_r2_keras_loss_epoch_37  val_r2_keras_loss_epoch_38  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.954                      -0.954   \n",
       "std                         0.013                       0.013   \n",
       "min                        -0.990                      -0.989   \n",
       "25%                        -0.963                      -0.964   \n",
       "50%                        -0.955                      -0.955   \n",
       "75%                        -0.946                      -0.946   \n",
       "max                        -0.838                      -0.870   \n",
       "\n",
       "       val_r2_keras_loss_epoch_39  val_r2_keras_loss_epoch_40  \\\n",
       "count                   50000.000                   50000.000   \n",
       "mean                       -0.955                      -0.956   \n",
       "std                         0.013                       0.013   \n",
       "min                        -0.990                      -0.990   \n",
       "25%                        -0.964                      -0.965   \n",
       "50%                        -0.956                      -0.957   \n",
       "75%                        -0.947                      -0.948   \n",
       "max                        -0.860                      -0.874   \n",
       "\n",
       "       val_r2_keras_loss_epoch_41  val_r2_keras_loss_epoch_42  \\\n",
       "count                   49999.000                   49999.000   \n",
       "mean                       -0.956                      -0.957   \n",
       "std                         0.013                       0.013   \n",
       "min                        -0.990                      -0.991   \n",
       "25%                        -0.965                      -0.966   \n",
       "50%                        -0.957                      -0.958   \n",
       "75%                        -0.949                      -0.949   \n",
       "max                        -0.870                      -0.873   \n",
       "\n",
       "       val_r2_keras_loss_epoch_43  val_r2_keras_loss_epoch_44  \\\n",
       "count                   49999.000                   49998.000   \n",
       "mean                       -0.957                      -0.958   \n",
       "std                         0.012                       0.012   \n",
       "min                        -0.991                      -0.991   \n",
       "25%                        -0.966                      -0.967   \n",
       "50%                        -0.958                      -0.959   \n",
       "75%                        -0.950                      -0.950   \n",
       "max                        -0.867                      -0.877   \n",
       "\n",
       "       val_r2_keras_loss_epoch_45  val_r2_keras_loss_epoch_46  \\\n",
       "count                   49997.000                   49995.000   \n",
       "mean                       -0.958                      -0.959   \n",
       "std                         0.012                       0.012   \n",
       "min                        -0.991                      -0.991   \n",
       "25%                        -0.967                      -0.967   \n",
       "50%                        -0.959                      -0.960   \n",
       "75%                        -0.951                      -0.951   \n",
       "max                        -0.876                      -0.875   \n",
       "\n",
       "       val_r2_keras_loss_epoch_47  val_r2_keras_loss_epoch_48  \\\n",
       "count                   49992.000                   49988.000   \n",
       "mean                       -0.959                      -0.960   \n",
       "std                         0.012                       0.012   \n",
       "min                        -0.991                      -0.991   \n",
       "25%                        -0.968                      -0.968   \n",
       "50%                        -0.960                      -0.961   \n",
       "75%                        -0.952                      -0.953   \n",
       "max                        -0.876                      -0.875   \n",
       "\n",
       "       val_r2_keras_loss_epoch_49  val_r2_keras_loss_epoch_50  \\\n",
       "count                   49987.000                   49985.000   \n",
       "mean                       -0.960                      -0.961   \n",
       "std                         0.012                       0.012   \n",
       "min                        -0.991                      -0.992   \n",
       "25%                        -0.969                      -0.969   \n",
       "50%                        -0.961                      -0.962   \n",
       "75%                        -0.953                      -0.953   \n",
       "max                        -0.880                      -0.880   \n",
       "\n",
       "       val_r2_keras_loss_epoch_51  val_r2_keras_loss_epoch_52  \\\n",
       "count                   49977.000                   49972.000   \n",
       "mean                       -0.961                      -0.961   \n",
       "std                         0.012                       0.012   \n",
       "min                        -0.991                      -0.992   \n",
       "25%                        -0.969                      -0.970   \n",
       "50%                        -0.962                      -0.963   \n",
       "75%                        -0.954                      -0.954   \n",
       "max                        -0.881                      -0.879   \n",
       "\n",
       "       val_r2_keras_loss_epoch_53  val_r2_keras_loss_epoch_54  \\\n",
       "count                   49963.000                   49954.000   \n",
       "mean                       -0.962                      -0.962   \n",
       "std                         0.012                       0.012   \n",
       "min                        -0.992                      -0.991   \n",
       "25%                        -0.970                      -0.970   \n",
       "50%                        -0.963                      -0.963   \n",
       "75%                        -0.955                      -0.955   \n",
       "max                        -0.870                      -0.880   \n",
       "\n",
       "       val_r2_keras_loss_epoch_55  val_r2_keras_loss_epoch_56  \\\n",
       "count                   49947.000                   49928.000   \n",
       "mean                       -0.962                      -0.963   \n",
       "std                         0.012                       0.011   \n",
       "min                        -0.992                      -0.992   \n",
       "25%                        -0.971                      -0.971   \n",
       "50%                        -0.964                      -0.964   \n",
       "75%                        -0.956                      -0.956   \n",
       "max                        -0.883                      -0.880   \n",
       "\n",
       "       val_r2_keras_loss_epoch_57  val_r2_keras_loss_epoch_58  \\\n",
       "count                   49915.000                   49892.000   \n",
       "mean                       -0.963                      -0.963   \n",
       "std                         0.011                       0.011   \n",
       "min                        -0.992                      -0.992   \n",
       "25%                        -0.971                      -0.972   \n",
       "50%                        -0.964                      -0.965   \n",
       "75%                        -0.956                      -0.957   \n",
       "max                        -0.878                      -0.880   \n",
       "\n",
       "       val_r2_keras_loss_epoch_59  val_r2_keras_loss_epoch_60  \\\n",
       "count                   49877.000                   49863.000   \n",
       "mean                       -0.964                      -0.964   \n",
       "std                         0.011                       0.011   \n",
       "min                        -0.993                      -0.992   \n",
       "25%                        -0.972                      -0.972   \n",
       "50%                        -0.965                      -0.965   \n",
       "75%                        -0.957                      -0.957   \n",
       "max                        -0.884                      -0.880   \n",
       "\n",
       "       val_r2_keras_loss_epoch_61  val_r2_keras_loss_epoch_62  \\\n",
       "count                   49844.000                   49815.000   \n",
       "mean                       -0.964                      -0.965   \n",
       "std                         0.011                       0.011   \n",
       "min                        -0.993                      -0.993   \n",
       "25%                        -0.972                      -0.973   \n",
       "50%                        -0.966                      -0.966   \n",
       "75%                        -0.958                      -0.958   \n",
       "max                        -0.879                      -0.885   \n",
       "\n",
       "       val_r2_keras_loss_epoch_63  val_r2_keras_loss_epoch_64  \\\n",
       "count                   49786.000                   49745.000   \n",
       "mean                       -0.965                      -0.965   \n",
       "std                         0.011                       0.011   \n",
       "min                        -0.993                      -0.992   \n",
       "25%                        -0.973                      -0.973   \n",
       "50%                        -0.966                      -0.966   \n",
       "75%                        -0.958                      -0.959   \n",
       "max                        -0.884                      -0.880   \n",
       "\n",
       "       val_r2_keras_loss_epoch_65  val_r2_keras_loss_epoch_66  \\\n",
       "count                   49715.000                   49684.000   \n",
       "mean                       -0.966                      -0.966   \n",
       "std                         0.011                       0.011   \n",
       "min                        -0.992                      -0.993   \n",
       "25%                        -0.973                      -0.974   \n",
       "50%                        -0.967                      -0.967   \n",
       "75%                        -0.959                      -0.959   \n",
       "max                        -0.866                      -0.882   \n",
       "\n",
       "       val_r2_keras_loss_epoch_67  val_r2_keras_loss_epoch_68  \\\n",
       "count                   49639.000                   49590.000   \n",
       "mean                       -0.966                      -0.966   \n",
       "std                         0.011                       0.011   \n",
       "min                        -0.993                      -0.993   \n",
       "25%                        -0.974                      -0.974   \n",
       "50%                        -0.967                      -0.968   \n",
       "75%                        -0.960                      -0.960   \n",
       "max                        -0.889                      -0.881   \n",
       "\n",
       "       val_r2_keras_loss_epoch_69  val_r2_keras_loss_epoch_70  \\\n",
       "count                   49533.000                   49489.000   \n",
       "mean                       -0.967                      -0.967   \n",
       "std                         0.011                       0.011   \n",
       "min                        -0.993                      -0.994   \n",
       "25%                        -0.974                      -0.974   \n",
       "50%                        -0.968                      -0.968   \n",
       "75%                        -0.960                      -0.960   \n",
       "max                        -0.882                      -0.861   \n",
       "\n",
       "       val_r2_keras_loss_epoch_71  val_r2_keras_loss_epoch_72  \\\n",
       "count                   49419.000                   49345.000   \n",
       "mean                       -0.967                      -0.967   \n",
       "std                         0.011                       0.011   \n",
       "min                        -0.993                      -0.994   \n",
       "25%                        -0.975                      -0.975   \n",
       "50%                        -0.968                      -0.968   \n",
       "75%                        -0.961                      -0.961   \n",
       "max                        -0.879                      -0.889   \n",
       "\n",
       "       val_r2_keras_loss_epoch_73  val_r2_keras_loss_epoch_74  \\\n",
       "count                   49250.000                   49171.000   \n",
       "mean                       -0.967                      -0.968   \n",
       "std                         0.011                       0.011   \n",
       "min                        -0.994                      -0.993   \n",
       "25%                        -0.975                      -0.975   \n",
       "50%                        -0.969                      -0.969   \n",
       "75%                        -0.961                      -0.962   \n",
       "max                        -0.889                      -0.893   \n",
       "\n",
       "       val_r2_keras_loss_epoch_75  val_r2_keras_loss_epoch_76  \\\n",
       "count                   49086.000                   48990.000   \n",
       "mean                       -0.968                      -0.968   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.993                      -0.994   \n",
       "25%                        -0.975                      -0.976   \n",
       "50%                        -0.969                      -0.969   \n",
       "75%                        -0.962                      -0.962   \n",
       "max                        -0.886                      -0.892   \n",
       "\n",
       "       val_r2_keras_loss_epoch_77  val_r2_keras_loss_epoch_78  \\\n",
       "count                   48876.000                   48734.000   \n",
       "mean                       -0.968                      -0.968   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.994                      -0.993   \n",
       "25%                        -0.976                      -0.976   \n",
       "50%                        -0.970                      -0.970   \n",
       "75%                        -0.962                      -0.962   \n",
       "max                        -0.892                      -0.892   \n",
       "\n",
       "       val_r2_keras_loss_epoch_79  val_r2_keras_loss_epoch_80  \\\n",
       "count                   48604.000                   48450.000   \n",
       "mean                       -0.969                      -0.969   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.994                      -0.994   \n",
       "25%                        -0.976                      -0.976   \n",
       "50%                        -0.970                      -0.970   \n",
       "75%                        -0.963                      -0.963   \n",
       "max                        -0.895                      -0.886   \n",
       "\n",
       "       val_r2_keras_loss_epoch_81  val_r2_keras_loss_epoch_82  \\\n",
       "count                   48292.000                   48129.000   \n",
       "mean                       -0.969                      -0.969   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.994                      -0.994   \n",
       "25%                        -0.976                      -0.977   \n",
       "50%                        -0.970                      -0.970   \n",
       "75%                        -0.963                      -0.963   \n",
       "max                        -0.871                      -0.887   \n",
       "\n",
       "       val_r2_keras_loss_epoch_83  val_r2_keras_loss_epoch_84  \\\n",
       "count                   47962.000                   47784.000   \n",
       "mean                       -0.969                      -0.970   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.994                      -0.994   \n",
       "25%                        -0.977                      -0.977   \n",
       "50%                        -0.971                      -0.971   \n",
       "75%                        -0.963                      -0.964   \n",
       "max                        -0.890                      -0.881   \n",
       "\n",
       "       val_r2_keras_loss_epoch_85  val_r2_keras_loss_epoch_86  \\\n",
       "count                   47623.000                   47424.000   \n",
       "mean                       -0.970                      -0.970   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.995                      -0.994   \n",
       "25%                        -0.977                      -0.977   \n",
       "50%                        -0.971                      -0.971   \n",
       "75%                        -0.964                      -0.964   \n",
       "max                        -0.895                      -0.897   \n",
       "\n",
       "       val_r2_keras_loss_epoch_87  val_r2_keras_loss_epoch_88  \\\n",
       "count                   47195.000                   46965.000   \n",
       "mean                       -0.970                      -0.970   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.994                      -0.994   \n",
       "25%                        -0.977                      -0.977   \n",
       "50%                        -0.971                      -0.971   \n",
       "75%                        -0.964                      -0.965   \n",
       "max                        -0.877                      -0.899   \n",
       "\n",
       "       val_r2_keras_loss_epoch_89  val_r2_keras_loss_epoch_90  \\\n",
       "count                   46709.000                   46440.000   \n",
       "mean                       -0.970                      -0.971   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.994                      -0.995   \n",
       "25%                        -0.978                      -0.978   \n",
       "50%                        -0.972                      -0.972   \n",
       "75%                        -0.965                      -0.965   \n",
       "max                        -0.899                      -0.895   \n",
       "\n",
       "       val_r2_keras_loss_epoch_91  val_r2_keras_loss_epoch_92  \\\n",
       "count                   46166.000                   45899.000   \n",
       "mean                       -0.971                      -0.971   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.995                      -0.995   \n",
       "25%                        -0.978                      -0.978   \n",
       "50%                        -0.972                      -0.972   \n",
       "75%                        -0.965                      -0.965   \n",
       "max                        -0.893                      -0.897   \n",
       "\n",
       "       val_r2_keras_loss_epoch_93  val_r2_keras_loss_epoch_94  \\\n",
       "count                   45611.000                   45339.000   \n",
       "mean                       -0.971                      -0.971   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.995                      -0.995   \n",
       "25%                        -0.978                      -0.978   \n",
       "50%                        -0.972                      -0.972   \n",
       "75%                        -0.965                      -0.966   \n",
       "max                        -0.900                      -0.894   \n",
       "\n",
       "       val_r2_keras_loss_epoch_95  val_r2_keras_loss_epoch_96  \\\n",
       "count                   45037.000                   44714.000   \n",
       "mean                       -0.971                      -0.972   \n",
       "std                         0.010                       0.010   \n",
       "min                        -0.995                      -0.995   \n",
       "25%                        -0.978                      -0.978   \n",
       "50%                        -0.973                      -0.973   \n",
       "75%                        -0.966                      -0.966   \n",
       "max                        -0.900                      -0.899   \n",
       "\n",
       "       val_r2_keras_loss_epoch_97  val_r2_keras_loss_epoch_98  \\\n",
       "count                   44353.000                   44009.000   \n",
       "mean                       -0.972                      -0.972   \n",
       "std                         0.010                       0.009   \n",
       "min                        -0.995                      -0.995   \n",
       "25%                        -0.979                      -0.979   \n",
       "50%                        -0.973                      -0.973   \n",
       "75%                        -0.966                      -0.966   \n",
       "max                        -0.899                      -0.891   \n",
       "\n",
       "       val_r2_keras_loss_epoch_99  val_r2_keras_loss_epoch_100  \\\n",
       "count                   43657.000                    43314.000   \n",
       "mean                       -0.972                       -0.972   \n",
       "std                         0.009                        0.009   \n",
       "min                        -0.995                       -0.995   \n",
       "25%                        -0.979                       -0.979   \n",
       "50%                        -0.973                       -0.973   \n",
       "75%                        -0.967                       -0.967   \n",
       "max                        -0.896                       -0.893   \n",
       "\n",
       "       val_r2_keras_loss_epoch_101  val_r2_keras_loss_epoch_102  \\\n",
       "count                    42929.000                    42508.000   \n",
       "mean                        -0.972                       -0.972   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.979                       -0.979   \n",
       "50%                         -0.973                       -0.974   \n",
       "75%                         -0.967                       -0.967   \n",
       "max                         -0.899                       -0.900   \n",
       "\n",
       "       val_r2_keras_loss_epoch_103  val_r2_keras_loss_epoch_104  \\\n",
       "count                    42093.000                    41659.000   \n",
       "mean                        -0.973                       -0.973   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.979                       -0.979   \n",
       "50%                         -0.974                       -0.974   \n",
       "75%                         -0.967                       -0.967   \n",
       "max                         -0.900                       -0.898   \n",
       "\n",
       "       val_r2_keras_loss_epoch_105  val_r2_keras_loss_epoch_106  \\\n",
       "count                    41237.000                    40826.000   \n",
       "mean                        -0.973                       -0.973   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.980                       -0.980   \n",
       "50%                         -0.974                       -0.974   \n",
       "75%                         -0.967                       -0.968   \n",
       "max                         -0.899                       -0.897   \n",
       "\n",
       "       val_r2_keras_loss_epoch_107  val_r2_keras_loss_epoch_108  \\\n",
       "count                    40392.000                    39922.000   \n",
       "mean                        -0.973                       -0.973   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.980                       -0.980   \n",
       "50%                         -0.974                       -0.974   \n",
       "75%                         -0.968                       -0.968   \n",
       "max                         -0.897                       -0.896   \n",
       "\n",
       "       val_r2_keras_loss_epoch_109  val_r2_keras_loss_epoch_110  \\\n",
       "count                    39426.000                    38899.000   \n",
       "mean                        -0.973                       -0.974   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.996                       -0.995   \n",
       "25%                         -0.980                       -0.980   \n",
       "50%                         -0.974                       -0.975   \n",
       "75%                         -0.968                       -0.968   \n",
       "max                         -0.896                       -0.899   \n",
       "\n",
       "       val_r2_keras_loss_epoch_111  val_r2_keras_loss_epoch_112  \\\n",
       "count                    38437.000                    37935.000   \n",
       "mean                        -0.974                       -0.974   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.980                       -0.980   \n",
       "50%                         -0.975                       -0.975   \n",
       "75%                         -0.968                       -0.969   \n",
       "max                         -0.904                       -0.902   \n",
       "\n",
       "       val_r2_keras_loss_epoch_113  val_r2_keras_loss_epoch_114  \\\n",
       "count                    37418.000                    36939.000   \n",
       "mean                        -0.974                       -0.974   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.995                       -0.996   \n",
       "25%                         -0.980                       -0.980   \n",
       "50%                         -0.975                       -0.975   \n",
       "75%                         -0.969                       -0.969   \n",
       "max                         -0.882                       -0.906   \n",
       "\n",
       "       val_r2_keras_loss_epoch_115  val_r2_keras_loss_epoch_116  \\\n",
       "count                    36417.000                    35913.000   \n",
       "mean                        -0.974                       -0.974   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.996                       -0.995   \n",
       "25%                         -0.981                       -0.981   \n",
       "50%                         -0.975                       -0.975   \n",
       "75%                         -0.969                       -0.969   \n",
       "max                         -0.896                       -0.907   \n",
       "\n",
       "       val_r2_keras_loss_epoch_117  val_r2_keras_loss_epoch_118  \\\n",
       "count                    35372.000                    34824.000   \n",
       "mean                        -0.974                       -0.974   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.996                       -0.995   \n",
       "25%                         -0.981                       -0.981   \n",
       "50%                         -0.976                       -0.976   \n",
       "75%                         -0.969                       -0.969   \n",
       "max                         -0.905                       -0.897   \n",
       "\n",
       "       val_r2_keras_loss_epoch_119  val_r2_keras_loss_epoch_120  \\\n",
       "count                    34290.000                    33739.000   \n",
       "mean                        -0.975                       -0.975   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.981                       -0.981   \n",
       "50%                         -0.976                       -0.976   \n",
       "75%                         -0.970                       -0.970   \n",
       "max                         -0.903                       -0.902   \n",
       "\n",
       "       val_r2_keras_loss_epoch_121  val_r2_keras_loss_epoch_122  \\\n",
       "count                    33162.000                    32587.000   \n",
       "mean                        -0.975                       -0.975   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.996                       -0.995   \n",
       "25%                         -0.981                       -0.981   \n",
       "50%                         -0.976                       -0.976   \n",
       "75%                         -0.970                       -0.970   \n",
       "max                         -0.896                       -0.906   \n",
       "\n",
       "       val_r2_keras_loss_epoch_123  val_r2_keras_loss_epoch_124  \\\n",
       "count                    32037.000                    31435.000   \n",
       "mean                        -0.975                       -0.975   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.981                       -0.981   \n",
       "50%                         -0.976                       -0.976   \n",
       "75%                         -0.970                       -0.970   \n",
       "max                         -0.904                       -0.905   \n",
       "\n",
       "       val_r2_keras_loss_epoch_125  val_r2_keras_loss_epoch_126  \\\n",
       "count                    30861.000                    30298.000   \n",
       "mean                        -0.975                       -0.975   \n",
       "std                          0.009                        0.009   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.981                       -0.982   \n",
       "50%                         -0.976                       -0.976   \n",
       "75%                         -0.970                       -0.971   \n",
       "max                         -0.894                       -0.907   \n",
       "\n",
       "       val_r2_keras_loss_epoch_127  val_r2_keras_loss_epoch_128  \\\n",
       "count                    29751.000                    29196.000   \n",
       "mean                        -0.975                       -0.976   \n",
       "std                          0.009                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.982                       -0.982   \n",
       "50%                         -0.977                       -0.977   \n",
       "75%                         -0.971                       -0.971   \n",
       "max                         -0.901                       -0.895   \n",
       "\n",
       "       val_r2_keras_loss_epoch_129  val_r2_keras_loss_epoch_130  \\\n",
       "count                    28593.000                    28022.000   \n",
       "mean                        -0.976                       -0.976   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.982                       -0.982   \n",
       "50%                         -0.977                       -0.977   \n",
       "75%                         -0.971                       -0.971   \n",
       "max                         -0.908                       -0.905   \n",
       "\n",
       "       val_r2_keras_loss_epoch_131  val_r2_keras_loss_epoch_132  \\\n",
       "count                    27441.000                    26875.000   \n",
       "mean                        -0.976                       -0.976   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.982                       -0.982   \n",
       "50%                         -0.977                       -0.977   \n",
       "75%                         -0.971                       -0.971   \n",
       "max                         -0.894                       -0.885   \n",
       "\n",
       "       val_r2_keras_loss_epoch_133  val_r2_keras_loss_epoch_134  \\\n",
       "count                    26276.000                    25731.000   \n",
       "mean                        -0.976                       -0.976   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.982                       -0.982   \n",
       "50%                         -0.977                       -0.977   \n",
       "75%                         -0.971                       -0.971   \n",
       "max                         -0.902                       -0.904   \n",
       "\n",
       "       val_r2_keras_loss_epoch_135  val_r2_keras_loss_epoch_136  \\\n",
       "count                    25168.000                    24625.000   \n",
       "mean                        -0.976                       -0.976   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.982                       -0.982   \n",
       "50%                         -0.977                       -0.978   \n",
       "75%                         -0.972                       -0.972   \n",
       "max                         -0.907                       -0.909   \n",
       "\n",
       "       val_r2_keras_loss_epoch_137  val_r2_keras_loss_epoch_138  \\\n",
       "count                    24089.000                    23560.000   \n",
       "mean                        -0.977                       -0.977   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.982                       -0.983   \n",
       "50%                         -0.978                       -0.978   \n",
       "75%                         -0.972                       -0.972   \n",
       "max                         -0.909                       -0.909   \n",
       "\n",
       "       val_r2_keras_loss_epoch_139  val_r2_keras_loss_epoch_140  \\\n",
       "count                    22983.000                    22420.000   \n",
       "mean                        -0.977                       -0.977   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.983                       -0.983   \n",
       "50%                         -0.978                       -0.978   \n",
       "75%                         -0.972                       -0.972   \n",
       "max                         -0.903                       -0.906   \n",
       "\n",
       "       val_r2_keras_loss_epoch_141  val_r2_keras_loss_epoch_142  \\\n",
       "count                    21915.000                    21414.000   \n",
       "mean                        -0.977                       -0.977   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.983                       -0.983   \n",
       "50%                         -0.978                       -0.978   \n",
       "75%                         -0.972                       -0.972   \n",
       "max                         -0.906                       -0.910   \n",
       "\n",
       "       val_r2_keras_loss_epoch_143  val_r2_keras_loss_epoch_144  \\\n",
       "count                    20861.000                    20337.000   \n",
       "mean                        -0.977                       -0.977   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.983                       -0.983   \n",
       "50%                         -0.978                       -0.978   \n",
       "75%                         -0.972                       -0.973   \n",
       "max                         -0.907                       -0.908   \n",
       "\n",
       "       val_r2_keras_loss_epoch_145  val_r2_keras_loss_epoch_146  \\\n",
       "count                    19866.000                    19357.000   \n",
       "mean                        -0.977                       -0.977   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.983                       -0.983   \n",
       "50%                         -0.978                       -0.978   \n",
       "75%                         -0.973                       -0.973   \n",
       "max                         -0.909                       -0.907   \n",
       "\n",
       "       val_r2_keras_loss_epoch_147  val_r2_keras_loss_epoch_148  \\\n",
       "count                    18812.000                    18312.000   \n",
       "mean                        -0.978                       -0.978   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.983                       -0.983   \n",
       "50%                         -0.979                       -0.979   \n",
       "75%                         -0.973                       -0.973   \n",
       "max                         -0.930                       -0.925   \n",
       "\n",
       "       val_r2_keras_loss_epoch_149  val_r2_keras_loss_epoch_150  \\\n",
       "count                    17824.000                    17372.000   \n",
       "mean                        -0.978                       -0.978   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.983                       -0.983   \n",
       "50%                         -0.979                       -0.979   \n",
       "75%                         -0.973                       -0.973   \n",
       "max                         -0.931                       -0.929   \n",
       "\n",
       "       val_r2_keras_loss_epoch_151  val_r2_keras_loss_epoch_152  \\\n",
       "count                    16916.000                    16427.000   \n",
       "mean                        -0.978                       -0.978   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.984                       -0.984   \n",
       "50%                         -0.979                       -0.979   \n",
       "75%                         -0.973                       -0.973   \n",
       "max                         -0.927                       -0.919   \n",
       "\n",
       "       val_r2_keras_loss_epoch_153  val_r2_keras_loss_epoch_154  \\\n",
       "count                    15973.000                    15529.000   \n",
       "mean                        -0.978                       -0.978   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.984                       -0.984   \n",
       "50%                         -0.979                       -0.979   \n",
       "75%                         -0.974                       -0.974   \n",
       "max                         -0.930                       -0.927   \n",
       "\n",
       "       val_r2_keras_loss_epoch_155  val_r2_keras_loss_epoch_156  \\\n",
       "count                    15086.000                    14656.000   \n",
       "mean                        -0.978                       -0.978   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.984                       -0.984   \n",
       "50%                         -0.979                       -0.979   \n",
       "75%                         -0.974                       -0.974   \n",
       "max                         -0.928                       -0.931   \n",
       "\n",
       "       val_r2_keras_loss_epoch_157  val_r2_keras_loss_epoch_158  \\\n",
       "count                    14242.000                    13836.000   \n",
       "mean                        -0.978                       -0.978   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.984                       -0.984   \n",
       "50%                         -0.980                       -0.980   \n",
       "75%                         -0.974                       -0.974   \n",
       "max                         -0.932                       -0.918   \n",
       "\n",
       "       val_r2_keras_loss_epoch_159  val_r2_keras_loss_epoch_160  \\\n",
       "count                    13441.000                    13031.000   \n",
       "mean                        -0.979                       -0.979   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.984                       -0.984   \n",
       "50%                         -0.980                       -0.980   \n",
       "75%                         -0.974                       -0.974   \n",
       "max                         -0.925                       -0.931   \n",
       "\n",
       "       val_r2_keras_loss_epoch_161  val_r2_keras_loss_epoch_162  \\\n",
       "count                    12650.000                    12264.000   \n",
       "mean                        -0.979                       -0.979   \n",
       "std                          0.008                        0.008   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.984                       -0.984   \n",
       "50%                         -0.980                       -0.980   \n",
       "75%                         -0.974                       -0.974   \n",
       "max                         -0.919                       -0.932   \n",
       "\n",
       "       val_r2_keras_loss_epoch_163  val_r2_keras_loss_epoch_164  \\\n",
       "count                    11884.000                    11508.000   \n",
       "mean                        -0.979                       -0.979   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.996                       -0.997   \n",
       "25%                         -0.984                       -0.984   \n",
       "50%                         -0.980                       -0.980   \n",
       "75%                         -0.975                       -0.975   \n",
       "max                         -0.933                       -0.932   \n",
       "\n",
       "       val_r2_keras_loss_epoch_165  val_r2_keras_loss_epoch_166  \\\n",
       "count                    11163.000                    10813.000   \n",
       "mean                        -0.979                       -0.979   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.984                       -0.984   \n",
       "50%                         -0.980                       -0.980   \n",
       "75%                         -0.975                       -0.975   \n",
       "max                         -0.930                       -0.932   \n",
       "\n",
       "       val_r2_keras_loss_epoch_167  val_r2_keras_loss_epoch_168  \\\n",
       "count                    10465.000                    10126.000   \n",
       "mean                        -0.979                       -0.979   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.997                       -0.996   \n",
       "25%                         -0.985                       -0.985   \n",
       "50%                         -0.980                       -0.980   \n",
       "75%                         -0.975                       -0.975   \n",
       "max                         -0.931                       -0.923   \n",
       "\n",
       "       val_r2_keras_loss_epoch_169  val_r2_keras_loss_epoch_170  \\\n",
       "count                     9791.000                     9460.000   \n",
       "mean                        -0.979                       -0.979   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.996                       -0.996   \n",
       "25%                         -0.985                       -0.985   \n",
       "50%                         -0.980                       -0.980   \n",
       "75%                         -0.975                       -0.975   \n",
       "max                         -0.934                       -0.934   \n",
       "\n",
       "       val_r2_keras_loss_epoch_171  val_r2_keras_loss_epoch_172  \\\n",
       "count                     9137.000                     8842.000   \n",
       "mean                        -0.979                       -0.980   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.996                       -0.997   \n",
       "25%                         -0.985                       -0.985   \n",
       "50%                         -0.980                       -0.981   \n",
       "75%                         -0.975                       -0.975   \n",
       "max                         -0.926                       -0.934   \n",
       "\n",
       "       val_r2_keras_loss_epoch_173  val_r2_keras_loss_epoch_174  \\\n",
       "count                     8512.000                     8217.000   \n",
       "mean                        -0.980                       -0.980   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.997                       -0.996   \n",
       "25%                         -0.985                       -0.985   \n",
       "50%                         -0.981                       -0.981   \n",
       "75%                         -0.975                       -0.975   \n",
       "max                         -0.934                       -0.929   \n",
       "\n",
       "       val_r2_keras_loss_epoch_175  val_r2_keras_loss_epoch_176  \\\n",
       "count                     7929.000                     7644.000   \n",
       "mean                        -0.980                       -0.980   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.996                       -0.997   \n",
       "25%                         -0.985                       -0.985   \n",
       "50%                         -0.981                       -0.981   \n",
       "75%                         -0.976                       -0.976   \n",
       "max                         -0.934                       -0.933   \n",
       "\n",
       "       val_r2_keras_loss_epoch_177  val_r2_keras_loss_epoch_178  \\\n",
       "count                     7362.000                     7087.000   \n",
       "mean                        -0.980                       -0.980   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.997                       -0.997   \n",
       "25%                         -0.985                       -0.985   \n",
       "50%                         -0.981                       -0.981   \n",
       "75%                         -0.976                       -0.976   \n",
       "max                         -0.931                       -0.933   \n",
       "\n",
       "       val_r2_keras_loss_epoch_179  val_r2_keras_loss_epoch_180  \\\n",
       "count                     6863.000                     6614.000   \n",
       "mean                        -0.980                       -0.980   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.997                       -0.997   \n",
       "25%                         -0.985                       -0.985   \n",
       "50%                         -0.981                       -0.981   \n",
       "75%                         -0.976                       -0.976   \n",
       "max                         -0.929                       -0.933   \n",
       "\n",
       "       val_r2_keras_loss_epoch_181  val_r2_keras_loss_epoch_182  \\\n",
       "count                     6328.000                     6076.000   \n",
       "mean                        -0.980                       -0.980   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.997                       -0.997   \n",
       "25%                         -0.985                       -0.985   \n",
       "50%                         -0.981                       -0.981   \n",
       "75%                         -0.976                       -0.976   \n",
       "max                         -0.935                       -0.924   \n",
       "\n",
       "       val_r2_keras_loss_epoch_183  val_r2_keras_loss_epoch_184  \\\n",
       "count                     5826.000                     5606.000   \n",
       "mean                        -0.980                       -0.980   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.997                       -0.997   \n",
       "25%                         -0.985                       -0.985   \n",
       "50%                         -0.981                       -0.981   \n",
       "75%                         -0.976                       -0.976   \n",
       "max                         -0.935                       -0.937   \n",
       "\n",
       "       val_r2_keras_loss_epoch_185  val_r2_keras_loss_epoch_186  \\\n",
       "count                     5383.000                     5167.000   \n",
       "mean                        -0.980                       -0.980   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.997                       -0.997   \n",
       "25%                         -0.985                       -0.985   \n",
       "50%                         -0.981                       -0.981   \n",
       "75%                         -0.976                       -0.976   \n",
       "max                         -0.934                       -0.938   \n",
       "\n",
       "       val_r2_keras_loss_epoch_187  val_r2_keras_loss_epoch_188  \\\n",
       "count                     4971.000                     4765.000   \n",
       "mean                        -0.980                       -0.980   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.982   \n",
       "75%                         -0.976                       -0.976   \n",
       "max                         -0.931                       -0.934   \n",
       "\n",
       "       val_r2_keras_loss_epoch_189  val_r2_keras_loss_epoch_190  \\\n",
       "count                     4597.000                     4430.000   \n",
       "mean                        -0.980                       -0.981   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.982   \n",
       "75%                         -0.977                       -0.977   \n",
       "max                         -0.938                       -0.936   \n",
       "\n",
       "       val_r2_keras_loss_epoch_191  val_r2_keras_loss_epoch_192  \\\n",
       "count                     4264.000                     4085.000   \n",
       "mean                        -0.981                       -0.981   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.982   \n",
       "75%                         -0.977                       -0.976   \n",
       "max                         -0.935                       -0.934   \n",
       "\n",
       "       val_r2_keras_loss_epoch_193  val_r2_keras_loss_epoch_194  \\\n",
       "count                     3910.000                     3758.000   \n",
       "mean                        -0.981                       -0.981   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.982   \n",
       "75%                         -0.977                       -0.977   \n",
       "max                         -0.934                       -0.936   \n",
       "\n",
       "       val_r2_keras_loss_epoch_195  val_r2_keras_loss_epoch_196  \\\n",
       "count                     3607.000                     3443.000   \n",
       "mean                        -0.981                       -0.981   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.982   \n",
       "75%                         -0.977                       -0.977   \n",
       "max                         -0.942                       -0.947   \n",
       "\n",
       "       val_r2_keras_loss_epoch_197  val_r2_keras_loss_epoch_198  \\\n",
       "count                     3313.000                     3155.000   \n",
       "mean                        -0.981                       -0.981   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.982   \n",
       "75%                         -0.977                       -0.977   \n",
       "max                         -0.947                       -0.948   \n",
       "\n",
       "       val_r2_keras_loss_epoch_199  val_r2_keras_loss_epoch_200  \\\n",
       "count                     3020.000                     2892.000   \n",
       "mean                        -0.981                       -0.981   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.982   \n",
       "75%                         -0.977                       -0.977   \n",
       "max                         -0.948                       -0.940   \n",
       "\n",
       "       val_r2_keras_loss_epoch_201  val_r2_keras_loss_epoch_202  \\\n",
       "count                     2746.000                     2641.000   \n",
       "mean                        -0.981                       -0.981   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.982   \n",
       "75%                         -0.977                       -0.977   \n",
       "max                         -0.948                       -0.949   \n",
       "\n",
       "       val_r2_keras_loss_epoch_203  val_r2_keras_loss_epoch_204  \\\n",
       "count                     2532.000                     2412.000   \n",
       "mean                        -0.981                       -0.981   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.982   \n",
       "75%                         -0.977                       -0.977   \n",
       "max                         -0.943                       -0.947   \n",
       "\n",
       "       val_r2_keras_loss_epoch_205  val_r2_keras_loss_epoch_206  \\\n",
       "count                     2290.000                     2169.000   \n",
       "mean                        -0.981                       -0.981   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.982   \n",
       "75%                         -0.978                       -0.978   \n",
       "max                         -0.946                       -0.948   \n",
       "\n",
       "       val_r2_keras_loss_epoch_207  val_r2_keras_loss_epoch_208  \\\n",
       "count                     2054.000                     1954.000   \n",
       "mean                        -0.981                       -0.982   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.982                       -0.983   \n",
       "75%                         -0.978                       -0.978   \n",
       "max                         -0.947                       -0.945   \n",
       "\n",
       "       val_r2_keras_loss_epoch_209  val_r2_keras_loss_epoch_210  \\\n",
       "count                     1872.000                     1772.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.007                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.978                       -0.978   \n",
       "max                         -0.952                       -0.954   \n",
       "\n",
       "       val_r2_keras_loss_epoch_211  val_r2_keras_loss_epoch_212  \\\n",
       "count                     1675.000                     1599.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.007                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.986                       -0.986   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.978                       -0.978   \n",
       "max                         -0.954                       -0.954   \n",
       "\n",
       "       val_r2_keras_loss_epoch_213  val_r2_keras_loss_epoch_214  \\\n",
       "count                     1532.000                     1449.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.996   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.978                       -0.978   \n",
       "max                         -0.956                       -0.955   \n",
       "\n",
       "       val_r2_keras_loss_epoch_215  val_r2_keras_loss_epoch_216  \\\n",
       "count                     1385.000                     1327.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.978                       -0.978   \n",
       "max                         -0.954                       -0.957   \n",
       "\n",
       "       val_r2_keras_loss_epoch_217  val_r2_keras_loss_epoch_218  \\\n",
       "count                     1262.000                     1198.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.978                       -0.979   \n",
       "max                         -0.942                       -0.956   \n",
       "\n",
       "       val_r2_keras_loss_epoch_219  val_r2_keras_loss_epoch_220  \\\n",
       "count                     1123.000                     1062.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.996                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.978   \n",
       "max                         -0.948                       -0.952   \n",
       "\n",
       "       val_r2_keras_loss_epoch_221  val_r2_keras_loss_epoch_222  \\\n",
       "count                     1009.000                      955.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.954                       -0.959   \n",
       "\n",
       "       val_r2_keras_loss_epoch_223  val_r2_keras_loss_epoch_224  \\\n",
       "count                      913.000                      871.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.996   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.959                       -0.960   \n",
       "\n",
       "       val_r2_keras_loss_epoch_225  val_r2_keras_loss_epoch_226  \\\n",
       "count                      818.000                      762.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.959                       -0.960   \n",
       "\n",
       "       val_r2_keras_loss_epoch_227  val_r2_keras_loss_epoch_228  \\\n",
       "count                      729.000                      687.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.959                       -0.957   \n",
       "\n",
       "       val_r2_keras_loss_epoch_229  val_r2_keras_loss_epoch_230  \\\n",
       "count                      653.000                      621.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.994                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.963                       -0.962   \n",
       "\n",
       "       val_r2_keras_loss_epoch_231  val_r2_keras_loss_epoch_232  \\\n",
       "count                      588.000                      556.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.962                       -0.963   \n",
       "\n",
       "       val_r2_keras_loss_epoch_233  val_r2_keras_loss_epoch_234  \\\n",
       "count                      526.000                      501.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.994   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.963                       -0.963   \n",
       "\n",
       "       val_r2_keras_loss_epoch_235  val_r2_keras_loss_epoch_236  \\\n",
       "count                      476.000                      448.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.960                       -0.962   \n",
       "\n",
       "       val_r2_keras_loss_epoch_237  val_r2_keras_loss_epoch_238  \\\n",
       "count                      424.000                      392.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.984                       -0.984   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.964                       -0.963   \n",
       "\n",
       "       val_r2_keras_loss_epoch_239  val_r2_keras_loss_epoch_240  \\\n",
       "count                      374.000                      357.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.984                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.964                       -0.962   \n",
       "\n",
       "       val_r2_keras_loss_epoch_241  val_r2_keras_loss_epoch_242  \\\n",
       "count                      333.000                      307.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.984                       -0.984   \n",
       "75%                         -0.979                       -0.980   \n",
       "max                         -0.964                       -0.964   \n",
       "\n",
       "       val_r2_keras_loss_epoch_243  val_r2_keras_loss_epoch_244  \\\n",
       "count                      290.000                      271.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.984                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.960                       -0.964   \n",
       "\n",
       "       val_r2_keras_loss_epoch_245  val_r2_keras_loss_epoch_246  \\\n",
       "count                      252.000                      234.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.994                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.961                       -0.965   \n",
       "\n",
       "       val_r2_keras_loss_epoch_247  val_r2_keras_loss_epoch_248  \\\n",
       "count                      220.000                      208.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.964                       -0.965   \n",
       "\n",
       "       val_r2_keras_loss_epoch_249  val_r2_keras_loss_epoch_250  \\\n",
       "count                      197.000                      184.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.994                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.962                       -0.963   \n",
       "\n",
       "       val_r2_keras_loss_epoch_251  val_r2_keras_loss_epoch_252  \\\n",
       "count                      178.000                      169.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.005                        0.006   \n",
       "min                         -0.995                       -0.995   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.965                       -0.965   \n",
       "\n",
       "       val_r2_keras_loss_epoch_253  val_r2_keras_loss_epoch_254  \\\n",
       "count                      158.000                      146.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.993                       -0.994   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.963                       -0.965   \n",
       "\n",
       "       val_r2_keras_loss_epoch_255  val_r2_keras_loss_epoch_256  \\\n",
       "count                      139.000                      129.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.005                        0.006   \n",
       "min                         -0.994                       -0.994   \n",
       "25%                         -0.986                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.980                       -0.980   \n",
       "max                         -0.966                       -0.966   \n",
       "\n",
       "       val_r2_keras_loss_epoch_257  val_r2_keras_loss_epoch_258  \\\n",
       "count                      122.000                      113.000   \n",
       "mean                        -0.982                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.994                       -0.994   \n",
       "25%                         -0.987                       -0.986   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.980                       -0.980   \n",
       "max                         -0.958                       -0.965   \n",
       "\n",
       "       val_r2_keras_loss_epoch_259  val_r2_keras_loss_epoch_260  \\\n",
       "count                      107.000                       98.000   \n",
       "mean                        -0.983                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.994                       -0.993   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.982   \n",
       "75%                         -0.980                       -0.979   \n",
       "max                         -0.966                       -0.963   \n",
       "\n",
       "       val_r2_keras_loss_epoch_261  val_r2_keras_loss_epoch_262  \\\n",
       "count                       95.000                       88.000   \n",
       "mean                        -0.982                       -0.983   \n",
       "std                          0.006                        0.005   \n",
       "min                         -0.993                       -0.993   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.980   \n",
       "max                         -0.966                       -0.966   \n",
       "\n",
       "       val_r2_keras_loss_epoch_263  val_r2_keras_loss_epoch_264  \\\n",
       "count                       78.000                       72.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.992                       -0.992   \n",
       "25%                         -0.986                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.964                       -0.965   \n",
       "\n",
       "       val_r2_keras_loss_epoch_265  val_r2_keras_loss_epoch_266  \\\n",
       "count                       69.000                       66.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.992                       -0.991   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.984   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.965                       -0.966   \n",
       "\n",
       "       val_r2_keras_loss_epoch_267  val_r2_keras_loss_epoch_268  \\\n",
       "count                       62.000                       61.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.992                       -0.991   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.978   \n",
       "max                         -0.967                       -0.964   \n",
       "\n",
       "       val_r2_keras_loss_epoch_269  val_r2_keras_loss_epoch_270  \\\n",
       "count                       54.000                       50.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.005                        0.006   \n",
       "min                         -0.991                       -0.992   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.980                       -0.979   \n",
       "max                         -0.967                       -0.966   \n",
       "\n",
       "       val_r2_keras_loss_epoch_271  val_r2_keras_loss_epoch_272  \\\n",
       "count                       47.000                       45.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.991                       -0.992   \n",
       "25%                         -0.987                       -0.986   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.980                       -0.981   \n",
       "max                         -0.966                       -0.966   \n",
       "\n",
       "       val_r2_keras_loss_epoch_273  val_r2_keras_loss_epoch_274  \\\n",
       "count                       38.000                       34.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.005                        0.006   \n",
       "min                         -0.991                       -0.991   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.984                       -0.983   \n",
       "75%                         -0.981                       -0.979   \n",
       "max                         -0.967                       -0.966   \n",
       "\n",
       "       val_r2_keras_loss_epoch_275  val_r2_keras_loss_epoch_276  \\\n",
       "count                       33.000                       30.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.991                       -0.991   \n",
       "25%                         -0.987                       -0.988   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.980                       -0.979   \n",
       "max                         -0.965                       -0.966   \n",
       "\n",
       "       val_r2_keras_loss_epoch_277  val_r2_keras_loss_epoch_278  \\\n",
       "count                       26.000                       26.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.991                       -0.990   \n",
       "25%                         -0.987                       -0.986   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.980                       -0.980   \n",
       "max                         -0.968                       -0.967   \n",
       "\n",
       "       val_r2_keras_loss_epoch_279  val_r2_keras_loss_epoch_280  \\\n",
       "count                       25.000                       25.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.991                       -0.991   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.979                       -0.979   \n",
       "max                         -0.966                       -0.965   \n",
       "\n",
       "       val_r2_keras_loss_epoch_281  val_r2_keras_loss_epoch_282  \\\n",
       "count                       22.000                       20.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.005                        0.006   \n",
       "min                         -0.988                       -0.990   \n",
       "25%                         -0.986                       -0.987   \n",
       "50%                         -0.983                       -0.982   \n",
       "75%                         -0.980                       -0.978   \n",
       "max                         -0.966                       -0.967   \n",
       "\n",
       "       val_r2_keras_loss_epoch_283  val_r2_keras_loss_epoch_284  \\\n",
       "count                       18.000                       17.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.990                       -0.990   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.978                       -0.979   \n",
       "max                         -0.964                       -0.967   \n",
       "\n",
       "       val_r2_keras_loss_epoch_285  val_r2_keras_loss_epoch_286  \\\n",
       "count                       17.000                       16.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.990                       -0.990   \n",
       "25%                         -0.987                       -0.986   \n",
       "50%                         -0.983                       -0.983   \n",
       "75%                         -0.978                       -0.978   \n",
       "max                         -0.966                       -0.968   \n",
       "\n",
       "       val_r2_keras_loss_epoch_287  val_r2_keras_loss_epoch_288  \\\n",
       "count                       14.000                       14.000   \n",
       "mean                        -0.981                       -0.982   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.990                       -0.991   \n",
       "25%                         -0.985                       -0.986   \n",
       "50%                         -0.982                       -0.983   \n",
       "75%                         -0.978                       -0.978   \n",
       "max                         -0.968                       -0.968   \n",
       "\n",
       "       val_r2_keras_loss_epoch_289  val_r2_keras_loss_epoch_290  \\\n",
       "count                       13.000                       13.000   \n",
       "mean                        -0.981                       -0.981   \n",
       "std                          0.007                        0.006   \n",
       "min                         -0.991                       -0.990   \n",
       "25%                         -0.987                       -0.986   \n",
       "50%                         -0.983                       -0.982   \n",
       "75%                         -0.977                       -0.978   \n",
       "max                         -0.963                       -0.967   \n",
       "\n",
       "       val_r2_keras_loss_epoch_291  val_r2_keras_loss_epoch_292  \\\n",
       "count                       11.000                       11.000   \n",
       "mean                        -0.982                       -0.982   \n",
       "std                          0.006                        0.007   \n",
       "min                         -0.990                       -0.988   \n",
       "25%                         -0.987                       -0.986   \n",
       "50%                         -0.984                       -0.984   \n",
       "75%                         -0.980                       -0.979   \n",
       "max                         -0.968                       -0.963   \n",
       "\n",
       "       val_r2_keras_loss_epoch_293  val_r2_keras_loss_epoch_294  \\\n",
       "count                        9.000                        8.000   \n",
       "mean                        -0.981                       -0.981   \n",
       "std                          0.006                        0.006   \n",
       "min                         -0.988                       -0.988   \n",
       "25%                         -0.986                       -0.985   \n",
       "50%                         -0.982                       -0.984   \n",
       "75%                         -0.978                       -0.978   \n",
       "max                         -0.967                       -0.968   \n",
       "\n",
       "       val_r2_keras_loss_epoch_295  val_r2_keras_loss_epoch_296  \\\n",
       "count                        8.000                        8.000   \n",
       "mean                        -0.982                       -0.981   \n",
       "std                          0.007                        0.007   \n",
       "min                         -0.988                       -0.988   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.984                       -0.983   \n",
       "75%                         -0.977                       -0.977   \n",
       "max                         -0.968                       -0.968   \n",
       "\n",
       "       val_r2_keras_loss_epoch_297  val_r2_keras_loss_epoch_298  \\\n",
       "count                        7.000                        7.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.005                        0.004   \n",
       "min                         -0.988                       -0.988   \n",
       "25%                         -0.987                       -0.986   \n",
       "50%                         -0.984                       -0.982   \n",
       "75%                         -0.979                       -0.980   \n",
       "max                         -0.976                       -0.978   \n",
       "\n",
       "       val_r2_keras_loss_epoch_299  val_r2_keras_loss_epoch_300  \\\n",
       "count                        7.000                        7.000   \n",
       "mean                        -0.983                       -0.984   \n",
       "std                          0.004                        0.005   \n",
       "min                         -0.988                       -0.989   \n",
       "25%                         -0.987                       -0.987   \n",
       "50%                         -0.982                       -0.984   \n",
       "75%                         -0.980                       -0.980   \n",
       "max                         -0.978                       -0.977   \n",
       "\n",
       "       val_r2_keras_loss_epoch_301  val_r2_keras_loss_epoch_302  \\\n",
       "count                        7.000                        6.000   \n",
       "mean                        -0.983                       -0.983   \n",
       "std                          0.005                        0.004   \n",
       "min                         -0.988                       -0.988   \n",
       "25%                         -0.987                       -0.986   \n",
       "50%                         -0.984                       -0.984   \n",
       "75%                         -0.981                       -0.980   \n",
       "max                         -0.977                       -0.977   \n",
       "\n",
       "       val_r2_keras_loss_epoch_303  val_r2_keras_loss_epoch_304  \\\n",
       "count                        4.000                        4.000   \n",
       "mean                        -0.985                       -0.985   \n",
       "std                          0.005                        0.005   \n",
       "min                         -0.989                       -0.988   \n",
       "25%                         -0.988                       -0.988   \n",
       "50%                         -0.987                       -0.988   \n",
       "75%                         -0.984                       -0.985   \n",
       "max                         -0.978                       -0.978   \n",
       "\n",
       "       val_r2_keras_loss_epoch_305  val_r2_keras_loss_epoch_306  \\\n",
       "count                        3.000                        2.000   \n",
       "mean                        -0.986                       -0.987   \n",
       "std                          0.002                        0.002   \n",
       "min                         -0.988                       -0.989   \n",
       "25%                         -0.987                       -0.988   \n",
       "50%                         -0.987                       -0.987   \n",
       "75%                         -0.985                       -0.986   \n",
       "max                         -0.984                       -0.985   \n",
       "\n",
       "       val_r2_keras_loss_epoch_307  val_r2_keras_loss_epoch_308  \\\n",
       "count                        2.000                        2.000   \n",
       "mean                        -0.988                       -0.987   \n",
       "std                          0.000                        0.002   \n",
       "min                         -0.988                       -0.988   \n",
       "25%                         -0.988                       -0.988   \n",
       "50%                         -0.988                       -0.987   \n",
       "75%                         -0.988                       -0.986   \n",
       "max                         -0.988                       -0.985   \n",
       "\n",
       "       val_r2_keras_loss_epoch_309  val_r2_keras_loss_epoch_310  \\\n",
       "count                        2.000                        2.000   \n",
       "mean                        -0.988                       -0.988   \n",
       "std                          0.001                        0.001   \n",
       "min                         -0.989                       -0.989   \n",
       "25%                         -0.988                       -0.988   \n",
       "50%                         -0.988                       -0.988   \n",
       "75%                         -0.988                       -0.988   \n",
       "max                         -0.988                       -0.987   \n",
       "\n",
       "       val_r2_keras_loss_epoch_311  val_r2_keras_loss_epoch_312  \\\n",
       "count                        2.000                        1.000   \n",
       "mean                        -0.987                       -0.988   \n",
       "std                          0.001                          NaN   \n",
       "min                         -0.988                       -0.988   \n",
       "25%                         -0.987                       -0.988   \n",
       "50%                         -0.987                       -0.988   \n",
       "75%                         -0.986                       -0.988   \n",
       "max                         -0.986                       -0.988   \n",
       "\n",
       "       val_r2_keras_loss_epoch_313  \n",
       "count                        1.000  \n",
       "mean                        -0.989  \n",
       "std                            NaN  \n",
       "min                         -0.989  \n",
       "25%                         -0.989  \n",
       "50%                         -0.989  \n",
       "75%                         -0.989  \n",
       "max                         -0.989  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + path_identifier_lambda_net_data + '/' + list(history.keys())[1] + '_epoch_' + str(epochs_lambda).zfill(3) + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 10#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model ' + list(history.keys())[1])\n",
    "plt.ylabel(list(history.keys())[1])\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + path_identifier_lambda_net_data + '/' + list(history.keys())[0] + '_epoch_' + str(epochs_lambda).zfill(3) + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True).iloc[:,1:]\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model ' + list(history.keys())[0])\n",
    "plt.ylabel(list(history.keys())[0])\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
