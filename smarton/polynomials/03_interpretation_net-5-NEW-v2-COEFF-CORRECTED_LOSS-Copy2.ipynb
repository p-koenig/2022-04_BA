{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 2, #degree\n",
    "        'n': 5, #number of variables\n",
    "        'monomial_vars': None, #int or None\n",
    "        'laurent': False, #use Laurent polynomials (negative degree with up to -d)  \n",
    "        'neg_d': 0,#int or None\n",
    "        'neg_d_prob': 0,\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': 5,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 1,\n",
    "        'a_min': -1,\n",
    "        'lambda_nets_total': 10000,\n",
    "        'noise': 0,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'border_min': 0.2, #needs to be between 0 and (x_max-x_min)/2\n",
    "        'border_max': 0.4,\n",
    "        'lower_degree_prob': 0.5,\n",
    "        'a_zero_prob': 0.25,\n",
    "        'a_random_prob': 0.1,      \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True,  #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sample_sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000,\n",
    "    },\n",
    "    'i_net': {\n",
    "        'optimizer': 'custom',#adam\n",
    "        'inet_loss': 'mae',\n",
    "        'inet_metrics': ['r2'],\n",
    "        'dropout': 0.25,\n",
    "        'dropout_output': 0,\n",
    "        'epochs': 500, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "        'dense_layers': [512, 1024],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'interpretation_net_output_monomials': 5, #(None, int) #CONSTANT IS NOT INCLUDED\n",
    "        'interpretation_net_output_shape': None, #calculated automatically later\n",
    "        'test_size': 100, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'normalize_inet_data': False,\n",
    "        'inet_training_without_noise': False, #dataset size without noise hardcoded to 50k in generate_paths\n",
    "        'sparse_poly_representation_version': 2, #(1, 2); 1=old, 2=new\n",
    "\n",
    "        'evaluate_with_real_function': False, #False\n",
    "        'consider_labels_training': False, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },\n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': True,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "        \n",
    "        #set if samples analysis should be performed\n",
    "        'samples_list': None,#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "       \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "        \n",
    "        'symbolic_metamodeling_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_evaluation': False,\n",
    "        'symbolic_metamodeling_function_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_function_evaluation': False,\n",
    "        \n",
    "        'symbolic_regression_evaluation': True,\n",
    "        'per_network_evaluation': False,\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 10,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "n_jobs = min((epochs_lambda//each_epochs_save_lambda+1, n_jobs)) if multi_epoch_analysis else min(len(samples_list), n_jobs) if samples_list!=None else 1\n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "data_reshape_version = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 243\n",
      "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 2], [0, 0, 0, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 1, 2], [0, 0, 0, 2, 0], [0, 0, 0, 2, 1], [0, 0, 0, 2, 2], [0, 0, 1, 0, 0], [0, 0, 1, 0, 1], [0, 0, 1, 0, 2], [0, 0, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 2], [0, 0, 1, 2, 0], [0, 0, 1, 2, 1], [0, 0, 1, 2, 2], [0, 0, 2, 0, 0], [0, 0, 2, 0, 1], [0, 0, 2, 0, 2], [0, 0, 2, 1, 0], [0, 0, 2, 1, 1], [0, 0, 2, 1, 2], [0, 0, 2, 2, 0], [0, 0, 2, 2, 1], [0, 0, 2, 2, 2], [0, 1, 0, 0, 0], [0, 1, 0, 0, 1], [0, 1, 0, 0, 2], [0, 1, 0, 1, 0], [0, 1, 0, 1, 1], [0, 1, 0, 1, 2], [0, 1, 0, 2, 0], [0, 1, 0, 2, 1], [0, 1, 0, 2, 2], [0, 1, 1, 0, 0], [0, 1, 1, 0, 1], [0, 1, 1, 0, 2], [0, 1, 1, 1, 0], [0, 1, 1, 1, 1], [0, 1, 1, 1, 2], [0, 1, 1, 2, 0], [0, 1, 1, 2, 1], [0, 1, 1, 2, 2], [0, 1, 2, 0, 0], [0, 1, 2, 0, 1], [0, 1, 2, 0, 2], [0, 1, 2, 1, 0], [0, 1, 2, 1, 1], [0, 1, 2, 1, 2], [0, 1, 2, 2, 0], [0, 1, 2, 2, 1], [0, 1, 2, 2, 2], [0, 2, 0, 0, 0], [0, 2, 0, 0, 1], [0, 2, 0, 0, 2], [0, 2, 0, 1, 0], [0, 2, 0, 1, 1], [0, 2, 0, 1, 2], [0, 2, 0, 2, 0], [0, 2, 0, 2, 1], [0, 2, 0, 2, 2], [0, 2, 1, 0, 0], [0, 2, 1, 0, 1], [0, 2, 1, 0, 2], [0, 2, 1, 1, 0], [0, 2, 1, 1, 1], [0, 2, 1, 1, 2], [0, 2, 1, 2, 0], [0, 2, 1, 2, 1], [0, 2, 1, 2, 2], [0, 2, 2, 0, 0], [0, 2, 2, 0, 1], [0, 2, 2, 0, 2], [0, 2, 2, 1, 0], [0, 2, 2, 1, 1], [0, 2, 2, 1, 2], [0, 2, 2, 2, 0], [0, 2, 2, 2, 1], [0, 2, 2, 2, 2], [1, 0, 0, 0, 0], [1, 0, 0, 0, 1], [1, 0, 0, 0, 2], [1, 0, 0, 1, 0], [1, 0, 0, 1, 1], [1, 0, 0, 1, 2], [1, 0, 0, 2, 0], [1, 0, 0, 2, 1], [1, 0, 0, 2, 2], [1, 0, 1, 0, 0], [1, 0, 1, 0, 1], [1, 0, 1, 0, 2], [1, 0, 1, 1, 0], [1, 0, 1, 1, 1], [1, 0, 1, 1, 2], [1, 0, 1, 2, 0], [1, 0, 1, 2, 1], [1, 0, 1, 2, 2], [1, 0, 2, 0, 0], [1, 0, 2, 0, 1], [1, 0, 2, 0, 2], [1, 0, 2, 1, 0], [1, 0, 2, 1, 1], [1, 0, 2, 1, 2], [1, 0, 2, 2, 0], [1, 0, 2, 2, 1], [1, 0, 2, 2, 2], [1, 1, 0, 0, 0], [1, 1, 0, 0, 1], [1, 1, 0, 0, 2], [1, 1, 0, 1, 0], [1, 1, 0, 1, 1], [1, 1, 0, 1, 2], [1, 1, 0, 2, 0], [1, 1, 0, 2, 1], [1, 1, 0, 2, 2], [1, 1, 1, 0, 0], [1, 1, 1, 0, 1], [1, 1, 1, 0, 2], [1, 1, 1, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 2], [1, 1, 1, 2, 0], [1, 1, 1, 2, 1], [1, 1, 1, 2, 2], [1, 1, 2, 0, 0], [1, 1, 2, 0, 1], [1, 1, 2, 0, 2], [1, 1, 2, 1, 0], [1, 1, 2, 1, 1], [1, 1, 2, 1, 2], [1, 1, 2, 2, 0], [1, 1, 2, 2, 1], [1, 1, 2, 2, 2], [1, 2, 0, 0, 0], [1, 2, 0, 0, 1], [1, 2, 0, 0, 2], [1, 2, 0, 1, 0], [1, 2, 0, 1, 1], [1, 2, 0, 1, 2], [1, 2, 0, 2, 0], [1, 2, 0, 2, 1], [1, 2, 0, 2, 2], [1, 2, 1, 0, 0], [1, 2, 1, 0, 1], [1, 2, 1, 0, 2], [1, 2, 1, 1, 0], [1, 2, 1, 1, 1], [1, 2, 1, 1, 2], [1, 2, 1, 2, 0], [1, 2, 1, 2, 1], [1, 2, 1, 2, 2], [1, 2, 2, 0, 0], [1, 2, 2, 0, 1], [1, 2, 2, 0, 2], [1, 2, 2, 1, 0], [1, 2, 2, 1, 1], [1, 2, 2, 1, 2], [1, 2, 2, 2, 0], [1, 2, 2, 2, 1], [1, 2, 2, 2, 2], [2, 0, 0, 0, 0], [2, 0, 0, 0, 1], [2, 0, 0, 0, 2], [2, 0, 0, 1, 0], [2, 0, 0, 1, 1], [2, 0, 0, 1, 2], [2, 0, 0, 2, 0], [2, 0, 0, 2, 1], [2, 0, 0, 2, 2], [2, 0, 1, 0, 0], [2, 0, 1, 0, 1], [2, 0, 1, 0, 2], [2, 0, 1, 1, 0], [2, 0, 1, 1, 1], [2, 0, 1, 1, 2], [2, 0, 1, 2, 0], [2, 0, 1, 2, 1], [2, 0, 1, 2, 2], [2, 0, 2, 0, 0], [2, 0, 2, 0, 1], [2, 0, 2, 0, 2], [2, 0, 2, 1, 0], [2, 0, 2, 1, 1], [2, 0, 2, 1, 2], [2, 0, 2, 2, 0], [2, 0, 2, 2, 1], [2, 0, 2, 2, 2], [2, 1, 0, 0, 0], [2, 1, 0, 0, 1], [2, 1, 0, 0, 2], [2, 1, 0, 1, 0], [2, 1, 0, 1, 1], [2, 1, 0, 1, 2], [2, 1, 0, 2, 0], [2, 1, 0, 2, 1], [2, 1, 0, 2, 2], [2, 1, 1, 0, 0], [2, 1, 1, 0, 1], [2, 1, 1, 0, 2], [2, 1, 1, 1, 0], [2, 1, 1, 1, 1], [2, 1, 1, 1, 2], [2, 1, 1, 2, 0], [2, 1, 1, 2, 1], [2, 1, 1, 2, 2], [2, 1, 2, 0, 0], [2, 1, 2, 0, 1], [2, 1, 2, 0, 2], [2, 1, 2, 1, 0], [2, 1, 2, 1, 1], [2, 1, 2, 1, 2], [2, 1, 2, 2, 0], [2, 1, 2, 2, 1], [2, 1, 2, 2, 2], [2, 2, 0, 0, 0], [2, 2, 0, 0, 1], [2, 2, 0, 0, 2], [2, 2, 0, 1, 0], [2, 2, 0, 1, 1], [2, 2, 0, 1, 2], [2, 2, 0, 2, 0], [2, 2, 0, 2, 1], [2, 2, 0, 2, 2], [2, 2, 1, 0, 0], [2, 2, 1, 0, 1], [2, 2, 1, 0, 2], [2, 2, 1, 1, 0], [2, 2, 1, 1, 1], [2, 2, 1, 1, 2], [2, 2, 1, 2, 0], [2, 2, 1, 2, 1], [2, 2, 1, 2, 2], [2, 2, 2, 0, 0], [2, 2, 2, 0, 1], [2, 2, 2, 0, 2], [2, 2, 2, 1, 0], [2, 2, 2, 1, 1], [2, 2, 2, 1, 2], [2, 2, 2, 2, 0], [2, 2, 2, 2, 1], [2, 2, 2, 2, 2]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1775bae74db743d7b630dab7abd74faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 21\n",
      "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 2], [0, 0, 0, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 2, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 1], [0, 0, 1, 1, 0], [0, 0, 2, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 1], [0, 1, 0, 1, 0], [0, 1, 1, 0, 0], [0, 2, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 1], [1, 0, 0, 1, 0], [1, 0, 1, 0, 0], [1, 1, 0, 0, 0], [2, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import flatten, rec_gen\n",
    "\n",
    "list_of_monomial_identifiers_extended = []\n",
    "\n",
    "if laurent:\n",
    "    variable_sets = [list(flatten([[_d for _d in range(d+1)], [-_d for _d in range(1, neg_d+1)]])) for _ in range(n)]\n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)    \n",
    "        \n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity:' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)        \n",
    "else:\n",
    "    variable_sets = [[_d for _d in range(d+1)] for _ in range(n)]  \n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)\n",
    "\n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity: ' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)    \n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    if np.sum(monomial_identifier) <= d:\n",
    "        if monomial_vars == None or len(list(filter(lambda x: x != 0, monomial_identifier))) <= monomial_vars:\n",
    "            list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "#print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "#print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape:  80\n"
     ]
    }
   ],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "config['i_net']['data_reshape_version'] = data_reshape_version\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d']) if not laurent else len(list_of_monomial_identifiers)\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "\n",
    "config['i_net']['interpretation_net_output_shape'] = config['data']['sparsity'] if config['i_net']['interpretation_net_output_monomials'] is None else config['data']['sparsity']*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials'] if config['i_net']['sparse_poly_representation_version'] == 1 else config['data']['n']*(config['data']['d']+1)*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials']  \n",
    "print('Output Shape: ', config['i_net']['interpretation_net_output_shape'])\n",
    "\n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sample_sparsity')*config['data']['sample_sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "\n",
    "\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='interpretation_net'))\n",
    "create_folders_inet()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inet_dense512-1024-output_80_drop0.25e500b256_custom/lnets_10000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-1_amax_1_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n",
      "lnets_10000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-1_amax_1_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net_data)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_degree(maximum, maximum_indices):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.8333333  0.16666667 0.25       0.33333334 0.41666666 0.5\n",
      " 0.25       1.5        0.75       0.9083333  0.9166667  1.\n",
      " 0.875      0.20833333 0.29166666 0.875      0.41666666 0.5416667\n",
      " 0.625      0.7083333  0.75       0.8333333  0.9166667  1.\n",
      " 0.8333333  0.16666667 0.25       0.33333334 0.41666666 0.5\n",
      " 0.25       1.5        0.75       0.9083333  0.9166667  1.\n",
      " 0.875      0.20833333 0.29166666 0.875      0.41666666 0.5416667\n",
      " 0.625      0.7083333  0.75       0.8333333  0.9166667  1.        ], shape=(48,), dtype=float32)\n",
      "[<tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
      "array([0.8333333 , 0.16666667, 0.25      , 0.33333334, 0.41666666,\n",
      "       0.5       , 0.25      , 1.5       , 0.75      , 0.9083333 ,\n",
      "       0.9166667 , 1.        ], dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
      "array([0.875     , 0.20833333, 0.29166666, 0.875     , 0.41666666,\n",
      "       0.5416667 , 0.625     , 0.7083333 , 0.75      , 0.8333333 ,\n",
      "       0.9166667 , 1.        ], dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
      "array([0.8333333 , 0.16666667, 0.25      , 0.33333334, 0.41666666,\n",
      "       0.5       , 0.25      , 1.5       , 0.75      , 0.9083333 ,\n",
      "       0.9166667 , 1.        ], dtype=float32)>, <tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
      "array([0.875     , 0.20833333, 0.29166666, 0.875     , 0.41666666,\n",
      "       0.5416667 , 0.625     , 0.7083333 , 0.75      , 0.8333333 ,\n",
      "       0.9166667 , 1.        ], dtype=float32)>]\n",
      "l1_by_monomial_by_var tf.Tensor(\n",
      "[[[0.8333333  0.16666667 0.25      ]\n",
      "  [0.33333334 0.41666666 0.5       ]\n",
      "  [0.25       1.5        0.75      ]\n",
      "  [0.9083333  0.9166667  1.        ]]\n",
      "\n",
      " [[0.875      0.20833333 0.29166666]\n",
      "  [0.875      0.41666666 0.5416667 ]\n",
      "  [0.625      0.7083333  0.75      ]\n",
      "  [0.8333333  0.9166667  1.        ]]\n",
      "\n",
      " [[0.8333333  0.16666667 0.25      ]\n",
      "  [0.33333334 0.41666666 0.5       ]\n",
      "  [0.25       1.5        0.75      ]\n",
      "  [0.9083333  0.9166667  1.        ]]\n",
      "\n",
      " [[0.875      0.20833333 0.29166666]\n",
      "  [0.875      0.41666666 0.5416667 ]\n",
      "  [0.625      0.7083333  0.75      ]\n",
      "  [0.8333333  0.9166667  1.        ]]], shape=(4, 4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "l1 = tf.constant([10,2,3,4,5,6,3,18,9,10.9,11,12,10.5,2.5,3.5,10.5,5,6.5,7.5,8.5,9,10,11,12,10,2,3,4,5,6,3,18,9,10.9,11,12,10.5,2.5,3.5,10.5,5,6.5,7.5,8.5,9,10,11,12])/12\n",
    "print(l1)\n",
    "l1_by_monomial = tf.split(l1, 4)\n",
    "print(l1_by_monomial)\n",
    "l1_by_monomial_split = []\n",
    "for tensor in l1_by_monomial:\n",
    "    l1_by_monomial_split.append(tf.split(tensor, 4))\n",
    "\n",
    "l1_by_monomial_split = tf.stack(l1_by_monomial_split)\n",
    "print('l1_by_monomial_by_var', l1_by_monomial_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.8333333  0.16666667 0.25       0.33333334 0.41666666 0.5\n",
      " 0.25       1.5        0.75       0.9083333  0.9166667  1.\n",
      " 0.875      0.20833333 0.29166666 0.875      0.41666666 0.5416667\n",
      " 0.625      0.7083333  0.75       0.8333333  0.9166667  1.\n",
      " 0.8333333  0.16666667 0.25       0.33333334 0.41666666 0.5\n",
      " 0.25       1.5        0.75       0.9083333  0.9166667  1.\n",
      " 0.875      0.20833333 0.29166666 0.875      0.41666666 0.5416667\n",
      " 0.625      0.7083333  0.75       0.8333333  0.9166667  1.        ], shape=(48,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.8333333  0.875      0.8333333  0.875     ]\n",
      " [0.16666667 0.20833333 0.16666667 0.20833333]\n",
      " [0.25       0.29166666 0.25       0.29166666]\n",
      " [0.33333334 0.875      0.33333334 0.875     ]\n",
      " [0.41666666 0.41666666 0.41666666 0.41666666]\n",
      " [0.5        0.5416667  0.5        0.5416667 ]\n",
      " [0.25       0.625      0.25       0.625     ]\n",
      " [1.5        0.7083333  1.5        0.7083333 ]\n",
      " [0.75       0.75       0.75       0.75      ]\n",
      " [0.9083333  0.8333333  0.9083333  0.8333333 ]\n",
      " [0.9166667  0.9166667  0.9166667  0.9166667 ]\n",
      " [1.         1.         1.         1.        ]], shape=(12, 4), dtype=float32)\n",
      "[<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
      "array([[0.8333333 , 0.875     , 0.8333333 , 0.875     ],\n",
      "       [0.16666667, 0.20833333, 0.16666667, 0.20833333],\n",
      "       [0.25      , 0.29166666, 0.25      , 0.29166666]], dtype=float32)>, <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
      "array([[0.33333334, 0.875     , 0.33333334, 0.875     ],\n",
      "       [0.41666666, 0.41666666, 0.41666666, 0.41666666],\n",
      "       [0.5       , 0.5416667 , 0.5       , 0.5416667 ]], dtype=float32)>, <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
      "array([[0.25     , 0.625    , 0.25     , 0.625    ],\n",
      "       [1.5      , 0.7083333, 1.5      , 0.7083333],\n",
      "       [0.75     , 0.75     , 0.75     , 0.75     ]], dtype=float32)>, <tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
      "array([[0.9083333, 0.8333333, 0.9083333, 0.8333333],\n",
      "       [0.9166667, 0.9166667, 0.9166667, 0.9166667],\n",
      "       [1.       , 1.       , 1.       , 1.       ]], dtype=float32)>]\n",
      "l1_by_monomial_by_var [<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[0.8333333 , 0.16666667, 0.25      ],\n",
      "       [0.875     , 0.20833333, 0.29166666],\n",
      "       [0.8333333 , 0.16666667, 0.25      ],\n",
      "       [0.875     , 0.20833333, 0.29166666]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[0.33333334, 0.41666666, 0.5       ],\n",
      "       [0.875     , 0.41666666, 0.5416667 ],\n",
      "       [0.33333334, 0.41666666, 0.5       ],\n",
      "       [0.875     , 0.41666666, 0.5416667 ]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[0.25     , 1.5      , 0.75     ],\n",
      "       [0.625    , 0.7083333, 0.75     ],\n",
      "       [0.25     , 1.5      , 0.75     ],\n",
      "       [0.625    , 0.7083333, 0.75     ]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[0.9083333, 0.9166667, 1.       ],\n",
      "       [0.8333333, 0.9166667, 1.       ],\n",
      "       [0.9083333, 0.9166667, 1.       ],\n",
      "       [0.8333333, 0.9166667, 1.       ]], dtype=float32)>]\n",
      "maximum tf.Tensor(\n",
      "[[0.8333333 0.5       1.5       1.       ]\n",
      " [0.875     0.875     0.75      1.       ]\n",
      " [0.8333333 0.5       1.5       1.       ]\n",
      " [0.875     0.875     0.75      1.       ]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0 2 1 2]\n",
      " [0 0 2 2]\n",
      " [0 2 1 2]\n",
      " [0 0 2 2]], shape=(4, 4), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "l1 = tf.constant([10,2,3,4,5,6,3,18,9,10.9,11,12,10.5,2.5,3.5,10.5,5,6.5,7.5,8.5,9,10,11,12,10,2,3,4,5,6,3,18,9,10.9,11,12,10.5,2.5,3.5,10.5,5,6.5,7.5,8.5,9,10,11,12])/12\n",
    "print(l1)\n",
    "l1_by_monomial = tf.transpose(tf.stack(tf.split(l1, 4)))\n",
    "print(l1_by_monomial)\n",
    "l1_by_monomial_by_var = tf.split(l1_by_monomial, 4, axis=0)\n",
    "print(l1_by_monomial_by_var)\n",
    "l1_by_monomial_by_var_new = []\n",
    "for tensor in l1_by_monomial_by_var:\n",
    "    l1_by_monomial_by_var_new.append(tf.transpose(tensor))\n",
    "l1_by_monomial_by_var = l1_by_monomial_by_var_new   \n",
    "print('l1_by_monomial_by_var', l1_by_monomial_by_var)\n",
    "\n",
    "maximum = tf.transpose(tf.reduce_max(l1_by_monomial_by_var, axis=2))\n",
    "print('maximum', maximum)\n",
    "\n",
    "indices = tf.transpose(tf.argmax(l1_by_monomial_by_var, axis=2))\n",
    "print(indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[1.5      , 1.       , 0.8333333, 0.5      ],\n",
       "       [1.       , 0.875    , 0.875    , 0.75     ],\n",
       "       [1.5      , 1.       , 0.8333333, 0.5      ],\n",
       "       [1.       , 0.875    , 0.875    , 0.75     ]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum_sort = tf.sort(maximum, direction='DESCENDING')\n",
    "maximum_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\n",
       "array([[2, 2, 1, 0],\n",
       "       [2, 2, 0, 0],\n",
       "       [2, 2, 1, 0],\n",
       "       [2, 2, 0, 0]])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_sort = tf.sort(indices, direction='DESCENDING')\n",
    "indices_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\n",
       "array([[2, 3, 0, 1],\n",
       "       [3, 0, 1, 2],\n",
       "       [2, 3, 0, 1],\n",
       "       [3, 0, 1, 2]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum_sort_index = tf.cast(tf.argsort(maximum, direction='DESCENDING'), tf.int64)\n",
    "\n",
    "maximum_sort_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\n",
       "array([[1, 3, 2, 0],\n",
       "       [2, 3, 0, 1],\n",
       "       [1, 3, 2, 0],\n",
       "       [2, 3, 0, 1]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_sort_index = tf.cast(tf.argsort(indices, direction='DESCENDING'), tf.int64)\n",
    "indices_sort_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_list_items(_list):\n",
    "    total = 0\n",
    "\n",
    "    def do_the_sum(_list):\n",
    "\n",
    "        # Define the total variable as non-local, causing it to bind\n",
    "        # to the nearest non-global variable also called total.\n",
    "        nonlocal total\n",
    "\n",
    "        for i in _list:\n",
    "            total += i\n",
    "\n",
    "    do_the_sum(_list)\n",
    "\n",
    "    return total\n",
    "\n",
    "sum_list_items([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_function_wrapper(whole_arrway):\n",
    "    #global function_counter\n",
    "    #global count_in_function\n",
    "    #global gather\n",
    "    #print('counte here', counter)\n",
    "    function_counter = 0#tf.Variable(0, dtype=tf.int64)\n",
    "    #count_in_function = 0\n",
    "    #gather=0\n",
    "    \n",
    "    def select_function(element):\n",
    "        nonlocal function_counter   \n",
    "        #tf.print('whole_arrway', whole_arrway)\n",
    "        #tf.print('element', element)\n",
    "        \n",
    "        #tf.print('counter', counter)\n",
    "        \n",
    "        gather = tf.gather(whole_arrway, element)\n",
    "        #count_in_function = gather\n",
    "        \n",
    "        #new_counter = counter + gather\n",
    "        #tf.print('new_counter', new_counter)\n",
    "     \n",
    "        if tf.math.greater(function_counter + gather, d):\n",
    "            return_value = 0\n",
    "        else:\n",
    "            return_value = gather\n",
    "            #count_in_function = gather\n",
    "            \n",
    "        function_counter = function_counter+return_value\n",
    "        #counter = counter + 1#+ gather\n",
    "        #tf.print('counter', counter)\n",
    "        #tf.print('gather', gather)\n",
    "        return return_value\n",
    "    \n",
    "    #return_result = select_function\n",
    "    \n",
    "    #print('counte after', counter)\n",
    "    \n",
    "    return select_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices [[0 2 1 2]\n",
      " [0 0 2 2]\n",
      " [0 2 1 2]\n",
      " [0 0 2 2]]\n",
      "maximum_sort_index [[2 3 0 1]\n",
      " [3 0 1 2]\n",
      " [2 3 0 1]\n",
      " [3 0 1 2]]\n",
      "indices TensorShape([4, 4])\n",
      "maximum_sort_index TensorShape([4, 4])\n",
      "indices tf.int64\n",
      "maximum_sort_index tf.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\n",
       "array([[1, 0, 0, 0],\n",
       "       [2, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [2, 0, 0, 0]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.print('indices', indices)\n",
    "tf.print('maximum_sort_index', maximum_sort_index)\n",
    "\n",
    "tf.print('indices', indices.shape)\n",
    "tf.print('maximum_sort_index', maximum_sort_index.shape)\n",
    "\n",
    "tf.print('indices', indices.dtype)\n",
    "tf.print('maximum_sort_index', maximum_sort_index.dtype)\n",
    "#maximum = tf.transpose(tf.reduce_max(l1_by_monomial_by_var, axis=2))\n",
    "#print('maximum', maximum)\n",
    "\n",
    "#indices = tf.transpose(tf.argmax(l1_by_monomial_by_var, axis=2))\n",
    "#print(indices)\n",
    "\n",
    "\n",
    "#maximum_sort_index = tf.argsort(maximum, direction='DESCENDING')\n",
    "\n",
    "\n",
    "tf.map_fn(fn=lambda x: tf.map_fn(fn=select_function_wrapper(x[0]),  elems=x[1], fn_output_signature=tf.int64),  elems=(indices, maximum_sort_index), fn_output_signature=tf.TensorSpec([maximum_sort_index.shape[1]], dtype=tf.int64))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_sth():\n",
    "    def limit_monomial_to_degree_wrapper(monomial_degrees_by_variable):\n",
    "        #global current_monomial_degree\n",
    "        current_monomial_degree = 0#tf.Variable(0, dtype=tf.int64)\n",
    "\n",
    "        def limit_monomial_to_degree(index):\n",
    "            nonlocal current_monomial_degree   \n",
    "            #tf.print('current_monomial_degree', current_monomial_degree, summarize=-1)\n",
    "            additional_degree = tf.gather(monomial_degrees_by_variable, index)\n",
    "\n",
    "            if tf.math.greater(current_monomial_degree + additional_degree, d):\n",
    "                adjusted_additional_degree = tf.cast(0, tf.int64)\n",
    "            else:\n",
    "                adjusted_additional_degree = additional_degree\n",
    "\n",
    "            current_monomial_degree = current_monomial_degree + adjusted_additional_degree\n",
    "\n",
    "            #tf.print('current_monomial_degree', current_monomial_degree, summarize=-1)\n",
    "            #tf.print('monomial_degrees_by_variable', monomial_degrees_by_variable, summarize=-1)\n",
    "            #tf.print('index', index, summarize=-1)\n",
    "            #tf.print('adjusted_additional_degree', adjusted_additional_degree, summarize=-1)\n",
    "\n",
    "            return tf.stack([index, adjusted_additional_degree])\n",
    "\n",
    "\n",
    "        return limit_monomial_to_degree   \n",
    "    \n",
    "    result = tf.map_fn(fn=lambda x: tf.map_fn(fn=limit_monomial_to_degree_wrapper(x[0]),  elems=x[1], fn_output_signature=tf.TensorSpec([2], dtype=tf.int64)),  elems=(indices, maximum_sort_index), fn_output_signature=tf.TensorSpec([maximum_sort_index.shape[1], 2], dtype=tf.int64))\n",
    "    print(result)\n",
    "    \n",
    "    result_sorted=tf.map_fn(fn=lambda x: tf.keras.backend.flatten(tf.gather(x[:,1:], x[:,:1])), elems=result)\n",
    "    print(result_sorted)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[2 1]\n",
      "  [3 0]\n",
      "  [0 0]\n",
      "  [1 0]]\n",
      "\n",
      " [[3 2]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [2 0]]\n",
      "\n",
      " [[2 1]\n",
      "  [3 0]\n",
      "  [0 0]\n",
      "  [1 0]]\n",
      "\n",
      " [[3 2]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [2 0]]], shape=(4, 4, 2), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0 0 1 0]\n",
      " [0 2 0 0]\n",
      " [0 0 1 0]\n",
      " [0 2 0 0]], shape=(4, 4), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "do_sth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpretation_net_output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_monomial_degree = tf.Variable(0, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.36730143>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_poly_fv_tf_wrapper(list_of_monomial_identifiers, tf.constant([random.uniform(0, 1) for i in range(interpretation_net_output_shape)], tf.float32), current_monomial_degree)(tf.constant([0.5 for i in range(5)], tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.map_fn(fn=lambda x: tf.map_fn(fn=limit_monomial_to_degree_wrapper(x[0]),  elems=x[1], fn_output_signature=tf.int64),  elems=(indices, maximum_sort_index), fn_output_signature=tf.TensorSpec([maximum_sort_index.shape[1]], dtype=tf.int64))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.map_fn(fn=lambda x: tf.map_fn(fn=limit_monomial_to_degree_wrapper(x[0]),  elems=x[1], fn_output_signature=tf.int64),  elems=(indices, maximum_sort_index), fn_output_signature=tf.TensorSpec([maximum_sort_index.shape[1]], dtype=tf.int64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index, no_noise=False):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path_identifier_lambda_net_data_loading = None \n",
    "                \n",
    "    if no_noise==True:\n",
    "        path_identifier_lambda_net_data_loading = generate_paths(path_type='interpretation_net_no_noise')['path_identifier_lambda_net_data']\n",
    "    else:\n",
    "        path_identifier_lambda_net_data_loading = path_identifier_lambda_net_data \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_identifier_lambda_net_data_loading + '/'\n",
    "    path_weights = directory + 'weights_epoch_' + str(index).zfill(3) + '.txt'\n",
    "    path_X_data = directory + 'lambda_X_test_data.txt'\n",
    "    path_y_data = directory + 'lambda_y_test_data.txt'        \n",
    "    \n",
    "    weight_data = pd.read_csv(path_weights, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        weight_data = weight_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_X_test_data = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_y_test_data = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "    lambda_nets = [None] * weight_data.shape[0]\n",
    "    for i, (row_weights, row_lambda_X_test_data, row_lambda_y_test_data) in enumerate(zip(weight_data.values, lambda_X_test_data.values, lambda_y_test_data.values)):        \n",
    "        lambda_net = LambdaNet(row_weights, row_lambda_X_test_data, row_lambda_y_test_data)\n",
    "        lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend MultiprocessingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 out of   1 | elapsed:   23.9s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if inet_training_without_noise:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list_without_noise = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1, no_noise=True) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "else:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:30:49.711839Z",
     "start_time": "2021-01-05T09:29:48.873305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.838</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.382</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.585</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.459</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.383</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.589</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.532</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.727</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.479</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.561</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.514</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.486</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.597</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.905</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.861</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.905</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.406</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.387</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.576</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "6252  1373158606         0.000         0.000         0.000         0.000   \n",
       "4684  1373158606         0.215         0.000         0.000         0.000   \n",
       "1731  1373158606         0.000         0.000         0.480         0.479   \n",
       "4742  1373158606         0.000         0.000         0.133         0.000   \n",
       "4521  1373158606         0.000         0.003         0.000         0.000   \n",
       "\n",
       "      00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "6252         0.000         0.000         0.000         0.135         0.000   \n",
       "4684         0.000         0.000         0.000         0.000         0.000   \n",
       "1731         0.000         0.000         0.000         0.000         0.000   \n",
       "4742         0.905        -0.360         0.000         0.000         0.000   \n",
       "4521        -0.949         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "6252         0.278         0.000        -0.016         0.000         0.000   \n",
       "4684         0.276         0.000         0.000         0.000        -0.446   \n",
       "1731         0.000         0.000         0.000         0.000        -0.350   \n",
       "4742         0.089         0.000         0.000         0.000         0.000   \n",
       "4521         0.000        -0.281         0.000         0.000         0.000   \n",
       "\n",
       "      02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "6252         0.000         0.000         0.000         0.000         0.705   \n",
       "4684        -0.313        -0.867         0.000         0.000         0.000   \n",
       "1731         0.793         0.000         0.000         0.000        -0.642   \n",
       "4742         0.000         0.000         0.000         0.000         0.000   \n",
       "4521         0.000         0.000         0.000        -0.529         0.000   \n",
       "\n",
       "      11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "6252         0.838         0.000              -0.029               0.015   \n",
       "4684         0.000         0.000               0.210               0.009   \n",
       "1731         0.000         0.000               0.006               0.030   \n",
       "4742         0.000        -0.173              -0.035               0.060   \n",
       "4521         0.000        -0.378               0.020              -0.009   \n",
       "\n",
       "      00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "6252              -0.011               0.019               0.005   \n",
       "4684              -0.006              -0.014              -0.003   \n",
       "1731               0.448               0.465               0.000   \n",
       "4742               0.103               0.034               0.861   \n",
       "4521               0.002              -0.041              -0.899   \n",
       "\n",
       "      00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "6252              -0.016               0.045               0.125   \n",
       "4684               0.009               0.010              -0.001   \n",
       "1731              -0.008               0.003              -0.004   \n",
       "4742              -0.369               0.032               0.005   \n",
       "4521               0.010              -0.027              -0.004   \n",
       "\n",
       "      00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "6252              -0.003               0.258               0.024   \n",
       "4684               0.004               0.257               0.007   \n",
       "1731               0.013              -0.020               0.054   \n",
       "4742              -0.014               0.063               0.027   \n",
       "4521               0.000               0.018              -0.281   \n",
       "\n",
       "      01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "6252              -0.022              -0.006              -0.002   \n",
       "4684               0.003               0.005              -0.426   \n",
       "1731               0.005               0.012              -0.380   \n",
       "4742              -0.005               0.000              -0.009   \n",
       "4521              -0.010               0.005               0.009   \n",
       "\n",
       "      02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "6252              -0.004               0.033              -0.000   \n",
       "4684              -0.325              -0.856              -0.005   \n",
       "1731               0.727              -0.047               0.003   \n",
       "4742              -0.003               0.001              -0.012   \n",
       "4521              -0.012              -0.018              -0.011   \n",
       "\n",
       "      10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "6252               0.001               0.661               0.810   \n",
       "4684               0.001              -0.005              -0.012   \n",
       "1731               0.024              -0.586               0.042   \n",
       "4742               0.001               0.014              -0.031   \n",
       "4521              -0.506               0.000               0.013   \n",
       "\n",
       "      20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "6252               0.001              -0.000               0.000   \n",
       "4684              -0.001               0.215               0.000   \n",
       "1731              -0.020              -0.000               0.000   \n",
       "4742              -0.161               0.000              -0.000   \n",
       "4521              -0.368               0.000               0.003   \n",
       "\n",
       "      00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "6252              -0.000               0.000               0.000   \n",
       "4684              -0.000              -0.000              -0.000   \n",
       "1731               0.480               0.479              -0.000   \n",
       "4742               0.133              -0.000               0.905   \n",
       "4521              -0.000               0.000              -0.949   \n",
       "\n",
       "      00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "6252              -0.000              -0.000               0.135   \n",
       "4684              -0.000               0.000              -0.000   \n",
       "1731               0.000               0.000               0.000   \n",
       "4742              -0.360              -0.000               0.000   \n",
       "4521               0.000               0.000              -0.000   \n",
       "\n",
       "      00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "6252              -0.000               0.278              -0.000   \n",
       "4684              -0.000               0.276              -0.000   \n",
       "1731              -0.000              -0.000               0.000   \n",
       "4742               0.000               0.089               0.000   \n",
       "4521              -0.000               0.000              -0.281   \n",
       "\n",
       "      01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "6252              -0.016               0.000               0.000   \n",
       "4684              -0.000               0.000              -0.446   \n",
       "1731              -0.000               0.000              -0.350   \n",
       "4742               0.000              -0.000               0.000   \n",
       "4521               0.000              -0.000               0.000   \n",
       "\n",
       "      02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "6252              -0.000               0.000               0.000   \n",
       "4684              -0.313              -0.867              -0.000   \n",
       "1731               0.793               0.000              -0.000   \n",
       "4742              -0.000               0.000               0.000   \n",
       "4521               0.000              -0.000               0.000   \n",
       "\n",
       "      10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "6252              -0.000               0.705               0.838   \n",
       "4684               0.000              -0.000               0.000   \n",
       "1731              -0.000              -0.642               0.000   \n",
       "4742              -0.000              -0.000              -0.000   \n",
       "4521              -0.529               0.000               0.000   \n",
       "\n",
       "      20000-lstsq_target   wb_0   wb_1  wb_2  wb_3   wb_4  wb_5  wb_6   wb_7  \\\n",
       "6252              -0.000 -0.056 -0.377 0.493 0.396  0.317 0.261 0.173 -0.144   \n",
       "4684              -0.000 -0.070 -0.047 0.411 0.121  0.141 0.051 0.277 -0.297   \n",
       "1731              -0.000 -0.074 -0.276 0.460 0.057  0.277 0.154 0.419 -0.351   \n",
       "4742              -0.173  0.006 -0.297 0.337 0.265 -0.014 0.152 0.297 -0.412   \n",
       "4521              -0.378 -0.058 -0.067 0.262 0.352  0.040 0.096 0.346 -0.121   \n",
       "\n",
       "       wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "6252 -0.055 0.467  0.176 -0.326 -0.269  0.352  0.250  0.410  0.298  0.375   \n",
       "4684  0.224 0.509  0.589 -0.202  0.162  0.439  0.037  0.547  0.002  0.168   \n",
       "1731  0.195 0.189  0.561 -0.306  0.211  0.406  0.148  0.442  0.194  0.259   \n",
       "4742  0.195 0.239  0.522 -0.194 -0.153  0.356  0.139  0.430  0.015  0.262   \n",
       "4521  0.260 0.395  0.543 -0.163  0.153  0.413  0.093  0.489  0.101  0.187   \n",
       "\n",
       "      wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "6252  0.201  0.426  0.271  0.295 -0.518 -0.180 -0.088 -0.160  0.303 -0.323   \n",
       "4684  0.199  0.209 -0.044  0.260  0.036  0.006 -0.501 -0.151  0.268 -0.138   \n",
       "1731  0.308  0.241  0.001 -0.074 -0.273  0.115 -0.484 -0.144 -0.065 -0.056   \n",
       "4742  0.304  0.196  0.052 -0.019  0.158 -0.013 -0.496 -0.004  0.002 -0.023   \n",
       "4521  0.274  0.211  0.099  0.289  0.176  0.059 -0.478 -0.149  0.005 -0.273   \n",
       "\n",
       "      wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "6252 -0.274  0.005  0.378  0.237  0.238  0.171  0.382 -0.372 -0.264  0.089   \n",
       "4684 -0.640 -0.045  0.203  0.460  0.063  0.385  0.094 -0.076 -0.413  0.187   \n",
       "1731 -0.303 -0.235  0.358  0.005 -0.023  0.216  0.066 -0.100 -0.552 -0.025   \n",
       "4742  0.011 -0.026  0.286  0.351  0.056  0.244 -0.021 -0.097 -0.321  0.009   \n",
       "4521 -0.047 -0.162  0.249  0.386  0.074  0.312 -0.017 -0.208 -0.411  0.096   \n",
       "\n",
       "      wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "6252 -0.365 -0.130 -0.137  0.222  0.546  0.007  0.418  0.003 -0.049  0.391   \n",
       "4684 -0.020 -0.313  0.274  0.195  0.354 -0.140  0.241 -0.229  0.167  0.567   \n",
       "1731 -0.022 -0.143  0.120  0.458  0.514 -0.078  0.645  0.206 -0.095  0.791   \n",
       "4742 -0.072 -0.234  0.020 -0.010  0.438 -0.138  0.403  0.209 -0.102  0.036   \n",
       "4521 -0.140 -0.207  0.100  0.012  0.378 -0.140  0.343 -0.027  0.032  0.471   \n",
       "\n",
       "      wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "6252  0.240 -0.150 -0.109  0.150 -0.243 -0.222 -0.193  0.536  0.302 -0.187   \n",
       "4684  0.412 -0.500 -0.105  0.053 -0.147 -0.163 -0.258  0.516  0.333 -0.514   \n",
       "1731  0.240 -0.493 -0.110 -0.021  0.001 -0.190 -0.349  0.384  0.500 -0.429   \n",
       "4742  0.252 -0.475  0.013 -0.001 -0.009 -0.003 -0.253  0.482  0.392 -0.529   \n",
       "4521  0.360 -0.485 -0.104  0.110 -0.455  0.009 -0.260  0.450  0.406 -0.234   \n",
       "\n",
       "      wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "6252 -0.101  0.177  0.217  0.215  0.185 -0.029  0.404 -0.064  0.114  0.510   \n",
       "4684 -0.001 -0.051  0.217  0.199  0.294  0.140  0.392 -0.007 -0.474  0.477   \n",
       "1731  0.140 -0.074  0.448  0.446  0.415  0.333  0.250  0.176 -0.351  0.347   \n",
       "4742 -0.091 -0.046  0.261  0.138  0.016  0.166  0.350 -0.019 -0.178  0.451   \n",
       "4521 -0.029  0.019  0.145  0.182  0.354  0.118  0.318 -0.085 -0.148  0.399   \n",
       "\n",
       "      wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "6252  0.418  0.144  0.585 -0.305  0.183  0.317  0.170  0.150  0.025 -0.079   \n",
       "4684  0.248 -0.445  0.532 -0.240  0.144  0.197 -0.073  0.153  0.124 -0.052   \n",
       "1731  0.016 -0.458  0.327 -0.525  0.151  0.486 -0.050  0.161  0.209 -0.221   \n",
       "4742  0.119 -0.287  0.448 -0.517  0.001  0.302 -0.037  0.354  0.322 -0.449   \n",
       "4521  0.117 -0.266  0.298 -0.272  0.056  0.396 -0.040  0.148  0.262 -0.178   \n",
       "\n",
       "      wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "6252  0.208  0.459 -0.086  0.140  0.267 -0.173  0.006 -0.549  0.148 -0.292   \n",
       "4684  0.151  0.508 -0.034  0.043  0.060 -0.057  0.276  0.039  0.115 -0.188   \n",
       "1731  0.239  0.286  0.111  0.101 -0.015 -0.083  0.026 -0.026  0.050 -0.188   \n",
       "4742  0.445  0.345  0.033  0.093  0.029 -0.110  0.532  0.006  0.240 -0.399   \n",
       "4521  0.417  0.424 -0.037  0.151  0.312 -0.003  0.347 -0.106  0.101 -0.183   \n",
       "\n",
       "      wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "6252 -0.010  0.262  0.021  0.003 -0.177  0.000 -0.020 -0.006  0.004 -0.002   \n",
       "4684 -0.030  0.312  0.167 -0.096 -0.144 -0.040 -0.089 -0.340  0.184 -0.088   \n",
       "1731 -0.113  0.456  0.092 -0.023  0.016  0.274  0.007 -0.303 -0.040  0.047   \n",
       "4742 -0.133  0.377  0.286 -0.183 -0.067  0.198 -0.062 -0.191 -0.058 -0.717   \n",
       "4521  0.068  0.226  0.133 -0.443 -0.244 -0.195 -0.188 -0.560  0.272 -0.208   \n",
       "\n",
       "      wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "6252  0.319 -0.111  -0.373  -0.016   0.011   0.036   0.193  -0.099  -0.176   \n",
       "4684  0.115 -0.329  -0.375   0.066   0.128  -0.034   0.137  -0.088  -0.227   \n",
       "1731  0.232 -0.365  -0.378  -0.048   0.048  -0.517   0.340   0.092  -0.200   \n",
       "4742  0.334 -0.370  -0.250  -0.201   0.218  -0.274   0.524   0.033  -0.268   \n",
       "4521  0.387 -0.305  -0.354   0.287  -0.058   0.197  -0.183  -0.190  -0.150   \n",
       "\n",
       "      wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "6252   0.357  -0.121  -0.037  -0.441  -0.507  -0.071   0.012   0.296  -0.155   \n",
       "4684   0.176   0.056  -0.162  -0.176  -0.523   0.167   0.001   0.286  -0.122   \n",
       "1731   0.061  -0.050  -0.949  -0.213  -0.140   0.029  -0.061   0.462  -0.199   \n",
       "4742   0.147   0.037  -0.389  -0.231  -0.449   0.218  -0.035   0.401  -0.198   \n",
       "4521   0.416   0.068   0.198  -0.183  -0.420   0.296   0.098   0.129  -0.020   \n",
       "\n",
       "      wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "6252  -0.020   0.452   0.084  -0.026   0.123   0.254   0.001  -0.098   0.414   \n",
       "4684   0.087   0.431  -0.000  -0.164   0.045   0.435   0.030  -0.174   0.277   \n",
       "1731   0.077   0.615   0.232   0.117   0.370   0.234  -0.060  -0.190   0.287   \n",
       "4742  -0.307   0.549   0.179   0.126   0.324   0.229   0.262  -0.285   0.286   \n",
       "4521   0.432   0.267  -0.079   0.035   0.437   0.545   0.217  -0.056   0.298   \n",
       "\n",
       "      wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "6252  -0.087   0.120   0.068  -0.095   0.037  -0.023   0.009   0.116  -0.112   \n",
       "4684  -0.095  -0.212   0.179   0.118   0.160   0.127  -0.153  -0.189  -0.088   \n",
       "1731  -0.086  -0.163   0.228  -0.157   0.064   0.114  -0.241   0.441   0.015   \n",
       "4742  -0.052   0.121   0.133  -0.224  -0.289   0.024   0.063  -0.181  -0.017   \n",
       "4521  -0.098  -0.302  -0.095  -0.377   0.013   0.080  -0.048  -0.237  -0.085   \n",
       "\n",
       "      wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "6252  -0.455  -0.116  -0.106  -0.120   0.034   0.007  -0.089  -0.398   0.009   \n",
       "4684   0.076   0.134  -0.177   0.038   0.012   0.138   0.012   0.162   0.124   \n",
       "1731   0.371   0.151  -0.109  -0.048   0.121   0.118   0.046  -0.319   0.119   \n",
       "4742   0.047   0.222  -0.192   0.286   0.131   0.007  -0.054   0.330   0.023   \n",
       "4521  -0.576  -0.131  -0.189   0.083  -0.099   0.065  -0.283   0.012   0.022   \n",
       "\n",
       "      wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "6252  -0.262  -0.279  -0.575  -0.056  -0.251   0.032   0.223  -0.198  -0.381   \n",
       "4684  -0.122   0.075   0.003   0.013  -0.421  -0.268  -0.142  -0.212  -0.289   \n",
       "1731   0.114  -0.154   0.018  -0.158  -0.318  -0.011  -0.131  -0.222  -0.268   \n",
       "4742   0.030  -0.004   0.009  -0.157  -0.143   0.040  -0.124  -0.363  -0.406   \n",
       "4521   0.009  -0.001  -0.109  -0.024   0.078  -0.041  -0.118  -0.206  -0.466   \n",
       "\n",
       "      wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "6252  -0.300  -0.151   0.249   0.383  -0.011  -0.248  -0.259   0.378   0.213   \n",
       "4684  -0.297  -0.201   0.202   0.275  -0.165  -0.309  -0.434  -0.139  -0.238   \n",
       "1731  -0.396  -0.182   0.262   0.307  -0.221  -0.541  -0.247   0.500  -0.375   \n",
       "4742  -0.327  -0.345   0.429   0.290  -0.122  -0.302  -0.284  -0.158  -0.245   \n",
       "4521  -0.177  -0.460   0.036   0.275  -0.150  -0.323  -0.392  -0.584  -0.147   \n",
       "\n",
       "      wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "6252  -0.066  -0.235  -0.226   0.260  -0.092   0.777   0.220   0.507   0.338   \n",
       "4684  -0.273  -0.320  -0.279   0.291  -0.393   0.426   0.103   0.204   0.159   \n",
       "1731   0.409  -0.394  -0.337   0.333  -0.206   0.766   0.209   0.340   0.519   \n",
       "4742  -0.139  -0.472  -0.237   0.316  -0.243   0.643   0.179   0.312   0.151   \n",
       "4521  -0.237  -0.410  -0.293   0.132  -0.369   0.644   0.051   0.248   0.074   \n",
       "\n",
       "      wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "6252   0.427  -0.090  -0.408  -0.149  -0.186   0.033  \n",
       "4684   0.152  -0.411  -0.393  -0.184  -0.320   0.136  \n",
       "1731   0.105  -0.259   0.597  -0.212  -0.331   0.115  \n",
       "4742   0.115  -0.232  -0.240  -0.187  -0.318   0.028  \n",
       "4521   0.292  -0.506  -0.104  -0.259  -0.305   0.038  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:31:56.898548Z",
     "start_time": "2021-01-05T09:30:49.715497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.251</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.251</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.052</td>\n",
       "      <td>-1.105</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-1.057</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-1.003</td>\n",
       "      <td>-1.025</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-1.005</td>\n",
       "      <td>-1.026</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-1.059</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.633</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.638</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.769</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-1.113</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-1.081</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-1.011</td>\n",
       "      <td>-1.261</td>\n",
       "      <td>-1.147</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>-0.718</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-1.157</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>-1.023</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>-1.141</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.819</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>-1.042</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-1.067</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-1.151</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.813</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.797</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-1.137</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>-1.160</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-1.255</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.713</td>\n",
       "      <td>-1.266</td>\n",
       "      <td>-1.282</td>\n",
       "      <td>-1.098</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.672</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.674</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>-0.806</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>-0.585</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>-0.778</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>-1.418</td>\n",
       "      <td>-1.144</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-1.041</td>\n",
       "      <td>-1.024</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-1.007</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>0.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.498</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.447</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.335</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.411</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1.935</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.806</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1.148</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.084</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.527</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.437</td>\n",
       "      <td>1.053</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.818</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.249</td>\n",
       "      <td>1.115</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.284</td>\n",
       "      <td>1.058</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.708</td>\n",
       "      <td>1.086</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.454</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>1.156</td>\n",
       "      <td>1.045</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.410</td>\n",
       "      <td>1.068</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.905</td>\n",
       "      <td>1.566</td>\n",
       "      <td>0.354</td>\n",
       "      <td>1.089</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.872</td>\n",
       "      <td>1.066</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "count      10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean  1373158606.000         0.004         0.001        -0.003         0.001   \n",
       "std            0.000         0.276         0.283         0.284         0.282   \n",
       "min   1373158606.000        -1.000        -0.999        -1.000        -0.999   \n",
       "25%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "50%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "75%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "max   1373158606.000         0.999         0.998         0.999         1.000   \n",
       "\n",
       "       00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean         -0.004         0.003         0.001        -0.001         0.005   \n",
       "std           0.281         0.274         0.279         0.281         0.277   \n",
       "min          -0.999        -0.999        -0.999        -0.998        -0.999   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max           1.000         1.000         0.999         1.000         1.000   \n",
       "\n",
       "       00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean          0.003         0.001        -0.004         0.001        -0.002   \n",
       "std           0.291         0.276         0.286         0.279         0.280   \n",
       "min          -0.999        -1.000        -0.998        -1.000        -0.997   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max           0.999         1.000         0.998         1.000         0.999   \n",
       "\n",
       "       02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean          0.001         0.002         0.004        -0.002         0.002   \n",
       "std           0.284         0.285         0.278         0.290         0.283   \n",
       "min          -1.000        -0.999        -0.998        -1.000        -1.000   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max           1.000         1.000         1.000         1.000         1.000   \n",
       "\n",
       "       11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "count     10000.000     10000.000           10000.000           10000.000   \n",
       "mean          0.001         0.001              -0.006               0.011   \n",
       "std           0.283         0.279               0.273               0.288   \n",
       "min          -1.000        -1.000              -1.052              -1.105   \n",
       "25%           0.000         0.000              -0.031              -0.018   \n",
       "50%           0.000         0.000              -0.004               0.003   \n",
       "75%           0.000         0.000               0.014               0.037   \n",
       "max           1.000         1.000               1.006               1.411   \n",
       "\n",
       "       00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.012               0.014              -0.004   \n",
       "std                 0.271               0.288               0.266   \n",
       "min                -0.995              -1.057              -0.985   \n",
       "25%                -0.020              -0.018              -0.009   \n",
       "50%                -0.004               0.006               0.001   \n",
       "75%                 0.007               0.042               0.010   \n",
       "max                 0.988               1.935               0.977   \n",
       "\n",
       "       00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.008               0.009              -0.002   \n",
       "std                 0.263               0.284               0.266   \n",
       "min                -1.003              -1.025              -0.987   \n",
       "25%                -0.023              -0.021              -0.009   \n",
       "50%                -0.006               0.002               0.000   \n",
       "75%                 0.006               0.033               0.008   \n",
       "max                 0.996               1.806               0.971   \n",
       "\n",
       "       00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                0.003              -0.005               0.014   \n",
       "std                 0.263               0.282               0.280   \n",
       "min                -0.984              -1.005              -1.026   \n",
       "25%                -0.009              -0.019              -0.014   \n",
       "50%                -0.001              -0.003               0.007   \n",
       "75%                 0.009               0.008               0.037   \n",
       "max                 0.990               0.988               1.148   \n",
       "\n",
       "       01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.004              -0.003              -0.002   \n",
       "std                 0.271               0.262               0.267   \n",
       "min                -0.990              -0.981              -0.969   \n",
       "25%                -0.009              -0.012              -0.008   \n",
       "50%                -0.000              -0.002               0.000   \n",
       "75%                 0.008               0.006               0.009   \n",
       "max                 0.976               0.967               0.985   \n",
       "\n",
       "       02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.010               0.010               0.003   \n",
       "std                 0.274               0.286               0.264   \n",
       "min                -0.991              -1.059              -0.978   \n",
       "25%                -0.023              -0.018              -0.008   \n",
       "50%                -0.007               0.004               0.000   \n",
       "75%                 0.003               0.034               0.009   \n",
       "max                 0.992               1.084               0.984   \n",
       "\n",
       "       10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.001               0.004               0.002   \n",
       "std                 0.274               0.270               0.269   \n",
       "min                -0.983              -0.988              -0.976   \n",
       "25%                -0.007              -0.006              -0.007   \n",
       "50%                 0.001               0.002               0.002   \n",
       "75%                 0.010               0.011               0.010   \n",
       "max                 0.975               0.989               0.991   \n",
       "\n",
       "       20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.010               0.004               0.001   \n",
       "std                 0.271               0.276               0.283   \n",
       "min                -0.990              -1.000              -0.999   \n",
       "25%                -0.023              -0.000              -0.000   \n",
       "50%                -0.007               0.000               0.000   \n",
       "75%                 0.003               0.000               0.000   \n",
       "max                 0.997               0.999               0.998   \n",
       "\n",
       "       00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.003               0.001              -0.004   \n",
       "std                 0.284               0.282               0.281   \n",
       "min                -1.000              -0.999              -0.999   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                -0.000               0.000              -0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                 0.999               1.000               1.000   \n",
       "\n",
       "       00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                0.003               0.001              -0.001   \n",
       "std                 0.274               0.279               0.281   \n",
       "min                -0.999              -0.999              -0.998   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                -0.000               0.000              -0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                 1.000               0.999               1.000   \n",
       "\n",
       "       00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                0.005               0.003               0.001   \n",
       "std                 0.277               0.291               0.276   \n",
       "min                -0.999              -0.999              -1.000   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                 0.000              -0.000              -0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                 1.000               0.999               1.000   \n",
       "\n",
       "       01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.004               0.001              -0.002   \n",
       "std                 0.286               0.279               0.280   \n",
       "min                -0.998              -1.000              -0.997   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                -0.000              -0.000               0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                 0.998               1.000               0.999   \n",
       "\n",
       "       02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                0.001               0.002               0.004   \n",
       "std                 0.284               0.285               0.278   \n",
       "min                -1.000              -0.999              -0.998   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                 0.000               0.000               0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                 1.000               1.000               1.000   \n",
       "\n",
       "       10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.002               0.002               0.001   \n",
       "std                 0.290               0.283               0.283   \n",
       "min                -1.000              -1.000              -1.000   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                -0.000              -0.000               0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                 1.000               1.000               1.000   \n",
       "\n",
       "       20000-lstsq_target      wb_0      wb_1      wb_2      wb_3      wb_4  \\\n",
       "count           10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean                0.001    -0.051    -0.150     0.295     0.119     0.192   \n",
       "std                 0.279     0.099     0.124     0.166     0.138     0.159   \n",
       "min                -1.000    -0.633    -0.842    -0.259    -0.525    -0.639   \n",
       "25%                -0.000    -0.081    -0.241     0.183     0.019     0.081   \n",
       "50%                 0.000    -0.070    -0.132     0.317     0.072     0.215   \n",
       "75%                 0.000    -0.016    -0.059     0.435     0.225     0.298   \n",
       "max                 1.000     0.490     0.324     0.707     0.581     0.915   \n",
       "\n",
       "           wb_5      wb_6      wb_7      wb_8      wb_9     wb_10     wb_11  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.157     0.248    -0.248     0.129     0.335     0.396    -0.111   \n",
       "std       0.098     0.090     0.170     0.117     0.227     0.169     0.142   \n",
       "min      -0.422    -0.659    -0.868    -0.411    -0.608    -0.286    -0.853   \n",
       "25%       0.095     0.199    -0.401     0.042     0.176     0.271    -0.213   \n",
       "50%       0.165     0.248    -0.284     0.138     0.365     0.421    -0.135   \n",
       "75%       0.218     0.303    -0.098     0.209     0.480     0.529    -0.011   \n",
       "max       0.685     0.944     0.327     0.527     1.092     0.862     0.459   \n",
       "\n",
       "          wb_12     wb_13     wb_14     wb_15     wb_16     wb_17     wb_18  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.021     0.320     0.159     0.377     0.040     0.276     0.318   \n",
       "std       0.127     0.112     0.093     0.125     0.149     0.084     0.110   \n",
       "min      -0.638    -0.121    -0.346    -0.160    -0.678    -0.026    -0.137   \n",
       "25%      -0.057     0.262     0.098     0.306    -0.034     0.215     0.258   \n",
       "50%       0.030     0.336     0.163     0.392     0.024     0.276     0.321   \n",
       "75%       0.114     0.394     0.215     0.462     0.114     0.328     0.375   \n",
       "max       0.463     0.759     0.555     0.759     0.761     0.622     0.771   \n",
       "\n",
       "          wb_19     wb_20     wb_21     wb_22     wb_23     wb_24     wb_25  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.286     0.066     0.130     0.034    -0.015    -0.339    -0.117   \n",
       "std       0.135     0.150     0.131     0.173     0.085     0.200     0.103   \n",
       "min      -0.313    -0.769    -0.526    -0.941    -0.502    -1.113    -0.783   \n",
       "25%       0.208    -0.013     0.028    -0.051    -0.060    -0.489    -0.153   \n",
       "50%       0.296     0.058     0.149     0.029    -0.009    -0.429    -0.145   \n",
       "75%       0.364     0.140     0.225     0.141     0.040    -0.147    -0.071   \n",
       "max       0.931     0.809     0.622     0.795     0.294     0.281     0.437   \n",
       "\n",
       "          wb_26     wb_27     wb_28     wb_29     wb_30     wb_31     wb_32  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.049    -0.177    -0.214    -0.124     0.279     0.341     0.152   \n",
       "std       0.111     0.160     0.158     0.144     0.100     0.095     0.140   \n",
       "min      -0.514    -1.081    -1.006    -0.943    -0.214    -0.073    -0.395   \n",
       "25%      -0.026    -0.333    -0.316    -0.185     0.225     0.288     0.054   \n",
       "50%       0.038    -0.134    -0.256    -0.091     0.297     0.330     0.091   \n",
       "75%       0.115    -0.046    -0.089    -0.031     0.346     0.389     0.259   \n",
       "max       0.558     0.327     0.414     0.491     0.690     0.834     0.709   \n",
       "\n",
       "          wb_33     wb_34     wb_35     wb_36     wb_37     wb_38     wb_39  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.277     0.038    -0.255    -0.321     0.028    -0.153    -0.211   \n",
       "std       0.106     0.202     0.199     0.164     0.117     0.128     0.093   \n",
       "min      -0.106    -1.011    -1.261    -1.147    -0.526    -0.893    -0.730   \n",
       "25%       0.214    -0.051    -0.423    -0.422    -0.039    -0.239    -0.258   \n",
       "50%       0.272     0.028    -0.213    -0.384     0.035    -0.130    -0.193   \n",
       "75%       0.333     0.136    -0.099    -0.205     0.102    -0.060    -0.148   \n",
       "max       0.725     0.893     0.461     0.268     0.483     0.259     0.048   \n",
       "\n",
       "          wb_40     wb_41     wb_42     wb_43     wb_44     wb_45     wb_46  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.134     0.191     0.436    -0.143     0.383     0.133     0.018   \n",
       "std       0.120     0.175     0.089     0.123     0.139     0.154     0.124   \n",
       "min      -0.453    -0.426     0.005    -0.741    -0.298    -0.556    -0.718   \n",
       "25%       0.044     0.035     0.380    -0.204     0.321     0.027    -0.061   \n",
       "50%       0.132     0.173     0.448    -0.119     0.408     0.138     0.021   \n",
       "75%       0.215     0.325     0.498    -0.060     0.468     0.237     0.097   \n",
       "max       0.592     0.757     0.719     0.347     0.879     0.879     0.437   \n",
       "\n",
       "          wb_47     wb_48     wb_49     wb_50     wb_51     wb_52     wb_53  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.409     0.311    -0.362    -0.082     0.124    -0.250    -0.127   \n",
       "std       0.171     0.087     0.188     0.104     0.116     0.169     0.154   \n",
       "min      -0.229    -0.100    -1.157    -0.793    -0.443    -0.994    -1.006   \n",
       "25%       0.320     0.257    -0.499    -0.109     0.033    -0.421    -0.209   \n",
       "50%       0.408     0.300    -0.471    -0.099     0.125    -0.207    -0.149   \n",
       "75%       0.505     0.357    -0.187    -0.052     0.198    -0.109    -0.023   \n",
       "max       1.053     0.740     0.294     0.516     0.643     0.273     0.501   \n",
       "\n",
       "          wb_54     wb_55     wb_56     wb_57     wb_58     wb_59     wb_60  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.266     0.456     0.395    -0.370    -0.024    -0.107     0.152   \n",
       "std       0.150     0.108     0.094     0.180     0.105     0.205     0.171   \n",
       "min      -1.023    -0.042    -0.263    -0.993    -0.677    -1.141    -0.908   \n",
       "25%      -0.346     0.396     0.340    -0.525    -0.075    -0.202     0.016   \n",
       "50%      -0.245     0.461     0.391    -0.422    -0.013    -0.072     0.160   \n",
       "75%      -0.167     0.521     0.447    -0.209     0.035     0.008     0.282   \n",
       "max       0.246     0.818     1.075     0.207     0.354     0.714     0.643   \n",
       "\n",
       "          wb_61     wb_62     wb_63     wb_64     wb_65     wb_66     wb_67  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.298     0.297     0.152     0.338    -0.101    -0.226     0.435   \n",
       "std       0.137     0.117     0.113     0.100     0.128     0.171     0.096   \n",
       "min      -0.137    -0.125    -0.512    -0.107    -0.819    -0.933     0.146   \n",
       "25%       0.190     0.202     0.065     0.278    -0.173    -0.352     0.372   \n",
       "50%       0.257     0.313     0.163     0.338    -0.076    -0.220     0.432   \n",
       "75%       0.401     0.378     0.228     0.399    -0.011    -0.097     0.495   \n",
       "max       0.748     0.651     0.598     0.764     0.315     0.530     0.759   \n",
       "\n",
       "          wb_68     wb_69     wb_70     wb_71     wb_72     wb_73     wb_74  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.093    -0.196     0.419    -0.310     0.020     0.377     0.051   \n",
       "std       0.123     0.132     0.147     0.147     0.162     0.093     0.153   \n",
       "min      -0.493    -1.042    -0.246    -0.938    -1.067    -0.089    -0.732   \n",
       "25%       0.014    -0.262     0.338    -0.412    -0.050     0.324    -0.065   \n",
       "50%       0.098    -0.181     0.422    -0.285     0.023     0.376    -0.012   \n",
       "75%       0.172    -0.107     0.500    -0.209     0.111     0.430     0.160   \n",
       "max       0.732     0.249     1.115     0.121     0.869     0.765     0.681   \n",
       "\n",
       "          wb_75     wb_76     wb_77     wb_78     wb_79     wb_80     wb_81  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.180     0.279    -0.235     0.303     0.448    -0.017     0.090   \n",
       "std       0.088     0.099     0.170     0.128     0.159     0.096     0.087   \n",
       "min      -0.172    -0.094    -0.973    -0.155    -0.166    -0.550    -0.821   \n",
       "25%       0.150     0.210    -0.403     0.222     0.370    -0.067     0.038   \n",
       "50%       0.161     0.282    -0.200     0.265     0.481    -0.010     0.087   \n",
       "75%       0.192     0.335    -0.104     0.393     0.550     0.043     0.139   \n",
       "max       0.562     0.653     0.281     0.736     0.972     0.400     0.683   \n",
       "\n",
       "          wb_82     wb_83     wb_84     wb_85     wb_86     wb_87     wb_88  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.130    -0.063     0.252    -0.151     0.188    -0.293    -0.091   \n",
       "std       0.129     0.096     0.213     0.178     0.127     0.133     0.116   \n",
       "min      -0.438    -0.593    -0.530    -1.151    -0.309    -0.813    -0.860   \n",
       "25%       0.035    -0.114     0.096    -0.272     0.104    -0.384    -0.177   \n",
       "50%       0.075    -0.052     0.250    -0.116     0.142    -0.284    -0.072   \n",
       "75%       0.229    -0.004     0.393    -0.012     0.281    -0.203    -0.011   \n",
       "max       0.586     0.284     1.058     0.478     0.608     0.106     0.404   \n",
       "\n",
       "          wb_89     wb_90     wb_91     wb_92     wb_93     wb_94     wb_95  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.312     0.126    -0.208    -0.135     0.079    -0.041    -0.329   \n",
       "std       0.104     0.110     0.178     0.096     0.150     0.135     0.168   \n",
       "min      -0.122    -0.481    -0.880    -0.584    -0.797    -0.955    -1.137   \n",
       "25%       0.255     0.047    -0.330    -0.196    -0.012    -0.098    -0.438   \n",
       "50%       0.321     0.129    -0.195    -0.126     0.105    -0.024    -0.323   \n",
       "75%       0.377     0.201    -0.069    -0.071     0.185     0.035    -0.225   \n",
       "max       0.661     0.517     0.488     0.191     0.586     0.546     0.412   \n",
       "\n",
       "          wb_96     wb_97     wb_98     wb_99    wb_100    wb_101    wb_102  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.107    -0.334     0.305    -0.225    -0.296     0.079     0.090   \n",
       "std       0.118     0.204     0.090     0.169     0.139     0.116     0.151   \n",
       "min      -0.434    -1.160    -0.208    -0.921    -0.892    -0.495    -0.610   \n",
       "25%       0.017    -0.462     0.255    -0.348    -0.377    -0.006    -0.019   \n",
       "50%       0.116    -0.342     0.303    -0.314    -0.369     0.072     0.067   \n",
       "75%       0.189    -0.199     0.354    -0.069    -0.221     0.153     0.210   \n",
       "max       0.648     0.421     0.619     0.478     0.330     0.607     0.596   \n",
       "\n",
       "         wb_103    wb_104    wb_105    wb_106    wb_107    wb_108    wb_109  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.325     0.154    -0.121    -0.227     0.251    -0.004    -0.342   \n",
       "std       0.210     0.161     0.115     0.092     0.145     0.108     0.253   \n",
       "min      -1.110    -0.927    -0.902    -1.255    -0.291    -0.713    -1.266   \n",
       "25%      -0.515     0.051    -0.181    -0.273     0.153    -0.048    -0.520   \n",
       "50%      -0.363     0.165    -0.110    -0.221     0.188     0.003    -0.326   \n",
       "75%      -0.144     0.248    -0.045    -0.172     0.366     0.055    -0.149   \n",
       "max       0.298     0.940     0.227     0.285     0.743     0.389     0.397   \n",
       "\n",
       "         wb_110    wb_111    wb_112    wb_113    wb_114    wb_115    wb_116  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.293    -0.321     0.148     0.002     0.251    -0.198     0.121   \n",
       "std       0.217     0.195     0.135     0.120     0.108     0.139     0.183   \n",
       "min      -1.282    -1.098    -0.485    -0.917    -0.329    -0.974    -0.648   \n",
       "25%      -0.478    -0.507     0.036    -0.059     0.185    -0.279     0.003   \n",
       "50%      -0.246    -0.351     0.160     0.014     0.258    -0.176     0.092   \n",
       "75%      -0.132    -0.157     0.247     0.074     0.321    -0.102     0.235   \n",
       "max       0.402     0.373     0.601     0.472     0.755     0.265     0.780   \n",
       "\n",
       "         wb_117    wb_118    wb_119    wb_120    wb_121    wb_122    wb_123  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.400     0.024     0.035     0.174     0.373     0.070    -0.135   \n",
       "std       0.101     0.140     0.159     0.169     0.138     0.176     0.085   \n",
       "min       0.075    -0.859    -0.949    -0.520    -0.120    -0.937    -0.672   \n",
       "25%       0.331    -0.040    -0.032     0.062     0.259    -0.023    -0.180   \n",
       "50%       0.402     0.031     0.044     0.173     0.392     0.065    -0.131   \n",
       "75%       0.466     0.108     0.126     0.270     0.469     0.186    -0.082   \n",
       "max       0.761     0.680     0.695     0.999     0.748     0.688     0.220   \n",
       "\n",
       "         wb_124    wb_125    wb_126    wb_127    wb_128    wb_129    wb_130  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.337    -0.048    -0.091     0.139    -0.064     0.038     0.029   \n",
       "std       0.136     0.134     0.134     0.192     0.166     0.167     0.100   \n",
       "min      -0.197    -0.513    -0.625    -0.500    -0.526    -0.674    -0.538   \n",
       "25%       0.263    -0.095    -0.169    -0.073    -0.163    -0.031    -0.006   \n",
       "50%       0.287    -0.084    -0.068     0.185    -0.121     0.068     0.043   \n",
       "75%       0.439    -0.044     0.012     0.290     0.041     0.141     0.085   \n",
       "max       0.791     0.475     0.377     0.580     0.554     0.628     0.425   \n",
       "\n",
       "         wb_131    wb_132    wb_133    wb_134    wb_135    wb_136    wb_137  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.046    -0.088    -0.073     0.016     0.051    -0.103     0.015   \n",
       "std       0.109     0.157     0.136     0.279     0.198     0.143     0.142   \n",
       "min      -0.701    -0.525    -0.574    -0.806    -0.535    -0.604    -0.523   \n",
       "25%      -0.078    -0.187    -0.138    -0.167    -0.122    -0.181    -0.098   \n",
       "50%      -0.025    -0.160    -0.045     0.060     0.053    -0.152     0.037   \n",
       "75%       0.017     0.038     0.022     0.209     0.209    -0.025     0.119   \n",
       "max       0.432     0.479     0.367     0.794     0.613     0.499     0.439   \n",
       "\n",
       "         wb_138    wb_139    wb_140    wb_141    wb_142    wb_143    wb_144  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.021     0.047    -0.023     0.039     0.040    -0.003    -0.024   \n",
       "std       0.154     0.078     0.141     0.215     0.066     0.145     0.155   \n",
       "min      -0.532    -0.572    -0.585    -0.569    -0.267    -0.578    -0.591   \n",
       "25%      -0.095     0.011    -0.112    -0.094     0.001    -0.069    -0.100   \n",
       "50%       0.054     0.053     0.003     0.041     0.040     0.027     0.009   \n",
       "75%       0.132     0.090     0.075     0.202     0.077     0.088     0.077   \n",
       "max       0.462     0.318     0.481     0.614     0.286     0.459     0.463   \n",
       "\n",
       "         wb_145    wb_146    wb_147    wb_148    wb_149    wb_150    wb_151  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.049    -0.013    -0.050    -0.024    -0.048    -0.316    -0.343   \n",
       "std       0.197     0.145     0.224     0.079     0.170     0.185     0.127   \n",
       "min      -0.743    -0.636    -0.778    -0.521    -0.549    -1.418    -1.144   \n",
       "25%      -0.150    -0.139    -0.203    -0.061    -0.149    -0.368    -0.404   \n",
       "50%      -0.001     0.013    -0.026    -0.014    -0.127    -0.225    -0.307   \n",
       "75%       0.078     0.091     0.106     0.025     0.060    -0.214    -0.262   \n",
       "max       0.547     0.468     0.742     0.350     0.553     0.827     0.194   \n",
       "\n",
       "         wb_152    wb_153    wb_154    wb_155    wb_156    wb_157    wb_158  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.379    -0.227     0.316     0.322    -0.101    -0.311    -0.395   \n",
       "std       0.160     0.132     0.134     0.083     0.109     0.114     0.161   \n",
       "min      -1.012    -0.862    -0.621     0.039    -0.635    -0.988    -1.192   \n",
       "25%      -0.497    -0.295     0.232     0.267    -0.160    -0.352    -0.458   \n",
       "50%      -0.371    -0.200     0.315     0.326    -0.103    -0.305    -0.359   \n",
       "75%      -0.253    -0.157     0.394     0.376    -0.053    -0.254    -0.285   \n",
       "max       0.722     0.708     1.086     0.835     0.856     0.454    -0.008   \n",
       "\n",
       "         wb_159    wb_160    wb_161    wb_162    wb_163    wb_164    wb_165  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.151    -0.123    -0.272    -0.399    -0.255     0.259    -0.243   \n",
       "std       0.295     0.232     0.124     0.123     0.166     0.087     0.142   \n",
       "min      -0.889    -0.848    -0.989    -1.041    -1.024    -0.076    -0.957   \n",
       "25%      -0.093    -0.286    -0.330    -0.485    -0.327     0.198    -0.323   \n",
       "50%       0.182    -0.174    -0.255    -0.406    -0.268     0.263    -0.240   \n",
       "75%       0.348    -0.025    -0.209    -0.309    -0.187     0.321    -0.150   \n",
       "max       1.156     1.045     0.512     0.410     1.068     0.794     0.905   \n",
       "\n",
       "         wb_166    wb_167    wb_168    wb_169    wb_170    wb_171    wb_172  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.569     0.145     0.388     0.252     0.185    -0.318    -0.113   \n",
       "std       0.224     0.065     0.148     0.134     0.181     0.130     0.255   \n",
       "min      -0.124    -0.081     0.005    -0.579    -0.946    -1.007    -0.934   \n",
       "25%       0.401     0.097     0.292     0.169     0.103    -0.406    -0.247   \n",
       "50%       0.542     0.150     0.370     0.237     0.169    -0.312    -0.148   \n",
       "75%       0.691     0.194     0.456     0.315     0.264    -0.236    -0.049   \n",
       "max       1.566     0.354     1.089     1.049     1.006     0.872     1.066   \n",
       "\n",
       "         wb_173    wb_174    wb_175  \n",
       "count 10000.000 10000.000 10000.000  \n",
       "mean     -0.218    -0.325     0.052  \n",
       "std       0.077     0.122     0.059  \n",
       "min      -0.594    -0.949    -0.171  \n",
       "25%      -0.267    -0.357     0.019  \n",
       "50%      -0.208    -0.312     0.053  \n",
       "75%      -0.162    -0.270     0.087  \n",
       "max       0.434     0.474     0.273  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61953536, 0.9741457 , 0.46680659, 0.49776036, 0.86019679],\n",
       "       [0.72910667, 0.61094799, 0.92371269, 0.08555449, 0.07980001],\n",
       "       [0.81699541, 0.46397194, 0.19362828, 0.24013508, 0.33092769],\n",
       "       [0.86053752, 0.33458561, 0.41499257, 0.28032716, 0.6508531 ],\n",
       "       [0.3949162 , 0.74394349, 0.86208017, 0.9772995 , 0.57861012],\n",
       "       [0.43701616, 0.5057832 , 0.07975839, 0.30121402, 0.94739198],\n",
       "       [0.26152861, 0.51987584, 0.75460088, 0.61431661, 0.19476404],\n",
       "       [0.35163938, 0.20872544, 0.14158339, 0.43655386, 0.40572667],\n",
       "       [0.65534944, 0.36340559, 0.8659874 , 0.14811638, 0.87008502],\n",
       "       [0.3979848 , 0.75216565, 0.32811658, 0.64674227, 0.5343556 ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.X_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81114727],\n",
       "       [1.09404528],\n",
       "       [0.44582668],\n",
       "       [0.57382601],\n",
       "       [0.7530337 ],\n",
       "       [0.21425484],\n",
       "       [0.42936283],\n",
       "       [0.10858684],\n",
       "       [0.90446246],\n",
       "       [0.39015335]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.y_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:32:09.782470Z",
     "start_time": "2021-01-05T09:31:56.901018Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "\n",
    "if inet_training_without_noise:\n",
    "   \n",
    "    for lambda_net_dataset, lambda_net_dataset_without_noise in zip(lambda_net_dataset_list, lambda_net_dataset_list_without_noise):\n",
    "        if inet_holdout_seed_evaluation:\n",
    "            raise SystemExit('Holdout Evaluation not implemented with inet training without noise')\n",
    "            \n",
    "        else:\n",
    "            lambda_net_train_dataset = lambda_net_dataset_without_noise\n",
    "\n",
    "            lambda_net_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_dataset_list_without_noise\n",
    "        \n",
    "else:\n",
    "\n",
    "    for lambda_net_dataset in lambda_net_dataset_list:\n",
    "\n",
    "        if inet_holdout_seed_evaluation:\n",
    "\n",
    "            complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "\n",
    "            if isinstance(test_size, float):\n",
    "                test_size = int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-test_size)))\n",
    "\n",
    "            test_seeds = random.sample(complete_seed_list, test_size)\n",
    "            lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "            valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-0.1))))\n",
    "            lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "            train_seeds = complete_seed_list\n",
    "            lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset\n",
    "        else:\n",
    "\n",
    "            lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "            lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.1)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "\n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:06.495716Z",
     "start_time": "2021-01-05T09:32:09.784760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8910, 240)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:08.945802Z",
     "start_time": "2021-01-05T09:33:06.499150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 240)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:11.543306Z",
     "start_time": "2021-01-05T09:33:08.947468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 240)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.389</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>0.247</td>\n",
       "      <td>-0.631</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.687</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.552</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.382</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.979</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.814</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.871</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.607</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.979</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.814</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.523</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.502</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.426</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.383</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.751</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>1.358</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.697</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.383</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.387</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4649</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.610</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.941</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.695</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.388</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.677</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.555</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.251</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "5548  1373158606         0.000         0.000         0.255         0.000   \n",
       "1854  1373158606         0.000         0.000        -0.016         0.000   \n",
       "739   1373158606         0.218         0.000         0.979        -0.272   \n",
       "3588  1373158606         0.000         0.000        -0.493         0.000   \n",
       "4649  1373158606         0.388         0.000         0.573         0.000   \n",
       "\n",
       "      00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "5548         0.000        -0.493         0.000         0.000         0.050   \n",
       "1854         0.000         0.472         0.561         0.000         0.000   \n",
       "739          0.696         0.000         0.000         0.000         0.000   \n",
       "3588         0.000         0.000         0.000         0.000         0.110   \n",
       "4649         0.677         0.000         0.895         0.000         0.000   \n",
       "\n",
       "      00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "5548         0.000         0.000         0.000         0.000         0.000   \n",
       "1854         0.000         0.000         0.000         0.000         0.000   \n",
       "739          0.000         0.000         0.000         0.000         0.000   \n",
       "3588        -0.701         0.000         0.000         0.000         0.000   \n",
       "4649         0.000         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "5548         0.000         0.000        -0.102         0.000         0.000   \n",
       "1854         0.000         0.000        -0.826         0.907         0.000   \n",
       "739         -0.814         0.000         0.000         0.000         0.000   \n",
       "3588         0.000         0.000        -0.432         0.000         0.000   \n",
       "4649         0.000         0.000         0.000         0.000         0.714   \n",
       "\n",
       "      11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "5548         0.000        -0.779               0.007               0.005   \n",
       "1854         0.000         0.000              -0.010               0.008   \n",
       "739          0.000         0.000               0.148               0.162   \n",
       "3588         0.492         0.000               0.005              -0.026   \n",
       "4649         0.000         0.000               0.307               0.129   \n",
       "\n",
       "      00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "5548               0.244              -0.011               0.006   \n",
       "1854              -0.053               0.043               0.039   \n",
       "739                0.871              -0.178               0.607   \n",
       "3588              -0.472               0.011               0.005   \n",
       "4649               0.495               0.091               0.610   \n",
       "\n",
       "      00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "5548              -0.485               0.004               0.001   \n",
       "1854               0.433               0.552              -0.026   \n",
       "739               -0.023               0.033              -0.008   \n",
       "3588              -0.012               0.008              -0.009   \n",
       "4649              -0.039               0.941              -0.009   \n",
       "\n",
       "      00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "5548               0.048              -0.007              -0.007   \n",
       "1854               0.016               0.012               0.008   \n",
       "739               -0.030              -0.014               0.033   \n",
       "3588               0.103              -0.697              -0.001   \n",
       "4649              -0.020              -0.021               0.036   \n",
       "\n",
       "      01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "5548              -0.004               0.002               0.001   \n",
       "1854              -0.015               0.009              -0.007   \n",
       "739               -0.014              -0.021              -0.005   \n",
       "3588              -0.007               0.005               0.005   \n",
       "4649              -0.012              -0.005               0.001   \n",
       "\n",
       "      02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "5548               0.005              -0.018              -0.094   \n",
       "1854              -0.007               0.021              -0.774   \n",
       "739               -0.823               0.006              -0.012   \n",
       "3588               0.010              -0.020              -0.414   \n",
       "4649              -0.018               0.070              -0.013   \n",
       "\n",
       "      10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "5548               0.000               0.005               0.006   \n",
       "1854               0.839               0.020               0.013   \n",
       "739               -0.000               0.017              -0.006   \n",
       "3588               0.001              -0.006               0.478   \n",
       "4649              -0.011               0.695              -0.023   \n",
       "\n",
       "      20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "5548              -0.768               0.000              -0.000   \n",
       "1854              -0.035               0.000               0.000   \n",
       "739               -0.006               0.218               0.000   \n",
       "3588               0.016              -0.000               0.000   \n",
       "4649              -0.032               0.388              -0.000   \n",
       "\n",
       "      00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "5548               0.255              -0.000               0.000   \n",
       "1854              -0.016               0.000              -0.000   \n",
       "739                0.979              -0.272               0.696   \n",
       "3588              -0.493               0.000              -0.000   \n",
       "4649               0.573               0.000               0.677   \n",
       "\n",
       "      00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "5548              -0.493               0.000               0.000   \n",
       "1854               0.472               0.561               0.000   \n",
       "739                0.000              -0.000               0.000   \n",
       "3588               0.000              -0.000              -0.000   \n",
       "4649              -0.000               0.895               0.000   \n",
       "\n",
       "      00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "5548               0.050              -0.000              -0.000   \n",
       "1854               0.000               0.000              -0.000   \n",
       "739                0.000              -0.000               0.000   \n",
       "3588               0.110              -0.701               0.000   \n",
       "4649               0.000               0.000              -0.000   \n",
       "\n",
       "      01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "5548              -0.000              -0.000               0.000   \n",
       "1854              -0.000               0.000               0.000   \n",
       "739               -0.000              -0.000               0.000   \n",
       "3588              -0.000               0.000               0.000   \n",
       "4649               0.000               0.000              -0.000   \n",
       "\n",
       "      02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "5548               0.000               0.000              -0.102   \n",
       "1854               0.000              -0.000              -0.826   \n",
       "739               -0.814              -0.000               0.000   \n",
       "3588              -0.000               0.000              -0.432   \n",
       "4649              -0.000              -0.000              -0.000   \n",
       "\n",
       "      10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "5548               0.000              -0.000              -0.000   \n",
       "1854               0.907               0.000              -0.000   \n",
       "739               -0.000               0.000               0.000   \n",
       "3588              -0.000              -0.000               0.492   \n",
       "4649              -0.000               0.714               0.000   \n",
       "\n",
       "      20000-lstsq_target   wb_0   wb_1  wb_2   wb_3  wb_4  wb_5  wb_6   wb_7  \\\n",
       "5548              -0.779 -0.003 -0.041 0.562  0.231 0.224 0.040 0.379 -0.559   \n",
       "1854              -0.000 -0.105 -0.337 0.453  0.000 0.170 0.183 0.228 -0.373   \n",
       "739                0.000  0.100 -0.059 0.163  0.072 0.038 0.202 0.265 -0.415   \n",
       "3588               0.000 -0.087 -0.116 0.383 -0.006 0.261 0.215 0.284  0.183   \n",
       "4649               0.000 -0.055 -0.267 0.427  0.153 0.008 0.195 0.163 -0.382   \n",
       "\n",
       "      wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "5548 0.365 0.494  0.619  0.121  0.035  0.485  0.019  0.475 -0.096  0.158   \n",
       "1854 0.180 0.721  0.288 -0.224  0.244  0.451  0.180  0.180 -0.101  0.295   \n",
       "739  0.047 0.435  0.523 -0.122 -0.011 -0.010  0.194  0.431 -0.007  0.314   \n",
       "3588 0.207 0.600  0.387 -0.055  0.028  0.329  0.106  0.441  0.061  0.203   \n",
       "4649 0.208 0.437  0.481 -0.219  0.045  0.156  0.193  0.404  0.036  0.309   \n",
       "\n",
       "      wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "5548  0.115  0.123  0.005  0.279  0.190  0.052 -0.469 -0.007  0.050 -0.004   \n",
       "1854  0.335  0.307  0.062  0.222  0.238 -0.099 -0.405 -0.163 -0.052 -0.053   \n",
       "739   0.355  0.321  0.102 -0.046 -0.049 -0.051 -0.501  0.128  0.310 -0.483   \n",
       "3588  0.291  0.442  0.049  0.161  0.013  0.021 -0.001  0.081  0.057 -0.275   \n",
       "4649  0.348  0.324  0.101  0.402  0.177 -0.230 -0.483 -0.143  0.075 -0.040   \n",
       "\n",
       "      wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "5548 -0.075 -0.056  0.327  0.309 -0.009  0.012  0.000 -0.025 -0.134 -0.088   \n",
       "1854 -0.317 -0.023  0.306  0.333  0.070  0.102  0.004 -0.433 -0.410  0.029   \n",
       "739  -0.100 -0.289  0.179  0.500  0.112  0.522  0.195 -0.035 -0.458  0.029   \n",
       "3588 -0.047 -0.016  0.296  0.293 -0.106  0.240  0.302 -0.444 -0.002 -0.052   \n",
       "4649  0.001 -0.008  0.268  0.166  0.074  0.351  0.006 -0.024 -0.420  0.026   \n",
       "\n",
       "      wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "5548 -0.013 -0.224  0.029  0.065  0.475 -0.036  0.295  0.241  0.039  0.284   \n",
       "1854 -0.009 -0.192  0.008 -0.002  0.458 -0.154  0.422  0.208  0.090  0.435   \n",
       "739  -0.779 -0.339  0.324 -0.020  0.315 -0.260  0.290  0.032 -0.114  0.737   \n",
       "3588 -0.189 -0.143  0.097  0.278  0.445  0.215  0.359 -0.070 -0.041  0.471   \n",
       "4649 -0.290 -0.239  0.039  0.035  0.413 -0.167  0.389  0.167  0.146  0.201   \n",
       "\n",
       "      wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "5548  0.268 -0.491 -0.015  0.093 -0.088 -0.032 -0.378  0.422  0.438 -0.365   \n",
       "1854  0.300 -0.492 -0.119 -0.030 -0.187 -0.213 -0.193  0.585  0.333 -0.544   \n",
       "739   0.476 -0.529  0.130  0.050 -0.363 -0.093 -0.260  0.450  0.431 -0.536   \n",
       "3588  0.263 -0.175 -0.613  0.279 -0.317  0.021 -0.252  0.316  0.503 -0.495   \n",
       "4649  0.160 -0.492 -0.120  0.255 -0.413 -0.117  0.007  0.635  0.422 -0.542   \n",
       "\n",
       "      wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "5548 -0.030 -0.103  0.176  0.248  0.163  0.028  0.280 -0.048 -0.093  0.388   \n",
       "1854 -0.041 -0.022  0.018  0.175  0.219  0.005  0.460 -0.260 -0.180  0.554   \n",
       "739   0.002  0.047  0.328  0.236  0.279  0.050  0.328  0.054  0.036  0.424   \n",
       "3588  0.072 -0.104  0.290  0.616  0.474  0.196  0.211  0.003 -0.065  0.275   \n",
       "4649 -0.070 -0.326  0.441  0.209  0.061  0.003  0.515 -0.421  0.094  0.618   \n",
       "\n",
       "      wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "5548  0.070 -0.151  0.389 -0.234  0.025  0.420 -0.047  0.287  0.404 -0.052   \n",
       "1854  0.221 -0.115  0.543 -0.291 -0.006  0.296 -0.090  0.176  0.202 -0.354   \n",
       "739   0.095 -0.232  0.410 -0.542  0.027  0.282 -0.071  0.273  0.184 -0.428   \n",
       "3588 -0.045 -0.094  0.427 -0.231 -0.007  0.499  0.297  0.070  0.137  0.019   \n",
       "4649  0.276 -0.034  0.629 -0.292 -0.055  0.312 -0.078  0.165  0.133 -0.187   \n",
       "\n",
       "      wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "5548  0.448  0.201 -0.116  0.193  0.246  0.106  0.379 -0.092  0.278 -0.482   \n",
       "1854  0.246  0.652  0.097  0.026  0.074 -0.228  0.390 -0.292  0.124 -0.407   \n",
       "739   0.479  0.502 -0.024  0.134  0.069 -0.012  0.270  0.004  0.291 -0.377   \n",
       "3588  0.091  0.515 -0.019  0.089  0.052 -0.042  0.085 -0.014 -0.056 -0.038   \n",
       "4649  0.465  0.460  0.087 -0.079  0.019 -0.056  0.409 -0.037  0.120 -0.500   \n",
       "\n",
       "      wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "5548 -0.100  0.171  0.159 -0.077 -0.229  0.053 -0.027 -0.307  0.247 -0.631   \n",
       "1854 -0.293  0.448 -0.007 -0.475 -0.003  0.272  0.079 -0.189 -0.022 -0.430   \n",
       "739   0.093  0.336  0.194  0.158 -0.113  0.159 -0.037 -0.274 -0.030 -0.218   \n",
       "3588 -0.015  0.287  0.151 -0.274 -0.210  0.006  0.000 -0.015  0.028 -0.378   \n",
       "4649 -0.218  0.435  0.174  0.071 -0.006  0.254  0.074 -0.159  0.001 -0.509   \n",
       "\n",
       "      wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "5548  0.427 -0.369  -0.017  -0.032   0.028  -0.004  -0.263  -0.035  -0.271   \n",
       "1854  0.201 -0.376  -0.374  -0.097   0.312  -0.523  -0.033  -0.088  -0.227   \n",
       "739   0.282 -0.327  -0.197  -0.113   0.033  -0.275   0.426  -0.008  -0.244   \n",
       "3588  0.320 -0.019  -0.232   0.169   0.206  -0.580   0.052  -0.222  -0.152   \n",
       "4649  0.297 -0.348  -0.362  -0.085   0.138  -0.366   0.468   0.058  -0.645   \n",
       "\n",
       "      wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "5548   0.040  -0.008  -0.207  -0.018  -0.460  -0.004   0.009   0.309  -0.030   \n",
       "1854   0.133   0.090  -0.553  -0.450  -0.522   0.283   0.340   0.301  -0.351   \n",
       "739    0.144  -0.067  -0.332  -0.208  -0.190   0.070   0.016   0.383  -0.233   \n",
       "3588   0.397   0.118  -0.274  -0.327  -0.098   0.228   0.183   0.103  -0.060   \n",
       "4649   0.127  -0.019  -0.298  -0.023  -0.508   0.201  -0.149   0.430  -0.100   \n",
       "\n",
       "      wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "5548   0.446   0.467  -0.552  -0.521   0.322   0.328   0.012  -0.207   0.260   \n",
       "1854   0.094   0.447   0.034   0.076   0.224   0.413   0.184  -0.198   0.257   \n",
       "739    0.695   0.523   0.163   0.162   0.335   0.241   0.011  -0.239   0.279   \n",
       "3588   0.071   0.220  -0.127  -0.209   0.355   0.591   0.187  -0.018   0.591   \n",
       "4649   0.531   0.582   0.203   0.213   0.384   0.043   0.349  -0.189   0.279   \n",
       "\n",
       "      wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "5548  -0.198  -0.210  -0.133  -0.250   0.178   0.046   0.005  -0.143  -0.299   \n",
       "1854  -0.055  -0.143   0.137  -0.130  -0.299  -0.020   0.028  -0.165  -0.125   \n",
       "739    0.104  -0.196  -0.083  -0.116  -0.164   0.105  -0.012  -0.149  -0.320   \n",
       "3588   0.117  -0.108   0.036   0.337   0.191   0.035  -0.065  -0.223  -0.023   \n",
       "4649  -0.090  -0.184  -0.011  -0.099  -0.472   0.096  -0.031  -0.183   0.051   \n",
       "\n",
       "      wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "5548   0.076  -0.220  -0.021   0.150  -0.300   0.001  -0.208  -0.160   0.042   \n",
       "1854  -0.243  -0.127  -0.161   0.048  -0.306  -0.007  -0.157   0.330  -0.007   \n",
       "739    0.197   0.183  -0.077  -0.038   0.096   0.115   0.077  -0.424   0.112   \n",
       "3588  -0.043   0.070  -0.297  -0.227  -0.274   0.066   0.016   0.018   0.005   \n",
       "4649   0.192  -0.426  -0.167  -0.091  -0.122   0.091  -0.086  -0.383   0.092   \n",
       "\n",
       "      wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "5548   0.042   0.157   0.081   0.068  -0.008  -0.010  -0.148  -0.640  -0.340   \n",
       "1854  -0.017  -0.018  -0.012   0.164   0.189   0.034  -0.163  -0.266  -0.277   \n",
       "739    0.124   0.100   0.100  -0.166  -0.261  -0.123  -0.132  -0.286  -0.340   \n",
       "3588  -0.264  -0.198  -0.464  -0.185  -0.057  -0.052   0.100  -0.429  -0.240   \n",
       "4649   0.096   0.102   0.088  -0.007  -0.032   0.017  -0.143  -0.217  -0.373   \n",
       "\n",
       "      wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "5548  -0.448  -0.216   0.072   0.223  -0.223  -0.326  -0.687  -0.149  -0.308   \n",
       "1854  -0.509  -0.191   0.536   0.382  -0.026  -0.283  -0.282   0.522   0.050   \n",
       "739   -0.216  -0.199   0.404   0.255  -0.196  -0.282  -0.751  -0.129  -0.226   \n",
       "3588  -0.282  -0.191   0.291   0.235  -0.118  -0.535  -0.286   0.203  -0.135   \n",
       "4649  -0.294  -0.230   0.531   0.406   0.189  -0.267  -0.374  -0.182   0.555   \n",
       "\n",
       "      wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "5548  -0.100  -0.297  -0.595   0.129  -0.305   0.453   0.114   0.240   0.095   \n",
       "1854  -0.213  -0.472  -0.768   0.363  -0.124   0.648   0.188   0.410   0.157   \n",
       "739    0.024  -0.230  -0.452   0.327  -0.338   1.358   0.176   0.417   0.151   \n",
       "3588  -0.509  -0.506  -0.336   0.093  -0.161   0.223   0.031   0.563   0.306   \n",
       "4649  -0.262  -0.136  -0.064   0.397  -0.283   0.786   0.240   0.447   0.200   \n",
       "\n",
       "      wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "5548   0.122  -0.355  -0.136  -0.269  -0.289   0.040  \n",
       "1854   0.129  -0.334  -0.205  -0.126  -0.278  -0.001  \n",
       "739    0.148  -0.265  -0.449  -0.273  -0.324   0.108  \n",
       "3588  -0.542  -0.448  -0.139  -0.240  -0.443   0.065  \n",
       "4649   0.251  -0.096  -0.114  -0.115  -0.301   0.103  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.898</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.952</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.532</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.726</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.406</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.758</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.615</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.516</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.466</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.553</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>0.750</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3815</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.551</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.495</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.747</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.891</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.743</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.899</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.747</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.501</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.486</td>\n",
       "      <td>-0.863</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.480</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.495</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>0.655</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.553</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.494</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.814</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.406</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "4197  1373158606         0.000         0.000        -0.432         0.000   \n",
       "5318  1373158606         0.000         0.770         0.000         0.000   \n",
       "3815  1373158606        -0.563         0.000         0.000         0.158   \n",
       "9727  1373158606         0.000         0.000         0.000         0.000   \n",
       "5410  1373158606         0.000         0.991         0.000         0.000   \n",
       "\n",
       "      00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "4197         0.000         0.000         0.000         0.952         0.000   \n",
       "5318        -0.280         0.000        -0.654         0.000         0.000   \n",
       "3815         0.000         0.000         0.000         0.256         0.000   \n",
       "9727         0.027         0.000         0.899         0.000         0.000   \n",
       "5410         0.000         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "4197         0.000         0.674         0.000         0.000         0.000   \n",
       "5318         0.000         0.000         0.000         0.935         0.000   \n",
       "3815         0.000         0.764         0.000         0.000         0.000   \n",
       "9727        -0.898         0.000         0.000         0.000         0.000   \n",
       "5410         0.554         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "4197        -0.603        -0.575         0.000         0.000         0.000   \n",
       "5318         0.000         0.000         0.000         0.000         0.000   \n",
       "3815         0.000         0.000         0.000         0.000         0.000   \n",
       "9727         0.000        -0.506         0.000         0.000         0.000   \n",
       "5410         0.000         0.000         0.000         0.000        -0.170   \n",
       "\n",
       "      11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "4197         0.000         0.000              -0.012              -0.004   \n",
       "5318         0.682         0.000              -0.084               0.758   \n",
       "3815        -0.833         0.000              -0.562              -0.004   \n",
       "9727         0.000         0.747              -0.005               0.012   \n",
       "5410        -0.952         0.655              -0.000               0.989   \n",
       "\n",
       "      00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "4197              -0.414               0.019               0.004   \n",
       "5318              -0.005               0.102              -0.260   \n",
       "3815               0.002               0.156               0.009   \n",
       "9727               0.002               0.009               0.022   \n",
       "5410               0.005              -0.012               0.011   \n",
       "\n",
       "      00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "4197              -0.009               0.047               0.898   \n",
       "5318              -0.052              -0.592               0.000   \n",
       "3815              -0.001              -0.002               0.238   \n",
       "9727              -0.015               0.891              -0.003   \n",
       "5410               0.004               0.009               0.004   \n",
       "\n",
       "      00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "4197              -0.026              -0.001               0.640   \n",
       "5318              -0.008              -0.031               0.102   \n",
       "3815               0.011               0.007               0.773   \n",
       "9727               0.011              -0.894               0.004   \n",
       "5410               0.002               0.527               0.001   \n",
       "\n",
       "      01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "4197               0.013               0.009              -0.006   \n",
       "5318               0.011               0.860               0.001   \n",
       "3815               0.011              -0.015              -0.011   \n",
       "9727              -0.010               0.007               0.003   \n",
       "5410              -0.008              -0.012              -0.004   \n",
       "\n",
       "      02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "4197              -0.575              -0.566               0.009   \n",
       "5318              -0.038               0.116              -0.009   \n",
       "3815              -0.019               0.006              -0.005   \n",
       "9727              -0.003              -0.498              -0.007   \n",
       "5410              -0.006               0.009              -0.009   \n",
       "\n",
       "      10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "4197              -0.005              -0.009               0.001   \n",
       "5318              -0.027              -0.040               0.615   \n",
       "3815               0.003               0.002              -0.795   \n",
       "9727               0.001              -0.004              -0.002   \n",
       "5410               0.015              -0.150              -0.912   \n",
       "\n",
       "      20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "4197              -0.005               0.000              -0.000   \n",
       "5318              -0.042              -0.000               0.770   \n",
       "3815              -0.025              -0.563               0.000   \n",
       "9727               0.743              -0.000              -0.000   \n",
       "5410               0.615               0.000               0.991   \n",
       "\n",
       "      00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "4197              -0.432               0.000              -0.000   \n",
       "5318              -0.000              -0.000              -0.280   \n",
       "3815               0.000               0.158              -0.000   \n",
       "9727               0.000              -0.000               0.027   \n",
       "5410               0.000              -0.000               0.000   \n",
       "\n",
       "      00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "4197              -0.000               0.000               0.952   \n",
       "5318               0.000              -0.654              -0.000   \n",
       "3815               0.000               0.000               0.256   \n",
       "9727               0.000               0.899              -0.000   \n",
       "5410               0.000              -0.000               0.000   \n",
       "\n",
       "      00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "4197              -0.000              -0.000               0.674   \n",
       "5318              -0.000              -0.000               0.000   \n",
       "3815               0.000              -0.000               0.764   \n",
       "9727               0.000              -0.898               0.000   \n",
       "5410               0.000               0.554              -0.000   \n",
       "\n",
       "      01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "4197               0.000               0.000               0.000   \n",
       "5318              -0.000               0.935              -0.000   \n",
       "3815               0.000               0.000               0.000   \n",
       "9727              -0.000              -0.000               0.000   \n",
       "5410              -0.000               0.000               0.000   \n",
       "\n",
       "      02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "4197              -0.603              -0.575               0.000   \n",
       "5318              -0.000               0.000              -0.000   \n",
       "3815               0.000              -0.000              -0.000   \n",
       "9727               0.000              -0.506               0.000   \n",
       "5410               0.000               0.000               0.000   \n",
       "\n",
       "      10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "4197              -0.000              -0.000              -0.000   \n",
       "5318              -0.000              -0.000               0.682   \n",
       "3815               0.000               0.000              -0.833   \n",
       "9727              -0.000              -0.000               0.000   \n",
       "5410              -0.000              -0.170              -0.952   \n",
       "\n",
       "      20000-lstsq_target   wb_0   wb_1  wb_2   wb_3  wb_4  wb_5  wb_6   wb_7  \\\n",
       "4197               0.000 -0.069 -0.020 0.450 -0.009 0.068 0.057 0.096 -0.036   \n",
       "5318               0.000  0.048 -0.198 0.443  0.236 0.340 0.236 0.221 -0.418   \n",
       "3815              -0.000 -0.111 -0.088 0.429  0.124 0.326 0.105 0.337 -0.222   \n",
       "9727               0.747 -0.060 -0.019 0.066  0.021 0.211 0.040 0.379  0.066   \n",
       "5410               0.655  0.092 -0.225 0.205  0.268 0.865 0.081 0.390 -0.390   \n",
       "\n",
       "       wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "4197  0.015 0.487  0.504  0.025 -0.073  0.439  0.052  0.532 -0.018  0.167   \n",
       "5318 -0.160 0.245  0.510  0.025  0.052  0.327  0.234  0.254  0.131  0.352   \n",
       "3815  0.295 0.418  0.535 -0.226  0.136  0.379  0.090  0.482 -0.264  0.213   \n",
       "9727  0.036 0.841  0.339 -0.033 -0.005  0.490  0.038  0.462 -0.001  0.149   \n",
       "5410  0.241 0.584  0.704 -0.300 -0.103  0.161  0.074  0.553 -0.025  0.184   \n",
       "\n",
       "      wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "4197  0.214  0.205 -0.068  0.071  0.017 -0.003  0.015 -0.150  0.066 -0.118   \n",
       "5318  0.387  0.347  0.134  0.034 -0.118 -0.001 -0.468 -0.331 -0.086 -0.359   \n",
       "3815  0.424  0.237 -0.213  0.069  0.200  0.066 -0.400 -0.148 -0.055 -0.045   \n",
       "9727  0.540  0.508 -0.033  0.036 -0.076  0.098 -0.423 -0.053  0.059  0.004   \n",
       "5410  0.222  0.209 -0.134 -0.053  0.227  0.144 -0.506 -0.103 -0.125 -0.351   \n",
       "\n",
       "      wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "4197 -0.726  0.024  0.330  0.327  0.217  0.386 -0.001 -0.164 -0.158 -0.007   \n",
       "5318 -0.482 -0.022  0.355  0.306  0.064  0.352  0.516 -0.288 -0.491  0.028   \n",
       "3815 -0.311 -0.357  0.399  0.278 -0.070  0.270 -0.018 -0.119 -0.328  0.015   \n",
       "9727 -0.273 -0.088  0.355  0.283  0.003  0.119 -0.060 -0.037 -0.325  0.011   \n",
       "5410 -0.035 -0.453  0.292  0.374  0.058  0.335  0.128 -0.350 -0.495 -0.111   \n",
       "\n",
       "      wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "4197 -0.098 -0.187  0.166  0.018  0.478 -0.157  0.406 -0.029 -0.244  0.640   \n",
       "5318 -0.200 -0.139 -0.025  0.397  0.515 -0.071  0.477  0.241 -0.163  0.463   \n",
       "3815 -0.146 -0.202  0.151  0.321  0.551 -0.378  0.518  0.084 -0.061  0.370   \n",
       "9727 -0.088 -0.151  0.129  0.173  0.501 -0.003  0.167  0.261  0.011  0.273   \n",
       "5410 -0.272 -0.203  0.245  0.082  0.433 -0.230  0.407  0.113 -0.072  0.432   \n",
       "\n",
       "      wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "4197  0.262 -0.729 -0.102  0.028 -0.203  0.208 -0.352  0.593  0.331 -0.348   \n",
       "5318  0.269 -0.508  0.229  0.136 -0.118 -0.078 -0.278  0.394  0.466 -0.523   \n",
       "3815  0.221 -0.374 -0.085  0.077 -0.147 -0.197 -0.221  0.528  0.283 -0.553   \n",
       "9727  0.253 -0.509 -0.231  0.293 -0.573 -0.271 -0.257  0.371  0.486 -0.863   \n",
       "5410  0.320 -0.494 -0.339 -0.050 -0.459  0.053 -0.130  0.614  0.241 -0.534   \n",
       "\n",
       "      wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "4197 -0.021 -0.080  0.242  0.378  0.207  0.054  0.461 -0.075 -0.301  0.560   \n",
       "5318  0.036  0.054  0.317  0.416  0.434  0.284  0.262 -0.210 -0.323  0.355   \n",
       "3815 -0.008 -0.092  0.248  0.283  0.297  0.146  0.378 -0.063 -0.208  0.495   \n",
       "9727 -0.336 -0.053  0.103  0.328  0.501  0.309  0.240  0.026 -0.195  0.336   \n",
       "5410 -0.009 -0.096 -0.094  0.217  0.191  0.028  0.512 -0.078 -0.671  0.600   \n",
       "\n",
       "      wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "4197  0.214 -0.136  0.547 -0.365 -0.059  0.323 -0.160  0.157  0.312 -0.049   \n",
       "5318  0.027 -0.334  0.287 -0.470  0.114  0.455 -0.043  0.182  0.221  0.065   \n",
       "3815 -0.110 -0.109  0.381 -0.446  0.043  0.324 -0.024  0.168  0.281 -0.154   \n",
       "9727 -0.044 -0.096  0.311 -0.506 -0.153  0.480 -0.048  0.222  0.289 -0.137   \n",
       "5410  0.271 -0.070  0.630 -0.526 -0.000  0.266 -0.077  0.192  0.232 -0.452   \n",
       "\n",
       "      wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "4197  0.055  0.447  0.002  0.017  0.026  0.006  0.220  0.052  0.018 -0.451   \n",
       "5318  0.395  0.553 -0.014  0.125  0.043 -0.314  0.461  0.066  0.303 -0.276   \n",
       "3815  0.284  0.286  0.036 -0.006  0.115  0.016  0.190 -0.010  0.091 -0.285   \n",
       "9727  0.235  0.495 -0.017  0.080  0.000  0.004  0.020 -0.369  0.082 -0.102   \n",
       "5410  0.350  0.002  0.078  0.030  0.040  0.007  0.187 -0.005  0.110 -0.482   \n",
       "\n",
       "      wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "4197 -0.045  0.336  0.126 -0.106 -0.108  0.131 -0.014 -0.302  0.050 -0.048   \n",
       "5318 -0.045  0.324 -0.009 -0.168 -0.111  0.152 -0.051 -0.442  0.110 -0.456   \n",
       "3815 -0.063  0.338  0.063 -0.006 -0.078 -0.052  0.033 -0.043  0.100 -0.288   \n",
       "9727 -0.026  0.325  0.177 -0.458 -0.120  0.004 -0.058 -0.292  0.024 -0.259   \n",
       "5410 -0.216  0.427  0.079 -0.011 -0.014  0.200  0.055 -0.254 -0.033 -0.011   \n",
       "\n",
       "      wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "4197  0.028 -0.104  -0.378   0.046   0.263  -0.283  -0.190  -0.075  -0.465   \n",
       "5318  0.325 -0.336  -0.058   0.119  -0.050  -0.003   0.299   0.003  -0.255   \n",
       "3815  0.264 -0.056  -0.365   0.052   0.183  -0.386  -0.016  -0.054  -0.355   \n",
       "9727  0.304 -0.361  -0.104   0.117  -0.033  -0.483   0.162  -0.172  -0.127   \n",
       "5410  0.257 -0.350  -0.297  -0.070  -0.015  -0.322  -0.001   0.111  -0.303   \n",
       "\n",
       "      wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "4197   0.542   0.007  -0.168  -0.269  -0.588   0.035   0.009   0.280  -0.106   \n",
       "5318   0.157  -0.116  -0.161  -0.221  -0.190   0.095  -0.029   0.368  -0.374   \n",
       "3815   0.190  -0.026  -0.180  -0.183  -0.251   0.178   0.021   0.273  -0.188   \n",
       "9727   0.006   0.012  -0.006  -0.499  -0.365   0.045   0.111   0.197  -0.080   \n",
       "5410   0.164   0.001  -0.393   0.002  -0.327  -0.043  -0.180   0.494  -0.268   \n",
       "\n",
       "      wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "4197  -0.227   0.428   0.037  -0.012   0.282   0.514   0.083  -0.522   0.327   \n",
       "5318   0.167   0.527   0.149   0.106   0.277   0.231  -0.339  -0.179   0.260   \n",
       "3815  -0.101   0.440  -0.038   0.107   0.402   0.326   0.063  -0.197   0.070   \n",
       "9727  -0.025   0.343   0.006   0.048   0.136   0.216   0.300  -0.069   0.293   \n",
       "5410   0.011   0.643   0.213   0.252   0.349   0.210  -0.012  -0.241   0.283   \n",
       "\n",
       "      wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "4197  -0.089  -0.020   0.207   0.098   0.142   0.087  -0.029  -0.093  -0.284   \n",
       "5318  -0.089  -0.087  -0.005  -0.109   0.113   0.049   0.035  -0.183  -0.043   \n",
       "3815  -0.083   0.039   0.244  -0.057   0.008   0.006  -0.041  -0.138  -0.198   \n",
       "9727   0.002  -0.100   0.412  -0.132   0.142   0.158  -0.097   0.216  -0.078   \n",
       "5410   0.039  -0.145  -0.080   0.236  -0.003   0.012   0.088  -0.184  -0.241   \n",
       "\n",
       "      wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "4197   0.143   0.194   0.300   0.060   0.095   0.075   0.081   0.322   0.082   \n",
       "5318  -0.566   0.228   0.044   0.139   0.171   0.056  -0.085  -0.153   0.047   \n",
       "3815   0.210   0.231   0.247   0.181   0.155  -0.061   0.058   0.129  -0.013   \n",
       "9727  -0.245   0.115  -0.205  -0.295  -0.013   0.158  -0.012  -0.097   0.156   \n",
       "5410   0.363  -0.291   0.015  -0.109  -0.108   0.019   0.195   0.314   0.008   \n",
       "\n",
       "      wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "4197   0.034   0.058  -0.160  -0.012  -0.290  -0.235   0.122  -0.222  -0.235   \n",
       "5318   0.054  -0.072  -0.050  -0.168   0.216   0.026  -0.146  -0.417  -0.201   \n",
       "3815   0.068   0.022  -0.276   0.024  -0.202   0.025   0.458  -0.228  -0.231   \n",
       "9727  -0.354  -0.201   0.153   0.178  -0.142  -0.099  -0.129  -0.095  -0.314   \n",
       "5410  -0.018   0.013  -0.097  -0.180  -0.423   0.081  -0.132  -0.340  -0.259   \n",
       "\n",
       "      wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "4197  -0.464  -0.280   0.183   0.378  -0.089  -0.418  -0.691  -0.091  -0.260   \n",
       "5318  -0.229  -0.376   0.405   0.307  -0.107  -0.313  -0.562   0.750  -0.324   \n",
       "3815  -0.414   0.092   0.115   0.371  -0.067  -0.262  -0.503  -0.059  -0.261   \n",
       "9727  -0.517  -0.094   0.256   0.289  -0.153  -0.565  -0.246   0.520   0.056   \n",
       "5410  -0.273  -0.223   0.682   0.372  -0.126  -0.302  -0.478  -0.284   0.814   \n",
       "\n",
       "      wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "4197  -0.460  -0.198  -0.279   0.291  -0.332   0.578   0.181   0.220   0.066   \n",
       "5318  -0.423  -0.472  -0.303   0.257  -0.078   0.591   0.196   0.330   0.299   \n",
       "3815  -0.174  -0.399  -0.257   0.234  -0.274   0.466   0.191   0.475   0.208   \n",
       "9727  -0.115  -0.652  -0.338   0.185  -0.172   0.291   0.109   0.885   0.214   \n",
       "5410  -0.059  -0.313  -0.076   0.406  -0.416   0.785   0.248   0.390   0.148   \n",
       "\n",
       "      wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "4197   0.221  -0.356  -0.364  -0.390  -0.276   0.082  \n",
       "5318   0.146  -0.258  -0.257  -0.228  -0.281   0.051  \n",
       "3815   0.197  -0.206  -0.088  -0.137  -0.521  -0.006  \n",
       "9727   0.053  -0.234  -0.030  -0.240  -0.323   0.165  \n",
       "5410   0.271  -0.235  -0.309  -0.176  -0.332   0.017  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.407</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.408</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.466</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.775</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.925</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.775</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.778</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.977</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>0.382</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.486</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.477</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.930</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.567</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.589</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.456</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>0.662</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "7217  1373158606         0.000         0.000         0.000         0.000   \n",
       "8291  1373158606         0.000         0.000        -0.541        -0.823   \n",
       "4607  1373158606        -0.775         0.513         0.000         0.000   \n",
       "5114  1373158606         0.889         0.000         0.000         0.000   \n",
       "1859  1373158606         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "7217         0.000        -0.275         0.000         0.000        -0.795   \n",
       "8291        -0.882         0.000         0.000         0.000         0.000   \n",
       "4607         0.000         0.000         0.000        -0.778         0.000   \n",
       "5114         0.000         0.000         0.038         0.000         0.000   \n",
       "1859         0.390         0.000         0.000         0.000        -0.950   \n",
       "\n",
       "      00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "7217         0.333         0.000         0.000         0.000         0.000   \n",
       "8291         0.000         0.000         0.000         0.000         0.000   \n",
       "4607         0.000         0.000        -0.296         0.000         0.000   \n",
       "5114         0.000         0.000        -0.707         0.000         0.000   \n",
       "1859         0.000         0.930         0.000         0.000         0.000   \n",
       "\n",
       "      02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "7217         0.000         0.640         0.053         0.000         0.000   \n",
       "8291         0.000        -0.272         0.000        -0.335         0.000   \n",
       "4607         0.000         0.000         0.000         0.000         0.977   \n",
       "5114         0.351         0.000         0.396         0.000         0.000   \n",
       "1859         0.457         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "7217         0.000         0.000               0.005              -0.009   \n",
       "8291         0.000         0.000               0.021              -0.032   \n",
       "4607         0.000         0.000              -0.766               0.460   \n",
       "5114         0.000         0.000               0.877               0.022   \n",
       "1859        -0.755         0.000               0.026              -0.014   \n",
       "\n",
       "      00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "7217               0.003              -0.010               0.014   \n",
       "8291              -0.509              -0.844              -0.859   \n",
       "4607               0.016              -0.020               0.012   \n",
       "5114              -0.029               0.009              -0.012   \n",
       "1859              -0.014              -0.010               0.396   \n",
       "\n",
       "      00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "7217              -0.289               0.006              -0.002   \n",
       "8291               0.003              -0.016               0.003   \n",
       "4607               0.013               0.016              -0.743   \n",
       "5114              -0.016               0.047               0.014   \n",
       "1859              -0.010              -0.016               0.021   \n",
       "\n",
       "      00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "7217              -0.755               0.300               0.015   \n",
       "8291               0.013               0.002              -0.012   \n",
       "4607              -0.007               0.002              -0.021   \n",
       "5114               0.012              -0.012               0.028   \n",
       "1859              -0.923              -0.018               0.942   \n",
       "\n",
       "      01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "7217               0.001              -0.015               0.008   \n",
       "8291              -0.000              -0.013               0.004   \n",
       "4607              -0.259               0.006              -0.004   \n",
       "5114              -0.654              -0.004              -0.015   \n",
       "1859               0.017              -0.009               0.007   \n",
       "\n",
       "      02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "7217              -0.001               0.634               0.049   \n",
       "8291               0.007              -0.306              -0.014   \n",
       "4607               0.001               0.025              -0.001   \n",
       "5114               0.286               0.035               0.355   \n",
       "1859               0.399              -0.027               0.002   \n",
       "\n",
       "      10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "7217               0.002               0.009              -0.013   \n",
       "8291              -0.315               0.001               0.018   \n",
       "4607               0.000               0.925              -0.000   \n",
       "5114               0.013              -0.002               0.034   \n",
       "1859               0.003               0.006              -0.682   \n",
       "\n",
       "      20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "7217               0.009               0.000              -0.000   \n",
       "8291               0.027               0.000               0.000   \n",
       "4607              -0.002              -0.775               0.513   \n",
       "5114              -0.039               0.889               0.000   \n",
       "1859              -0.019              -0.000               0.000   \n",
       "\n",
       "      00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "7217               0.000              -0.000              -0.000   \n",
       "8291              -0.541              -0.823              -0.882   \n",
       "4607              -0.000               0.000              -0.000   \n",
       "5114              -0.000               0.000               0.000   \n",
       "1859               0.000               0.000               0.390   \n",
       "\n",
       "      00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "7217              -0.275              -0.000              -0.000   \n",
       "8291              -0.000               0.000               0.000   \n",
       "4607               0.000               0.000              -0.778   \n",
       "5114               0.000               0.038              -0.000   \n",
       "1859               0.000               0.000               0.000   \n",
       "\n",
       "      00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "7217              -0.795               0.333              -0.000   \n",
       "8291               0.000              -0.000              -0.000   \n",
       "4607              -0.000              -0.000               0.000   \n",
       "5114               0.000               0.000              -0.000   \n",
       "1859              -0.950              -0.000               0.930   \n",
       "\n",
       "      01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "7217              -0.000              -0.000               0.000   \n",
       "8291               0.000               0.000              -0.000   \n",
       "4607              -0.296              -0.000               0.000   \n",
       "5114              -0.707               0.000              -0.000   \n",
       "1859              -0.000               0.000               0.000   \n",
       "\n",
       "      02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "7217               0.000               0.640               0.053   \n",
       "8291              -0.000              -0.272               0.000   \n",
       "4607               0.000               0.000               0.000   \n",
       "5114               0.351              -0.000               0.396   \n",
       "1859               0.457               0.000               0.000   \n",
       "\n",
       "      10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "7217               0.000               0.000               0.000   \n",
       "8291              -0.335               0.000               0.000   \n",
       "4607              -0.000               0.977              -0.000   \n",
       "5114              -0.000              -0.000               0.000   \n",
       "1859              -0.000              -0.000              -0.755   \n",
       "\n",
       "      20000-lstsq_target   wb_0   wb_1  wb_2  wb_3  wb_4  wb_5  wb_6   wb_7  \\\n",
       "7217               0.000 -0.013 -0.135 0.225 0.015 0.106 0.215 0.186 -0.184   \n",
       "8291              -0.000  0.145 -0.072 0.214 0.324 0.068 0.154 0.313  0.100   \n",
       "4607              -0.000 -0.082 -0.306 0.382 0.107 0.327 0.289 0.191 -0.158   \n",
       "5114               0.000 -0.077 -0.141 0.227 0.210 0.060 0.167 0.259 -0.210   \n",
       "1859              -0.000 -0.020 -0.076 0.457 0.137 0.018 0.153 0.265 -0.420   \n",
       "\n",
       "      wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "7217 0.121  0.252  0.271 -0.014 -0.120  0.160  0.277  0.339  0.225  0.389   \n",
       "8291 0.223  0.339  0.118 -0.018  0.116  0.270  0.126  0.484  0.196  0.223   \n",
       "4607 0.098  0.325  0.401 -0.297 -0.083  0.256  0.254  0.356 -0.110  0.344   \n",
       "5114 0.173 -0.274  0.357 -0.141 -0.120  0.261  0.164  0.254  0.065  0.277   \n",
       "1859 0.237  0.468  0.567 -0.078  0.108  0.401  0.159  0.298  0.085  0.260   \n",
       "\n",
       "      wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "7217  0.426  0.371  0.273  0.101 -0.131 -0.122 -0.364  0.002  0.024 -0.266   \n",
       "8291  0.217  0.147  0.263  0.192 -0.011  0.033  0.022 -0.069 -0.043 -0.372   \n",
       "4607  0.372  0.380  0.314  0.139  0.040 -0.084 -0.331 -0.191  0.046 -0.078   \n",
       "5114  0.322  0.294  0.064  0.048  0.013  0.034 -0.485 -0.146  0.085 -0.330   \n",
       "1859  0.314  0.280 -0.023 -0.001 -0.501  0.031 -0.506 -0.013 -0.068 -0.064   \n",
       "\n",
       "      wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "7217 -0.307 -0.070  0.218  0.331  0.300  0.264  0.098 -0.433 -0.005 -0.109   \n",
       "8291 -0.141 -0.304  0.329  0.333  0.047  0.280 -0.337 -0.165 -0.163  0.020   \n",
       "4607 -0.253 -0.064  0.283  0.350  0.291  0.289 -0.046 -0.261 -0.305  0.096   \n",
       "5114 -0.337 -0.784  0.257  0.387  0.187  0.337  0.582 -0.350 -0.416  0.186   \n",
       "1859 -0.023 -0.067  0.356  0.241  0.048  0.163 -0.198 -0.182 -0.089 -0.024   \n",
       "\n",
       "      wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "7217 -0.293 -0.197  0.116  0.355  0.423 -0.107  0.407 -0.039  0.074  0.059   \n",
       "8291 -0.144 -0.317  0.142  0.067  0.412 -0.005  0.275  0.034  0.042  0.363   \n",
       "4607 -0.149 -0.184  0.005  0.026  0.422 -0.149  0.418  0.013  0.096  0.403   \n",
       "5114 -0.199 -0.249 -0.147  0.354  0.409 -0.180  0.377  0.161  0.129  0.258   \n",
       "1859 -0.189 -0.162  0.160  0.417  0.511 -0.099  0.491  0.166  0.014  0.888   \n",
       "\n",
       "      wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "7217  0.306 -0.143  0.102  0.192 -0.283  0.019 -0.645  0.464  0.408 -0.138   \n",
       "8291  0.309 -0.059 -0.005  0.068 -0.439  0.040 -0.432  0.491  0.378 -0.139   \n",
       "4607  0.325 -0.074 -0.133  0.221 -0.395 -0.194 -0.156  0.375  0.424 -0.205   \n",
       "5114  0.347 -0.493 -0.099  0.039 -0.447 -0.064 -0.025  0.460  0.416 -0.083   \n",
       "1859  0.140 -0.286  0.275  0.325 -0.136  0.084 -0.525  0.444  0.423 -0.526   \n",
       "\n",
       "      wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "7217 -0.029 -0.610  0.006  0.240  0.139 -0.005  0.295 -0.076 -0.119  0.389   \n",
       "8291 -0.087 -0.428  0.026  0.231  0.306  0.160  0.295 -0.116 -0.172  0.416   \n",
       "4607 -0.067 -0.561 -0.404  0.299  0.358  0.079  0.317 -0.345 -0.362  0.400   \n",
       "5114  0.061 -0.026  0.084  0.199  0.082  0.036  0.329 -0.131 -0.219  0.427   \n",
       "1859  0.037 -0.269  0.319  0.390  0.352  0.245  0.309  0.003 -0.119  0.408   \n",
       "\n",
       "      wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "7217  0.065 -0.214  0.544 -0.217 -0.261  0.403 -0.157  0.266  0.420 -0.467   \n",
       "8291 -0.019 -0.317  0.291 -0.229  0.048  0.369  0.126  0.426  0.436 -0.276   \n",
       "4607  0.291 -0.095  0.463 -0.325  0.051  0.413  0.322  0.125  0.279 -0.151   \n",
       "5114  0.098 -0.211  0.421 -0.054  0.112  0.405 -0.062  0.153  0.149 -0.449   \n",
       "1859  0.072 -0.176  0.536 -0.436 -0.112  0.384 -0.010  0.235  0.266 -0.102   \n",
       "\n",
       "      wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "7217  0.250  0.309 -0.237  0.150  0.337 -0.076  0.334 -0.266  0.466 -0.468   \n",
       "8291  0.552  0.272 -0.055  0.302  0.331  0.101  0.112 -0.047  0.444 -0.107   \n",
       "4607  0.134  0.543 -0.091  0.093  0.238 -0.122  0.139 -0.015  0.097 -0.291   \n",
       "5114  0.175  0.067  0.024  0.132 -0.010 -0.008  0.081 -0.000  0.129 -0.081   \n",
       "1859  0.428  0.589 -0.086  0.180  0.026 -0.028  0.301  0.098  0.456 -0.164   \n",
       "\n",
       "      wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "7217 -0.201  0.187  0.119 -0.176 -0.280 -0.016 -0.086 -0.429  0.202 -0.629   \n",
       "8291  0.140 -0.055  0.333 -0.427 -0.331 -0.531 -0.357 -0.616  0.279 -0.176   \n",
       "4607 -0.091  0.317  0.129 -0.184 -0.140 -0.015  0.025 -0.332  0.117 -0.330   \n",
       "5114 -0.086  0.365  0.021 -0.161 -0.081  0.191  0.017 -0.257  0.122 -0.227   \n",
       "1859  0.057  0.245  0.133 -0.201 -0.191  0.071 -0.041 -0.538 -0.262  0.092   \n",
       "\n",
       "      wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "7217  0.442 -0.379   0.009   0.037  -0.154  -0.499   0.181  -0.077  -0.282   \n",
       "8291  0.537  0.280   0.095   0.421   0.000  -0.003  -0.083  -0.203  -0.026   \n",
       "4607  0.310 -0.054  -0.359   0.066  -0.192  -0.369   0.231  -0.411  -0.263   \n",
       "5114  0.295 -0.339  -0.377   0.168  -0.007  -0.516   0.181  -0.147  -0.098   \n",
       "1859  0.317 -0.152  -0.066   0.021   0.244  -0.313   0.271  -0.105  -0.279   \n",
       "\n",
       "      wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "7217   0.422  -0.032  -0.274  -0.527  -0.034   0.035  -0.154   0.236  -0.273   \n",
       "8291   0.427   0.220  -0.155  -0.654  -0.105   0.436   0.309  -0.019   0.014   \n",
       "4607   0.382  -0.004  -0.406  -0.417  -0.239   0.203   0.018   0.257  -0.220   \n",
       "5114   0.347   0.101  -0.776  -0.566  -0.525   0.290  -0.415   0.215  -0.392   \n",
       "1859   0.165   0.043  -0.352  -0.146  -0.198   0.237   0.073   0.259  -0.154   \n",
       "\n",
       "      wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "7217   0.205   0.373   0.033   0.035   0.060   0.459   0.008  -0.126   0.349   \n",
       "8291   0.234   0.184   0.123  -0.022   0.253   0.619   0.326   0.076   0.438   \n",
       "4607   0.335   0.399  -0.052   0.099  -0.027   0.406   0.151  -0.170   0.417   \n",
       "5114  -0.002   0.361  -0.003   0.009   0.145   0.486   0.477  -0.031   0.302   \n",
       "1859   0.206   0.404   0.035   0.073  -0.156   0.277  -0.015  -0.237   0.177   \n",
       "\n",
       "      wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "7217  -0.268  -0.166  -0.035  -0.175   0.068   0.002  -0.028   0.145  -0.005   \n",
       "8291   0.038  -0.224  -0.114   0.041  -0.119   0.133  -0.005  -0.461  -0.018   \n",
       "4607  -0.054   0.066   0.245   0.323   0.047  -0.223   0.093   0.182   0.080   \n",
       "5114  -0.093  -0.197  -0.065   0.087   0.235   0.222  -0.068  -0.278  -0.049   \n",
       "1859  -0.288  -0.129   0.217  -0.194  -0.063   0.062  -0.073  -0.181  -0.234   \n",
       "\n",
       "      wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "7217   0.093  -0.133  -0.356  -0.132  -0.114   0.029  -0.129   0.146   0.001   \n",
       "8291   0.005   0.351  -0.232   0.057  -0.176   0.006   0.016   0.123   0.021   \n",
       "4607   0.239   0.075   0.245   0.163   0.173  -0.050   0.176   0.154  -0.076   \n",
       "5114   0.207   0.236  -0.189  -0.231   0.032   0.225  -0.118   0.295   0.218   \n",
       "1859   0.052   0.218  -0.175   0.092   0.135   0.045  -0.297   0.189   0.065   \n",
       "\n",
       "      wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "7217   0.043   0.017  -0.204   0.099   0.319  -0.008  -0.044  -0.784  -0.387   \n",
       "8291  -0.045   0.025  -0.007  -0.160  -0.047  -0.006  -0.474  -0.280  -0.565   \n",
       "4607  -0.225   0.007  -0.141   0.117   0.157   0.091   0.200  -0.213  -0.352   \n",
       "5114   0.225   0.226   0.229  -0.014  -0.052  -0.106  -0.123  -0.213  -0.155   \n",
       "1859   0.042   0.106   0.185  -0.089  -0.173  -0.182   0.294  -0.836  -0.328   \n",
       "\n",
       "      wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "7217  -0.215  -0.122   0.371   0.312  -0.097  -0.355  -0.244   0.220   0.029   \n",
       "8291  -0.222  -0.284  -0.058   0.364  -0.126  -0.643  -0.387   0.043  -0.185   \n",
       "4607  -0.473  -0.263   0.327   0.486  -0.114  -0.338  -0.310  -0.297  -0.213   \n",
       "5114  -0.283  -0.126   0.434   0.339  -0.111  -0.243  -0.363   0.381  -0.242   \n",
       "1859  -0.463  -0.179   0.531   0.347  -0.080  -0.318  -0.385   0.184  -0.318   \n",
       "\n",
       "      wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "7217  -0.415  -0.290  -0.053   0.199  -0.136   0.548   0.157   0.344   0.210   \n",
       "8291  -0.149  -0.408  -0.325   0.086  -0.305   0.468   0.089   0.421   0.070   \n",
       "4607  -0.250  -0.441  -0.110   0.254  -0.292   0.576   0.139   0.553   0.200   \n",
       "5114  -0.252  -0.362  -0.229   0.293  -0.094   0.600   0.143   0.389   0.217   \n",
       "1859  -0.332  -0.359  -0.309   0.164  -0.223   0.657   0.181   0.207   0.267   \n",
       "\n",
       "      wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "7217   0.284  -0.283  -0.239  -0.280  -0.073   0.013  \n",
       "8291   0.206  -0.536  -0.023  -0.301  -0.640   0.004  \n",
       "4607   0.191  -0.302  -0.072  -0.247  -0.428  -0.075  \n",
       "5114   0.127  -0.218  -0.105  -0.224  -0.335   0.209  \n",
       "1859   0.362  -0.267   0.662  -0.185  -0.195   0.054  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=int64, numpy=\n",
       "array([[1, 0],\n",
       "       [4, 2],\n",
       "       [0, 0],\n",
       "       [2, 0],\n",
       "       [3, 0]])>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.constant([[1, 0],\n",
    "  [4, 2],\n",
    "  [0, 0],\n",
    "  [2, 0],\n",
    "  [3, 0]], dtype=tf.int64)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[1],\n",
       "       [4],\n",
       "       [0],\n",
       "       [2],\n",
       "       [3]])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 0, 3, 4, 1], dtype=int32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argsort(tf.keras.backend.flatten(x[:,:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 0, 2])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.gather(tf.reshape(x[:,1:], -1), tf.argsort(tf.keras.backend.flatten(x[:,:1]))),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------- TRAINING INTERPRETATION NET -----------------------------------------------\n",
      "Epoch 1/500\n",
      "35/35 [==============================] - 90s 2s/step - loss: 0.4431 - r2_inet_lambda_fv_loss: 2.6552 - r2_inet_poly_fv_loss: 2.6647 - mae_inet_poly_fv_loss: 0.4429 - val_loss: 0.3928 - val_r2_inet_lambda_fv_loss: 1.9166 - val_r2_inet_poly_fv_loss: 1.9247 - val_mae_inet_poly_fv_loss: 0.3929\n",
      "Epoch 2/500\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.3871 - r2_inet_lambda_fv_loss: 1.8683 - r2_inet_poly_fv_loss: 1.8763 - mae_inet_poly_fv_loss: 0.3868 - val_loss: 0.3639 - val_r2_inet_lambda_fv_loss: 1.5702 - val_r2_inet_poly_fv_loss: 1.5774 - val_mae_inet_poly_fv_loss: 0.3641\n",
      "Epoch 3/500\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.3585 - r2_inet_lambda_fv_loss: 1.5687 - r2_inet_poly_fv_loss: 1.5769 - mae_inet_poly_fv_loss: 0.3581 - val_loss: 0.3532 - val_r2_inet_lambda_fv_loss: 1.4566 - val_r2_inet_poly_fv_loss: 1.4633 - val_mae_inet_poly_fv_loss: 0.3532\n",
      "Epoch 4/500\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.3519 - r2_inet_lambda_fv_loss: 1.5413 - r2_inet_poly_fv_loss: 1.5494 - mae_inet_poly_fv_loss: 0.3515 - val_loss: 0.3452 - val_r2_inet_lambda_fv_loss: 1.3667 - val_r2_inet_poly_fv_loss: 1.3731 - val_mae_inet_poly_fv_loss: 0.3451\n",
      "Epoch 5/500\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.3475 - r2_inet_lambda_fv_loss: 1.4508 - r2_inet_poly_fv_loss: 1.4583 - mae_inet_poly_fv_loss: 0.3471 - val_loss: 0.3400 - val_r2_inet_lambda_fv_loss: 1.2898 - val_r2_inet_poly_fv_loss: 1.2958 - val_mae_inet_poly_fv_loss: 0.3399\n",
      "Epoch 6/500\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.3406 - r2_inet_lambda_fv_loss: 1.4828 - r2_inet_poly_fv_loss: 1.4925 - mae_inet_poly_fv_loss: 0.3402 - val_loss: 0.3371 - val_r2_inet_lambda_fv_loss: 1.2210 - val_r2_inet_poly_fv_loss: 1.2259 - val_mae_inet_poly_fv_loss: 0.3370\n",
      "Epoch 7/500\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.3390 - r2_inet_lambda_fv_loss: 1.3197 - r2_inet_poly_fv_loss: 1.3279 - mae_inet_poly_fv_loss: 0.3386 - val_loss: 0.3379 - val_r2_inet_lambda_fv_loss: 1.2302 - val_r2_inet_poly_fv_loss: 1.2356 - val_mae_inet_poly_fv_loss: 0.3379\n",
      "Epoch 8/500\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3395 - r2_inet_lambda_fv_loss: 1.2928 - r2_inet_poly_fv_loss: 1.2984 - mae_inet_poly_fv_loss: 0.3392 - val_loss: 0.3376 - val_r2_inet_lambda_fv_loss: 1.2316 - val_r2_inet_poly_fv_loss: 1.2370 - val_mae_inet_poly_fv_loss: 0.3376\n",
      "Epoch 9/500\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3340 - r2_inet_lambda_fv_loss: 1.2146 - r2_inet_poly_fv_loss: 1.2209 - mae_inet_poly_fv_loss: 0.3336 - val_loss: 0.3312 - val_r2_inet_lambda_fv_loss: 1.1483 - val_r2_inet_poly_fv_loss: 1.1535 - val_mae_inet_poly_fv_loss: 0.3313\n",
      "Epoch 10/500\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.3293 - r2_inet_lambda_fv_loss: 1.1949 - r2_inet_poly_fv_loss: 1.2002 - mae_inet_poly_fv_loss: 0.3290 - val_loss: 0.3297 - val_r2_inet_lambda_fv_loss: 1.1566 - val_r2_inet_poly_fv_loss: 1.1619 - val_mae_inet_poly_fv_loss: 0.3297\n",
      "Epoch 11/500\n",
      "35/35 [==============================] - 75s 2s/step - loss: 0.3314 - r2_inet_lambda_fv_loss: 1.2268 - r2_inet_poly_fv_loss: 1.2325 - mae_inet_poly_fv_loss: 0.3310 - val_loss: 0.3328 - val_r2_inet_lambda_fv_loss: 1.1870 - val_r2_inet_poly_fv_loss: 1.1923 - val_mae_inet_poly_fv_loss: 0.3328\n",
      "Epoch 12/500\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3291 - r2_inet_lambda_fv_loss: 1.1449 - r2_inet_poly_fv_loss: 1.1497 - mae_inet_poly_fv_loss: 0.3288 - val_loss: 0.3262 - val_r2_inet_lambda_fv_loss: 1.1076 - val_r2_inet_poly_fv_loss: 1.1127 - val_mae_inet_poly_fv_loss: 0.3262\n",
      "Epoch 13/500\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3304 - r2_inet_lambda_fv_loss: 1.2382 - r2_inet_poly_fv_loss: 1.2451 - mae_inet_poly_fv_loss: 0.3301 - val_loss: 0.3253 - val_r2_inet_lambda_fv_loss: 1.1047 - val_r2_inet_poly_fv_loss: 1.1098 - val_mae_inet_poly_fv_loss: 0.3253\n",
      "Epoch 14/500\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3275 - r2_inet_lambda_fv_loss: 1.1594 - r2_inet_poly_fv_loss: 1.1647 - mae_inet_poly_fv_loss: 0.3271 - val_loss: 0.3215 - val_r2_inet_lambda_fv_loss: 1.0314 - val_r2_inet_poly_fv_loss: 1.0356 - val_mae_inet_poly_fv_loss: 0.3215\n",
      "Epoch 15/500\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3234 - r2_inet_lambda_fv_loss: 1.1357 - r2_inet_poly_fv_loss: 1.1411 - mae_inet_poly_fv_loss: 0.3230 - val_loss: 0.3210 - val_r2_inet_lambda_fv_loss: 1.0148 - val_r2_inet_poly_fv_loss: 1.0191 - val_mae_inet_poly_fv_loss: 0.3210\n",
      "Epoch 16/500\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3238 - r2_inet_lambda_fv_loss: 1.0786 - r2_inet_poly_fv_loss: 1.0835 - mae_inet_poly_fv_loss: 0.3234 - val_loss: 0.3199 - val_r2_inet_lambda_fv_loss: 1.0069 - val_r2_inet_poly_fv_loss: 1.0110 - val_mae_inet_poly_fv_loss: 0.3199\n",
      "Epoch 17/500\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.3231 - r2_inet_lambda_fv_loss: 1.1116 - r2_inet_poly_fv_loss: 1.1171 - mae_inet_poly_fv_loss: 0.3227 - val_loss: 0.3164 - val_r2_inet_lambda_fv_loss: 0.9626 - val_r2_inet_poly_fv_loss: 0.9667 - val_mae_inet_poly_fv_loss: 0.3164\n",
      "Epoch 18/500\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.3244 - r2_inet_lambda_fv_loss: 1.2081 - r2_inet_poly_fv_loss: 1.2152 - mae_inet_poly_fv_loss: 0.3240 - val_loss: 0.3162 - val_r2_inet_lambda_fv_loss: 0.9835 - val_r2_inet_poly_fv_loss: 0.9878 - val_mae_inet_poly_fv_loss: 0.3162\n",
      "Epoch 19/500\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3162 - r2_inet_lambda_fv_loss: 0.9783 - r2_inet_poly_fv_loss: 0.9832 - mae_inet_poly_fv_loss: 0.3158 - val_loss: 0.3122 - val_r2_inet_lambda_fv_loss: 0.9360 - val_r2_inet_poly_fv_loss: 0.9401 - val_mae_inet_poly_fv_loss: 0.3121\n",
      "Epoch 20/500\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3143 - r2_inet_lambda_fv_loss: 1.0460 - r2_inet_poly_fv_loss: 1.0543 - mae_inet_poly_fv_loss: 0.3139 - val_loss: 0.3172 - val_r2_inet_lambda_fv_loss: 1.0056 - val_r2_inet_poly_fv_loss: 1.0104 - val_mae_inet_poly_fv_loss: 0.3173\n",
      "Epoch 21/500\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.3148 - r2_inet_lambda_fv_loss: 1.0099 - r2_inet_poly_fv_loss: 1.0160 - mae_inet_poly_fv_loss: 0.3144 - val_loss: 0.3077 - val_r2_inet_lambda_fv_loss: 0.9049 - val_r2_inet_poly_fv_loss: 0.9091 - val_mae_inet_poly_fv_loss: 0.3076\n",
      "Epoch 22/500\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.3129 - r2_inet_lambda_fv_loss: 1.0038 - r2_inet_poly_fv_loss: 1.0073 - mae_inet_poly_fv_loss: 0.3124 - val_loss: 0.3098 - val_r2_inet_lambda_fv_loss: 0.9035 - val_r2_inet_poly_fv_loss: 0.9076 - val_mae_inet_poly_fv_loss: 0.3098\n",
      "Epoch 23/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.3120 - r2_inet_lambda_fv_loss: 1.0633 - r2_inet_poly_fv_loss: 1.0690 - mae_inet_poly_fv_loss: 0.3116 - val_loss: 0.3107 - val_r2_inet_lambda_fv_loss: 0.9131 - val_r2_inet_poly_fv_loss: 0.9174 - val_mae_inet_poly_fv_loss: 0.3107\n",
      "Epoch 24/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.3130 - r2_inet_lambda_fv_loss: 1.0175 - r2_inet_poly_fv_loss: 1.0230 - mae_inet_poly_fv_loss: 0.3126 - val_loss: 0.3002 - val_r2_inet_lambda_fv_loss: 0.7958 - val_r2_inet_poly_fv_loss: 0.7996 - val_mae_inet_poly_fv_loss: 0.3000\n",
      "Epoch 25/500\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.3060 - r2_inet_lambda_fv_loss: 0.8663 - r2_inet_poly_fv_loss: 0.8700 - mae_inet_poly_fv_loss: 0.3055 - val_loss: 0.2959 - val_r2_inet_lambda_fv_loss: 0.7475 - val_r2_inet_poly_fv_loss: 0.7511 - val_mae_inet_poly_fv_loss: 0.2957\n",
      "Epoch 26/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2990 - r2_inet_lambda_fv_loss: 0.8246 - r2_inet_poly_fv_loss: 0.8319 - mae_inet_poly_fv_loss: 0.2986 - val_loss: 0.2841 - val_r2_inet_lambda_fv_loss: 0.5958 - val_r2_inet_poly_fv_loss: 0.5991 - val_mae_inet_poly_fv_loss: 0.2839\n",
      "Epoch 27/500\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.2936 - r2_inet_lambda_fv_loss: 0.7323 - r2_inet_poly_fv_loss: 0.7366 - mae_inet_poly_fv_loss: 0.2932 - val_loss: 0.2816 - val_r2_inet_lambda_fv_loss: 0.5648 - val_r2_inet_poly_fv_loss: 0.5679 - val_mae_inet_poly_fv_loss: 0.2812\n",
      "Epoch 28/500\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.2915 - r2_inet_lambda_fv_loss: 0.7796 - r2_inet_poly_fv_loss: 0.7855 - mae_inet_poly_fv_loss: 0.2911 - val_loss: 0.2802 - val_r2_inet_lambda_fv_loss: 0.5464 - val_r2_inet_poly_fv_loss: 0.5495 - val_mae_inet_poly_fv_loss: 0.2799\n",
      "Epoch 29/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2910 - r2_inet_lambda_fv_loss: 0.7809 - r2_inet_poly_fv_loss: 0.7851 - mae_inet_poly_fv_loss: 0.2906 - val_loss: 0.2809 - val_r2_inet_lambda_fv_loss: 0.5580 - val_r2_inet_poly_fv_loss: 0.5611 - val_mae_inet_poly_fv_loss: 0.2805\n",
      "Epoch 30/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2867 - r2_inet_lambda_fv_loss: 0.7260 - r2_inet_poly_fv_loss: 0.7303 - mae_inet_poly_fv_loss: 0.2862 - val_loss: 0.2751 - val_r2_inet_lambda_fv_loss: 0.4809 - val_r2_inet_poly_fv_loss: 0.4837 - val_mae_inet_poly_fv_loss: 0.2747\n",
      "Epoch 31/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2853 - r2_inet_lambda_fv_loss: 0.6804 - r2_inet_poly_fv_loss: 0.6849 - mae_inet_poly_fv_loss: 0.2849 - val_loss: 0.2803 - val_r2_inet_lambda_fv_loss: 0.5589 - val_r2_inet_poly_fv_loss: 0.5622 - val_mae_inet_poly_fv_loss: 0.2799\n",
      "Epoch 32/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2909 - r2_inet_lambda_fv_loss: 0.7583 - r2_inet_poly_fv_loss: 0.7623 - mae_inet_poly_fv_loss: 0.2904 - val_loss: 0.2806 - val_r2_inet_lambda_fv_loss: 0.5716 - val_r2_inet_poly_fv_loss: 0.5749 - val_mae_inet_poly_fv_loss: 0.2801\n",
      "Epoch 33/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2902 - r2_inet_lambda_fv_loss: 0.7001 - r2_inet_poly_fv_loss: 0.7039 - mae_inet_poly_fv_loss: 0.2898 - val_loss: 0.2787 - val_r2_inet_lambda_fv_loss: 0.5507 - val_r2_inet_poly_fv_loss: 0.5539 - val_mae_inet_poly_fv_loss: 0.2782\n",
      "Epoch 34/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2888 - r2_inet_lambda_fv_loss: 0.7132 - r2_inet_poly_fv_loss: 0.7171 - mae_inet_poly_fv_loss: 0.2883 - val_loss: 0.2819 - val_r2_inet_lambda_fv_loss: 0.5995 - val_r2_inet_poly_fv_loss: 0.6029 - val_mae_inet_poly_fv_loss: 0.2815\n",
      "Epoch 35/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2886 - r2_inet_lambda_fv_loss: 0.7006 - r2_inet_poly_fv_loss: 0.7045 - mae_inet_poly_fv_loss: 0.2881 - val_loss: 0.2797 - val_r2_inet_lambda_fv_loss: 0.5672 - val_r2_inet_poly_fv_loss: 0.5705 - val_mae_inet_poly_fv_loss: 0.2793\n",
      "Epoch 36/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2877 - r2_inet_lambda_fv_loss: 0.7486 - r2_inet_poly_fv_loss: 0.7536 - mae_inet_poly_fv_loss: 0.2872 - val_loss: 0.2853 - val_r2_inet_lambda_fv_loss: 0.6533 - val_r2_inet_poly_fv_loss: 0.6567 - val_mae_inet_poly_fv_loss: 0.2849\n",
      "Epoch 37/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2917 - r2_inet_lambda_fv_loss: 0.7332 - r2_inet_poly_fv_loss: 0.7373 - mae_inet_poly_fv_loss: 0.2913 - val_loss: 0.2829 - val_r2_inet_lambda_fv_loss: 0.6284 - val_r2_inet_poly_fv_loss: 0.6319 - val_mae_inet_poly_fv_loss: 0.2825\n",
      "Epoch 38/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2892 - r2_inet_lambda_fv_loss: 0.8180 - r2_inet_poly_fv_loss: 0.8225 - mae_inet_poly_fv_loss: 0.2887 - val_loss: 0.2860 - val_r2_inet_lambda_fv_loss: 0.6762 - val_r2_inet_poly_fv_loss: 0.6798 - val_mae_inet_poly_fv_loss: 0.2857\n",
      "Epoch 39/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2926 - r2_inet_lambda_fv_loss: 0.7931 - r2_inet_poly_fv_loss: 0.7974 - mae_inet_poly_fv_loss: 0.2921 - val_loss: 0.2895 - val_r2_inet_lambda_fv_loss: 0.7135 - val_r2_inet_poly_fv_loss: 0.7174 - val_mae_inet_poly_fv_loss: 0.2892\n",
      "Epoch 40/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2961 - r2_inet_lambda_fv_loss: 0.7511 - r2_inet_poly_fv_loss: 0.7548 - mae_inet_poly_fv_loss: 0.2956 - val_loss: 0.2902 - val_r2_inet_lambda_fv_loss: 0.7388 - val_r2_inet_poly_fv_loss: 0.7427 - val_mae_inet_poly_fv_loss: 0.2898\n",
      "Epoch 41/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2922 - r2_inet_lambda_fv_loss: 0.8030 - r2_inet_poly_fv_loss: 0.8105 - mae_inet_poly_fv_loss: 0.2917 - val_loss: 0.2864 - val_r2_inet_lambda_fv_loss: 0.6854 - val_r2_inet_poly_fv_loss: 0.6893 - val_mae_inet_poly_fv_loss: 0.2861\n",
      "Epoch 42/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2911 - r2_inet_lambda_fv_loss: 0.7768 - r2_inet_poly_fv_loss: 0.7820 - mae_inet_poly_fv_loss: 0.2906 - val_loss: 0.2832 - val_r2_inet_lambda_fv_loss: 0.6385 - val_r2_inet_poly_fv_loss: 0.6423 - val_mae_inet_poly_fv_loss: 0.2829\n",
      "Epoch 43/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2854 - r2_inet_lambda_fv_loss: 0.7029 - r2_inet_poly_fv_loss: 0.7096 - mae_inet_poly_fv_loss: 0.2850 - val_loss: 0.2856 - val_r2_inet_lambda_fv_loss: 0.6717 - val_r2_inet_poly_fv_loss: 0.6756 - val_mae_inet_poly_fv_loss: 0.2852\n",
      "Epoch 44/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2885 - r2_inet_lambda_fv_loss: 0.7031 - r2_inet_poly_fv_loss: 0.7059 - mae_inet_poly_fv_loss: 0.2880 - val_loss: 0.2827 - val_r2_inet_lambda_fv_loss: 0.6351 - val_r2_inet_poly_fv_loss: 0.6387 - val_mae_inet_poly_fv_loss: 0.2823\n",
      "Epoch 45/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2848 - r2_inet_lambda_fv_loss: 0.6972 - r2_inet_poly_fv_loss: 0.7009 - mae_inet_poly_fv_loss: 0.2843 - val_loss: 0.2777 - val_r2_inet_lambda_fv_loss: 0.5727 - val_r2_inet_poly_fv_loss: 0.5761 - val_mae_inet_poly_fv_loss: 0.2773\n",
      "Epoch 46/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2830 - r2_inet_lambda_fv_loss: 0.6254 - r2_inet_poly_fv_loss: 0.6286 - mae_inet_poly_fv_loss: 0.2826 - val_loss: 0.2847 - val_r2_inet_lambda_fv_loss: 0.6642 - val_r2_inet_poly_fv_loss: 0.6679 - val_mae_inet_poly_fv_loss: 0.2845\n",
      "Epoch 47/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2887 - r2_inet_lambda_fv_loss: 0.7498 - r2_inet_poly_fv_loss: 0.7537 - mae_inet_poly_fv_loss: 0.2882 - val_loss: 0.2895 - val_r2_inet_lambda_fv_loss: 0.7115 - val_r2_inet_poly_fv_loss: 0.7153 - val_mae_inet_poly_fv_loss: 0.2892\n",
      "Epoch 48/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2944 - r2_inet_lambda_fv_loss: 0.8417 - r2_inet_poly_fv_loss: 0.8461 - mae_inet_poly_fv_loss: 0.2940 - val_loss: 0.2902 - val_r2_inet_lambda_fv_loss: 0.7217 - val_r2_inet_poly_fv_loss: 0.7255 - val_mae_inet_poly_fv_loss: 0.2899\n",
      "Epoch 49/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2918 - r2_inet_lambda_fv_loss: 0.8483 - r2_inet_poly_fv_loss: 0.8532 - mae_inet_poly_fv_loss: 0.2914 - val_loss: 0.2931 - val_r2_inet_lambda_fv_loss: 0.7546 - val_r2_inet_poly_fv_loss: 0.7585 - val_mae_inet_poly_fv_loss: 0.2928\n",
      "Epoch 50/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2934 - r2_inet_lambda_fv_loss: 0.7516 - r2_inet_poly_fv_loss: 0.7554 - mae_inet_poly_fv_loss: 0.2930 - val_loss: 0.2845 - val_r2_inet_lambda_fv_loss: 0.6401 - val_r2_inet_poly_fv_loss: 0.6435 - val_mae_inet_poly_fv_loss: 0.2842\n",
      "Epoch 51/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2895 - r2_inet_lambda_fv_loss: 0.7195 - r2_inet_poly_fv_loss: 0.7237 - mae_inet_poly_fv_loss: 0.2890 - val_loss: 0.2859 - val_r2_inet_lambda_fv_loss: 0.6660 - val_r2_inet_poly_fv_loss: 0.6697 - val_mae_inet_poly_fv_loss: 0.2856\n",
      "Epoch 52/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2918 - r2_inet_lambda_fv_loss: 0.7824 - r2_inet_poly_fv_loss: 0.7869 - mae_inet_poly_fv_loss: 0.2914 - val_loss: 0.2896 - val_r2_inet_lambda_fv_loss: 0.7071 - val_r2_inet_poly_fv_loss: 0.7108 - val_mae_inet_poly_fv_loss: 0.2893\n",
      "Epoch 53/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2941 - r2_inet_lambda_fv_loss: 0.8484 - r2_inet_poly_fv_loss: 0.8523 - mae_inet_poly_fv_loss: 0.2937 - val_loss: 0.2923 - val_r2_inet_lambda_fv_loss: 0.7468 - val_r2_inet_poly_fv_loss: 0.7508 - val_mae_inet_poly_fv_loss: 0.2921\n",
      "Epoch 54/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2891 - r2_inet_lambda_fv_loss: 0.7009 - r2_inet_poly_fv_loss: 0.7048 - mae_inet_poly_fv_loss: 0.2887 - val_loss: 0.2886 - val_r2_inet_lambda_fv_loss: 0.7043 - val_r2_inet_poly_fv_loss: 0.7081 - val_mae_inet_poly_fv_loss: 0.2883\n",
      "Epoch 55/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2872 - r2_inet_lambda_fv_loss: 0.7442 - r2_inet_poly_fv_loss: 0.7481 - mae_inet_poly_fv_loss: 0.2868 - val_loss: 0.2825 - val_r2_inet_lambda_fv_loss: 0.6275 - val_r2_inet_poly_fv_loss: 0.6313 - val_mae_inet_poly_fv_loss: 0.2822\n",
      "Epoch 56/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2839 - r2_inet_lambda_fv_loss: 0.7012 - r2_inet_poly_fv_loss: 0.7070 - mae_inet_poly_fv_loss: 0.2834 - val_loss: 0.2781 - val_r2_inet_lambda_fv_loss: 0.5681 - val_r2_inet_poly_fv_loss: 0.5714 - val_mae_inet_poly_fv_loss: 0.2778\n",
      "Epoch 57/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2836 - r2_inet_lambda_fv_loss: 0.6599 - r2_inet_poly_fv_loss: 0.6633 - mae_inet_poly_fv_loss: 0.2832 - val_loss: 0.2809 - val_r2_inet_lambda_fv_loss: 0.6131 - val_r2_inet_poly_fv_loss: 0.6166 - val_mae_inet_poly_fv_loss: 0.2806\n",
      "Epoch 58/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2861 - r2_inet_lambda_fv_loss: 0.7774 - r2_inet_poly_fv_loss: 0.7834 - mae_inet_poly_fv_loss: 0.2857 - val_loss: 0.2870 - val_r2_inet_lambda_fv_loss: 0.6864 - val_r2_inet_poly_fv_loss: 0.6903 - val_mae_inet_poly_fv_loss: 0.2868\n",
      "Epoch 59/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2860 - r2_inet_lambda_fv_loss: 0.7599 - r2_inet_poly_fv_loss: 0.7672 - mae_inet_poly_fv_loss: 0.2856 - val_loss: 0.2920 - val_r2_inet_lambda_fv_loss: 0.7257 - val_r2_inet_poly_fv_loss: 0.7296 - val_mae_inet_poly_fv_loss: 0.2917\n",
      "Epoch 60/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2911 - r2_inet_lambda_fv_loss: 0.7818 - r2_inet_poly_fv_loss: 0.7883 - mae_inet_poly_fv_loss: 0.2907 - val_loss: 0.2910 - val_r2_inet_lambda_fv_loss: 0.7156 - val_r2_inet_poly_fv_loss: 0.7193 - val_mae_inet_poly_fv_loss: 0.2907\n",
      "Epoch 61/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2932 - r2_inet_lambda_fv_loss: 0.8776 - r2_inet_poly_fv_loss: 0.8842 - mae_inet_poly_fv_loss: 0.2928 - val_loss: 0.2871 - val_r2_inet_lambda_fv_loss: 0.6792 - val_r2_inet_poly_fv_loss: 0.6829 - val_mae_inet_poly_fv_loss: 0.2867\n",
      "Epoch 62/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2883 - r2_inet_lambda_fv_loss: 0.7728 - r2_inet_poly_fv_loss: 0.7774 - mae_inet_poly_fv_loss: 0.2878 - val_loss: 0.2858 - val_r2_inet_lambda_fv_loss: 0.6658 - val_r2_inet_poly_fv_loss: 0.6694 - val_mae_inet_poly_fv_loss: 0.2855\n",
      "Epoch 63/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2901 - r2_inet_lambda_fv_loss: 0.7166 - r2_inet_poly_fv_loss: 0.7209 - mae_inet_poly_fv_loss: 0.2897 - val_loss: 0.2870 - val_r2_inet_lambda_fv_loss: 0.6808 - val_r2_inet_poly_fv_loss: 0.6843 - val_mae_inet_poly_fv_loss: 0.2866\n",
      "Epoch 64/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2878 - r2_inet_lambda_fv_loss: 0.7199 - r2_inet_poly_fv_loss: 0.7232 - mae_inet_poly_fv_loss: 0.2874 - val_loss: 0.2879 - val_r2_inet_lambda_fv_loss: 0.6880 - val_r2_inet_poly_fv_loss: 0.6916 - val_mae_inet_poly_fv_loss: 0.2876\n",
      "Epoch 65/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2904 - r2_inet_lambda_fv_loss: 0.8019 - r2_inet_poly_fv_loss: 0.8062 - mae_inet_poly_fv_loss: 0.2900 - val_loss: 0.2884 - val_r2_inet_lambda_fv_loss: 0.6899 - val_r2_inet_poly_fv_loss: 0.6936 - val_mae_inet_poly_fv_loss: 0.2880\n",
      "Epoch 66/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2895 - r2_inet_lambda_fv_loss: 0.7312 - r2_inet_poly_fv_loss: 0.7366 - mae_inet_poly_fv_loss: 0.2890 - val_loss: 0.2919 - val_r2_inet_lambda_fv_loss: 0.7451 - val_r2_inet_poly_fv_loss: 0.7489 - val_mae_inet_poly_fv_loss: 0.2915\n",
      "Epoch 67/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2908 - r2_inet_lambda_fv_loss: 0.8084 - r2_inet_poly_fv_loss: 0.8129 - mae_inet_poly_fv_loss: 0.2904 - val_loss: 0.2922 - val_r2_inet_lambda_fv_loss: 0.7501 - val_r2_inet_poly_fv_loss: 0.7538 - val_mae_inet_poly_fv_loss: 0.2919\n",
      "Epoch 68/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2889 - r2_inet_lambda_fv_loss: 0.6920 - r2_inet_poly_fv_loss: 0.6949 - mae_inet_poly_fv_loss: 0.2884 - val_loss: 0.2814 - val_r2_inet_lambda_fv_loss: 0.5884 - val_r2_inet_poly_fv_loss: 0.5915 - val_mae_inet_poly_fv_loss: 0.2810\n",
      "Epoch 69/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2815 - r2_inet_lambda_fv_loss: 0.7169 - r2_inet_poly_fv_loss: 0.7214 - mae_inet_poly_fv_loss: 0.2810 - val_loss: 0.2811 - val_r2_inet_lambda_fv_loss: 0.6082 - val_r2_inet_poly_fv_loss: 0.6114 - val_mae_inet_poly_fv_loss: 0.2807\n",
      "Epoch 70/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2843 - r2_inet_lambda_fv_loss: 0.6879 - r2_inet_poly_fv_loss: 0.6917 - mae_inet_poly_fv_loss: 0.2838 - val_loss: 0.2882 - val_r2_inet_lambda_fv_loss: 0.6905 - val_r2_inet_poly_fv_loss: 0.6940 - val_mae_inet_poly_fv_loss: 0.2879\n",
      "Epoch 71/500\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.2874 - r2_inet_lambda_fv_loss: 0.7239 - r2_inet_poly_fv_loss: 0.7281 - mae_inet_poly_fv_loss: 0.2869 - val_loss: 0.2888 - val_r2_inet_lambda_fv_loss: 0.7064 - val_r2_inet_poly_fv_loss: 0.7101 - val_mae_inet_poly_fv_loss: 0.2886\n",
      "Epoch 72/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2867 - r2_inet_lambda_fv_loss: 0.7738 - r2_inet_poly_fv_loss: 0.7780 - mae_inet_poly_fv_loss: 0.2862 - val_loss: 0.2901 - val_r2_inet_lambda_fv_loss: 0.7175 - val_r2_inet_poly_fv_loss: 0.7212 - val_mae_inet_poly_fv_loss: 0.2897\n",
      "Epoch 73/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2904 - r2_inet_lambda_fv_loss: 0.7803 - r2_inet_poly_fv_loss: 0.7856 - mae_inet_poly_fv_loss: 0.2899 - val_loss: 0.2944 - val_r2_inet_lambda_fv_loss: 0.7921 - val_r2_inet_poly_fv_loss: 0.7964 - val_mae_inet_poly_fv_loss: 0.2940\n",
      "Epoch 74/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2947 - r2_inet_lambda_fv_loss: 0.8364 - r2_inet_poly_fv_loss: 0.8411 - mae_inet_poly_fv_loss: 0.2943 - val_loss: 0.2916 - val_r2_inet_lambda_fv_loss: 0.7288 - val_r2_inet_poly_fv_loss: 0.7324 - val_mae_inet_poly_fv_loss: 0.2913\n",
      "Epoch 75/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2920 - r2_inet_lambda_fv_loss: 0.7895 - r2_inet_poly_fv_loss: 0.7943 - mae_inet_poly_fv_loss: 0.2916 - val_loss: 0.2952 - val_r2_inet_lambda_fv_loss: 0.7962 - val_r2_inet_poly_fv_loss: 0.8003 - val_mae_inet_poly_fv_loss: 0.2948\n",
      "Epoch 76/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2926 - r2_inet_lambda_fv_loss: 0.8362 - r2_inet_poly_fv_loss: 0.8425 - mae_inet_poly_fv_loss: 0.2921 - val_loss: 0.2933 - val_r2_inet_lambda_fv_loss: 0.7696 - val_r2_inet_poly_fv_loss: 0.7732 - val_mae_inet_poly_fv_loss: 0.2930\n",
      "Epoch 77/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2928 - r2_inet_lambda_fv_loss: 0.8219 - r2_inet_poly_fv_loss: 0.8265 - mae_inet_poly_fv_loss: 0.2924 - val_loss: 0.2918 - val_r2_inet_lambda_fv_loss: 0.7571 - val_r2_inet_poly_fv_loss: 0.7608 - val_mae_inet_poly_fv_loss: 0.2914\n",
      "Epoch 78/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2901 - r2_inet_lambda_fv_loss: 0.7758 - r2_inet_poly_fv_loss: 0.7802 - mae_inet_poly_fv_loss: 0.2896 - val_loss: 0.2933 - val_r2_inet_lambda_fv_loss: 0.7979 - val_r2_inet_poly_fv_loss: 0.8021 - val_mae_inet_poly_fv_loss: 0.2930\n",
      "Epoch 79/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2941 - r2_inet_lambda_fv_loss: 0.8701 - r2_inet_poly_fv_loss: 0.8753 - mae_inet_poly_fv_loss: 0.2936 - val_loss: 0.2872 - val_r2_inet_lambda_fv_loss: 0.7031 - val_r2_inet_poly_fv_loss: 0.7066 - val_mae_inet_poly_fv_loss: 0.2869\n",
      "Epoch 80/500\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2889 - r2_inet_lambda_fv_loss: 0.7962 - r2_inet_poly_fv_loss: 0.8011 - mae_inet_poly_fv_loss: 0.2884 - val_loss: 0.2948 - val_r2_inet_lambda_fv_loss: 0.8119 - val_r2_inet_poly_fv_loss: 0.8160 - val_mae_inet_poly_fv_loss: 0.2944\n",
      "Training Time: 1:36:40\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:01\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------- PREDICT INET ------------------------------------------------------\n",
      "Predict Time: 0:00:00\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=10)]: Done   2 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=10)]: Done   3 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=10)]: Done   4 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=10)]: Done   6 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=10)]: Done   7 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=10)]: Done   8 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=10)]: Done   9 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=10)]: Done  10 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=10)]: Done  11 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=10)]: Done  13 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=10)]: Done  14 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=10)]: Done  15 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=10)]: Done  16 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=10)]: Done  17 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=10)]: Done  18 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=10)]: Done  19 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=10)]: Done  20 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=10)]: Done  22 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=10)]: Done  23 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=10)]: Done  24 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=10)]: Done  25 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=10)]: Done  26 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=10)]: Done  27 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=10)]: Done  28 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=10)]: Done  29 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=10)]: Done  31 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=10)]: Done  32 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=10)]: Done  33 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=10)]: Done  34 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=10)]: Done  35 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=10)]: Done  36 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=10)]: Done  37 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=10)]: Done  38 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=10)]: Done  39 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=10)]: Done  40 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=10)]: Done  42 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=10)]: Done  43 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=10)]: Done  44 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=10)]: Done  45 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=10)]: Done  46 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=10)]: Done  47 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=10)]: Done  48 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=10)]: Done  49 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=10)]: Done  50 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=10)]: Done  51 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=10)]: Done  53 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=10)]: Done  54 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=10)]: Done  55 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=10)]: Done  56 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=10)]: Done  57 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=10)]: Done  58 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=10)]: Done  59 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=10)]: Done  60 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=10)]: Done  61 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=10)]: Done  62 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=10)]: Done  63 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=10)]: Done  64 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=10)]: Done  66 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=10)]: Done  67 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=10)]: Done  68 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=10)]: Done  69 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=10)]: Done  70 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=10)]: Done  71 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=10)]: Done  72 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=10)]: Done  73 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=10)]: Done  74 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=10)]: Done  75 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=10)]: Done  76 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=10)]: Done  77 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=10)]: Done  79 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=10)]: Done  80 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=10)]: Done  81 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=10)]: Done  91 out of 100 | elapsed: 17.0min remaining:  1.7min\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed: 18.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Regression Optimization Time: 0:18:01\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n\n    TypeError: tf__eval_multinomial() takes from 1 to 3 positional arguments but 5 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7ded959402c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdistrib_dict_test_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_interpretation_net_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_train_dataset_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                                                    \u001b[0mlambda_net_valid_dataset_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                    lambda_net_test_dataset_list)\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/InterpretationNet.py\u001b[0m in \u001b[0;36mcalculate_interpretation_net_results\u001b[0;34m(lambda_net_train_dataset_list, lambda_net_valid_dataset_list, lambda_net_test_dataset_list)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m     \u001b[0mfunction_values_test_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlambda_net_test_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_test_dataset_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/InterpretationNet.py\u001b[0m in \u001b[0;36mcalculate_all_function_values\u001b[0;34m(lambda_net_dataset, polynomial_dict)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threading'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m     function_value_dict = {\n\u001b[1;32m   1119\u001b[0m         \u001b[0;34m'lambda_preds'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_prediction_on_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/LambdaNet.py\u001b[0m in \u001b[0;36mreturn_target_poly_fvs_on_test_data\u001b[0;34m(self, n_jobs_parallel_fv, backend)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtarget_poly_fvs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_fv_calculation_from_polynomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_polynomial_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevaluation_dataset\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_polynomial_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_complete_poly_representation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs_parallel_fv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threading'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_poly_fvs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreturn_target_poly_fvs_on_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs_parallel_fv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threading'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/utility_functions.py\u001b[0m in \u001b[0;36mparallel_fv_calculation_from_polynomial\u001b[0;34m(polynomial_list, lambda_input_list, force_complete_poly_representation, n_jobs_parallel_fv, backend)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;31m#print(polynomial_list.dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;31m#print(polynomial_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m     \u001b[0;31m#print(polynomial_list[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m     \u001b[0;31m#print(type(polynomial_list[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;31m#print(polynomial_list[0].dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;31m# We capture the KeyboardInterrupt and reraise it as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    628\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m           \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: tf__eval_multinomial() takes from 1 to 3 positional arguments but 5 were given\n"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "(history_list, \n",
    "\n",
    "#scores_valid_list,\n",
    "scores_test_list, \n",
    "\n",
    "#function_values_valid_list, \n",
    "function_values_test_list, \n",
    "\n",
    "#polynomial_dict_valid_list,\n",
    "polynomial_dict_test_list,\n",
    "\n",
    "#distrib_dict_valid_list,\n",
    "distrib_dict_test_list,\n",
    "\n",
    "model_list) = calculate_interpretation_net_results(lambda_net_train_dataset_list, \n",
    "                                                   lambda_net_valid_dataset_list, \n",
    "                                                   lambda_net_test_dataset_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_true_monomial_list = tf.constant([0,2,2,1,2,1, 0,2,2,1,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack(tf.split(polynomial_true_monomial_list, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack(tf.split(tf.stack(tf.split(polynomial_true_monomial_list, 2)), 3, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_true_monomial_list = tf.map_fn(fn=lambda x: tf.sparse.to_dense(tf.SparseTensor([[x]], [1], [d+1])), elems=polynomial_true_monomial_list, fn_output_signature=tf.TensorSpec([d+1], dtype=tf.int32))\n",
    "polynomial_true_monomial_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=tf.sparse.to_dense(tf.SparseTensor([[1]], [1], [d+1]))\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_dict_test_list[-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if evaluate_with_real_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
