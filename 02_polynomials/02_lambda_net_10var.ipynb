{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:15:26.281299Z",
     "iopub.status.busy": "2021-10-18T08:15:26.281017Z",
     "iopub.status.idle": "2021-10-18T08:15:26.303137Z",
     "shell.execute_reply": "2021-10-18T08:15:26.301711Z",
     "shell.execute_reply.started": "2021-10-18T08:15:26.281233Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 3, #degree\n",
    "        'n': 10, #number of variables\n",
    "        'monomial_vars': None, #int or None\n",
    "        'laurent': False, #use Laurent polynomials (negative degree with up to -d)\n",
    "        'neg_d': 0,#int or None\n",
    "        'neg_d_prob': 0,\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': 10,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 1,\n",
    "        'a_min': -1,\n",
    "        'polynomial_data_size': 50000,  #number of generated polynomials (for loading)\n",
    "        'lambda_nets_total': 50000, #number of lambda-nets to train\n",
    "        'noise': 0,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'shift_polynomial': False,\n",
    "        \n",
    "        'border_min': 0.2, # defines an intervall. Value is randomly chosen and defines the minimum gap between x_min / x_max and the outermost stationary points => two values (left and right gap will be generated per variable)\n",
    "        'border_max': 0.4,\n",
    "        'lower_degree_prob': 0.5, # probability that the degree of the whole polynomial will be reduced\n",
    "        'a_random_prob': 0.5, # probability that a random generated function is used without adjustement\n",
    "                \n",
    "        'global_stationary_prob': 1, # probability that all variables are used for adjustement (0 recommended for higher number of variables)\n",
    "        'bulge_min': 1, # bulge_min and bulge_max define an intervall of how much the function is bulged\n",
    "        'bulge_max': 4,\n",
    "        'min_variables_used': 2, # defines an Intervall of how many variables are used to get stationary points and therefore adjust the function\n",
    "        'max_variables_used': 6,\n",
    "        'max_monomials': 7, # maximum number of monomials, before adjusting the function (monomial of degree 0 is always defined, but is included in this number)\n",
    "        'max_monomials_random': 10, #maximum number of monomials for random generated functions \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sample_sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000, #lambda-net training dataset size\n",
    "    },    \n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': False,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "    \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "    },    \n",
    "    'computation':{\n",
    "        'n_jobs': 16,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:15:26.313486Z",
     "iopub.status.busy": "2021-10-18T08:15:26.313241Z",
     "iopub.status.idle": "2021-10-18T08:15:26.317714Z",
     "shell.execute_reply": "2021-10-18T08:15:26.317070Z",
     "shell.execute_reply.started": "2021-10-18T08:15:26.313461Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:15:26.319217Z",
     "iopub.status.busy": "2021-10-18T08:15:26.318917Z",
     "iopub.status.idle": "2021-10-18T08:15:31.103380Z",
     "shell.execute_reply": "2021-10-18T08:15:31.102156Z",
     "shell.execute_reply.started": "2021-10-18T08:15:26.319193Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('WARNING')\n",
    "tf.autograph.set_verbosity(2)\n",
    "\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:15:31.105483Z",
     "iopub.status.busy": "2021-10-18T08:15:31.105195Z",
     "iopub.status.idle": "2021-10-18T08:15:31.114189Z",
     "shell.execute_reply": "2021-10-18T08:15:31.113558Z",
     "shell.execute_reply.started": "2021-10-18T08:15:31.105455Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n] \n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "    \n",
    "    \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:15:31.115388Z",
     "iopub.status.busy": "2021-10-18T08:15:31.115137Z",
     "iopub.status.idle": "2021-10-18T08:15:32.358302Z",
     "shell.execute_reply": "2021-10-18T08:15:32.357674Z",
     "shell.execute_reply.started": "2021-10-18T08:15:31.115350Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 56\n",
      "[[3, 0, 0, 0, 0], [2, 1, 0, 0, 0], [2, 0, 1, 0, 0], [2, 0, 0, 1, 0], [2, 0, 0, 0, 1], [2, 0, 0, 0, 0], [1, 2, 0, 0, 0], [1, 1, 1, 0, 0], [1, 1, 0, 1, 0], [1, 1, 0, 0, 1], [1, 1, 0, 0, 0], [1, 0, 2, 0, 0], [1, 0, 1, 1, 0], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 0, 0, 2, 0], [1, 0, 0, 1, 1], [1, 0, 0, 1, 0], [1, 0, 0, 0, 2], [1, 0, 0, 0, 1], [1, 0, 0, 0, 0], [0, 3, 0, 0, 0], [0, 2, 1, 0, 0], [0, 2, 0, 1, 0], [0, 2, 0, 0, 1], [0, 2, 0, 0, 0], [0, 1, 2, 0, 0], [0, 1, 1, 1, 0], [0, 1, 1, 0, 1], [0, 1, 1, 0, 0], [0, 1, 0, 2, 0], [0, 1, 0, 1, 1], [0, 1, 0, 1, 0], [0, 1, 0, 0, 2], [0, 1, 0, 0, 1], [0, 1, 0, 0, 0], [0, 0, 3, 0, 0], [0, 0, 2, 1, 0], [0, 0, 2, 0, 1], [0, 0, 2, 0, 0], [0, 0, 1, 2, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 0], [0, 0, 1, 0, 2], [0, 0, 1, 0, 1], [0, 0, 1, 0, 0], [0, 0, 0, 3, 0], [0, 0, 0, 2, 1], [0, 0, 0, 2, 0], [0, 0, 0, 1, 2], [0, 0, 0, 1, 1], [0, 0, 0, 1, 0], [0, 0, 0, 0, 3], [0, 0, 0, 0, 2], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import flatten, rec_gen, gen_monomial_identifier_list\n",
    "\n",
    "list_of_monomial_identifiers_extended = []\n",
    "\n",
    "if laurent:\n",
    "    variable_sets = [list(flatten([[_d for _d in range(d+1)], [-_d for _d in range(1, neg_d+1)]])) for _ in range(n)]\n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)    \n",
    "        \n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity:' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)     \n",
    "        \n",
    "    list_of_monomial_identifiers = []\n",
    "    for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "        if np.sum(monomial_identifier) <= d:\n",
    "            if monomial_vars == None or len(list(filter(lambda x: x != 0, monomial_identifier))) <= monomial_vars:\n",
    "                list_of_monomial_identifiers.append(monomial_identifier)        \n",
    "else:\n",
    "    variable_list = ['x'+ str(i) for i in range(n)]\n",
    "    list_of_monomial_identifiers = gen_monomial_identifier_list(variable_list, d, n)\n",
    "            \n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "#print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "#print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:15:32.360555Z",
     "iopub.status.busy": "2021-10-18T08:15:32.360350Z",
     "iopub.status.idle": "2021-10-18T08:15:32.383094Z",
     "shell.execute_reply": "2021-10-18T08:15:32.382458Z",
     "shell.execute_reply.started": "2021-10-18T08:15:32.360529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'KeyError'>\n",
      "<class 'KeyError'>\n",
      "<class 'KeyError'>\n"
     ]
    }
   ],
   "source": [
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d']) if not laurent else len(list_of_monomial_identifiers)\n",
    "\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "    \n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sample_sparsity')*config['data']['sample_sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='lambda_net'))\n",
    "generate_directory_structure()\n",
    "generate_lambda_net_directory()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:15:32.384272Z",
     "iopub.status.busy": "2021-10-18T08:15:32.384082Z",
     "iopub.status.idle": "2021-10-18T08:15:32.387887Z",
     "shell.execute_reply": "2021-10-18T08:15:32.387305Z",
     "shell.execute_reply.started": "2021-10-18T08:15:32.384248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lnets_50000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_3_negd_0_prob_0_spars_5_amin_-1_amax_1_xdist_uniform_noise_normal_0\n",
      "poly_50000_train_5000_var_5_d_3_negd_0_prob_0_spars_5_amin_-1_amax_1_xdist_uniform_noise_normal_0_diffX\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_lambda_net_data)\n",
    "\n",
    "print(path_identifier_polynomial_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:15:32.388905Z",
     "iopub.status.busy": "2021-10-18T08:15:32.388675Z",
     "iopub.status.idle": "2021-10-18T08:15:32.395801Z",
     "shell.execute_reply": "2021-10-18T08:15:32.395231Z",
     "shell.execute_reply.started": "2021-10-18T08:15:32.388880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T08:15:32.396777Z",
     "iopub.status.busy": "2021-10-18T08:15:32.396593Z",
     "iopub.status.idle": "2021-10-18T08:15:32.405493Z",
     "shell.execute_reply": "2021-10-18T08:15:32.404942Z",
     "shell.execute_reply.started": "2021-10-18T08:15:32.396754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_network_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:11.182937Z",
     "start_time": "2021-01-17T09:44:31.797522Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-18T08:15:32.406537Z",
     "iopub.status.busy": "2021-10-18T08:15:32.406314Z",
     "iopub.status.idle": "2021-10-18T08:17:58.729957Z",
     "shell.execute_reply": "2021-10-18T08:17:58.729049Z",
     "shell.execute_reply.started": "2021-10-18T08:15:32.406513Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample_' + path_identifier_polynomial_data + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if lambda_nets_total < polynomial_data_size:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=lambda_nets_total, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, lambda_nets_total)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, lambda_nets_total)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:12.626401Z",
     "start_time": "2021-01-17T09:46:12.608200Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-18T08:17:58.751490Z",
     "iopub.status.busy": "2021-10-18T08:17:58.751138Z",
     "iopub.status.idle": "2021-10-18T08:17:58.771223Z",
     "shell.execute_reply": "2021-10-18T08:17:58.770614Z",
     "shell.execute_reply.started": "2021-10-18T08:17:58.751460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.646</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.792</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.979</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.640</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.265</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.618</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.360</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.671</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a     b     c     d     e\n",
       "0 0.549 0.715 0.603 0.545 0.424\n",
       "1 0.646 0.438 0.892 0.964 0.383\n",
       "2 0.792 0.529 0.568 0.926 0.071\n",
       "3 0.087 0.020 0.833 0.778 0.870\n",
       "4 0.979 0.799 0.461 0.781 0.118\n",
       "5 0.640 0.143 0.945 0.522 0.415\n",
       "6 0.265 0.774 0.456 0.568 0.019\n",
       "7 0.618 0.612 0.617 0.944 0.682\n",
       "8 0.360 0.437 0.698 0.060 0.667\n",
       "9 0.671 0.210 0.129 0.315 0.364"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:12.636995Z",
     "start_time": "2021-01-17T09:46:12.629349Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-18T08:17:58.772452Z",
     "iopub.status.busy": "2021-10-18T08:17:58.772260Z",
     "iopub.status.idle": "2021-10-18T08:17:58.779344Z",
     "shell.execute_reply": "2021-10-18T08:17:58.778718Z",
     "shell.execute_reply.started": "2021-10-18T08:17:58.772427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   result\n",
       "0   0.595\n",
       "1   0.442\n",
       "2   0.358\n",
       "3   0.418\n",
       "4   0.560\n",
       "5   0.815\n",
       "6   0.301\n",
       "7   0.387\n",
       "8   0.584\n",
       "9   0.641"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.853Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-18T08:17:58.780486Z",
     "iopub.status.busy": "2021-10-18T08:17:58.780250Z",
     "iopub.status.idle": "2021-10-18T08:17:58.786574Z",
     "shell.execute_reply": "2021-10-18T08:17:58.785978Z",
     "shell.execute_reply.started": "2021-10-18T08:17:58.780461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000   0.000\n",
       "21000   0.000\n",
       "20100   0.000\n",
       "20010   0.000\n",
       "20001   0.000\n",
       "20000   0.000\n",
       "12000   0.000\n",
       "11100   0.000\n",
       "11010   0.000\n",
       "11001   0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.855Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-18T08:17:58.787728Z",
     "iopub.status.busy": "2021-10-18T08:17:58.787488Z",
     "iopub.status.idle": "2021-10-18T08:17:58.793503Z",
     "shell.execute_reply": "2021-10-18T08:17:58.792892Z",
     "shell.execute_reply.started": "2021-10-18T08:17:58.787703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000   0.000\n",
       "21000   0.000\n",
       "20100   0.000\n",
       "20010   0.000\n",
       "20001   0.000\n",
       "20000   0.000\n",
       "12000   0.000\n",
       "11100   0.000\n",
       "11010   0.000\n",
       "11001   0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.863Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-18T08:17:58.796478Z",
     "iopub.status.busy": "2021-10-18T08:17:58.796067Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4374c60f1e4c4ed89300a2c320b9bc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend MultiprocessingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=16)]: Done 480 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed: 27.7min\n",
      "[Parallel(n_jobs=16)]: Done 1120 tasks      | elapsed: 39.9min\n",
      "[Parallel(n_jobs=16)]: Done 1536 tasks      | elapsed: 54.5min\n",
      "[Parallel(n_jobs=16)]: Done 2016 tasks      | elapsed: 72.2min\n",
      "[Parallel(n_jobs=16)]: Done 2560 tasks      | elapsed: 91.4min\n",
      "[Parallel(n_jobs=16)]: Done 3168 tasks      | elapsed: 112.8min\n",
      "[Parallel(n_jobs=16)]: Done 3840 tasks      | elapsed: 136.2min\n",
      "[Parallel(n_jobs=16)]: Done 4576 tasks      | elapsed: 161.9min\n",
      "[Parallel(n_jobs=16)]: Done 5000 out of 5000 | elapsed: 176.5min finished\n",
      "[Parallel(n_jobs=16)]: Using backend MultiprocessingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=16)]: Done 256 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=16)]: Done 480 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed: 27.4min\n",
      "[Parallel(n_jobs=16)]: Done 1120 tasks      | elapsed: 40.0min\n",
      "[Parallel(n_jobs=16)]: Done 1536 tasks      | elapsed: 54.5min\n",
      "[Parallel(n_jobs=16)]: Done 2016 tasks      | elapsed: 71.2min\n"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "clf_list = []\n",
    "chunksize = 5000 if lambda_nets_total > 50000 else max(lambda_nets_total//10, min(50, lambda_nets_total))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "max_seed = 2147483646\n",
    "seed_list = random.sample(range(0, max_seed), number_different_lambda_trainings)\n",
    "chunk_multiplier = 0\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    clf_sublist = parallel(delayed(train_nn)(chunksize*chunk_multiplier+index, X_data[1].values, y_data[1].values, X_data[0], seed_list, return_history=True, each_epochs_save=each_epochs_save_lambda, printing=True) for index, (X_data, y_data) in enumerate(zip(X_data_list_split, y_data_list_split)))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "    chunk_multiplier +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "rand_index = np.random.randint(lambda_nets_total)\n",
    "\n",
    "random_network = train_nn(rand_index, X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], seed_list, callbacks=[PlotLossesKerasTF()], return_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Plot Lambda-Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.874Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "index_list = []\n",
    "\n",
    "\n",
    "max_training_epochs = 0\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    history = entry[3]\n",
    "    \n",
    "    current_training_epochs = len(history[list(history.keys())[0]])\n",
    "    max_training_epochs = max(max_training_epochs, current_training_epochs)\n",
    "\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    history = entry[3]\n",
    "    index = entry[0][0]\n",
    "    \n",
    "    current_training_epochs = len(history[list(history.keys())[0]])\n",
    "    \n",
    "    loss_list = np.full(max_training_epochs, np.nan)\n",
    "    metric_list = np.full(max_training_epochs, np.nan)\n",
    "    val_loss_list = np.full(max_training_epochs, np.nan)\n",
    "    val_metric_list = np.full(max_training_epochs, np.nan) \n",
    "\n",
    "    for i in range(current_training_epochs):  \n",
    "        loss_list[i] = history[list(history.keys())[0]][i]\n",
    "        metric_list[i] = history[list(history.keys())[1]][i]\n",
    "        val_loss_list[i] = history[list(history.keys())[len(history.keys())//2]][i]\n",
    "        val_metric_list[i] = history[list(history.keys())[len(history.keys())//2+1]][i]\n",
    "    \n",
    "    index_list.append([index])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=np.hstack([index_list, loss_list_total]), columns=list(flatten(['index', [list(history.keys())[0] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]])))\n",
    "#loss_df['index'] = loss_df['index'].astype(int)\n",
    "metric_df = pd.DataFrame(data=np.hstack([index_list, metric_list_total]), columns=list(flatten(['index', [list(history.keys())[1] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]]))) \n",
    "#metric_df['index'] = metric_df['index'].astype(int)\n",
    "val_loss_df = pd.DataFrame(data=np.hstack([index_list, val_loss_list_total]), columns=list(flatten(['index', [list(history.keys())[len(history.keys())//2] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]])))\n",
    "#val_loss_df['index'] = val_loss_df['index'].astype(int)\n",
    "val_metric_df = pd.DataFrame(data=np.hstack([index_list, val_metric_list_total]), columns=list(flatten(['index', [list(history.keys())[len(history.keys())//2+1] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]]))) \n",
    "#val_metric_df['index'] = val_metric_df['index'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.875Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[0] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "path_metric = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[1] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[len(history.keys())//2] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[len(history.keys())//2+1] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.876Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.877Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.879Z"
    }
   },
   "outputs": [],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.880Z"
    }
   },
   "outputs": [],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.881Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + path_identifier_lambda_net_data + '/' + list(history.keys())[1] + '_epoch_' + str(epochs_lambda).zfill(3) + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 10#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model ' + list(history.keys())[1])\n",
    "plt.ylabel(list(history.keys())[1])\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.883Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + path_identifier_lambda_net_data + '/' + list(history.keys())[0] + '_epoch_' + str(epochs_lambda).zfill(3) + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True).iloc[:,1:]\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model ' + list(history.keys())[0])\n",
    "plt.ylabel(list(history.keys())[0])\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
