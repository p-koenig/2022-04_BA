{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:23.008085Z",
     "iopub.status.busy": "2021-10-20T16:36:23.007550Z",
     "iopub.status.idle": "2021-10-20T16:36:23.034013Z",
     "shell.execute_reply": "2021-10-20T16:36:23.032894Z",
     "shell.execute_reply.started": "2021-10-20T16:36:23.007968Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 3, #degree\n",
    "        'n': 1, #number of variables\n",
    "        'monomial_vars': None, #int or None\n",
    "        'laurent': False, #use Laurent polynomials (negative degree with up to -d)  \n",
    "        'neg_d': 0,#int or None\n",
    "        'neg_d_prob': 0,\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': 4,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 1,\n",
    "        'a_min': -1,\n",
    "        'lambda_nets_total': 50000,\n",
    "        'noise': 0,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'shift_polynomial': False,\n",
    "        \n",
    "        'border_min': 0.2, # defines an intervall. Value is randomly chosen and defines the minimum gap between x_min / x_max and the outermost stationary points => two values (left and right gap will be generated per variable)\n",
    "        'border_max': 0.4,\n",
    "        'lower_degree_prob': 0.5, # probability that the degree of the whole polynomial will be reduced\n",
    "        'a_random_prob': 0.5, # probability that a random generated function is used without adjustement\n",
    "                \n",
    "        'global_stationary_prob': 1, # probability that all variables are used for adjustement (0 recommended for higher number of variables)\n",
    "        'bulge_min': 1, # bulge_min and bulge_max define an intervall of how much the function is bulged\n",
    "        'bulge_max': 4,\n",
    "        'min_variables_used': 2, # defines an Intervall of how many variables are used to get stationary points and therefore adjust the function\n",
    "        'max_variables_used': 6,\n",
    "        'max_monomials': 7, # maximum number of monomials, before adjusting the function (monomial of degree 0 is always defined, but is included in this number)\n",
    "        'max_monomials_random': 10, #maximum number of monomials for random generated functions\n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True,  #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sample_sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000,\n",
    "    },\n",
    "    'i_net': {\n",
    "        'optimizer': 'custom',#adam\n",
    "        'inet_loss': 'mae',\n",
    "        'inet_metrics': ['r2'],\n",
    "        'dropout': 0,\n",
    "        'dropout_output': 0,\n",
    "        'epochs': 2000, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "        'dense_layers': [2048, 1024],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'interpretation_dataset_size': 50000,\n",
    "                \n",
    "        'interpretation_net_output_monomials': 2, #(None, int) #CONSTANT IS NOT INCLUDED\n",
    "        'interpretation_net_output_shape': None, #calculated automatically later\n",
    "        'test_size': 100, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'normalize_inet_data': False,\n",
    "        'inet_training_without_noise': False, #dataset size without noise hardcoded to 50k in generate_paths\n",
    "        'sparse_poly_representation_version': 1, #(1, 2); 1=old, 2=new\n",
    "\n",
    "        'evaluate_with_real_function': False, #False\n",
    "        'consider_labels_training': False, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },\n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': True,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "        \n",
    "        'max_optimization_minutes': 30,\n",
    "        \n",
    "        #set if samples analysis should be performed\n",
    "        'samples_list': None,#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "       \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "        \n",
    "        'adjusted_symbolic_metamodeling_code': False,\n",
    "        'symbolic_metamodeling_evaluation': True,\n",
    "        'symbolic_metamodeling_poly_evaluation': False,\n",
    "        'symbolic_metamodeling_function_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_function_evaluation': False,\n",
    "        \n",
    "        \n",
    "        'symbolic_regression_evaluation': True,\n",
    "        'per_network_evaluation': False,\n",
    "    },\n",
    "    'computation':{\n",
    "        'train_model': True,\n",
    "        'n_jobs': 10,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:23.036897Z",
     "iopub.status.busy": "2021-10-20T16:36:23.036164Z",
     "iopub.status.idle": "2021-10-20T16:36:23.045590Z",
     "shell.execute_reply": "2021-10-20T16:36:23.044563Z",
     "shell.execute_reply.started": "2021-10-20T16:36:23.036847Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:23.048028Z",
     "iopub.status.busy": "2021-10-20T16:36:23.047296Z",
     "iopub.status.idle": "2021-10-20T16:36:26.444684Z",
     "shell.execute_reply": "2021-10-20T16:36:26.443928Z",
     "shell.execute_reply.started": "2021-10-20T16:36:23.047978Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('WARNING')\n",
    "tf.autograph.set_verbosity(2)\n",
    "\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random \n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:26.446122Z",
     "iopub.status.busy": "2021-10-20T16:36:26.445805Z",
     "iopub.status.idle": "2021-10-20T16:36:26.453687Z",
     "shell.execute_reply": "2021-10-20T16:36:26.452921Z",
     "shell.execute_reply.started": "2021-10-20T16:36:26.446087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:26.455840Z",
     "iopub.status.busy": "2021-10-20T16:36:26.455572Z",
     "iopub.status.idle": "2021-10-20T16:36:26.463424Z",
     "shell.execute_reply": "2021-10-20T16:36:26.462930Z",
     "shell.execute_reply.started": "2021-10-20T16:36:26.455818Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "n_jobs = min((epochs_lambda//each_epochs_save_lambda+1, n_jobs)) if multi_epoch_analysis else min(len(samples_list), n_jobs) if samples_list!=None else 1\n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "data_reshape_version = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if use_gpu else ''\n",
    "\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/local/cuda-10.1'\n",
    "\n",
    "#os.environ['XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda' if use_gpu else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2, --tf_xla_enable_xla_devices' if use_gpu else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:26.464506Z",
     "iopub.status.busy": "2021-10-20T16:36:26.464224Z",
     "iopub.status.idle": "2021-10-20T16:36:27.262576Z",
     "shell.execute_reply": "2021-10-20T16:36:27.261563Z",
     "shell.execute_reply.started": "2021-10-20T16:36:26.464486Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 4\n",
      "[[3], [2], [1], [0]]\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import flatten, rec_gen, gen_monomial_identifier_list\n",
    "\n",
    "list_of_monomial_identifiers_extended = []\n",
    "\n",
    "if laurent:\n",
    "    variable_sets = [list(flatten([[_d for _d in range(d+1)], [-_d for _d in range(1, neg_d+1)]])) for _ in range(n)]\n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)    \n",
    "        \n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity:' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)     \n",
    "        \n",
    "    list_of_monomial_identifiers = []\n",
    "    for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "        if np.sum(monomial_identifier) <= d:\n",
    "            if monomial_vars == None or len(list(filter(lambda x: x != 0, monomial_identifier))) <= monomial_vars:\n",
    "                list_of_monomial_identifiers.append(monomial_identifier)        \n",
    "else:\n",
    "    variable_list = ['x'+ str(i) for i in range(n)]\n",
    "    list_of_monomial_identifiers = gen_monomial_identifier_list(variable_list, d, n)\n",
    "            \n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "#print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "#print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:27.264472Z",
     "iopub.status.busy": "2021-10-20T16:36:27.263889Z",
     "iopub.status.idle": "2021-10-20T16:36:27.363154Z",
     "shell.execute_reply": "2021-10-20T16:36:27.362674Z",
     "shell.execute_reply.started": "2021-10-20T16:36:27.264433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape:  10\n"
     ]
    }
   ],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "config['i_net']['data_reshape_version'] = data_reshape_version\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d']) if not laurent else len(list_of_monomial_identifiers)\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "\n",
    "config['i_net']['interpretation_net_output_shape'] = config['data']['sparsity'] if config['i_net']['interpretation_net_output_monomials'] is None else config['data']['sparsity']*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials'] if config['i_net']['sparse_poly_representation_version'] == 1 else config['data']['n']*(config['data']['d']+1)*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials']  \n",
    "print('Output Shape: ', config['i_net']['interpretation_net_output_shape'])\n",
    "\n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sample_sparsity')*config['data']['sample_sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "\n",
    "\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='interpretation_net'))\n",
    "create_folders_inet()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:27.364181Z",
     "iopub.status.busy": "2021-10-20T16:36:27.363966Z",
     "iopub.status.idle": "2021-10-20T16:36:27.368774Z",
     "shell.execute_reply": "2021-10-20T16:36:27.368017Z",
     "shell.execute_reply.started": "2021-10-20T16:36:27.364155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inet_dense2048-output_10_drop0e2000b256_custom/lnets_50000_20-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_1_d_3_negd_0_prob_0_spars_4_amin_-1_amax_1_xdist_uniform_noise_normal_0\n",
      "lnets_50000_20-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_1_d_3_negd_0_prob_0_spars_4_amin_-1_amax_1_xdist_uniform_noise_normal_0\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net_data)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:27.369885Z",
     "iopub.status.busy": "2021-10-20T16:36:27.369701Z",
     "iopub.status.idle": "2021-10-20T16:36:27.376255Z",
     "shell.execute_reply": "2021-10-20T16:36:27.375547Z",
     "shell.execute_reply.started": "2021-10-20T16:36:27.369862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:27.377854Z",
     "iopub.status.busy": "2021-10-20T16:36:27.377234Z",
     "iopub.status.idle": "2021-10-20T16:36:27.387624Z",
     "shell.execute_reply": "2021-10-20T16:36:27.386843Z",
     "shell.execute_reply.started": "2021-10-20T16:36:27.377829Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index, no_noise=False):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path_identifier_lambda_net_data_loading = None \n",
    "                \n",
    "    if no_noise==True:\n",
    "        path_identifier_lambda_net_data_loading = generate_paths(path_type='interpretation_net_no_noise')['path_identifier_lambda_net_data']\n",
    "    else:\n",
    "        path_identifier_lambda_net_data_loading = path_identifier_lambda_net_data \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_identifier_lambda_net_data_loading + '/'\n",
    "    path_weights = directory + 'weights_epoch_' + str(index).zfill(3) + '.txt'\n",
    "    path_X_data = directory + 'lambda_X_test_data.txt'\n",
    "    path_y_data = directory + 'lambda_y_test_data.txt'        \n",
    "    \n",
    "    weight_data = pd.read_csv(path_weights, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        weight_data = weight_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_X_test_data = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_y_test_data = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "    lambda_nets = [None] * weight_data.shape[0]\n",
    "    for i, (row_weights, row_lambda_X_test_data, row_lambda_y_test_data) in enumerate(zip(weight_data.values, lambda_X_test_data.values, lambda_y_test_data.values)):        \n",
    "        lambda_net = LambdaNet(row_weights, row_lambda_X_test_data, row_lambda_y_test_data)\n",
    "        lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:36:27.388851Z",
     "iopub.status.busy": "2021-10-20T16:36:27.388579Z",
     "iopub.status.idle": "2021-10-20T16:37:09.065002Z",
     "shell.execute_reply": "2021-10-20T16:37:09.064306Z",
     "shell.execute_reply.started": "2021-10-20T16:36:27.388826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend MultiprocessingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   1 out of   1 | elapsed:   41.4s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if inet_training_without_noise:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list_without_noise = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1, no_noise=True) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "else:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:30:49.711839Z",
     "start_time": "2021-01-05T09:29:48.873305Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:09.066313Z",
     "iopub.status.busy": "2021-10-20T16:37:09.066032Z",
     "iopub.status.idle": "2021-10-20T16:37:11.249651Z",
     "shell.execute_reply": "2021-10-20T16:37:11.248970Z",
     "shell.execute_reply.started": "2021-10-20T16:37:09.066290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>3-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>0-target</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33553</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.505</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.736</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.525</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.736</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.589</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.479</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.389</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.696</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>0.480</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.957</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.957</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.426</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.251</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39489</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.367</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             seed  3-target  2-target  1-target  0-target  3-lstsq_lambda  \\\n",
       "33553  1373158606     0.047     0.040     0.339     0.505           0.005   \n",
       "9427   1373158606    -0.383     0.736    -0.337     0.589           0.002   \n",
       "199    1373158606    -0.216    -0.516    -0.499    -0.033          -0.206   \n",
       "12447  1373158606     0.957    -0.893    -0.418     0.426           0.006   \n",
       "39489  1373158606    -0.500     0.755     0.504     0.403          -0.176   \n",
       "\n",
       "       2-lstsq_lambda  1-lstsq_lambda  0-lstsq_lambda  3-lstsq_target  \\\n",
       "33553          -0.009           0.422           0.489           0.047   \n",
       "9427           -0.003           0.083           0.525          -0.383   \n",
       "199            -0.532          -0.493          -0.033          -0.216   \n",
       "12447          -0.009          -0.529           0.419           0.957   \n",
       "39489           0.196           0.784           0.367          -0.500   \n",
       "\n",
       "       2-lstsq_target  1-lstsq_target  0-lstsq_target   wb_0   wb_1  wb_2  \\\n",
       "33553           0.040           0.339           0.505 -0.011 -0.275 0.296   \n",
       "9427            0.736          -0.337           0.589 -0.011 -0.275 0.334   \n",
       "199            -0.516          -0.499          -0.033 -0.011 -0.275 0.515   \n",
       "12447          -0.893          -0.418           0.426 -0.011 -0.275 0.399   \n",
       "39489           0.755           0.504           0.403 -0.011 -0.275 0.194   \n",
       "\n",
       "       wb_3  wb_4  wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "33553 0.176 0.358 0.282 0.245 -0.361 0.139 0.367  0.441 -0.107 -0.023  0.299   \n",
       "9427  0.214 0.315 0.240 0.283 -0.361 0.178 0.318  0.479 -0.107 -0.023  0.337   \n",
       "199   0.387 0.153 0.085 0.455 -0.361 0.334 0.619  0.696 -0.107 -0.023  0.530   \n",
       "12447 0.272 0.248 0.172 0.350 -0.361 0.232 0.459  0.550 -0.107 -0.023  0.405   \n",
       "39489 0.147 0.393 0.317 0.172 -0.361 0.128 0.419  0.409 -0.107 -0.023  0.250   \n",
       "\n",
       "       wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "33553  0.278  0.423  0.110  0.419  0.463  0.438  0.000  0.000 -0.002 -0.001   \n",
       "9427   0.235  0.461  0.068  0.374  0.421  0.394  0.000  0.000 -0.002 -0.001   \n",
       "199    0.084  0.670  0.012  0.277  0.251  0.257  0.000  0.000 -0.082 -0.186   \n",
       "12447  0.167  0.530 -0.004  0.309  0.353  0.328  0.000  0.000 -0.004 -0.002   \n",
       "39489  0.314  0.388  0.144  0.457  0.497  0.474  0.000  0.000 -0.149 -0.118   \n",
       "\n",
       "       wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "33553  0.134  0.134 -0.002  0.000 -0.001  0.091 -0.010  0.000  0.000 -0.003   \n",
       "9427   0.150  0.150 -0.001  0.000 -0.000  0.077 -0.005  0.000  0.000 -0.002   \n",
       "199   -0.023 -0.088 -0.275  0.000 -0.281 -0.451 -0.007  0.000  0.000 -0.175   \n",
       "12447  0.128  0.130 -0.003  0.000 -0.002 -0.004 -0.007  0.000  0.000 -0.004   \n",
       "39489  0.099  0.099 -0.133  0.000 -0.107  0.038  0.000  0.000  0.000 -0.001   \n",
       "\n",
       "       wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "33553  0.136 -0.008  0.132  0.141  0.133  0.138 -0.309 -0.436 -0.299 -0.294   \n",
       "9427   0.153 -0.005  0.150  0.157  0.148  0.153 -0.309 -0.436 -0.346 -0.347   \n",
       "199   -0.086 -0.000 -0.026 -0.071 -0.038 -0.036 -0.309 -0.436 -0.535 -0.616   \n",
       "12447  0.130 -0.007  0.139  0.124  0.127  0.125 -0.309 -0.436 -0.412 -0.408   \n",
       "39489  0.100  0.000  0.098  0.103  0.099  0.101 -0.309 -0.436 -0.276 -0.357   \n",
       "\n",
       "       wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "33553  0.365  0.405 -0.104 -0.495 -0.319  0.052 -0.135 -0.428 -0.442 -0.126   \n",
       "9427   0.343  0.389 -0.154 -0.495 -0.380  0.016 -0.180 -0.428 -0.442 -0.173   \n",
       "199    0.113  0.211 -0.455 -0.495 -0.749 -0.404 -0.377 -0.428 -0.442 -0.407   \n",
       "12447  0.278  0.330 -0.218 -0.495 -0.438 -0.083 -0.248 -0.428 -0.442 -0.239   \n",
       "39489  0.375  0.412 -0.108 -0.495 -0.409  0.074 -0.086 -0.428 -0.442 -0.024   \n",
       "\n",
       "       wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  \n",
       "33553  0.326 -0.228  0.637  0.219  0.408  0.266  0.126  \n",
       "9427   0.310 -0.273  0.651  0.196  0.382  0.242  0.141  \n",
       "199    0.130 -0.470  0.480 -0.056  0.160  0.012 -0.034  \n",
       "12447  0.251 -0.341  0.668  0.127  0.314  0.173  0.129  \n",
       "39489  0.332 -0.180  0.623  0.232  0.423  0.280  0.095  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:31:56.898548Z",
     "start_time": "2021-01-05T09:30:49.715497Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:11.250950Z",
     "iopub.status.busy": "2021-10-20T16:37:11.250782Z",
     "iopub.status.idle": "2021-10-20T16:37:13.701273Z",
     "shell.execute_reply": "2021-10-20T16:37:13.700594Z",
     "shell.execute_reply.started": "2021-10-20T16:37:11.250929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>3-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>0-target</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.524</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.405</td>\n",
       "      <td>-1.078</td>\n",
       "      <td>-1.394</td>\n",
       "      <td>-1.005</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>-1.044</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.804</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-1.315</td>\n",
       "      <td>-1.311</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-1.147</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-1.723</td>\n",
       "      <td>-1.504</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-1.720</td>\n",
       "      <td>-1.005</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>-0.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.495</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.474</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.506</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.658</td>\n",
       "      <td>1.335</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.813</td>\n",
       "      <td>1.631</td>\n",
       "      <td>1.483</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.766</td>\n",
       "      <td>1.052</td>\n",
       "      <td>0.647</td>\n",
       "      <td>1.436</td>\n",
       "      <td>0.892</td>\n",
       "      <td>1.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.193</td>\n",
       "      <td>1.835</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>1.494</td>\n",
       "      <td>1.616</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>1.802</td>\n",
       "      <td>1.245</td>\n",
       "      <td>1.838</td>\n",
       "      <td>1.951</td>\n",
       "      <td>1.160</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                seed  3-target  2-target  1-target  0-target  3-lstsq_lambda  \\\n",
       "count      50000.000 50000.000 50000.000 50000.000 50000.000       50000.000   \n",
       "mean  1373158606.000     0.000     0.005     0.002    -0.000           0.018   \n",
       "std            0.000     0.577     0.579     0.576     0.578           0.494   \n",
       "min   1373158606.000    -1.000    -1.000    -1.000    -1.000          -1.405   \n",
       "25%   1373158606.000    -0.502    -0.500    -0.495    -0.503          -0.259   \n",
       "50%   1373158606.000     0.005     0.007     0.007    -0.003          -0.000   \n",
       "75%   1373158606.000     0.499     0.509     0.502     0.503           0.325   \n",
       "max   1373158606.000     1.000     1.000     1.000     1.000           1.506   \n",
       "\n",
       "       2-lstsq_lambda  1-lstsq_lambda  0-lstsq_lambda  3-lstsq_target  \\\n",
       "count       50000.000       50000.000       50000.000       50000.000   \n",
       "mean            0.016          -0.024           0.005           0.000   \n",
       "std             0.461           0.582           0.552           0.577   \n",
       "min            -1.078          -1.394          -1.005          -1.000   \n",
       "25%            -0.265          -0.519          -0.462          -0.502   \n",
       "50%             0.001          -0.030          -0.003           0.005   \n",
       "75%             0.323           0.475           0.475           0.499   \n",
       "max             1.064           1.250           1.007           1.000   \n",
       "\n",
       "       2-lstsq_target  1-lstsq_target  0-lstsq_target      wb_0      wb_1  \\\n",
       "count       50000.000       50000.000       50000.000 50000.000 50000.000   \n",
       "mean            0.005           0.002          -0.000    -0.011    -0.275   \n",
       "std             0.579           0.576           0.578     0.000     0.000   \n",
       "min            -1.000          -1.000          -1.000    -0.011    -0.275   \n",
       "25%            -0.500          -0.495          -0.503    -0.011    -0.275   \n",
       "50%             0.007           0.007          -0.003    -0.011    -0.275   \n",
       "75%             0.509           0.502           0.503    -0.011    -0.275   \n",
       "max             1.000           1.000           1.000    -0.011    -0.275   \n",
       "\n",
       "           wb_2      wb_3      wb_4      wb_5      wb_6      wb_7      wb_8  \\\n",
       "count 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000   \n",
       "mean      0.332     0.215     0.321     0.250     0.306    -0.361     0.180   \n",
       "std       0.126     0.128     0.130     0.132     0.153     0.000     0.130   \n",
       "min      -0.049    -0.225    -0.086    -0.169    -0.141    -0.361    -0.277   \n",
       "25%       0.253     0.101     0.239     0.157     0.210    -0.361     0.082   \n",
       "50%       0.335     0.214     0.321     0.250     0.292    -0.361     0.175   \n",
       "75%       0.411     0.294     0.399     0.331     0.380    -0.361     0.259   \n",
       "max       0.759     0.731     0.848     0.658     1.335    -0.361     0.813   \n",
       "\n",
       "           wb_9     wb_10     wb_11     wb_12     wb_13     wb_14     wb_15  \\\n",
       "count 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000   \n",
       "mean      0.489     0.518    -0.107    -0.023     0.364     0.249     0.474   \n",
       "std       0.236     0.147     0.000     0.000     0.148     0.137     0.124   \n",
       "min      -0.099     0.130    -0.107    -0.023    -0.064    -0.185     0.106   \n",
       "25%       0.344     0.425    -0.107    -0.023     0.275     0.151     0.399   \n",
       "50%       0.427     0.495    -0.107    -0.023     0.349     0.246     0.468   \n",
       "75%       0.592     0.579    -0.107    -0.023     0.430     0.333     0.543   \n",
       "max       1.631     1.483    -0.107    -0.023     1.347     0.766     1.052   \n",
       "\n",
       "          wb_16     wb_17     wb_18     wb_19     wb_20     wb_21     wb_22  \\\n",
       "count 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000   \n",
       "mean      0.079     0.434     0.429     0.430     0.000     0.000     0.014   \n",
       "std       0.132     0.179     0.133     0.149     0.000     0.000     0.151   \n",
       "min      -0.312    -0.040     0.040    -0.004     0.000     0.000    -0.452   \n",
       "25%       0.012     0.320     0.347     0.331     0.000     0.000    -0.134   \n",
       "50%       0.026     0.402     0.424     0.411     0.000     0.000    -0.000   \n",
       "75%       0.140     0.502     0.501     0.501     0.000     0.000     0.147   \n",
       "max       0.647     1.436     0.892     1.109     0.000     0.000     0.397   \n",
       "\n",
       "          wb_23     wb_24     wb_25     wb_26     wb_27     wb_28     wb_29  \\\n",
       "count 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000   \n",
       "mean     -0.013     0.010    -0.001    -0.021     0.000    -0.018    -0.066   \n",
       "std       0.177     0.155     0.167     0.195     0.000     0.185     0.306   \n",
       "min      -0.479    -0.457    -0.522    -0.741     0.000    -0.518    -1.044   \n",
       "25%      -0.106    -0.113    -0.098    -0.150     0.000    -0.089    -0.321   \n",
       "50%      -0.001    -0.000    -0.000    -0.001     0.000    -0.001     0.015   \n",
       "75%       0.144     0.141     0.141     0.144     0.000     0.142     0.171   \n",
       "max       0.369     0.358     0.311     0.368     0.000     0.351     0.353   \n",
       "\n",
       "          wb_30     wb_31     wb_32     wb_33     wb_34     wb_35     wb_36  \\\n",
       "count 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000   \n",
       "mean      0.029     0.000     0.000    -0.006    -0.014     0.038     0.015   \n",
       "std       0.150     0.000     0.000     0.181     0.187     0.128     0.152   \n",
       "min      -0.892     0.000     0.000    -0.804    -0.541    -0.534    -0.442   \n",
       "25%      -0.010     0.000     0.000    -0.152    -0.096    -0.008    -0.026   \n",
       "50%      -0.000     0.000     0.000    -0.000    -0.000     0.000    -0.001   \n",
       "75%       0.142     0.000     0.000     0.144     0.142     0.147     0.141   \n",
       "max       0.301     0.000     0.000     0.365     0.314     0.443     0.311   \n",
       "\n",
       "          wb_37     wb_38     wb_39     wb_40     wb_41     wb_42     wb_43  \\\n",
       "count 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000   \n",
       "mean     -0.023     0.043     0.020    -0.309    -0.436    -0.412    -0.469   \n",
       "std       0.215     0.113     0.147     0.000     0.000     0.134     0.192   \n",
       "min      -0.881    -0.451    -0.758    -0.309    -0.436    -1.110    -1.315   \n",
       "25%      -0.120    -0.007    -0.013    -0.309    -0.436    -0.466    -0.513   \n",
       "50%      -0.001     0.000    -0.000    -0.309    -0.436    -0.403    -0.427   \n",
       "75%       0.136     0.140     0.139    -0.309    -0.436    -0.320    -0.328   \n",
       "max       0.273     0.391     0.262    -0.309    -0.436    -0.086    -0.178   \n",
       "\n",
       "          wb_44     wb_45     wb_46     wb_47     wb_48     wb_49     wb_50  \\\n",
       "count 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000   \n",
       "mean      0.368     0.436    -0.226    -0.495    -0.524    -0.003    -0.185   \n",
       "std       0.152     0.181     0.265     0.000     0.221     0.334     0.244   \n",
       "min      -1.311     0.132    -1.147    -0.495    -1.723    -1.504    -0.653   \n",
       "25%       0.272     0.320    -0.294    -0.495    -0.570    -0.131    -0.283   \n",
       "50%       0.351     0.403    -0.222    -0.495    -0.466    -0.041    -0.215   \n",
       "75%       0.418     0.474    -0.128    -0.495    -0.353     0.088    -0.140   \n",
       "max       1.006     1.193     1.835    -0.495    -0.236     1.494     1.616   \n",
       "\n",
       "          wb_51     wb_52     wb_53     wb_54     wb_55     wb_56     wb_57  \\\n",
       "count 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000 50000.000   \n",
       "mean     -0.428    -0.442    -0.212     0.371    -0.305     0.695     0.159   \n",
       "std       0.000     0.000     0.265     0.202     0.162     0.258     0.327   \n",
       "min      -0.428    -0.442    -0.875     0.033    -0.720     0.405    -1.720   \n",
       "25%      -0.428    -0.442    -0.295     0.241    -0.378     0.480     0.118   \n",
       "50%      -0.428    -0.442    -0.230     0.325    -0.310     0.628     0.199   \n",
       "75%      -0.428    -0.442    -0.142     0.403    -0.238     0.762     0.268   \n",
       "max      -0.428    -0.442     1.802     1.245     1.838     1.951     1.160   \n",
       "\n",
       "          wb_58     wb_59     wb_60  \n",
       "count 50000.000 50000.000 50000.000  \n",
       "mean      0.384     0.220    -0.006  \n",
       "std       0.125     0.243     0.154  \n",
       "min      -1.005    -1.738    -0.504  \n",
       "25%       0.307     0.166    -0.152  \n",
       "50%       0.379     0.242    -0.002  \n",
       "75%       0.445     0.311     0.139  \n",
       "max       0.960     0.825     0.421  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:13.704585Z",
     "iopub.status.busy": "2021-10-20T16:37:13.704332Z",
     "iopub.status.idle": "2021-10-20T16:37:13.709302Z",
     "shell.execute_reply": "2021-10-20T16:37:13.708796Z",
     "shell.execute_reply.started": "2021-10-20T16:37:13.704562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24457676],\n",
       "       [0.19267431],\n",
       "       [0.71539711],\n",
       "       [0.13782812],\n",
       "       [0.94373167],\n",
       "       [0.85158305],\n",
       "       [0.19021396],\n",
       "       [0.00893767],\n",
       "       [0.91506872],\n",
       "       [0.50437039]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.X_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:13.710124Z",
     "iopub.status.busy": "2021-10-20T16:37:13.709983Z",
     "iopub.status.idle": "2021-10-20T16:37:13.716690Z",
     "shell.execute_reply": "2021-10-20T16:37:13.716194Z",
     "shell.execute_reply.started": "2021-10-20T16:37:13.710105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59131193],\n",
       "       [0.57244897],\n",
       "       [0.78575999],\n",
       "       [0.55291176],\n",
       "       [0.90085733],\n",
       "       [0.85241354],\n",
       "       [0.5715642 ],\n",
       "       [0.50833553],\n",
       "       [0.88548255],\n",
       "       [0.69260138]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.y_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:32:09.782470Z",
     "start_time": "2021-01-05T09:31:56.901018Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:13.717700Z",
     "iopub.status.busy": "2021-10-20T16:37:13.717486Z",
     "iopub.status.idle": "2021-10-20T16:37:14.253140Z",
     "shell.execute_reply": "2021-10-20T16:37:14.252515Z",
     "shell.execute_reply.started": "2021-10-20T16:37:13.717681Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "\n",
    "if inet_training_without_noise:\n",
    "   \n",
    "    for lambda_net_dataset, lambda_net_dataset_without_noise in zip(lambda_net_dataset_list, lambda_net_dataset_list_without_noise):\n",
    "        if inet_holdout_seed_evaluation:\n",
    "            raise SystemExit('Holdout Evaluation not implemented with inet training without noise')\n",
    "            \n",
    "        else:\n",
    "            lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_dataset_without_noise, test_split=0.1)\n",
    "\n",
    "            _, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_dataset_list_without_noise\n",
    "        \n",
    "else:\n",
    "\n",
    "    for lambda_net_dataset in lambda_net_dataset_list:\n",
    "\n",
    "        if inet_holdout_seed_evaluation:\n",
    "\n",
    "            complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "\n",
    "            if isinstance(test_size, float):\n",
    "                test_size = int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-test_size)))\n",
    "\n",
    "            test_seeds = random.sample(complete_seed_list, test_size)\n",
    "            lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "            valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-0.1))))\n",
    "            lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "            train_seeds = complete_seed_list\n",
    "            lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset\n",
    "        else:\n",
    "\n",
    "            lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "            lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.1)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "\n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:06.495716Z",
     "start_time": "2021-01-05T09:32:09.784760Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:14.254176Z",
     "iopub.status.busy": "2021-10-20T16:37:14.253961Z",
     "iopub.status.idle": "2021-10-20T16:37:16.244213Z",
     "shell.execute_reply": "2021-10-20T16:37:16.243638Z",
     "shell.execute_reply.started": "2021-10-20T16:37:14.254155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44910, 74)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:08.945802Z",
     "start_time": "2021-01-05T09:33:06.499150Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:16.245243Z",
     "iopub.status.busy": "2021-10-20T16:37:16.245033Z",
     "iopub.status.idle": "2021-10-20T16:37:16.433383Z",
     "shell.execute_reply": "2021-10-20T16:37:16.432845Z",
     "shell.execute_reply.started": "2021-10-20T16:37:16.245222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4990, 74)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:11.543306Z",
     "start_time": "2021-01-05T09:33:08.947468Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:16.434662Z",
     "iopub.status.busy": "2021-10-20T16:37:16.434448Z",
     "iopub.status.idle": "2021-10-20T16:37:16.446301Z",
     "shell.execute_reply": "2021-10-20T16:37:16.445780Z",
     "shell.execute_reply.started": "2021-10-20T16:37:16.434642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 74)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:16.447226Z",
     "iopub.status.busy": "2021-10-20T16:37:16.447081Z",
     "iopub.status.idle": "2021-10-20T16:37:18.452814Z",
     "shell.execute_reply": "2021-10-20T16:37:18.452140Z",
     "shell.execute_reply.started": "2021-10-20T16:37:16.447207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>3-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>0-target</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.918</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.930</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.918</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.501</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>0.480</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18619</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33312</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47430</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.871</td>\n",
       "      <td>-0.668</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.833</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.871</td>\n",
       "      <td>-0.668</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.474</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.909</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0.369</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>1.429</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             seed  3-target  2-target  1-target  0-target  3-lstsq_lambda  \\\n",
       "9901   1373158606    -0.327     0.619     0.918    -0.537          -0.292   \n",
       "5910   1373158606     0.437    -0.791    -0.791    -0.593           0.364   \n",
       "18619  1373158606     0.270    -0.369     0.066     0.068           0.078   \n",
       "33312  1373158606     0.440    -0.619     0.605     0.145           0.148   \n",
       "47430  1373158606     0.672     0.871    -0.668    -0.189           0.693   \n",
       "\n",
       "       2-lstsq_lambda  1-lstsq_lambda  0-lstsq_lambda  3-lstsq_target  \\\n",
       "9901            0.577           0.930          -0.538          -0.327   \n",
       "5910           -0.665          -0.852          -0.586           0.437   \n",
       "18619          -0.139           0.002           0.071           0.270   \n",
       "33312          -0.266           0.500           0.150           0.440   \n",
       "47430           0.833          -0.649          -0.192           0.672   \n",
       "\n",
       "       2-lstsq_target  1-lstsq_target  0-lstsq_target   wb_0   wb_1  wb_2  \\\n",
       "9901            0.619           0.918          -0.537 -0.011 -0.275 0.203   \n",
       "5910           -0.791          -0.791          -0.593 -0.011 -0.275 0.421   \n",
       "18619          -0.369           0.066           0.068 -0.011 -0.275 0.332   \n",
       "33312          -0.619           0.605           0.145 -0.011 -0.275 0.284   \n",
       "47430           0.871          -0.668          -0.189 -0.011 -0.275 0.306   \n",
       "\n",
       "       wb_3  wb_4  wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "9901  0.073 0.487 0.411 0.174 -0.361 0.081 0.555  0.371 -0.107 -0.023  0.225   \n",
       "5910  0.292 0.159 0.135 0.371 -0.361 0.252 0.461  0.574 -0.107 -0.023  0.427   \n",
       "18619 0.202 0.300 0.224 0.278 -0.361 0.160 0.349  0.482 -0.107 -0.023  0.335   \n",
       "33312 0.157 0.347 0.271 0.231 -0.361 0.072 0.362  0.440 -0.107 -0.023  0.288   \n",
       "47430 0.173 0.533 0.475 0.267 -0.361 0.129 0.702  0.474 -0.107 -0.023  0.323   \n",
       "\n",
       "       wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "9901   0.413  0.340  0.236  0.547  0.589  0.572  0.000  0.000  0.239  0.174   \n",
       "5910   0.134  0.554  0.011  0.542  0.339  0.313  0.000  0.000  0.151  0.151   \n",
       "18619  0.220  0.464  0.053  0.359  0.405  0.379  0.000  0.000 -0.004 -0.014   \n",
       "33312  0.267  0.421  0.100  0.408  0.452  0.427  0.000  0.000 -0.033 -0.032   \n",
       "47430  0.479  0.444  0.349  0.615  0.652  0.636  0.000  0.000  0.075  0.077   \n",
       "\n",
       "       wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "9901  -0.006 -0.004  0.206  0.000 -0.092 -0.203  0.210  0.000  0.000  0.214   \n",
       "5910  -0.139 -0.111  0.158  0.000  0.151  0.179  0.155  0.000  0.000  0.156   \n",
       "18619  0.021  0.021 -0.004  0.000 -0.026 -0.010 -0.006  0.000  0.000 -0.004   \n",
       "33312  0.043  0.043 -0.044  0.000 -0.075 -0.043 -0.013  0.000  0.000 -0.039   \n",
       "47430 -0.332 -0.342  0.065  0.000  0.078 -0.120  0.066  0.000  0.000  0.067   \n",
       "\n",
       "       wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "9901  -0.004  0.232 -0.002 -0.094 -0.008 -0.020 -0.309 -0.436 -0.328 -0.306   \n",
       "5910  -0.107  0.152 -0.026 -0.165 -0.001 -0.001 -0.309 -0.436 -0.465 -0.466   \n",
       "18619  0.021 -0.006  0.021  0.020  0.021  0.021 -0.309 -0.436 -0.343 -0.332   \n",
       "33312  0.043 -0.009  0.043  0.044  0.043  0.043 -0.309 -0.436 -0.291 -0.280   \n",
       "47430 -0.388  0.072 -0.315 -0.325 -0.194 -0.271 -0.309 -0.436 -0.310 -0.313   \n",
       "\n",
       "       wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "9901   0.458  0.501 -0.125 -0.495 -0.305  0.186 -0.119 -0.428 -0.442 -0.137   \n",
       "5910   0.219  0.306 -0.274 -0.495 -0.499 -0.138 -0.297 -0.428 -0.442 -0.293   \n",
       "18619  0.279  0.314 -0.147 -0.495 -0.360 -0.013 -0.183 -0.428 -0.442 -0.171   \n",
       "33312  0.324  0.358 -0.096 -0.495 -0.301  0.037 -0.134 -0.428 -0.442 -0.119   \n",
       "47430  0.790  0.909 -0.119 -0.495 -0.349  0.369 -0.146 -0.428 -0.442 -0.138   \n",
       "\n",
       "       wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  \n",
       "9901   0.422 -0.224  0.751  0.307  0.502  0.357 -0.259  \n",
       "5910   0.225 -0.391  0.480 -0.416  0.244  0.094 -0.143  \n",
       "18619  0.234 -0.276  0.515  0.137  0.328  0.185  0.021  \n",
       "33312  0.278 -0.227  0.557  0.181  0.373  0.230  0.043  \n",
       "47430  0.900 -0.239  1.429  0.602  0.665  0.601 -0.082  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:18.454777Z",
     "iopub.status.busy": "2021-10-20T16:37:18.454149Z",
     "iopub.status.idle": "2021-10-20T16:37:18.673921Z",
     "shell.execute_reply": "2021-10-20T16:37:18.673250Z",
     "shell.execute_reply.started": "2021-10-20T16:37:18.454752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>3-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>0-target</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7038</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>0.646</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>0.645</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>0.646</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.583</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.612</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.783</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.692</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-1.029</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>0.846</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30770</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.558</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41342</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.725</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.715</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.725</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.626</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.610</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21084</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.777</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.739</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.777</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.561</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47211</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.664</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>1.162</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             seed  3-target  2-target  1-target  0-target  3-lstsq_lambda  \\\n",
       "7038   1373158606    -0.989    -0.035    -0.980     0.646          -0.959   \n",
       "30770  1373158606     0.629    -0.845    -0.341     0.188           0.155   \n",
       "41342  1373158606     0.525     0.040     0.725    -0.409           0.511   \n",
       "21084  1373158606    -0.777    -0.243     0.164    -0.426          -0.014   \n",
       "47211  1373158606     0.505     0.230    -0.332    -0.317           0.494   \n",
       "\n",
       "       2-lstsq_lambda  1-lstsq_lambda  0-lstsq_lambda  3-lstsq_target  \\\n",
       "7038           -0.075          -0.966           0.645          -0.989   \n",
       "30770          -0.275          -0.507           0.196           0.629   \n",
       "41342           0.062           0.715          -0.408           0.525   \n",
       "21084           0.024          -0.739          -0.223          -0.777   \n",
       "47211           0.242          -0.334          -0.318           0.505   \n",
       "\n",
       "       2-lstsq_target  1-lstsq_target  0-lstsq_target   wb_0   wb_1  wb_2  \\\n",
       "7038           -0.035          -0.980           0.646 -0.011 -0.275 0.581   \n",
       "30770          -0.845          -0.341           0.188 -0.011 -0.275 0.407   \n",
       "41342           0.040           0.725          -0.409 -0.011 -0.275 0.145   \n",
       "21084          -0.243           0.164          -0.426 -0.011 -0.275 0.410   \n",
       "47211           0.230          -0.332          -0.317 -0.011 -0.275 0.208   \n",
       "\n",
       "       wb_3  wb_4   wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "7038  0.507 0.035 -0.060 0.583 -0.361 0.474 0.685  0.811 -0.107 -0.023  0.612   \n",
       "30770 0.275 0.232  0.156 0.354 -0.361 0.235 0.440  0.558 -0.107 -0.023  0.411   \n",
       "41342 0.011 0.475  0.432 0.129 -0.361 0.076 0.595  0.333 -0.107 -0.023  0.180   \n",
       "21084 0.281 0.233  0.157 0.358 -0.361 0.241 0.440  0.561 -0.107 -0.023  0.414   \n",
       "47211 0.073 0.405  0.351 0.155 -0.361 0.031 0.350  0.368 -0.107 -0.023  0.215   \n",
       "\n",
       "       wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "7038  -0.067  0.783 -0.243  0.216  0.151  0.161  0.000  0.000 -0.160 -0.387   \n",
       "30770  0.152  0.539  0.009  0.294  0.337  0.312  0.000  0.000 -0.001 -0.029   \n",
       "41342  0.432  0.289  0.315  0.637  0.655  0.647  0.000  0.000  0.202  0.190   \n",
       "21084  0.153  0.542  0.012  0.293  0.335  0.311  0.000  0.000  0.063  0.063   \n",
       "47211  0.355  0.344  0.243  0.480  0.516  0.499  0.000  0.000  0.121  0.123   \n",
       "\n",
       "       wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "7038   0.206  0.217 -0.382  0.000 -0.405 -0.382 -0.011  0.000  0.000 -0.272   \n",
       "30770  0.084  0.084 -0.009  0.000 -0.037 -0.001 -0.005  0.000  0.000 -0.001   \n",
       "41342 -0.131 -0.206  0.157  0.000 -0.088 -0.437  0.162  0.000  0.000  0.167   \n",
       "21084 -0.005 -0.001  0.064  0.000  0.063  0.070  0.064  0.000  0.000  0.064   \n",
       "47211 -0.248 -0.251  0.119  0.000  0.125  0.078  0.115  0.000  0.000  0.118   \n",
       "\n",
       "       wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "7038   0.217 -0.003  0.244  0.126  0.201  0.175 -0.309 -0.436 -0.628 -0.945   \n",
       "30770  0.083 -0.004 -0.028  0.079  0.085  0.082 -0.309 -0.436 -0.417 -0.407   \n",
       "41342 -0.269  0.192 -0.268 -0.004 -0.004 -0.002 -0.309 -0.436 -0.272 -0.317   \n",
       "21084 -0.002  0.063 -0.026 -0.014 -0.011 -0.012 -0.309 -0.436 -0.424 -0.416   \n",
       "47211 -0.283  0.118 -0.215 -0.244 -0.123 -0.200 -0.309 -0.436 -0.307 -0.351   \n",
       "\n",
       "       wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "7038   0.213  0.334 -0.692 -0.495 -1.029 -0.502 -0.489 -0.428 -0.442 -0.572   \n",
       "30770  0.227  0.268 -0.221 -0.495 -0.436 -0.087 -0.255 -0.428 -0.442 -0.244   \n",
       "41342  0.468  0.626 -0.067 -0.495 -0.300  0.404 -0.056 -0.428 -0.442 -0.077   \n",
       "21084  0.199  0.230 -0.229 -0.495 -0.444 -0.094 -0.262 -0.428 -0.442 -0.251   \n",
       "47211  0.597  0.704 -0.128 -0.495 -0.407  0.038 -0.116 -0.428 -0.442 -0.133   \n",
       "\n",
       "       wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  \n",
       "7038   0.257 -0.582  0.846 -0.013  0.197  0.049  0.218  \n",
       "30770  0.188 -0.349  0.477  0.081  0.271  0.129  0.088  \n",
       "41342  0.610 -0.163  1.150  0.383  0.571  0.430 -0.228  \n",
       "21084  0.148 -0.356  0.480  0.057  0.250  0.106 -0.061  \n",
       "47211  0.664 -0.213  1.162  0.414  0.473  0.414 -0.123  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:18.675005Z",
     "iopub.status.busy": "2021-10-20T16:37:18.674850Z",
     "iopub.status.idle": "2021-10-20T16:37:18.714526Z",
     "shell.execute_reply": "2021-10-20T16:37:18.713845Z",
     "shell.execute_reply.started": "2021-10-20T16:37:18.674984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>3-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>0-target</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35587</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.605</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.605</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>0.593</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32681</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.696</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.686</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.696</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.694</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.675</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.477</td>\n",
       "      <td>-0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40971</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.728</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-1.010</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.479</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21022</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.915</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.785</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.785</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.915</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.785</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.531</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.733</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.575</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.727</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6403</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.465</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.022</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.703</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.480</td>\n",
       "      <td>-1.232</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>-0.228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             seed  3-target  2-target  1-target  0-target  3-lstsq_lambda  \\\n",
       "35587  1373158606    -0.423     0.605    -0.763     0.593           0.000   \n",
       "32681  1373158606     0.573     0.425     0.696    -0.089           0.559   \n",
       "40971  1373158606    -0.738    -0.737     0.350    -0.071          -0.781   \n",
       "21022  1373158606    -0.385    -0.915    -0.257     0.785          -0.370   \n",
       "6403   1373158606    -0.526    -0.885    -0.241    -0.955          -0.682   \n",
       "\n",
       "       2-lstsq_lambda  1-lstsq_lambda  0-lstsq_lambda  3-lstsq_target  \\\n",
       "35587          -0.001          -0.530           0.574          -0.423   \n",
       "32681           0.448           0.686          -0.089           0.573   \n",
       "40971          -0.661           0.310          -0.065          -0.738   \n",
       "21022          -0.938          -0.250           0.785          -0.385   \n",
       "6403           -0.635          -0.355          -0.939          -0.526   \n",
       "\n",
       "       2-lstsq_target  1-lstsq_target  0-lstsq_target   wb_0   wb_1  wb_2  \\\n",
       "35587           0.605          -0.763           0.593 -0.011 -0.275 0.407   \n",
       "32681           0.425           0.696          -0.089 -0.011 -0.275 0.152   \n",
       "40971          -0.737           0.350          -0.071 -0.011 -0.275 0.556   \n",
       "21022          -0.915          -0.257           0.785 -0.011 -0.275 0.569   \n",
       "6403           -0.885          -0.241          -0.955 -0.011 -0.275 0.309   \n",
       "\n",
       "       wb_3  wb_4   wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "35587 0.282 0.247  0.170 0.360 -0.361 0.244 0.469  0.557 -0.107 -0.023  0.414   \n",
       "32681 0.103 0.561  0.487 0.130 -0.361 0.084 0.801  0.380 -0.107 -0.023  0.240   \n",
       "40971 0.453 0.170  0.089 0.530 -0.361 0.423 0.686  0.690 -0.107 -0.023  0.572   \n",
       "21022 0.436 0.060 -0.023 0.531 -0.361 0.394 0.641  0.733 -0.107 -0.023  0.575   \n",
       "6403  0.176 0.132  0.098 0.255 -0.361 0.135 0.349  0.465 -0.107 -0.023  0.314   \n",
       "\n",
       "       wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "35587  0.165  0.537 -0.005  0.305  0.353  0.326  0.000  0.000 -0.001 -0.001   \n",
       "32681  0.498  0.304  0.347  0.661  0.707  0.708  0.000  0.000 -0.152 -0.103   \n",
       "40971  0.095  0.687  0.011  0.298  0.271  0.267  0.000  0.000 -0.259 -0.369   \n",
       "21022 -0.035  0.727 -0.199  0.117  0.177  0.145  0.000  0.000 -0.188 -0.292   \n",
       "6403   0.096  0.445  0.012  1.022  0.430  0.941  0.000  0.000  0.241  0.244   \n",
       "\n",
       "       wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "35587  0.165  0.168 -0.001  0.000 -0.000 -0.001 -0.001  0.000  0.000 -0.001   \n",
       "32681 -0.108 -0.243 -0.130  0.000 -0.086 -0.596 -0.028  0.000  0.000 -0.136   \n",
       "40971 -0.000 -0.000 -0.389  0.000 -0.377 -0.447 -0.243  0.000  0.000 -0.323   \n",
       "21022  0.208  0.214 -0.299  0.000 -0.339 -0.493 -0.130  0.000  0.000 -0.263   \n",
       "6403  -0.132 -0.098  0.254  0.000  0.244  0.283  0.247  0.000  0.000  0.250   \n",
       "\n",
       "       wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "35587  0.171 -0.001  0.178  0.166  0.162  0.164 -0.309 -0.436 -0.420 -0.420   \n",
       "32681 -0.314 -0.057 -0.298 -0.238 -0.004 -0.004 -0.309 -0.436 -0.218 -0.274   \n",
       "40971 -0.000 -0.156 -0.027 -0.045 -0.000 -0.000 -0.309 -0.436 -0.728 -0.927   \n",
       "21022  0.220 -0.024  0.221  0.208  0.199  0.203 -0.309 -0.436 -0.679 -0.834   \n",
       "6403  -0.096  0.242 -0.026 -0.703  0.000 -0.348 -0.309 -0.436 -0.458 -0.484   \n",
       "\n",
       "       wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "35587  0.315  0.374 -0.228 -0.495 -0.453 -0.088 -0.253 -0.428 -0.442 -0.247   \n",
       "32681  0.546  0.694 -0.012 -0.495 -0.316  0.521  0.040 -0.428 -0.442  0.119   \n",
       "40971  0.142  0.185 -0.686 -0.495 -1.010 -0.541 -0.483 -0.428 -0.442 -0.613   \n",
       "21022  0.350  0.457 -0.602 -0.495 -0.933 -0.535 -0.447 -0.428 -0.442 -0.561   \n",
       "6403   0.169  0.229 -0.279 -0.495 -0.526 -0.136 -0.272 -0.428 -0.442 -0.287   \n",
       "\n",
       "       wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  \n",
       "35587  0.297 -0.347  0.727  0.160  0.342  0.204  0.161  \n",
       "32681  0.675 -0.073  1.190  0.463  0.616  0.477 -0.087  \n",
       "40971  0.104 -0.542  0.479 -0.067  0.185  0.040 -0.056  \n",
       "21022  0.386 -0.542  0.857  0.166  0.324  0.199  0.198  \n",
       "6403   0.149 -0.366  0.480 -1.232  0.336 -1.012 -0.228  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:18.715638Z",
     "iopub.status.busy": "2021-10-20T16:37:18.715490Z",
     "iopub.status.idle": "2021-10-20T16:37:18.719327Z",
     "shell.execute_reply": "2021-10-20T16:37:18.718447Z",
     "shell.execute_reply.started": "2021-10-20T16:37:18.715618Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:18.720484Z",
     "iopub.status.busy": "2021-10-20T16:37:18.720329Z",
     "iopub.status.idle": "2021-10-20T16:37:18.725381Z",
     "shell.execute_reply": "2021-10-20T16:37:18.724754Z",
     "shell.execute_reply.started": "2021-10-20T16:37:18.720464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[0].weight_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T16:37:18.726306Z",
     "iopub.status.busy": "2021-10-20T16:37:18.726150Z",
     "iopub.status.idle": "2021-10-20T17:14:33.283566Z",
     "shell.execute_reply": "2021-10-20T17:14:33.282677Z",
     "shell.execute_reply.started": "2021-10-20T16:37:18.726287Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------- TRAINING INTERPRETATION NET -----------------------------------------------\n",
      "Epoch 1/2000\n",
      "176/176 [==============================] - 22s 109ms/step - loss: 0.4124 - r2_inet_lambda_fv_loss: 6944.6868 - val_loss: 0.2050 - val_r2_inet_lambda_fv_loss: 105.4875\n",
      "Epoch 2/2000\n",
      "176/176 [==============================] - 16s 93ms/step - loss: 0.2114 - r2_inet_lambda_fv_loss: 1806.0803 - val_loss: 0.1943 - val_r2_inet_lambda_fv_loss: 58.7493\n",
      "Epoch 3/2000\n",
      "176/176 [==============================] - 17s 98ms/step - loss: 0.1771 - r2_inet_lambda_fv_loss: 38.9518 - val_loss: 0.1275 - val_r2_inet_lambda_fv_loss: 3.3631\n",
      "Epoch 4/2000\n",
      "176/176 [==============================] - 17s 99ms/step - loss: 0.1334 - r2_inet_lambda_fv_loss: 7.3986 - val_loss: 0.1249 - val_r2_inet_lambda_fv_loss: 3.3056\n",
      "Epoch 5/2000\n",
      "176/176 [==============================] - 19s 106ms/step - loss: 0.1297 - r2_inet_lambda_fv_loss: 4.5248 - val_loss: 0.1233 - val_r2_inet_lambda_fv_loss: 3.3007\n",
      "Epoch 6/2000\n",
      "176/176 [==============================] - 18s 105ms/step - loss: 0.1239 - r2_inet_lambda_fv_loss: 9.0292 - val_loss: 0.1179 - val_r2_inet_lambda_fv_loss: 2.8293\n",
      "Epoch 7/2000\n",
      "176/176 [==============================] - 18s 104ms/step - loss: 0.1213 - r2_inet_lambda_fv_loss: 5.6287 - val_loss: 0.1143 - val_r2_inet_lambda_fv_loss: 2.3346\n",
      "Epoch 8/2000\n",
      "176/176 [==============================] - 19s 106ms/step - loss: 0.1165 - r2_inet_lambda_fv_loss: 5.0657 - val_loss: 0.1111 - val_r2_inet_lambda_fv_loss: 2.0010\n",
      "Epoch 9/2000\n",
      "176/176 [==============================] - 18s 105ms/step - loss: 0.1134 - r2_inet_lambda_fv_loss: 5.7059 - val_loss: 0.1103 - val_r2_inet_lambda_fv_loss: 1.9305\n",
      "Epoch 10/2000\n",
      "176/176 [==============================] - 25s 143ms/step - loss: 0.1130 - r2_inet_lambda_fv_loss: 10.5687 - val_loss: 0.1110 - val_r2_inet_lambda_fv_loss: 2.0606\n",
      "Epoch 11/2000\n",
      "176/176 [==============================] - 22s 127ms/step - loss: 0.1119 - r2_inet_lambda_fv_loss: 3.8871 - val_loss: 0.1095 - val_r2_inet_lambda_fv_loss: 1.9001\n",
      "Epoch 12/2000\n",
      "176/176 [==============================] - 24s 136ms/step - loss: 0.1130 - r2_inet_lambda_fv_loss: 4.3913 - val_loss: 0.1103 - val_r2_inet_lambda_fv_loss: 1.7637\n",
      "Epoch 13/2000\n",
      "176/176 [==============================] - 24s 139ms/step - loss: 0.1126 - r2_inet_lambda_fv_loss: 3.5409 - val_loss: 0.1097 - val_r2_inet_lambda_fv_loss: 1.6824\n",
      "Epoch 14/2000\n",
      "176/176 [==============================] - 25s 141ms/step - loss: 0.1120 - r2_inet_lambda_fv_loss: 3.3791 - val_loss: 0.1097 - val_r2_inet_lambda_fv_loss: 1.5054\n",
      "Epoch 15/2000\n",
      "176/176 [==============================] - 25s 141ms/step - loss: 0.1104 - r2_inet_lambda_fv_loss: 1.8874 - val_loss: 0.1085 - val_r2_inet_lambda_fv_loss: 1.3561\n",
      "Epoch 16/2000\n",
      "176/176 [==============================] - 25s 141ms/step - loss: 0.1095 - r2_inet_lambda_fv_loss: 3.7708 - val_loss: 0.1079 - val_r2_inet_lambda_fv_loss: 1.3102\n",
      "Epoch 17/2000\n",
      "176/176 [==============================] - 24s 139ms/step - loss: 0.1100 - r2_inet_lambda_fv_loss: 2.5244 - val_loss: 0.1070 - val_r2_inet_lambda_fv_loss: 1.2591\n",
      "Epoch 18/2000\n",
      "176/176 [==============================] - 25s 143ms/step - loss: 0.1080 - r2_inet_lambda_fv_loss: 1.6584 - val_loss: 0.1064 - val_r2_inet_lambda_fv_loss: 1.2148\n",
      "Epoch 19/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1093 - r2_inet_lambda_fv_loss: 1.5742 - val_loss: 0.1060 - val_r2_inet_lambda_fv_loss: 1.2160\n",
      "Epoch 20/2000\n",
      "176/176 [==============================] - 24s 137ms/step - loss: 0.1076 - r2_inet_lambda_fv_loss: 3.9158 - val_loss: 0.1048 - val_r2_inet_lambda_fv_loss: 1.1507\n",
      "Epoch 21/2000\n",
      "176/176 [==============================] - 25s 141ms/step - loss: 0.1068 - r2_inet_lambda_fv_loss: 1.5991 - val_loss: 0.1055 - val_r2_inet_lambda_fv_loss: 1.4170\n",
      "Epoch 22/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1066 - r2_inet_lambda_fv_loss: 1.6200 - val_loss: 0.1048 - val_r2_inet_lambda_fv_loss: 1.1359\n",
      "Epoch 23/2000\n",
      "176/176 [==============================] - 25s 141ms/step - loss: 0.1081 - r2_inet_lambda_fv_loss: 1.4634 - val_loss: 0.1043 - val_r2_inet_lambda_fv_loss: 1.1342\n",
      "Epoch 24/2000\n",
      "176/176 [==============================] - 24s 138ms/step - loss: 0.1075 - r2_inet_lambda_fv_loss: 4.2641 - val_loss: 0.1049 - val_r2_inet_lambda_fv_loss: 1.1441\n",
      "Epoch 25/2000\n",
      "176/176 [==============================] - 25s 143ms/step - loss: 0.1074 - r2_inet_lambda_fv_loss: 9.7862 - val_loss: 0.1043 - val_r2_inet_lambda_fv_loss: 1.0450\n",
      "Epoch 26/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1068 - r2_inet_lambda_fv_loss: 1.8676 - val_loss: 0.1045 - val_r2_inet_lambda_fv_loss: 1.1348\n",
      "Epoch 27/2000\n",
      "176/176 [==============================] - 25s 143ms/step - loss: 0.1098 - r2_inet_lambda_fv_loss: 3.0510 - val_loss: 0.1055 - val_r2_inet_lambda_fv_loss: 0.8970\n",
      "Epoch 28/2000\n",
      "176/176 [==============================] - 24s 137ms/step - loss: 0.1099 - r2_inet_lambda_fv_loss: 1.6670 - val_loss: 0.1033 - val_r2_inet_lambda_fv_loss: 0.8954\n",
      "Epoch 29/2000\n",
      "176/176 [==============================] - 25s 142ms/step - loss: 0.1087 - r2_inet_lambda_fv_loss: 3.0957 - val_loss: 0.1029 - val_r2_inet_lambda_fv_loss: 0.8821\n",
      "Epoch 30/2000\n",
      "176/176 [==============================] - 24s 135ms/step - loss: 0.1073 - r2_inet_lambda_fv_loss: 1.6693 - val_loss: 0.1031 - val_r2_inet_lambda_fv_loss: 1.0002\n",
      "Epoch 31/2000\n",
      "176/176 [==============================] - 24s 137ms/step - loss: 0.1066 - r2_inet_lambda_fv_loss: 1.8403 - val_loss: 0.1031 - val_r2_inet_lambda_fv_loss: 1.3626\n",
      "Epoch 32/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1071 - r2_inet_lambda_fv_loss: 2.6352 - val_loss: 0.1005 - val_r2_inet_lambda_fv_loss: 0.9103\n",
      "Epoch 33/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1056 - r2_inet_lambda_fv_loss: 2.4661 - val_loss: 0.1014 - val_r2_inet_lambda_fv_loss: 1.0622\n",
      "Epoch 34/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1053 - r2_inet_lambda_fv_loss: 2.0310 - val_loss: 0.1017 - val_r2_inet_lambda_fv_loss: 0.9760\n",
      "Epoch 35/2000\n",
      "176/176 [==============================] - 25s 142ms/step - loss: 0.1066 - r2_inet_lambda_fv_loss: 2.7487 - val_loss: 0.1031 - val_r2_inet_lambda_fv_loss: 1.0997\n",
      "Epoch 36/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1073 - r2_inet_lambda_fv_loss: 4.8888 - val_loss: 0.1047 - val_r2_inet_lambda_fv_loss: 1.3131\n",
      "Epoch 37/2000\n",
      "176/176 [==============================] - 24s 139ms/step - loss: 0.1068 - r2_inet_lambda_fv_loss: 1.6969 - val_loss: 0.1030 - val_r2_inet_lambda_fv_loss: 1.0656\n",
      "Epoch 38/2000\n",
      "176/176 [==============================] - 24s 136ms/step - loss: 0.1064 - r2_inet_lambda_fv_loss: 5.0317 - val_loss: 0.1027 - val_r2_inet_lambda_fv_loss: 1.1389\n",
      "Epoch 39/2000\n",
      "176/176 [==============================] - 24s 137ms/step - loss: 0.1060 - r2_inet_lambda_fv_loss: 1.6450 - val_loss: 0.1023 - val_r2_inet_lambda_fv_loss: 1.0335\n",
      "Epoch 40/2000\n",
      "176/176 [==============================] - 24s 137ms/step - loss: 0.1060 - r2_inet_lambda_fv_loss: 1.4710 - val_loss: 0.1010 - val_r2_inet_lambda_fv_loss: 0.7991\n",
      "Epoch 41/2000\n",
      "176/176 [==============================] - 24s 134ms/step - loss: 0.1040 - r2_inet_lambda_fv_loss: 1.9544 - val_loss: 0.1005 - val_r2_inet_lambda_fv_loss: 0.8551\n",
      "Epoch 42/2000\n",
      "176/176 [==============================] - 24s 137ms/step - loss: 0.1022 - r2_inet_lambda_fv_loss: 1.6867 - val_loss: 0.0958 - val_r2_inet_lambda_fv_loss: 0.4654\n",
      "Epoch 43/2000\n",
      "176/176 [==============================] - 24s 137ms/step - loss: 0.0975 - r2_inet_lambda_fv_loss: 1.5034 - val_loss: 0.0944 - val_r2_inet_lambda_fv_loss: 0.5191\n",
      "Epoch 44/2000\n",
      "176/176 [==============================] - 24s 138ms/step - loss: 0.0980 - r2_inet_lambda_fv_loss: 1.6125 - val_loss: 0.0962 - val_r2_inet_lambda_fv_loss: 0.4982\n",
      "Epoch 45/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1001 - r2_inet_lambda_fv_loss: 1.6032 - val_loss: 0.0928 - val_r2_inet_lambda_fv_loss: 0.4158\n",
      "Epoch 46/2000\n",
      "176/176 [==============================] - 25s 143ms/step - loss: 0.0961 - r2_inet_lambda_fv_loss: 2.4947 - val_loss: 0.0969 - val_r2_inet_lambda_fv_loss: 0.6446\n",
      "Epoch 47/2000\n",
      "176/176 [==============================] - 25s 141ms/step - loss: 0.1013 - r2_inet_lambda_fv_loss: 1.5218 - val_loss: 0.0989 - val_r2_inet_lambda_fv_loss: 0.8757\n",
      "Epoch 48/2000\n",
      "176/176 [==============================] - 25s 141ms/step - loss: 0.1023 - r2_inet_lambda_fv_loss: 1.5437 - val_loss: 0.0984 - val_r2_inet_lambda_fv_loss: 0.8111\n",
      "Epoch 49/2000\n",
      "176/176 [==============================] - 26s 146ms/step - loss: 0.1009 - r2_inet_lambda_fv_loss: 2.2167 - val_loss: 0.0969 - val_r2_inet_lambda_fv_loss: 0.6321\n",
      "Epoch 50/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1009 - r2_inet_lambda_fv_loss: 1.8302 - val_loss: 0.0991 - val_r2_inet_lambda_fv_loss: 0.8090\n",
      "Epoch 51/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1028 - r2_inet_lambda_fv_loss: 1.8034 - val_loss: 0.0998 - val_r2_inet_lambda_fv_loss: 0.9140\n",
      "Epoch 52/2000\n",
      "176/176 [==============================] - 25s 139ms/step - loss: 0.1038 - r2_inet_lambda_fv_loss: 3.5329 - val_loss: 0.1039 - val_r2_inet_lambda_fv_loss: 1.0647\n",
      "Epoch 53/2000\n",
      "176/176 [==============================] - 25s 144ms/step - loss: 0.1072 - r2_inet_lambda_fv_loss: 2.3063 - val_loss: 0.1071 - val_r2_inet_lambda_fv_loss: 1.2315\n",
      "Epoch 54/2000\n",
      "176/176 [==============================] - 25s 140ms/step - loss: 0.1111 - r2_inet_lambda_fv_loss: 4.9714 - val_loss: 0.1076 - val_r2_inet_lambda_fv_loss: 1.3848\n",
      "Epoch 55/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.1143 - r2_inet_lambda_fv_loss: 2.2177 - val_loss: 0.1128 - val_r2_inet_lambda_fv_loss: 1.4976\n",
      "Epoch 56/2000\n",
      "176/176 [==============================] - 23s 130ms/step - loss: 0.1163 - r2_inet_lambda_fv_loss: 2.5338 - val_loss: 0.1146 - val_r2_inet_lambda_fv_loss: 1.6535\n",
      "Epoch 57/2000\n",
      "176/176 [==============================] - 24s 135ms/step - loss: 0.1187 - r2_inet_lambda_fv_loss: 2.6727 - val_loss: 0.1169 - val_r2_inet_lambda_fv_loss: 1.7766\n",
      "Epoch 58/2000\n",
      "176/176 [==============================] - 24s 137ms/step - loss: 0.1216 - r2_inet_lambda_fv_loss: 8.3631 - val_loss: 0.1207 - val_r2_inet_lambda_fv_loss: 1.8879\n",
      "Epoch 59/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.1264 - r2_inet_lambda_fv_loss: 3.0322 - val_loss: 0.1213 - val_r2_inet_lambda_fv_loss: 1.9870\n",
      "Epoch 60/2000\n",
      "176/176 [==============================] - 23s 130ms/step - loss: 0.1257 - r2_inet_lambda_fv_loss: 5.6321 - val_loss: 0.1201 - val_r2_inet_lambda_fv_loss: 1.8798\n",
      "Epoch 61/2000\n",
      "176/176 [==============================] - 23s 130ms/step - loss: 0.1265 - r2_inet_lambda_fv_loss: 3.3900 - val_loss: 0.1264 - val_r2_inet_lambda_fv_loss: 2.3275\n",
      "Epoch 62/2000\n",
      "176/176 [==============================] - 24s 137ms/step - loss: 0.1321 - r2_inet_lambda_fv_loss: 4.3423 - val_loss: 0.1262 - val_r2_inet_lambda_fv_loss: 2.2281\n",
      "Epoch 63/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.1320 - r2_inet_lambda_fv_loss: 4.0293 - val_loss: 0.1275 - val_r2_inet_lambda_fv_loss: 2.4250\n",
      "Epoch 64/2000\n",
      "176/176 [==============================] - 24s 135ms/step - loss: 0.1339 - r2_inet_lambda_fv_loss: 3.9013 - val_loss: 0.1302 - val_r2_inet_lambda_fv_loss: 2.5001\n",
      "Epoch 65/2000\n",
      "176/176 [==============================] - 24s 134ms/step - loss: 0.1390 - r2_inet_lambda_fv_loss: 4.0717 - val_loss: 0.1315 - val_r2_inet_lambda_fv_loss: 2.5726\n",
      "Epoch 66/2000\n",
      "176/176 [==============================] - 24s 134ms/step - loss: 0.1406 - r2_inet_lambda_fv_loss: 4.3902 - val_loss: 0.1438 - val_r2_inet_lambda_fv_loss: 3.9675\n",
      "Epoch 67/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.1497 - r2_inet_lambda_fv_loss: 5.7329 - val_loss: 0.1515 - val_r2_inet_lambda_fv_loss: 4.8255\n",
      "Epoch 68/2000\n",
      "176/176 [==============================] - 24s 135ms/step - loss: 0.1617 - r2_inet_lambda_fv_loss: 34.5403 - val_loss: 0.1695 - val_r2_inet_lambda_fv_loss: 24.5709\n",
      "Epoch 69/2000\n",
      "176/176 [==============================] - 24s 134ms/step - loss: 0.1767 - r2_inet_lambda_fv_loss: 428.0795 - val_loss: 0.1780 - val_r2_inet_lambda_fv_loss: 50.9575\n",
      "Epoch 70/2000\n",
      "176/176 [==============================] - 24s 137ms/step - loss: 0.1965 - r2_inet_lambda_fv_loss: 659.2824 - val_loss: 0.2017 - val_r2_inet_lambda_fv_loss: 103.9991\n",
      "Epoch 71/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.2073 - r2_inet_lambda_fv_loss: 3307.5985 - val_loss: 0.1966 - val_r2_inet_lambda_fv_loss: 103.6806\n",
      "Epoch 72/2000\n",
      "176/176 [==============================] - 23s 134ms/step - loss: 0.2004 - r2_inet_lambda_fv_loss: 4150.8702 - val_loss: 0.1995 - val_r2_inet_lambda_fv_loss: 103.1867\n",
      "Epoch 73/2000\n",
      "176/176 [==============================] - 23s 134ms/step - loss: 0.2072 - r2_inet_lambda_fv_loss: 5076.2050 - val_loss: 0.2040 - val_r2_inet_lambda_fv_loss: 110.0282\n",
      "Epoch 74/2000\n",
      "176/176 [==============================] - 24s 136ms/step - loss: 0.2118 - r2_inet_lambda_fv_loss: 6849.4901 - val_loss: 0.2085 - val_r2_inet_lambda_fv_loss: 112.5994\n",
      "Epoch 75/2000\n",
      "176/176 [==============================] - 24s 135ms/step - loss: 0.2141 - r2_inet_lambda_fv_loss: 4566.7771 - val_loss: 0.2131 - val_r2_inet_lambda_fv_loss: 106.9633\n",
      "Epoch 76/2000\n",
      "176/176 [==============================] - 23s 132ms/step - loss: 0.2193 - r2_inet_lambda_fv_loss: 3359.9141 - val_loss: 0.2220 - val_r2_inet_lambda_fv_loss: 107.3066\n",
      "Epoch 77/2000\n",
      "176/176 [==============================] - 24s 135ms/step - loss: 0.2259 - r2_inet_lambda_fv_loss: 5599.2929 - val_loss: 0.2196 - val_r2_inet_lambda_fv_loss: 105.8185\n",
      "Epoch 78/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.2242 - r2_inet_lambda_fv_loss: 2926.5624 - val_loss: 0.2214 - val_r2_inet_lambda_fv_loss: 105.5623\n",
      "Epoch 79/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.2241 - r2_inet_lambda_fv_loss: 8575.5106 - val_loss: 0.2213 - val_r2_inet_lambda_fv_loss: 106.0593\n",
      "Epoch 80/2000\n",
      "176/176 [==============================] - 23s 131ms/step - loss: 0.2279 - r2_inet_lambda_fv_loss: 2979.6513 - val_loss: 0.2273 - val_r2_inet_lambda_fv_loss: 109.7431\n",
      "Epoch 81/2000\n",
      "176/176 [==============================] - 23s 134ms/step - loss: 0.2340 - r2_inet_lambda_fv_loss: 6002.3939 - val_loss: 0.2325 - val_r2_inet_lambda_fv_loss: 109.7600\n",
      "Epoch 82/2000\n",
      "176/176 [==============================] - 24s 135ms/step - loss: 0.2360 - r2_inet_lambda_fv_loss: 6613.4694 - val_loss: 0.2282 - val_r2_inet_lambda_fv_loss: 105.8769\n",
      "Epoch 83/2000\n",
      "176/176 [==============================] - 24s 136ms/step - loss: 0.2332 - r2_inet_lambda_fv_loss: 4112.0489 - val_loss: 0.2254 - val_r2_inet_lambda_fv_loss: 107.7131\n",
      "Epoch 84/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.2287 - r2_inet_lambda_fv_loss: 2086.5413 - val_loss: 0.2219 - val_r2_inet_lambda_fv_loss: 105.3704\n",
      "Epoch 85/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.2259 - r2_inet_lambda_fv_loss: 3166.8349 - val_loss: 0.2234 - val_r2_inet_lambda_fv_loss: 107.5192\n",
      "Epoch 86/2000\n",
      "176/176 [==============================] - 23s 131ms/step - loss: 0.2272 - r2_inet_lambda_fv_loss: 4959.5810 - val_loss: 0.2203 - val_r2_inet_lambda_fv_loss: 105.9525\n",
      "Epoch 87/2000\n",
      "176/176 [==============================] - 23s 130ms/step - loss: 0.2265 - r2_inet_lambda_fv_loss: 9659.2566 - val_loss: 0.2231 - val_r2_inet_lambda_fv_loss: 109.2882\n",
      "Epoch 88/2000\n",
      "176/176 [==============================] - 23s 131ms/step - loss: 0.2276 - r2_inet_lambda_fv_loss: 2085.6962 - val_loss: 0.2210 - val_r2_inet_lambda_fv_loss: 108.6321\n",
      "Epoch 89/2000\n",
      "176/176 [==============================] - 24s 135ms/step - loss: 0.2267 - r2_inet_lambda_fv_loss: 1838.9370 - val_loss: 0.2208 - val_r2_inet_lambda_fv_loss: 108.6851\n",
      "Epoch 90/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.2258 - r2_inet_lambda_fv_loss: 8633.0570 - val_loss: 0.2205 - val_r2_inet_lambda_fv_loss: 108.3754\n",
      "Epoch 91/2000\n",
      "176/176 [==============================] - 24s 134ms/step - loss: 0.2239 - r2_inet_lambda_fv_loss: 2443.6442 - val_loss: 0.2229 - val_r2_inet_lambda_fv_loss: 108.0913\n",
      "Epoch 92/2000\n",
      "176/176 [==============================] - 23s 129ms/step - loss: 0.2281 - r2_inet_lambda_fv_loss: 11528.7068 - val_loss: 0.2188 - val_r2_inet_lambda_fv_loss: 107.9830\n",
      "Epoch 93/2000\n",
      "176/176 [==============================] - 23s 133ms/step - loss: 0.2239 - r2_inet_lambda_fv_loss: 2593.2532 - val_loss: 0.2230 - val_r2_inet_lambda_fv_loss: 108.2185\n",
      "Epoch 94/2000\n",
      "176/176 [==============================] - 24s 135ms/step - loss: 0.2273 - r2_inet_lambda_fv_loss: 11120.8862 - val_loss: 0.2192 - val_r2_inet_lambda_fv_loss: 107.8019\n",
      "Epoch 95/2000\n",
      "176/176 [==============================] - 23s 131ms/step - loss: 0.2258 - r2_inet_lambda_fv_loss: 708.1455 - val_loss: 0.2236 - val_r2_inet_lambda_fv_loss: 108.3640\n",
      "Training Time: 0:37:13\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "((X_valid_list, y_valid_list), \n",
    " (X_test_list, y_test_list),\n",
    " history_list, \n",
    "\n",
    " #scores_valid_list,\n",
    " #scores_test_list, \n",
    "\n",
    " #function_values_valid_list, \n",
    " #function_values_test_list, \n",
    "\n",
    " #polynomial_dict_valid_list,\n",
    " #polynomial_dict_test_list,\n",
    "\n",
    " #distrib_dict_valid_list,\n",
    " #distrib_dict_test_list,\n",
    "\n",
    " model_list) = interpretation_net_training(lambda_net_train_dataset_list, \n",
    "                                           lambda_net_valid_dataset_list, \n",
    "                                           lambda_net_test_dataset_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T17:14:33.285807Z",
     "iopub.status.busy": "2021-10-20T17:14:33.285569Z",
     "iopub.status.idle": "2021-10-20T17:14:33.294633Z",
     "shell.execute_reply": "2021-10-20T17:14:33.293023Z",
     "shell.execute_reply.started": "2021-10-20T17:14:33.285775Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 61)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden1_2048 (Dense)            (None, 2048)         126976      input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation1_relu (Activation)   (None, 2048)         0           hidden1_2048[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_coeff_2 (Dense)          (None, 2)            4098        activation1_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier1_4 (Dense)    (None, 4)            8196        activation1_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier2_4 (Dense)    (None, 4)            8196        activation1_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_combined (Concatenate)   (None, 10)           0           output_coeff_2[0][0]             \n",
      "                                                                 output_identifier1_4[0][0]       \n",
      "                                                                 output_identifier2_4[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 147,466\n",
      "Trainable params: 147,466\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_list[-1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T17:14:33.295836Z",
     "iopub.status.busy": "2021-10-20T17:14:33.295606Z",
     "iopub.status.idle": "2021-10-20T17:14:33.314379Z",
     "shell.execute_reply": "2021-10-20T17:14:33.313851Z",
     "shell.execute_reply.started": "2021-10-20T17:14:33.295805Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#polynomial_dict_valid_list = []\n",
    "polynomial_dict_test_list = []  \n",
    "\n",
    "\n",
    "for lambda_net_valid_dataset, lambda_net_test_dataset in zip(lambda_net_valid_dataset_list, lambda_net_test_dataset_list):\n",
    "\n",
    "    #polynomial_dict_valid = {'lstsq_lambda_pred_polynomials': lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "    #                        'lstsq_target_polynomials': lambda_net_valid_dataset.lstsq_target_polynomial_list,\n",
    "    #                        'target_polynomials': lambda_net_valid_dataset.target_polynomial_list}    \n",
    "\n",
    "    polynomial_dict_test = {'lstsq_lambda_pred_polynomials': lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "                            'lstsq_target_polynomials': lambda_net_test_dataset.lstsq_target_polynomial_list,\n",
    "                            'target_polynomials': lambda_net_test_dataset.target_polynomial_list}    \n",
    "\n",
    "    #polynomial_dict_valid_list.append(polynomial_dict_valid)  \n",
    "    polynomial_dict_test_list.append(polynomial_dict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T17:14:33.315362Z",
     "iopub.status.busy": "2021-10-20T17:14:33.315176Z",
     "iopub.status.idle": "2021-10-20T17:14:33.488660Z",
     "shell.execute_reply": "2021-10-20T17:14:33.488086Z",
     "shell.execute_reply.started": "2021-10-20T17:14:33.315342Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------- PREDICT INET ------------------------------------------------------\n",
      "Predict Time: 0:00:00\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------- PREDICT INET ------------------------------------------------------')\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "for i, (X_test, model) in enumerate(zip(X_test_list, model_list)):\n",
    "    #y_test_pred = model.predict(X_test)    \n",
    "    #print(model.summary())\n",
    "    #print(X_test.shape)\n",
    "    y_test_pred = make_inet_prediction(model, X_test, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    #print(y_test_pred.shape)   \n",
    "    polynomial_dict_test_list[i]['inet_polynomials'] = y_test_pred\n",
    "\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Predict Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T17:14:33.489724Z",
     "iopub.status.busy": "2021-10-20T17:14:33.489519Z",
     "iopub.status.idle": "2021-10-20T17:14:33.496416Z",
     "shell.execute_reply": "2021-10-20T17:14:33.495099Z",
     "shell.execute_reply.started": "2021-10-20T17:14:33.489701Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_poly_evaluation:\n",
    "    print('-------------------------------------------------- CALCULATE METAMODEL POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_poly'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Poly Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-10-21T07:43:37.798636Z",
     "iopub.status.busy": "2021-10-21T07:43:37.798167Z",
     "iopub.status.idle": "2021-10-21T08:42:48.664131Z",
     "shell.execute_reply": "2021-10-21T08:42:48.663186Z",
     "shell.execute_reply.started": "2021-10-21T07:43:37.798585Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------- CALCULATE METAMODEL --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "---- Tuning the basis functions ----\n",
      "---- Tuning the basis functions ----\n",
      "---- Tuning the basis functions ----\n",
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.459\n",
      "Iteration: 1 \t--- Loss: 0.435\n",
      "Iteration: 2 \t--- Loss: 0.448\n",
      "Iteration: 3 \t--- Loss: 0.396\n",
      "Iteration: 4 \t--- Loss: 0.341\n",
      "Iteration: 5 \t--- Loss: 0.333\n",
      "Iteration: 6 \t--- Loss: 0.364\n",
      "Iteration: 7 \t--- Loss: 0.300\n",
      "Iteration: 8 \t--- Loss: 0.313\n",
      "Iteration: 9 \t--- Loss: 0.289\n",
      "Iteration: 10 \t--- Loss: 0.352\n",
      "Iteration: 11 \t--- Loss: 0.262\n",
      "Iteration: 12 \t--- Loss: 0.261\n",
      "Iteration: 13 \t--- Loss: 0.244\n",
      "Iteration: 14 \t--- Loss: 0.263\n",
      "Iteration: 15 \t--- Loss: 0.256\n",
      "Iteration: 16 \t--- Loss: 0.240\n",
      "Iteration: 17 \t--- Loss: 0.250\n",
      "Iteration: 18 \t--- Loss: 0.251\n",
      "Iteration: 19 \t--- Loss: 0.247\n",
      "Iteration: 20 \t--- Loss: 0.249\n",
      "Iteration: 21 \t--- Loss: 0.282\n",
      "Iteration: 22 \t--- Loss: 0.246\n",
      "Iteration: 23 \t--- Loss: 0.272\n",
      "Iteration: 24 \t--- Loss: 0.216\n",
      "Iteration: 25 \t--- Loss: 0.231\n",
      "Iteration: 26 \t--- Loss: 0.223\n",
      "Iteration: 27 \t--- Loss: 0.221\n",
      "Iteration: 28 \t--- Loss: 0.249\n",
      "Iteration: 29 \t--- Loss: 0.252\n",
      "Iteration: 30 \t--- Loss: 0.239\n",
      "Iteration: 31 \t--- Loss: 0.196\n",
      "Iteration: 32 \t--- Loss: 0.238\n",
      "Iteration: 33 \t--- Loss: 0.214\n",
      "Iteration: 34 \t--- Loss: 0.222\n",
      "Iteration: 35 \t--- Loss: 0.224\n",
      "Iteration: 36 \t--- Loss: 0.224\n",
      "Iteration: 37 \t--- Loss: 0.245\n",
      "Iteration: 38 \t--- Loss: 0.246\n",
      "Iteration: 39 \t--- Loss: 0.215\n",
      "Iteration: 40 \t--- Loss: 0.217\n",
      "Iteration: 41 \t--- Loss: 0.230\n",
      "Iteration: 42 \t--- Loss: 0.224\n",
      "Iteration: 43 \t--- Loss: 0.226\n",
      "Iteration: 44 \t--- Loss: 0.230\n",
      "Iteration: 45 \t--- Loss: 0.242\n",
      "Iteration: 46 \t--- Loss: 0.229\n",
      "Iteration: 47 \t--- Loss: 0.224\n",
      "Iteration: 48 \t--- Loss: 0.249\n",
      "Iteration: 49 \t--- Loss: 0.208\n",
      "Iteration: 50 \t--- Loss: 0.225\n",
      "Iteration: 51 \t--- Loss: 0.217\n",
      "Iteration: 52 \t--- Loss: 0.227\n",
      "Iteration: 53 \t--- Loss: 0.212\n",
      "Iteration: 54 \t--- Loss: 0.237\n",
      "Iteration: 55 \t--- Loss: 0.220\n",
      "Iteration: 56 \t--- Loss: 0.229\n",
      "Iteration: 57 \t--- Loss: 0.196\n",
      "Iteration: 58 \t--- Loss: 0.199\n",
      "Iteration: 59 \t--- Loss: 0.230\n",
      "Iteration: 60 \t--- Loss: 0.231\n",
      "Iteration: 61 \t--- Loss: 0.211\n",
      "Iteration: 62 \t--- Loss: 0.212\n",
      "Iteration: 63 \t--- Loss: 0.231\n",
      "Iteration: 64 \t--- Loss: 0.203\n",
      "Iteration: 65 \t--- Loss: 0.217\n",
      "Iteration: 66 \t--- Loss: 0.235\n",
      "Iteration: 67 \t--- Loss: 0.234\n",
      "Iteration: 68 \t--- Loss: 0.205\n",
      "Iteration: 69 \t--- Loss: 0.226\n",
      "Iteration: 70 \t--- Loss: 0.209\n",
      "Iteration: 71 \t--- Loss: 0.220\n",
      "Iteration: 72 \t--- Loss: 0.220\n",
      "Iteration: 73 \t--- Loss: 0.195\n",
      "Iteration: 74 \t--- Loss: 0.228\n",
      "Iteration: 75 \t--- Loss: 0.193\n",
      "Iteration: 76 \t--- Loss: 0.224\n",
      "Iteration: 77 \t--- Loss: 0.216\n",
      "Iteration: 78 \t--- Loss: 0.216\n",
      "Iteration: 79 \t--- Loss: 0.205\n",
      "Iteration: 80 \t--- Loss: 0.214\n",
      "Iteration: 81 \t--- Loss: 0.228\n",
      "Iteration: 82 \t--- Loss: 0.232\n",
      "Iteration: 83 \t--- Loss: 0.197\n",
      "Iteration: 84 \t--- Loss: 0.219\n",
      "Iteration: 85 \t--- Loss: 0.227\n",
      "Iteration: 86 \t--- Loss: 0.214\n",
      "Iteration: 87 \t--- Loss: 0.206\n",
      "Iteration: 88 \t--- Loss: 0.199\n",
      "Iteration: 89 \t--- Loss: 0.226\n",
      "Iteration: 90 \t--- Loss: 0.210\n",
      "Iteration: 91 \t--- Loss: 0.214\n",
      "Iteration: 92 \t--- Loss: 0.263\n",
      "Iteration: 93 \t--- Loss: 0.209\n",
      "Iteration: 94 \t--- Loss: 0.219\n",
      "Iteration: 95 \t--- Loss: 0.245\n",
      "Iteration: 96 \t--- Loss: 0.230\n",
      "Iteration: 97 \t--- Loss: 0.217\n",
      "Iteration: 98 \t--- Loss: 0.217\n",
      "Iteration: 99 \t--- Loss: 0.230\n",
      "Iteration: 100 \t--- Loss: 0.213\n",
      "Iteration: 101 \t--- Loss: 0.213\n",
      "Iteration: 102 \t--- Loss: 0.221\n",
      "Iteration: 103 \t--- Loss: 0.201\n",
      "Iteration: 104 \t--- Loss: 0.238\n",
      "Iteration: 105 \t--- Loss: 0.236\n",
      "Iteration: 106 \t--- Loss: 0.233\n",
      "Iteration: 107 \t--- Loss: 0.214\n",
      "Iteration: 108 \t--- Loss: 0.209\n",
      "Iteration: 109 \t--- Loss: 0.242\n",
      "Iteration: 110 \t--- Loss: 0.191\n",
      "Iteration: 111 \t--- Loss: 0.234\n",
      "Iteration: 112 \t--- Loss: 0.217\n",
      "Iteration: 113 \t--- Loss: 0.232\n",
      "Iteration: 114 \t--- Loss: 0.227\n",
      "Iteration: 115 \t--- Loss: 0.228\n",
      "Iteration: 116 \t--- Loss: 0.232\n",
      "Iteration: 117 \t--- Loss: 0.225\n",
      "Iteration: 118 \t--- Loss: 0.228\n",
      "Iteration: 119 \t--- Loss: 0.210\n",
      "Iteration: 120 \t--- Loss: 0.217\n",
      "Iteration: 121 \t--- Loss: 0.195\n",
      "Iteration: 122 \t--- Loss: 0.185\n",
      "Iteration: 123 \t--- Loss: 0.219\n",
      "Iteration: 124 \t--- Loss: 0.204\n",
      "Iteration: 125 \t--- Loss: 0.202\n",
      "Iteration: 126 \t--- Loss: 0.220\n",
      "Iteration: 127 \t--- Loss: 0.217\n",
      "Iteration: 128 \t--- Loss: 0.220\n",
      "Iteration: 129 \t--- Loss: 0.241\n",
      "Iteration: 130 \t--- Loss: 0.247\n",
      "Iteration: 131 \t--- Loss: 0.225\n",
      "Iteration: 132 \t--- Loss: 0.202\n",
      "Iteration: 133 \t--- Loss: 0.219\n",
      "Iteration: 134 \t--- Loss: 0.233\n",
      "Iteration: 135 \t--- Loss: 0.219\n",
      "Iteration: 136 \t--- Loss: 0.223\n",
      "Iteration: 137 \t--- Loss: 0.226\n",
      "Iteration: 138 \t--- Loss: 0.214\n",
      "Iteration: 139 \t--- Loss: 0.216\n",
      "Iteration: 140 \t--- Loss: 0.204\n",
      "Iteration: 141 \t--- Loss: 0.218\n",
      "Iteration: 142 \t--- Loss: 0.192\n",
      "Iteration: 143 \t--- Loss: 0.186\n",
      "Iteration: 144 \t--- Loss: 0.218\n",
      "Iteration: 145 \t--- Loss: 0.201\n",
      "Iteration: 146 \t--- Loss: 0.207\n",
      "Iteration: 147 \t--- Loss: 0.210\n",
      "Iteration: 148 \t--- Loss: 0.213\n",
      "Iteration: 149 \t--- Loss: 0.208\n",
      "Iteration: 150 \t--- Loss: 0.214\n",
      "Iteration: 151 \t--- Loss: 0.221\n",
      "Iteration: 152 \t--- Loss: 0.224\n",
      "Iteration: 153 \t--- Loss: 0.213\n",
      "Iteration: 154 \t--- Loss: 0.229\n",
      "Iteration: 155 \t--- Loss: 0.226\n",
      "Iteration: 156 \t--- Loss: 0.243\n",
      "Iteration: 157 \t--- Loss: 0.226\n",
      "Iteration: 158 \t--- Loss: 0.223\n",
      "Iteration: 159 \t--- Loss: 0.226\n",
      "Iteration: 160 \t--- Loss: 0.208\n",
      "Iteration: 161 \t--- Loss: 0.204\n",
      "Iteration: 162 \t--- Loss: 0.210\n",
      "Iteration: 163 \t--- Loss: 0.205\n",
      "Iteration: 164 \t--- Loss: 0.248\n",
      "Iteration: 165 \t--- Loss: 0.216\n",
      "Iteration: 166 \t--- Loss: 0.181\n",
      "Iteration: 167 \t--- Loss: 0.218\n",
      "Iteration: 168 \t--- Loss: 0.211\n",
      "Iteration: 169 \t--- Loss: 0.202\n",
      "Iteration: 170 \t--- Loss: 0.202\n",
      "Iteration: 171 \t--- Loss: 0.236\n",
      "Iteration: 172 \t--- Loss: 0.228\n",
      "Iteration: 173 \t--- Loss: 0.251\n",
      "Iteration: 174 \t--- Loss: 0.233\n",
      "Iteration: 175 \t--- Loss: 0.267\n",
      "Iteration: 176 \t--- Loss: 0.214\n",
      "Iteration: 177 \t--- Loss: 0.214\n",
      "Iteration: 178 \t--- Loss: 0.204\n",
      "Iteration: 179 \t--- Loss: 0.211\n",
      "Iteration: 180 \t--- Loss: 0.203\n",
      "Iteration: 181 \t--- Loss: 0.212\n",
      "Iteration: 182 \t--- Loss: 0.211\n",
      "Iteration: 183 \t--- Loss: 0.236\n",
      "Iteration: 184 \t--- Loss: 0.230\n",
      "Iteration: 185 \t--- Loss: 0.260\n",
      "Iteration: 186 \t--- Loss: 0.197\n",
      "Iteration: 187 \t--- Loss: 0.228\n",
      "Iteration: 188 \t--- Loss: 0.212\n",
      "Iteration: 189 \t--- Loss: 0.231\n",
      "Iteration: 190 \t--- Loss: 0.195\n",
      "Iteration: 191 \t--- Loss: 0.204\n",
      "Iteration: 192 \t--- Loss: 0.229\n",
      "Iteration: 193 \t--- Loss: 0.220\n",
      "Iteration: 194 \t--- Loss: 0.217\n",
      "Iteration: 195 \t--- Loss: 0.224\n",
      "Iteration: 196 \t--- Loss: 0.230\n",
      "Iteration: 197 \t--- Loss: 0.216\n",
      "Iteration: 198 \t--- Loss: 0.204\n",
      "Iteration: 199 \t--- Loss: 0.237\n",
      "Iteration: 200 \t--- Loss: 0.220\n",
      "Iteration: 201 \t--- Loss: 0.199\n",
      "Iteration: 202 \t--- Loss: 0.231\n",
      "Iteration: 203 \t--- Loss: 0.194\n",
      "Iteration: 204 \t--- Loss: 0.216\n",
      "Iteration: 205 \t--- Loss: 0.195\n",
      "Iteration: 206 \t--- Loss: 0.215\n",
      "Iteration: 207 \t--- Loss: 0.209\n",
      "Iteration: 208 \t--- Loss: 0.229\n",
      "Iteration: 209 \t--- Loss: 0.229\n",
      "Iteration: 210 \t--- Loss: 0.219\n",
      "Iteration: 211 \t--- Loss: 0.187\n",
      "Iteration: 212 \t--- Loss: 0.224\n",
      "Iteration: 213 \t--- Loss: 0.223\n",
      "Iteration: 214 \t--- Loss: 0.218\n",
      "Iteration: 215 \t--- Loss: 0.204\n",
      "Iteration: 216 \t--- Loss: 0.201\n",
      "Iteration: 217 \t--- Loss: 0.211\n",
      "Iteration: 218 \t--- Loss: 0.224\n",
      "Iteration: 219 \t--- Loss: 0.191\n",
      "Iteration: 220 \t--- Loss: 0.221\n",
      "Iteration: 221 \t--- Loss: 0.216\n",
      "Iteration: 222 \t--- Loss: 0.213\n",
      "Iteration: 223 \t--- Loss: 0.217\n",
      "Iteration: 224 \t--- Loss: 0.213\n",
      "Iteration: 225 \t--- Loss: 0.202\n",
      "Iteration: 226 \t--- Loss: 0.206\n",
      "Iteration: 227 \t--- Loss: 0.214\n",
      "Iteration: 228 \t--- Loss: 0.202\n",
      "Iteration: 229 \t--- Loss: 0.215\n",
      "Iteration: 230 \t--- Loss: 0.210\n",
      "Iteration: 231 \t--- Loss: 0.209\n",
      "Iteration: 232 \t--- Loss: 0.220\n",
      "Iteration: 233 \t--- Loss: 0.197\n",
      "Iteration: 234 \t--- Loss: 0.215\n",
      "Iteration: 235 \t--- Loss: 0.189\n",
      "Iteration: 236 \t--- Loss: 0.202\n",
      "Iteration: 237 \t--- Loss: 0.204\n",
      "Iteration: 238 \t--- Loss: 0.186\n",
      "Iteration: 239 \t--- Loss: 0.217\n",
      "Iteration: 240 \t--- Loss: 0.221\n",
      "Iteration: 241 \t--- Loss: 0.219\n",
      "Iteration: 242 \t--- Loss: 0.240\n",
      "Iteration: 243 \t--- Loss: 0.197\n",
      "Iteration: 244 \t--- Loss: 0.207\n",
      "Iteration: 245 \t--- Loss: 0.192\n",
      "Iteration: 246 \t--- Loss: 0.224\n",
      "Iteration: 247 \t--- Loss: 0.215\n",
      "Iteration: 248 \t--- Loss: 0.215\n",
      "Iteration: 249 \t--- Loss: 0.212\n",
      "Iteration: 250 \t--- Loss: 0.208\n",
      "Iteration: 251 \t--- Loss: 0.210\n",
      "Iteration: 252 \t--- Loss: 0.223\n",
      "Iteration: 253 \t--- Loss: 0.206\n",
      "Iteration: 254 \t--- Loss: 0.209\n",
      "Iteration: 255 \t--- Loss: 0.222\n",
      "Iteration: 256 \t--- Loss: 0.204\n",
      "Iteration: 257 \t--- Loss: 0.202\n",
      "Iteration: 258 \t--- Loss: 0.219\n",
      "Iteration: 259 \t--- Loss: 0.197Iteration: 0 \t--- Loss: 3.394\n",
      "Iteration: 1 \t--- Loss: 2.913\n",
      "Iteration: 2 \t--- Loss: 2.943\n",
      "Iteration: 3 \t--- Loss: 2.737\n",
      "Iteration: 4 \t--- Loss: 2.726\n",
      "Iteration: 5 \t--- Loss: 2.546\n",
      "Iteration: 6 \t--- Loss: 2.391\n",
      "Iteration: 7 \t--- Loss: 2.609\n",
      "Iteration: 8 \t--- Loss: 2.500\n",
      "Iteration: 9 \t--- Loss: 2.454\n",
      "Iteration: 10 \t--- Loss: 2.522\n",
      "Iteration: 11 \t--- Loss: 2.500\n",
      "Iteration: 12 \t--- Loss: 2.580\n",
      "Iteration: 13 \t--- Loss: 2.483\n",
      "Iteration: 14 \t--- Loss: 2.540\n",
      "Iteration: 15 \t--- Loss: 2.453\n",
      "Iteration: 16 \t--- Loss: 2.339\n",
      "Iteration: 17 \t--- Loss: 2.571\n",
      "Iteration: 18 \t--- Loss: 2.421\n",
      "Iteration: 19 \t--- Loss: 2.419\n",
      "Iteration: 20 \t--- Loss: 2.491\n",
      "Iteration: 21 \t--- Loss: 2.363\n",
      "Iteration: 22 \t--- Loss: 2.430\n",
      "Iteration: 23 \t--- Loss: 2.465\n",
      "Iteration: 24 \t--- Loss: 2.556\n",
      "Iteration: 25 \t--- Loss: 2.423\n",
      "Iteration: 26 \t--- Loss: 2.472\n",
      "Iteration: 27 \t--- Loss: 2.555\n",
      "Iteration: 28 \t--- Loss: 2.509\n",
      "Iteration: 29 \t--- Loss: 2.378\n",
      "Iteration: 30 \t--- Loss: 2.504\n",
      "Iteration: 31 \t--- Loss: 2.393\n",
      "Iteration: 32 \t--- Loss: 2.480\n",
      "Iteration: 33 \t--- Loss: 2.436\n",
      "Iteration: 34 \t--- Loss: 2.505\n",
      "Iteration: 35 \t--- Loss: 2.502\n",
      "Iteration: 36 \t--- Loss: 2.512\n",
      "Iteration: 37 \t--- Loss: 2.468\n",
      "Iteration: 38 \t--- Loss: 2.402\n",
      "Iteration: 39 \t--- Loss: 2.351\n",
      "Iteration: 40 \t--- Loss: 2.510\n",
      "Iteration: 41 \t--- Loss: 2.414\n",
      "Iteration: 42 \t--- Loss: 2.472\n",
      "Iteration: 43 \t--- Loss: 2.411\n",
      "Iteration: 44 \t--- Loss: 2.422\n",
      "Iteration: 45 \t--- Loss: 2.406\n",
      "Iteration: 46 \t--- Loss: 2.463\n",
      "Iteration: 47 \t--- Loss: 2.425\n",
      "Iteration: 48 \t--- Loss: 2.348\n",
      "Iteration: 49 \t--- Loss: 2.491\n",
      "Iteration: 50 \t--- Loss: 2.410\n",
      "Iteration: 51 \t--- Loss: 2.428\n",
      "Iteration: 52 \t--- Loss: 2.499\n",
      "Iteration: 53 \t--- Loss: 2.475\n",
      "Iteration: 54 \t--- Loss: 2.470\n",
      "Iteration: 55 \t--- Loss: 2.446\n",
      "Iteration: 56 \t--- Loss: 2.475\n",
      "Iteration: 57 \t--- Loss: 2.501\n",
      "Iteration: 58 \t--- Loss: 2.532\n",
      "Iteration: 59 \t--- Loss: 2.419\n",
      "Iteration: 60 \t--- Loss: 2.380\n",
      "Iteration: 61 \t--- Loss: 2.574\n",
      "Iteration: 62 \t--- Loss: 2.533\n",
      "Iteration: 63 \t--- Loss: 2.365\n",
      "Iteration: 64 \t--- Loss: 2.402\n",
      "Iteration: 65 \t--- Loss: 2.559\n",
      "Iteration: 66 \t--- Loss: 2.425\n",
      "Iteration: 67 \t--- Loss: 2.556\n",
      "Iteration: 68 \t--- Loss: 2.565\n",
      "Iteration: 69 \t--- Loss: 2.549\n",
      "Iteration: 70 \t--- Loss: 2.346\n",
      "Iteration: 71 \t--- Loss: 2.584\n",
      "Iteration: 72 \t--- Loss: 2.473\n",
      "Iteration: 73 \t--- Loss: 2.426\n",
      "Iteration: 74 \t--- Loss: 2.462\n",
      "Iteration: 75 \t--- Loss: 2.504\n",
      "Iteration: 76 \t--- Loss: 2.548\n",
      "Iteration: 77 \t--- Loss: 2.600\n",
      "Iteration: 78 \t--- Loss: 2.521\n",
      "Iteration: 79 \t--- Loss: 2.564\n",
      "Iteration: 80 \t--- Loss: 2.382\n",
      "Iteration: 81 \t--- Loss: 2.451\n",
      "Iteration: 82 \t--- Loss: 2.415\n",
      "Iteration: 83 \t--- Loss: 2.463\n",
      "Iteration: 84 \t--- Loss: 2.536\n",
      "Iteration: 85 \t--- Loss: 2.478\n",
      "Iteration: 86 \t--- Loss: 2.522\n",
      "Iteration: 87 \t--- Loss: 2.384\n",
      "Iteration: 88 \t--- Loss: 2.362\n",
      "Iteration: 89 \t--- Loss: 2.488\n",
      "Iteration: 90 \t--- Loss: 2.474\n",
      "Iteration: 91 \t--- Loss: 2.517\n",
      "Iteration: 92 \t--- Loss: 2.456\n",
      "Iteration: 93 \t--- Loss: 2.447\n",
      "Iteration: 94 \t--- Loss: 2.522\n",
      "Iteration: 95 \t--- Loss: 2.423\n",
      "Iteration: 96 \t--- Loss: 2.467\n",
      "Iteration: 97 \t--- Loss: 2.454\n",
      "Iteration: 98 \t--- Loss: 2.413\n",
      "Iteration: 99 \t--- Loss: 2.493\n",
      "Iteration: 100 \t--- Loss: 2.438\n",
      "Iteration: 101 \t--- Loss: 2.489\n",
      "Iteration: 102 \t--- Loss: 2.440\n",
      "Iteration: 103 \t--- Loss: 2.447\n",
      "Iteration: 104 \t--- Loss: 2.452\n",
      "Iteration: 105 \t--- Loss: 2.473\n",
      "Iteration: 106 \t--- Loss: 2.547\n",
      "Iteration: 107 \t--- Loss: 2.465\n",
      "Iteration: 108 \t--- Loss: 2.493\n",
      "Iteration: 109 \t--- Loss: 2.464\n",
      "Iteration: 110 \t--- Loss: 2.439\n",
      "Iteration: 111 \t--- Loss: 2.371\n",
      "Iteration: 112 \t--- Loss: 2.462\n",
      "Iteration: 113 \t--- Loss: 2.536\n",
      "Iteration: 114 \t--- Loss: 2.374\n",
      "Iteration: 115 \t--- Loss: 2.359\n",
      "Iteration: 116 \t--- Loss: 2.519\n",
      "Iteration: 117 \t--- Loss: 2.442\n",
      "Iteration: 118 \t--- Loss: 2.457\n",
      "Iteration: 119 \t--- Loss: 2.594\n",
      "Iteration: 120 \t--- Loss: 2.385\n",
      "Iteration: 121 \t--- Loss: 2.395\n",
      "Iteration: 122 \t--- Loss: 2.531\n",
      "Iteration: 123 \t--- Loss: 2.424\n",
      "Iteration: 124 \t--- Loss: 2.564\n",
      "Iteration: 125 \t--- Loss: 2.503\n",
      "Iteration: 126 \t--- Loss: 2.455\n",
      "Iteration: 127 \t--- Loss: 2.585\n",
      "Iteration: 128 \t--- Loss: 2.502\n",
      "Iteration: 129 \t--- Loss: 2.383\n",
      "Iteration: 130 \t--- Loss: 2.570\n",
      "Iteration: 131 \t--- Loss: 2.584\n",
      "Iteration: 132 \t--- Loss: 2.484\n",
      "Iteration: 133 \t--- Loss: 2.481\n",
      "Iteration: 134 \t--- Loss: 2.494\n",
      "Iteration: 135 \t--- Loss: 2.425\n",
      "Iteration: 136 \t--- Loss: 2.469\n",
      "Iteration: 137 \t--- Loss: 2.550\n",
      "Iteration: 138 \t--- Loss: 2.559\n",
      "Iteration: 139 \t--- Loss: 2.387\n",
      "Iteration: 140 \t--- Loss: 2.434\n",
      "Iteration: 141 \t--- Loss: 2.515\n",
      "Iteration: 142 \t--- Loss: 2.370\n",
      "Iteration: 143 \t--- Loss: 2.433\n",
      "Iteration: 144 \t--- Loss: 2.516\n",
      "Iteration: 145 \t--- Loss: 2.544\n",
      "Iteration: 146 \t--- Loss: 2.414\n",
      "Iteration: 147 \t--- Loss: 2.541\n",
      "Iteration: 148 \t--- Loss: 2.494\n",
      "Iteration: 149 \t--- Loss: 2.603\n",
      "Iteration: 150 \t--- Loss: 2.450\n",
      "Iteration: 151 \t--- Loss: 2.462\n",
      "Iteration: 152 \t--- Loss: 2.415\n",
      "Iteration: 153 \t--- Loss: 2.449\n",
      "Iteration: 154 \t--- Loss: 2.540\n",
      "Iteration: 155 \t--- Loss: 2.524\n",
      "Iteration: 156 \t--- Loss: 2.399\n",
      "Iteration: 157 \t--- Loss: 2.434\n",
      "Iteration: 158 \t--- Loss: 2.496\n",
      "Iteration: 159 \t--- Loss: 2.404\n",
      "Iteration: 160 \t--- Loss: 2.516\n",
      "Iteration: 161 \t--- Loss: 2.601\n",
      "Iteration: 162 \t--- Loss: 2.469\n",
      "Iteration: 163 \t--- Loss: 2.459\n",
      "Iteration: 164 \t--- Loss: 2.554\n",
      "Iteration: 165 \t--- Loss: 2.483\n",
      "Iteration: 166 \t--- Loss: 2.395\n",
      "Iteration: 167 \t--- Loss: 2.487\n",
      "Iteration: 168 \t--- Loss: 2.446\n",
      "Iteration: 169 \t--- Loss: 2.379\n",
      "Iteration: 170 \t--- Loss: 2.388\n",
      "Iteration: 171 \t--- Loss: 2.525\n",
      "Iteration: 172 \t--- Loss: 2.506\n",
      "Iteration: 173 \t--- Loss: 2.584\n",
      "Iteration: 174 \t--- Loss: 2.457\n",
      "Iteration: 175 \t--- Loss: 2.418\n",
      "Iteration: 176 \t--- Loss: 2.622\n",
      "Iteration: 177 \t--- Loss: 2.457\n",
      "Iteration: 178 \t--- Loss: 2.409\n",
      "Iteration: 179 \t--- Loss: 2.406\n",
      "Iteration: 180 \t--- Loss: 2.633\n",
      "Iteration: 181 \t--- Loss: 2.555\n",
      "Iteration: 182 \t--- Loss: 2.416\n",
      "Iteration: 183 \t--- Loss: 2.452\n",
      "Iteration: 184 \t--- Loss: 2.517\n",
      "Iteration: 185 \t--- Loss: 2.459\n",
      "Iteration: 186 \t--- Loss: 2.592\n",
      "Iteration: 187 \t--- Loss: 2.548\n",
      "Iteration: 188 \t--- Loss: 2.434\n",
      "Iteration: 189 \t--- Loss: 2.443\n",
      "Iteration: 190 \t--- Loss: 2.372\n",
      "Iteration: 191 \t--- Loss: 2.452\n",
      "Iteration: 192 \t--- Loss: 2.352\n",
      "Iteration: 193 \t--- Loss: 2.570\n",
      "Iteration: 194 \t--- Loss: 2.658\n",
      "Iteration: 195 \t--- Loss: 2.579\n",
      "Iteration: 196 \t--- Loss: 2.470\n",
      "Iteration: 197 \t--- Loss: 2.489\n",
      "Iteration: 198 \t--- Loss: 2.424\n",
      "Iteration: 199 \t--- Loss: 2.493\n",
      "Iteration: 200 \t--- Loss: 2.426\n",
      "Iteration: 201 \t--- Loss: 2.538\n",
      "Iteration: 202 \t--- Loss: 2.461\n",
      "Iteration: 203 \t--- Loss: 2.381\n",
      "Iteration: 204 \t--- Loss: 2.528\n",
      "Iteration: 205 \t--- Loss: 2.478\n",
      "Iteration: 206 \t--- Loss: 2.445\n",
      "Iteration: 207 \t--- Loss: 2.412\n",
      "Iteration: 208 \t--- Loss: 2.416\n",
      "Iteration: 209 \t--- Loss: 2.499\n",
      "Iteration: 210 \t--- Loss: 2.405\n",
      "Iteration: 211 \t--- Loss: 2.357\n",
      "Iteration: 212 \t--- Loss: 2.407\n",
      "Iteration: 213 \t--- Loss: 2.554\n",
      "Iteration: 214 \t--- Loss: 2.393\n",
      "Iteration: 215 \t--- Loss: 2.555\n",
      "Iteration: 216 \t--- Loss: 2.563\n",
      "Iteration: 217 \t--- Loss: 2.509\n",
      "Iteration: 218 \t--- Loss: 2.405\n",
      "Iteration: 219 \t--- Loss: 2.436\n",
      "Iteration: 220 \t--- Loss: 2.445\n",
      "Iteration: 221 \t--- Loss: 2.408\n",
      "Iteration: 222 \t--- Loss: 2.472\n",
      "Iteration: 223 \t--- Loss: 2.492\n",
      "Iteration: 224 \t--- Loss: 2.555\n",
      "Iteration: 225 \t--- Loss: 2.514\n",
      "Iteration: 226 \t--- Loss: 2.352\n",
      "Iteration: 227 \t--- Loss: 2.556\n",
      "Iteration: 228 \t--- Loss: 2.345\n",
      "Iteration: 229 \t--- Loss: 2.483\n",
      "Iteration: 230 \t--- Loss: 2.459\n",
      "Iteration: 231 \t--- Loss: 2.395\n",
      "Iteration: 232 \t--- Loss: 2.518\n",
      "Iteration: 233 \t--- Loss: 2.449\n",
      "Iteration: 234 \t--- Loss: 2.513\n",
      "Iteration: 235 \t--- Loss: 2.408\n",
      "Iteration: 236 \t--- Loss: 2.406\n",
      "Iteration: 237 \t--- Loss: 2.483\n",
      "Iteration: 238 \t--- Loss: 2.528\n",
      "Iteration: 239 \t--- Loss: 2.523\n",
      "Iteration: 240 \t--- Loss: 2.372\n",
      "Iteration: 241 \t--- Loss: 2.472\n",
      "Iteration: 242 \t--- Loss: 2.521\n",
      "Iteration: 243 \t--- Loss: 2.605\n",
      "Iteration: 244 \t--- Loss: 2.281\n",
      "Iteration: 245 \t--- Loss: 2.538\n",
      "Iteration: 246 \t--- Loss: 2.480\n",
      "Iteration: 247 \t--- Loss: 2.421\n",
      "Iteration: 248 \t--- Loss: 2.471\n",
      "Iteration: 249 \t--- Loss: 2.430\n",
      "Iteration: 250 \t--- Loss: 2.509\n",
      "Iteration: 251 \t--- Loss: 2.389\n",
      "Iteration: 252 \t--- Loss: 2.433\n",
      "Iteration: 253 \t--- Loss: 2.398\n",
      "Iteration: 254 \t--- Loss: 2.474\n",
      "Iteration: 255 \t--- Loss: 2.420\n",
      "Iteration: 256 \t--- Loss: 2.386\n",
      "Iteration: 257 \t--- Loss: 2.536\n",
      "Iteration: 258 \t--- Loss: 2.459\n",
      "Iteration: 259 \t--- Loss: 2.429Iteration: 0 \t--- Loss: 0.070\n",
      "Iteration: 1 \t--- Loss: 0.069\n",
      "Iteration: 2 \t--- Loss: 0.071\n",
      "Iteration: 3 \t--- Loss: 0.071\n",
      "Iteration: 4 \t--- Loss: 0.072\n",
      "Iteration: 5 \t--- Loss: 0.074\n",
      "Iteration: 6 \t--- Loss: 0.070\n",
      "Iteration: 7 \t--- Loss: 0.063\n",
      "Iteration: 8 \t--- Loss: 0.067\n",
      "Iteration: 9 \t--- Loss: 0.067\n",
      "Iteration: 10 \t--- Loss: 0.069\n",
      "Iteration: 11 \t--- Loss: 0.064\n",
      "Iteration: 12 \t--- Loss: 0.069\n",
      "Iteration: 13 \t--- Loss: 0.063\n",
      "Iteration: 14 \t--- Loss: 0.066\n",
      "Iteration: 15 \t--- Loss: 0.067\n",
      "Iteration: 16 \t--- Loss: 0.068\n",
      "Iteration: 17 \t--- Loss: 0.064\n",
      "Iteration: 18 \t--- Loss: 0.069\n",
      "Iteration: 19 \t--- Loss: 0.062\n",
      "Iteration: 20 \t--- Loss: 0.064\n",
      "Iteration: 21 \t--- Loss: 0.063\n",
      "Iteration: 22 \t--- Loss: 0.065\n",
      "Iteration: 23 \t--- Loss: 0.070\n",
      "Iteration: 24 \t--- Loss: 0.064\n",
      "Iteration: 25 \t--- Loss: 0.067\n",
      "Iteration: 26 \t--- Loss: 0.062\n",
      "Iteration: 27 \t--- Loss: 0.063\n",
      "Iteration: 28 \t--- Loss: 0.063\n",
      "Iteration: 29 \t--- Loss: 0.063\n",
      "Iteration: 30 \t--- Loss: 0.063\n",
      "Iteration: 31 \t--- Loss: 0.068\n",
      "Iteration: 32 \t--- Loss: 0.062\n",
      "Iteration: 33 \t--- Loss: 0.064\n",
      "Iteration: 34 \t--- Loss: 0.062\n",
      "Iteration: 35 \t--- Loss: 0.062\n",
      "Iteration: 36 \t--- Loss: 0.060\n",
      "Iteration: 37 \t--- Loss: 0.067\n",
      "Iteration: 38 \t--- Loss: 0.057\n",
      "Iteration: 39 \t--- Loss: 0.062\n",
      "Iteration: 40 \t--- Loss: 0.065\n",
      "Iteration: 41 \t--- Loss: 0.065\n",
      "Iteration: 42 \t--- Loss: 0.067\n",
      "Iteration: 43 \t--- Loss: 0.064\n",
      "Iteration: 44 \t--- Loss: 0.057\n",
      "Iteration: 45 \t--- Loss: 0.063\n",
      "Iteration: 46 \t--- Loss: 0.067\n",
      "Iteration: 47 \t--- Loss: 0.069\n",
      "Iteration: 48 \t--- Loss: 0.064\n",
      "Iteration: 49 \t--- Loss: 0.062\n",
      "Iteration: 50 \t--- Loss: 0.062\n",
      "Iteration: 51 \t--- Loss: 0.063\n",
      "Iteration: 52 \t--- Loss: 0.063\n",
      "Iteration: 53 \t--- Loss: 0.060\n",
      "Iteration: 54 \t--- Loss: 0.065\n",
      "Iteration: 55 \t--- Loss: 0.054\n",
      "Iteration: 56 \t--- Loss: 0.066\n",
      "Iteration: 57 \t--- Loss: 0.062\n",
      "Iteration: 58 \t--- Loss: 0.061\n",
      "Iteration: 59 \t--- Loss: 0.061\n",
      "Iteration: 60 \t--- Loss: 0.060\n",
      "Iteration: 61 \t--- Loss: 0.066\n",
      "Iteration: 62 \t--- Loss: 0.063\n",
      "Iteration: 63 \t--- Loss: 0.059\n",
      "Iteration: 64 \t--- Loss: 0.056\n",
      "Iteration: 65 \t--- Loss: 0.057\n",
      "Iteration: 66 \t--- Loss: 0.061\n",
      "Iteration: 67 \t--- Loss: 0.067\n",
      "Iteration: 68 \t--- Loss: 0.063\n",
      "Iteration: 69 \t--- Loss: 0.059\n",
      "Iteration: 70 \t--- Loss: 0.062\n",
      "Iteration: 71 \t--- Loss: 0.064\n",
      "Iteration: 72 \t--- Loss: 0.059\n",
      "Iteration: 73 \t--- Loss: 0.058\n",
      "Iteration: 74 \t--- Loss: 0.061\n",
      "Iteration: 75 \t--- Loss: 0.062\n",
      "Iteration: 76 \t--- Loss: 0.067\n",
      "Iteration: 77 \t--- Loss: 0.068\n",
      "Iteration: 78 \t--- Loss: 0.060\n",
      "Iteration: 79 \t--- Loss: 0.058\n",
      "Iteration: 80 \t--- Loss: 0.060\n",
      "Iteration: 81 \t--- Loss: 0.061\n",
      "Iteration: 82 \t--- Loss: 0.061\n",
      "Iteration: 83 \t--- Loss: 0.063\n",
      "Iteration: 84 \t--- Loss: 0.066\n",
      "Iteration: 85 \t--- Loss: 0.062\n",
      "Iteration: 86 \t--- Loss: 0.064\n",
      "Iteration: 87 \t--- Loss: 0.059\n",
      "Iteration: 88 \t--- Loss: 0.057\n",
      "Iteration: 89 \t--- Loss: 0.065\n",
      "Iteration: 90 \t--- Loss: 0.059\n",
      "Iteration: 91 \t--- Loss: 0.060\n",
      "Iteration: 92 \t--- Loss: 0.067\n",
      "Iteration: 93 \t--- Loss: 0.063\n",
      "Iteration: 94 \t--- Loss: 0.066\n",
      "Iteration: 95 \t--- Loss: 0.061\n",
      "Iteration: 96 \t--- Loss: 0.060\n",
      "Iteration: 97 \t--- Loss: 0.062\n",
      "Iteration: 98 \t--- Loss: 0.060\n",
      "Iteration: 99 \t--- Loss: 0.065\n",
      "Iteration: 100 \t--- Loss: 0.056\n",
      "Iteration: 101 \t--- Loss: 0.068\n",
      "Iteration: 102 \t--- Loss: 0.061\n",
      "Iteration: 103 \t--- Loss: 0.060\n",
      "Iteration: 104 \t--- Loss: 0.060\n",
      "Iteration: 105 \t--- Loss: 0.063\n",
      "Iteration: 106 \t--- Loss: 0.057\n",
      "Iteration: 107 \t--- Loss: 0.062\n",
      "Iteration: 108 \t--- Loss: 0.063\n",
      "Iteration: 109 \t--- Loss: 0.060\n",
      "Iteration: 110 \t--- Loss: 0.058\n",
      "Iteration: 111 \t--- Loss: 0.064\n",
      "Iteration: 112 \t--- Loss: 0.067\n",
      "Iteration: 113 \t--- Loss: 0.066\n",
      "Iteration: 114 \t--- Loss: 0.059\n",
      "Iteration: 115 \t--- Loss: 0.061\n",
      "Iteration: 116 \t--- Loss: 0.057\n",
      "Iteration: 117 \t--- Loss: 0.061\n",
      "Iteration: 118 \t--- Loss: 0.061\n",
      "Iteration: 119 \t--- Loss: 0.060\n",
      "Iteration: 120 \t--- Loss: 0.064\n",
      "Iteration: 121 \t--- Loss: 0.065\n",
      "Iteration: 122 \t--- Loss: 0.059\n",
      "Iteration: 123 \t--- Loss: 0.059\n",
      "Iteration: 124 \t--- Loss: 0.056\n",
      "Iteration: 125 \t--- Loss: 0.056\n",
      "Iteration: 126 \t--- Loss: 0.063\n",
      "Iteration: 127 \t--- Loss: 0.058\n",
      "Iteration: 128 \t--- Loss: 0.061\n",
      "Iteration: 129 \t--- Loss: 0.063\n",
      "Iteration: 130 \t--- Loss: 0.059\n",
      "Iteration: 131 \t--- Loss: 0.064\n",
      "Iteration: 132 \t--- Loss: 0.063\n",
      "Iteration: 133 \t--- Loss: 0.061\n",
      "Iteration: 134 \t--- Loss: 0.063\n",
      "Iteration: 135 \t--- Loss: 0.064\n",
      "Iteration: 136 \t--- Loss: 0.068\n",
      "Iteration: 137 \t--- Loss: 0.056\n",
      "Iteration: 138 \t--- Loss: 0.065\n",
      "Iteration: 139 \t--- Loss: 0.061\n",
      "Iteration: 140 \t--- Loss: 0.063\n",
      "Iteration: 141 \t--- Loss: 0.060\n",
      "Iteration: 142 \t--- Loss: 0.058\n",
      "Iteration: 143 \t--- Loss: 0.053\n",
      "Iteration: 144 \t--- Loss: 0.060\n",
      "Iteration: 145 \t--- Loss: 0.061\n",
      "Iteration: 146 \t--- Loss: 0.059\n",
      "Iteration: 147 \t--- Loss: 0.059\n",
      "Iteration: 148 \t--- Loss: 0.060\n",
      "Iteration: 149 \t--- Loss: 0.063\n",
      "Iteration: 150 \t--- Loss: 0.062\n",
      "Iteration: 151 \t--- Loss: 0.061\n",
      "Iteration: 152 \t--- Loss: 0.061\n",
      "Iteration: 153 \t--- Loss: 0.061\n",
      "Iteration: 154 \t--- Loss: 0.056\n",
      "Iteration: 155 \t--- Loss: 0.055\n",
      "Iteration: 156 \t--- Loss: 0.056\n",
      "Iteration: 157 \t--- Loss: 0.055\n",
      "Iteration: 158 \t--- Loss: 0.063\n",
      "Iteration: 159 \t--- Loss: 0.057\n",
      "Iteration: 160 \t--- Loss: 0.060\n",
      "Iteration: 161 \t--- Loss: 0.065\n",
      "Iteration: 162 \t--- Loss: 0.060\n",
      "Iteration: 163 \t--- Loss: 0.059\n",
      "Iteration: 164 \t--- Loss: 0.064\n",
      "Iteration: 165 \t--- Loss: 0.055\n",
      "Iteration: 166 \t--- Loss: 0.056\n",
      "Iteration: 167 \t--- Loss: 0.059\n",
      "Iteration: 168 \t--- Loss: 0.063\n",
      "Iteration: 169 \t--- Loss: 0.055\n",
      "Iteration: 170 \t--- Loss: 0.065\n",
      "Iteration: 171 \t--- Loss: 0.061\n",
      "Iteration: 172 \t--- Loss: 0.057\n",
      "Iteration: 173 \t--- Loss: 0.058\n",
      "Iteration: 174 \t--- Loss: 0.062\n",
      "Iteration: 175 \t--- Loss: 0.059\n",
      "Iteration: 176 \t--- Loss: 0.062\n",
      "Iteration: 177 \t--- Loss: 0.053\n",
      "Iteration: 178 \t--- Loss: 0.055\n",
      "Iteration: 179 \t--- Loss: 0.059\n",
      "Iteration: 180 \t--- Loss: 0.058\n",
      "Iteration: 181 \t--- Loss: 0.060\n",
      "Iteration: 182 \t--- Loss: 0.062\n",
      "Iteration: 183 \t--- Loss: 0.053\n",
      "Iteration: 184 \t--- Loss: 0.058\n",
      "Iteration: 185 \t--- Loss: 0.062\n",
      "Iteration: 186 \t--- Loss: 0.061\n",
      "Iteration: 187 \t--- Loss: 0.061\n",
      "Iteration: 188 \t--- Loss: 0.057\n",
      "Iteration: 189 \t--- Loss: 0.060\n",
      "Iteration: 190 \t--- Loss: 0.059\n",
      "Iteration: 191 \t--- Loss: 0.064\n",
      "Iteration: 192 \t--- Loss: 0.064\n",
      "Iteration: 193 \t--- Loss: 0.056\n",
      "Iteration: 194 \t--- Loss: 0.057\n",
      "Iteration: 195 \t--- Loss: 0.057\n",
      "Iteration: 196 \t--- Loss: 0.060\n",
      "Iteration: 197 \t--- Loss: 0.058\n",
      "Iteration: 198 \t--- Loss: 0.057\n",
      "Iteration: 199 \t--- Loss: 0.060\n",
      "Iteration: 200 \t--- Loss: 0.057\n",
      "Iteration: 201 \t--- Loss: 0.057\n",
      "Iteration: 202 \t--- Loss: 0.061\n",
      "Iteration: 203 \t--- Loss: 0.058\n",
      "Iteration: 204 \t--- Loss: 0.058\n",
      "Iteration: 205 \t--- Loss: 0.055\n",
      "Iteration: 206 \t--- Loss: 0.057\n",
      "Iteration: 207 \t--- Loss: 0.061\n",
      "Iteration: 208 \t--- Loss: 0.056\n",
      "Iteration: 209 \t--- Loss: 0.060\n",
      "Iteration: 210 \t--- Loss: 0.058\n",
      "Iteration: 211 \t--- Loss: 0.058\n",
      "Iteration: 212 \t--- Loss: 0.055\n",
      "Iteration: 213 \t--- Loss: 0.057\n",
      "Iteration: 214 \t--- Loss: 0.055\n",
      "Iteration: 215 \t--- Loss: 0.056\n",
      "Iteration: 216 \t--- Loss: 0.056\n",
      "Iteration: 217 \t--- Loss: 0.062\n",
      "Iteration: 218 \t--- Loss: 0.059\n",
      "Iteration: 219 \t--- Loss: 0.056\n",
      "Iteration: 220 \t--- Loss: 0.057\n",
      "Iteration: 221 \t--- Loss: 0.057\n",
      "Iteration: 222 \t--- Loss: 0.057\n",
      "Iteration: 223 \t--- Loss: 0.050\n",
      "Iteration: 224 \t--- Loss: 0.056\n",
      "Iteration: 225 \t--- Loss: 0.058\n",
      "Iteration: 226 \t--- Loss: 0.051\n",
      "Iteration: 227 \t--- Loss: 0.057\n",
      "Iteration: 228 \t--- Loss: 0.055\n",
      "Iteration: 229 \t--- Loss: 0.057\n",
      "Iteration: 230 \t--- Loss: 0.061\n",
      "Iteration: 231 \t--- Loss: 0.053\n",
      "Iteration: 232 \t--- Loss: 0.058\n",
      "Iteration: 233 \t--- Loss: 0.057\n",
      "Iteration: 234 \t--- Loss: 0.056\n",
      "Iteration: 235 \t--- Loss: 0.061\n",
      "Iteration: 236 \t--- Loss: 0.055\n",
      "Iteration: 237 \t--- Loss: 0.057\n",
      "Iteration: 238 \t--- Loss: 0.051\n",
      "Iteration: 239 \t--- Loss: 0.056\n",
      "Iteration: 240 \t--- Loss: 0.061\n",
      "Iteration: 241 \t--- Loss: 0.057\n",
      "Iteration: 242 \t--- Loss: 0.054\n",
      "Iteration: 243 \t--- Loss: 0.058\n",
      "Iteration: 244 \t--- Loss: 0.055\n",
      "Iteration: 245 \t--- Loss: 0.056\n",
      "Iteration: 246 \t--- Loss: 0.062\n",
      "Iteration: 247 \t--- Loss: 0.056\n",
      "Iteration: 248 \t--- Loss: 0.057\n",
      "Iteration: 249 \t--- Loss: 0.061\n",
      "Iteration: 250 \t--- Loss: 0.059\n",
      "Iteration: 251 \t--- Loss: 0.063\n",
      "Iteration: 252 \t--- Loss: 0.058\n",
      "Iteration: 253 \t--- Loss: 0.056\n",
      "Iteration: 254 \t--- Loss: 0.055\n",
      "Iteration: 255 \t--- Loss: 0.055\n",
      "Iteration: 256 \t--- Loss: 0.059\n",
      "Iteration: 257 \t--- Loss: 0.055\n",
      "Iteration: 258 \t--- Loss: 0.060\n",
      "Iteration: 259 \t--- Loss: 0.054Iteration: 0 \t--- Loss: 0.256\n",
      "Iteration: 1 \t--- Loss: 0.249\n",
      "Iteration: 2 \t--- Loss: 0.235\n",
      "Iteration: 3 \t--- Loss: 0.239\n",
      "Iteration: 4 \t--- Loss: 0.232\n",
      "Iteration: 5 \t--- Loss: 0.256\n",
      "Iteration: 6 \t--- Loss: 0.247\n",
      "Iteration: 7 \t--- Loss: 0.230\n",
      "Iteration: 8 \t--- Loss: 0.238\n",
      "Iteration: 9 \t--- Loss: 0.230\n",
      "Iteration: 10 \t--- Loss: 0.241\n",
      "Iteration: 11 \t--- Loss: 0.242\n",
      "Iteration: 12 \t--- Loss: 0.242\n",
      "Iteration: 13 \t--- Loss: 0.228\n",
      "Iteration: 14 \t--- Loss: 0.230\n",
      "Iteration: 15 \t--- Loss: 0.220\n",
      "Iteration: 16 \t--- Loss: 0.227\n",
      "Iteration: 17 \t--- Loss: 0.245\n",
      "Iteration: 18 \t--- Loss: 0.241\n",
      "Iteration: 19 \t--- Loss: 0.234\n",
      "Iteration: 20 \t--- Loss: 0.246\n",
      "Iteration: 21 \t--- Loss: 0.235\n",
      "Iteration: 22 \t--- Loss: 0.234\n",
      "Iteration: 23 \t--- Loss: 0.237\n",
      "Iteration: 24 \t--- Loss: 0.240\n",
      "Iteration: 25 \t--- Loss: 0.240\n",
      "Iteration: 26 \t--- Loss: 0.249\n",
      "Iteration: 27 \t--- Loss: 0.232\n",
      "Iteration: 28 \t--- Loss: 0.242\n",
      "Iteration: 29 \t--- Loss: 0.223\n",
      "Iteration: 30 \t--- Loss: 0.244\n",
      "Iteration: 31 \t--- Loss: 0.233\n",
      "Iteration: 32 \t--- Loss: 0.228\n",
      "Iteration: 33 \t--- Loss: 0.239\n",
      "Iteration: 34 \t--- Loss: 0.236\n",
      "Iteration: 35 \t--- Loss: 0.234\n",
      "Iteration: 36 \t--- Loss: 0.230\n",
      "Iteration: 37 \t--- Loss: 0.249\n",
      "Iteration: 38 \t--- Loss: 0.241\n",
      "Iteration: 39 \t--- Loss: 0.252\n",
      "Iteration: 40 \t--- Loss: 0.245\n",
      "Iteration: 41 \t--- Loss: 0.234\n",
      "Iteration: 42 \t--- Loss: 0.237\n",
      "Iteration: 43 \t--- Loss: 0.247\n",
      "Iteration: 44 \t--- Loss: 0.220\n",
      "Iteration: 45 \t--- Loss: 0.237\n",
      "Iteration: 46 \t--- Loss: 0.217\n",
      "Iteration: 47 \t--- Loss: 0.226\n",
      "Iteration: 48 \t--- Loss: 0.226\n",
      "Iteration: 49 \t--- Loss: 0.248\n",
      "Iteration: 50 \t--- Loss: 0.227\n",
      "Iteration: 51 \t--- Loss: 0.233\n",
      "Iteration: 52 \t--- Loss: 0.232\n",
      "Iteration: 53 \t--- Loss: 0.236\n",
      "Iteration: 54 \t--- Loss: 0.242\n",
      "Iteration: 55 \t--- Loss: 0.236\n",
      "Iteration: 56 \t--- Loss: 0.238\n",
      "Iteration: 57 \t--- Loss: 0.244\n",
      "Iteration: 58 \t--- Loss: 0.246\n",
      "Iteration: 59 \t--- Loss: 0.232\n",
      "Iteration: 60 \t--- Loss: 0.223\n",
      "Iteration: 61 \t--- Loss: 0.233\n",
      "Iteration: 62 \t--- Loss: 0.217\n",
      "Iteration: 63 \t--- Loss: 0.216\n",
      "Iteration: 64 \t--- Loss: 0.241\n",
      "Iteration: 65 \t--- Loss: 0.237\n",
      "Iteration: 66 \t--- Loss: 0.237\n",
      "Iteration: 67 \t--- Loss: 0.230\n",
      "Iteration: 68 \t--- Loss: 0.237\n",
      "Iteration: 69 \t--- Loss: 0.239\n",
      "Iteration: 70 \t--- Loss: 0.229\n",
      "Iteration: 71 \t--- Loss: 0.240\n",
      "Iteration: 72 \t--- Loss: 0.227\n",
      "Iteration: 73 \t--- Loss: 0.231\n",
      "Iteration: 74 \t--- Loss: 0.234\n",
      "Iteration: 75 \t--- Loss: 0.223\n",
      "Iteration: 76 \t--- Loss: 0.253\n",
      "Iteration: 77 \t--- Loss: 0.231\n",
      "Iteration: 78 \t--- Loss: 0.228\n",
      "Iteration: 79 \t--- Loss: 0.232\n",
      "Iteration: 80 \t--- Loss: 0.216\n",
      "Iteration: 81 \t--- Loss: 0.241\n",
      "Iteration: 82 \t--- Loss: 0.236\n",
      "Iteration: 83 \t--- Loss: 0.244\n",
      "Iteration: 84 \t--- Loss: 0.233\n",
      "Iteration: 85 \t--- Loss: 0.213\n",
      "Iteration: 86 \t--- Loss: 0.238\n",
      "Iteration: 87 \t--- Loss: 0.223\n",
      "Iteration: 88 \t--- Loss: 0.234\n",
      "Iteration: 89 \t--- Loss: 0.242\n",
      "Iteration: 90 \t--- Loss: 0.231\n",
      "Iteration: 91 \t--- Loss: 0.238\n",
      "Iteration: 92 \t--- Loss: 0.215\n",
      "Iteration: 93 \t--- Loss: 0.247\n",
      "Iteration: 94 \t--- Loss: 0.225\n",
      "Iteration: 95 \t--- Loss: 0.216\n",
      "Iteration: 96 \t--- Loss: 0.225\n",
      "Iteration: 97 \t--- Loss: 0.232\n",
      "Iteration: 98 \t--- Loss: 0.227\n",
      "Iteration: 99 \t--- Loss: 0.254\n",
      "Iteration: 100 \t--- Loss: 0.226\n",
      "Iteration: 101 \t--- Loss: 0.238\n",
      "Iteration: 102 \t--- Loss: 0.245\n",
      "Iteration: 103 \t--- Loss: 0.240\n",
      "Iteration: 104 \t--- Loss: 0.234\n",
      "Iteration: 105 \t--- Loss: 0.229\n",
      "Iteration: 106 \t--- Loss: 0.230\n",
      "Iteration: 107 \t--- Loss: 0.236\n",
      "Iteration: 108 \t--- Loss: 0.231\n",
      "Iteration: 109 \t--- Loss: 0.245\n",
      "Iteration: 110 \t--- Loss: 0.241\n",
      "Iteration: 111 \t--- Loss: 0.247\n",
      "Iteration: 112 \t--- Loss: 0.236\n",
      "Iteration: 113 \t--- Loss: 0.232\n",
      "Iteration: 114 \t--- Loss: 0.235\n",
      "Iteration: 115 \t--- Loss: 0.231\n",
      "Iteration: 116 \t--- Loss: 0.240\n",
      "Iteration: 117 \t--- Loss: 0.236\n",
      "Iteration: 118 \t--- Loss: 0.226\n",
      "Iteration: 119 \t--- Loss: 0.229\n",
      "Iteration: 120 \t--- Loss: 0.222\n",
      "Iteration: 121 \t--- Loss: 0.228\n",
      "Iteration: 122 \t--- Loss: 0.215\n",
      "Iteration: 123 \t--- Loss: 0.218\n",
      "Iteration: 124 \t--- Loss: 0.237\n",
      "Iteration: 125 \t--- Loss: 0.237\n",
      "Iteration: 126 \t--- Loss: 0.234\n",
      "Iteration: 127 \t--- Loss: 0.230\n",
      "Iteration: 128 \t--- Loss: 0.227\n",
      "Iteration: 129 \t--- Loss: 0.224\n",
      "Iteration: 130 \t--- Loss: 0.252\n",
      "Iteration: 131 \t--- Loss: 0.227\n",
      "Iteration: 132 \t--- Loss: 0.245\n",
      "Iteration: 133 \t--- Loss: 0.242\n",
      "Iteration: 134 \t--- Loss: 0.243\n",
      "Iteration: 135 \t--- Loss: 0.223\n",
      "Iteration: 136 \t--- Loss: 0.233\n",
      "Iteration: 137 \t--- Loss: 0.240\n",
      "Iteration: 138 \t--- Loss: 0.221\n",
      "Iteration: 139 \t--- Loss: 0.227\n",
      "Iteration: 140 \t--- Loss: 0.231\n",
      "Iteration: 141 \t--- Loss: 0.228\n",
      "Iteration: 142 \t--- Loss: 0.212\n",
      "Iteration: 143 \t--- Loss: 0.224\n",
      "Iteration: 144 \t--- Loss: 0.241\n",
      "Iteration: 145 \t--- Loss: 0.222\n",
      "Iteration: 146 \t--- Loss: 0.222\n",
      "Iteration: 147 \t--- Loss: 0.241\n",
      "Iteration: 148 \t--- Loss: 0.232\n",
      "Iteration: 149 \t--- Loss: 0.239\n",
      "Iteration: 150 \t--- Loss: 0.243\n",
      "Iteration: 151 \t--- Loss: 0.233\n",
      "Iteration: 152 \t--- Loss: 0.216\n",
      "Iteration: 153 \t--- Loss: 0.233\n",
      "Iteration: 154 \t--- Loss: 0.237\n",
      "Iteration: 155 \t--- Loss: 0.222\n",
      "Iteration: 156 \t--- Loss: 0.222\n",
      "Iteration: 157 \t--- Loss: 0.234\n",
      "Iteration: 158 \t--- Loss: 0.238\n",
      "Iteration: 159 \t--- Loss: 0.237\n",
      "Iteration: 160 \t--- Loss: 0.246\n",
      "Iteration: 161 \t--- Loss: 0.228\n",
      "Iteration: 162 \t--- Loss: 0.249\n",
      "Iteration: 163 \t--- Loss: 0.229\n",
      "Iteration: 164 \t--- Loss: 0.230\n",
      "Iteration: 165 \t--- Loss: 0.224\n",
      "Iteration: 166 \t--- Loss: 0.222\n",
      "Iteration: 167 \t--- Loss: 0.231\n",
      "Iteration: 168 \t--- Loss: 0.219\n",
      "Iteration: 169 \t--- Loss: 0.223\n",
      "Iteration: 170 \t--- Loss: 0.229\n",
      "Iteration: 171 \t--- Loss: 0.246\n",
      "Iteration: 172 \t--- Loss: 0.225\n",
      "Iteration: 173 \t--- Loss: 0.229\n",
      "Iteration: 174 \t--- Loss: 0.230\n",
      "Iteration: 175 \t--- Loss: 0.227\n",
      "Iteration: 176 \t--- Loss: 0.239\n",
      "Iteration: 177 \t--- Loss: 0.242\n",
      "Iteration: 178 \t--- Loss: 0.218\n",
      "Iteration: 179 \t--- Loss: 0.236\n",
      "Iteration: 180 \t--- Loss: 0.234\n",
      "Iteration: 181 \t--- Loss: 0.238\n",
      "Iteration: 182 \t--- Loss: 0.223\n",
      "Iteration: 183 \t--- Loss: 0.241\n",
      "Iteration: 184 \t--- Loss: 0.227\n",
      "Iteration: 185 \t--- Loss: 0.221\n",
      "Iteration: 186 \t--- Loss: 0.223\n",
      "Iteration: 187 \t--- Loss: 0.220\n",
      "Iteration: 188 \t--- Loss: 0.226\n",
      "Iteration: 189 \t--- Loss: 0.223\n",
      "Iteration: 190 \t--- Loss: 0.241\n",
      "Iteration: 191 \t--- Loss: 0.240\n",
      "Iteration: 192 \t--- Loss: 0.242\n",
      "Iteration: 193 \t--- Loss: 0.225\n",
      "Iteration: 194 \t--- Loss: 0.221\n",
      "Iteration: 195 \t--- Loss: 0.244\n",
      "Iteration: 196 \t--- Loss: 0.220\n",
      "Iteration: 197 \t--- Loss: 0.231\n",
      "Iteration: 198 \t--- Loss: 0.230\n",
      "Iteration: 199 \t--- Loss: 0.233\n",
      "Iteration: 200 \t--- Loss: 0.227\n",
      "Iteration: 201 \t--- Loss: 0.227\n",
      "Iteration: 202 \t--- Loss: 0.243\n",
      "Iteration: 203 \t--- Loss: 0.226\n",
      "Iteration: 204 \t--- Loss: 0.225\n",
      "Iteration: 205 \t--- Loss: 0.225\n",
      "Iteration: 206 \t--- Loss: 0.223\n",
      "Iteration: 207 \t--- Loss: 0.232\n",
      "Iteration: 208 \t--- Loss: 0.249\n",
      "Iteration: 209 \t--- Loss: 0.228\n",
      "Iteration: 210 \t--- Loss: 0.222\n",
      "Iteration: 211 \t--- Loss: 0.219\n",
      "Iteration: 212 \t--- Loss: 0.230\n",
      "Iteration: 213 \t--- Loss: 0.234\n",
      "Iteration: 214 \t--- Loss: 0.232\n",
      "Iteration: 215 \t--- Loss: 0.231\n",
      "Iteration: 216 \t--- Loss: 0.240\n",
      "Iteration: 217 \t--- Loss: 0.223\n",
      "Iteration: 218 \t--- Loss: 0.221\n",
      "Iteration: 219 \t--- Loss: 0.230\n",
      "Iteration: 220 \t--- Loss: 0.233\n",
      "Iteration: 221 \t--- Loss: 0.219\n",
      "Iteration: 222 \t--- Loss: 0.227\n",
      "Iteration: 223 \t--- Loss: 0.223\n",
      "Iteration: 224 \t--- Loss: 0.224\n",
      "Iteration: 225 \t--- Loss: 0.233\n",
      "Iteration: 226 \t--- Loss: 0.228\n",
      "Iteration: 227 \t--- Loss: 0.227\n",
      "Iteration: 228 \t--- Loss: 0.239\n",
      "Iteration: 229 \t--- Loss: 0.258\n",
      "Iteration: 230 \t--- Loss: 0.233\n",
      "Iteration: 231 \t--- Loss: 0.211\n",
      "Iteration: 232 \t--- Loss: 0.222\n",
      "Iteration: 233 \t--- Loss: 0.244\n",
      "Iteration: 234 \t--- Loss: 0.225\n",
      "Iteration: 235 \t--- Loss: 0.224\n",
      "Iteration: 236 \t--- Loss: 0.226\n",
      "Iteration: 237 \t--- Loss: 0.233\n",
      "Iteration: 238 \t--- Loss: 0.236\n",
      "Iteration: 239 \t--- Loss: 0.233\n",
      "Iteration: 240 \t--- Loss: 0.221\n",
      "Iteration: 241 \t--- Loss: 0.229\n",
      "Iteration: 242 \t--- Loss: 0.219\n",
      "Iteration: 243 \t--- Loss: 0.228\n",
      "Iteration: 244 \t--- Loss: 0.228\n",
      "Iteration: 245 \t--- Loss: 0.233\n",
      "Iteration: 246 \t--- Loss: 0.229\n",
      "Iteration: 247 \t--- Loss: 0.217\n",
      "Iteration: 248 \t--- Loss: 0.233\n",
      "Iteration: 249 \t--- Loss: 0.225\n",
      "Iteration: 250 \t--- Loss: 0.218\n",
      "Iteration: 251 \t--- Loss: 0.232\n",
      "Iteration: 252 \t--- Loss: 0.221\n",
      "Iteration: 253 \t--- Loss: 0.215\n",
      "Iteration: 254 \t--- Loss: 0.220\n",
      "Iteration: 255 \t--- Loss: 0.219\n",
      "Iteration: 256 \t--- Loss: 0.220\n",
      "Iteration: 257 \t--- Loss: 0.233\n",
      "Iteration: 258 \t--- Loss: 0.236\n",
      "Iteration: 259 \t--- Loss: 0.224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:49<00:00, 49.25s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.213\n",
      "Iteration: 261 \t--- Loss: 0.226\n",
      "Iteration: 262 \t--- Loss: 0.220\n",
      "Iteration: 263 \t--- Loss: 0.195\n",
      "Iteration: 264 \t--- Loss: 0.243\n",
      "Iteration: 265 \t--- Loss: 0.219\n",
      "Iteration: 266 \t--- Loss: 0.248\n",
      "Iteration: 267 \t--- Loss: 0.222\n",
      "Iteration: 268 \t--- Loss: 0.225\n",
      "Iteration: 269 \t--- Loss: 0.230\n",
      "Iteration: 270 \t--- Loss: 0.205\n",
      "Iteration: 271 \t--- Loss: 0.232\n",
      "Iteration: 272 \t--- Loss: 0.210\n",
      "Iteration: 273 \t--- Loss: 0.229\n",
      "Iteration: 274 \t--- Loss: 0.227\n",
      "Iteration: 275 \t--- Loss: 0.229\n",
      "Iteration: 276 \t--- Loss: 0.226\n",
      "Iteration: 277 \t--- Loss: 0.214\n",
      "Iteration: 278 \t--- Loss: 0.229\n",
      "Iteration: 279 \t--- Loss: 0.227\n",
      "Iteration: 280 \t--- Loss: 0.202\n",
      "Iteration: 281 \t--- Loss: 0.209\n",
      "Iteration: 282 \t--- Loss: 0.206\n",
      "Iteration: 283 \t--- Loss: 0.216\n",
      "Iteration: 284 \t--- Loss: 0.189\n",
      "Iteration: 285 \t--- Loss: 0.233\n",
      "Iteration: 286 \t--- Loss: 0.217\n",
      "Iteration: 287 \t--- Loss: 0.211\n",
      "Iteration: 288 \t--- Loss: 0.202\n",
      "Iteration: 289 \t--- Loss: 0.214\n",
      "Iteration: 290 \t--- Loss: 0.208\n",
      "Iteration: 291 \t--- Loss: 0.236\n",
      "Iteration: 292 \t--- Loss: 0.201\n",
      "Iteration: 293 \t--- Loss: 0.202\n",
      "Iteration: 294 \t--- Loss: 0.210\n",
      "Iteration: 295 \t--- Loss: 0.226\n",
      "Iteration: 296 \t--- Loss: 0.212\n",
      "Iteration: 297 \t--- Loss: 0.226\n",
      "Iteration: 298 \t--- Loss: 0.239\n",
      "Iteration: 299 \t--- Loss: 0.216\n",
      "Iteration: 300 \t--- Loss: 0.237\n",
      "Iteration: 301 \t--- Loss: 0.226\n",
      "Iteration: 302 \t--- Loss: 0.197\n",
      "Iteration: 303 \t--- Loss: 0.185\n",
      "Iteration: 304 \t--- Loss: 0.204\n",
      "Iteration: 305 \t--- Loss: 0.235\n",
      "Iteration: 306 \t--- Loss: 0.205\n",
      "Iteration: 307 \t--- Loss: 0.200\n",
      "Iteration: 308 \t--- Loss: 0.210\n",
      "Iteration: 309 \t--- Loss: 0.214\n",
      "Iteration: 310 \t--- Loss: 0.231\n",
      "Iteration: 311 \t--- Loss: 0.194\n",
      "Iteration: 312 \t--- Loss: 0.211\n",
      "Iteration: 313 \t--- Loss: 0.229\n",
      "Iteration: 314 \t--- Loss: 0.217\n",
      "Iteration: 315 \t--- Loss: 0.233\n",
      "Iteration: 316 \t--- Loss: 0.223\n",
      "Iteration: 317 \t--- Loss: 0.225\n",
      "Iteration: 318 \t--- Loss: 0.218\n",
      "Iteration: 319 \t--- Loss: 0.208\n",
      "Iteration: 320 \t--- Loss: 0.216\n",
      "Iteration: 321 \t--- Loss: 0.204\n",
      "Iteration: 322 \t--- Loss: 0.206\n",
      "Iteration: 323 \t--- Loss: 0.205\n",
      "Iteration: 324 \t--- Loss: 0.227\n",
      "Iteration: 325 \t--- Loss: 0.230\n",
      "Iteration: 326 \t--- Loss: 0.199\n",
      "Iteration: 327 \t--- Loss: 0.232\n",
      "Iteration: 328 \t--- Loss: 0.225\n",
      "Iteration: 329 \t--- Loss: 0.195\n",
      "Iteration: 330 \t--- Loss: 0.204\n",
      "Iteration: 331 \t--- Loss: 0.232\n",
      "Iteration: 332 \t--- Loss: 0.202\n",
      "Iteration: 333 \t--- Loss: 0.176\n",
      "Iteration: 334 \t--- Loss: 0.228\n",
      "Iteration: 335 \t--- Loss: 0.208\n",
      "Iteration: 336 \t--- Loss: 0.240\n",
      "Iteration: 337 \t--- Loss: 0.204\n",
      "Iteration: 338 \t--- Loss: 0.193\n",
      "Iteration: 339 \t--- Loss: 0.219\n",
      "Iteration: 340 \t--- Loss: 0.211\n",
      "Iteration: 341 \t--- Loss: 0.203\n",
      "Iteration: 342 \t--- Loss: 0.226\n",
      "Iteration: 343 \t--- Loss: 0.208\n",
      "Iteration: 344 \t--- Loss: 0.235\n",
      "Iteration: 345 \t--- Loss: 0.238\n",
      "Iteration: 346 \t--- Loss: 0.230\n",
      "Iteration: 347 \t--- Loss: 0.222\n",
      "Iteration: 348 \t--- Loss: 0.189\n",
      "Iteration: 349 \t--- Loss: 0.234\n",
      "Iteration: 350 \t--- Loss: 0.236\n",
      "Iteration: 351 \t--- Loss: 0.201\n",
      "Iteration: 352 \t--- Loss: 0.222\n",
      "Iteration: 353 \t--- Loss: 0.207\n",
      "Iteration: 354 \t--- Loss: 0.196\n",
      "Iteration: 355 \t--- Loss: 0.187\n",
      "Iteration: 356 \t--- Loss: 0.255\n",
      "Iteration: 357 \t--- Loss: 0.202\n",
      "Iteration: 358 \t--- Loss: 0.247\n",
      "Iteration: 359 \t--- Loss: 0.227\n",
      "Iteration: 360 \t--- Loss: 0.247\n",
      "Iteration: 361 \t--- Loss: 0.210\n",
      "Iteration: 362 \t--- Loss: 0.189\n",
      "Iteration: 363 \t--- Loss: 0.226\n",
      "Iteration: 364 \t--- Loss: 0.219\n",
      "Iteration: 365 \t--- Loss: 0.213\n",
      "Iteration: 366 \t--- Loss: 0.228\n",
      "Iteration: 367 \t--- Loss: 0.220\n",
      "Iteration: 368 \t--- Loss: 0.222\n",
      "Iteration: 369 \t--- Loss: 0.254\n",
      "Iteration: 370 \t--- Loss: 0.228\n",
      "Iteration: 371 \t--- Loss: 0.198\n",
      "Iteration: 372 \t--- Loss: 0.200\n",
      "Iteration: 373 \t--- Loss: 0.222\n",
      "Iteration: 374 \t--- Loss: 0.228\n",
      "Iteration: 375 \t--- Loss: 0.211\n",
      "Iteration: 376 \t--- Loss: 0.220\n",
      "Iteration: 377 \t--- Loss: 0.239\n",
      "Iteration: 378 \t--- Loss: 0.234\n",
      "Iteration: 379 \t--- Loss: 0.204\n",
      "Iteration: 380 \t--- Loss: 0.197\n",
      "Iteration: 381 \t--- Loss: 0.226\n",
      "Iteration: 382 \t--- Loss: 0.209\n",
      "Iteration: 383 \t--- Loss: 0.212\n",
      "Iteration: 384 \t--- Loss: 0.183\n",
      "Iteration: 385 \t--- Loss: 0.245\n",
      "Iteration: 386 \t--- Loss: 0.224\n",
      "Iteration: 387 \t--- Loss: 0.236\n",
      "Iteration: 388 \t--- Loss: 0.238\n",
      "Iteration: 389 \t--- Loss: 0.184\n",
      "Iteration: 390 \t--- Loss: 0.222\n",
      "Iteration: 391 \t--- Loss: 0.216\n",
      "Iteration: 392 \t--- Loss: 0.228\n",
      "Iteration: 393 \t--- Loss: 0.234\n",
      "Iteration: 394 \t--- Loss: 0.229\n",
      "Iteration: 395 \t--- Loss: 0.189\n",
      "Iteration: 396 \t--- Loss: 0.206\n",
      "Iteration: 397 \t--- Loss: 0.203\n",
      "Iteration: 398 \t--- Loss: 0.228\n",
      "Iteration: 399 \t--- Loss: 0.228\n",
      "Iteration: 400 \t--- Loss: 0.218\n",
      "Iteration: 401 \t--- Loss: 0.207\n",
      "Iteration: 402 \t--- Loss: 0.253\n",
      "Iteration: 403 \t--- Loss: 0.238\n",
      "Iteration: 404 \t--- Loss: 0.203\n",
      "Iteration: 405 \t--- Loss: 0.202\n",
      "Iteration: 406 \t--- Loss: 0.209\n",
      "Iteration: 407 \t--- Loss: 0.215\n",
      "Iteration: 408 \t--- Loss: 0.254\n",
      "Iteration: 409 \t--- Loss: 0.211\n",
      "Iteration: 410 \t--- Loss: 0.189\n",
      "Iteration: 411 \t--- Loss: 0.235\n",
      "Iteration: 412 \t--- Loss: 0.186\n",
      "Iteration: 413 \t--- Loss: 0.234\n",
      "Iteration: 414 \t--- Loss: 0.216\n",
      "Iteration: 415 \t--- Loss: 0.225\n",
      "Iteration: 416 \t--- Loss: 0.208\n",
      "Iteration: 417 \t--- Loss: 0.204\n",
      "Iteration: 418 \t--- Loss: 0.214\n",
      "Iteration: 419 \t--- Loss: 0.243\n",
      "Iteration: 420 \t--- Loss: 0.238\n",
      "Iteration: 421 \t--- Loss: 0.208\n",
      "Iteration: 422 \t--- Loss: 0.227\n",
      "Iteration: 423 \t--- Loss: 0.218\n",
      "Iteration: 424 \t--- Loss: 0.193\n",
      "Iteration: 425 \t--- Loss: 0.191\n",
      "Iteration: 426 \t--- Loss: 0.214\n",
      "Iteration: 427 \t--- Loss: 0.226\n",
      "Iteration: 428 \t--- Loss: 0.234\n",
      "Iteration: 429 \t--- Loss: 0.213\n",
      "Iteration: 430 \t--- Loss: 0.194\n",
      "Iteration: 431 \t--- Loss: 0.225\n",
      "Iteration: 432 \t--- Loss: 0.247\n",
      "Iteration: 433 \t--- Loss: 0.233\n",
      "Iteration: 434 \t--- Loss: 0.223\n",
      "Iteration: 435 \t--- Loss: 0.191\n",
      "Iteration: 436 \t--- Loss: 0.231\n",
      "Iteration: 437 \t--- Loss: 0.219\n",
      "Iteration: 438 \t--- Loss: 0.216\n",
      "Iteration: 439 \t--- Loss: 0.228\n",
      "Iteration: 440 \t--- Loss: 0.180\n",
      "Iteration: 441 \t--- Loss: 0.171\n",
      "Iteration: 442 \t--- Loss: 0.230\n",
      "Iteration: 443 \t--- Loss: 0.214\n",
      "Iteration: 444 \t--- Loss: 0.208\n",
      "Iteration: 445 \t--- Loss: 0.224\n",
      "Iteration: 446 \t--- Loss: 0.203\n",
      "Iteration: 447 \t--- Loss: 0.224\n",
      "Iteration: 448 \t--- Loss: 0.210\n",
      "Iteration: 449 \t--- Loss: 0.202\n",
      "Iteration: 450 \t--- Loss: 0.217\n",
      "Iteration: 451 \t--- Loss: 0.225\n",
      "Iteration: 452 \t--- Loss: 0.235\n",
      "Iteration: 453 \t--- Loss: 0.218\n",
      "Iteration: 454 \t--- Loss: 0.225\n",
      "Iteration: 455 \t--- Loss: 0.207\n",
      "Iteration: 456 \t--- Loss: 0.217\n",
      "Iteration: 457 \t--- Loss: 0.213\n",
      "Iteration: 458 \t--- Loss: 0.220\n",
      "Iteration: 459 \t--- Loss: 0.214\n",
      "Iteration: 460 \t--- Loss: 0.235\n",
      "Iteration: 461 \t--- Loss: 0.209\n",
      "Iteration: 462 \t--- Loss: 0.227\n",
      "Iteration: 463 \t--- Loss: 0.203\n",
      "Iteration: 464 \t--- Loss: 0.239\n",
      "Iteration: 465 \t--- Loss: 0.216\n",
      "Iteration: 466 \t--- Loss: 0.200\n",
      "Iteration: 467 \t--- Loss: 0.239\n",
      "Iteration: 468 \t--- Loss: 0.234\n",
      "Iteration: 469 \t--- Loss: 0.258\n",
      "Iteration: 470 \t--- Loss: 0.225\n",
      "Iteration: 471 \t--- Loss: 0.212\n",
      "Iteration: 472 \t--- Loss: 0.213\n",
      "Iteration: 473 \t--- Loss: 0.223\n",
      "Iteration: 474 \t--- Loss: 0.216\n",
      "Iteration: 475 \t--- Loss: 0.196\n",
      "Iteration: 476 \t--- Loss: 0.215\n",
      "Iteration: 477 \t--- Loss: 0.196\n",
      "Iteration: 478 \t--- Loss: 0.201\n",
      "Iteration: 479 \t--- Loss: 0.214\n",
      "Iteration: 480 \t--- Loss: 0.218\n",
      "Iteration: 481 \t--- Loss: 0.217\n",
      "Iteration: 482 \t--- Loss: 0.195\n",
      "Iteration: 483 \t--- Loss: 0.195\n",
      "Iteration: 484 \t--- Loss: 0.215\n",
      "Iteration: 485 \t--- Loss: 0.208\n",
      "Iteration: 486 \t--- Loss: 0.223\n",
      "Iteration: 487 \t--- Loss: 0.231\n",
      "Iteration: 488 \t--- Loss: 0.208\n",
      "Iteration: 489 \t--- Loss: 0.253\n",
      "Iteration: 490 \t--- Loss: 0.197\n",
      "Iteration: 491 \t--- Loss: 0.220\n",
      "Iteration: 492 \t--- Loss: 0.233\n",
      "Iteration: 493 \t--- Loss: 0.214\n",
      "Iteration: 494 \t--- Loss: 0.210\n",
      "Iteration: 495 \t--- Loss: 0.187\n",
      "Iteration: 496 \t--- Loss: 0.212\n",
      "Iteration: 497 \t--- Loss: 0.244\n",
      "Iteration: 498 \t--- Loss: 0.233\n",
      "Iteration: 499 \t--- Loss: 0.200\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:09,  1.19s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 2.482\n",
      "Iteration: 261 \t--- Loss: 2.551\n",
      "Iteration: 262 \t--- Loss: 2.460\n",
      "Iteration: 263 \t--- Loss: 2.394\n",
      "Iteration: 264 \t--- Loss: 2.521\n",
      "Iteration: 265 \t--- Loss: 2.354\n",
      "Iteration: 266 \t--- Loss: 2.427\n",
      "Iteration: 267 \t--- Loss: 2.618\n",
      "Iteration: 268 \t--- Loss: 2.541\n",
      "Iteration: 269 \t--- Loss: 2.381\n",
      "Iteration: 270 \t--- Loss: 2.380\n",
      "Iteration: 271 \t--- Loss: 2.429\n",
      "Iteration: 272 \t--- Loss: 2.509\n",
      "Iteration: 273 \t--- Loss: 2.419\n",
      "Iteration: 274 \t--- Loss: 2.303\n",
      "Iteration: 275 \t--- Loss: 2.452\n",
      "Iteration: 276 \t--- Loss: 2.452\n",
      "Iteration: 277 \t--- Loss: 2.411\n",
      "Iteration: 278 \t--- Loss: 2.518\n",
      "Iteration: 279 \t--- Loss: 2.607\n",
      "Iteration: 280 \t--- Loss: 2.552\n",
      "Iteration: 281 \t--- Loss: 2.393\n",
      "Iteration: 282 \t--- Loss: 2.398\n",
      "Iteration: 283 \t--- Loss: 2.384\n",
      "Iteration: 284 \t--- Loss: 2.405\n",
      "Iteration: 285 \t--- Loss: 2.491\n",
      "Iteration: 286 \t--- Loss: 2.436\n",
      "Iteration: 287 \t--- Loss: 2.471\n",
      "Iteration: 288 \t--- Loss: 2.503\n",
      "Iteration: 289 \t--- Loss: 2.482\n",
      "Iteration: 290 \t--- Loss: 2.461\n",
      "Iteration: 291 \t--- Loss: 2.474\n",
      "Iteration: 292 \t--- Loss: 2.470\n",
      "Iteration: 293 \t--- Loss: 2.482\n",
      "Iteration: 294 \t--- Loss: 2.563\n",
      "Iteration: 295 \t--- Loss: 2.451\n",
      "Iteration: 296 \t--- Loss: 2.443\n",
      "Iteration: 297 \t--- Loss: 2.410\n",
      "Iteration: 298 \t--- Loss: 2.461\n",
      "Iteration: 299 \t--- Loss: 2.367\n",
      "Iteration: 300 \t--- Loss: 2.390\n",
      "Iteration: 301 \t--- Loss: 2.450\n",
      "Iteration: 302 \t--- Loss: 2.446\n",
      "Iteration: 303 \t--- Loss: 2.538\n",
      "Iteration: 304 \t--- Loss: 2.526\n",
      "Iteration: 305 \t--- Loss: 2.545\n",
      "Iteration: 306 \t--- Loss: 2.383\n",
      "Iteration: 307 \t--- Loss: 2.480\n",
      "Iteration: 308 \t--- Loss: 2.376\n",
      "Iteration: 309 \t--- Loss: 2.448\n",
      "Iteration: 310 \t--- Loss: 2.440\n",
      "Iteration: 311 \t--- Loss: 2.468\n",
      "Iteration: 312 \t--- Loss: 2.439\n",
      "Iteration: 313 \t--- Loss: 2.361\n",
      "Iteration: 314 \t--- Loss: 2.309\n",
      "Iteration: 315 \t--- Loss: 2.273\n",
      "Iteration: 316 \t--- Loss: 2.527\n",
      "Iteration: 317 \t--- Loss: 2.420\n",
      "Iteration: 318 \t--- Loss: 2.352\n",
      "Iteration: 319 \t--- Loss: 2.489\n",
      "Iteration: 320 \t--- Loss: 2.417\n",
      "Iteration: 321 \t--- Loss: 2.390\n",
      "Iteration: 322 \t--- Loss: 2.467\n",
      "Iteration: 323 \t--- Loss: 2.436\n",
      "Iteration: 324 \t--- Loss: 2.629\n",
      "Iteration: 325 \t--- Loss: 2.585\n",
      "Iteration: 326 \t--- Loss: 2.473\n",
      "Iteration: 327 \t--- Loss: 2.363\n",
      "Iteration: 328 \t--- Loss: 2.528\n",
      "Iteration: 329 \t--- Loss: 2.295\n",
      "Iteration: 330 \t--- Loss: 2.405\n",
      "Iteration: 331 \t--- Loss: 2.583\n",
      "Iteration: 332 \t--- Loss: 2.408\n",
      "Iteration: 333 \t--- Loss: 2.507\n",
      "Iteration: 334 \t--- Loss: 2.435\n",
      "Iteration: 335 \t--- Loss: 2.488\n",
      "Iteration: 336 \t--- Loss: 2.455\n",
      "Iteration: 337 \t--- Loss: 2.529\n",
      "Iteration: 338 \t--- Loss: 2.329\n",
      "Iteration: 339 \t--- Loss: 2.508\n",
      "Iteration: 340 \t--- Loss: 2.315\n",
      "Iteration: 341 \t--- Loss: 2.490\n",
      "Iteration: 342 \t--- Loss: 2.645\n",
      "Iteration: 343 \t--- Loss: 2.597\n",
      "Iteration: 344 \t--- Loss: 2.375\n",
      "Iteration: 345 \t--- Loss: 2.452\n",
      "Iteration: 346 \t--- Loss: 2.527\n",
      "Iteration: 347 \t--- Loss: 2.520\n",
      "Iteration: 348 \t--- Loss: 2.459\n",
      "Iteration: 349 \t--- Loss: 2.418\n",
      "Iteration: 350 \t--- Loss: 2.635\n",
      "Iteration: 351 \t--- Loss: 2.552\n",
      "Iteration: 352 \t--- Loss: 2.386\n",
      "Iteration: 353 \t--- Loss: 2.395\n",
      "Iteration: 354 \t--- Loss: 2.423\n",
      "Iteration: 355 \t--- Loss: 2.508\n",
      "Iteration: 356 \t--- Loss: 2.494\n",
      "Iteration: 357 \t--- Loss: 2.495\n",
      "Iteration: 358 \t--- Loss: 2.527\n",
      "Iteration: 359 \t--- Loss: 2.385\n",
      "Iteration: 360 \t--- Loss: 2.292\n",
      "Iteration: 361 \t--- Loss: 2.565\n",
      "Iteration: 362 \t--- Loss: 2.326\n",
      "Iteration: 363 \t--- Loss: 2.625\n",
      "Iteration: 364 \t--- Loss: 2.408\n",
      "Iteration: 365 \t--- Loss: 2.399\n",
      "Iteration: 366 \t--- Loss: 2.459\n",
      "Iteration: 367 \t--- Loss: 2.408\n",
      "Iteration: 368 \t--- Loss: 2.440\n",
      "Iteration: 369 \t--- Loss: 2.402\n",
      "Iteration: 370 \t--- Loss: 2.521\n",
      "Iteration: 371 \t--- Loss: 2.517\n",
      "Iteration: 372 \t--- Loss: 2.547\n",
      "Iteration: 373 \t--- Loss: 2.322\n",
      "Iteration: 374 \t--- Loss: 2.446\n",
      "Iteration: 375 \t--- Loss: 2.588\n",
      "Iteration: 376 \t--- Loss: 2.346\n",
      "Iteration: 377 \t--- Loss: 2.562\n",
      "Iteration: 378 \t--- Loss: 2.442\n",
      "Iteration: 379 \t--- Loss: 2.480\n",
      "Iteration: 380 \t--- Loss: 2.481\n",
      "Iteration: 381 \t--- Loss: 2.476\n",
      "Iteration: 382 \t--- Loss: 2.574\n",
      "Iteration: 383 \t--- Loss: 2.386\n",
      "Iteration: 384 \t--- Loss: 2.455\n",
      "Iteration: 385 \t--- Loss: 2.390\n",
      "Iteration: 386 \t--- Loss: 2.498\n",
      "Iteration: 387 \t--- Loss: 2.473\n",
      "Iteration: 388 \t--- Loss: 2.384\n",
      "Iteration: 389 \t--- Loss: 2.473\n",
      "Iteration: 390 \t--- Loss: 2.532\n",
      "Iteration: 391 \t--- Loss: 2.461\n",
      "Iteration: 392 \t--- Loss: 2.437\n",
      "Iteration: 393 \t--- Loss: 2.477\n",
      "Iteration: 394 \t--- Loss: 2.510\n",
      "Iteration: 395 \t--- Loss: 2.494\n",
      "Iteration: 396 \t--- Loss: 2.449\n",
      "Iteration: 397 \t--- Loss: 2.577\n",
      "Iteration: 398 \t--- Loss: 2.521\n",
      "Iteration: 399 \t--- Loss: 2.506\n",
      "Iteration: 400 \t--- Loss: 2.496\n",
      "Iteration: 401 \t--- Loss: 2.527\n",
      "Iteration: 402 \t--- Loss: 2.523\n",
      "Iteration: 403 \t--- Loss: 2.525\n",
      "Iteration: 404 \t--- Loss: 2.403\n",
      "Iteration: 405 \t--- Loss: 2.391\n",
      "Iteration: 406 \t--- Loss: 2.516\n",
      "Iteration: 407 \t--- Loss: 2.461\n",
      "Iteration: 408 \t--- Loss: 2.502\n",
      "Iteration: 409 \t--- Loss: 2.525\n",
      "Iteration: 410 \t--- Loss: 2.449\n",
      "Iteration: 411 \t--- Loss: 2.532\n",
      "Iteration: 412 \t--- Loss: 2.350\n",
      "Iteration: 413 \t--- Loss: 2.396\n",
      "Iteration: 414 \t--- Loss: 2.575\n",
      "Iteration: 415 \t--- Loss: 2.337\n",
      "Iteration: 416 \t--- Loss: 2.559\n",
      "Iteration: 417 \t--- Loss: 2.323\n",
      "Iteration: 418 \t--- Loss: 2.406\n",
      "Iteration: 419 \t--- Loss: 2.467\n",
      "Iteration: 420 \t--- Loss: 2.328\n",
      "Iteration: 421 \t--- Loss: 2.282\n",
      "Iteration: 422 \t--- Loss: 2.444\n",
      "Iteration: 423 \t--- Loss: 2.301\n",
      "Iteration: 424 \t--- Loss: 2.382\n",
      "Iteration: 425 \t--- Loss: 2.446\n",
      "Iteration: 426 \t--- Loss: 2.448\n",
      "Iteration: 427 \t--- Loss: 2.526\n",
      "Iteration: 428 \t--- Loss: 2.464\n",
      "Iteration: 429 \t--- Loss: 2.500\n",
      "Iteration: 430 \t--- Loss: 2.556\n",
      "Iteration: 431 \t--- Loss: 2.376\n",
      "Iteration: 432 \t--- Loss: 2.476\n",
      "Iteration: 433 \t--- Loss: 2.688\n",
      "Iteration: 434 \t--- Loss: 2.408\n",
      "Iteration: 435 \t--- Loss: 2.424\n",
      "Iteration: 436 \t--- Loss: 2.567\n",
      "Iteration: 437 \t--- Loss: 2.419\n",
      "Iteration: 438 \t--- Loss: 2.413\n",
      "Iteration: 439 \t--- Loss: 2.464\n",
      "Iteration: 440 \t--- Loss: 2.499\n",
      "Iteration: 441 \t--- Loss: 2.534\n",
      "Iteration: 442 \t--- Loss: 2.404\n",
      "Iteration: 443 \t--- Loss: 2.431\n",
      "Iteration: 444 \t--- Loss: 2.393\n",
      "Iteration: 445 \t--- Loss: 2.427\n",
      "Iteration: 446 \t--- Loss: 2.609\n",
      "Iteration: 447 \t--- Loss: 2.588\n",
      "Iteration: 448 \t--- Loss: 2.547\n",
      "Iteration: 449 \t--- Loss: 2.447\n",
      "Iteration: 450 \t--- Loss: 2.612\n",
      "Iteration: 451 \t--- Loss: 2.511\n",
      "Iteration: 452 \t--- Loss: 2.536\n",
      "Iteration: 453 \t--- Loss: 2.438\n",
      "Iteration: 454 \t--- Loss: 2.287\n",
      "Iteration: 455 \t--- Loss: 2.615\n",
      "Iteration: 456 \t--- Loss: 2.491\n",
      "Iteration: 457 \t--- Loss: 2.493\n",
      "Iteration: 458 \t--- Loss: 2.410\n",
      "Iteration: 459 \t--- Loss: 2.556\n",
      "Iteration: 460 \t--- Loss: 2.460\n",
      "Iteration: 461 \t--- Loss: 2.347\n",
      "Iteration: 462 \t--- Loss: 2.441\n",
      "Iteration: 463 \t--- Loss: 2.508\n",
      "Iteration: 464 \t--- Loss: 2.516\n",
      "Iteration: 465 \t--- Loss: 2.460\n",
      "Iteration: 466 \t--- Loss: 2.456\n",
      "Iteration: 467 \t--- Loss: 2.349\n",
      "Iteration: 468 \t--- Loss: 2.448\n",
      "Iteration: 469 \t--- Loss: 2.449\n",
      "Iteration: 470 \t--- Loss: 2.514\n",
      "Iteration: 471 \t--- Loss: 2.411\n",
      "Iteration: 472 \t--- Loss: 2.476\n",
      "Iteration: 473 \t--- Loss: 2.446\n",
      "Iteration: 474 \t--- Loss: 2.421\n",
      "Iteration: 475 \t--- Loss: 2.468\n",
      "Iteration: 476 \t--- Loss: 2.647\n",
      "Iteration: 477 \t--- Loss: 2.593\n",
      "Iteration: 478 \t--- Loss: 2.378\n",
      "Iteration: 479 \t--- Loss: 2.424\n",
      "Iteration: 480 \t--- Loss: 2.438\n",
      "Iteration: 481 \t--- Loss: 2.326\n",
      "Iteration: 482 \t--- Loss: 2.350\n",
      "Iteration: 483 \t--- Loss: 2.409\n",
      "Iteration: 484 \t--- Loss: 2.420\n",
      "Iteration: 485 \t--- Loss: 2.392\n",
      "Iteration: 486 \t--- Loss: 2.472\n",
      "Iteration: 487 \t--- Loss: 2.405\n",
      "Iteration: 488 \t--- Loss: 2.474\n",
      "Iteration: 489 \t--- Loss: 2.471\n",
      "Iteration: 490 \t--- Loss: 2.533\n",
      "Iteration: 491 \t--- Loss: 2.480\n",
      "Iteration: 492 \t--- Loss: 2.511\n",
      "Iteration: 493 \t--- Loss: 2.311\n",
      "Iteration: 494 \t--- Loss: 2.443\n",
      "Iteration: 495 \t--- Loss: 2.494\n",
      "Iteration: 496 \t--- Loss: 2.440\n",
      "Iteration: 497 \t--- Loss: 2.316\n",
      "Iteration: 498 \t--- Loss: 2.587\n",
      "Iteration: 499 \t--- Loss: 2.473\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:58<00:00, 58.19s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.057\n",
      "Iteration: 261 \t--- Loss: 0.057\n",
      "Iteration: 262 \t--- Loss: 0.059\n",
      "Iteration: 263 \t--- Loss: 0.060\n",
      "Iteration: 264 \t--- Loss: 0.055\n",
      "Iteration: 265 \t--- Loss: 0.059\n",
      "Iteration: 266 \t--- Loss: 0.055\n",
      "Iteration: 267 \t--- Loss: 0.058\n",
      "Iteration: 268 \t--- Loss: 0.051\n",
      "Iteration: 269 \t--- Loss: 0.051\n",
      "Iteration: 270 \t--- Loss: 0.053\n",
      "Iteration: 271 \t--- Loss: 0.056\n",
      "Iteration: 272 \t--- Loss: 0.054\n",
      "Iteration: 273 \t--- Loss: 0.055\n",
      "Iteration: 274 \t--- Loss: 0.051\n",
      "Iteration: 275 \t--- Loss: 0.055\n",
      "Iteration: 276 \t--- Loss: 0.055\n",
      "Iteration: 277 \t--- Loss: 0.055\n",
      "Iteration: 278 \t--- Loss: 0.054\n",
      "Iteration: 279 \t--- Loss: 0.052\n",
      "Iteration: 280 \t--- Loss: 0.056\n",
      "Iteration: 281 \t--- Loss: 0.051\n",
      "Iteration: 282 \t--- Loss: 0.054\n",
      "Iteration: 283 \t--- Loss: 0.059\n",
      "Iteration: 284 \t--- Loss: 0.052\n",
      "Iteration: 285 \t--- Loss: 0.050\n",
      "Iteration: 286 \t--- Loss: 0.056\n",
      "Iteration: 287 \t--- Loss: 0.056\n",
      "Iteration: 288 \t--- Loss: 0.059\n",
      "Iteration: 289 \t--- Loss: 0.050\n",
      "Iteration: 290 \t--- Loss: 0.053\n",
      "Iteration: 291 \t--- Loss: 0.056\n",
      "Iteration: 292 \t--- Loss: 0.055\n",
      "Iteration: 293 \t--- Loss: 0.048\n",
      "Iteration: 294 \t--- Loss: 0.055\n",
      "Iteration: 295 \t--- Loss: 0.058\n",
      "Iteration: 296 \t--- Loss: 0.052\n",
      "Iteration: 297 \t--- Loss: 0.054\n",
      "Iteration: 298 \t--- Loss: 0.057\n",
      "Iteration: 299 \t--- Loss: 0.055\n",
      "Iteration: 300 \t--- Loss: 0.056\n",
      "Iteration: 301 \t--- Loss: 0.055\n",
      "Iteration: 302 \t--- Loss: 0.052\n",
      "Iteration: 303 \t--- Loss: 0.060\n",
      "Iteration: 304 \t--- Loss: 0.053\n",
      "Iteration: 305 \t--- Loss: 0.055\n",
      "Iteration: 306 \t--- Loss: 0.054\n",
      "Iteration: 307 \t--- Loss: 0.049\n",
      "Iteration: 308 \t--- Loss: 0.048\n",
      "Iteration: 309 \t--- Loss: 0.052\n",
      "Iteration: 310 \t--- Loss: 0.050\n",
      "Iteration: 311 \t--- Loss: 0.051\n",
      "Iteration: 312 \t--- Loss: 0.053\n",
      "Iteration: 313 \t--- Loss: 0.052\n",
      "Iteration: 314 \t--- Loss: 0.052\n",
      "Iteration: 315 \t--- Loss: 0.050\n",
      "Iteration: 316 \t--- Loss: 0.052\n",
      "Iteration: 317 \t--- Loss: 0.053\n",
      "Iteration: 318 \t--- Loss: 0.052\n",
      "Iteration: 319 \t--- Loss: 0.048\n",
      "Iteration: 320 \t--- Loss: 0.049\n",
      "Iteration: 321 \t--- Loss: 0.048\n",
      "Iteration: 322 \t--- Loss: 0.049\n",
      "Iteration: 323 \t--- Loss: 0.049\n",
      "Iteration: 324 \t--- Loss: 0.052\n",
      "Iteration: 325 \t--- Loss: 0.050\n",
      "Iteration: 326 \t--- Loss: 0.050\n",
      "Iteration: 327 \t--- Loss: 0.053\n",
      "Iteration: 328 \t--- Loss: 0.054\n",
      "Iteration: 329 \t--- Loss: 0.052\n",
      "Iteration: 330 \t--- Loss: 0.051\n",
      "Iteration: 331 \t--- Loss: 0.048\n",
      "Iteration: 332 \t--- Loss: 0.048\n",
      "Iteration: 333 \t--- Loss: 0.053\n",
      "Iteration: 334 \t--- Loss: 0.050\n",
      "Iteration: 335 \t--- Loss: 0.049\n",
      "Iteration: 336 \t--- Loss: 0.049\n",
      "Iteration: 337 \t--- Loss: 0.050\n",
      "Iteration: 338 \t--- Loss: 0.046\n",
      "Iteration: 339 \t--- Loss: 0.051\n",
      "Iteration: 340 \t--- Loss: 0.050\n",
      "Iteration: 341 \t--- Loss: 0.047\n",
      "Iteration: 342 \t--- Loss: 0.044\n",
      "Iteration: 343 \t--- Loss: 0.048\n",
      "Iteration: 344 \t--- Loss: 0.045\n",
      "Iteration: 345 \t--- Loss: 0.045\n",
      "Iteration: 346 \t--- Loss: 0.046\n",
      "Iteration: 347 \t--- Loss: 0.043\n",
      "Iteration: 348 \t--- Loss: 0.046\n",
      "Iteration: 349 \t--- Loss: 0.047\n",
      "Iteration: 350 \t--- Loss: 0.042\n",
      "Iteration: 351 \t--- Loss: 0.046\n",
      "Iteration: 352 \t--- Loss: 0.041\n",
      "Iteration: 353 \t--- Loss: 0.042\n",
      "Iteration: 354 \t--- Loss: 0.043\n",
      "Iteration: 355 \t--- Loss: 0.046\n",
      "Iteration: 356 \t--- Loss: 0.040\n",
      "Iteration: 357 \t--- Loss: 0.042\n",
      "Iteration: 358 \t--- Loss: 0.043\n",
      "Iteration: 359 \t--- Loss: 0.042\n",
      "Iteration: 360 \t--- Loss: 0.038\n",
      "Iteration: 361 \t--- Loss: 0.044\n",
      "Iteration: 362 \t--- Loss: 0.042\n",
      "Iteration: 363 \t--- Loss: 0.036\n",
      "Iteration: 364 \t--- Loss: 0.042\n",
      "Iteration: 365 \t--- Loss: 0.043\n",
      "Iteration: 366 \t--- Loss: 0.042\n",
      "Iteration: 367 \t--- Loss: 0.043\n",
      "Iteration: 368 \t--- Loss: 0.038\n",
      "Iteration: 369 \t--- Loss: 0.043\n",
      "Iteration: 370 \t--- Loss: 0.038\n",
      "Iteration: 371 \t--- Loss: 0.040\n",
      "Iteration: 372 \t--- Loss: 0.036\n",
      "Iteration: 373 \t--- Loss: 0.039\n",
      "Iteration: 374 \t--- Loss: 0.039\n",
      "Iteration: 375 \t--- Loss: 0.040\n",
      "Iteration: 376 \t--- Loss: 0.038\n",
      "Iteration: 377 \t--- Loss: 0.044\n",
      "Iteration: 378 \t--- Loss: 0.037\n",
      "Iteration: 379 \t--- Loss: 0.032\n",
      "Iteration: 380 \t--- Loss: 0.039\n",
      "Iteration: 381 \t--- Loss: 0.039\n",
      "Iteration: 382 \t--- Loss: 0.040\n",
      "Iteration: 383 \t--- Loss: 0.039\n",
      "Iteration: 384 \t--- Loss: 0.040\n",
      "Iteration: 385 \t--- Loss: 0.037\n",
      "Iteration: 386 \t--- Loss: 0.036\n",
      "Iteration: 387 \t--- Loss: 0.039\n",
      "Iteration: 388 \t--- Loss: 0.035\n",
      "Iteration: 389 \t--- Loss: 0.037\n",
      "Iteration: 390 \t--- Loss: 0.040\n",
      "Iteration: 391 \t--- Loss: 0.037\n",
      "Iteration: 392 \t--- Loss: 0.039\n",
      "Iteration: 393 \t--- Loss: 0.036\n",
      "Iteration: 394 \t--- Loss: 0.037\n",
      "Iteration: 395 \t--- Loss: 0.037\n",
      "Iteration: 396 \t--- Loss: 0.036\n",
      "Iteration: 397 \t--- Loss: 0.043\n",
      "Iteration: 398 \t--- Loss: 0.034\n",
      "Iteration: 399 \t--- Loss: 0.036\n",
      "Iteration: 400 \t--- Loss: 0.037\n",
      "Iteration: 401 \t--- Loss: 0.036\n",
      "Iteration: 402 \t--- Loss: 0.033\n",
      "Iteration: 403 \t--- Loss: 0.038\n",
      "Iteration: 404 \t--- Loss: 0.038\n",
      "Iteration: 405 \t--- Loss: 0.038\n",
      "Iteration: 406 \t--- Loss: 0.041\n",
      "Iteration: 407 \t--- Loss: 0.035\n",
      "Iteration: 408 \t--- Loss: 0.035\n",
      "Iteration: 409 \t--- Loss: 0.036\n",
      "Iteration: 410 \t--- Loss: 0.037\n",
      "Iteration: 411 \t--- Loss: 0.035\n",
      "Iteration: 412 \t--- Loss: 0.034\n",
      "Iteration: 413 \t--- Loss: 0.037\n",
      "Iteration: 414 \t--- Loss: 0.032\n",
      "Iteration: 415 \t--- Loss: 0.033\n",
      "Iteration: 416 \t--- Loss: 0.034\n",
      "Iteration: 417 \t--- Loss: 0.033\n",
      "Iteration: 418 \t--- Loss: 0.032\n",
      "Iteration: 419 \t--- Loss: 0.035\n",
      "Iteration: 420 \t--- Loss: 0.035\n",
      "Iteration: 421 \t--- Loss: 0.033\n",
      "Iteration: 422 \t--- Loss: 0.037\n",
      "Iteration: 423 \t--- Loss: 0.034\n",
      "Iteration: 424 \t--- Loss: 0.033\n",
      "Iteration: 425 \t--- Loss: 0.034\n",
      "Iteration: 426 \t--- Loss: 0.031\n",
      "Iteration: 427 \t--- Loss: 0.036\n",
      "Iteration: 428 \t--- Loss: 0.031\n",
      "Iteration: 429 \t--- Loss: 0.035\n",
      "Iteration: 430 \t--- Loss: 0.038\n",
      "Iteration: 431 \t--- Loss: 0.033\n",
      "Iteration: 432 \t--- Loss: 0.033\n",
      "Iteration: 433 \t--- Loss: 0.038\n",
      "Iteration: 434 \t--- Loss: 0.035\n",
      "Iteration: 435 \t--- Loss: 0.030\n",
      "Iteration: 436 \t--- Loss: 0.032\n",
      "Iteration: 437 \t--- Loss: 0.034\n",
      "Iteration: 438 \t--- Loss: 0.034\n",
      "Iteration: 439 \t--- Loss: 0.032\n",
      "Iteration: 440 \t--- Loss: 0.033\n",
      "Iteration: 441 \t--- Loss: 0.037\n",
      "Iteration: 442 \t--- Loss: 0.034\n",
      "Iteration: 443 \t--- Loss: 0.032\n",
      "Iteration: 444 \t--- Loss: 0.030\n",
      "Iteration: 445 \t--- Loss: 0.034\n",
      "Iteration: 446 \t--- Loss: 0.034\n",
      "Iteration: 447 \t--- Loss: 0.035\n",
      "Iteration: 448 \t--- Loss: 0.032\n",
      "Iteration: 449 \t--- Loss: 0.031\n",
      "Iteration: 450 \t--- Loss: 0.034\n",
      "Iteration: 451 \t--- Loss: 0.038\n",
      "Iteration: 452 \t--- Loss: 0.030\n",
      "Iteration: 453 \t--- Loss: 0.036\n",
      "Iteration: 454 \t--- Loss: 0.032\n",
      "Iteration: 455 \t--- Loss: 0.033\n",
      "Iteration: 456 \t--- Loss: 0.031\n",
      "Iteration: 457 \t--- Loss: 0.034\n",
      "Iteration: 458 \t--- Loss: 0.034\n",
      "Iteration: 459 \t--- Loss: 0.033\n",
      "Iteration: 460 \t--- Loss: 0.031\n",
      "Iteration: 461 \t--- Loss: 0.032\n",
      "Iteration: 462 \t--- Loss: 0.033\n",
      "Iteration: 463 \t--- Loss: 0.035\n",
      "Iteration: 464 \t--- Loss: 0.031\n",
      "Iteration: 465 \t--- Loss: 0.036\n",
      "Iteration: 466 \t--- Loss: 0.033\n",
      "Iteration: 467 \t--- Loss: 0.033\n",
      "Iteration: 468 \t--- Loss: 0.031\n",
      "Iteration: 469 \t--- Loss: 0.035\n",
      "Iteration: 470 \t--- Loss: 0.035\n",
      "Iteration: 471 \t--- Loss: 0.034\n",
      "Iteration: 472 \t--- Loss: 0.035\n",
      "Iteration: 473 \t--- Loss: 0.032\n",
      "Iteration: 474 \t--- Loss: 0.032\n",
      "Iteration: 475 \t--- Loss: 0.032\n",
      "Iteration: 476 \t--- Loss: 0.034\n",
      "Iteration: 477 \t--- Loss: 0.031\n",
      "Iteration: 478 \t--- Loss: 0.031\n",
      "Iteration: 479 \t--- Loss: 0.032\n",
      "Iteration: 480 \t--- Loss: 0.031\n",
      "Iteration: 481 \t--- Loss: 0.029\n",
      "Iteration: 482 \t--- Loss: 0.030\n",
      "Iteration: 483 \t--- Loss: 0.035\n",
      "Iteration: 484 \t--- Loss: 0.027\n",
      "Iteration: 485 \t--- Loss: 0.032\n",
      "Iteration: 486 \t--- Loss: 0.032\n",
      "Iteration: 487 \t--- Loss: 0.034\n",
      "Iteration: 488 \t--- Loss: 0.032\n",
      "Iteration: 489 \t--- Loss: 0.035\n",
      "Iteration: 490 \t--- Loss: 0.030\n",
      "Iteration: 491 \t--- Loss: 0.032\n",
      "Iteration: 492 \t--- Loss: 0.032\n",
      "Iteration: 493 \t--- Loss: 0.031\n",
      "Iteration: 494 \t--- Loss: 0.032\n",
      "Iteration: 495 \t--- Loss: 0.029\n",
      "Iteration: 496 \t--- Loss: 0.033\n",
      "Iteration: 497 \t--- Loss: 0.033\n",
      "Iteration: 498 \t--- Loss: 0.036\n",
      "Iteration: 499 \t--- Loss: 0.033\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:00<00:00, 60.56s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.218\n",
      "Iteration: 261 \t--- Loss: 0.228\n",
      "Iteration: 262 \t--- Loss: 0.228\n",
      "Iteration: 263 \t--- Loss: 0.223\n",
      "Iteration: 264 \t--- Loss: 0.231\n",
      "Iteration: 265 \t--- Loss: 0.227\n",
      "Iteration: 266 \t--- Loss: 0.224\n",
      "Iteration: 267 \t--- Loss: 0.242\n",
      "Iteration: 268 \t--- Loss: 0.238\n",
      "Iteration: 269 \t--- Loss: 0.227\n",
      "Iteration: 270 \t--- Loss: 0.220\n",
      "Iteration: 271 \t--- Loss: 0.227\n",
      "Iteration: 272 \t--- Loss: 0.221\n",
      "Iteration: 273 \t--- Loss: 0.232\n",
      "Iteration: 274 \t--- Loss: 0.223\n",
      "Iteration: 275 \t--- Loss: 0.221\n",
      "Iteration: 276 \t--- Loss: 0.220\n",
      "Iteration: 277 \t--- Loss: 0.221\n",
      "Iteration: 278 \t--- Loss: 0.238\n",
      "Iteration: 279 \t--- Loss: 0.230\n",
      "Iteration: 280 \t--- Loss: 0.232\n",
      "Iteration: 281 \t--- Loss: 0.238\n",
      "Iteration: 282 \t--- Loss: 0.239\n",
      "Iteration: 283 \t--- Loss: 0.210\n",
      "Iteration: 284 \t--- Loss: 0.236\n",
      "Iteration: 285 \t--- Loss: 0.234\n",
      "Iteration: 286 \t--- Loss: 0.217\n",
      "Iteration: 287 \t--- Loss: 0.226\n",
      "Iteration: 288 \t--- Loss: 0.230\n",
      "Iteration: 289 \t--- Loss: 0.220\n",
      "Iteration: 290 \t--- Loss: 0.227\n",
      "Iteration: 291 \t--- Loss: 0.241\n",
      "Iteration: 292 \t--- Loss: 0.205\n",
      "Iteration: 293 \t--- Loss: 0.226\n",
      "Iteration: 294 \t--- Loss: 0.249\n",
      "Iteration: 295 \t--- Loss: 0.212\n",
      "Iteration: 296 \t--- Loss: 0.234\n",
      "Iteration: 297 \t--- Loss: 0.220\n",
      "Iteration: 298 \t--- Loss: 0.220\n",
      "Iteration: 299 \t--- Loss: 0.236\n",
      "Iteration: 300 \t--- Loss: 0.229\n",
      "Iteration: 301 \t--- Loss: 0.237\n",
      "Iteration: 302 \t--- Loss: 0.228\n",
      "Iteration: 303 \t--- Loss: 0.213\n",
      "Iteration: 304 \t--- Loss: 0.227\n",
      "Iteration: 305 \t--- Loss: 0.231\n",
      "Iteration: 306 \t--- Loss: 0.233\n",
      "Iteration: 307 \t--- Loss: 0.226\n",
      "Iteration: 308 \t--- Loss: 0.229\n",
      "Iteration: 309 \t--- Loss: 0.218\n",
      "Iteration: 310 \t--- Loss: 0.227\n",
      "Iteration: 311 \t--- Loss: 0.242\n",
      "Iteration: 312 \t--- Loss: 0.223\n",
      "Iteration: 313 \t--- Loss: 0.230\n",
      "Iteration: 314 \t--- Loss: 0.226\n",
      "Iteration: 315 \t--- Loss: 0.219\n",
      "Iteration: 316 \t--- Loss: 0.238\n",
      "Iteration: 317 \t--- Loss: 0.230\n",
      "Iteration: 318 \t--- Loss: 0.225\n",
      "Iteration: 319 \t--- Loss: 0.230\n",
      "Iteration: 320 \t--- Loss: 0.220\n",
      "Iteration: 321 \t--- Loss: 0.221\n",
      "Iteration: 322 \t--- Loss: 0.219\n",
      "Iteration: 323 \t--- Loss: 0.239\n",
      "Iteration: 324 \t--- Loss: 0.235\n",
      "Iteration: 325 \t--- Loss: 0.214\n",
      "Iteration: 326 \t--- Loss: 0.227\n",
      "Iteration: 327 \t--- Loss: 0.224\n",
      "Iteration: 328 \t--- Loss: 0.220\n",
      "Iteration: 329 \t--- Loss: 0.228\n",
      "Iteration: 330 \t--- Loss: 0.227\n",
      "Iteration: 331 \t--- Loss: 0.247\n",
      "Iteration: 332 \t--- Loss: 0.222\n",
      "Iteration: 333 \t--- Loss: 0.241\n",
      "Iteration: 334 \t--- Loss: 0.241\n",
      "Iteration: 335 \t--- Loss: 0.240\n",
      "Iteration: 336 \t--- Loss: 0.223\n",
      "Iteration: 337 \t--- Loss: 0.230\n",
      "Iteration: 338 \t--- Loss: 0.223\n",
      "Iteration: 339 \t--- Loss: 0.230\n",
      "Iteration: 340 \t--- Loss: 0.237\n",
      "Iteration: 341 \t--- Loss: 0.242\n",
      "Iteration: 342 \t--- Loss: 0.226\n",
      "Iteration: 343 \t--- Loss: 0.216\n",
      "Iteration: 344 \t--- Loss: 0.220\n",
      "Iteration: 345 \t--- Loss: 0.232\n",
      "Iteration: 346 \t--- Loss: 0.225\n",
      "Iteration: 347 \t--- Loss: 0.221\n",
      "Iteration: 348 \t--- Loss: 0.231\n",
      "Iteration: 349 \t--- Loss: 0.235\n",
      "Iteration: 350 \t--- Loss: 0.243\n",
      "Iteration: 351 \t--- Loss: 0.245\n",
      "Iteration: 352 \t--- Loss: 0.226\n",
      "Iteration: 353 \t--- Loss: 0.237\n",
      "Iteration: 354 \t--- Loss: 0.239\n",
      "Iteration: 355 \t--- Loss: 0.208\n",
      "Iteration: 356 \t--- Loss: 0.212\n",
      "Iteration: 357 \t--- Loss: 0.228\n",
      "Iteration: 358 \t--- Loss: 0.241\n",
      "Iteration: 359 \t--- Loss: 0.227\n",
      "Iteration: 360 \t--- Loss: 0.227\n",
      "Iteration: 361 \t--- Loss: 0.231\n",
      "Iteration: 362 \t--- Loss: 0.232\n",
      "Iteration: 363 \t--- Loss: 0.228\n",
      "Iteration: 364 \t--- Loss: 0.231\n",
      "Iteration: 365 \t--- Loss: 0.218\n",
      "Iteration: 366 \t--- Loss: 0.240\n",
      "Iteration: 367 \t--- Loss: 0.226\n",
      "Iteration: 368 \t--- Loss: 0.230\n",
      "Iteration: 369 \t--- Loss: 0.234\n",
      "Iteration: 370 \t--- Loss: 0.222\n",
      "Iteration: 371 \t--- Loss: 0.225\n",
      "Iteration: 372 \t--- Loss: 0.225\n",
      "Iteration: 373 \t--- Loss: 0.233\n",
      "Iteration: 374 \t--- Loss: 0.238\n",
      "Iteration: 375 \t--- Loss: 0.229\n",
      "Iteration: 376 \t--- Loss: 0.230\n",
      "Iteration: 377 \t--- Loss: 0.241\n",
      "Iteration: 378 \t--- Loss: 0.226\n",
      "Iteration: 379 \t--- Loss: 0.227\n",
      "Iteration: 380 \t--- Loss: 0.247\n",
      "Iteration: 381 \t--- Loss: 0.237\n",
      "Iteration: 382 \t--- Loss: 0.232\n",
      "Iteration: 383 \t--- Loss: 0.239\n",
      "Iteration: 384 \t--- Loss: 0.228\n",
      "Iteration: 385 \t--- Loss: 0.235\n",
      "Iteration: 386 \t--- Loss: 0.235\n",
      "Iteration: 387 \t--- Loss: 0.224\n",
      "Iteration: 388 \t--- Loss: 0.212\n",
      "Iteration: 389 \t--- Loss: 0.218\n",
      "Iteration: 390 \t--- Loss: 0.230\n",
      "Iteration: 391 \t--- Loss: 0.225\n",
      "Iteration: 392 \t--- Loss: 0.244\n",
      "Iteration: 393 \t--- Loss: 0.221\n",
      "Iteration: 394 \t--- Loss: 0.229\n",
      "Iteration: 395 \t--- Loss: 0.236\n",
      "Iteration: 396 \t--- Loss: 0.227\n",
      "Iteration: 397 \t--- Loss: 0.234\n",
      "Iteration: 398 \t--- Loss: 0.234\n",
      "Iteration: 399 \t--- Loss: 0.226\n",
      "Iteration: 400 \t--- Loss: 0.237\n",
      "Iteration: 401 \t--- Loss: 0.241\n",
      "Iteration: 402 \t--- Loss: 0.223\n",
      "Iteration: 403 \t--- Loss: 0.226\n",
      "Iteration: 404 \t--- Loss: 0.224\n",
      "Iteration: 405 \t--- Loss: 0.218\n",
      "Iteration: 406 \t--- Loss: 0.202\n",
      "Iteration: 407 \t--- Loss: 0.235\n",
      "Iteration: 408 \t--- Loss: 0.225\n",
      "Iteration: 409 \t--- Loss: 0.236\n",
      "Iteration: 410 \t--- Loss: 0.221\n",
      "Iteration: 411 \t--- Loss: 0.232\n",
      "Iteration: 412 \t--- Loss: 0.235\n",
      "Iteration: 413 \t--- Loss: 0.229\n",
      "Iteration: 414 \t--- Loss: 0.228\n",
      "Iteration: 415 \t--- Loss: 0.234\n",
      "Iteration: 416 \t--- Loss: 0.241\n",
      "Iteration: 417 \t--- Loss: 0.240\n",
      "Iteration: 418 \t--- Loss: 0.232\n",
      "Iteration: 419 \t--- Loss: 0.227\n",
      "Iteration: 420 \t--- Loss: 0.238\n",
      "Iteration: 421 \t--- Loss: 0.235\n",
      "Iteration: 422 \t--- Loss: 0.241\n",
      "Iteration: 423 \t--- Loss: 0.236\n",
      "Iteration: 424 \t--- Loss: 0.231\n",
      "Iteration: 425 \t--- Loss: 0.226\n",
      "Iteration: 426 \t--- Loss: 0.221\n",
      "Iteration: 427 \t--- Loss: 0.236\n",
      "Iteration: 428 \t--- Loss: 0.223\n",
      "Iteration: 429 \t--- Loss: 0.233\n",
      "Iteration: 430 \t--- Loss: 0.221\n",
      "Iteration: 431 \t--- Loss: 0.218\n",
      "Iteration: 432 \t--- Loss: 0.217\n",
      "Iteration: 433 \t--- Loss: 0.226\n",
      "Iteration: 434 \t--- Loss: 0.236\n",
      "Iteration: 435 \t--- Loss: 0.236\n",
      "Iteration: 436 \t--- Loss: 0.232\n",
      "Iteration: 437 \t--- Loss: 0.221\n",
      "Iteration: 438 \t--- Loss: 0.228\n",
      "Iteration: 439 \t--- Loss: 0.216\n",
      "Iteration: 440 \t--- Loss: 0.223\n",
      "Iteration: 441 \t--- Loss: 0.228\n",
      "Iteration: 442 \t--- Loss: 0.218\n",
      "Iteration: 443 \t--- Loss: 0.228\n",
      "Iteration: 444 \t--- Loss: 0.234\n",
      "Iteration: 445 \t--- Loss: 0.224\n",
      "Iteration: 446 \t--- Loss: 0.201\n",
      "Iteration: 447 \t--- Loss: 0.220\n",
      "Iteration: 448 \t--- Loss: 0.228\n",
      "Iteration: 449 \t--- Loss: 0.211\n",
      "Iteration: 450 \t--- Loss: 0.229\n",
      "Iteration: 451 \t--- Loss: 0.220\n",
      "Iteration: 452 \t--- Loss: 0.237\n",
      "Iteration: 453 \t--- Loss: 0.222\n",
      "Iteration: 454 \t--- Loss: 0.230\n",
      "Iteration: 455 \t--- Loss: 0.221\n",
      "Iteration: 456 \t--- Loss: 0.212\n",
      "Iteration: 457 \t--- Loss: 0.247\n",
      "Iteration: 458 \t--- Loss: 0.222\n",
      "Iteration: 459 \t--- Loss: 0.224\n",
      "Iteration: 460 \t--- Loss: 0.228\n",
      "Iteration: 461 \t--- Loss: 0.230\n",
      "Iteration: 462 \t--- Loss: 0.232\n",
      "Iteration: 463 \t--- Loss: 0.225\n",
      "Iteration: 464 \t--- Loss: 0.236\n",
      "Iteration: 465 \t--- Loss: 0.229\n",
      "Iteration: 466 \t--- Loss: 0.236\n",
      "Iteration: 467 \t--- Loss: 0.221\n",
      "Iteration: 468 \t--- Loss: 0.239\n",
      "Iteration: 469 \t--- Loss: 0.233\n",
      "Iteration: 470 \t--- Loss: 0.221\n",
      "Iteration: 471 \t--- Loss: 0.228\n",
      "Iteration: 472 \t--- Loss: 0.229\n",
      "Iteration: 473 \t--- Loss: 0.230\n",
      "Iteration: 474 \t--- Loss: 0.216\n",
      "Iteration: 475 \t--- Loss: 0.229\n",
      "Iteration: 476 \t--- Loss: 0.240\n",
      "Iteration: 477 \t--- Loss: 0.202\n",
      "Iteration: 478 \t--- Loss: 0.225\n",
      "Iteration: 479 \t--- Loss: 0.220\n",
      "Iteration: 480 \t--- Loss: 0.244\n",
      "Iteration: 481 \t--- Loss: 0.244\n",
      "Iteration: 482 \t--- Loss: 0.223\n",
      "Iteration: 483 \t--- Loss: 0.242\n",
      "Iteration: 484 \t--- Loss: 0.214\n",
      "Iteration: 485 \t--- Loss: 0.226\n",
      "Iteration: 486 \t--- Loss: 0.234\n",
      "Iteration: 487 \t--- Loss: 0.220\n",
      "Iteration: 488 \t--- Loss: 0.241\n",
      "Iteration: 489 \t--- Loss: 0.231\n",
      "Iteration: 490 \t--- Loss: 0.237\n",
      "Iteration: 491 \t--- Loss: 0.227\n",
      "Iteration: 492 \t--- Loss: 0.222\n",
      "Iteration: 493 \t--- Loss: 0.232\n",
      "Iteration: 494 \t--- Loss: 0.218\n",
      "Iteration: 495 \t--- Loss: 0.227\n",
      "Iteration: 496 \t--- Loss: 0.224\n",
      "Iteration: 497 \t--- Loss: 0.226\n",
      "Iteration: 498 \t--- Loss: 0.234\n",
      "Iteration: 499 \t--- Loss: 0.232\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:09<00:02,  1.17s/it][Parallel(n_jobs=5)]: Done   1 tasks      | elapsed:  1.1min\n",
      "\n",
      " 10%|█         | 1/10 [00:00<00:08,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.19s/it][Parallel(n_jobs=5)]: Done   2 tasks      | elapsed:  1.2min\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.14s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.16s/it][Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:  1.3min\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n",
      " 80%|████████  | 8/10 [00:09<00:02,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.20s/it][Parallel(n_jobs=5)]: Done   4 tasks      | elapsed:  1.3min\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.685\n",
      "Iteration: 1 \t--- Loss: 0.537\n",
      "Iteration: 2 \t--- Loss: 0.516\n",
      "Iteration: 3 \t--- Loss: 0.534\n",
      "Iteration: 4 \t--- Loss: 0.491\n",
      "Iteration: 5 \t--- Loss: 0.439\n",
      "Iteration: 6 \t--- Loss: 0.482\n",
      "Iteration: 7 \t--- Loss: 0.452\n",
      "Iteration: 8 \t--- Loss: 0.460\n",
      "Iteration: 9 \t--- Loss: 0.471\n",
      "Iteration: 10 \t--- Loss: 0.441\n",
      "Iteration: 11 \t--- Loss: 0.449\n",
      "Iteration: 12 \t--- Loss: 0.436\n",
      "Iteration: 13 \t--- Loss: 0.392\n",
      "Iteration: 14 \t--- Loss: 0.433\n",
      "Iteration: 15 \t--- Loss: 0.412\n",
      "Iteration: 16 \t--- Loss: 0.397\n",
      "Iteration: 17 \t--- Loss: 0.377\n",
      "Iteration: 18 \t--- Loss: 0.364\n",
      "Iteration: 19 \t--- Loss: 0.421\n",
      "Iteration: 20 \t--- Loss: 0.400\n",
      "Iteration: 21 \t--- Loss: 0.422\n",
      "Iteration: 22 \t--- Loss: 0.367\n",
      "Iteration: 23 \t--- Loss: 0.378\n",
      "Iteration: 24 \t--- Loss: 0.396\n",
      "Iteration: 25 \t--- Loss: 0.387\n",
      "Iteration: 26 \t--- Loss: 0.389\n",
      "Iteration: 27 \t--- Loss: 0.365\n",
      "Iteration: 28 \t--- Loss: 0.331\n",
      "Iteration: 29 \t--- Loss: 0.403\n",
      "Iteration: 30 \t--- Loss: 0.413\n",
      "Iteration: 31 \t--- Loss: 0.365\n",
      "Iteration: 32 \t--- Loss: 0.375\n",
      "Iteration: 33 \t--- Loss: 0.379\n",
      "Iteration: 34 \t--- Loss: 0.382\n",
      "Iteration: 35 \t--- Loss: 0.343\n",
      "Iteration: 36 \t--- Loss: 0.381\n",
      "Iteration: 37 \t--- Loss: 0.361\n",
      "Iteration: 38 \t--- Loss: 0.371\n",
      "Iteration: 39 \t--- Loss: 0.363\n",
      "Iteration: 40 \t--- Loss: 0.445\n",
      "Iteration: 41 \t--- Loss: 0.336\n",
      "Iteration: 42 \t--- Loss: 0.379\n",
      "Iteration: 43 \t--- Loss: 0.383\n",
      "Iteration: 44 \t--- Loss: 0.349\n",
      "Iteration: 45 \t--- Loss: 0.390\n",
      "Iteration: 46 \t--- Loss: 0.369\n",
      "Iteration: 47 \t--- Loss: 0.403\n",
      "Iteration: 48 \t--- Loss: 0.331\n",
      "Iteration: 49 \t--- Loss: 0.368\n",
      "Iteration: 50 \t--- Loss: 0.418\n",
      "Iteration: 51 \t--- Loss: 0.356\n",
      "Iteration: 52 \t--- Loss: 0.360\n",
      "Iteration: 53 \t--- Loss: 0.380\n",
      "Iteration: 54 \t--- Loss: 0.344\n",
      "Iteration: 55 \t--- Loss: 0.360\n",
      "Iteration: 56 \t--- Loss: 0.350\n",
      "Iteration: 57 \t--- Loss: 0.381\n",
      "Iteration: 58 \t--- Loss: 0.375\n",
      "Iteration: 59 \t--- Loss: 0.369\n",
      "Iteration: 60 \t--- Loss: 0.335\n",
      "Iteration: 61 \t--- Loss: 0.376\n",
      "Iteration: 62 \t--- Loss: 0.340\n",
      "Iteration: 63 \t--- Loss: 0.355\n",
      "Iteration: 64 \t--- Loss: 0.405\n",
      "Iteration: 65 \t--- Loss: 0.337\n",
      "Iteration: 66 \t--- Loss: 0.377\n",
      "Iteration: 67 \t--- Loss: 0.392\n",
      "Iteration: 68 \t--- Loss: 0.350\n",
      "Iteration: 69 \t--- Loss: 0.363\n",
      "Iteration: 70 \t--- Loss: 0.348\n",
      "Iteration: 71 \t--- Loss: 0.355\n",
      "Iteration: 72 \t--- Loss: 0.378\n",
      "Iteration: 73 \t--- Loss: 0.397\n",
      "Iteration: 74 \t--- Loss: 0.346\n",
      "Iteration: 75 \t--- Loss: 0.376\n",
      "Iteration: 76 \t--- Loss: 0.366\n",
      "Iteration: 77 \t--- Loss: 0.369\n",
      "Iteration: 78 \t--- Loss: 0.371\n",
      "Iteration: 79 \t--- Loss: 0.369\n",
      "Iteration: 80 \t--- Loss: 0.339\n",
      "Iteration: 81 \t--- Loss: 0.382\n",
      "Iteration: 82 \t--- Loss: 0.322\n",
      "Iteration: 83 \t--- Loss: 0.332\n",
      "Iteration: 84 \t--- Loss: 0.415\n",
      "Iteration: 85 \t--- Loss: 0.386\n",
      "Iteration: 86 \t--- Loss: 0.357\n",
      "Iteration: 87 \t--- Loss: 0.379\n",
      "Iteration: 88 \t--- Loss: 0.338\n",
      "Iteration: 89 \t--- Loss: 0.371\n",
      "Iteration: 90 \t--- Loss: 0.366\n",
      "Iteration: 91 \t--- Loss: 0.369\n",
      "Iteration: 92 \t--- Loss: 0.356\n",
      "Iteration: 93 \t--- Loss: 0.353\n",
      "Iteration: 94 \t--- Loss: 0.378\n",
      "Iteration: 95 \t--- Loss: 0.327\n",
      "Iteration: 96 \t--- Loss: 0.367\n",
      "Iteration: 97 \t--- Loss: 0.355\n",
      "Iteration: 98 \t--- Loss: 0.367\n",
      "Iteration: 99 \t--- Loss: 0.388\n",
      "Iteration: 100 \t--- Loss: 0.378\n",
      "Iteration: 101 \t--- Loss: 0.371\n",
      "Iteration: 102 \t--- Loss: 0.344\n",
      "Iteration: 103 \t--- Loss: 0.351\n",
      "Iteration: 104 \t--- Loss: 0.383\n",
      "Iteration: 105 \t--- Loss: 0.388\n",
      "Iteration: 106 \t--- Loss: 0.347\n",
      "Iteration: 107 \t--- Loss: 0.382\n",
      "Iteration: 108 \t--- Loss: 0.350\n",
      "Iteration: 109 \t--- Loss: 0.380\n",
      "Iteration: 110 \t--- Loss: 0.357\n",
      "Iteration: 111 \t--- Loss: 0.378\n",
      "Iteration: 112 \t--- Loss: 0.392\n",
      "Iteration: 113 \t--- Loss: 0.349\n",
      "Iteration: 114 \t--- Loss: 0.402\n",
      "Iteration: 115 \t--- Loss: 0.384\n",
      "Iteration: 116 \t--- Loss: 0.386\n",
      "Iteration: 117 \t--- Loss: 0.394\n",
      "Iteration: 118 \t--- Loss: 0.336\n",
      "Iteration: 119 \t--- Loss: 0.334\n",
      "Iteration: 120 \t--- Loss: 0.385\n",
      "Iteration: 121 \t--- Loss: 0.348\n",
      "Iteration: 122 \t--- Loss: 0.360\n",
      "Iteration: 123 \t--- Loss: 0.345\n",
      "Iteration: 124 \t--- Loss: 0.386\n",
      "Iteration: 125 \t--- Loss: 0.378\n",
      "Iteration: 126 \t--- Loss: 0.370\n",
      "Iteration: 127 \t--- Loss: 0.335\n",
      "Iteration: 128 \t--- Loss: 0.358\n",
      "Iteration: 129 \t--- Loss: 0.378\n",
      "Iteration: 130 \t--- Loss: 0.366\n",
      "Iteration: 131 \t--- Loss: 0.391\n",
      "Iteration: 132 \t--- Loss: 0.368\n",
      "Iteration: 133 \t--- Loss: 0.371\n",
      "Iteration: 134 \t--- Loss: 0.376\n",
      "Iteration: 135 \t--- Loss: 0.374\n",
      "Iteration: 136 \t--- Loss: 0.349\n",
      "Iteration: 137 \t--- Loss: 0.382\n",
      "Iteration: 138 \t--- Loss: 0.396\n",
      "Iteration: 139 \t--- Loss: 0.381\n",
      "Iteration: 140 \t--- Loss: 0.354\n",
      "Iteration: 141 \t--- Loss: 0.348\n",
      "Iteration: 142 \t--- Loss: 0.328\n",
      "Iteration: 143 \t--- Loss: 0.358\n",
      "Iteration: 144 \t--- Loss: 0.405\n",
      "Iteration: 145 \t--- Loss: 0.366\n",
      "Iteration: 146 \t--- Loss: 0.339\n",
      "Iteration: 147 \t--- Loss: 0.412\n",
      "Iteration: 148 \t--- Loss: 0.341\n",
      "Iteration: 149 \t--- Loss: 0.364\n",
      "Iteration: 150 \t--- Loss: 0.376\n",
      "Iteration: 151 \t--- Loss: 0.375\n",
      "Iteration: 152 \t--- Loss: 0.347\n",
      "Iteration: 153 \t--- Loss: 0.402\n",
      "Iteration: 154 \t--- Loss: 0.348\n",
      "Iteration: 155 \t--- Loss: 0.361\n",
      "Iteration: 156 \t--- Loss: 0.411\n",
      "Iteration: 157 \t--- Loss: 0.376\n",
      "Iteration: 158 \t--- Loss: 0.406\n",
      "Iteration: 159 \t--- Loss: 0.380\n",
      "Iteration: 160 \t--- Loss: 0.365\n",
      "Iteration: 161 \t--- Loss: 0.374\n",
      "Iteration: 162 \t--- Loss: 0.375\n",
      "Iteration: 163 \t--- Loss: 0.349\n",
      "Iteration: 164 \t--- Loss: 0.394\n",
      "Iteration: 165 \t--- Loss: 0.389\n",
      "Iteration: 166 \t--- Loss: 0.353\n",
      "Iteration: 167 \t--- Loss: 0.402\n",
      "Iteration: 168 \t--- Loss: 0.355\n",
      "Iteration: 169 \t--- Loss: 0.311\n",
      "Iteration: 170 \t--- Loss: 0.340\n",
      "Iteration: 171 \t--- Loss: 0.351\n",
      "Iteration: 172 \t--- Loss: 0.366\n",
      "Iteration: 173 \t--- Loss: 0.327\n",
      "Iteration: 174 \t--- Loss: 0.337\n",
      "Iteration: 175 \t--- Loss: 0.362\n",
      "Iteration: 176 \t--- Loss: 0.373\n",
      "Iteration: 177 \t--- Loss: 0.321\n",
      "Iteration: 178 \t--- Loss: 0.372\n",
      "Iteration: 179 \t--- Loss: 0.360\n",
      "Iteration: 180 \t--- Loss: 0.381\n",
      "Iteration: 181 \t--- Loss: 0.339\n",
      "Iteration: 182 \t--- Loss: 0.352\n",
      "Iteration: 183 \t--- Loss: 0.390\n",
      "Iteration: 184 \t--- Loss: 0.426\n",
      "Iteration: 185 \t--- Loss: 0.348\n",
      "Iteration: 186 \t--- Loss: 0.362\n",
      "Iteration: 187 \t--- Loss: 0.350\n",
      "Iteration: 188 \t--- Loss: 0.341\n",
      "Iteration: 189 \t--- Loss: 0.382\n",
      "Iteration: 190 \t--- Loss: 0.342\n",
      "Iteration: 191 \t--- Loss: 0.397\n",
      "Iteration: 192 \t--- Loss: 0.343\n",
      "Iteration: 193 \t--- Loss: 0.432\n",
      "Iteration: 194 \t--- Loss: 0.368\n",
      "Iteration: 195 \t--- Loss: 0.332\n",
      "Iteration: 196 \t--- Loss: 0.386\n",
      "Iteration: 197 \t--- Loss: 0.362\n",
      "Iteration: 198 \t--- Loss: 0.333\n",
      "Iteration: 199 \t--- Loss: 0.376\n",
      "Iteration: 200 \t--- Loss: 0.358\n",
      "Iteration: 201 \t--- Loss: 0.365\n",
      "Iteration: 202 \t--- Loss: 0.373\n",
      "Iteration: 203 \t--- Loss: 0.338\n",
      "Iteration: 204 \t--- Loss: 0.371\n",
      "Iteration: 205 \t--- Loss: 0.367\n",
      "Iteration: 206 \t--- Loss: 0.396\n",
      "Iteration: 207 \t--- Loss: 0.365\n",
      "Iteration: 208 \t--- Loss: 0.377\n",
      "Iteration: 209 \t--- Loss: 0.384\n",
      "Iteration: 210 \t--- Loss: 0.334\n",
      "Iteration: 211 \t--- Loss: 0.321\n",
      "Iteration: 212 \t--- Loss: 0.355\n",
      "Iteration: 213 \t--- Loss: 0.362\n",
      "Iteration: 214 \t--- Loss: 0.343\n",
      "Iteration: 215 \t--- Loss: 0.392\n",
      "Iteration: 216 \t--- Loss: 0.348\n",
      "Iteration: 217 \t--- Loss: 0.356\n",
      "Iteration: 218 \t--- Loss: 0.407\n",
      "Iteration: 219 \t--- Loss: 0.354\n",
      "Iteration: 220 \t--- Loss: 0.354\n",
      "Iteration: 221 \t--- Loss: 0.399\n",
      "Iteration: 222 \t--- Loss: 0.326\n",
      "Iteration: 223 \t--- Loss: 0.332\n",
      "Iteration: 224 \t--- Loss: 0.395\n",
      "Iteration: 225 \t--- Loss: 0.338\n",
      "Iteration: 226 \t--- Loss: 0.354\n",
      "Iteration: 227 \t--- Loss: 0.321\n",
      "Iteration: 228 \t--- Loss: 0.374\n",
      "Iteration: 229 \t--- Loss: 0.316\n",
      "Iteration: 230 \t--- Loss: 0.356\n",
      "Iteration: 231 \t--- Loss: 0.358\n",
      "Iteration: 232 \t--- Loss: 0.366\n",
      "Iteration: 233 \t--- Loss: 0.352\n",
      "Iteration: 234 \t--- Loss: 0.359\n",
      "Iteration: 235 \t--- Loss: 0.353\n",
      "Iteration: 236 \t--- Loss: 0.349\n",
      "Iteration: 237 \t--- Loss: 0.370\n",
      "Iteration: 238 \t--- Loss: 0.384\n",
      "Iteration: 239 \t--- Loss: 0.392\n",
      "Iteration: 240 \t--- Loss: 0.379\n",
      "Iteration: 241 \t--- Loss: 0.368\n",
      "Iteration: 242 \t--- Loss: 0.386\n",
      "Iteration: 243 \t--- Loss: 0.350\n",
      "Iteration: 244 \t--- Loss: 0.382\n",
      "Iteration: 245 \t--- Loss: 0.347\n",
      "Iteration: 246 \t--- Loss: 0.417\n",
      "Iteration: 247 \t--- Loss: 0.350\n",
      "Iteration: 248 \t--- Loss: 0.344\n",
      "Iteration: 249 \t--- Loss: 0.372\n",
      "Iteration: 250 \t--- Loss: 0.405\n",
      "Iteration: 251 \t--- Loss: 0.384\n",
      "Iteration: 252 \t--- Loss: 0.352\n",
      "Iteration: 253 \t--- Loss: 0.362\n",
      "Iteration: 254 \t--- Loss: 0.368\n",
      "Iteration: 255 \t--- Loss: 0.357\n",
      "Iteration: 256 \t--- Loss: 0.365\n",
      "Iteration: 257 \t--- Loss: 0.335\n",
      "Iteration: 258 \t--- Loss: 0.353\n",
      "Iteration: 259 \t--- Loss: 0.377Iteration: 0 \t--- Loss: 0.293\n",
      "Iteration: 1 \t--- Loss: 0.282\n",
      "Iteration: 2 \t--- Loss: 0.249\n",
      "Iteration: 3 \t--- Loss: 0.223\n",
      "Iteration: 4 \t--- Loss: 0.219\n",
      "Iteration: 5 \t--- Loss: 0.185\n",
      "Iteration: 6 \t--- Loss: 0.172\n",
      "Iteration: 7 \t--- Loss: 0.158\n",
      "Iteration: 8 \t--- Loss: 0.146\n",
      "Iteration: 9 \t--- Loss: 0.141\n",
      "Iteration: 10 \t--- Loss: 0.129\n",
      "Iteration: 11 \t--- Loss: 0.116\n",
      "Iteration: 12 \t--- Loss: 0.117\n",
      "Iteration: 13 \t--- Loss: 0.120\n",
      "Iteration: 14 \t--- Loss: 0.109\n",
      "Iteration: 15 \t--- Loss: 0.105\n",
      "Iteration: 16 \t--- Loss: 0.101\n",
      "Iteration: 17 \t--- Loss: 0.098\n",
      "Iteration: 18 \t--- Loss: 0.096\n",
      "Iteration: 19 \t--- Loss: 0.098\n",
      "Iteration: 20 \t--- Loss: 0.091\n",
      "Iteration: 21 \t--- Loss: 0.089\n",
      "Iteration: 22 \t--- Loss: 0.090\n",
      "Iteration: 23 \t--- Loss: 0.087\n",
      "Iteration: 24 \t--- Loss: 0.083\n",
      "Iteration: 25 \t--- Loss: 0.083\n",
      "Iteration: 26 \t--- Loss: 0.084\n",
      "Iteration: 27 \t--- Loss: 0.082\n",
      "Iteration: 28 \t--- Loss: 0.081\n",
      "Iteration: 29 \t--- Loss: 0.077\n",
      "Iteration: 30 \t--- Loss: 0.079\n",
      "Iteration: 31 \t--- Loss: 0.078\n",
      "Iteration: 32 \t--- Loss: 0.078\n",
      "Iteration: 33 \t--- Loss: 0.076\n",
      "Iteration: 34 \t--- Loss: 0.073\n",
      "Iteration: 35 \t--- Loss: 0.075\n",
      "Iteration: 36 \t--- Loss: 0.078\n",
      "Iteration: 37 \t--- Loss: 0.078\n",
      "Iteration: 38 \t--- Loss: 0.075\n",
      "Iteration: 39 \t--- Loss: 0.074\n",
      "Iteration: 40 \t--- Loss: 0.074\n",
      "Iteration: 41 \t--- Loss: 0.072\n",
      "Iteration: 42 \t--- Loss: 0.076\n",
      "Iteration: 43 \t--- Loss: 0.072\n",
      "Iteration: 44 \t--- Loss: 0.073\n",
      "Iteration: 45 \t--- Loss: 0.070\n",
      "Iteration: 46 \t--- Loss: 0.071\n",
      "Iteration: 47 \t--- Loss: 0.070\n",
      "Iteration: 48 \t--- Loss: 0.074\n",
      "Iteration: 49 \t--- Loss: 0.070\n",
      "Iteration: 50 \t--- Loss: 0.071\n",
      "Iteration: 51 \t--- Loss: 0.071\n",
      "Iteration: 52 \t--- Loss: 0.072\n",
      "Iteration: 53 \t--- Loss: 0.070\n",
      "Iteration: 54 \t--- Loss: 0.070\n",
      "Iteration: 55 \t--- Loss: 0.068\n",
      "Iteration: 56 \t--- Loss: 0.071\n",
      "Iteration: 57 \t--- Loss: 0.070\n",
      "Iteration: 58 \t--- Loss: 0.071\n",
      "Iteration: 59 \t--- Loss: 0.069\n",
      "Iteration: 60 \t--- Loss: 0.069\n",
      "Iteration: 61 \t--- Loss: 0.070\n",
      "Iteration: 62 \t--- Loss: 0.069\n",
      "Iteration: 63 \t--- Loss: 0.069\n",
      "Iteration: 64 \t--- Loss: 0.069\n",
      "Iteration: 65 \t--- Loss: 0.068\n",
      "Iteration: 66 \t--- Loss: 0.069\n",
      "Iteration: 67 \t--- Loss: 0.069\n",
      "Iteration: 68 \t--- Loss: 0.069\n",
      "Iteration: 69 \t--- Loss: 0.067\n",
      "Iteration: 70 \t--- Loss: 0.068\n",
      "Iteration: 71 \t--- Loss: 0.068\n",
      "Iteration: 72 \t--- Loss: 0.068\n",
      "Iteration: 73 \t--- Loss: 0.070\n",
      "Iteration: 74 \t--- Loss: 0.068\n",
      "Iteration: 75 \t--- Loss: 0.068\n",
      "Iteration: 76 \t--- Loss: 0.069\n",
      "Iteration: 77 \t--- Loss: 0.068\n",
      "Iteration: 78 \t--- Loss: 0.069\n",
      "Iteration: 79 \t--- Loss: 0.069\n",
      "Iteration: 80 \t--- Loss: 0.068\n",
      "Iteration: 81 \t--- Loss: 0.067\n",
      "Iteration: 82 \t--- Loss: 0.067\n",
      "Iteration: 83 \t--- Loss: 0.068\n",
      "Iteration: 84 \t--- Loss: 0.068\n",
      "Iteration: 85 \t--- Loss: 0.069\n",
      "Iteration: 86 \t--- Loss: 0.068\n",
      "Iteration: 87 \t--- Loss: 0.068\n",
      "Iteration: 88 \t--- Loss: 0.067\n",
      "Iteration: 89 \t--- Loss: 0.068\n",
      "Iteration: 90 \t--- Loss: 0.067\n",
      "Iteration: 91 \t--- Loss: 0.067\n",
      "Iteration: 92 \t--- Loss: 0.066\n",
      "Iteration: 93 \t--- Loss: 0.067\n",
      "Iteration: 94 \t--- Loss: 0.068\n",
      "Iteration: 95 \t--- Loss: 0.067\n",
      "Iteration: 96 \t--- Loss: 0.067\n",
      "Iteration: 97 \t--- Loss: 0.067\n",
      "Iteration: 98 \t--- Loss: 0.068\n",
      "Iteration: 99 \t--- Loss: 0.067\n",
      "Iteration: 100 \t--- Loss: 0.067\n",
      "Iteration: 101 \t--- Loss: 0.067\n",
      "Iteration: 102 \t--- Loss: 0.067\n",
      "Iteration: 103 \t--- Loss: 0.067\n",
      "Iteration: 104 \t--- Loss: 0.069\n",
      "Iteration: 105 \t--- Loss: 0.066\n",
      "Iteration: 106 \t--- Loss: 0.068\n",
      "Iteration: 107 \t--- Loss: 0.067\n",
      "Iteration: 108 \t--- Loss: 0.067\n",
      "Iteration: 109 \t--- Loss: 0.067\n",
      "Iteration: 110 \t--- Loss: 0.065\n",
      "Iteration: 111 \t--- Loss: 0.068\n",
      "Iteration: 112 \t--- Loss: 0.066\n",
      "Iteration: 113 \t--- Loss: 0.067\n",
      "Iteration: 114 \t--- Loss: 0.068\n",
      "Iteration: 115 \t--- Loss: 0.067\n",
      "Iteration: 116 \t--- Loss: 0.066\n",
      "Iteration: 117 \t--- Loss: 0.069\n",
      "Iteration: 118 \t--- Loss: 0.066\n",
      "Iteration: 119 \t--- Loss: 0.067\n",
      "Iteration: 120 \t--- Loss: 0.067\n",
      "Iteration: 121 \t--- Loss: 0.066\n",
      "Iteration: 122 \t--- Loss: 0.067\n",
      "Iteration: 123 \t--- Loss: 0.067\n",
      "Iteration: 124 \t--- Loss: 0.067\n",
      "Iteration: 125 \t--- Loss: 0.066\n",
      "Iteration: 126 \t--- Loss: 0.066\n",
      "Iteration: 127 \t--- Loss: 0.066\n",
      "Iteration: 128 \t--- Loss: 0.067\n",
      "Iteration: 129 \t--- Loss: 0.067\n",
      "Iteration: 130 \t--- Loss: 0.067\n",
      "Iteration: 131 \t--- Loss: 0.068\n",
      "Iteration: 132 \t--- Loss: 0.067\n",
      "Iteration: 133 \t--- Loss: 0.066\n",
      "Iteration: 134 \t--- Loss: 0.066\n",
      "Iteration: 135 \t--- Loss: 0.067\n",
      "Iteration: 136 \t--- Loss: 0.066\n",
      "Iteration: 137 \t--- Loss: 0.067\n",
      "Iteration: 138 \t--- Loss: 0.068\n",
      "Iteration: 139 \t--- Loss: 0.066\n",
      "Iteration: 140 \t--- Loss: 0.069\n",
      "Iteration: 141 \t--- Loss: 0.068\n",
      "Iteration: 142 \t--- Loss: 0.067\n",
      "Iteration: 143 \t--- Loss: 0.067\n",
      "Iteration: 144 \t--- Loss: 0.068\n",
      "Iteration: 145 \t--- Loss: 0.067\n",
      "Iteration: 146 \t--- Loss: 0.066\n",
      "Iteration: 147 \t--- Loss: 0.067\n",
      "Iteration: 148 \t--- Loss: 0.066\n",
      "Iteration: 149 \t--- Loss: 0.067\n",
      "Iteration: 150 \t--- Loss: 0.065\n",
      "Iteration: 151 \t--- Loss: 0.068\n",
      "Iteration: 152 \t--- Loss: 0.066\n",
      "Iteration: 153 \t--- Loss: 0.067\n",
      "Iteration: 154 \t--- Loss: 0.067\n",
      "Iteration: 155 \t--- Loss: 0.067\n",
      "Iteration: 156 \t--- Loss: 0.068\n",
      "Iteration: 157 \t--- Loss: 0.067\n",
      "Iteration: 158 \t--- Loss: 0.066\n",
      "Iteration: 159 \t--- Loss: 0.065\n",
      "Iteration: 160 \t--- Loss: 0.067\n",
      "Iteration: 161 \t--- Loss: 0.068\n",
      "Iteration: 162 \t--- Loss: 0.066\n",
      "Iteration: 163 \t--- Loss: 0.065\n",
      "Iteration: 164 \t--- Loss: 0.065\n",
      "Iteration: 165 \t--- Loss: 0.066\n",
      "Iteration: 166 \t--- Loss: 0.065\n",
      "Iteration: 167 \t--- Loss: 0.066\n",
      "Iteration: 168 \t--- Loss: 0.066\n",
      "Iteration: 169 \t--- Loss: 0.067\n",
      "Iteration: 170 \t--- Loss: 0.066\n",
      "Iteration: 171 \t--- Loss: 0.066\n",
      "Iteration: 172 \t--- Loss: 0.066\n",
      "Iteration: 173 \t--- Loss: 0.068\n",
      "Iteration: 174 \t--- Loss: 0.066\n",
      "Iteration: 175 \t--- Loss: 0.067\n",
      "Iteration: 176 \t--- Loss: 0.068\n",
      "Iteration: 177 \t--- Loss: 0.066\n",
      "Iteration: 178 \t--- Loss: 0.067\n",
      "Iteration: 179 \t--- Loss: 0.066\n",
      "Iteration: 180 \t--- Loss: 0.067\n",
      "Iteration: 181 \t--- Loss: 0.066\n",
      "Iteration: 182 \t--- Loss: 0.065\n",
      "Iteration: 183 \t--- Loss: 0.066\n",
      "Iteration: 184 \t--- Loss: 0.066\n",
      "Iteration: 185 \t--- Loss: 0.067\n",
      "Iteration: 186 \t--- Loss: 0.067\n",
      "Iteration: 187 \t--- Loss: 0.067\n",
      "Iteration: 188 \t--- Loss: 0.066\n",
      "Iteration: 189 \t--- Loss: 0.066\n",
      "Iteration: 190 \t--- Loss: 0.067\n",
      "Iteration: 191 \t--- Loss: 0.067\n",
      "Iteration: 192 \t--- Loss: 0.067\n",
      "Iteration: 193 \t--- Loss: 0.065\n",
      "Iteration: 194 \t--- Loss: 0.067\n",
      "Iteration: 195 \t--- Loss: 0.068\n",
      "Iteration: 196 \t--- Loss: 0.067\n",
      "Iteration: 197 \t--- Loss: 0.064\n",
      "Iteration: 198 \t--- Loss: 0.066\n",
      "Iteration: 199 \t--- Loss: 0.068\n",
      "Iteration: 200 \t--- Loss: 0.068\n",
      "Iteration: 201 \t--- Loss: 0.065\n",
      "Iteration: 202 \t--- Loss: 0.067\n",
      "Iteration: 203 \t--- Loss: 0.066\n",
      "Iteration: 204 \t--- Loss: 0.064\n",
      "Iteration: 205 \t--- Loss: 0.067\n",
      "Iteration: 206 \t--- Loss: 0.067\n",
      "Iteration: 207 \t--- Loss: 0.068\n",
      "Iteration: 208 \t--- Loss: 0.069\n",
      "Iteration: 209 \t--- Loss: 0.068\n",
      "Iteration: 210 \t--- Loss: 0.067\n",
      "Iteration: 211 \t--- Loss: 0.067\n",
      "Iteration: 212 \t--- Loss: 0.066\n",
      "Iteration: 213 \t--- Loss: 0.066\n",
      "Iteration: 214 \t--- Loss: 0.066\n",
      "Iteration: 215 \t--- Loss: 0.066\n",
      "Iteration: 216 \t--- Loss: 0.065\n",
      "Iteration: 217 \t--- Loss: 0.067\n",
      "Iteration: 218 \t--- Loss: 0.066\n",
      "Iteration: 219 \t--- Loss: 0.067\n",
      "Iteration: 220 \t--- Loss: 0.067\n",
      "Iteration: 221 \t--- Loss: 0.067\n",
      "Iteration: 222 \t--- Loss: 0.065\n",
      "Iteration: 223 \t--- Loss: 0.066\n",
      "Iteration: 224 \t--- Loss: 0.066\n",
      "Iteration: 225 \t--- Loss: 0.066\n",
      "Iteration: 226 \t--- Loss: 0.067\n",
      "Iteration: 227 \t--- Loss: 0.066\n",
      "Iteration: 228 \t--- Loss: 0.067\n",
      "Iteration: 229 \t--- Loss: 0.068\n",
      "Iteration: 230 \t--- Loss: 0.067\n",
      "Iteration: 231 \t--- Loss: 0.067\n",
      "Iteration: 232 \t--- Loss: 0.066\n",
      "Iteration: 233 \t--- Loss: 0.067\n",
      "Iteration: 234 \t--- Loss: 0.067\n",
      "Iteration: 235 \t--- Loss: 0.067\n",
      "Iteration: 236 \t--- Loss: 0.067\n",
      "Iteration: 237 \t--- Loss: 0.066\n",
      "Iteration: 238 \t--- Loss: 0.067\n",
      "Iteration: 239 \t--- Loss: 0.065\n",
      "Iteration: 240 \t--- Loss: 0.067\n",
      "Iteration: 241 \t--- Loss: 0.066\n",
      "Iteration: 242 \t--- Loss: 0.067\n",
      "Iteration: 243 \t--- Loss: 0.065\n",
      "Iteration: 244 \t--- Loss: 0.066\n",
      "Iteration: 245 \t--- Loss: 0.066\n",
      "Iteration: 246 \t--- Loss: 0.067\n",
      "Iteration: 247 \t--- Loss: 0.068\n",
      "Iteration: 248 \t--- Loss: 0.067\n",
      "Iteration: 249 \t--- Loss: 0.066\n",
      "Iteration: 250 \t--- Loss: 0.066\n",
      "Iteration: 251 \t--- Loss: 0.067\n",
      "Iteration: 252 \t--- Loss: 0.067\n",
      "Iteration: 253 \t--- Loss: 0.069\n",
      "Iteration: 254 \t--- Loss: 0.066\n",
      "Iteration: 255 \t--- Loss: 0.067\n",
      "Iteration: 256 \t--- Loss: 0.066\n",
      "Iteration: 257 \t--- Loss: 0.065\n",
      "Iteration: 258 \t--- Loss: 0.066\n",
      "Iteration: 259 \t--- Loss: 0.067Iteration: 0 \t--- Loss: 1.399\n",
      "Iteration: 1 \t--- Loss: 1.292\n",
      "Iteration: 2 \t--- Loss: 1.271\n",
      "Iteration: 3 \t--- Loss: 1.184\n",
      "Iteration: 4 \t--- Loss: 1.114\n",
      "Iteration: 5 \t--- Loss: 1.062\n",
      "Iteration: 6 \t--- Loss: 1.044\n",
      "Iteration: 7 \t--- Loss: 0.992\n",
      "Iteration: 8 \t--- Loss: 0.979\n",
      "Iteration: 9 \t--- Loss: 0.940\n",
      "Iteration: 10 \t--- Loss: 0.985\n",
      "Iteration: 11 \t--- Loss: 0.898\n",
      "Iteration: 12 \t--- Loss: 0.875\n",
      "Iteration: 13 \t--- Loss: 0.858\n",
      "Iteration: 14 \t--- Loss: 0.949\n",
      "Iteration: 15 \t--- Loss: 0.928\n",
      "Iteration: 16 \t--- Loss: 0.916\n",
      "Iteration: 17 \t--- Loss: 0.949\n",
      "Iteration: 18 \t--- Loss: 0.914\n",
      "Iteration: 19 \t--- Loss: 0.906\n",
      "Iteration: 20 \t--- Loss: 0.905\n",
      "Iteration: 21 \t--- Loss: 0.852\n",
      "Iteration: 22 \t--- Loss: 0.863\n",
      "Iteration: 23 \t--- Loss: 0.858\n",
      "Iteration: 24 \t--- Loss: 0.923\n",
      "Iteration: 25 \t--- Loss: 0.930\n",
      "Iteration: 26 \t--- Loss: 0.946\n",
      "Iteration: 27 \t--- Loss: 0.886\n",
      "Iteration: 28 \t--- Loss: 0.879\n",
      "Iteration: 29 \t--- Loss: 0.879\n",
      "Iteration: 30 \t--- Loss: 0.848\n",
      "Iteration: 31 \t--- Loss: 0.887\n",
      "Iteration: 32 \t--- Loss: 0.890\n",
      "Iteration: 33 \t--- Loss: 0.870\n",
      "Iteration: 34 \t--- Loss: 0.877\n",
      "Iteration: 35 \t--- Loss: 0.905\n",
      "Iteration: 36 \t--- Loss: 0.897\n",
      "Iteration: 37 \t--- Loss: 0.936\n",
      "Iteration: 38 \t--- Loss: 0.892\n",
      "Iteration: 39 \t--- Loss: 0.817\n",
      "Iteration: 40 \t--- Loss: 0.886\n",
      "Iteration: 41 \t--- Loss: 0.903\n",
      "Iteration: 42 \t--- Loss: 0.948\n",
      "Iteration: 43 \t--- Loss: 0.872\n",
      "Iteration: 44 \t--- Loss: 0.874\n",
      "Iteration: 45 \t--- Loss: 0.898\n",
      "Iteration: 46 \t--- Loss: 0.896\n",
      "Iteration: 47 \t--- Loss: 0.898\n",
      "Iteration: 48 \t--- Loss: 0.881\n",
      "Iteration: 49 \t--- Loss: 0.854\n",
      "Iteration: 50 \t--- Loss: 0.869\n",
      "Iteration: 51 \t--- Loss: 0.883\n",
      "Iteration: 52 \t--- Loss: 0.873\n",
      "Iteration: 53 \t--- Loss: 0.849\n",
      "Iteration: 54 \t--- Loss: 0.925\n",
      "Iteration: 55 \t--- Loss: 0.874\n",
      "Iteration: 56 \t--- Loss: 0.855\n",
      "Iteration: 57 \t--- Loss: 0.893\n",
      "Iteration: 58 \t--- Loss: 0.901\n",
      "Iteration: 59 \t--- Loss: 0.876\n",
      "Iteration: 60 \t--- Loss: 0.911\n",
      "Iteration: 61 \t--- Loss: 0.907\n",
      "Iteration: 62 \t--- Loss: 0.907\n",
      "Iteration: 63 \t--- Loss: 0.841\n",
      "Iteration: 64 \t--- Loss: 0.875\n",
      "Iteration: 65 \t--- Loss: 0.884\n",
      "Iteration: 66 \t--- Loss: 0.896\n",
      "Iteration: 67 \t--- Loss: 0.869\n",
      "Iteration: 68 \t--- Loss: 0.893\n",
      "Iteration: 69 \t--- Loss: 0.880\n",
      "Iteration: 70 \t--- Loss: 0.910\n",
      "Iteration: 71 \t--- Loss: 0.909\n",
      "Iteration: 72 \t--- Loss: 0.878\n",
      "Iteration: 73 \t--- Loss: 0.903\n",
      "Iteration: 74 \t--- Loss: 0.891\n",
      "Iteration: 75 \t--- Loss: 0.868\n",
      "Iteration: 76 \t--- Loss: 0.910\n",
      "Iteration: 77 \t--- Loss: 0.868\n",
      "Iteration: 78 \t--- Loss: 0.887\n",
      "Iteration: 79 \t--- Loss: 0.850\n",
      "Iteration: 80 \t--- Loss: 0.878\n",
      "Iteration: 81 \t--- Loss: 0.900\n",
      "Iteration: 82 \t--- Loss: 0.880\n",
      "Iteration: 83 \t--- Loss: 0.868\n",
      "Iteration: 84 \t--- Loss: 0.894\n",
      "Iteration: 85 \t--- Loss: 0.871\n",
      "Iteration: 86 \t--- Loss: 0.833\n",
      "Iteration: 87 \t--- Loss: 0.895\n",
      "Iteration: 88 \t--- Loss: 0.856\n",
      "Iteration: 89 \t--- Loss: 0.860\n",
      "Iteration: 90 \t--- Loss: 0.886\n",
      "Iteration: 91 \t--- Loss: 0.885\n",
      "Iteration: 92 \t--- Loss: 0.878\n",
      "Iteration: 93 \t--- Loss: 0.895\n",
      "Iteration: 94 \t--- Loss: 0.864\n",
      "Iteration: 95 \t--- Loss: 0.845\n",
      "Iteration: 96 \t--- Loss: 0.879\n",
      "Iteration: 97 \t--- Loss: 0.856\n",
      "Iteration: 98 \t--- Loss: 0.922\n",
      "Iteration: 99 \t--- Loss: 0.915\n",
      "Iteration: 100 \t--- Loss: 0.924\n",
      "Iteration: 101 \t--- Loss: 0.914\n",
      "Iteration: 102 \t--- Loss: 0.906\n",
      "Iteration: 103 \t--- Loss: 0.885\n",
      "Iteration: 104 \t--- Loss: 0.885\n",
      "Iteration: 105 \t--- Loss: 0.909\n",
      "Iteration: 106 \t--- Loss: 0.857\n",
      "Iteration: 107 \t--- Loss: 0.927\n",
      "Iteration: 108 \t--- Loss: 0.876\n",
      "Iteration: 109 \t--- Loss: 0.953\n",
      "Iteration: 110 \t--- Loss: 0.829\n",
      "Iteration: 111 \t--- Loss: 0.871\n",
      "Iteration: 112 \t--- Loss: 0.908\n",
      "Iteration: 113 \t--- Loss: 0.879\n",
      "Iteration: 114 \t--- Loss: 0.854\n",
      "Iteration: 115 \t--- Loss: 0.882\n",
      "Iteration: 116 \t--- Loss: 0.899\n",
      "Iteration: 117 \t--- Loss: 0.912\n",
      "Iteration: 118 \t--- Loss: 0.898\n",
      "Iteration: 119 \t--- Loss: 0.892\n",
      "Iteration: 120 \t--- Loss: 0.902\n",
      "Iteration: 121 \t--- Loss: 0.905\n",
      "Iteration: 122 \t--- Loss: 0.891\n",
      "Iteration: 123 \t--- Loss: 0.859\n",
      "Iteration: 124 \t--- Loss: 0.935\n",
      "Iteration: 125 \t--- Loss: 0.865\n",
      "Iteration: 126 \t--- Loss: 0.870\n",
      "Iteration: 127 \t--- Loss: 0.842\n",
      "Iteration: 128 \t--- Loss: 0.897\n",
      "Iteration: 129 \t--- Loss: 0.918\n",
      "Iteration: 130 \t--- Loss: 0.879\n",
      "Iteration: 131 \t--- Loss: 0.881\n",
      "Iteration: 132 \t--- Loss: 0.922\n",
      "Iteration: 133 \t--- Loss: 0.899\n",
      "Iteration: 134 \t--- Loss: 0.890\n",
      "Iteration: 135 \t--- Loss: 0.863\n",
      "Iteration: 136 \t--- Loss: 0.908\n",
      "Iteration: 137 \t--- Loss: 0.857\n",
      "Iteration: 138 \t--- Loss: 0.932\n",
      "Iteration: 139 \t--- Loss: 0.849\n",
      "Iteration: 140 \t--- Loss: 0.931\n",
      "Iteration: 141 \t--- Loss: 0.975\n",
      "Iteration: 142 \t--- Loss: 0.908\n",
      "Iteration: 143 \t--- Loss: 0.862\n",
      "Iteration: 144 \t--- Loss: 0.918\n",
      "Iteration: 145 \t--- Loss: 0.863\n",
      "Iteration: 146 \t--- Loss: 0.870\n",
      "Iteration: 147 \t--- Loss: 0.861\n",
      "Iteration: 148 \t--- Loss: 0.873\n",
      "Iteration: 149 \t--- Loss: 0.875\n",
      "Iteration: 150 \t--- Loss: 0.884\n",
      "Iteration: 151 \t--- Loss: 0.914\n",
      "Iteration: 152 \t--- Loss: 0.899\n",
      "Iteration: 153 \t--- Loss: 0.886\n",
      "Iteration: 154 \t--- Loss: 0.898\n",
      "Iteration: 155 \t--- Loss: 0.891\n",
      "Iteration: 156 \t--- Loss: 0.921\n",
      "Iteration: 157 \t--- Loss: 0.924\n",
      "Iteration: 158 \t--- Loss: 0.861\n",
      "Iteration: 159 \t--- Loss: 0.915\n",
      "Iteration: 160 \t--- Loss: 0.876\n",
      "Iteration: 161 \t--- Loss: 0.864\n",
      "Iteration: 162 \t--- Loss: 0.899\n",
      "Iteration: 163 \t--- Loss: 0.886\n",
      "Iteration: 164 \t--- Loss: 0.919\n",
      "Iteration: 165 \t--- Loss: 0.885\n",
      "Iteration: 166 \t--- Loss: 0.862\n",
      "Iteration: 167 \t--- Loss: 0.914\n",
      "Iteration: 168 \t--- Loss: 0.879\n",
      "Iteration: 169 \t--- Loss: 0.866\n",
      "Iteration: 170 \t--- Loss: 0.865\n",
      "Iteration: 171 \t--- Loss: 0.910\n",
      "Iteration: 172 \t--- Loss: 0.873\n",
      "Iteration: 173 \t--- Loss: 0.914\n",
      "Iteration: 174 \t--- Loss: 0.906\n",
      "Iteration: 175 \t--- Loss: 0.927\n",
      "Iteration: 176 \t--- Loss: 0.850\n",
      "Iteration: 177 \t--- Loss: 0.920\n",
      "Iteration: 178 \t--- Loss: 0.890\n",
      "Iteration: 179 \t--- Loss: 0.887\n",
      "Iteration: 180 \t--- Loss: 0.912\n",
      "Iteration: 181 \t--- Loss: 0.912\n",
      "Iteration: 182 \t--- Loss: 0.891\n",
      "Iteration: 183 \t--- Loss: 0.833\n",
      "Iteration: 184 \t--- Loss: 0.957\n",
      "Iteration: 185 \t--- Loss: 0.875\n",
      "Iteration: 186 \t--- Loss: 0.856\n",
      "Iteration: 187 \t--- Loss: 0.897\n",
      "Iteration: 188 \t--- Loss: 0.882\n",
      "Iteration: 189 \t--- Loss: 0.933\n",
      "Iteration: 190 \t--- Loss: 0.875\n",
      "Iteration: 191 \t--- Loss: 0.933\n",
      "Iteration: 192 \t--- Loss: 0.872\n",
      "Iteration: 193 \t--- Loss: 0.867\n",
      "Iteration: 194 \t--- Loss: 0.898\n",
      "Iteration: 195 \t--- Loss: 0.931\n",
      "Iteration: 196 \t--- Loss: 0.926\n",
      "Iteration: 197 \t--- Loss: 0.876\n",
      "Iteration: 198 \t--- Loss: 0.865\n",
      "Iteration: 199 \t--- Loss: 0.893\n",
      "Iteration: 200 \t--- Loss: 0.951\n",
      "Iteration: 201 \t--- Loss: 0.900\n",
      "Iteration: 202 \t--- Loss: 0.893\n",
      "Iteration: 203 \t--- Loss: 0.892\n",
      "Iteration: 204 \t--- Loss: 0.885\n",
      "Iteration: 205 \t--- Loss: 0.892\n",
      "Iteration: 206 \t--- Loss: 0.914\n",
      "Iteration: 207 \t--- Loss: 0.898\n",
      "Iteration: 208 \t--- Loss: 0.820\n",
      "Iteration: 209 \t--- Loss: 0.838\n",
      "Iteration: 210 \t--- Loss: 0.907\n",
      "Iteration: 211 \t--- Loss: 0.903\n",
      "Iteration: 212 \t--- Loss: 0.893\n",
      "Iteration: 213 \t--- Loss: 0.954\n",
      "Iteration: 214 \t--- Loss: 0.891\n",
      "Iteration: 215 \t--- Loss: 0.873\n",
      "Iteration: 216 \t--- Loss: 0.853\n",
      "Iteration: 217 \t--- Loss: 0.901\n",
      "Iteration: 218 \t--- Loss: 0.873\n",
      "Iteration: 219 \t--- Loss: 0.871\n",
      "Iteration: 220 \t--- Loss: 0.897\n",
      "Iteration: 221 \t--- Loss: 0.895\n",
      "Iteration: 222 \t--- Loss: 0.880\n",
      "Iteration: 223 \t--- Loss: 0.888\n",
      "Iteration: 224 \t--- Loss: 0.887\n",
      "Iteration: 225 \t--- Loss: 0.937\n",
      "Iteration: 226 \t--- Loss: 0.859\n",
      "Iteration: 227 \t--- Loss: 0.865\n",
      "Iteration: 228 \t--- Loss: 0.893\n",
      "Iteration: 229 \t--- Loss: 0.869\n",
      "Iteration: 230 \t--- Loss: 0.859\n",
      "Iteration: 231 \t--- Loss: 0.884\n",
      "Iteration: 232 \t--- Loss: 0.879\n",
      "Iteration: 233 \t--- Loss: 0.891\n",
      "Iteration: 234 \t--- Loss: 0.943\n",
      "Iteration: 235 \t--- Loss: 0.873\n",
      "Iteration: 236 \t--- Loss: 0.856\n",
      "Iteration: 237 \t--- Loss: 0.871\n",
      "Iteration: 238 \t--- Loss: 0.890\n",
      "Iteration: 239 \t--- Loss: 0.919\n",
      "Iteration: 240 \t--- Loss: 0.919\n",
      "Iteration: 241 \t--- Loss: 0.860\n",
      "Iteration: 242 \t--- Loss: 0.855\n",
      "Iteration: 243 \t--- Loss: 0.898\n",
      "Iteration: 244 \t--- Loss: 0.898\n",
      "Iteration: 245 \t--- Loss: 0.859\n",
      "Iteration: 246 \t--- Loss: 0.921\n",
      "Iteration: 247 \t--- Loss: 0.851\n",
      "Iteration: 248 \t--- Loss: 0.859\n",
      "Iteration: 249 \t--- Loss: 0.899\n",
      "Iteration: 250 \t--- Loss: 0.863\n",
      "Iteration: 251 \t--- Loss: 0.943\n",
      "Iteration: 252 \t--- Loss: 0.891\n",
      "Iteration: 253 \t--- Loss: 0.836\n",
      "Iteration: 254 \t--- Loss: 0.914\n",
      "Iteration: 255 \t--- Loss: 0.874\n",
      "Iteration: 256 \t--- Loss: 0.817\n",
      "Iteration: 257 \t--- Loss: 0.890\n",
      "Iteration: 258 \t--- Loss: 0.868\n",
      "Iteration: 259 \t--- Loss: 0.900Iteration: 0 \t--- Loss: 0.642\n",
      "Iteration: 1 \t--- Loss: 0.585\n",
      "Iteration: 2 \t--- Loss: 0.673\n",
      "Iteration: 3 \t--- Loss: 0.598\n",
      "Iteration: 4 \t--- Loss: 0.573\n",
      "Iteration: 5 \t--- Loss: 0.546\n",
      "Iteration: 6 \t--- Loss: 0.508\n",
      "Iteration: 7 \t--- Loss: 0.612\n",
      "Iteration: 8 \t--- Loss: 0.468\n",
      "Iteration: 9 \t--- Loss: 0.511\n",
      "Iteration: 10 \t--- Loss: 0.449\n",
      "Iteration: 11 \t--- Loss: 0.458\n",
      "Iteration: 12 \t--- Loss: 0.443\n",
      "Iteration: 13 \t--- Loss: 0.459\n",
      "Iteration: 14 \t--- Loss: 0.451\n",
      "Iteration: 15 \t--- Loss: 0.426\n",
      "Iteration: 16 \t--- Loss: 0.471\n",
      "Iteration: 17 \t--- Loss: 0.448\n",
      "Iteration: 18 \t--- Loss: 0.460\n",
      "Iteration: 19 \t--- Loss: 0.408\n",
      "Iteration: 20 \t--- Loss: 0.419\n",
      "Iteration: 21 \t--- Loss: 0.421\n",
      "Iteration: 22 \t--- Loss: 0.407\n",
      "Iteration: 23 \t--- Loss: 0.390\n",
      "Iteration: 24 \t--- Loss: 0.419\n",
      "Iteration: 25 \t--- Loss: 0.452\n",
      "Iteration: 26 \t--- Loss: 0.404\n",
      "Iteration: 27 \t--- Loss: 0.410\n",
      "Iteration: 28 \t--- Loss: 0.420\n",
      "Iteration: 29 \t--- Loss: 0.430\n",
      "Iteration: 30 \t--- Loss: 0.410\n",
      "Iteration: 31 \t--- Loss: 0.398\n",
      "Iteration: 32 \t--- Loss: 0.405\n",
      "Iteration: 33 \t--- Loss: 0.414\n",
      "Iteration: 34 \t--- Loss: 0.379\n",
      "Iteration: 35 \t--- Loss: 0.362\n",
      "Iteration: 36 \t--- Loss: 0.375\n",
      "Iteration: 37 \t--- Loss: 0.377\n",
      "Iteration: 38 \t--- Loss: 0.385\n",
      "Iteration: 39 \t--- Loss: 0.399\n",
      "Iteration: 40 \t--- Loss: 0.391\n",
      "Iteration: 41 \t--- Loss: 0.425\n",
      "Iteration: 42 \t--- Loss: 0.414\n",
      "Iteration: 43 \t--- Loss: 0.399\n",
      "Iteration: 44 \t--- Loss: 0.367\n",
      "Iteration: 45 \t--- Loss: 0.349\n",
      "Iteration: 46 \t--- Loss: 0.363\n",
      "Iteration: 47 \t--- Loss: 0.417\n",
      "Iteration: 48 \t--- Loss: 0.469\n",
      "Iteration: 49 \t--- Loss: 0.413\n",
      "Iteration: 50 \t--- Loss: 0.389\n",
      "Iteration: 51 \t--- Loss: 0.372\n",
      "Iteration: 52 \t--- Loss: 0.424\n",
      "Iteration: 53 \t--- Loss: 0.413\n",
      "Iteration: 54 \t--- Loss: 0.363\n",
      "Iteration: 55 \t--- Loss: 0.412\n",
      "Iteration: 56 \t--- Loss: 0.366\n",
      "Iteration: 57 \t--- Loss: 0.423\n",
      "Iteration: 58 \t--- Loss: 0.450\n",
      "Iteration: 59 \t--- Loss: 0.427\n",
      "Iteration: 60 \t--- Loss: 0.386\n",
      "Iteration: 61 \t--- Loss: 0.392\n",
      "Iteration: 62 \t--- Loss: 0.454\n",
      "Iteration: 63 \t--- Loss: 0.355\n",
      "Iteration: 64 \t--- Loss: 0.388\n",
      "Iteration: 65 \t--- Loss: 0.421\n",
      "Iteration: 66 \t--- Loss: 0.358\n",
      "Iteration: 67 \t--- Loss: 0.395\n",
      "Iteration: 68 \t--- Loss: 0.372\n",
      "Iteration: 69 \t--- Loss: 0.389\n",
      "Iteration: 70 \t--- Loss: 0.439\n",
      "Iteration: 71 \t--- Loss: 0.355\n",
      "Iteration: 72 \t--- Loss: 0.369\n",
      "Iteration: 73 \t--- Loss: 0.375\n",
      "Iteration: 74 \t--- Loss: 0.383\n",
      "Iteration: 75 \t--- Loss: 0.343\n",
      "Iteration: 76 \t--- Loss: 0.361\n",
      "Iteration: 77 \t--- Loss: 0.394\n",
      "Iteration: 78 \t--- Loss: 0.410\n",
      "Iteration: 79 \t--- Loss: 0.401\n",
      "Iteration: 80 \t--- Loss: 0.406\n",
      "Iteration: 81 \t--- Loss: 0.411\n",
      "Iteration: 82 \t--- Loss: 0.416\n",
      "Iteration: 83 \t--- Loss: 0.361\n",
      "Iteration: 84 \t--- Loss: 0.377\n",
      "Iteration: 85 \t--- Loss: 0.406\n",
      "Iteration: 86 \t--- Loss: 0.408\n",
      "Iteration: 87 \t--- Loss: 0.377\n",
      "Iteration: 88 \t--- Loss: 0.388\n",
      "Iteration: 89 \t--- Loss: 0.376\n",
      "Iteration: 90 \t--- Loss: 0.395\n",
      "Iteration: 91 \t--- Loss: 0.451\n",
      "Iteration: 92 \t--- Loss: 0.366\n",
      "Iteration: 93 \t--- Loss: 0.394\n",
      "Iteration: 94 \t--- Loss: 0.391\n",
      "Iteration: 95 \t--- Loss: 0.419\n",
      "Iteration: 96 \t--- Loss: 0.398\n",
      "Iteration: 97 \t--- Loss: 0.406\n",
      "Iteration: 98 \t--- Loss: 0.409\n",
      "Iteration: 99 \t--- Loss: 0.399\n",
      "Iteration: 100 \t--- Loss: 0.403\n",
      "Iteration: 101 \t--- Loss: 0.407\n",
      "Iteration: 102 \t--- Loss: 0.363\n",
      "Iteration: 103 \t--- Loss: 0.390\n",
      "Iteration: 104 \t--- Loss: 0.361\n",
      "Iteration: 105 \t--- Loss: 0.387\n",
      "Iteration: 106 \t--- Loss: 0.421\n",
      "Iteration: 107 \t--- Loss: 0.382\n",
      "Iteration: 108 \t--- Loss: 0.398\n",
      "Iteration: 109 \t--- Loss: 0.421\n",
      "Iteration: 110 \t--- Loss: 0.381\n",
      "Iteration: 111 \t--- Loss: 0.368\n",
      "Iteration: 112 \t--- Loss: 0.388\n",
      "Iteration: 113 \t--- Loss: 0.406\n",
      "Iteration: 114 \t--- Loss: 0.404\n",
      "Iteration: 115 \t--- Loss: 0.422\n",
      "Iteration: 116 \t--- Loss: 0.393\n",
      "Iteration: 117 \t--- Loss: 0.384\n",
      "Iteration: 118 \t--- Loss: 0.395\n",
      "Iteration: 119 \t--- Loss: 0.398\n",
      "Iteration: 120 \t--- Loss: 0.393\n",
      "Iteration: 121 \t--- Loss: 0.393\n",
      "Iteration: 122 \t--- Loss: 0.379\n",
      "Iteration: 123 \t--- Loss: 0.416\n",
      "Iteration: 124 \t--- Loss: 0.343\n",
      "Iteration: 125 \t--- Loss: 0.402\n",
      "Iteration: 126 \t--- Loss: 0.389\n",
      "Iteration: 127 \t--- Loss: 0.382\n",
      "Iteration: 128 \t--- Loss: 0.391\n",
      "Iteration: 129 \t--- Loss: 0.411\n",
      "Iteration: 130 \t--- Loss: 0.414\n",
      "Iteration: 131 \t--- Loss: 0.430\n",
      "Iteration: 132 \t--- Loss: 0.423\n",
      "Iteration: 133 \t--- Loss: 0.372\n",
      "Iteration: 134 \t--- Loss: 0.422\n",
      "Iteration: 135 \t--- Loss: 0.434\n",
      "Iteration: 136 \t--- Loss: 0.397\n",
      "Iteration: 137 \t--- Loss: 0.396\n",
      "Iteration: 138 \t--- Loss: 0.411\n",
      "Iteration: 139 \t--- Loss: 0.412\n",
      "Iteration: 140 \t--- Loss: 0.430\n",
      "Iteration: 141 \t--- Loss: 0.372\n",
      "Iteration: 142 \t--- Loss: 0.411\n",
      "Iteration: 143 \t--- Loss: 0.403\n",
      "Iteration: 144 \t--- Loss: 0.414\n",
      "Iteration: 145 \t--- Loss: 0.394\n",
      "Iteration: 146 \t--- Loss: 0.443\n",
      "Iteration: 147 \t--- Loss: 0.393\n",
      "Iteration: 148 \t--- Loss: 0.330\n",
      "Iteration: 149 \t--- Loss: 0.366\n",
      "Iteration: 150 \t--- Loss: 0.388\n",
      "Iteration: 151 \t--- Loss: 0.423\n",
      "Iteration: 152 \t--- Loss: 0.391\n",
      "Iteration: 153 \t--- Loss: 0.411\n",
      "Iteration: 154 \t--- Loss: 0.382\n",
      "Iteration: 155 \t--- Loss: 0.346\n",
      "Iteration: 156 \t--- Loss: 0.362\n",
      "Iteration: 157 \t--- Loss: 0.398\n",
      "Iteration: 158 \t--- Loss: 0.389\n",
      "Iteration: 159 \t--- Loss: 0.448\n",
      "Iteration: 160 \t--- Loss: 0.461\n",
      "Iteration: 161 \t--- Loss: 0.401\n",
      "Iteration: 162 \t--- Loss: 0.394\n",
      "Iteration: 163 \t--- Loss: 0.439\n",
      "Iteration: 164 \t--- Loss: 0.407\n",
      "Iteration: 165 \t--- Loss: 0.388\n",
      "Iteration: 166 \t--- Loss: 0.408\n",
      "Iteration: 167 \t--- Loss: 0.390\n",
      "Iteration: 168 \t--- Loss: 0.364\n",
      "Iteration: 169 \t--- Loss: 0.382\n",
      "Iteration: 170 \t--- Loss: 0.368\n",
      "Iteration: 171 \t--- Loss: 0.370\n",
      "Iteration: 172 \t--- Loss: 0.405\n",
      "Iteration: 173 \t--- Loss: 0.376\n",
      "Iteration: 174 \t--- Loss: 0.441\n",
      "Iteration: 175 \t--- Loss: 0.390\n",
      "Iteration: 176 \t--- Loss: 0.419\n",
      "Iteration: 177 \t--- Loss: 0.420\n",
      "Iteration: 178 \t--- Loss: 0.401\n",
      "Iteration: 179 \t--- Loss: 0.397\n",
      "Iteration: 180 \t--- Loss: 0.378\n",
      "Iteration: 181 \t--- Loss: 0.413\n",
      "Iteration: 182 \t--- Loss: 0.426\n",
      "Iteration: 183 \t--- Loss: 0.383\n",
      "Iteration: 184 \t--- Loss: 0.384\n",
      "Iteration: 185 \t--- Loss: 0.470\n",
      "Iteration: 186 \t--- Loss: 0.391\n",
      "Iteration: 187 \t--- Loss: 0.389\n",
      "Iteration: 188 \t--- Loss: 0.408\n",
      "Iteration: 189 \t--- Loss: 0.442\n",
      "Iteration: 190 \t--- Loss: 0.382\n",
      "Iteration: 191 \t--- Loss: 0.387\n",
      "Iteration: 192 \t--- Loss: 0.392\n",
      "Iteration: 193 \t--- Loss: 0.416\n",
      "Iteration: 194 \t--- Loss: 0.407\n",
      "Iteration: 195 \t--- Loss: 0.355\n",
      "Iteration: 196 \t--- Loss: 0.377\n",
      "Iteration: 197 \t--- Loss: 0.384\n",
      "Iteration: 198 \t--- Loss: 0.389\n",
      "Iteration: 199 \t--- Loss: 0.432\n",
      "Iteration: 200 \t--- Loss: 0.371\n",
      "Iteration: 201 \t--- Loss: 0.449\n",
      "Iteration: 202 \t--- Loss: 0.387\n",
      "Iteration: 203 \t--- Loss: 0.410\n",
      "Iteration: 204 \t--- Loss: 0.397\n",
      "Iteration: 205 \t--- Loss: 0.399\n",
      "Iteration: 206 \t--- Loss: 0.383\n",
      "Iteration: 207 \t--- Loss: 0.386\n",
      "Iteration: 208 \t--- Loss: 0.393\n",
      "Iteration: 209 \t--- Loss: 0.350\n",
      "Iteration: 210 \t--- Loss: 0.455\n",
      "Iteration: 211 \t--- Loss: 0.379\n",
      "Iteration: 212 \t--- Loss: 0.437\n",
      "Iteration: 213 \t--- Loss: 0.411\n",
      "Iteration: 214 \t--- Loss: 0.350\n",
      "Iteration: 215 \t--- Loss: 0.380\n",
      "Iteration: 216 \t--- Loss: 0.383\n",
      "Iteration: 217 \t--- Loss: 0.391\n",
      "Iteration: 218 \t--- Loss: 0.397\n",
      "Iteration: 219 \t--- Loss: 0.394\n",
      "Iteration: 220 \t--- Loss: 0.418\n",
      "Iteration: 221 \t--- Loss: 0.393\n",
      "Iteration: 222 \t--- Loss: 0.410\n",
      "Iteration: 223 \t--- Loss: 0.368\n",
      "Iteration: 224 \t--- Loss: 0.395\n",
      "Iteration: 225 \t--- Loss: 0.395\n",
      "Iteration: 226 \t--- Loss: 0.387\n",
      "Iteration: 227 \t--- Loss: 0.406\n",
      "Iteration: 228 \t--- Loss: 0.384\n",
      "Iteration: 229 \t--- Loss: 0.363\n",
      "Iteration: 230 \t--- Loss: 0.387\n",
      "Iteration: 231 \t--- Loss: 0.406\n",
      "Iteration: 232 \t--- Loss: 0.404\n",
      "Iteration: 233 \t--- Loss: 0.412\n",
      "Iteration: 234 \t--- Loss: 0.371\n",
      "Iteration: 235 \t--- Loss: 0.398\n",
      "Iteration: 236 \t--- Loss: 0.386\n",
      "Iteration: 237 \t--- Loss: 0.359\n",
      "Iteration: 238 \t--- Loss: 0.381\n",
      "Iteration: 239 \t--- Loss: 0.389\n",
      "Iteration: 240 \t--- Loss: 0.408\n",
      "Iteration: 241 \t--- Loss: 0.380\n",
      "Iteration: 242 \t--- Loss: 0.381\n",
      "Iteration: 243 \t--- Loss: 0.317\n",
      "Iteration: 244 \t--- Loss: 0.388\n",
      "Iteration: 245 \t--- Loss: 0.409\n",
      "Iteration: 246 \t--- Loss: 0.398\n",
      "Iteration: 247 \t--- Loss: 0.366\n",
      "Iteration: 248 \t--- Loss: 0.396\n",
      "Iteration: 249 \t--- Loss: 0.328\n",
      "Iteration: 250 \t--- Loss: 0.366\n",
      "Iteration: 251 \t--- Loss: 0.415\n",
      "Iteration: 252 \t--- Loss: 0.378\n",
      "Iteration: 253 \t--- Loss: 0.429\n",
      "Iteration: 254 \t--- Loss: 0.389\n",
      "Iteration: 255 \t--- Loss: 0.393\n",
      "Iteration: 256 \t--- Loss: 0.424\n",
      "Iteration: 257 \t--- Loss: 0.373\n",
      "Iteration: 258 \t--- Loss: 0.389\n",
      "Iteration: 259 \t--- Loss: 0.410"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.35s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.373\n",
      "Iteration: 261 \t--- Loss: 0.383\n",
      "Iteration: 262 \t--- Loss: 0.362\n",
      "Iteration: 263 \t--- Loss: 0.395\n",
      "Iteration: 264 \t--- Loss: 0.393\n",
      "Iteration: 265 \t--- Loss: 0.363\n",
      "Iteration: 266 \t--- Loss: 0.376\n",
      "Iteration: 267 \t--- Loss: 0.377\n",
      "Iteration: 268 \t--- Loss: 0.392\n",
      "Iteration: 269 \t--- Loss: 0.374\n",
      "Iteration: 270 \t--- Loss: 0.350\n",
      "Iteration: 271 \t--- Loss: 0.390\n",
      "Iteration: 272 \t--- Loss: 0.371\n",
      "Iteration: 273 \t--- Loss: 0.400\n",
      "Iteration: 274 \t--- Loss: 0.365\n",
      "Iteration: 275 \t--- Loss: 0.332\n",
      "Iteration: 276 \t--- Loss: 0.337\n",
      "Iteration: 277 \t--- Loss: 0.357\n",
      "Iteration: 278 \t--- Loss: 0.370\n",
      "Iteration: 279 \t--- Loss: 0.394\n",
      "Iteration: 280 \t--- Loss: 0.361\n",
      "Iteration: 281 \t--- Loss: 0.360\n",
      "Iteration: 282 \t--- Loss: 0.381\n",
      "Iteration: 283 \t--- Loss: 0.331\n",
      "Iteration: 284 \t--- Loss: 0.368\n",
      "Iteration: 285 \t--- Loss: 0.374\n",
      "Iteration: 286 \t--- Loss: 0.381\n",
      "Iteration: 287 \t--- Loss: 0.363\n",
      "Iteration: 288 \t--- Loss: 0.374\n",
      "Iteration: 289 \t--- Loss: 0.341\n",
      "Iteration: 290 \t--- Loss: 0.364\n",
      "Iteration: 291 \t--- Loss: 0.369\n",
      "Iteration: 292 \t--- Loss: 0.400\n",
      "Iteration: 293 \t--- Loss: 0.393\n",
      "Iteration: 294 \t--- Loss: 0.383\n",
      "Iteration: 295 \t--- Loss: 0.391\n",
      "Iteration: 296 \t--- Loss: 0.411\n",
      "Iteration: 297 \t--- Loss: 0.397\n",
      "Iteration: 298 \t--- Loss: 0.353\n",
      "Iteration: 299 \t--- Loss: 0.431\n",
      "Iteration: 300 \t--- Loss: 0.401\n",
      "Iteration: 301 \t--- Loss: 0.403\n",
      "Iteration: 302 \t--- Loss: 0.343\n",
      "Iteration: 303 \t--- Loss: 0.352\n",
      "Iteration: 304 \t--- Loss: 0.412\n",
      "Iteration: 305 \t--- Loss: 0.318\n",
      "Iteration: 306 \t--- Loss: 0.361\n",
      "Iteration: 307 \t--- Loss: 0.370\n",
      "Iteration: 308 \t--- Loss: 0.335\n",
      "Iteration: 309 \t--- Loss: 0.364\n",
      "Iteration: 310 \t--- Loss: 0.378\n",
      "Iteration: 311 \t--- Loss: 0.368\n",
      "Iteration: 312 \t--- Loss: 0.350\n",
      "Iteration: 313 \t--- Loss: 0.359\n",
      "Iteration: 314 \t--- Loss: 0.378\n",
      "Iteration: 315 \t--- Loss: 0.354\n",
      "Iteration: 316 \t--- Loss: 0.340\n",
      "Iteration: 317 \t--- Loss: 0.379\n",
      "Iteration: 318 \t--- Loss: 0.355\n",
      "Iteration: 319 \t--- Loss: 0.376\n",
      "Iteration: 320 \t--- Loss: 0.357\n",
      "Iteration: 321 \t--- Loss: 0.354\n",
      "Iteration: 322 \t--- Loss: 0.347\n",
      "Iteration: 323 \t--- Loss: 0.389\n",
      "Iteration: 324 \t--- Loss: 0.364\n",
      "Iteration: 325 \t--- Loss: 0.374\n",
      "Iteration: 326 \t--- Loss: 0.381\n",
      "Iteration: 327 \t--- Loss: 0.353\n",
      "Iteration: 328 \t--- Loss: 0.370\n",
      "Iteration: 329 \t--- Loss: 0.350\n",
      "Iteration: 330 \t--- Loss: 0.376\n",
      "Iteration: 331 \t--- Loss: 0.384\n",
      "Iteration: 332 \t--- Loss: 0.384\n",
      "Iteration: 333 \t--- Loss: 0.344\n",
      "Iteration: 334 \t--- Loss: 0.335\n",
      "Iteration: 335 \t--- Loss: 0.387\n",
      "Iteration: 336 \t--- Loss: 0.367\n",
      "Iteration: 337 \t--- Loss: 0.393\n",
      "Iteration: 338 \t--- Loss: 0.365\n",
      "Iteration: 339 \t--- Loss: 0.378\n",
      "Iteration: 340 \t--- Loss: 0.380\n",
      "Iteration: 341 \t--- Loss: 0.363\n",
      "Iteration: 342 \t--- Loss: 0.341\n",
      "Iteration: 343 \t--- Loss: 0.311\n",
      "Iteration: 344 \t--- Loss: 0.356\n",
      "Iteration: 345 \t--- Loss: 0.327\n",
      "Iteration: 346 \t--- Loss: 0.352\n",
      "Iteration: 347 \t--- Loss: 0.331\n",
      "Iteration: 348 \t--- Loss: 0.360\n",
      "Iteration: 349 \t--- Loss: 0.337\n",
      "Iteration: 350 \t--- Loss: 0.378\n",
      "Iteration: 351 \t--- Loss: 0.408\n",
      "Iteration: 352 \t--- Loss: 0.364\n",
      "Iteration: 353 \t--- Loss: 0.378\n",
      "Iteration: 354 \t--- Loss: 0.375\n",
      "Iteration: 355 \t--- Loss: 0.386\n",
      "Iteration: 356 \t--- Loss: 0.343\n",
      "Iteration: 357 \t--- Loss: 0.441\n",
      "Iteration: 358 \t--- Loss: 0.340\n",
      "Iteration: 359 \t--- Loss: 0.351\n",
      "Iteration: 360 \t--- Loss: 0.349\n",
      "Iteration: 361 \t--- Loss: 0.346\n",
      "Iteration: 362 \t--- Loss: 0.357\n",
      "Iteration: 363 \t--- Loss: 0.366\n",
      "Iteration: 364 \t--- Loss: 0.366\n",
      "Iteration: 365 \t--- Loss: 0.338\n",
      "Iteration: 366 \t--- Loss: 0.404\n",
      "Iteration: 367 \t--- Loss: 0.376\n",
      "Iteration: 368 \t--- Loss: 0.364\n",
      "Iteration: 369 \t--- Loss: 0.356\n",
      "Iteration: 370 \t--- Loss: 0.367\n",
      "Iteration: 371 \t--- Loss: 0.371\n",
      "Iteration: 372 \t--- Loss: 0.364\n",
      "Iteration: 373 \t--- Loss: 0.380\n",
      "Iteration: 374 \t--- Loss: 0.350\n",
      "Iteration: 375 \t--- Loss: 0.354\n",
      "Iteration: 376 \t--- Loss: 0.381\n",
      "Iteration: 377 \t--- Loss: 0.369\n",
      "Iteration: 378 \t--- Loss: 0.374\n",
      "Iteration: 379 \t--- Loss: 0.376\n",
      "Iteration: 380 \t--- Loss: 0.365\n",
      "Iteration: 381 \t--- Loss: 0.383\n",
      "Iteration: 382 \t--- Loss: 0.330\n",
      "Iteration: 383 \t--- Loss: 0.354\n",
      "Iteration: 384 \t--- Loss: 0.369\n",
      "Iteration: 385 \t--- Loss: 0.356\n",
      "Iteration: 386 \t--- Loss: 0.393\n",
      "Iteration: 387 \t--- Loss: 0.351\n",
      "Iteration: 388 \t--- Loss: 0.361\n",
      "Iteration: 389 \t--- Loss: 0.389\n",
      "Iteration: 390 \t--- Loss: 0.358\n",
      "Iteration: 391 \t--- Loss: 0.359\n",
      "Iteration: 392 \t--- Loss: 0.322\n",
      "Iteration: 393 \t--- Loss: 0.455\n",
      "Iteration: 394 \t--- Loss: 0.387\n",
      "Iteration: 395 \t--- Loss: 0.362\n",
      "Iteration: 396 \t--- Loss: 0.357\n",
      "Iteration: 397 \t--- Loss: 0.346\n",
      "Iteration: 398 \t--- Loss: 0.396\n",
      "Iteration: 399 \t--- Loss: 0.363\n",
      "Iteration: 400 \t--- Loss: 0.403\n",
      "Iteration: 401 \t--- Loss: 0.370\n",
      "Iteration: 402 \t--- Loss: 0.352\n",
      "Iteration: 403 \t--- Loss: 0.372\n",
      "Iteration: 404 \t--- Loss: 0.343\n",
      "Iteration: 405 \t--- Loss: 0.391\n",
      "Iteration: 406 \t--- Loss: 0.366\n",
      "Iteration: 407 \t--- Loss: 0.341\n",
      "Iteration: 408 \t--- Loss: 0.356\n",
      "Iteration: 409 \t--- Loss: 0.378\n",
      "Iteration: 410 \t--- Loss: 0.318\n",
      "Iteration: 411 \t--- Loss: 0.355\n",
      "Iteration: 412 \t--- Loss: 0.404\n",
      "Iteration: 413 \t--- Loss: 0.319\n",
      "Iteration: 414 \t--- Loss: 0.392\n",
      "Iteration: 415 \t--- Loss: 0.362\n",
      "Iteration: 416 \t--- Loss: 0.366\n",
      "Iteration: 417 \t--- Loss: 0.355\n",
      "Iteration: 418 \t--- Loss: 0.349\n",
      "Iteration: 419 \t--- Loss: 0.395\n",
      "Iteration: 420 \t--- Loss: 0.344\n",
      "Iteration: 421 \t--- Loss: 0.402\n",
      "Iteration: 422 \t--- Loss: 0.369\n",
      "Iteration: 423 \t--- Loss: 0.330\n",
      "Iteration: 424 \t--- Loss: 0.368\n",
      "Iteration: 425 \t--- Loss: 0.349\n",
      "Iteration: 426 \t--- Loss: 0.369\n",
      "Iteration: 427 \t--- Loss: 0.396\n",
      "Iteration: 428 \t--- Loss: 0.350\n",
      "Iteration: 429 \t--- Loss: 0.337\n",
      "Iteration: 430 \t--- Loss: 0.343\n",
      "Iteration: 431 \t--- Loss: 0.368\n",
      "Iteration: 432 \t--- Loss: 0.355\n",
      "Iteration: 433 \t--- Loss: 0.358\n",
      "Iteration: 434 \t--- Loss: 0.392\n",
      "Iteration: 435 \t--- Loss: 0.356\n",
      "Iteration: 436 \t--- Loss: 0.358\n",
      "Iteration: 437 \t--- Loss: 0.365\n",
      "Iteration: 438 \t--- Loss: 0.395\n",
      "Iteration: 439 \t--- Loss: 0.356\n",
      "Iteration: 440 \t--- Loss: 0.339\n",
      "Iteration: 441 \t--- Loss: 0.370\n",
      "Iteration: 442 \t--- Loss: 0.409\n",
      "Iteration: 443 \t--- Loss: 0.385\n",
      "Iteration: 444 \t--- Loss: 0.330\n",
      "Iteration: 445 \t--- Loss: 0.307\n",
      "Iteration: 446 \t--- Loss: 0.372\n",
      "Iteration: 447 \t--- Loss: 0.370\n",
      "Iteration: 448 \t--- Loss: 0.384\n",
      "Iteration: 449 \t--- Loss: 0.371\n",
      "Iteration: 450 \t--- Loss: 0.358\n",
      "Iteration: 451 \t--- Loss: 0.372\n",
      "Iteration: 452 \t--- Loss: 0.375\n",
      "Iteration: 453 \t--- Loss: 0.325\n",
      "Iteration: 454 \t--- Loss: 0.375\n",
      "Iteration: 455 \t--- Loss: 0.397\n",
      "Iteration: 456 \t--- Loss: 0.379\n",
      "Iteration: 457 \t--- Loss: 0.380\n",
      "Iteration: 458 \t--- Loss: 0.347\n",
      "Iteration: 459 \t--- Loss: 0.374\n",
      "Iteration: 460 \t--- Loss: 0.414\n",
      "Iteration: 461 \t--- Loss: 0.340\n",
      "Iteration: 462 \t--- Loss: 0.378\n",
      "Iteration: 463 \t--- Loss: 0.353\n",
      "Iteration: 464 \t--- Loss: 0.381\n",
      "Iteration: 465 \t--- Loss: 0.355\n",
      "Iteration: 466 \t--- Loss: 0.390\n",
      "Iteration: 467 \t--- Loss: 0.351\n",
      "Iteration: 468 \t--- Loss: 0.331\n",
      "Iteration: 469 \t--- Loss: 0.371\n",
      "Iteration: 470 \t--- Loss: 0.343\n",
      "Iteration: 471 \t--- Loss: 0.384\n",
      "Iteration: 472 \t--- Loss: 0.392\n",
      "Iteration: 473 \t--- Loss: 0.352\n",
      "Iteration: 474 \t--- Loss: 0.337\n",
      "Iteration: 475 \t--- Loss: 0.367\n",
      "Iteration: 476 \t--- Loss: 0.355\n",
      "Iteration: 477 \t--- Loss: 0.365\n",
      "Iteration: 478 \t--- Loss: 0.394\n",
      "Iteration: 479 \t--- Loss: 0.354\n",
      "Iteration: 480 \t--- Loss: 0.381\n",
      "Iteration: 481 \t--- Loss: 0.413\n",
      "Iteration: 482 \t--- Loss: 0.380\n",
      "Iteration: 483 \t--- Loss: 0.355\n",
      "Iteration: 484 \t--- Loss: 0.382\n",
      "Iteration: 485 \t--- Loss: 0.325\n",
      "Iteration: 486 \t--- Loss: 0.338\n",
      "Iteration: 487 \t--- Loss: 0.367\n",
      "Iteration: 488 \t--- Loss: 0.391\n",
      "Iteration: 489 \t--- Loss: 0.340\n",
      "Iteration: 490 \t--- Loss: 0.352\n",
      "Iteration: 491 \t--- Loss: 0.357\n",
      "Iteration: 492 \t--- Loss: 0.341\n",
      "Iteration: 493 \t--- Loss: 0.394\n",
      "Iteration: 494 \t--- Loss: 0.400\n",
      "Iteration: 495 \t--- Loss: 0.377\n",
      "Iteration: 496 \t--- Loss: 0.386\n",
      "Iteration: 497 \t--- Loss: 0.357\n",
      "Iteration: 498 \t--- Loss: 0.307\n",
      "Iteration: 499 \t--- Loss: 0.312\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:02<00:07,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.379\n",
      "Iteration: 1 \t--- Loss: 0.365\n",
      "Iteration: 2 \t--- Loss: 0.379\n",
      "Iteration: 3 \t--- Loss: 0.353\n",
      "Iteration: 4 \t--- Loss: 0.362\n",
      "Iteration: 5 \t--- Loss: 0.404\n",
      "Iteration: 6 \t--- Loss: 0.343\n",
      "Iteration: 7 \t--- Loss: 0.379\n",
      "Iteration: 8 \t--- Loss: 0.374\n",
      "Iteration: 9 \t--- Loss: 0.373\n",
      "Iteration: 10 \t--- Loss: 0.389\n",
      "Iteration: 11 \t--- Loss: 0.348\n",
      "Iteration: 12 \t--- Loss: 0.336\n",
      "Iteration: 13 \t--- Loss: 0.328\n",
      "Iteration: 14 \t--- Loss: 0.350\n",
      "Iteration: 15 \t--- Loss: 0.320\n",
      "Iteration: 16 \t--- Loss: 0.346\n",
      "Iteration: 17 \t--- Loss: 0.374\n",
      "Iteration: 18 \t--- Loss: 0.326\n",
      "Iteration: 19 \t--- Loss: 0.367\n",
      "Iteration: 20 \t--- Loss: 0.337\n",
      "Iteration: 21 \t--- Loss: 0.323\n",
      "Iteration: 22 \t--- Loss: 0.331\n",
      "Iteration: 23 \t--- Loss: 0.336\n",
      "Iteration: 24 \t--- Loss: 0.349\n",
      "Iteration: 25 \t--- Loss: 0.335\n",
      "Iteration: 26 \t--- Loss: 0.353\n",
      "Iteration: 27 \t--- Loss: 0.327\n",
      "Iteration: 28 \t--- Loss: 0.366\n",
      "Iteration: 29 \t--- Loss: 0.362\n",
      "Iteration: 30 \t--- Loss: 0.338\n",
      "Iteration: 31 \t--- Loss: 0.345\n",
      "Iteration: 32 \t--- Loss: 0.374\n",
      "Iteration: 33 \t--- Loss: 0.350\n",
      "Iteration: 34 \t--- Loss: 0.354\n",
      "Iteration: 35 \t--- Loss: 0.342\n",
      "Iteration: 36 \t--- Loss: 0.351\n",
      "Iteration: 37 \t--- Loss: 0.356\n",
      "Iteration: 38 \t--- Loss: 0.340\n",
      "Iteration: 39 \t--- Loss: 0.323\n",
      "Iteration: 40 \t--- Loss: 0.311\n",
      "Iteration: 41 \t--- Loss: 0.346\n",
      "Iteration: 42 \t--- Loss: 0.279\n",
      "Iteration: 43 \t--- Loss: 0.316\n",
      "Iteration: 44 \t--- Loss: 0.323\n",
      "Iteration: 45 \t--- Loss: 0.313\n",
      "Iteration: 46 \t--- Loss: 0.299\n",
      "Iteration: 47 \t--- Loss: 0.292\n",
      "Iteration: 48 \t--- Loss: 0.301\n",
      "Iteration: 49 \t--- Loss: 0.297\n",
      "Iteration: 50 \t--- Loss: 0.309\n",
      "Iteration: 51 \t--- Loss: 0.349\n",
      "Iteration: 52 \t--- Loss: 0.362\n",
      "Iteration: 53 \t--- Loss: 0.346\n",
      "Iteration: 54 \t--- Loss: 0.386\n",
      "Iteration: 55 \t--- Loss: 0.372\n",
      "Iteration: 56 \t--- Loss: 0.358\n",
      "Iteration: 57 \t--- Loss: 0.279\n",
      "Iteration: 58 \t--- Loss: 0.317\n",
      "Iteration: 59 \t--- Loss: 0.317\n",
      "Iteration: 60 \t--- Loss: 0.303\n",
      "Iteration: 61 \t--- Loss: 0.306\n",
      "Iteration: 62 \t--- Loss: 0.324\n",
      "Iteration: 63 \t--- Loss: 0.333\n",
      "Iteration: 64 \t--- Loss: 0.314\n",
      "Iteration: 65 \t--- Loss: 0.354\n",
      "Iteration: 66 \t--- Loss: 0.328\n",
      "Iteration: 67 \t--- Loss: 0.323\n",
      "Iteration: 68 \t--- Loss: 0.334\n",
      "Iteration: 69 \t--- Loss: 0.294\n",
      "Iteration: 70 \t--- Loss: 0.305\n",
      "Iteration: 71 \t--- Loss: 0.312\n",
      "Iteration: 72 \t--- Loss: 0.344\n",
      "Iteration: 73 \t--- Loss: 0.331\n",
      "Iteration: 74 \t--- Loss: 0.356\n",
      "Iteration: 75 \t--- Loss: 0.318\n",
      "Iteration: 76 \t--- Loss: 0.290\n",
      "Iteration: 77 \t--- Loss: 0.295\n",
      "Iteration: 78 \t--- Loss: 0.300\n",
      "Iteration: 79 \t--- Loss: 0.294\n",
      "Iteration: 80 \t--- Loss: 0.330\n",
      "Iteration: 81 \t--- Loss: 0.368\n",
      "Iteration: 82 \t--- Loss: 0.313\n",
      "Iteration: 83 \t--- Loss: 0.315\n",
      "Iteration: 84 \t--- Loss: 0.290\n",
      "Iteration: 85 \t--- Loss: 0.284\n",
      "Iteration: 86 \t--- Loss: 0.279\n",
      "Iteration: 87 \t--- Loss: 0.300\n",
      "Iteration: 88 \t--- Loss: 0.339\n",
      "Iteration: 89 \t--- Loss: 0.282\n",
      "Iteration: 90 \t--- Loss: 0.266\n",
      "Iteration: 91 \t--- Loss: 0.258\n",
      "Iteration: 92 \t--- Loss: 0.279\n",
      "Iteration: 93 \t--- Loss: 0.313\n",
      "Iteration: 94 \t--- Loss: 0.277\n",
      "Iteration: 95 \t--- Loss: 0.256\n",
      "Iteration: 96 \t--- Loss: 0.271\n",
      "Iteration: 97 \t--- Loss: 0.297\n",
      "Iteration: 98 \t--- Loss: 0.285\n",
      "Iteration: 99 \t--- Loss: 0.313\n",
      "Iteration: 100 \t--- Loss: 0.315\n",
      "Iteration: 101 \t--- Loss: 0.304\n",
      "Iteration: 102 \t--- Loss: 0.256\n",
      "Iteration: 103 \t--- Loss: 0.267\n",
      "Iteration: 104 \t--- Loss: 0.273\n",
      "Iteration: 105 \t--- Loss: 0.339\n",
      "Iteration: 106 \t--- Loss: 0.242\n",
      "Iteration: 107 \t--- Loss: 0.260\n",
      "Iteration: 108 \t--- Loss: 0.264\n",
      "Iteration: 109 \t--- Loss: 0.271\n",
      "Iteration: 110 \t--- Loss: 0.293\n",
      "Iteration: 111 \t--- Loss: 0.281\n",
      "Iteration: 112 \t--- Loss: 0.334\n",
      "Iteration: 113 \t--- Loss: 0.266\n",
      "Iteration: 114 \t--- Loss: 0.283\n",
      "Iteration: 115 \t--- Loss: 0.303\n",
      "Iteration: 116 \t--- Loss: 0.257\n",
      "Iteration: 117 \t--- Loss: 0.264\n",
      "Iteration: 118 \t--- Loss: 0.241\n",
      "Iteration: 119 \t--- Loss: 0.295\n",
      "Iteration: 120 \t--- Loss: 0.314\n",
      "Iteration: 121 \t--- Loss: 0.232\n",
      "Iteration: 122 \t--- Loss: 0.262\n",
      "Iteration: 123 \t--- Loss: 0.280\n",
      "Iteration: 124 \t--- Loss: 0.280\n",
      "Iteration: 125 \t--- Loss: 0.291\n",
      "Iteration: 126 \t--- Loss: 0.248\n",
      "Iteration: 127 \t--- Loss: 0.260\n",
      "Iteration: 128 \t--- Loss: 0.274\n",
      "Iteration: 129 \t--- Loss: 0.284\n",
      "Iteration: 130 \t--- Loss: 0.243\n",
      "Iteration: 131 \t--- Loss: 0.281\n",
      "Iteration: 132 \t--- Loss: 0.264\n",
      "Iteration: 133 \t--- Loss: 0.236\n",
      "Iteration: 134 \t--- Loss: 0.270\n",
      "Iteration: 135 \t--- Loss: 0.262\n",
      "Iteration: 136 \t--- Loss: 0.243\n",
      "Iteration: 137 \t--- Loss: 0.265\n",
      "Iteration: 138 \t--- Loss: 0.261\n",
      "Iteration: 139 \t--- Loss: 0.285\n",
      "Iteration: 140 \t--- Loss: 0.251\n",
      "Iteration: 141 \t--- Loss: 0.251\n",
      "Iteration: 142 \t--- Loss: 0.284\n",
      "Iteration: 143 \t--- Loss: 0.283\n",
      "Iteration: 144 \t--- Loss: 0.232\n",
      "Iteration: 145 \t--- Loss: 0.273\n",
      "Iteration: 146 \t--- Loss: 0.261\n",
      "Iteration: 147 \t--- Loss: 0.273\n",
      "Iteration: 148 \t--- Loss: 0.257\n",
      "Iteration: 149 \t--- Loss: 0.244\n",
      "Iteration: 150 \t--- Loss: 0.263\n",
      "Iteration: 151 \t--- Loss: 0.264\n",
      "Iteration: 152 \t--- Loss: 0.255\n",
      "Iteration: 153 \t--- Loss: 0.268\n",
      "Iteration: 154 \t--- Loss: 0.232\n",
      "Iteration: 155 \t--- Loss: 0.255\n",
      "Iteration: 156 \t--- Loss: 0.258\n",
      "Iteration: 157 \t--- Loss: 0.255\n",
      "Iteration: 158 \t--- Loss: 0.293\n",
      "Iteration: 159 \t--- Loss: 0.271\n",
      "Iteration: 160 \t--- Loss: 0.243\n",
      "Iteration: 161 \t--- Loss: 0.265\n",
      "Iteration: 162 \t--- Loss: 0.243\n",
      "Iteration: 163 \t--- Loss: 0.238\n",
      "Iteration: 164 \t--- Loss: 0.230\n",
      "Iteration: 165 \t--- Loss: 0.224\n",
      "Iteration: 166 \t--- Loss: 0.237\n",
      "Iteration: 167 \t--- Loss: 0.248\n",
      "Iteration: 168 \t--- Loss: 0.258\n",
      "Iteration: 169 \t--- Loss: 0.219\n",
      "Iteration: 170 \t--- Loss: 0.247\n",
      "Iteration: 171 \t--- Loss: 0.260\n",
      "Iteration: 172 \t--- Loss: 0.265\n",
      "Iteration: 173 \t--- Loss: 0.245\n",
      "Iteration: 174 \t--- Loss: 0.238\n",
      "Iteration: 175 \t--- Loss: 0.265\n",
      "Iteration: 176 \t--- Loss: 0.238\n",
      "Iteration: 177 \t--- Loss: 0.276\n",
      "Iteration: 178 \t--- Loss: 0.266\n",
      "Iteration: 179 \t--- Loss: 0.245\n",
      "Iteration: 180 \t--- Loss: 0.273\n",
      "Iteration: 181 \t--- Loss: 0.241\n",
      "Iteration: 182 \t--- Loss: 0.259\n",
      "Iteration: 183 \t--- Loss: 0.253\n",
      "Iteration: 184 \t--- Loss: 0.238\n",
      "Iteration: 185 \t--- Loss: 0.257\n",
      "Iteration: 186 \t--- Loss: 0.255\n",
      "Iteration: 187 \t--- Loss: 0.244\n",
      "Iteration: 188 \t--- Loss: 0.231\n",
      "Iteration: 189 \t--- Loss: 0.252\n",
      "Iteration: 190 \t--- Loss: 0.242\n",
      "Iteration: 191 \t--- Loss: 0.232\n",
      "Iteration: 192 \t--- Loss: 0.249\n",
      "Iteration: 193 \t--- Loss: 0.248\n",
      "Iteration: 194 \t--- Loss: 0.233\n",
      "Iteration: 195 \t--- Loss: 0.241\n",
      "Iteration: 196 \t--- Loss: 0.262\n",
      "Iteration: 197 \t--- Loss: 0.245\n",
      "Iteration: 198 \t--- Loss: 0.244\n",
      "Iteration: 199 \t--- Loss: 0.236\n",
      "Iteration: 200 \t--- Loss: 0.227\n",
      "Iteration: 201 \t--- Loss: 0.242\n",
      "Iteration: 202 \t--- Loss: 0.239\n",
      "Iteration: 203 \t--- Loss: 0.229\n",
      "Iteration: 204 \t--- Loss: 0.241\n",
      "Iteration: 205 \t--- Loss: 0.250\n",
      "Iteration: 206 \t--- Loss: 0.247\n",
      "Iteration: 207 \t--- Loss: 0.237\n",
      "Iteration: 208 \t--- Loss: 0.255\n",
      "Iteration: 209 \t--- Loss: 0.254\n",
      "Iteration: 210 \t--- Loss: 0.218\n",
      "Iteration: 211 \t--- Loss: 0.232\n",
      "Iteration: 212 \t--- Loss: 0.235\n",
      "Iteration: 213 \t--- Loss: 0.259\n",
      "Iteration: 214 \t--- Loss: 0.277\n",
      "Iteration: 215 \t--- Loss: 0.248\n",
      "Iteration: 216 \t--- Loss: 0.235\n",
      "Iteration: 217 \t--- Loss: 0.246\n",
      "Iteration: 218 \t--- Loss: 0.246\n",
      "Iteration: 219 \t--- Loss: 0.252\n",
      "Iteration: 220 \t--- Loss: 0.234\n",
      "Iteration: 221 \t--- Loss: 0.249\n",
      "Iteration: 222 \t--- Loss: 0.239\n",
      "Iteration: 223 \t--- Loss: 0.226\n",
      "Iteration: 224 \t--- Loss: 0.217\n",
      "Iteration: 225 \t--- Loss: 0.253\n",
      "Iteration: 226 \t--- Loss: 0.245\n",
      "Iteration: 227 \t--- Loss: 0.247\n",
      "Iteration: 228 \t--- Loss: 0.244\n",
      "Iteration: 229 \t--- Loss: 0.232\n",
      "Iteration: 230 \t--- Loss: 0.241\n",
      "Iteration: 231 \t--- Loss: 0.245\n",
      "Iteration: 232 \t--- Loss: 0.239\n",
      "Iteration: 233 \t--- Loss: 0.262\n",
      "Iteration: 234 \t--- Loss: 0.238\n",
      "Iteration: 235 \t--- Loss: 0.230\n",
      "Iteration: 236 \t--- Loss: 0.237\n",
      "Iteration: 237 \t--- Loss: 0.238\n",
      "Iteration: 238 \t--- Loss: 0.249\n",
      "Iteration: 239 \t--- Loss: 0.287\n",
      "Iteration: 240 \t--- Loss: 0.238\n",
      "Iteration: 241 \t--- Loss: 0.243\n",
      "Iteration: 242 \t--- Loss: 0.231\n",
      "Iteration: 243 \t--- Loss: 0.251\n",
      "Iteration: 244 \t--- Loss: 0.273\n",
      "Iteration: 245 \t--- Loss: 0.265\n",
      "Iteration: 246 \t--- Loss: 0.226\n",
      "Iteration: 247 \t--- Loss: 0.240\n",
      "Iteration: 248 \t--- Loss: 0.244\n",
      "Iteration: 249 \t--- Loss: 0.233\n",
      "Iteration: 250 \t--- Loss: 0.228\n",
      "Iteration: 251 \t--- Loss: 0.219\n",
      "Iteration: 252 \t--- Loss: 0.227\n",
      "Iteration: 253 \t--- Loss: 0.226\n",
      "Iteration: 254 \t--- Loss: 0.243\n",
      "Iteration: 255 \t--- Loss: 0.233\n",
      "Iteration: 256 \t--- Loss: 0.232\n",
      "Iteration: 257 \t--- Loss: 0.228\n",
      "Iteration: 258 \t--- Loss: 0.230\n",
      "Iteration: 259 \t--- Loss: 0.229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:52<00:00, 52.22s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.065\n",
      "Iteration: 261 \t--- Loss: 0.066\n",
      "Iteration: 262 \t--- Loss: 0.067\n",
      "Iteration: 263 \t--- Loss: 0.066\n",
      "Iteration: 264 \t--- Loss: 0.065\n",
      "Iteration: 265 \t--- Loss: 0.067\n",
      "Iteration: 266 \t--- Loss: 0.067\n",
      "Iteration: 267 \t--- Loss: 0.067\n",
      "Iteration: 268 \t--- Loss: 0.065\n",
      "Iteration: 269 \t--- Loss: 0.067\n",
      "Iteration: 270 \t--- Loss: 0.068\n",
      "Iteration: 271 \t--- Loss: 0.067\n",
      "Iteration: 272 \t--- Loss: 0.067\n",
      "Iteration: 273 \t--- Loss: 0.067\n",
      "Iteration: 274 \t--- Loss: 0.066\n",
      "Iteration: 275 \t--- Loss: 0.067\n",
      "Iteration: 276 \t--- Loss: 0.066\n",
      "Iteration: 277 \t--- Loss: 0.066\n",
      "Iteration: 278 \t--- Loss: 0.067\n",
      "Iteration: 279 \t--- Loss: 0.067\n",
      "Iteration: 280 \t--- Loss: 0.065\n",
      "Iteration: 281 \t--- Loss: 0.069\n",
      "Iteration: 282 \t--- Loss: 0.066\n",
      "Iteration: 283 \t--- Loss: 0.067\n",
      "Iteration: 284 \t--- Loss: 0.066\n",
      "Iteration: 285 \t--- Loss: 0.067\n",
      "Iteration: 286 \t--- Loss: 0.065\n",
      "Iteration: 287 \t--- Loss: 0.066\n",
      "Iteration: 288 \t--- Loss: 0.066\n",
      "Iteration: 289 \t--- Loss: 0.068\n",
      "Iteration: 290 \t--- Loss: 0.068\n",
      "Iteration: 291 \t--- Loss: 0.067\n",
      "Iteration: 292 \t--- Loss: 0.067\n",
      "Iteration: 293 \t--- Loss: 0.067\n",
      "Iteration: 294 \t--- Loss: 0.067\n",
      "Iteration: 295 \t--- Loss: 0.065\n",
      "Iteration: 296 \t--- Loss: 0.067\n",
      "Iteration: 297 \t--- Loss: 0.065\n",
      "Iteration: 298 \t--- Loss: 0.066\n",
      "Iteration: 299 \t--- Loss: 0.065\n",
      "Iteration: 300 \t--- Loss: 0.067\n",
      "Iteration: 301 \t--- Loss: 0.066\n",
      "Iteration: 302 \t--- Loss: 0.065\n",
      "Iteration: 303 \t--- Loss: 0.068\n",
      "Iteration: 304 \t--- Loss: 0.067\n",
      "Iteration: 305 \t--- Loss: 0.066\n",
      "Iteration: 306 \t--- Loss: 0.066\n",
      "Iteration: 307 \t--- Loss: 0.066\n",
      "Iteration: 308 \t--- Loss: 0.068\n",
      "Iteration: 309 \t--- Loss: 0.067\n",
      "Iteration: 310 \t--- Loss: 0.065\n",
      "Iteration: 311 \t--- Loss: 0.066\n",
      "Iteration: 312 \t--- Loss: 0.067\n",
      "Iteration: 313 \t--- Loss: 0.066\n",
      "Iteration: 314 \t--- Loss: 0.067\n",
      "Iteration: 315 \t--- Loss: 0.068\n",
      "Iteration: 316 \t--- Loss: 0.066\n",
      "Iteration: 317 \t--- Loss: 0.066\n",
      "Iteration: 318 \t--- Loss: 0.065\n",
      "Iteration: 319 \t--- Loss: 0.067\n",
      "Iteration: 320 \t--- Loss: 0.066\n",
      "Iteration: 321 \t--- Loss: 0.068\n",
      "Iteration: 322 \t--- Loss: 0.068\n",
      "Iteration: 323 \t--- Loss: 0.068\n",
      "Iteration: 324 \t--- Loss: 0.065\n",
      "Iteration: 325 \t--- Loss: 0.065\n",
      "Iteration: 326 \t--- Loss: 0.065\n",
      "Iteration: 327 \t--- Loss: 0.067\n",
      "Iteration: 328 \t--- Loss: 0.066\n",
      "Iteration: 329 \t--- Loss: 0.065\n",
      "Iteration: 330 \t--- Loss: 0.065\n",
      "Iteration: 331 \t--- Loss: 0.067\n",
      "Iteration: 332 \t--- Loss: 0.068\n",
      "Iteration: 333 \t--- Loss: 0.068\n",
      "Iteration: 334 \t--- Loss: 0.067\n",
      "Iteration: 335 \t--- Loss: 0.067\n",
      "Iteration: 336 \t--- Loss: 0.065\n",
      "Iteration: 337 \t--- Loss: 0.066\n",
      "Iteration: 338 \t--- Loss: 0.066\n",
      "Iteration: 339 \t--- Loss: 0.066\n",
      "Iteration: 340 \t--- Loss: 0.066\n",
      "Iteration: 341 \t--- Loss: 0.067\n",
      "Iteration: 342 \t--- Loss: 0.067\n",
      "Iteration: 343 \t--- Loss: 0.067\n",
      "Iteration: 344 \t--- Loss: 0.065\n",
      "Iteration: 345 \t--- Loss: 0.067\n",
      "Iteration: 346 \t--- Loss: 0.066\n",
      "Iteration: 347 \t--- Loss: 0.066\n",
      "Iteration: 348 \t--- Loss: 0.067\n",
      "Iteration: 349 \t--- Loss: 0.066\n",
      "Iteration: 350 \t--- Loss: 0.066\n",
      "Iteration: 351 \t--- Loss: 0.068\n",
      "Iteration: 352 \t--- Loss: 0.068\n",
      "Iteration: 353 \t--- Loss: 0.068\n",
      "Iteration: 354 \t--- Loss: 0.067\n",
      "Iteration: 355 \t--- Loss: 0.068\n",
      "Iteration: 356 \t--- Loss: 0.067\n",
      "Iteration: 357 \t--- Loss: 0.066\n",
      "Iteration: 358 \t--- Loss: 0.067\n",
      "Iteration: 359 \t--- Loss: 0.066\n",
      "Iteration: 360 \t--- Loss: 0.065\n",
      "Iteration: 361 \t--- Loss: 0.067\n",
      "Iteration: 362 \t--- Loss: 0.066\n",
      "Iteration: 363 \t--- Loss: 0.065\n",
      "Iteration: 364 \t--- Loss: 0.067\n",
      "Iteration: 365 \t--- Loss: 0.066\n",
      "Iteration: 366 \t--- Loss: 0.067\n",
      "Iteration: 367 \t--- Loss: 0.066\n",
      "Iteration: 368 \t--- Loss: 0.067\n",
      "Iteration: 369 \t--- Loss: 0.066\n",
      "Iteration: 370 \t--- Loss: 0.067\n",
      "Iteration: 371 \t--- Loss: 0.066\n",
      "Iteration: 372 \t--- Loss: 0.067\n",
      "Iteration: 373 \t--- Loss: 0.066\n",
      "Iteration: 374 \t--- Loss: 0.067\n",
      "Iteration: 375 \t--- Loss: 0.067\n",
      "Iteration: 376 \t--- Loss: 0.064\n",
      "Iteration: 377 \t--- Loss: 0.068\n",
      "Iteration: 378 \t--- Loss: 0.066\n",
      "Iteration: 379 \t--- Loss: 0.066\n",
      "Iteration: 380 \t--- Loss: 0.066\n",
      "Iteration: 381 \t--- Loss: 0.068\n",
      "Iteration: 382 \t--- Loss: 0.065\n",
      "Iteration: 383 \t--- Loss: 0.067\n",
      "Iteration: 384 \t--- Loss: 0.067\n",
      "Iteration: 385 \t--- Loss: 0.065\n",
      "Iteration: 386 \t--- Loss: 0.067\n",
      "Iteration: 387 \t--- Loss: 0.067\n",
      "Iteration: 388 \t--- Loss: 0.066\n",
      "Iteration: 389 \t--- Loss: 0.066\n",
      "Iteration: 390 \t--- Loss: 0.066\n",
      "Iteration: 391 \t--- Loss: 0.066\n",
      "Iteration: 392 \t--- Loss: 0.067\n",
      "Iteration: 393 \t--- Loss: 0.066\n",
      "Iteration: 394 \t--- Loss: 0.066\n",
      "Iteration: 395 \t--- Loss: 0.067\n",
      "Iteration: 396 \t--- Loss: 0.066\n",
      "Iteration: 397 \t--- Loss: 0.067\n",
      "Iteration: 398 \t--- Loss: 0.067\n",
      "Iteration: 399 \t--- Loss: 0.068\n",
      "Iteration: 400 \t--- Loss: 0.066\n",
      "Iteration: 401 \t--- Loss: 0.066\n",
      "Iteration: 402 \t--- Loss: 0.066\n",
      "Iteration: 403 \t--- Loss: 0.067\n",
      "Iteration: 404 \t--- Loss: 0.068\n",
      "Iteration: 405 \t--- Loss: 0.067\n",
      "Iteration: 406 \t--- Loss: 0.067\n",
      "Iteration: 407 \t--- Loss: 0.067\n",
      "Iteration: 408 \t--- Loss: 0.066\n",
      "Iteration: 409 \t--- Loss: 0.065\n",
      "Iteration: 410 \t--- Loss: 0.068\n",
      "Iteration: 411 \t--- Loss: 0.066\n",
      "Iteration: 412 \t--- Loss: 0.066\n",
      "Iteration: 413 \t--- Loss: 0.068\n",
      "Iteration: 414 \t--- Loss: 0.068\n",
      "Iteration: 415 \t--- Loss: 0.067\n",
      "Iteration: 416 \t--- Loss: 0.065\n",
      "Iteration: 417 \t--- Loss: 0.069\n",
      "Iteration: 418 \t--- Loss: 0.067\n",
      "Iteration: 419 \t--- Loss: 0.065\n",
      "Iteration: 420 \t--- Loss: 0.066\n",
      "Iteration: 421 \t--- Loss: 0.067\n",
      "Iteration: 422 \t--- Loss: 0.067\n",
      "Iteration: 423 \t--- Loss: 0.067\n",
      "Iteration: 424 \t--- Loss: 0.066\n",
      "Iteration: 425 \t--- Loss: 0.067\n",
      "Iteration: 426 \t--- Loss: 0.068\n",
      "Iteration: 427 \t--- Loss: 0.067\n",
      "Iteration: 428 \t--- Loss: 0.067\n",
      "Iteration: 429 \t--- Loss: 0.066\n",
      "Iteration: 430 \t--- Loss: 0.066\n",
      "Iteration: 431 \t--- Loss: 0.067\n",
      "Iteration: 432 \t--- Loss: 0.066\n",
      "Iteration: 433 \t--- Loss: 0.067\n",
      "Iteration: 434 \t--- Loss: 0.066\n",
      "Iteration: 435 \t--- Loss: 0.066\n",
      "Iteration: 436 \t--- Loss: 0.066\n",
      "Iteration: 437 \t--- Loss: 0.066\n",
      "Iteration: 438 \t--- Loss: 0.069\n",
      "Iteration: 439 \t--- Loss: 0.066\n",
      "Iteration: 440 \t--- Loss: 0.065\n",
      "Iteration: 441 \t--- Loss: 0.067\n",
      "Iteration: 442 \t--- Loss: 0.068\n",
      "Iteration: 443 \t--- Loss: 0.067\n",
      "Iteration: 444 \t--- Loss: 0.066\n",
      "Iteration: 445 \t--- Loss: 0.066\n",
      "Iteration: 446 \t--- Loss: 0.065\n",
      "Iteration: 447 \t--- Loss: 0.067\n",
      "Iteration: 448 \t--- Loss: 0.065\n",
      "Iteration: 449 \t--- Loss: 0.066\n",
      "Iteration: 450 \t--- Loss: 0.067\n",
      "Iteration: 451 \t--- Loss: 0.067\n",
      "Iteration: 452 \t--- Loss: 0.065\n",
      "Iteration: 453 \t--- Loss: 0.067\n",
      "Iteration: 454 \t--- Loss: 0.066\n",
      "Iteration: 455 \t--- Loss: 0.067\n",
      "Iteration: 456 \t--- Loss: 0.065\n",
      "Iteration: 457 \t--- Loss: 0.066\n",
      "Iteration: 458 \t--- Loss: 0.068\n",
      "Iteration: 459 \t--- Loss: 0.067\n",
      "Iteration: 460 \t--- Loss: 0.066\n",
      "Iteration: 461 \t--- Loss: 0.067\n",
      "Iteration: 462 \t--- Loss: 0.067\n",
      "Iteration: 463 \t--- Loss: 0.067\n",
      "Iteration: 464 \t--- Loss: 0.066\n",
      "Iteration: 465 \t--- Loss: 0.068\n",
      "Iteration: 466 \t--- Loss: 0.067\n",
      "Iteration: 467 \t--- Loss: 0.068\n",
      "Iteration: 468 \t--- Loss: 0.067\n",
      "Iteration: 469 \t--- Loss: 0.066\n",
      "Iteration: 470 \t--- Loss: 0.068\n",
      "Iteration: 471 \t--- Loss: 0.066\n",
      "Iteration: 472 \t--- Loss: 0.067\n",
      "Iteration: 473 \t--- Loss: 0.067\n",
      "Iteration: 474 \t--- Loss: 0.065\n",
      "Iteration: 475 \t--- Loss: 0.068\n",
      "Iteration: 476 \t--- Loss: 0.066\n",
      "Iteration: 477 \t--- Loss: 0.069\n",
      "Iteration: 478 \t--- Loss: 0.067\n",
      "Iteration: 479 \t--- Loss: 0.066\n",
      "Iteration: 480 \t--- Loss: 0.066\n",
      "Iteration: 481 \t--- Loss: 0.064\n",
      "Iteration: 482 \t--- Loss: 0.066\n",
      "Iteration: 483 \t--- Loss: 0.068\n",
      "Iteration: 484 \t--- Loss: 0.068\n",
      "Iteration: 485 \t--- Loss: 0.065\n",
      "Iteration: 486 \t--- Loss: 0.067\n",
      "Iteration: 487 \t--- Loss: 0.065\n",
      "Iteration: 488 \t--- Loss: 0.067\n",
      "Iteration: 489 \t--- Loss: 0.066\n",
      "Iteration: 490 \t--- Loss: 0.067\n",
      "Iteration: 491 \t--- Loss: 0.067\n",
      "Iteration: 492 \t--- Loss: 0.066\n",
      "Iteration: 493 \t--- Loss: 0.066\n",
      "Iteration: 494 \t--- Loss: 0.069\n",
      "Iteration: 495 \t--- Loss: 0.067\n",
      "Iteration: 496 \t--- Loss: 0.067\n",
      "Iteration: 497 \t--- Loss: 0.068\n",
      "Iteration: 498 \t--- Loss: 0.065\n",
      "Iteration: 499 \t--- Loss: 0.068\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.62s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.907\n",
      "Iteration: 261 \t--- Loss: 0.867\n",
      "Iteration: 262 \t--- Loss: 0.891\n",
      "Iteration: 263 \t--- Loss: 0.874\n",
      "Iteration: 264 \t--- Loss: 0.914\n",
      "Iteration: 265 \t--- Loss: 0.847\n",
      "Iteration: 266 \t--- Loss: 0.871\n",
      "Iteration: 267 \t--- Loss: 0.916\n",
      "Iteration: 268 \t--- Loss: 0.879\n",
      "Iteration: 269 \t--- Loss: 0.890\n",
      "Iteration: 270 \t--- Loss: 0.880\n",
      "Iteration: 271 \t--- Loss: 0.842\n",
      "Iteration: 272 \t--- Loss: 0.929\n",
      "Iteration: 273 \t--- Loss: 0.894\n",
      "Iteration: 274 \t--- Loss: 0.811\n",
      "Iteration: 275 \t--- Loss: 0.887\n",
      "Iteration: 276 \t--- Loss: 0.872\n",
      "Iteration: 277 \t--- Loss: 0.921\n",
      "Iteration: 278 \t--- Loss: 0.832\n",
      "Iteration: 279 \t--- Loss: 0.868\n",
      "Iteration: 280 \t--- Loss: 0.886\n",
      "Iteration: 281 \t--- Loss: 0.895\n",
      "Iteration: 282 \t--- Loss: 0.913\n",
      "Iteration: 283 \t--- Loss: 0.906\n",
      "Iteration: 284 \t--- Loss: 0.919\n",
      "Iteration: 285 \t--- Loss: 0.908\n",
      "Iteration: 286 \t--- Loss: 0.890\n",
      "Iteration: 287 \t--- Loss: 0.958\n",
      "Iteration: 288 \t--- Loss: 0.913\n",
      "Iteration: 289 \t--- Loss: 0.906\n",
      "Iteration: 290 \t--- Loss: 0.837\n",
      "Iteration: 291 \t--- Loss: 0.893\n",
      "Iteration: 292 \t--- Loss: 0.906\n",
      "Iteration: 293 \t--- Loss: 0.916\n",
      "Iteration: 294 \t--- Loss: 0.844\n",
      "Iteration: 295 \t--- Loss: 0.839\n",
      "Iteration: 296 \t--- Loss: 0.864\n",
      "Iteration: 297 \t--- Loss: 0.852\n",
      "Iteration: 298 \t--- Loss: 0.918\n",
      "Iteration: 299 \t--- Loss: 0.929\n",
      "Iteration: 300 \t--- Loss: 0.840\n",
      "Iteration: 301 \t--- Loss: 0.901\n",
      "Iteration: 302 \t--- Loss: 0.901\n",
      "Iteration: 303 \t--- Loss: 0.906\n",
      "Iteration: 304 \t--- Loss: 0.908\n",
      "Iteration: 305 \t--- Loss: 0.898\n",
      "Iteration: 306 \t--- Loss: 0.897\n",
      "Iteration: 307 \t--- Loss: 0.879\n",
      "Iteration: 308 \t--- Loss: 0.890\n",
      "Iteration: 309 \t--- Loss: 0.841\n",
      "Iteration: 310 \t--- Loss: 0.871\n",
      "Iteration: 311 \t--- Loss: 0.937\n",
      "Iteration: 312 \t--- Loss: 0.918\n",
      "Iteration: 313 \t--- Loss: 0.917\n",
      "Iteration: 314 \t--- Loss: 0.874\n",
      "Iteration: 315 \t--- Loss: 0.816\n",
      "Iteration: 316 \t--- Loss: 0.890\n",
      "Iteration: 317 \t--- Loss: 0.895\n",
      "Iteration: 318 \t--- Loss: 0.882\n",
      "Iteration: 319 \t--- Loss: 0.903\n",
      "Iteration: 320 \t--- Loss: 0.930\n",
      "Iteration: 321 \t--- Loss: 0.893\n",
      "Iteration: 322 \t--- Loss: 0.880\n",
      "Iteration: 323 \t--- Loss: 0.852\n",
      "Iteration: 324 \t--- Loss: 0.830\n",
      "Iteration: 325 \t--- Loss: 0.844\n",
      "Iteration: 326 \t--- Loss: 0.893\n",
      "Iteration: 327 \t--- Loss: 0.856\n",
      "Iteration: 328 \t--- Loss: 0.956\n",
      "Iteration: 329 \t--- Loss: 0.925\n",
      "Iteration: 330 \t--- Loss: 0.862\n",
      "Iteration: 331 \t--- Loss: 0.869\n",
      "Iteration: 332 \t--- Loss: 0.885\n",
      "Iteration: 333 \t--- Loss: 0.915\n",
      "Iteration: 334 \t--- Loss: 0.839\n",
      "Iteration: 335 \t--- Loss: 0.867\n",
      "Iteration: 336 \t--- Loss: 0.882\n",
      "Iteration: 337 \t--- Loss: 0.879\n",
      "Iteration: 338 \t--- Loss: 0.956\n",
      "Iteration: 339 \t--- Loss: 0.866\n",
      "Iteration: 340 \t--- Loss: 0.875\n",
      "Iteration: 341 \t--- Loss: 0.823\n",
      "Iteration: 342 \t--- Loss: 0.835\n",
      "Iteration: 343 \t--- Loss: 0.922\n",
      "Iteration: 344 \t--- Loss: 0.917\n",
      "Iteration: 345 \t--- Loss: 0.928\n",
      "Iteration: 346 \t--- Loss: 0.953\n",
      "Iteration: 347 \t--- Loss: 0.839\n",
      "Iteration: 348 \t--- Loss: 0.852\n",
      "Iteration: 349 \t--- Loss: 0.861\n",
      "Iteration: 350 \t--- Loss: 0.835\n",
      "Iteration: 351 \t--- Loss: 0.873\n",
      "Iteration: 352 \t--- Loss: 0.880\n",
      "Iteration: 353 \t--- Loss: 0.919\n",
      "Iteration: 354 \t--- Loss: 0.871\n",
      "Iteration: 355 \t--- Loss: 0.940\n",
      "Iteration: 356 \t--- Loss: 0.937\n",
      "Iteration: 357 \t--- Loss: 0.829\n",
      "Iteration: 358 \t--- Loss: 0.949\n",
      "Iteration: 359 \t--- Loss: 0.878\n",
      "Iteration: 360 \t--- Loss: 0.931\n",
      "Iteration: 361 \t--- Loss: 0.885\n",
      "Iteration: 362 \t--- Loss: 0.898\n",
      "Iteration: 363 \t--- Loss: 0.919\n",
      "Iteration: 364 \t--- Loss: 0.943\n",
      "Iteration: 365 \t--- Loss: 0.888\n",
      "Iteration: 366 \t--- Loss: 0.846\n",
      "Iteration: 367 \t--- Loss: 0.847\n",
      "Iteration: 368 \t--- Loss: 0.929\n",
      "Iteration: 369 \t--- Loss: 0.866\n",
      "Iteration: 370 \t--- Loss: 0.889\n",
      "Iteration: 371 \t--- Loss: 0.872\n",
      "Iteration: 372 \t--- Loss: 0.914\n",
      "Iteration: 373 \t--- Loss: 0.883\n",
      "Iteration: 374 \t--- Loss: 0.939\n",
      "Iteration: 375 \t--- Loss: 0.887\n",
      "Iteration: 376 \t--- Loss: 0.867\n",
      "Iteration: 377 \t--- Loss: 0.843\n",
      "Iteration: 378 \t--- Loss: 0.860\n",
      "Iteration: 379 \t--- Loss: 0.868\n",
      "Iteration: 380 \t--- Loss: 0.886\n",
      "Iteration: 381 \t--- Loss: 0.852\n",
      "Iteration: 382 \t--- Loss: 0.851\n",
      "Iteration: 383 \t--- Loss: 0.878\n",
      "Iteration: 384 \t--- Loss: 0.875\n",
      "Iteration: 385 \t--- Loss: 0.823\n",
      "Iteration: 386 \t--- Loss: 0.945\n",
      "Iteration: 387 \t--- Loss: 0.859\n",
      "Iteration: 388 \t--- Loss: 0.889\n",
      "Iteration: 389 \t--- Loss: 0.879\n",
      "Iteration: 390 \t--- Loss: 0.876\n",
      "Iteration: 391 \t--- Loss: 0.832\n",
      "Iteration: 392 \t--- Loss: 0.896\n",
      "Iteration: 393 \t--- Loss: 0.888\n",
      "Iteration: 394 \t--- Loss: 0.893\n",
      "Iteration: 395 \t--- Loss: 0.888\n",
      "Iteration: 396 \t--- Loss: 0.898\n",
      "Iteration: 397 \t--- Loss: 0.909\n",
      "Iteration: 398 \t--- Loss: 0.873\n",
      "Iteration: 399 \t--- Loss: 0.838\n",
      "Iteration: 400 \t--- Loss: 0.891\n",
      "Iteration: 401 \t--- Loss: 0.912\n",
      "Iteration: 402 \t--- Loss: 0.946\n",
      "Iteration: 403 \t--- Loss: 0.883\n",
      "Iteration: 404 \t--- Loss: 0.899\n",
      "Iteration: 405 \t--- Loss: 0.932\n",
      "Iteration: 406 \t--- Loss: 0.894\n",
      "Iteration: 407 \t--- Loss: 0.856\n",
      "Iteration: 408 \t--- Loss: 0.888\n",
      "Iteration: 409 \t--- Loss: 0.928\n",
      "Iteration: 410 \t--- Loss: 0.911\n",
      "Iteration: 411 \t--- Loss: 0.897\n",
      "Iteration: 412 \t--- Loss: 0.840\n",
      "Iteration: 413 \t--- Loss: 0.857\n",
      "Iteration: 414 \t--- Loss: 0.893\n",
      "Iteration: 415 \t--- Loss: 0.868\n",
      "Iteration: 416 \t--- Loss: 0.887\n",
      "Iteration: 417 \t--- Loss: 0.870\n",
      "Iteration: 418 \t--- Loss: 0.875\n",
      "Iteration: 419 \t--- Loss: 0.882\n",
      "Iteration: 420 \t--- Loss: 0.883\n",
      "Iteration: 421 \t--- Loss: 0.895\n",
      "Iteration: 422 \t--- Loss: 0.915\n",
      "Iteration: 423 \t--- Loss: 0.893\n",
      "Iteration: 424 \t--- Loss: 0.881\n",
      "Iteration: 425 \t--- Loss: 0.844\n",
      "Iteration: 426 \t--- Loss: 0.873\n",
      "Iteration: 427 \t--- Loss: 0.938\n",
      "Iteration: 428 \t--- Loss: 0.906\n",
      "Iteration: 429 \t--- Loss: 0.895\n",
      "Iteration: 430 \t--- Loss: 0.863\n",
      "Iteration: 431 \t--- Loss: 0.876\n",
      "Iteration: 432 \t--- Loss: 0.870\n",
      "Iteration: 433 \t--- Loss: 0.951\n",
      "Iteration: 434 \t--- Loss: 0.851\n",
      "Iteration: 435 \t--- Loss: 0.873\n",
      "Iteration: 436 \t--- Loss: 0.925\n",
      "Iteration: 437 \t--- Loss: 0.885\n",
      "Iteration: 438 \t--- Loss: 0.891\n",
      "Iteration: 439 \t--- Loss: 0.856\n",
      "Iteration: 440 \t--- Loss: 0.865\n",
      "Iteration: 441 \t--- Loss: 0.876\n",
      "Iteration: 442 \t--- Loss: 0.831\n",
      "Iteration: 443 \t--- Loss: 0.903\n",
      "Iteration: 444 \t--- Loss: 0.904\n",
      "Iteration: 445 \t--- Loss: 0.884\n",
      "Iteration: 446 \t--- Loss: 0.897\n",
      "Iteration: 447 \t--- Loss: 0.889\n",
      "Iteration: 448 \t--- Loss: 0.897\n",
      "Iteration: 449 \t--- Loss: 0.872\n",
      "Iteration: 450 \t--- Loss: 0.863\n",
      "Iteration: 451 \t--- Loss: 0.908\n",
      "Iteration: 452 \t--- Loss: 0.890\n",
      "Iteration: 453 \t--- Loss: 0.855\n",
      "Iteration: 454 \t--- Loss: 0.921\n",
      "Iteration: 455 \t--- Loss: 0.887\n",
      "Iteration: 456 \t--- Loss: 0.914\n",
      "Iteration: 457 \t--- Loss: 0.875\n",
      "Iteration: 458 \t--- Loss: 0.937\n",
      "Iteration: 459 \t--- Loss: 0.878\n",
      "Iteration: 460 \t--- Loss: 0.884\n",
      "Iteration: 461 \t--- Loss: 0.881\n",
      "Iteration: 462 \t--- Loss: 0.858\n",
      "Iteration: 463 \t--- Loss: 0.832\n",
      "Iteration: 464 \t--- Loss: 0.891\n",
      "Iteration: 465 \t--- Loss: 0.902\n",
      "Iteration: 466 \t--- Loss: 0.859\n",
      "Iteration: 467 \t--- Loss: 0.931\n",
      "Iteration: 468 \t--- Loss: 0.867\n",
      "Iteration: 469 \t--- Loss: 0.896\n",
      "Iteration: 470 \t--- Loss: 0.923\n",
      "Iteration: 471 \t--- Loss: 0.910\n",
      "Iteration: 472 \t--- Loss: 0.873\n",
      "Iteration: 473 \t--- Loss: 0.833\n",
      "Iteration: 474 \t--- Loss: 0.840\n",
      "Iteration: 475 \t--- Loss: 0.890\n",
      "Iteration: 476 \t--- Loss: 0.879\n",
      "Iteration: 477 \t--- Loss: 0.846\n",
      "Iteration: 478 \t--- Loss: 0.923\n",
      "Iteration: 479 \t--- Loss: 0.867\n",
      "Iteration: 480 \t--- Loss: 0.903\n",
      "Iteration: 481 \t--- Loss: 0.877\n",
      "Iteration: 482 \t--- Loss: 0.906\n",
      "Iteration: 483 \t--- Loss: 0.924\n",
      "Iteration: 484 \t--- Loss: 0.884\n",
      "Iteration: 485 \t--- Loss: 0.815\n",
      "Iteration: 486 \t--- Loss: 0.873\n",
      "Iteration: 487 \t--- Loss: 0.919\n",
      "Iteration: 488 \t--- Loss: 0.930\n",
      "Iteration: 489 \t--- Loss: 0.900\n",
      "Iteration: 490 \t--- Loss: 0.859\n",
      "Iteration: 491 \t--- Loss: 0.952\n",
      "Iteration: 492 \t--- Loss: 0.854\n",
      "Iteration: 493 \t--- Loss: 0.876\n",
      "Iteration: 494 \t--- Loss: 0.902\n",
      "Iteration: 495 \t--- Loss: 0.866\n",
      "Iteration: 496 \t--- Loss: 0.865\n",
      "Iteration: 497 \t--- Loss: 0.889\n",
      "Iteration: 498 \t--- Loss: 0.866\n",
      "Iteration: 499 \t--- Loss: 0.889\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.17s/it][Parallel(n_jobs=5)]: Done   5 tasks      | elapsed:  2.2min\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.93s/it]]\n",
      " 40%|████      | 4/10 [00:04<00:06,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.379\n",
      "Iteration: 261 \t--- Loss: 0.381\n",
      "Iteration: 262 \t--- Loss: 0.353\n",
      "Iteration: 263 \t--- Loss: 0.426\n",
      "Iteration: 264 \t--- Loss: 0.389\n",
      "Iteration: 265 \t--- Loss: 0.383\n",
      "Iteration: 266 \t--- Loss: 0.383\n",
      "Iteration: 267 \t--- Loss: 0.432\n",
      "Iteration: 268 \t--- Loss: 0.353\n",
      "Iteration: 269 \t--- Loss: 0.393\n",
      "Iteration: 270 \t--- Loss: 0.413\n",
      "Iteration: 271 \t--- Loss: 0.374\n",
      "Iteration: 272 \t--- Loss: 0.381\n",
      "Iteration: 273 \t--- Loss: 0.416\n",
      "Iteration: 274 \t--- Loss: 0.417\n",
      "Iteration: 275 \t--- Loss: 0.414\n",
      "Iteration: 276 \t--- Loss: 0.355\n",
      "Iteration: 277 \t--- Loss: 0.387\n",
      "Iteration: 278 \t--- Loss: 0.382\n",
      "Iteration: 279 \t--- Loss: 0.368\n",
      "Iteration: 280 \t--- Loss: 0.389\n",
      "Iteration: 281 \t--- Loss: 0.423\n",
      "Iteration: 282 \t--- Loss: 0.393\n",
      "Iteration: 283 \t--- Loss: 0.361\n",
      "Iteration: 284 \t--- Loss: 0.368\n",
      "Iteration: 285 \t--- Loss: 0.401\n",
      "Iteration: 286 \t--- Loss: 0.403\n",
      "Iteration: 287 \t--- Loss: 0.400\n",
      "Iteration: 288 \t--- Loss: 0.416\n",
      "Iteration: 289 \t--- Loss: 0.383\n",
      "Iteration: 290 \t--- Loss: 0.385\n",
      "Iteration: 291 \t--- Loss: 0.450\n",
      "Iteration: 292 \t--- Loss: 0.438\n",
      "Iteration: 293 \t--- Loss: 0.440\n",
      "Iteration: 294 \t--- Loss: 0.401\n",
      "Iteration: 295 \t--- Loss: 0.398\n",
      "Iteration: 296 \t--- Loss: 0.395\n",
      "Iteration: 297 \t--- Loss: 0.419\n",
      "Iteration: 298 \t--- Loss: 0.346\n",
      "Iteration: 299 \t--- Loss: 0.388\n",
      "Iteration: 300 \t--- Loss: 0.365\n",
      "Iteration: 301 \t--- Loss: 0.376\n",
      "Iteration: 302 \t--- Loss: 0.420\n",
      "Iteration: 303 \t--- Loss: 0.441\n",
      "Iteration: 304 \t--- Loss: 0.402\n",
      "Iteration: 305 \t--- Loss: 0.371\n",
      "Iteration: 306 \t--- Loss: 0.370\n",
      "Iteration: 307 \t--- Loss: 0.387\n",
      "Iteration: 308 \t--- Loss: 0.363\n",
      "Iteration: 309 \t--- Loss: 0.397\n",
      "Iteration: 310 \t--- Loss: 0.406\n",
      "Iteration: 311 \t--- Loss: 0.416\n",
      "Iteration: 312 \t--- Loss: 0.408\n",
      "Iteration: 313 \t--- Loss: 0.411\n",
      "Iteration: 314 \t--- Loss: 0.376\n",
      "Iteration: 315 \t--- Loss: 0.399\n",
      "Iteration: 316 \t--- Loss: 0.402\n",
      "Iteration: 317 \t--- Loss: 0.319\n",
      "Iteration: 318 \t--- Loss: 0.411\n",
      "Iteration: 319 \t--- Loss: 0.434\n",
      "Iteration: 320 \t--- Loss: 0.401\n",
      "Iteration: 321 \t--- Loss: 0.363\n",
      "Iteration: 322 \t--- Loss: 0.380\n",
      "Iteration: 323 \t--- Loss: 0.433\n",
      "Iteration: 324 \t--- Loss: 0.394\n",
      "Iteration: 325 \t--- Loss: 0.408\n",
      "Iteration: 326 \t--- Loss: 0.381\n",
      "Iteration: 327 \t--- Loss: 0.400\n",
      "Iteration: 328 \t--- Loss: 0.434\n",
      "Iteration: 329 \t--- Loss: 0.400\n",
      "Iteration: 330 \t--- Loss: 0.392\n",
      "Iteration: 331 \t--- Loss: 0.411\n",
      "Iteration: 332 \t--- Loss: 0.384\n",
      "Iteration: 333 \t--- Loss: 0.386\n",
      "Iteration: 334 \t--- Loss: 0.361\n",
      "Iteration: 335 \t--- Loss: 0.354\n",
      "Iteration: 336 \t--- Loss: 0.398\n",
      "Iteration: 337 \t--- Loss: 0.373\n",
      "Iteration: 338 \t--- Loss: 0.379\n",
      "Iteration: 339 \t--- Loss: 0.389\n",
      "Iteration: 340 \t--- Loss: 0.386\n",
      "Iteration: 341 \t--- Loss: 0.418\n",
      "Iteration: 342 \t--- Loss: 0.399\n",
      "Iteration: 343 \t--- Loss: 0.448\n",
      "Iteration: 344 \t--- Loss: 0.331\n",
      "Iteration: 345 \t--- Loss: 0.412\n",
      "Iteration: 346 \t--- Loss: 0.360\n",
      "Iteration: 347 \t--- Loss: 0.446\n",
      "Iteration: 348 \t--- Loss: 0.383\n",
      "Iteration: 349 \t--- Loss: 0.423\n",
      "Iteration: 350 \t--- Loss: 0.398\n",
      "Iteration: 351 \t--- Loss: 0.421\n",
      "Iteration: 352 \t--- Loss: 0.397\n",
      "Iteration: 353 \t--- Loss: 0.401\n",
      "Iteration: 354 \t--- Loss: 0.356\n",
      "Iteration: 355 \t--- Loss: 0.387\n",
      "Iteration: 356 \t--- Loss: 0.399\n",
      "Iteration: 357 \t--- Loss: 0.369\n",
      "Iteration: 358 \t--- Loss: 0.400\n",
      "Iteration: 359 \t--- Loss: 0.390\n",
      "Iteration: 360 \t--- Loss: 0.431\n",
      "Iteration: 361 \t--- Loss: 0.431\n",
      "Iteration: 362 \t--- Loss: 0.346\n",
      "Iteration: 363 \t--- Loss: 0.408\n",
      "Iteration: 364 \t--- Loss: 0.351\n",
      "Iteration: 365 \t--- Loss: 0.379\n",
      "Iteration: 366 \t--- Loss: 0.399\n",
      "Iteration: 367 \t--- Loss: 0.400\n",
      "Iteration: 368 \t--- Loss: 0.391\n",
      "Iteration: 369 \t--- Loss: 0.362\n",
      "Iteration: 370 \t--- Loss: 0.390\n",
      "Iteration: 371 \t--- Loss: 0.398\n",
      "Iteration: 372 \t--- Loss: 0.389\n",
      "Iteration: 373 \t--- Loss: 0.433\n",
      "Iteration: 374 \t--- Loss: 0.392\n",
      "Iteration: 375 \t--- Loss: 0.365\n",
      "Iteration: 376 \t--- Loss: 0.351\n",
      "Iteration: 377 \t--- Loss: 0.379\n",
      "Iteration: 378 \t--- Loss: 0.428\n",
      "Iteration: 379 \t--- Loss: 0.408\n",
      "Iteration: 380 \t--- Loss: 0.411\n",
      "Iteration: 381 \t--- Loss: 0.414\n",
      "Iteration: 382 \t--- Loss: 0.422\n",
      "Iteration: 383 \t--- Loss: 0.449\n",
      "Iteration: 384 \t--- Loss: 0.402\n",
      "Iteration: 385 \t--- Loss: 0.394\n",
      "Iteration: 386 \t--- Loss: 0.378\n",
      "Iteration: 387 \t--- Loss: 0.368\n",
      "Iteration: 388 \t--- Loss: 0.419\n",
      "Iteration: 389 \t--- Loss: 0.381\n",
      "Iteration: 390 \t--- Loss: 0.397\n",
      "Iteration: 391 \t--- Loss: 0.448\n",
      "Iteration: 392 \t--- Loss: 0.395\n",
      "Iteration: 393 \t--- Loss: 0.397\n",
      "Iteration: 394 \t--- Loss: 0.381\n",
      "Iteration: 395 \t--- Loss: 0.378\n",
      "Iteration: 396 \t--- Loss: 0.371\n",
      "Iteration: 397 \t--- Loss: 0.363\n",
      "Iteration: 398 \t--- Loss: 0.394\n",
      "Iteration: 399 \t--- Loss: 0.336\n",
      "Iteration: 400 \t--- Loss: 0.424\n",
      "Iteration: 401 \t--- Loss: 0.433\n",
      "Iteration: 402 \t--- Loss: 0.423\n",
      "Iteration: 403 \t--- Loss: 0.370\n",
      "Iteration: 404 \t--- Loss: 0.426\n",
      "Iteration: 405 \t--- Loss: 0.411\n",
      "Iteration: 406 \t--- Loss: 0.463\n",
      "Iteration: 407 \t--- Loss: 0.356\n",
      "Iteration: 408 \t--- Loss: 0.376\n",
      "Iteration: 409 \t--- Loss: 0.396\n",
      "Iteration: 410 \t--- Loss: 0.368\n",
      "Iteration: 411 \t--- Loss: 0.443\n",
      "Iteration: 412 \t--- Loss: 0.408\n",
      "Iteration: 413 \t--- Loss: 0.388\n",
      "Iteration: 414 \t--- Loss: 0.393\n",
      "Iteration: 415 \t--- Loss: 0.380\n",
      "Iteration: 416 \t--- Loss: 0.402\n",
      "Iteration: 417 \t--- Loss: 0.360\n",
      "Iteration: 418 \t--- Loss: 0.377\n",
      "Iteration: 419 \t--- Loss: 0.360\n",
      "Iteration: 420 \t--- Loss: 0.379\n",
      "Iteration: 421 \t--- Loss: 0.409\n",
      "Iteration: 422 \t--- Loss: 0.392\n",
      "Iteration: 423 \t--- Loss: 0.382\n",
      "Iteration: 424 \t--- Loss: 0.417\n",
      "Iteration: 425 \t--- Loss: 0.403\n",
      "Iteration: 426 \t--- Loss: 0.362\n",
      "Iteration: 427 \t--- Loss: 0.395\n",
      "Iteration: 428 \t--- Loss: 0.372\n",
      "Iteration: 429 \t--- Loss: 0.359\n",
      "Iteration: 430 \t--- Loss: 0.415\n",
      "Iteration: 431 \t--- Loss: 0.398\n",
      "Iteration: 432 \t--- Loss: 0.397\n",
      "Iteration: 433 \t--- Loss: 0.393\n",
      "Iteration: 434 \t--- Loss: 0.402\n",
      "Iteration: 435 \t--- Loss: 0.402\n",
      "Iteration: 436 \t--- Loss: 0.381\n",
      "Iteration: 437 \t--- Loss: 0.360\n",
      "Iteration: 438 \t--- Loss: 0.402\n",
      "Iteration: 439 \t--- Loss: 0.395\n",
      "Iteration: 440 \t--- Loss: 0.385\n",
      "Iteration: 441 \t--- Loss: 0.382\n",
      "Iteration: 442 \t--- Loss: 0.381\n",
      "Iteration: 443 \t--- Loss: 0.332\n",
      "Iteration: 444 \t--- Loss: 0.403\n",
      "Iteration: 445 \t--- Loss: 0.356\n",
      "Iteration: 446 \t--- Loss: 0.404\n",
      "Iteration: 447 \t--- Loss: 0.402\n",
      "Iteration: 448 \t--- Loss: 0.430\n",
      "Iteration: 449 \t--- Loss: 0.406\n",
      "Iteration: 450 \t--- Loss: 0.376\n",
      "Iteration: 451 \t--- Loss: 0.386\n",
      "Iteration: 452 \t--- Loss: 0.414\n",
      "Iteration: 453 \t--- Loss: 0.361\n",
      "Iteration: 454 \t--- Loss: 0.382\n",
      "Iteration: 455 \t--- Loss: 0.385\n",
      "Iteration: 456 \t--- Loss: 0.380\n",
      "Iteration: 457 \t--- Loss: 0.426\n",
      "Iteration: 458 \t--- Loss: 0.395\n",
      "Iteration: 459 \t--- Loss: 0.376\n",
      "Iteration: 460 \t--- Loss: 0.404\n",
      "Iteration: 461 \t--- Loss: 0.371\n",
      "Iteration: 462 \t--- Loss: 0.384\n",
      "Iteration: 463 \t--- Loss: 0.386\n",
      "Iteration: 464 \t--- Loss: 0.370\n",
      "Iteration: 465 \t--- Loss: 0.417\n",
      "Iteration: 466 \t--- Loss: 0.439\n",
      "Iteration: 467 \t--- Loss: 0.406\n",
      "Iteration: 468 \t--- Loss: 0.448\n",
      "Iteration: 469 \t--- Loss: 0.410\n",
      "Iteration: 470 \t--- Loss: 0.429\n",
      "Iteration: 471 \t--- Loss: 0.401\n",
      "Iteration: 472 \t--- Loss: 0.372\n",
      "Iteration: 473 \t--- Loss: 0.381\n",
      "Iteration: 474 \t--- Loss: 0.392\n",
      "Iteration: 475 \t--- Loss: 0.396\n",
      "Iteration: 476 \t--- Loss: 0.371\n",
      "Iteration: 477 \t--- Loss: 0.402\n",
      "Iteration: 478 \t--- Loss: 0.432\n",
      "Iteration: 479 \t--- Loss: 0.379\n",
      "Iteration: 480 \t--- Loss: 0.375\n",
      "Iteration: 481 \t--- Loss: 0.389\n",
      "Iteration: 482 \t--- Loss: 0.399\n",
      "Iteration: 483 \t--- Loss: 0.369\n",
      "Iteration: 484 \t--- Loss: 0.411\n",
      "Iteration: 485 \t--- Loss: 0.367\n",
      "Iteration: 486 \t--- Loss: 0.391\n",
      "Iteration: 487 \t--- Loss: 0.369\n",
      "Iteration: 488 \t--- Loss: 0.374\n",
      "Iteration: 489 \t--- Loss: 0.362\n",
      "Iteration: 490 \t--- Loss: 0.417\n",
      "Iteration: 491 \t--- Loss: 0.381\n",
      "Iteration: 492 \t--- Loss: 0.374\n",
      "Iteration: 493 \t--- Loss: 0.392\n",
      "Iteration: 494 \t--- Loss: 0.413\n",
      "Iteration: 495 \t--- Loss: 0.388\n",
      "Iteration: 496 \t--- Loss: 0.375\n",
      "Iteration: 497 \t--- Loss: 0.407\n",
      "Iteration: 498 \t--- Loss: 0.403\n",
      "Iteration: 499 \t--- Loss: 0.394\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.19s/it][Parallel(n_jobs=5)]: Done   6 tasks      | elapsed:  2.2min\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.20s/it][Parallel(n_jobs=5)]: Done   7 tasks      | elapsed:  2.3min\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.15s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.17s/it][Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:  2.4min\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:22<00:00, 142.84s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.237\n",
      "Iteration: 261 \t--- Loss: 0.228\n",
      "Iteration: 262 \t--- Loss: 0.227\n",
      "Iteration: 263 \t--- Loss: 0.245\n",
      "Iteration: 264 \t--- Loss: 0.245\n",
      "Iteration: 265 \t--- Loss: 0.245\n",
      "Iteration: 266 \t--- Loss: 0.235\n",
      "Iteration: 267 \t--- Loss: 0.244\n",
      "Iteration: 268 \t--- Loss: 0.233\n",
      "Iteration: 269 \t--- Loss: 0.228\n",
      "Iteration: 270 \t--- Loss: 0.248\n",
      "Iteration: 271 \t--- Loss: 0.250\n",
      "Iteration: 272 \t--- Loss: 0.253\n",
      "Iteration: 273 \t--- Loss: 0.241\n",
      "Iteration: 274 \t--- Loss: 0.226\n",
      "Iteration: 275 \t--- Loss: 0.236\n",
      "Iteration: 276 \t--- Loss: 0.227\n",
      "Iteration: 277 \t--- Loss: 0.242\n",
      "Iteration: 278 \t--- Loss: 0.248\n",
      "Iteration: 279 \t--- Loss: 0.226\n",
      "Iteration: 280 \t--- Loss: 0.248\n",
      "Iteration: 281 \t--- Loss: 0.223\n",
      "Iteration: 282 \t--- Loss: 0.231\n",
      "Iteration: 283 \t--- Loss: 0.232\n",
      "Iteration: 284 \t--- Loss: 0.252\n",
      "Iteration: 285 \t--- Loss: 0.231\n",
      "Iteration: 286 \t--- Loss: 0.243\n",
      "Iteration: 287 \t--- Loss: 0.247\n",
      "Iteration: 288 \t--- Loss: 0.265\n",
      "Iteration: 289 \t--- Loss: 0.240\n",
      "Iteration: 290 \t--- Loss: 0.233\n",
      "Iteration: 291 \t--- Loss: 0.242\n",
      "Iteration: 292 \t--- Loss: 0.242\n",
      "Iteration: 293 \t--- Loss: 0.229\n",
      "Iteration: 294 \t--- Loss: 0.224\n",
      "Iteration: 295 \t--- Loss: 0.249\n",
      "Iteration: 296 \t--- Loss: 0.252\n",
      "Iteration: 297 \t--- Loss: 0.254\n",
      "Iteration: 298 \t--- Loss: 0.237\n",
      "Iteration: 299 \t--- Loss: 0.235\n",
      "Iteration: 300 \t--- Loss: 0.233\n",
      "Iteration: 301 \t--- Loss: 0.230\n",
      "Iteration: 302 \t--- Loss: 0.243\n",
      "Iteration: 303 \t--- Loss: 0.246\n",
      "Iteration: 304 \t--- Loss: 0.235\n",
      "Iteration: 305 \t--- Loss: 0.240\n",
      "Iteration: 306 \t--- Loss: 0.230\n",
      "Iteration: 307 \t--- Loss: 0.236\n",
      "Iteration: 308 \t--- Loss: 0.228\n",
      "Iteration: 309 \t--- Loss: 0.235\n",
      "Iteration: 310 \t--- Loss: 0.235\n",
      "Iteration: 311 \t--- Loss: 0.246\n",
      "Iteration: 312 \t--- Loss: 0.227\n",
      "Iteration: 313 \t--- Loss: 0.232\n",
      "Iteration: 314 \t--- Loss: 0.213\n",
      "Iteration: 315 \t--- Loss: 0.236\n",
      "Iteration: 316 \t--- Loss: 0.229\n",
      "Iteration: 317 \t--- Loss: 0.224\n",
      "Iteration: 318 \t--- Loss: 0.238\n",
      "Iteration: 319 \t--- Loss: 0.248\n",
      "Iteration: 320 \t--- Loss: 0.243\n",
      "Iteration: 321 \t--- Loss: 0.241\n",
      "Iteration: 322 \t--- Loss: 0.239\n",
      "Iteration: 323 \t--- Loss: 0.243\n",
      "Iteration: 324 \t--- Loss: 0.248\n",
      "Iteration: 325 \t--- Loss: 0.235\n",
      "Iteration: 326 \t--- Loss: 0.218\n",
      "Iteration: 327 \t--- Loss: 0.238\n",
      "Iteration: 328 \t--- Loss: 0.259\n",
      "Iteration: 329 \t--- Loss: 0.246\n",
      "Iteration: 330 \t--- Loss: 0.259\n",
      "Iteration: 331 \t--- Loss: 0.235\n",
      "Iteration: 332 \t--- Loss: 0.242\n",
      "Iteration: 333 \t--- Loss: 0.223\n",
      "Iteration: 334 \t--- Loss: 0.227\n",
      "Iteration: 335 \t--- Loss: 0.229\n",
      "Iteration: 336 \t--- Loss: 0.237\n",
      "Iteration: 337 \t--- Loss: 0.251\n",
      "Iteration: 338 \t--- Loss: 0.235\n",
      "Iteration: 339 \t--- Loss: 0.237\n",
      "Iteration: 340 \t--- Loss: 0.250\n",
      "Iteration: 341 \t--- Loss: 0.235\n",
      "Iteration: 342 \t--- Loss: 0.250\n",
      "Iteration: 343 \t--- Loss: 0.241\n",
      "Iteration: 344 \t--- Loss: 0.221\n",
      "Iteration: 345 \t--- Loss: 0.242\n",
      "Iteration: 346 \t--- Loss: 0.239\n",
      "Iteration: 347 \t--- Loss: 0.242\n",
      "Iteration: 348 \t--- Loss: 0.222\n",
      "Iteration: 349 \t--- Loss: 0.250\n",
      "Iteration: 350 \t--- Loss: 0.246\n",
      "Iteration: 351 \t--- Loss: 0.255\n",
      "Iteration: 352 \t--- Loss: 0.230\n",
      "Iteration: 353 \t--- Loss: 0.230\n",
      "Iteration: 354 \t--- Loss: 0.219\n",
      "Iteration: 355 \t--- Loss: 0.256\n",
      "Iteration: 356 \t--- Loss: 0.224\n",
      "Iteration: 357 \t--- Loss: 0.247\n",
      "Iteration: 358 \t--- Loss: 0.212\n",
      "Iteration: 359 \t--- Loss: 0.232\n",
      "Iteration: 360 \t--- Loss: 0.235\n",
      "Iteration: 361 \t--- Loss: 0.219\n",
      "Iteration: 362 \t--- Loss: 0.240\n",
      "Iteration: 363 \t--- Loss: 0.243\n",
      "Iteration: 364 \t--- Loss: 0.238\n",
      "Iteration: 365 \t--- Loss: 0.236\n",
      "Iteration: 366 \t--- Loss: 0.235\n",
      "Iteration: 367 \t--- Loss: 0.244\n",
      "Iteration: 368 \t--- Loss: 0.231\n",
      "Iteration: 369 \t--- Loss: 0.240\n",
      "Iteration: 370 \t--- Loss: 0.234\n",
      "Iteration: 371 \t--- Loss: 0.226\n",
      "Iteration: 372 \t--- Loss: 0.243\n",
      "Iteration: 373 \t--- Loss: 0.234\n",
      "Iteration: 374 \t--- Loss: 0.236\n",
      "Iteration: 375 \t--- Loss: 0.249\n",
      "Iteration: 376 \t--- Loss: 0.238\n",
      "Iteration: 377 \t--- Loss: 0.244\n",
      "Iteration: 378 \t--- Loss: 0.226\n",
      "Iteration: 379 \t--- Loss: 0.257\n",
      "Iteration: 380 \t--- Loss: 0.228\n",
      "Iteration: 381 \t--- Loss: 0.250\n",
      "Iteration: 382 \t--- Loss: 0.229\n",
      "Iteration: 383 \t--- Loss: 0.244\n",
      "Iteration: 384 \t--- Loss: 0.238\n",
      "Iteration: 385 \t--- Loss: 0.239\n",
      "Iteration: 386 \t--- Loss: 0.226\n",
      "Iteration: 387 \t--- Loss: 0.237\n",
      "Iteration: 388 \t--- Loss: 0.235\n",
      "Iteration: 389 \t--- Loss: 0.241\n",
      "Iteration: 390 \t--- Loss: 0.250\n",
      "Iteration: 391 \t--- Loss: 0.238\n",
      "Iteration: 392 \t--- Loss: 0.244\n",
      "Iteration: 393 \t--- Loss: 0.232\n",
      "Iteration: 394 \t--- Loss: 0.230\n",
      "Iteration: 395 \t--- Loss: 0.222\n",
      "Iteration: 396 \t--- Loss: 0.250\n",
      "Iteration: 397 \t--- Loss: 0.250\n",
      "Iteration: 398 \t--- Loss: 0.250\n",
      "Iteration: 399 \t--- Loss: 0.238\n",
      "Iteration: 400 \t--- Loss: 0.225\n",
      "Iteration: 401 \t--- Loss: 0.230\n",
      "Iteration: 402 \t--- Loss: 0.233\n",
      "Iteration: 403 \t--- Loss: 0.246\n",
      "Iteration: 404 \t--- Loss: 0.241\n",
      "Iteration: 405 \t--- Loss: 0.250\n",
      "Iteration: 406 \t--- Loss: 0.247\n",
      "Iteration: 407 \t--- Loss: 0.248\n",
      "Iteration: 408 \t--- Loss: 0.261\n",
      "Iteration: 409 \t--- Loss: 0.244\n",
      "Iteration: 410 \t--- Loss: 0.245\n",
      "Iteration: 411 \t--- Loss: 0.235\n",
      "Iteration: 412 \t--- Loss: 0.232\n",
      "Iteration: 413 \t--- Loss: 0.231\n",
      "Iteration: 414 \t--- Loss: 0.242\n",
      "Iteration: 415 \t--- Loss: 0.247\n",
      "Iteration: 416 \t--- Loss: 0.245\n",
      "Iteration: 417 \t--- Loss: 0.232\n",
      "Iteration: 418 \t--- Loss: 0.226\n",
      "Iteration: 419 \t--- Loss: 0.247\n",
      "Iteration: 420 \t--- Loss: 0.237\n",
      "Iteration: 421 \t--- Loss: 0.253\n",
      "Iteration: 422 \t--- Loss: 0.249\n",
      "Iteration: 423 \t--- Loss: 0.249\n",
      "Iteration: 424 \t--- Loss: 0.243\n",
      "Iteration: 425 \t--- Loss: 0.215\n",
      "Iteration: 426 \t--- Loss: 0.242\n",
      "Iteration: 427 \t--- Loss: 0.253\n",
      "Iteration: 428 \t--- Loss: 0.245\n",
      "Iteration: 429 \t--- Loss: 0.233\n",
      "Iteration: 430 \t--- Loss: 0.238\n",
      "Iteration: 431 \t--- Loss: 0.221\n",
      "Iteration: 432 \t--- Loss: 0.237\n",
      "Iteration: 433 \t--- Loss: 0.231\n",
      "Iteration: 434 \t--- Loss: 0.243\n",
      "Iteration: 435 \t--- Loss: 0.225\n",
      "Iteration: 436 \t--- Loss: 0.234\n",
      "Iteration: 437 \t--- Loss: 0.249\n",
      "Iteration: 438 \t--- Loss: 0.232\n",
      "Iteration: 439 \t--- Loss: 0.249\n",
      "Iteration: 440 \t--- Loss: 0.214\n",
      "Iteration: 441 \t--- Loss: 0.255\n",
      "Iteration: 442 \t--- Loss: 0.223\n",
      "Iteration: 443 \t--- Loss: 0.238\n",
      "Iteration: 444 \t--- Loss: 0.230\n",
      "Iteration: 445 \t--- Loss: 0.226\n",
      "Iteration: 446 \t--- Loss: 0.222\n",
      "Iteration: 447 \t--- Loss: 0.261\n",
      "Iteration: 448 \t--- Loss: 0.252\n",
      "Iteration: 449 \t--- Loss: 0.215\n",
      "Iteration: 450 \t--- Loss: 0.236\n",
      "Iteration: 451 \t--- Loss: 0.222\n",
      "Iteration: 452 \t--- Loss: 0.231\n",
      "Iteration: 453 \t--- Loss: 0.244\n",
      "Iteration: 454 \t--- Loss: 0.221\n",
      "Iteration: 455 \t--- Loss: 0.233\n",
      "Iteration: 456 \t--- Loss: 0.253\n",
      "Iteration: 457 \t--- Loss: 0.222\n",
      "Iteration: 458 \t--- Loss: 0.226\n",
      "Iteration: 459 \t--- Loss: 0.238\n",
      "Iteration: 460 \t--- Loss: 0.238\n",
      "Iteration: 461 \t--- Loss: 0.231\n",
      "Iteration: 462 \t--- Loss: 0.251\n",
      "Iteration: 463 \t--- Loss: 0.232\n",
      "Iteration: 464 \t--- Loss: 0.244\n",
      "Iteration: 465 \t--- Loss: 0.239\n",
      "Iteration: 466 \t--- Loss: 0.265\n",
      "Iteration: 467 \t--- Loss: 0.229\n",
      "Iteration: 468 \t--- Loss: 0.235\n",
      "Iteration: 469 \t--- Loss: 0.257\n",
      "Iteration: 470 \t--- Loss: 0.253\n",
      "Iteration: 471 \t--- Loss: 0.238\n",
      "Iteration: 472 \t--- Loss: 0.222\n",
      "Iteration: 473 \t--- Loss: 0.232\n",
      "Iteration: 474 \t--- Loss: 0.211\n",
      "Iteration: 475 \t--- Loss: 0.225\n",
      "Iteration: 476 \t--- Loss: 0.251\n",
      "Iteration: 477 \t--- Loss: 0.252\n",
      "Iteration: 478 \t--- Loss: 0.231\n",
      "Iteration: 479 \t--- Loss: 0.250\n",
      "Iteration: 480 \t--- Loss: 0.229\n",
      "Iteration: 481 \t--- Loss: 0.251\n",
      "Iteration: 482 \t--- Loss: 0.251\n",
      "Iteration: 483 \t--- Loss: 0.232\n",
      "Iteration: 484 \t--- Loss: 0.223\n",
      "Iteration: 485 \t--- Loss: 0.236\n",
      "Iteration: 486 \t--- Loss: 0.246\n",
      "Iteration: 487 \t--- Loss: 0.230\n",
      "Iteration: 488 \t--- Loss: 0.251\n",
      "Iteration: 489 \t--- Loss: 0.242\n",
      "Iteration: 490 \t--- Loss: 0.236\n",
      "Iteration: 491 \t--- Loss: 0.240\n",
      "Iteration: 492 \t--- Loss: 0.242\n",
      "Iteration: 493 \t--- Loss: 0.226\n",
      "Iteration: 494 \t--- Loss: 0.236\n",
      "Iteration: 495 \t--- Loss: 0.235\n",
      "Iteration: 496 \t--- Loss: 0.238\n",
      "Iteration: 497 \t--- Loss: 0.257\n",
      "Iteration: 498 \t--- Loss: 0.244\n",
      "Iteration: 499 \t--- Loss: 0.237\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:10<00:01,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.589\n",
      "Iteration: 1 \t--- Loss: 0.510\n",
      "Iteration: 2 \t--- Loss: 0.514\n",
      "Iteration: 3 \t--- Loss: 0.452\n",
      "Iteration: 4 \t--- Loss: 0.480\n",
      "Iteration: 5 \t--- Loss: 0.384\n",
      "Iteration: 6 \t--- Loss: 0.364\n",
      "Iteration: 7 \t--- Loss: 0.369\n",
      "Iteration: 8 \t--- Loss: 0.385\n",
      "Iteration: 9 \t--- Loss: 0.417\n",
      "Iteration: 10 \t--- Loss: 0.355\n",
      "Iteration: 11 \t--- Loss: 0.338\n",
      "Iteration: 12 \t--- Loss: 0.334\n",
      "Iteration: 13 \t--- Loss: 0.300\n",
      "Iteration: 14 \t--- Loss: 0.331\n",
      "Iteration: 15 \t--- Loss: 0.284\n",
      "Iteration: 16 \t--- Loss: 0.322\n",
      "Iteration: 17 \t--- Loss: 0.265\n",
      "Iteration: 18 \t--- Loss: 0.295\n",
      "Iteration: 19 \t--- Loss: 0.312\n",
      "Iteration: 20 \t--- Loss: 0.306\n",
      "Iteration: 21 \t--- Loss: 0.303\n",
      "Iteration: 22 \t--- Loss: 0.312\n",
      "Iteration: 23 \t--- Loss: 0.314\n",
      "Iteration: 24 \t--- Loss: 0.322\n",
      "Iteration: 25 \t--- Loss: 0.327\n",
      "Iteration: 26 \t--- Loss: 0.299\n",
      "Iteration: 27 \t--- Loss: 0.280\n",
      "Iteration: 28 \t--- Loss: 0.310\n",
      "Iteration: 29 \t--- Loss: 0.296\n",
      "Iteration: 30 \t--- Loss: 0.280\n",
      "Iteration: 31 \t--- Loss: 0.305\n",
      "Iteration: 32 \t--- Loss: 0.319\n",
      "Iteration: 33 \t--- Loss: 0.298\n",
      "Iteration: 34 \t--- Loss: 0.279\n",
      "Iteration: 35 \t--- Loss: 0.273\n",
      "Iteration: 36 \t--- Loss: 0.285\n",
      "Iteration: 37 \t--- Loss: 0.299\n",
      "Iteration: 38 \t--- Loss: 0.301\n",
      "Iteration: 39 \t--- Loss: 0.286\n",
      "Iteration: 40 \t--- Loss: 0.268\n",
      "Iteration: 41 \t--- Loss: 0.293\n",
      "Iteration: 42 \t--- Loss: 0.316\n",
      "Iteration: 43 \t--- Loss: 0.330\n",
      "Iteration: 44 \t--- Loss: 0.291\n",
      "Iteration: 45 \t--- Loss: 0.268\n",
      "Iteration: 46 \t--- Loss: 0.291\n",
      "Iteration: 47 \t--- Loss: 0.273\n",
      "Iteration: 48 \t--- Loss: 0.304\n",
      "Iteration: 49 \t--- Loss: 0.306\n",
      "Iteration: 50 \t--- Loss: 0.273\n",
      "Iteration: 51 \t--- Loss: 0.276\n",
      "Iteration: 52 \t--- Loss: 0.274\n",
      "Iteration: 53 \t--- Loss: 0.284\n",
      "Iteration: 54 \t--- Loss: 0.261\n",
      "Iteration: 55 \t--- Loss: 0.295\n",
      "Iteration: 56 \t--- Loss: 0.300\n",
      "Iteration: 57 \t--- Loss: 0.268\n",
      "Iteration: 58 \t--- Loss: 0.268\n",
      "Iteration: 59 \t--- Loss: 0.290\n",
      "Iteration: 60 \t--- Loss: 0.256\n",
      "Iteration: 61 \t--- Loss: 0.272\n",
      "Iteration: 62 \t--- Loss: 0.269\n",
      "Iteration: 63 \t--- Loss: 0.302\n",
      "Iteration: 64 \t--- Loss: 0.280\n",
      "Iteration: 65 \t--- Loss: 0.308\n",
      "Iteration: 66 \t--- Loss: 0.285\n",
      "Iteration: 67 \t--- Loss: 0.279\n",
      "Iteration: 68 \t--- Loss: 0.278\n",
      "Iteration: 69 \t--- Loss: 0.283\n",
      "Iteration: 70 \t--- Loss: 0.287\n",
      "Iteration: 71 \t--- Loss: 0.304\n",
      "Iteration: 72 \t--- Loss: 0.266\n",
      "Iteration: 73 \t--- Loss: 0.281\n",
      "Iteration: 74 \t--- Loss: 0.289\n",
      "Iteration: 75 \t--- Loss: 0.270\n",
      "Iteration: 76 \t--- Loss: 0.280\n",
      "Iteration: 77 \t--- Loss: 0.297\n",
      "Iteration: 78 \t--- Loss: 0.283\n",
      "Iteration: 79 \t--- Loss: 0.280\n",
      "Iteration: 80 \t--- Loss: 0.278\n",
      "Iteration: 81 \t--- Loss: 0.286\n",
      "Iteration: 82 \t--- Loss: 0.279\n",
      "Iteration: 83 \t--- Loss: 0.305\n",
      "Iteration: 84 \t--- Loss: 0.281\n",
      "Iteration: 85 \t--- Loss: 0.302\n",
      "Iteration: 86 \t--- Loss: 0.277\n",
      "Iteration: 87 \t--- Loss: 0.312\n",
      "Iteration: 88 \t--- Loss: 0.312\n",
      "Iteration: 89 \t--- Loss: 0.281\n",
      "Iteration: 90 \t--- Loss: 0.275\n",
      "Iteration: 91 \t--- Loss: 0.298\n",
      "Iteration: 92 \t--- Loss: 0.294\n",
      "Iteration: 93 \t--- Loss: 0.291\n",
      "Iteration: 94 \t--- Loss: 0.306\n",
      "Iteration: 95 \t--- Loss: 0.273\n",
      "Iteration: 96 \t--- Loss: 0.282\n",
      "Iteration: 97 \t--- Loss: 0.296\n",
      "Iteration: 98 \t--- Loss: 0.288\n",
      "Iteration: 99 \t--- Loss: 0.284\n",
      "Iteration: 100 \t--- Loss: 0.295\n",
      "Iteration: 101 \t--- Loss: 0.266\n",
      "Iteration: 102 \t--- Loss: 0.293\n",
      "Iteration: 103 \t--- Loss: 0.292\n",
      "Iteration: 104 \t--- Loss: 0.304\n",
      "Iteration: 105 \t--- Loss: 0.281\n",
      "Iteration: 106 \t--- Loss: 0.269\n",
      "Iteration: 107 \t--- Loss: 0.294\n",
      "Iteration: 108 \t--- Loss: 0.288\n",
      "Iteration: 109 \t--- Loss: 0.279\n",
      "Iteration: 110 \t--- Loss: 0.270\n",
      "Iteration: 111 \t--- Loss: 0.289\n",
      "Iteration: 112 \t--- Loss: 0.276\n",
      "Iteration: 113 \t--- Loss: 0.283\n",
      "Iteration: 114 \t--- Loss: 0.314\n",
      "Iteration: 115 \t--- Loss: 0.275\n",
      "Iteration: 116 \t--- Loss: 0.282\n",
      "Iteration: 117 \t--- Loss: 0.310\n",
      "Iteration: 118 \t--- Loss: 0.278\n",
      "Iteration: 119 \t--- Loss: 0.283\n",
      "Iteration: 120 \t--- Loss: 0.266\n",
      "Iteration: 121 \t--- Loss: 0.280\n",
      "Iteration: 122 \t--- Loss: 0.295\n",
      "Iteration: 123 \t--- Loss: 0.295\n",
      "Iteration: 124 \t--- Loss: 0.249\n",
      "Iteration: 125 \t--- Loss: 0.303\n",
      "Iteration: 126 \t--- Loss: 0.274\n",
      "Iteration: 127 \t--- Loss: 0.270\n",
      "Iteration: 128 \t--- Loss: 0.258\n",
      "Iteration: 129 \t--- Loss: 0.304\n",
      "Iteration: 130 \t--- Loss: 0.283\n",
      "Iteration: 131 \t--- Loss: 0.296\n",
      "Iteration: 132 \t--- Loss: 0.290\n",
      "Iteration: 133 \t--- Loss: 0.285\n",
      "Iteration: 134 \t--- Loss: 0.256\n",
      "Iteration: 135 \t--- Loss: 0.291\n",
      "Iteration: 136 \t--- Loss: 0.266\n",
      "Iteration: 137 \t--- Loss: 0.272\n",
      "Iteration: 138 \t--- Loss: 0.299\n",
      "Iteration: 139 \t--- Loss: 0.299\n",
      "Iteration: 140 \t--- Loss: 0.252\n",
      "Iteration: 141 \t--- Loss: 0.253\n",
      "Iteration: 142 \t--- Loss: 0.265\n",
      "Iteration: 143 \t--- Loss: 0.265\n",
      "Iteration: 144 \t--- Loss: 0.288\n",
      "Iteration: 145 \t--- Loss: 0.290\n",
      "Iteration: 146 \t--- Loss: 0.299\n",
      "Iteration: 147 \t--- Loss: 0.253\n",
      "Iteration: 148 \t--- Loss: 0.295\n",
      "Iteration: 149 \t--- Loss: 0.297\n",
      "Iteration: 150 \t--- Loss: 0.263\n",
      "Iteration: 151 \t--- Loss: 0.281\n",
      "Iteration: 152 \t--- Loss: 0.321\n",
      "Iteration: 153 \t--- Loss: 0.301\n",
      "Iteration: 154 \t--- Loss: 0.289\n",
      "Iteration: 155 \t--- Loss: 0.281\n",
      "Iteration: 156 \t--- Loss: 0.299\n",
      "Iteration: 157 \t--- Loss: 0.277\n",
      "Iteration: 158 \t--- Loss: 0.259\n",
      "Iteration: 159 \t--- Loss: 0.322\n",
      "Iteration: 160 \t--- Loss: 0.279\n",
      "Iteration: 161 \t--- Loss: 0.291\n",
      "Iteration: 162 \t--- Loss: 0.309\n",
      "Iteration: 163 \t--- Loss: 0.269\n",
      "Iteration: 164 \t--- Loss: 0.258\n",
      "Iteration: 165 \t--- Loss: 0.278\n",
      "Iteration: 166 \t--- Loss: 0.277\n",
      "Iteration: 167 \t--- Loss: 0.284\n",
      "Iteration: 168 \t--- Loss: 0.258\n",
      "Iteration: 169 \t--- Loss: 0.301\n",
      "Iteration: 170 \t--- Loss: 0.304\n",
      "Iteration: 171 \t--- Loss: 0.269\n",
      "Iteration: 172 \t--- Loss: 0.275\n",
      "Iteration: 173 \t--- Loss: 0.273\n",
      "Iteration: 174 \t--- Loss: 0.298\n",
      "Iteration: 175 \t--- Loss: 0.290\n",
      "Iteration: 176 \t--- Loss: 0.284\n",
      "Iteration: 177 \t--- Loss: 0.284\n",
      "Iteration: 178 \t--- Loss: 0.292\n",
      "Iteration: 179 \t--- Loss: 0.307\n",
      "Iteration: 180 \t--- Loss: 0.269\n",
      "Iteration: 181 \t--- Loss: 0.293\n",
      "Iteration: 182 \t--- Loss: 0.290\n",
      "Iteration: 183 \t--- Loss: 0.284\n",
      "Iteration: 184 \t--- Loss: 0.237\n",
      "Iteration: 185 \t--- Loss: 0.276\n",
      "Iteration: 186 \t--- Loss: 0.286\n",
      "Iteration: 187 \t--- Loss: 0.296\n",
      "Iteration: 188 \t--- Loss: 0.261\n",
      "Iteration: 189 \t--- Loss: 0.301\n",
      "Iteration: 190 \t--- Loss: 0.285\n",
      "Iteration: 191 \t--- Loss: 0.279\n",
      "Iteration: 192 \t--- Loss: 0.279\n",
      "Iteration: 193 \t--- Loss: 0.308\n",
      "Iteration: 194 \t--- Loss: 0.287\n",
      "Iteration: 195 \t--- Loss: 0.278\n",
      "Iteration: 196 \t--- Loss: 0.258\n",
      "Iteration: 197 \t--- Loss: 0.270\n",
      "Iteration: 198 \t--- Loss: 0.285\n",
      "Iteration: 199 \t--- Loss: 0.259\n",
      "Iteration: 200 \t--- Loss: 0.275\n",
      "Iteration: 201 \t--- Loss: 0.310\n",
      "Iteration: 202 \t--- Loss: 0.301\n",
      "Iteration: 203 \t--- Loss: 0.273\n",
      "Iteration: 204 \t--- Loss: 0.273\n",
      "Iteration: 205 \t--- Loss: 0.285\n",
      "Iteration: 206 \t--- Loss: 0.290\n",
      "Iteration: 207 \t--- Loss: 0.281\n",
      "Iteration: 208 \t--- Loss: 0.267\n",
      "Iteration: 209 \t--- Loss: 0.261\n",
      "Iteration: 210 \t--- Loss: 0.273\n",
      "Iteration: 211 \t--- Loss: 0.304\n",
      "Iteration: 212 \t--- Loss: 0.265\n",
      "Iteration: 213 \t--- Loss: 0.284\n",
      "Iteration: 214 \t--- Loss: 0.326\n",
      "Iteration: 215 \t--- Loss: 0.252\n",
      "Iteration: 216 \t--- Loss: 0.282\n",
      "Iteration: 217 \t--- Loss: 0.288\n",
      "Iteration: 218 \t--- Loss: 0.288\n",
      "Iteration: 219 \t--- Loss: 0.286\n",
      "Iteration: 220 \t--- Loss: 0.266\n",
      "Iteration: 221 \t--- Loss: 0.287\n",
      "Iteration: 222 \t--- Loss: 0.261\n",
      "Iteration: 223 \t--- Loss: 0.285\n",
      "Iteration: 224 \t--- Loss: 0.296\n",
      "Iteration: 225 \t--- Loss: 0.307\n",
      "Iteration: 226 \t--- Loss: 0.282\n",
      "Iteration: 227 \t--- Loss: 0.269\n",
      "Iteration: 228 \t--- Loss: 0.294\n",
      "Iteration: 229 \t--- Loss: 0.264\n",
      "Iteration: 230 \t--- Loss: 0.279\n",
      "Iteration: 231 \t--- Loss: 0.293\n",
      "Iteration: 232 \t--- Loss: 0.315\n",
      "Iteration: 233 \t--- Loss: 0.278\n",
      "Iteration: 234 \t--- Loss: 0.285\n",
      "Iteration: 235 \t--- Loss: 0.299\n",
      "Iteration: 236 \t--- Loss: 0.276\n",
      "Iteration: 237 \t--- Loss: 0.300\n",
      "Iteration: 238 \t--- Loss: 0.320\n",
      "Iteration: 239 \t--- Loss: 0.299\n",
      "Iteration: 240 \t--- Loss: 0.269\n",
      "Iteration: 241 \t--- Loss: 0.279\n",
      "Iteration: 242 \t--- Loss: 0.287\n",
      "Iteration: 243 \t--- Loss: 0.272\n",
      "Iteration: 244 \t--- Loss: 0.311\n",
      "Iteration: 245 \t--- Loss: 0.289\n",
      "Iteration: 246 \t--- Loss: 0.270\n",
      "Iteration: 247 \t--- Loss: 0.284\n",
      "Iteration: 248 \t--- Loss: 0.283\n",
      "Iteration: 249 \t--- Loss: 0.275\n",
      "Iteration: 250 \t--- Loss: 0.322\n",
      "Iteration: 251 \t--- Loss: 0.263\n",
      "Iteration: 252 \t--- Loss: 0.297\n",
      "Iteration: 253 \t--- Loss: 0.297\n",
      "Iteration: 254 \t--- Loss: 0.292\n",
      "Iteration: 255 \t--- Loss: 0.286\n",
      "Iteration: 256 \t--- Loss: 0.302\n",
      "Iteration: 257 \t--- Loss: 0.268\n",
      "Iteration: 258 \t--- Loss: 0.272\n",
      "Iteration: 259 \t--- Loss: 0.278"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.16s/it][Parallel(n_jobs=5)]: Done   9 tasks      | elapsed:  2.7min\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.16s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 1.905\n",
      "Iteration: 1 \t--- Loss: 1.865\n",
      "Iteration: 2 \t--- Loss: 1.654\n",
      "Iteration: 3 \t--- Loss: 1.483\n",
      "Iteration: 4 \t--- Loss: 1.554\n",
      "Iteration: 5 \t--- Loss: 1.417\n",
      "Iteration: 6 \t--- Loss: 1.386\n",
      "Iteration: 7 \t--- Loss: 1.395\n",
      "Iteration: 8 \t--- Loss: 1.389\n",
      "Iteration: 9 \t--- Loss: 1.380\n",
      "Iteration: 10 \t--- Loss: 1.418\n",
      "Iteration: 11 \t--- Loss: 1.337\n",
      "Iteration: 12 \t--- Loss: 1.385\n",
      "Iteration: 13 \t--- Loss: 1.393\n",
      "Iteration: 14 \t--- Loss: 1.356\n",
      "Iteration: 15 \t--- Loss: 1.311\n",
      "Iteration: 16 \t--- Loss: 1.302\n",
      "Iteration: 17 \t--- Loss: 1.350\n",
      "Iteration: 18 \t--- Loss: 1.335\n",
      "Iteration: 19 \t--- Loss: 1.329\n",
      "Iteration: 20 \t--- Loss: 1.358\n",
      "Iteration: 21 \t--- Loss: 1.289\n",
      "Iteration: 22 \t--- Loss: 1.333\n",
      "Iteration: 23 \t--- Loss: 1.308\n",
      "Iteration: 24 \t--- Loss: 1.331\n",
      "Iteration: 25 \t--- Loss: 1.378\n",
      "Iteration: 26 \t--- Loss: 1.306\n",
      "Iteration: 27 \t--- Loss: 1.331\n",
      "Iteration: 28 \t--- Loss: 1.345\n",
      "Iteration: 29 \t--- Loss: 1.284\n",
      "Iteration: 30 \t--- Loss: 1.222\n",
      "Iteration: 31 \t--- Loss: 1.284\n",
      "Iteration: 32 \t--- Loss: 1.205\n",
      "Iteration: 33 \t--- Loss: 1.281\n",
      "Iteration: 34 \t--- Loss: 1.282\n",
      "Iteration: 35 \t--- Loss: 1.287\n",
      "Iteration: 36 \t--- Loss: 1.276\n",
      "Iteration: 37 \t--- Loss: 1.350\n",
      "Iteration: 38 \t--- Loss: 1.364\n",
      "Iteration: 39 \t--- Loss: 1.273\n",
      "Iteration: 40 \t--- Loss: 1.246\n",
      "Iteration: 41 \t--- Loss: 1.340\n",
      "Iteration: 42 \t--- Loss: 1.270\n",
      "Iteration: 43 \t--- Loss: 1.220\n",
      "Iteration: 44 \t--- Loss: 1.377\n",
      "Iteration: 45 \t--- Loss: 1.287\n",
      "Iteration: 46 \t--- Loss: 1.347\n",
      "Iteration: 47 \t--- Loss: 1.272\n",
      "Iteration: 48 \t--- Loss: 1.302\n",
      "Iteration: 49 \t--- Loss: 1.289\n",
      "Iteration: 50 \t--- Loss: 1.316\n",
      "Iteration: 51 \t--- Loss: 1.288\n",
      "Iteration: 52 \t--- Loss: 1.318\n",
      "Iteration: 53 \t--- Loss: 1.283\n",
      "Iteration: 54 \t--- Loss: 1.320\n",
      "Iteration: 55 \t--- Loss: 1.267\n",
      "Iteration: 56 \t--- Loss: 1.280\n",
      "Iteration: 57 \t--- Loss: 1.290\n",
      "Iteration: 58 \t--- Loss: 1.283\n",
      "Iteration: 59 \t--- Loss: 1.298\n",
      "Iteration: 60 \t--- Loss: 1.342\n",
      "Iteration: 61 \t--- Loss: 1.284\n",
      "Iteration: 62 \t--- Loss: 1.286\n",
      "Iteration: 63 \t--- Loss: 1.201\n",
      "Iteration: 64 \t--- Loss: 1.340\n",
      "Iteration: 65 \t--- Loss: 1.444\n",
      "Iteration: 66 \t--- Loss: 1.343\n",
      "Iteration: 67 \t--- Loss: 1.268\n",
      "Iteration: 68 \t--- Loss: 1.307\n",
      "Iteration: 69 \t--- Loss: 1.333\n",
      "Iteration: 70 \t--- Loss: 1.274\n",
      "Iteration: 71 \t--- Loss: 1.309\n",
      "Iteration: 72 \t--- Loss: 1.329\n",
      "Iteration: 73 \t--- Loss: 1.308\n",
      "Iteration: 74 \t--- Loss: 1.299\n",
      "Iteration: 75 \t--- Loss: 1.292\n",
      "Iteration: 76 \t--- Loss: 1.352\n",
      "Iteration: 77 \t--- Loss: 1.315\n",
      "Iteration: 78 \t--- Loss: 1.322\n",
      "Iteration: 79 \t--- Loss: 1.215\n",
      "Iteration: 80 \t--- Loss: 1.221\n",
      "Iteration: 81 \t--- Loss: 1.221\n",
      "Iteration: 82 \t--- Loss: 1.404\n",
      "Iteration: 83 \t--- Loss: 1.248\n",
      "Iteration: 84 \t--- Loss: 1.298\n",
      "Iteration: 85 \t--- Loss: 1.311\n",
      "Iteration: 86 \t--- Loss: 1.349\n",
      "Iteration: 87 \t--- Loss: 1.264\n",
      "Iteration: 88 \t--- Loss: 1.240\n",
      "Iteration: 89 \t--- Loss: 1.337\n",
      "Iteration: 90 \t--- Loss: 1.257\n",
      "Iteration: 91 \t--- Loss: 1.287\n",
      "Iteration: 92 \t--- Loss: 1.361\n",
      "Iteration: 93 \t--- Loss: 1.319\n",
      "Iteration: 94 \t--- Loss: 1.300\n",
      "Iteration: 95 \t--- Loss: 1.243\n",
      "Iteration: 96 \t--- Loss: 1.314\n",
      "Iteration: 97 \t--- Loss: 1.379\n",
      "Iteration: 98 \t--- Loss: 1.369\n",
      "Iteration: 99 \t--- Loss: 1.333\n",
      "Iteration: 100 \t--- Loss: 1.213\n",
      "Iteration: 101 \t--- Loss: 1.380\n",
      "Iteration: 102 \t--- Loss: 1.301\n",
      "Iteration: 103 \t--- Loss: 1.318\n",
      "Iteration: 104 \t--- Loss: 1.336\n",
      "Iteration: 105 \t--- Loss: 1.356\n",
      "Iteration: 106 \t--- Loss: 1.343\n",
      "Iteration: 107 \t--- Loss: 1.321\n",
      "Iteration: 108 \t--- Loss: 1.206\n",
      "Iteration: 109 \t--- Loss: 1.294\n",
      "Iteration: 110 \t--- Loss: 1.329\n",
      "Iteration: 111 \t--- Loss: 1.267\n",
      "Iteration: 112 \t--- Loss: 1.356\n",
      "Iteration: 113 \t--- Loss: 1.317\n",
      "Iteration: 114 \t--- Loss: 1.320\n",
      "Iteration: 115 \t--- Loss: 1.278\n",
      "Iteration: 116 \t--- Loss: 1.297\n",
      "Iteration: 117 \t--- Loss: 1.376\n",
      "Iteration: 118 \t--- Loss: 1.225\n",
      "Iteration: 119 \t--- Loss: 1.368\n",
      "Iteration: 120 \t--- Loss: 1.272\n",
      "Iteration: 121 \t--- Loss: 1.303\n",
      "Iteration: 122 \t--- Loss: 1.322\n",
      "Iteration: 123 \t--- Loss: 1.267\n",
      "Iteration: 124 \t--- Loss: 1.257\n",
      "Iteration: 125 \t--- Loss: 1.354\n",
      "Iteration: 126 \t--- Loss: 1.349\n",
      "Iteration: 127 \t--- Loss: 1.285\n",
      "Iteration: 128 \t--- Loss: 1.283\n",
      "Iteration: 129 \t--- Loss: 1.340\n",
      "Iteration: 130 \t--- Loss: 1.305\n",
      "Iteration: 131 \t--- Loss: 1.304\n",
      "Iteration: 132 \t--- Loss: 1.305\n",
      "Iteration: 133 \t--- Loss: 1.316\n",
      "Iteration: 134 \t--- Loss: 1.285\n",
      "Iteration: 135 \t--- Loss: 1.415\n",
      "Iteration: 136 \t--- Loss: 1.370\n",
      "Iteration: 137 \t--- Loss: 1.328\n",
      "Iteration: 138 \t--- Loss: 1.284\n",
      "Iteration: 139 \t--- Loss: 1.317\n",
      "Iteration: 140 \t--- Loss: 1.306\n",
      "Iteration: 141 \t--- Loss: 1.345\n",
      "Iteration: 142 \t--- Loss: 1.291\n",
      "Iteration: 143 \t--- Loss: 1.285\n",
      "Iteration: 144 \t--- Loss: 1.253\n",
      "Iteration: 145 \t--- Loss: 1.264\n",
      "Iteration: 146 \t--- Loss: 1.294\n",
      "Iteration: 147 \t--- Loss: 1.248\n",
      "Iteration: 148 \t--- Loss: 1.346\n",
      "Iteration: 149 \t--- Loss: 1.301\n",
      "Iteration: 150 \t--- Loss: 1.291\n",
      "Iteration: 151 \t--- Loss: 1.361\n",
      "Iteration: 152 \t--- Loss: 1.341\n",
      "Iteration: 153 \t--- Loss: 1.351\n",
      "Iteration: 154 \t--- Loss: 1.327\n",
      "Iteration: 155 \t--- Loss: 1.313\n",
      "Iteration: 156 \t--- Loss: 1.320\n",
      "Iteration: 157 \t--- Loss: 1.263\n",
      "Iteration: 158 \t--- Loss: 1.174\n",
      "Iteration: 159 \t--- Loss: 1.363\n",
      "Iteration: 160 \t--- Loss: 1.345\n",
      "Iteration: 161 \t--- Loss: 1.362\n",
      "Iteration: 162 \t--- Loss: 1.312\n",
      "Iteration: 163 \t--- Loss: 1.289\n",
      "Iteration: 164 \t--- Loss: 1.320\n",
      "Iteration: 165 \t--- Loss: 1.303\n",
      "Iteration: 166 \t--- Loss: 1.280\n",
      "Iteration: 167 \t--- Loss: 1.360\n",
      "Iteration: 168 \t--- Loss: 1.273\n",
      "Iteration: 169 \t--- Loss: 1.316\n",
      "Iteration: 170 \t--- Loss: 1.240\n",
      "Iteration: 171 \t--- Loss: 1.280\n",
      "Iteration: 172 \t--- Loss: 1.280\n",
      "Iteration: 173 \t--- Loss: 1.246\n",
      "Iteration: 174 \t--- Loss: 1.243\n",
      "Iteration: 175 \t--- Loss: 1.267\n",
      "Iteration: 176 \t--- Loss: 1.308\n",
      "Iteration: 177 \t--- Loss: 1.309\n",
      "Iteration: 178 \t--- Loss: 1.179\n",
      "Iteration: 179 \t--- Loss: 1.301\n",
      "Iteration: 180 \t--- Loss: 1.303\n",
      "Iteration: 181 \t--- Loss: 1.306\n",
      "Iteration: 182 \t--- Loss: 1.331\n",
      "Iteration: 183 \t--- Loss: 1.320\n",
      "Iteration: 184 \t--- Loss: 1.276\n",
      "Iteration: 185 \t--- Loss: 1.279\n",
      "Iteration: 186 \t--- Loss: 1.367\n",
      "Iteration: 187 \t--- Loss: 1.359\n",
      "Iteration: 188 \t--- Loss: 1.303\n",
      "Iteration: 189 \t--- Loss: 1.293\n",
      "Iteration: 190 \t--- Loss: 1.255\n",
      "Iteration: 191 \t--- Loss: 1.297\n",
      "Iteration: 192 \t--- Loss: 1.325\n",
      "Iteration: 193 \t--- Loss: 1.276\n",
      "Iteration: 194 \t--- Loss: 1.218\n",
      "Iteration: 195 \t--- Loss: 1.346\n",
      "Iteration: 196 \t--- Loss: 1.357\n",
      "Iteration: 197 \t--- Loss: 1.294\n",
      "Iteration: 198 \t--- Loss: 1.348\n",
      "Iteration: 199 \t--- Loss: 1.323\n",
      "Iteration: 200 \t--- Loss: 1.284\n",
      "Iteration: 201 \t--- Loss: 1.347\n",
      "Iteration: 202 \t--- Loss: 1.287\n",
      "Iteration: 203 \t--- Loss: 1.323\n",
      "Iteration: 204 \t--- Loss: 1.408\n",
      "Iteration: 205 \t--- Loss: 1.331\n",
      "Iteration: 206 \t--- Loss: 1.293\n",
      "Iteration: 207 \t--- Loss: 1.340\n",
      "Iteration: 208 \t--- Loss: 1.345\n",
      "Iteration: 209 \t--- Loss: 1.288\n",
      "Iteration: 210 \t--- Loss: 1.281\n",
      "Iteration: 211 \t--- Loss: 1.313\n",
      "Iteration: 212 \t--- Loss: 1.193\n",
      "Iteration: 213 \t--- Loss: 1.277\n",
      "Iteration: 214 \t--- Loss: 1.286\n",
      "Iteration: 215 \t--- Loss: 1.296\n",
      "Iteration: 216 \t--- Loss: 1.301\n",
      "Iteration: 217 \t--- Loss: 1.335\n",
      "Iteration: 218 \t--- Loss: 1.348\n",
      "Iteration: 219 \t--- Loss: 1.343\n",
      "Iteration: 220 \t--- Loss: 1.349\n",
      "Iteration: 221 \t--- Loss: 1.243\n",
      "Iteration: 222 \t--- Loss: 1.226\n",
      "Iteration: 223 \t--- Loss: 1.326\n",
      "Iteration: 224 \t--- Loss: 1.316\n",
      "Iteration: 225 \t--- Loss: 1.286\n",
      "Iteration: 226 \t--- Loss: 1.299\n",
      "Iteration: 227 \t--- Loss: 1.287\n",
      "Iteration: 228 \t--- Loss: 1.338\n",
      "Iteration: 229 \t--- Loss: 1.264\n",
      "Iteration: 230 \t--- Loss: 1.281\n",
      "Iteration: 231 \t--- Loss: 1.347\n",
      "Iteration: 232 \t--- Loss: 1.332\n",
      "Iteration: 233 \t--- Loss: 1.218\n",
      "Iteration: 234 \t--- Loss: 1.368\n",
      "Iteration: 235 \t--- Loss: 1.306\n",
      "Iteration: 236 \t--- Loss: 1.272\n",
      "Iteration: 237 \t--- Loss: 1.349\n",
      "Iteration: 238 \t--- Loss: 1.285\n",
      "Iteration: 239 \t--- Loss: 1.409\n",
      "Iteration: 240 \t--- Loss: 1.296\n",
      "Iteration: 241 \t--- Loss: 1.259\n",
      "Iteration: 242 \t--- Loss: 1.253\n",
      "Iteration: 243 \t--- Loss: 1.321\n",
      "Iteration: 244 \t--- Loss: 1.245\n",
      "Iteration: 245 \t--- Loss: 1.273\n",
      "Iteration: 246 \t--- Loss: 1.233\n",
      "Iteration: 247 \t--- Loss: 1.362\n",
      "Iteration: 248 \t--- Loss: 1.323\n",
      "Iteration: 249 \t--- Loss: 1.314\n",
      "Iteration: 250 \t--- Loss: 1.309\n",
      "Iteration: 251 \t--- Loss: 1.341\n",
      "Iteration: 252 \t--- Loss: 1.305\n",
      "Iteration: 253 \t--- Loss: 1.291\n",
      "Iteration: 254 \t--- Loss: 1.355\n",
      "Iteration: 255 \t--- Loss: 1.295\n",
      "Iteration: 256 \t--- Loss: 1.280\n",
      "Iteration: 257 \t--- Loss: 1.235\n",
      "Iteration: 258 \t--- Loss: 1.327\n",
      "Iteration: 259 \t--- Loss: 1.278"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.34s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.301\n",
      "Iteration: 261 \t--- Loss: 0.278\n",
      "Iteration: 262 \t--- Loss: 0.303\n",
      "Iteration: 263 \t--- Loss: 0.292\n",
      "Iteration: 264 \t--- Loss: 0.282\n",
      "Iteration: 265 \t--- Loss: 0.287\n",
      "Iteration: 266 \t--- Loss: 0.295\n",
      "Iteration: 267 \t--- Loss: 0.258\n",
      "Iteration: 268 \t--- Loss: 0.294\n",
      "Iteration: 269 \t--- Loss: 0.263\n",
      "Iteration: 270 \t--- Loss: 0.295\n",
      "Iteration: 271 \t--- Loss: 0.284\n",
      "Iteration: 272 \t--- Loss: 0.287\n",
      "Iteration: 273 \t--- Loss: 0.279\n",
      "Iteration: 274 \t--- Loss: 0.305\n",
      "Iteration: 275 \t--- Loss: 0.283\n",
      "Iteration: 276 \t--- Loss: 0.279\n",
      "Iteration: 277 \t--- Loss: 0.277\n",
      "Iteration: 278 \t--- Loss: 0.276\n",
      "Iteration: 279 \t--- Loss: 0.308\n",
      "Iteration: 280 \t--- Loss: 0.290\n",
      "Iteration: 281 \t--- Loss: 0.296\n",
      "Iteration: 282 \t--- Loss: 0.260\n",
      "Iteration: 283 \t--- Loss: 0.287\n",
      "Iteration: 284 \t--- Loss: 0.285\n",
      "Iteration: 285 \t--- Loss: 0.313\n",
      "Iteration: 286 \t--- Loss: 0.277\n",
      "Iteration: 287 \t--- Loss: 0.279\n",
      "Iteration: 288 \t--- Loss: 0.243\n",
      "Iteration: 289 \t--- Loss: 0.302\n",
      "Iteration: 290 \t--- Loss: 0.290\n",
      "Iteration: 291 \t--- Loss: 0.284\n",
      "Iteration: 292 \t--- Loss: 0.285\n",
      "Iteration: 293 \t--- Loss: 0.284\n",
      "Iteration: 294 \t--- Loss: 0.276\n",
      "Iteration: 295 \t--- Loss: 0.294\n",
      "Iteration: 296 \t--- Loss: 0.261\n",
      "Iteration: 297 \t--- Loss: 0.287\n",
      "Iteration: 298 \t--- Loss: 0.268\n",
      "Iteration: 299 \t--- Loss: 0.284\n",
      "Iteration: 300 \t--- Loss: 0.273\n",
      "Iteration: 301 \t--- Loss: 0.283\n",
      "Iteration: 302 \t--- Loss: 0.316\n",
      "Iteration: 303 \t--- Loss: 0.256\n",
      "Iteration: 304 \t--- Loss: 0.264\n",
      "Iteration: 305 \t--- Loss: 0.280\n",
      "Iteration: 306 \t--- Loss: 0.302\n",
      "Iteration: 307 \t--- Loss: 0.290\n",
      "Iteration: 308 \t--- Loss: 0.303\n",
      "Iteration: 309 \t--- Loss: 0.278\n",
      "Iteration: 310 \t--- Loss: 0.317\n",
      "Iteration: 311 \t--- Loss: 0.278\n",
      "Iteration: 312 \t--- Loss: 0.310\n",
      "Iteration: 313 \t--- Loss: 0.245\n",
      "Iteration: 314 \t--- Loss: 0.277\n",
      "Iteration: 315 \t--- Loss: 0.251\n",
      "Iteration: 316 \t--- Loss: 0.293\n",
      "Iteration: 317 \t--- Loss: 0.273\n",
      "Iteration: 318 \t--- Loss: 0.279\n",
      "Iteration: 319 \t--- Loss: 0.273\n",
      "Iteration: 320 \t--- Loss: 0.306\n",
      "Iteration: 321 \t--- Loss: 0.270\n",
      "Iteration: 322 \t--- Loss: 0.293\n",
      "Iteration: 323 \t--- Loss: 0.282\n",
      "Iteration: 324 \t--- Loss: 0.281\n",
      "Iteration: 325 \t--- Loss: 0.274\n",
      "Iteration: 326 \t--- Loss: 0.331\n",
      "Iteration: 327 \t--- Loss: 0.276\n",
      "Iteration: 328 \t--- Loss: 0.259\n",
      "Iteration: 329 \t--- Loss: 0.299\n",
      "Iteration: 330 \t--- Loss: 0.271\n",
      "Iteration: 331 \t--- Loss: 0.285\n",
      "Iteration: 332 \t--- Loss: 0.280\n",
      "Iteration: 333 \t--- Loss: 0.287\n",
      "Iteration: 334 \t--- Loss: 0.304\n",
      "Iteration: 335 \t--- Loss: 0.282\n",
      "Iteration: 336 \t--- Loss: 0.283\n",
      "Iteration: 337 \t--- Loss: 0.264\n",
      "Iteration: 338 \t--- Loss: 0.269\n",
      "Iteration: 339 \t--- Loss: 0.279\n",
      "Iteration: 340 \t--- Loss: 0.244\n",
      "Iteration: 341 \t--- Loss: 0.269\n",
      "Iteration: 342 \t--- Loss: 0.286\n",
      "Iteration: 343 \t--- Loss: 0.252\n",
      "Iteration: 344 \t--- Loss: 0.286\n",
      "Iteration: 345 \t--- Loss: 0.284\n",
      "Iteration: 346 \t--- Loss: 0.269\n",
      "Iteration: 347 \t--- Loss: 0.297\n",
      "Iteration: 348 \t--- Loss: 0.291\n",
      "Iteration: 349 \t--- Loss: 0.280\n",
      "Iteration: 350 \t--- Loss: 0.270\n",
      "Iteration: 351 \t--- Loss: 0.263\n",
      "Iteration: 352 \t--- Loss: 0.274\n",
      "Iteration: 353 \t--- Loss: 0.281\n",
      "Iteration: 354 \t--- Loss: 0.293\n",
      "Iteration: 355 \t--- Loss: 0.281\n",
      "Iteration: 356 \t--- Loss: 0.308\n",
      "Iteration: 357 \t--- Loss: 0.308\n",
      "Iteration: 358 \t--- Loss: 0.267\n",
      "Iteration: 359 \t--- Loss: 0.309\n",
      "Iteration: 360 \t--- Loss: 0.273\n",
      "Iteration: 361 \t--- Loss: 0.287\n",
      "Iteration: 362 \t--- Loss: 0.277\n",
      "Iteration: 363 \t--- Loss: 0.283\n",
      "Iteration: 364 \t--- Loss: 0.283\n",
      "Iteration: 365 \t--- Loss: 0.290\n",
      "Iteration: 366 \t--- Loss: 0.257\n",
      "Iteration: 367 \t--- Loss: 0.286\n",
      "Iteration: 368 \t--- Loss: 0.310\n",
      "Iteration: 369 \t--- Loss: 0.285\n",
      "Iteration: 370 \t--- Loss: 0.263\n",
      "Iteration: 371 \t--- Loss: 0.264\n",
      "Iteration: 372 \t--- Loss: 0.277\n",
      "Iteration: 373 \t--- Loss: 0.298\n",
      "Iteration: 374 \t--- Loss: 0.262\n",
      "Iteration: 375 \t--- Loss: 0.293\n",
      "Iteration: 376 \t--- Loss: 0.278\n",
      "Iteration: 377 \t--- Loss: 0.294\n",
      "Iteration: 378 \t--- Loss: 0.277\n",
      "Iteration: 379 \t--- Loss: 0.251\n",
      "Iteration: 380 \t--- Loss: 0.287\n",
      "Iteration: 381 \t--- Loss: 0.288\n",
      "Iteration: 382 \t--- Loss: 0.298\n",
      "Iteration: 383 \t--- Loss: 0.272\n",
      "Iteration: 384 \t--- Loss: 0.286\n",
      "Iteration: 385 \t--- Loss: 0.246\n",
      "Iteration: 386 \t--- Loss: 0.305\n",
      "Iteration: 387 \t--- Loss: 0.288\n",
      "Iteration: 388 \t--- Loss: 0.317\n",
      "Iteration: 389 \t--- Loss: 0.300\n",
      "Iteration: 390 \t--- Loss: 0.303\n",
      "Iteration: 391 \t--- Loss: 0.280\n",
      "Iteration: 392 \t--- Loss: 0.293\n",
      "Iteration: 393 \t--- Loss: 0.262\n",
      "Iteration: 394 \t--- Loss: 0.284\n",
      "Iteration: 395 \t--- Loss: 0.282\n",
      "Iteration: 396 \t--- Loss: 0.294\n",
      "Iteration: 397 \t--- Loss: 0.303\n",
      "Iteration: 398 \t--- Loss: 0.287\n",
      "Iteration: 399 \t--- Loss: 0.278\n",
      "Iteration: 400 \t--- Loss: 0.322\n",
      "Iteration: 401 \t--- Loss: 0.289\n",
      "Iteration: 402 \t--- Loss: 0.285\n",
      "Iteration: 403 \t--- Loss: 0.325\n",
      "Iteration: 404 \t--- Loss: 0.312\n",
      "Iteration: 405 \t--- Loss: 0.302\n",
      "Iteration: 406 \t--- Loss: 0.277\n",
      "Iteration: 407 \t--- Loss: 0.275\n",
      "Iteration: 408 \t--- Loss: 0.282\n",
      "Iteration: 409 \t--- Loss: 0.262\n",
      "Iteration: 410 \t--- Loss: 0.256\n",
      "Iteration: 411 \t--- Loss: 0.299\n",
      "Iteration: 412 \t--- Loss: 0.301\n",
      "Iteration: 413 \t--- Loss: 0.290\n",
      "Iteration: 414 \t--- Loss: 0.267\n",
      "Iteration: 415 \t--- Loss: 0.276\n",
      "Iteration: 416 \t--- Loss: 0.292\n",
      "Iteration: 417 \t--- Loss: 0.280\n",
      "Iteration: 418 \t--- Loss: 0.271\n",
      "Iteration: 419 \t--- Loss: 0.276\n",
      "Iteration: 420 \t--- Loss: 0.262\n",
      "Iteration: 421 \t--- Loss: 0.300\n",
      "Iteration: 422 \t--- Loss: 0.269\n",
      "Iteration: 423 \t--- Loss: 0.271\n",
      "Iteration: 424 \t--- Loss: 0.278\n",
      "Iteration: 425 \t--- Loss: 0.277\n",
      "Iteration: 426 \t--- Loss: 0.298\n",
      "Iteration: 427 \t--- Loss: 0.273\n",
      "Iteration: 428 \t--- Loss: 0.296\n",
      "Iteration: 429 \t--- Loss: 0.306\n",
      "Iteration: 430 \t--- Loss: 0.276\n",
      "Iteration: 431 \t--- Loss: 0.279\n",
      "Iteration: 432 \t--- Loss: 0.274\n",
      "Iteration: 433 \t--- Loss: 0.302\n",
      "Iteration: 434 \t--- Loss: 0.279\n",
      "Iteration: 435 \t--- Loss: 0.287\n",
      "Iteration: 436 \t--- Loss: 0.277\n",
      "Iteration: 437 \t--- Loss: 0.281\n",
      "Iteration: 438 \t--- Loss: 0.298\n",
      "Iteration: 439 \t--- Loss: 0.270\n",
      "Iteration: 440 \t--- Loss: 0.310\n",
      "Iteration: 441 \t--- Loss: 0.282\n",
      "Iteration: 442 \t--- Loss: 0.289\n",
      "Iteration: 443 \t--- Loss: 0.274\n",
      "Iteration: 444 \t--- Loss: 0.278\n",
      "Iteration: 445 \t--- Loss: 0.296\n",
      "Iteration: 446 \t--- Loss: 0.262\n",
      "Iteration: 447 \t--- Loss: 0.296\n",
      "Iteration: 448 \t--- Loss: 0.277\n",
      "Iteration: 449 \t--- Loss: 0.297\n",
      "Iteration: 450 \t--- Loss: 0.297\n",
      "Iteration: 451 \t--- Loss: 0.257\n",
      "Iteration: 452 \t--- Loss: 0.293\n",
      "Iteration: 453 \t--- Loss: 0.299\n",
      "Iteration: 454 \t--- Loss: 0.240\n",
      "Iteration: 455 \t--- Loss: 0.255\n",
      "Iteration: 456 \t--- Loss: 0.289\n",
      "Iteration: 457 \t--- Loss: 0.275\n",
      "Iteration: 458 \t--- Loss: 0.302\n",
      "Iteration: 459 \t--- Loss: 0.300\n",
      "Iteration: 460 \t--- Loss: 0.270\n",
      "Iteration: 461 \t--- Loss: 0.290\n",
      "Iteration: 462 \t--- Loss: 0.280\n",
      "Iteration: 463 \t--- Loss: 0.275\n",
      "Iteration: 464 \t--- Loss: 0.303\n",
      "Iteration: 465 \t--- Loss: 0.284\n",
      "Iteration: 466 \t--- Loss: 0.286\n",
      "Iteration: 467 \t--- Loss: 0.285\n",
      "Iteration: 468 \t--- Loss: 0.292\n",
      "Iteration: 469 \t--- Loss: 0.293\n",
      "Iteration: 470 \t--- Loss: 0.283\n",
      "Iteration: 471 \t--- Loss: 0.268\n",
      "Iteration: 472 \t--- Loss: 0.304\n",
      "Iteration: 473 \t--- Loss: 0.290\n",
      "Iteration: 474 \t--- Loss: 0.289\n",
      "Iteration: 475 \t--- Loss: 0.290\n",
      "Iteration: 476 \t--- Loss: 0.279\n",
      "Iteration: 477 \t--- Loss: 0.285\n",
      "Iteration: 478 \t--- Loss: 0.282\n",
      "Iteration: 479 \t--- Loss: 0.290\n",
      "Iteration: 480 \t--- Loss: 0.295\n",
      "Iteration: 481 \t--- Loss: 0.290\n",
      "Iteration: 482 \t--- Loss: 0.266\n",
      "Iteration: 483 \t--- Loss: 0.304\n",
      "Iteration: 484 \t--- Loss: 0.289\n",
      "Iteration: 485 \t--- Loss: 0.292\n",
      "Iteration: 486 \t--- Loss: 0.315\n",
      "Iteration: 487 \t--- Loss: 0.301\n",
      "Iteration: 488 \t--- Loss: 0.287\n",
      "Iteration: 489 \t--- Loss: 0.251\n",
      "Iteration: 490 \t--- Loss: 0.310\n",
      "Iteration: 491 \t--- Loss: 0.282\n",
      "Iteration: 492 \t--- Loss: 0.264\n",
      "Iteration: 493 \t--- Loss: 0.296\n",
      "Iteration: 494 \t--- Loss: 0.280\n",
      "Iteration: 495 \t--- Loss: 0.280\n",
      "Iteration: 496 \t--- Loss: 0.290\n",
      "Iteration: 497 \t--- Loss: 0.295\n",
      "Iteration: 498 \t--- Loss: 0.304\n",
      "Iteration: 499 \t--- Loss: 0.312\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.20s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 1.272\n",
      "Iteration: 261 \t--- Loss: 1.316\n",
      "Iteration: 262 \t--- Loss: 1.297\n",
      "Iteration: 263 \t--- Loss: 1.331\n",
      "Iteration: 264 \t--- Loss: 1.242\n",
      "Iteration: 265 \t--- Loss: 1.294\n",
      "Iteration: 266 \t--- Loss: 1.313\n",
      "Iteration: 267 \t--- Loss: 1.250\n",
      "Iteration: 268 \t--- Loss: 1.302\n",
      "Iteration: 269 \t--- Loss: 1.338\n",
      "Iteration: 270 \t--- Loss: 1.291\n",
      "Iteration: 271 \t--- Loss: 1.250\n",
      "Iteration: 272 \t--- Loss: 1.362\n",
      "Iteration: 273 \t--- Loss: 1.260\n",
      "Iteration: 274 \t--- Loss: 1.358\n",
      "Iteration: 275 \t--- Loss: 1.264\n",
      "Iteration: 276 \t--- Loss: 1.305\n",
      "Iteration: 277 \t--- Loss: 1.315\n",
      "Iteration: 278 \t--- Loss: 1.330\n",
      "Iteration: 279 \t--- Loss: 1.267\n",
      "Iteration: 280 \t--- Loss: 1.297\n",
      "Iteration: 281 \t--- Loss: 1.382\n",
      "Iteration: 282 \t--- Loss: 1.391\n",
      "Iteration: 283 \t--- Loss: 1.248\n",
      "Iteration: 284 \t--- Loss: 1.293\n",
      "Iteration: 285 \t--- Loss: 1.309\n",
      "Iteration: 286 \t--- Loss: 1.235\n",
      "Iteration: 287 \t--- Loss: 1.206\n",
      "Iteration: 288 \t--- Loss: 1.294\n",
      "Iteration: 289 \t--- Loss: 1.318\n",
      "Iteration: 290 \t--- Loss: 1.298\n",
      "Iteration: 291 \t--- Loss: 1.273\n",
      "Iteration: 292 \t--- Loss: 1.336\n",
      "Iteration: 293 \t--- Loss: 1.286\n",
      "Iteration: 294 \t--- Loss: 1.291\n",
      "Iteration: 295 \t--- Loss: 1.276\n",
      "Iteration: 296 \t--- Loss: 1.310\n",
      "Iteration: 297 \t--- Loss: 1.329\n",
      "Iteration: 298 \t--- Loss: 1.311\n",
      "Iteration: 299 \t--- Loss: 1.252\n",
      "Iteration: 300 \t--- Loss: 1.267\n",
      "Iteration: 301 \t--- Loss: 1.252\n",
      "Iteration: 302 \t--- Loss: 1.261\n",
      "Iteration: 303 \t--- Loss: 1.311\n",
      "Iteration: 304 \t--- Loss: 1.298\n",
      "Iteration: 305 \t--- Loss: 1.285\n",
      "Iteration: 306 \t--- Loss: 1.290\n",
      "Iteration: 307 \t--- Loss: 1.283\n",
      "Iteration: 308 \t--- Loss: 1.300\n",
      "Iteration: 309 \t--- Loss: 1.371\n",
      "Iteration: 310 \t--- Loss: 1.308\n",
      "Iteration: 311 \t--- Loss: 1.376\n",
      "Iteration: 312 \t--- Loss: 1.260\n",
      "Iteration: 313 \t--- Loss: 1.310\n",
      "Iteration: 314 \t--- Loss: 1.305\n",
      "Iteration: 315 \t--- Loss: 1.222\n",
      "Iteration: 316 \t--- Loss: 1.373\n",
      "Iteration: 317 \t--- Loss: 1.300\n",
      "Iteration: 318 \t--- Loss: 1.274\n",
      "Iteration: 319 \t--- Loss: 1.255\n",
      "Iteration: 320 \t--- Loss: 1.336\n",
      "Iteration: 321 \t--- Loss: 1.315\n",
      "Iteration: 322 \t--- Loss: 1.289\n",
      "Iteration: 323 \t--- Loss: 1.276\n",
      "Iteration: 324 \t--- Loss: 1.302\n",
      "Iteration: 325 \t--- Loss: 1.264\n",
      "Iteration: 326 \t--- Loss: 1.233\n",
      "Iteration: 327 \t--- Loss: 1.316\n",
      "Iteration: 328 \t--- Loss: 1.307\n",
      "Iteration: 329 \t--- Loss: 1.304\n",
      "Iteration: 330 \t--- Loss: 1.332\n",
      "Iteration: 331 \t--- Loss: 1.267\n",
      "Iteration: 332 \t--- Loss: 1.322\n",
      "Iteration: 333 \t--- Loss: 1.313\n",
      "Iteration: 334 \t--- Loss: 1.317\n",
      "Iteration: 335 \t--- Loss: 1.291\n",
      "Iteration: 336 \t--- Loss: 1.310\n",
      "Iteration: 337 \t--- Loss: 1.220\n",
      "Iteration: 338 \t--- Loss: 1.364\n",
      "Iteration: 339 \t--- Loss: 1.292\n",
      "Iteration: 340 \t--- Loss: 1.370\n",
      "Iteration: 341 \t--- Loss: 1.426\n",
      "Iteration: 342 \t--- Loss: 1.327\n",
      "Iteration: 343 \t--- Loss: 1.286\n",
      "Iteration: 344 \t--- Loss: 1.380\n",
      "Iteration: 345 \t--- Loss: 1.274\n",
      "Iteration: 346 \t--- Loss: 1.286\n",
      "Iteration: 347 \t--- Loss: 1.337\n",
      "Iteration: 348 \t--- Loss: 1.272\n",
      "Iteration: 349 \t--- Loss: 1.310\n",
      "Iteration: 350 \t--- Loss: 1.294\n",
      "Iteration: 351 \t--- Loss: 1.309\n",
      "Iteration: 352 \t--- Loss: 1.315\n",
      "Iteration: 353 \t--- Loss: 1.345\n",
      "Iteration: 354 \t--- Loss: 1.264\n",
      "Iteration: 355 \t--- Loss: 1.281\n",
      "Iteration: 356 \t--- Loss: 1.332\n",
      "Iteration: 357 \t--- Loss: 1.261\n",
      "Iteration: 358 \t--- Loss: 1.312\n",
      "Iteration: 359 \t--- Loss: 1.299\n",
      "Iteration: 360 \t--- Loss: 1.302\n",
      "Iteration: 361 \t--- Loss: 1.270\n",
      "Iteration: 362 \t--- Loss: 1.413\n",
      "Iteration: 363 \t--- Loss: 1.314\n",
      "Iteration: 364 \t--- Loss: 1.273\n",
      "Iteration: 365 \t--- Loss: 1.373\n",
      "Iteration: 366 \t--- Loss: 1.311\n",
      "Iteration: 367 \t--- Loss: 1.289\n",
      "Iteration: 368 \t--- Loss: 1.299\n",
      "Iteration: 369 \t--- Loss: 1.292\n",
      "Iteration: 370 \t--- Loss: 1.313\n",
      "Iteration: 371 \t--- Loss: 1.301\n",
      "Iteration: 372 \t--- Loss: 1.304\n",
      "Iteration: 373 \t--- Loss: 1.322\n",
      "Iteration: 374 \t--- Loss: 1.296\n",
      "Iteration: 375 \t--- Loss: 1.357\n",
      "Iteration: 376 \t--- Loss: 1.363\n",
      "Iteration: 377 \t--- Loss: 1.282\n",
      "Iteration: 378 \t--- Loss: 1.282\n",
      "Iteration: 379 \t--- Loss: 1.324\n",
      "Iteration: 380 \t--- Loss: 1.341\n",
      "Iteration: 381 \t--- Loss: 1.290\n",
      "Iteration: 382 \t--- Loss: 1.266\n",
      "Iteration: 383 \t--- Loss: 1.322\n",
      "Iteration: 384 \t--- Loss: 1.316\n",
      "Iteration: 385 \t--- Loss: 1.310\n",
      "Iteration: 386 \t--- Loss: 1.255\n",
      "Iteration: 387 \t--- Loss: 1.270\n",
      "Iteration: 388 \t--- Loss: 1.285\n",
      "Iteration: 389 \t--- Loss: 1.280\n",
      "Iteration: 390 \t--- Loss: 1.292\n",
      "Iteration: 391 \t--- Loss: 1.317\n",
      "Iteration: 392 \t--- Loss: 1.226\n",
      "Iteration: 393 \t--- Loss: 1.256\n",
      "Iteration: 394 \t--- Loss: 1.263\n",
      "Iteration: 395 \t--- Loss: 1.312\n",
      "Iteration: 396 \t--- Loss: 1.254\n",
      "Iteration: 397 \t--- Loss: 1.277\n",
      "Iteration: 398 \t--- Loss: 1.241\n",
      "Iteration: 399 \t--- Loss: 1.245\n",
      "Iteration: 400 \t--- Loss: 1.333\n",
      "Iteration: 401 \t--- Loss: 1.322\n",
      "Iteration: 402 \t--- Loss: 1.297\n",
      "Iteration: 403 \t--- Loss: 1.332\n",
      "Iteration: 404 \t--- Loss: 1.296\n",
      "Iteration: 405 \t--- Loss: 1.282\n",
      "Iteration: 406 \t--- Loss: 1.296\n",
      "Iteration: 407 \t--- Loss: 1.333\n",
      "Iteration: 408 \t--- Loss: 1.267\n",
      "Iteration: 409 \t--- Loss: 1.195\n",
      "Iteration: 410 \t--- Loss: 1.211\n",
      "Iteration: 411 \t--- Loss: 1.226\n",
      "Iteration: 412 \t--- Loss: 1.297\n",
      "Iteration: 413 \t--- Loss: 1.294\n",
      "Iteration: 414 \t--- Loss: 1.261\n",
      "Iteration: 415 \t--- Loss: 1.359\n",
      "Iteration: 416 \t--- Loss: 1.352\n",
      "Iteration: 417 \t--- Loss: 1.265\n",
      "Iteration: 418 \t--- Loss: 1.284\n",
      "Iteration: 419 \t--- Loss: 1.240\n",
      "Iteration: 420 \t--- Loss: 1.310\n",
      "Iteration: 421 \t--- Loss: 1.235\n",
      "Iteration: 422 \t--- Loss: 1.304\n",
      "Iteration: 423 \t--- Loss: 1.303\n",
      "Iteration: 424 \t--- Loss: 1.314\n",
      "Iteration: 425 \t--- Loss: 1.213\n",
      "Iteration: 426 \t--- Loss: 1.253\n",
      "Iteration: 427 \t--- Loss: 1.261\n",
      "Iteration: 428 \t--- Loss: 1.389\n",
      "Iteration: 429 \t--- Loss: 1.284\n",
      "Iteration: 430 \t--- Loss: 1.299\n",
      "Iteration: 431 \t--- Loss: 1.358\n",
      "Iteration: 432 \t--- Loss: 1.291\n",
      "Iteration: 433 \t--- Loss: 1.334\n",
      "Iteration: 434 \t--- Loss: 1.293\n",
      "Iteration: 435 \t--- Loss: 1.240\n",
      "Iteration: 436 \t--- Loss: 1.332\n",
      "Iteration: 437 \t--- Loss: 1.313\n",
      "Iteration: 438 \t--- Loss: 1.337\n",
      "Iteration: 439 \t--- Loss: 1.263\n",
      "Iteration: 440 \t--- Loss: 1.319\n",
      "Iteration: 441 \t--- Loss: 1.315\n",
      "Iteration: 442 \t--- Loss: 1.287\n",
      "Iteration: 443 \t--- Loss: 1.379\n",
      "Iteration: 444 \t--- Loss: 1.350\n",
      "Iteration: 445 \t--- Loss: 1.325\n",
      "Iteration: 446 \t--- Loss: 1.293\n",
      "Iteration: 447 \t--- Loss: 1.319\n",
      "Iteration: 448 \t--- Loss: 1.274\n",
      "Iteration: 449 \t--- Loss: 1.267\n",
      "Iteration: 450 \t--- Loss: 1.243\n",
      "Iteration: 451 \t--- Loss: 1.276\n",
      "Iteration: 452 \t--- Loss: 1.338\n",
      "Iteration: 453 \t--- Loss: 1.331\n",
      "Iteration: 454 \t--- Loss: 1.414\n",
      "Iteration: 455 \t--- Loss: 1.324\n",
      "Iteration: 456 \t--- Loss: 1.244\n",
      "Iteration: 457 \t--- Loss: 1.266\n",
      "Iteration: 458 \t--- Loss: 1.252\n",
      "Iteration: 459 \t--- Loss: 1.215\n",
      "Iteration: 460 \t--- Loss: 1.332\n",
      "Iteration: 461 \t--- Loss: 1.206\n",
      "Iteration: 462 \t--- Loss: 1.361\n",
      "Iteration: 463 \t--- Loss: 1.279\n",
      "Iteration: 464 \t--- Loss: 1.318\n",
      "Iteration: 465 \t--- Loss: 1.361\n",
      "Iteration: 466 \t--- Loss: 1.381\n",
      "Iteration: 467 \t--- Loss: 1.283\n",
      "Iteration: 468 \t--- Loss: 1.348\n",
      "Iteration: 469 \t--- Loss: 1.352\n",
      "Iteration: 470 \t--- Loss: 1.248\n",
      "Iteration: 471 \t--- Loss: 1.349\n",
      "Iteration: 472 \t--- Loss: 1.305\n",
      "Iteration: 473 \t--- Loss: 1.346\n",
      "Iteration: 474 \t--- Loss: 1.283\n",
      "Iteration: 475 \t--- Loss: 1.243\n",
      "Iteration: 476 \t--- Loss: 1.337\n",
      "Iteration: 477 \t--- Loss: 1.339\n",
      "Iteration: 478 \t--- Loss: 1.318\n",
      "Iteration: 479 \t--- Loss: 1.312\n",
      "Iteration: 480 \t--- Loss: 1.277\n",
      "Iteration: 481 \t--- Loss: 1.398\n",
      "Iteration: 482 \t--- Loss: 1.342\n",
      "Iteration: 483 \t--- Loss: 1.310\n",
      "Iteration: 484 \t--- Loss: 1.347\n",
      "Iteration: 485 \t--- Loss: 1.380\n",
      "Iteration: 486 \t--- Loss: 1.348\n",
      "Iteration: 487 \t--- Loss: 1.224\n",
      "Iteration: 488 \t--- Loss: 1.284\n",
      "Iteration: 489 \t--- Loss: 1.233\n",
      "Iteration: 490 \t--- Loss: 1.251\n",
      "Iteration: 491 \t--- Loss: 1.228\n",
      "Iteration: 492 \t--- Loss: 1.261\n",
      "Iteration: 493 \t--- Loss: 1.282\n",
      "Iteration: 494 \t--- Loss: 1.187\n",
      "Iteration: 495 \t--- Loss: 1.266\n",
      "Iteration: 496 \t--- Loss: 1.247\n",
      "Iteration: 497 \t--- Loss: 1.295\n",
      "Iteration: 498 \t--- Loss: 1.243\n",
      "Iteration: 499 \t--- Loss: 1.287\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.13s/it][Parallel(n_jobs=5)]: Done  10 tasks      | elapsed:  3.3min\n",
      " 30%|███       | 3/10 [00:03<00:07,  1.07s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.40s/it][Parallel(n_jobs=5)]: Done  11 tasks      | elapsed:  3.4min\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.23s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.947\n",
      "Iteration: 1 \t--- Loss: 0.825\n",
      "Iteration: 2 \t--- Loss: 0.735\n",
      "Iteration: 3 \t--- Loss: 0.730\n",
      "Iteration: 4 \t--- Loss: 0.657\n",
      "Iteration: 5 \t--- Loss: 0.629\n",
      "Iteration: 6 \t--- Loss: 0.605\n",
      "Iteration: 7 \t--- Loss: 0.552\n",
      "Iteration: 8 \t--- Loss: 0.568\n",
      "Iteration: 9 \t--- Loss: 0.562\n",
      "Iteration: 10 \t--- Loss: 0.532\n",
      "Iteration: 11 \t--- Loss: 0.523\n",
      "Iteration: 12 \t--- Loss: 0.506\n",
      "Iteration: 13 \t--- Loss: 0.502\n",
      "Iteration: 14 \t--- Loss: 0.480\n",
      "Iteration: 15 \t--- Loss: 0.498\n",
      "Iteration: 16 \t--- Loss: 0.490\n",
      "Iteration: 17 \t--- Loss: 0.500\n",
      "Iteration: 18 \t--- Loss: 0.499\n",
      "Iteration: 19 \t--- Loss: 0.491\n",
      "Iteration: 20 \t--- Loss: 0.468\n",
      "Iteration: 21 \t--- Loss: 0.482\n",
      "Iteration: 22 \t--- Loss: 0.473\n",
      "Iteration: 23 \t--- Loss: 0.467\n",
      "Iteration: 24 \t--- Loss: 0.464\n",
      "Iteration: 25 \t--- Loss: 0.475\n",
      "Iteration: 26 \t--- Loss: 0.474\n",
      "Iteration: 27 \t--- Loss: 0.474\n",
      "Iteration: 28 \t--- Loss: 0.468\n",
      "Iteration: 29 \t--- Loss: 0.467\n",
      "Iteration: 30 \t--- Loss: 0.478\n",
      "Iteration: 31 \t--- Loss: 0.470\n",
      "Iteration: 32 \t--- Loss: 0.473\n",
      "Iteration: 33 \t--- Loss: 0.465\n",
      "Iteration: 34 \t--- Loss: 0.478\n",
      "Iteration: 35 \t--- Loss: 0.473\n",
      "Iteration: 36 \t--- Loss: 0.467\n",
      "Iteration: 37 \t--- Loss: 0.470\n",
      "Iteration: 38 \t--- Loss: 0.472\n",
      "Iteration: 39 \t--- Loss: 0.468\n",
      "Iteration: 40 \t--- Loss: 0.467\n",
      "Iteration: 41 \t--- Loss: 0.465\n",
      "Iteration: 42 \t--- Loss: 0.470\n",
      "Iteration: 43 \t--- Loss: 0.462\n",
      "Iteration: 44 \t--- Loss: 0.462\n",
      "Iteration: 45 \t--- Loss: 0.475\n",
      "Iteration: 46 \t--- Loss: 0.481\n",
      "Iteration: 47 \t--- Loss: 0.474\n",
      "Iteration: 48 \t--- Loss: 0.454\n",
      "Iteration: 49 \t--- Loss: 0.460\n",
      "Iteration: 50 \t--- Loss: 0.461\n",
      "Iteration: 51 \t--- Loss: 0.454\n",
      "Iteration: 52 \t--- Loss: 0.461\n",
      "Iteration: 53 \t--- Loss: 0.472\n",
      "Iteration: 54 \t--- Loss: 0.471\n",
      "Iteration: 55 \t--- Loss: 0.467\n",
      "Iteration: 56 \t--- Loss: 0.468\n",
      "Iteration: 57 \t--- Loss: 0.461\n",
      "Iteration: 58 \t--- Loss: 0.468\n",
      "Iteration: 59 \t--- Loss: 0.477\n",
      "Iteration: 60 \t--- Loss: 0.462\n",
      "Iteration: 61 \t--- Loss: 0.466\n",
      "Iteration: 62 \t--- Loss: 0.464\n",
      "Iteration: 63 \t--- Loss: 0.460\n",
      "Iteration: 64 \t--- Loss: 0.476\n",
      "Iteration: 65 \t--- Loss: 0.469\n",
      "Iteration: 66 \t--- Loss: 0.462\n",
      "Iteration: 67 \t--- Loss: 0.466\n",
      "Iteration: 68 \t--- Loss: 0.471\n",
      "Iteration: 69 \t--- Loss: 0.464\n",
      "Iteration: 70 \t--- Loss: 0.466\n",
      "Iteration: 71 \t--- Loss: 0.463\n",
      "Iteration: 72 \t--- Loss: 0.469\n",
      "Iteration: 73 \t--- Loss: 0.459\n",
      "Iteration: 74 \t--- Loss: 0.480\n",
      "Iteration: 75 \t--- Loss: 0.458\n",
      "Iteration: 76 \t--- Loss: 0.469\n",
      "Iteration: 77 \t--- Loss: 0.475\n",
      "Iteration: 78 \t--- Loss: 0.467\n",
      "Iteration: 79 \t--- Loss: 0.469\n",
      "Iteration: 80 \t--- Loss: 0.462\n",
      "Iteration: 81 \t--- Loss: 0.468\n",
      "Iteration: 82 \t--- Loss: 0.460\n",
      "Iteration: 83 \t--- Loss: 0.470\n",
      "Iteration: 84 \t--- Loss: 0.468\n",
      "Iteration: 85 \t--- Loss: 0.465\n",
      "Iteration: 86 \t--- Loss: 0.472\n",
      "Iteration: 87 \t--- Loss: 0.481\n",
      "Iteration: 88 \t--- Loss: 0.473\n",
      "Iteration: 89 \t--- Loss: 0.466\n",
      "Iteration: 90 \t--- Loss: 0.469\n",
      "Iteration: 91 \t--- Loss: 0.471\n",
      "Iteration: 92 \t--- Loss: 0.476\n",
      "Iteration: 93 \t--- Loss: 0.475\n",
      "Iteration: 94 \t--- Loss: 0.461\n",
      "Iteration: 95 \t--- Loss: 0.466\n",
      "Iteration: 96 \t--- Loss: 0.458\n",
      "Iteration: 97 \t--- Loss: 0.462\n",
      "Iteration: 98 \t--- Loss: 0.471\n",
      "Iteration: 99 \t--- Loss: 0.467\n",
      "Iteration: 100 \t--- Loss: 0.456\n",
      "Iteration: 101 \t--- Loss: 0.459\n",
      "Iteration: 102 \t--- Loss: 0.478\n",
      "Iteration: 103 \t--- Loss: 0.471\n",
      "Iteration: 104 \t--- Loss: 0.463\n",
      "Iteration: 105 \t--- Loss: 0.462\n",
      "Iteration: 106 \t--- Loss: 0.455\n",
      "Iteration: 107 \t--- Loss: 0.468\n",
      "Iteration: 108 \t--- Loss: 0.469\n",
      "Iteration: 109 \t--- Loss: 0.467\n",
      "Iteration: 110 \t--- Loss: 0.469\n",
      "Iteration: 111 \t--- Loss: 0.469\n",
      "Iteration: 112 \t--- Loss: 0.469\n",
      "Iteration: 113 \t--- Loss: 0.458\n",
      "Iteration: 114 \t--- Loss: 0.461\n",
      "Iteration: 115 \t--- Loss: 0.453\n",
      "Iteration: 116 \t--- Loss: 0.469\n",
      "Iteration: 117 \t--- Loss: 0.467\n",
      "Iteration: 118 \t--- Loss: 0.469\n",
      "Iteration: 119 \t--- Loss: 0.468\n",
      "Iteration: 120 \t--- Loss: 0.465\n",
      "Iteration: 121 \t--- Loss: 0.480\n",
      "Iteration: 122 \t--- Loss: 0.455\n",
      "Iteration: 123 \t--- Loss: 0.475\n",
      "Iteration: 124 \t--- Loss: 0.464\n",
      "Iteration: 125 \t--- Loss: 0.475\n",
      "Iteration: 126 \t--- Loss: 0.472\n",
      "Iteration: 127 \t--- Loss: 0.474\n",
      "Iteration: 128 \t--- Loss: 0.455\n",
      "Iteration: 129 \t--- Loss: 0.462\n",
      "Iteration: 130 \t--- Loss: 0.462\n",
      "Iteration: 131 \t--- Loss: 0.461\n",
      "Iteration: 132 \t--- Loss: 0.478\n",
      "Iteration: 133 \t--- Loss: 0.451\n",
      "Iteration: 134 \t--- Loss: 0.487\n",
      "Iteration: 135 \t--- Loss: 0.469\n",
      "Iteration: 136 \t--- Loss: 0.465\n",
      "Iteration: 137 \t--- Loss: 0.476\n",
      "Iteration: 138 \t--- Loss: 0.472\n",
      "Iteration: 139 \t--- Loss: 0.456\n",
      "Iteration: 140 \t--- Loss: 0.462\n",
      "Iteration: 141 \t--- Loss: 0.457\n",
      "Iteration: 142 \t--- Loss: 0.472\n",
      "Iteration: 143 \t--- Loss: 0.462\n",
      "Iteration: 144 \t--- Loss: 0.467\n",
      "Iteration: 145 \t--- Loss: 0.467\n",
      "Iteration: 146 \t--- Loss: 0.465\n",
      "Iteration: 147 \t--- Loss: 0.472\n",
      "Iteration: 148 \t--- Loss: 0.459\n",
      "Iteration: 149 \t--- Loss: 0.477\n",
      "Iteration: 150 \t--- Loss: 0.465\n",
      "Iteration: 151 \t--- Loss: 0.470\n",
      "Iteration: 152 \t--- Loss: 0.463\n",
      "Iteration: 153 \t--- Loss: 0.464\n",
      "Iteration: 154 \t--- Loss: 0.465\n",
      "Iteration: 155 \t--- Loss: 0.463\n",
      "Iteration: 156 \t--- Loss: 0.483\n",
      "Iteration: 157 \t--- Loss: 0.466\n",
      "Iteration: 158 \t--- Loss: 0.459\n",
      "Iteration: 159 \t--- Loss: 0.463\n",
      "Iteration: 160 \t--- Loss: 0.464\n",
      "Iteration: 161 \t--- Loss: 0.476\n",
      "Iteration: 162 \t--- Loss: 0.472\n",
      "Iteration: 163 \t--- Loss: 0.482\n",
      "Iteration: 164 \t--- Loss: 0.460\n",
      "Iteration: 165 \t--- Loss: 0.465\n",
      "Iteration: 166 \t--- Loss: 0.476\n",
      "Iteration: 167 \t--- Loss: 0.454\n",
      "Iteration: 168 \t--- Loss: 0.476\n",
      "Iteration: 169 \t--- Loss: 0.464\n",
      "Iteration: 170 \t--- Loss: 0.463\n",
      "Iteration: 171 \t--- Loss: 0.463\n",
      "Iteration: 172 \t--- Loss: 0.454\n",
      "Iteration: 173 \t--- Loss: 0.476\n",
      "Iteration: 174 \t--- Loss: 0.471\n",
      "Iteration: 175 \t--- Loss: 0.467\n",
      "Iteration: 176 \t--- Loss: 0.482\n",
      "Iteration: 177 \t--- Loss: 0.475\n",
      "Iteration: 178 \t--- Loss: 0.462\n",
      "Iteration: 179 \t--- Loss: 0.468\n",
      "Iteration: 180 \t--- Loss: 0.478\n",
      "Iteration: 181 \t--- Loss: 0.466\n",
      "Iteration: 182 \t--- Loss: 0.468\n",
      "Iteration: 183 \t--- Loss: 0.476\n",
      "Iteration: 184 \t--- Loss: 0.465\n",
      "Iteration: 185 \t--- Loss: 0.474\n",
      "Iteration: 186 \t--- Loss: 0.461\n",
      "Iteration: 187 \t--- Loss: 0.470\n",
      "Iteration: 188 \t--- Loss: 0.466\n",
      "Iteration: 189 \t--- Loss: 0.464\n",
      "Iteration: 190 \t--- Loss: 0.470\n",
      "Iteration: 191 \t--- Loss: 0.468\n",
      "Iteration: 192 \t--- Loss: 0.458\n",
      "Iteration: 193 \t--- Loss: 0.473\n",
      "Iteration: 194 \t--- Loss: 0.450\n",
      "Iteration: 195 \t--- Loss: 0.463\n",
      "Iteration: 196 \t--- Loss: 0.474\n",
      "Iteration: 197 \t--- Loss: 0.466\n",
      "Iteration: 198 \t--- Loss: 0.469\n",
      "Iteration: 199 \t--- Loss: 0.462\n",
      "Iteration: 200 \t--- Loss: 0.473\n",
      "Iteration: 201 \t--- Loss: 0.467\n",
      "Iteration: 202 \t--- Loss: 0.465\n",
      "Iteration: 203 \t--- Loss: 0.459\n",
      "Iteration: 204 \t--- Loss: 0.471\n",
      "Iteration: 205 \t--- Loss: 0.468\n",
      "Iteration: 206 \t--- Loss: 0.464\n",
      "Iteration: 207 \t--- Loss: 0.450\n",
      "Iteration: 208 \t--- Loss: 0.472\n",
      "Iteration: 209 \t--- Loss: 0.466\n",
      "Iteration: 210 \t--- Loss: 0.463\n",
      "Iteration: 211 \t--- Loss: 0.470\n",
      "Iteration: 212 \t--- Loss: 0.473\n",
      "Iteration: 213 \t--- Loss: 0.458\n",
      "Iteration: 214 \t--- Loss: 0.474\n",
      "Iteration: 215 \t--- Loss: 0.456\n",
      "Iteration: 216 \t--- Loss: 0.473\n",
      "Iteration: 217 \t--- Loss: 0.461\n",
      "Iteration: 218 \t--- Loss: 0.474\n",
      "Iteration: 219 \t--- Loss: 0.460\n",
      "Iteration: 220 \t--- Loss: 0.468\n",
      "Iteration: 221 \t--- Loss: 0.469\n",
      "Iteration: 222 \t--- Loss: 0.468\n",
      "Iteration: 223 \t--- Loss: 0.466\n",
      "Iteration: 224 \t--- Loss: 0.464\n",
      "Iteration: 225 \t--- Loss: 0.459\n",
      "Iteration: 226 \t--- Loss: 0.474\n",
      "Iteration: 227 \t--- Loss: 0.470\n",
      "Iteration: 228 \t--- Loss: 0.473\n",
      "Iteration: 229 \t--- Loss: 0.452\n",
      "Iteration: 230 \t--- Loss: 0.472\n",
      "Iteration: 231 \t--- Loss: 0.463\n",
      "Iteration: 232 \t--- Loss: 0.465\n",
      "Iteration: 233 \t--- Loss: 0.466\n",
      "Iteration: 234 \t--- Loss: 0.477\n",
      "Iteration: 235 \t--- Loss: 0.466\n",
      "Iteration: 236 \t--- Loss: 0.467\n",
      "Iteration: 237 \t--- Loss: 0.468\n",
      "Iteration: 238 \t--- Loss: 0.466\n",
      "Iteration: 239 \t--- Loss: 0.467\n",
      "Iteration: 240 \t--- Loss: 0.454\n",
      "Iteration: 241 \t--- Loss: 0.477\n",
      "Iteration: 242 \t--- Loss: 0.463\n",
      "Iteration: 243 \t--- Loss: 0.455\n",
      "Iteration: 244 \t--- Loss: 0.475\n",
      "Iteration: 245 \t--- Loss: 0.464\n",
      "Iteration: 246 \t--- Loss: 0.479\n",
      "Iteration: 247 \t--- Loss: 0.452\n",
      "Iteration: 248 \t--- Loss: 0.467\n",
      "Iteration: 249 \t--- Loss: 0.466\n",
      "Iteration: 250 \t--- Loss: 0.459\n",
      "Iteration: 251 \t--- Loss: 0.476\n",
      "Iteration: 252 \t--- Loss: 0.462\n",
      "Iteration: 253 \t--- Loss: 0.457\n",
      "Iteration: 254 \t--- Loss: 0.463\n",
      "Iteration: 255 \t--- Loss: 0.464\n",
      "Iteration: 256 \t--- Loss: 0.465\n",
      "Iteration: 257 \t--- Loss: 0.470\n",
      "Iteration: 258 \t--- Loss: 0.466\n",
      "Iteration: 259 \t--- Loss: 0.468Iteration: 0 \t--- Loss: 1.246\n",
      "Iteration: 1 \t--- Loss: 1.162\n",
      "Iteration: 2 \t--- Loss: 1.175\n",
      "Iteration: 3 \t--- Loss: 1.089\n",
      "Iteration: 4 \t--- Loss: 0.984\n",
      "Iteration: 5 \t--- Loss: 1.058\n",
      "Iteration: 6 \t--- Loss: 0.994\n",
      "Iteration: 7 \t--- Loss: 0.974\n",
      "Iteration: 8 \t--- Loss: 0.995\n",
      "Iteration: 9 \t--- Loss: 0.979\n",
      "Iteration: 10 \t--- Loss: 1.005\n",
      "Iteration: 11 \t--- Loss: 0.946\n",
      "Iteration: 12 \t--- Loss: 0.977\n",
      "Iteration: 13 \t--- Loss: 0.966\n",
      "Iteration: 14 \t--- Loss: 0.896\n",
      "Iteration: 15 \t--- Loss: 0.867\n",
      "Iteration: 16 \t--- Loss: 0.952\n",
      "Iteration: 17 \t--- Loss: 0.872\n",
      "Iteration: 18 \t--- Loss: 0.839\n",
      "Iteration: 19 \t--- Loss: 0.896\n",
      "Iteration: 20 \t--- Loss: 0.808\n",
      "Iteration: 21 \t--- Loss: 0.885\n",
      "Iteration: 22 \t--- Loss: 0.853\n",
      "Iteration: 23 \t--- Loss: 0.862\n",
      "Iteration: 24 \t--- Loss: 0.828\n",
      "Iteration: 25 \t--- Loss: 0.801\n",
      "Iteration: 26 \t--- Loss: 0.819\n",
      "Iteration: 27 \t--- Loss: 0.762\n",
      "Iteration: 28 \t--- Loss: 0.792\n",
      "Iteration: 29 \t--- Loss: 0.725\n",
      "Iteration: 30 \t--- Loss: 0.727\n",
      "Iteration: 31 \t--- Loss: 0.745\n",
      "Iteration: 32 \t--- Loss: 0.771\n",
      "Iteration: 33 \t--- Loss: 0.695\n",
      "Iteration: 34 \t--- Loss: 0.718\n",
      "Iteration: 35 \t--- Loss: 0.735\n",
      "Iteration: 36 \t--- Loss: 0.843\n",
      "Iteration: 37 \t--- Loss: 1.602\n",
      "Iteration: 38 \t--- Loss: 1.640\n",
      "Iteration: 39 \t--- Loss: 1.639\n",
      "Iteration: 40 \t--- Loss: 1.695\n",
      "Iteration: 41 \t--- Loss: 1.565\n",
      "Iteration: 42 \t--- Loss: 1.480\n",
      "Iteration: 43 \t--- Loss: 1.634\n",
      "Iteration: 44 \t--- Loss: 1.583\n",
      "Iteration: 45 \t--- Loss: 1.580\n",
      "Iteration: 46 \t--- Loss: 1.638\n",
      "Iteration: 47 \t--- Loss: 1.633\n",
      "Iteration: 48 \t--- Loss: 1.544\n",
      "Iteration: 49 \t--- Loss: 1.614\n",
      "Iteration: 50 \t--- Loss: 1.680\n",
      "Iteration: 51 \t--- Loss: 1.505\n",
      "Iteration: 52 \t--- Loss: 1.448\n",
      "Iteration: 53 \t--- Loss: 1.531\n",
      "Iteration: 54 \t--- Loss: 1.350\n",
      "Iteration: 55 \t--- Loss: 1.321\n",
      "Iteration: 56 \t--- Loss: 1.350\n",
      "Iteration: 57 \t--- Loss: 1.423\n",
      "Iteration: 58 \t--- Loss: 1.246\n",
      "Iteration: 59 \t--- Loss: 1.238\n",
      "Iteration: 60 \t--- Loss: 1.195\n",
      "Iteration: 61 \t--- Loss: 1.101\n",
      "Iteration: 62 \t--- Loss: 1.065\n",
      "Iteration: 63 \t--- Loss: 1.036\n",
      "Iteration: 64 \t--- Loss: 1.007\n",
      "Iteration: 65 \t--- Loss: 0.954\n",
      "Iteration: 66 \t--- Loss: 0.932\n",
      "Iteration: 67 \t--- Loss: 0.951\n",
      "Iteration: 68 \t--- Loss: 0.863\n",
      "Iteration: 69 \t--- Loss: 0.889\n",
      "Iteration: 70 \t--- Loss: 0.891\n",
      "Iteration: 71 \t--- Loss: 0.891\n",
      "Iteration: 72 \t--- Loss: 0.846\n",
      "Iteration: 73 \t--- Loss: 0.782\n",
      "Iteration: 74 \t--- Loss: 0.852\n",
      "Iteration: 75 \t--- Loss: 0.830\n",
      "Iteration: 76 \t--- Loss: 0.810\n",
      "Iteration: 77 \t--- Loss: 0.822\n",
      "Iteration: 78 \t--- Loss: 0.773\n",
      "Iteration: 79 \t--- Loss: 0.769\n",
      "Iteration: 80 \t--- Loss: 0.781\n",
      "Iteration: 81 \t--- Loss: 0.747\n",
      "Iteration: 82 \t--- Loss: 0.753\n",
      "Iteration: 83 \t--- Loss: 0.785\n",
      "Iteration: 84 \t--- Loss: 0.805\n",
      "Iteration: 85 \t--- Loss: 0.779\n",
      "Iteration: 86 \t--- Loss: 0.733\n",
      "Iteration: 87 \t--- Loss: 0.802\n",
      "Iteration: 88 \t--- Loss: 0.785\n",
      "Iteration: 89 \t--- Loss: 0.717\n",
      "Iteration: 90 \t--- Loss: 0.839\n",
      "Iteration: 91 \t--- Loss: 0.707\n",
      "Iteration: 92 \t--- Loss: 0.752\n",
      "Iteration: 93 \t--- Loss: 0.746\n",
      "Iteration: 94 \t--- Loss: 0.740\n",
      "Iteration: 95 \t--- Loss: 0.732\n",
      "Iteration: 96 \t--- Loss: 0.779\n",
      "Iteration: 97 \t--- Loss: 0.794\n",
      "Iteration: 98 \t--- Loss: 0.763\n",
      "Iteration: 99 \t--- Loss: 0.765\n",
      "Iteration: 100 \t--- Loss: 0.733\n",
      "Iteration: 101 \t--- Loss: 0.765\n",
      "Iteration: 102 \t--- Loss: 0.750\n",
      "Iteration: 103 \t--- Loss: 0.795\n",
      "Iteration: 104 \t--- Loss: 0.756\n",
      "Iteration: 105 \t--- Loss: 0.731\n",
      "Iteration: 106 \t--- Loss: 0.713\n",
      "Iteration: 107 \t--- Loss: 0.708\n",
      "Iteration: 108 \t--- Loss: 0.722\n",
      "Iteration: 109 \t--- Loss: 0.749\n",
      "Iteration: 110 \t--- Loss: 0.667\n",
      "Iteration: 111 \t--- Loss: 0.722\n",
      "Iteration: 112 \t--- Loss: 0.672\n",
      "Iteration: 113 \t--- Loss: 0.713\n",
      "Iteration: 114 \t--- Loss: 0.706\n",
      "Iteration: 115 \t--- Loss: 0.684\n",
      "Iteration: 116 \t--- Loss: 0.680\n",
      "Iteration: 117 \t--- Loss: 0.707\n",
      "Iteration: 118 \t--- Loss: 0.650\n",
      "Iteration: 119 \t--- Loss: 0.614\n",
      "Iteration: 120 \t--- Loss: 0.632\n",
      "Iteration: 121 \t--- Loss: 0.631\n",
      "Iteration: 122 \t--- Loss: 0.616\n",
      "Iteration: 123 \t--- Loss: 0.642\n",
      "Iteration: 124 \t--- Loss: 0.620\n",
      "Iteration: 125 \t--- Loss: 0.537\n",
      "Iteration: 126 \t--- Loss: 0.600\n",
      "Iteration: 127 \t--- Loss: 0.507\n",
      "Iteration: 128 \t--- Loss: 0.529\n",
      "Iteration: 129 \t--- Loss: 0.503\n",
      "Iteration: 130 \t--- Loss: 0.493\n",
      "Iteration: 131 \t--- Loss: 0.434\n",
      "Iteration: 132 \t--- Loss: 0.486\n",
      "Iteration: 133 \t--- Loss: 0.461\n",
      "Iteration: 134 \t--- Loss: 0.382\n",
      "Iteration: 135 \t--- Loss: 0.412\n",
      "Iteration: 136 \t--- Loss: 0.389\n",
      "Iteration: 137 \t--- Loss: 0.477\n",
      "Iteration: 138 \t--- Loss: 0.366\n",
      "Iteration: 139 \t--- Loss: 0.342\n",
      "Iteration: 140 \t--- Loss: 0.393\n",
      "Iteration: 141 \t--- Loss: 0.393\n",
      "Iteration: 142 \t--- Loss: 0.359\n",
      "Iteration: 143 \t--- Loss: 0.356\n",
      "Iteration: 144 \t--- Loss: 0.365\n",
      "Iteration: 145 \t--- Loss: 0.351\n",
      "Iteration: 146 \t--- Loss: 0.373\n",
      "Iteration: 147 \t--- Loss: 0.363\n",
      "Iteration: 148 \t--- Loss: 0.321\n",
      "Iteration: 149 \t--- Loss: 0.364\n",
      "Iteration: 150 \t--- Loss: 0.393\n",
      "Iteration: 151 \t--- Loss: 0.364\n",
      "Iteration: 152 \t--- Loss: 0.340\n",
      "Iteration: 153 \t--- Loss: 0.299\n",
      "Iteration: 154 \t--- Loss: 0.333\n",
      "Iteration: 155 \t--- Loss: 0.365\n",
      "Iteration: 156 \t--- Loss: 0.308\n",
      "Iteration: 157 \t--- Loss: 0.377\n",
      "Iteration: 158 \t--- Loss: 0.332\n",
      "Iteration: 159 \t--- Loss: 0.321\n",
      "Iteration: 160 \t--- Loss: 0.313\n",
      "Iteration: 161 \t--- Loss: 0.315\n",
      "Iteration: 162 \t--- Loss: 0.329\n",
      "Iteration: 163 \t--- Loss: 0.323\n",
      "Iteration: 164 \t--- Loss: 0.321\n",
      "Iteration: 165 \t--- Loss: 0.308\n",
      "Iteration: 166 \t--- Loss: 0.325\n",
      "Iteration: 167 \t--- Loss: 0.311\n",
      "Iteration: 168 \t--- Loss: 0.293\n",
      "Iteration: 169 \t--- Loss: 0.310\n",
      "Iteration: 170 \t--- Loss: 0.314\n",
      "Iteration: 171 \t--- Loss: 0.324\n",
      "Iteration: 172 \t--- Loss: 0.318\n",
      "Iteration: 173 \t--- Loss: 0.336\n",
      "Iteration: 174 \t--- Loss: 0.328\n",
      "Iteration: 175 \t--- Loss: 0.330\n",
      "Iteration: 176 \t--- Loss: 0.306\n",
      "Iteration: 177 \t--- Loss: 0.297\n",
      "Iteration: 178 \t--- Loss: 0.320\n",
      "Iteration: 179 \t--- Loss: 0.327\n",
      "Iteration: 180 \t--- Loss: 0.315\n",
      "Iteration: 181 \t--- Loss: 0.314\n",
      "Iteration: 182 \t--- Loss: 0.301\n",
      "Iteration: 183 \t--- Loss: 0.339\n",
      "Iteration: 184 \t--- Loss: 0.318\n",
      "Iteration: 185 \t--- Loss: 0.361\n",
      "Iteration: 186 \t--- Loss: 0.340\n",
      "Iteration: 187 \t--- Loss: 0.316\n",
      "Iteration: 188 \t--- Loss: 0.319\n",
      "Iteration: 189 \t--- Loss: 0.306\n",
      "Iteration: 190 \t--- Loss: 0.314\n",
      "Iteration: 191 \t--- Loss: 0.305\n",
      "Iteration: 192 \t--- Loss: 0.308\n",
      "Iteration: 193 \t--- Loss: 0.332\n",
      "Iteration: 194 \t--- Loss: 0.299\n",
      "Iteration: 195 \t--- Loss: 0.308\n",
      "Iteration: 196 \t--- Loss: 0.314\n",
      "Iteration: 197 \t--- Loss: 0.302\n",
      "Iteration: 198 \t--- Loss: 0.312\n",
      "Iteration: 199 \t--- Loss: 0.334\n",
      "Iteration: 200 \t--- Loss: 0.298\n",
      "Iteration: 201 \t--- Loss: 0.330\n",
      "Iteration: 202 \t--- Loss: 0.341\n",
      "Iteration: 203 \t--- Loss: 0.321\n",
      "Iteration: 204 \t--- Loss: 0.316\n",
      "Iteration: 205 \t--- Loss: 0.364\n",
      "Iteration: 206 \t--- Loss: 0.349\n",
      "Iteration: 207 \t--- Loss: 0.410\n",
      "Iteration: 208 \t--- Loss: 0.315\n",
      "Iteration: 209 \t--- Loss: 0.305\n",
      "Iteration: 210 \t--- Loss: 0.326\n",
      "Iteration: 211 \t--- Loss: 0.297\n",
      "Iteration: 212 \t--- Loss: 0.304\n",
      "Iteration: 213 \t--- Loss: 0.312\n",
      "Iteration: 214 \t--- Loss: 0.309\n",
      "Iteration: 215 \t--- Loss: 0.330\n",
      "Iteration: 216 \t--- Loss: 0.298\n",
      "Iteration: 217 \t--- Loss: 0.303\n",
      "Iteration: 218 \t--- Loss: 0.312\n",
      "Iteration: 219 \t--- Loss: 0.285\n",
      "Iteration: 220 \t--- Loss: 0.311\n",
      "Iteration: 221 \t--- Loss: 0.326\n",
      "Iteration: 222 \t--- Loss: 0.300\n",
      "Iteration: 223 \t--- Loss: 0.327\n",
      "Iteration: 224 \t--- Loss: 0.320\n",
      "Iteration: 225 \t--- Loss: 0.300\n",
      "Iteration: 226 \t--- Loss: 0.287\n",
      "Iteration: 227 \t--- Loss: 0.332\n",
      "Iteration: 228 \t--- Loss: 0.350\n",
      "Iteration: 229 \t--- Loss: 0.423\n",
      "Iteration: 230 \t--- Loss: 0.328\n",
      "Iteration: 231 \t--- Loss: 0.285\n",
      "Iteration: 232 \t--- Loss: 0.339\n",
      "Iteration: 233 \t--- Loss: 0.324\n",
      "Iteration: 234 \t--- Loss: 0.363\n",
      "Iteration: 235 \t--- Loss: 0.339\n",
      "Iteration: 236 \t--- Loss: 0.287\n",
      "Iteration: 237 \t--- Loss: 0.286\n",
      "Iteration: 238 \t--- Loss: 0.333\n",
      "Iteration: 239 \t--- Loss: 0.312\n",
      "Iteration: 240 \t--- Loss: 0.365\n",
      "Iteration: 241 \t--- Loss: 0.330\n",
      "Iteration: 242 \t--- Loss: 0.367\n",
      "Iteration: 243 \t--- Loss: 0.298\n",
      "Iteration: 244 \t--- Loss: 0.341\n",
      "Iteration: 245 \t--- Loss: 0.300\n",
      "Iteration: 246 \t--- Loss: 0.291\n",
      "Iteration: 247 \t--- Loss: 0.322\n",
      "Iteration: 248 \t--- Loss: 0.320\n",
      "Iteration: 249 \t--- Loss: 0.346\n",
      "Iteration: 250 \t--- Loss: 0.290\n",
      "Iteration: 251 \t--- Loss: 0.320\n",
      "Iteration: 252 \t--- Loss: 0.310\n",
      "Iteration: 253 \t--- Loss: 0.334\n",
      "Iteration: 254 \t--- Loss: 0.336\n",
      "Iteration: 255 \t--- Loss: 0.339\n",
      "Iteration: 256 \t--- Loss: 0.312\n",
      "Iteration: 257 \t--- Loss: 0.343\n",
      "Iteration: 258 \t--- Loss: 0.322\n",
      "Iteration: 259 \t--- Loss: 0.358"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:24<00:00, 84.37s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.461\n",
      "Iteration: 261 \t--- Loss: 0.471\n",
      "Iteration: 262 \t--- Loss: 0.466\n",
      "Iteration: 263 \t--- Loss: 0.467\n",
      "Iteration: 264 \t--- Loss: 0.463\n",
      "Iteration: 265 \t--- Loss: 0.469\n",
      "Iteration: 266 \t--- Loss: 0.452\n",
      "Iteration: 267 \t--- Loss: 0.473\n",
      "Iteration: 268 \t--- Loss: 0.472\n",
      "Iteration: 269 \t--- Loss: 0.454\n",
      "Iteration: 270 \t--- Loss: 0.469\n",
      "Iteration: 271 \t--- Loss: 0.468\n",
      "Iteration: 272 \t--- Loss: 0.457\n",
      "Iteration: 273 \t--- Loss: 0.467\n",
      "Iteration: 274 \t--- Loss: 0.474\n",
      "Iteration: 275 \t--- Loss: 0.470\n",
      "Iteration: 276 \t--- Loss: 0.455\n",
      "Iteration: 277 \t--- Loss: 0.464\n",
      "Iteration: 278 \t--- Loss: 0.468\n",
      "Iteration: 279 \t--- Loss: 0.475\n",
      "Iteration: 280 \t--- Loss: 0.460\n",
      "Iteration: 281 \t--- Loss: 0.471\n",
      "Iteration: 282 \t--- Loss: 0.465\n",
      "Iteration: 283 \t--- Loss: 0.466\n",
      "Iteration: 284 \t--- Loss: 0.474\n",
      "Iteration: 285 \t--- Loss: 0.469\n",
      "Iteration: 286 \t--- Loss: 0.460\n",
      "Iteration: 287 \t--- Loss: 0.460\n",
      "Iteration: 288 \t--- Loss: 0.457\n",
      "Iteration: 289 \t--- Loss: 0.457\n",
      "Iteration: 290 \t--- Loss: 0.476\n",
      "Iteration: 291 \t--- Loss: 0.467\n",
      "Iteration: 292 \t--- Loss: 0.472\n",
      "Iteration: 293 \t--- Loss: 0.465\n",
      "Iteration: 294 \t--- Loss: 0.467\n",
      "Iteration: 295 \t--- Loss: 0.467\n",
      "Iteration: 296 \t--- Loss: 0.472\n",
      "Iteration: 297 \t--- Loss: 0.461\n",
      "Iteration: 298 \t--- Loss: 0.475\n",
      "Iteration: 299 \t--- Loss: 0.460\n",
      "Iteration: 300 \t--- Loss: 0.460\n",
      "Iteration: 301 \t--- Loss: 0.479\n",
      "Iteration: 302 \t--- Loss: 0.474\n",
      "Iteration: 303 \t--- Loss: 0.466\n",
      "Iteration: 304 \t--- Loss: 0.465\n",
      "Iteration: 305 \t--- Loss: 0.454\n",
      "Iteration: 306 \t--- Loss: 0.464\n",
      "Iteration: 307 \t--- Loss: 0.474\n",
      "Iteration: 308 \t--- Loss: 0.456\n",
      "Iteration: 309 \t--- Loss: 0.463\n",
      "Iteration: 310 \t--- Loss: 0.473\n",
      "Iteration: 311 \t--- Loss: 0.468\n",
      "Iteration: 312 \t--- Loss: 0.476\n",
      "Iteration: 313 \t--- Loss: 0.478\n",
      "Iteration: 314 \t--- Loss: 0.465\n",
      "Iteration: 315 \t--- Loss: 0.480\n",
      "Iteration: 316 \t--- Loss: 0.473\n",
      "Iteration: 317 \t--- Loss: 0.472\n",
      "Iteration: 318 \t--- Loss: 0.460\n",
      "Iteration: 319 \t--- Loss: 0.465\n",
      "Iteration: 320 \t--- Loss: 0.460\n",
      "Iteration: 321 \t--- Loss: 0.459\n",
      "Iteration: 322 \t--- Loss: 0.476\n",
      "Iteration: 323 \t--- Loss: 0.465\n",
      "Iteration: 324 \t--- Loss: 0.470\n",
      "Iteration: 325 \t--- Loss: 0.473\n",
      "Iteration: 326 \t--- Loss: 0.471\n",
      "Iteration: 327 \t--- Loss: 0.476\n",
      "Iteration: 328 \t--- Loss: 0.467\n",
      "Iteration: 329 \t--- Loss: 0.462\n",
      "Iteration: 330 \t--- Loss: 0.465\n",
      "Iteration: 331 \t--- Loss: 0.472\n",
      "Iteration: 332 \t--- Loss: 0.458\n",
      "Iteration: 333 \t--- Loss: 0.459\n",
      "Iteration: 334 \t--- Loss: 0.479\n",
      "Iteration: 335 \t--- Loss: 0.469\n",
      "Iteration: 336 \t--- Loss: 0.463\n",
      "Iteration: 337 \t--- Loss: 0.472\n",
      "Iteration: 338 \t--- Loss: 0.465\n",
      "Iteration: 339 \t--- Loss: 0.473\n",
      "Iteration: 340 \t--- Loss: 0.462\n",
      "Iteration: 341 \t--- Loss: 0.471\n",
      "Iteration: 342 \t--- Loss: 0.472\n",
      "Iteration: 343 \t--- Loss: 0.458\n",
      "Iteration: 344 \t--- Loss: 0.469\n",
      "Iteration: 345 \t--- Loss: 0.469\n",
      "Iteration: 346 \t--- Loss: 0.459\n",
      "Iteration: 347 \t--- Loss: 0.475\n",
      "Iteration: 348 \t--- Loss: 0.470\n",
      "Iteration: 349 \t--- Loss: 0.477\n",
      "Iteration: 350 \t--- Loss: 0.473\n",
      "Iteration: 351 \t--- Loss: 0.473\n",
      "Iteration: 352 \t--- Loss: 0.468\n",
      "Iteration: 353 \t--- Loss: 0.456\n",
      "Iteration: 354 \t--- Loss: 0.469\n",
      "Iteration: 355 \t--- Loss: 0.467\n",
      "Iteration: 356 \t--- Loss: 0.470\n",
      "Iteration: 357 \t--- Loss: 0.462\n",
      "Iteration: 358 \t--- Loss: 0.464\n",
      "Iteration: 359 \t--- Loss: 0.481\n",
      "Iteration: 360 \t--- Loss: 0.458\n",
      "Iteration: 361 \t--- Loss: 0.466\n",
      "Iteration: 362 \t--- Loss: 0.476\n",
      "Iteration: 363 \t--- Loss: 0.471\n",
      "Iteration: 364 \t--- Loss: 0.458\n",
      "Iteration: 365 \t--- Loss: 0.468\n",
      "Iteration: 366 \t--- Loss: 0.468\n",
      "Iteration: 367 \t--- Loss: 0.469\n",
      "Iteration: 368 \t--- Loss: 0.463\n",
      "Iteration: 369 \t--- Loss: 0.464\n",
      "Iteration: 370 \t--- Loss: 0.478\n",
      "Iteration: 371 \t--- Loss: 0.472\n",
      "Iteration: 372 \t--- Loss: 0.476\n",
      "Iteration: 373 \t--- Loss: 0.455\n",
      "Iteration: 374 \t--- Loss: 0.458\n",
      "Iteration: 375 \t--- Loss: 0.467\n",
      "Iteration: 376 \t--- Loss: 0.468\n",
      "Iteration: 377 \t--- Loss: 0.479\n",
      "Iteration: 378 \t--- Loss: 0.464\n",
      "Iteration: 379 \t--- Loss: 0.471\n",
      "Iteration: 380 \t--- Loss: 0.469\n",
      "Iteration: 381 \t--- Loss: 0.472\n",
      "Iteration: 382 \t--- Loss: 0.465\n",
      "Iteration: 383 \t--- Loss: 0.471\n",
      "Iteration: 384 \t--- Loss: 0.464\n",
      "Iteration: 385 \t--- Loss: 0.479\n",
      "Iteration: 386 \t--- Loss: 0.470\n",
      "Iteration: 387 \t--- Loss: 0.468\n",
      "Iteration: 388 \t--- Loss: 0.471\n",
      "Iteration: 389 \t--- Loss: 0.479\n",
      "Iteration: 390 \t--- Loss: 0.466\n",
      "Iteration: 391 \t--- Loss: 0.464\n",
      "Iteration: 392 \t--- Loss: 0.454\n",
      "Iteration: 393 \t--- Loss: 0.471\n",
      "Iteration: 394 \t--- Loss: 0.481\n",
      "Iteration: 395 \t--- Loss: 0.471\n",
      "Iteration: 396 \t--- Loss: 0.465\n",
      "Iteration: 397 \t--- Loss: 0.471\n",
      "Iteration: 398 \t--- Loss: 0.479\n",
      "Iteration: 399 \t--- Loss: 0.470\n",
      "Iteration: 400 \t--- Loss: 0.478\n",
      "Iteration: 401 \t--- Loss: 0.452\n",
      "Iteration: 402 \t--- Loss: 0.467\n",
      "Iteration: 403 \t--- Loss: 0.480\n",
      "Iteration: 404 \t--- Loss: 0.464\n",
      "Iteration: 405 \t--- Loss: 0.476\n",
      "Iteration: 406 \t--- Loss: 0.458\n",
      "Iteration: 407 \t--- Loss: 0.470\n",
      "Iteration: 408 \t--- Loss: 0.471\n",
      "Iteration: 409 \t--- Loss: 0.490\n",
      "Iteration: 410 \t--- Loss: 0.462\n",
      "Iteration: 411 \t--- Loss: 0.469\n",
      "Iteration: 412 \t--- Loss: 0.460\n",
      "Iteration: 413 \t--- Loss: 0.472\n",
      "Iteration: 414 \t--- Loss: 0.462\n",
      "Iteration: 415 \t--- Loss: 0.469\n",
      "Iteration: 416 \t--- Loss: 0.464\n",
      "Iteration: 417 \t--- Loss: 0.473\n",
      "Iteration: 418 \t--- Loss: 0.473\n",
      "Iteration: 419 \t--- Loss: 0.477\n",
      "Iteration: 420 \t--- Loss: 0.472\n",
      "Iteration: 421 \t--- Loss: 0.471\n",
      "Iteration: 422 \t--- Loss: 0.457\n",
      "Iteration: 423 \t--- Loss: 0.470\n",
      "Iteration: 424 \t--- Loss: 0.454\n",
      "Iteration: 425 \t--- Loss: 0.462\n",
      "Iteration: 426 \t--- Loss: 0.475\n",
      "Iteration: 427 \t--- Loss: 0.471\n",
      "Iteration: 428 \t--- Loss: 0.474\n",
      "Iteration: 429 \t--- Loss: 0.472\n",
      "Iteration: 430 \t--- Loss: 0.469\n",
      "Iteration: 431 \t--- Loss: 0.469\n",
      "Iteration: 432 \t--- Loss: 0.477\n",
      "Iteration: 433 \t--- Loss: 0.460\n",
      "Iteration: 434 \t--- Loss: 0.477\n",
      "Iteration: 435 \t--- Loss: 0.459\n",
      "Iteration: 436 \t--- Loss: 0.471\n",
      "Iteration: 437 \t--- Loss: 0.477\n",
      "Iteration: 438 \t--- Loss: 0.449\n",
      "Iteration: 439 \t--- Loss: 0.465\n",
      "Iteration: 440 \t--- Loss: 0.479\n",
      "Iteration: 441 \t--- Loss: 0.459\n",
      "Iteration: 442 \t--- Loss: 0.470\n",
      "Iteration: 443 \t--- Loss: 0.462\n",
      "Iteration: 444 \t--- Loss: 0.460\n",
      "Iteration: 445 \t--- Loss: 0.468\n",
      "Iteration: 446 \t--- Loss: 0.471\n",
      "Iteration: 447 \t--- Loss: 0.466\n",
      "Iteration: 448 \t--- Loss: 0.461\n",
      "Iteration: 449 \t--- Loss: 0.470\n",
      "Iteration: 450 \t--- Loss: 0.471\n",
      "Iteration: 451 \t--- Loss: 0.466\n",
      "Iteration: 452 \t--- Loss: 0.460\n",
      "Iteration: 453 \t--- Loss: 0.465\n",
      "Iteration: 454 \t--- Loss: 0.471\n",
      "Iteration: 455 \t--- Loss: 0.461\n",
      "Iteration: 456 \t--- Loss: 0.453\n",
      "Iteration: 457 \t--- Loss: 0.468\n",
      "Iteration: 458 \t--- Loss: 0.472\n",
      "Iteration: 459 \t--- Loss: 0.450\n",
      "Iteration: 460 \t--- Loss: 0.474\n",
      "Iteration: 461 \t--- Loss: 0.467\n",
      "Iteration: 462 \t--- Loss: 0.467\n",
      "Iteration: 463 \t--- Loss: 0.469\n",
      "Iteration: 464 \t--- Loss: 0.472\n",
      "Iteration: 465 \t--- Loss: 0.466\n",
      "Iteration: 466 \t--- Loss: 0.456\n",
      "Iteration: 467 \t--- Loss: 0.468\n",
      "Iteration: 468 \t--- Loss: 0.469\n",
      "Iteration: 469 \t--- Loss: 0.466\n",
      "Iteration: 470 \t--- Loss: 0.469\n",
      "Iteration: 471 \t--- Loss: 0.480\n",
      "Iteration: 472 \t--- Loss: 0.478\n",
      "Iteration: 473 \t--- Loss: 0.466\n",
      "Iteration: 474 \t--- Loss: 0.466\n",
      "Iteration: 475 \t--- Loss: 0.463\n",
      "Iteration: 476 \t--- Loss: 0.474\n",
      "Iteration: 477 \t--- Loss: 0.470\n",
      "Iteration: 478 \t--- Loss: 0.474\n",
      "Iteration: 479 \t--- Loss: 0.462\n",
      "Iteration: 480 \t--- Loss: 0.471\n",
      "Iteration: 481 \t--- Loss: 0.461\n",
      "Iteration: 482 \t--- Loss: 0.458\n",
      "Iteration: 483 \t--- Loss: 0.464\n",
      "Iteration: 484 \t--- Loss: 0.456\n",
      "Iteration: 485 \t--- Loss: 0.464\n",
      "Iteration: 486 \t--- Loss: 0.473\n",
      "Iteration: 487 \t--- Loss: 0.471\n",
      "Iteration: 488 \t--- Loss: 0.471\n",
      "Iteration: 489 \t--- Loss: 0.472\n",
      "Iteration: 490 \t--- Loss: 0.476\n",
      "Iteration: 491 \t--- Loss: 0.471\n",
      "Iteration: 492 \t--- Loss: 0.458\n",
      "Iteration: 493 \t--- Loss: 0.472\n",
      "Iteration: 494 \t--- Loss: 0.468\n",
      "Iteration: 495 \t--- Loss: 0.465\n",
      "Iteration: 496 \t--- Loss: 0.468\n",
      "Iteration: 497 \t--- Loss: 0.465\n",
      "Iteration: 498 \t--- Loss: 0.465\n",
      "Iteration: 499 \t--- Loss: 0.465\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it][Parallel(n_jobs=5)]: Done  12 tasks      | elapsed:  5.0min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [01:36<00:00, 96.47s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.333\n",
      "Iteration: 261 \t--- Loss: 0.327\n",
      "Iteration: 262 \t--- Loss: 0.304\n",
      "Iteration: 263 \t--- Loss: 0.306\n",
      "Iteration: 264 \t--- Loss: 0.295\n",
      "Iteration: 265 \t--- Loss: 0.378\n",
      "Iteration: 266 \t--- Loss: 0.382\n",
      "Iteration: 267 \t--- Loss: 0.488\n",
      "Iteration: 268 \t--- Loss: 0.302\n",
      "Iteration: 269 \t--- Loss: 0.325\n",
      "Iteration: 270 \t--- Loss: 0.310\n",
      "Iteration: 271 \t--- Loss: 0.300\n",
      "Iteration: 272 \t--- Loss: 0.325\n",
      "Iteration: 273 \t--- Loss: 0.280\n",
      "Iteration: 274 \t--- Loss: 0.305\n",
      "Iteration: 275 \t--- Loss: 0.305\n",
      "Iteration: 276 \t--- Loss: 0.300\n",
      "Iteration: 277 \t--- Loss: 0.269\n",
      "Iteration: 278 \t--- Loss: 0.286\n",
      "Iteration: 279 \t--- Loss: 0.293\n",
      "Iteration: 280 \t--- Loss: 0.311\n",
      "Iteration: 281 \t--- Loss: 0.314\n",
      "Iteration: 282 \t--- Loss: 0.334\n",
      "Iteration: 283 \t--- Loss: 0.336\n",
      "Iteration: 284 \t--- Loss: 0.335\n",
      "Iteration: 285 \t--- Loss: 0.314\n",
      "Iteration: 286 \t--- Loss: 0.341\n",
      "Iteration: 287 \t--- Loss: 0.360\n",
      "Iteration: 288 \t--- Loss: 0.299\n",
      "Iteration: 289 \t--- Loss: 0.280\n",
      "Iteration: 290 \t--- Loss: 0.301\n",
      "Iteration: 291 \t--- Loss: 0.302\n",
      "Iteration: 292 \t--- Loss: 0.313\n",
      "Iteration: 293 \t--- Loss: 0.293\n",
      "Iteration: 294 \t--- Loss: 0.293\n",
      "Iteration: 295 \t--- Loss: 0.288\n",
      "Iteration: 296 \t--- Loss: 0.322\n",
      "Iteration: 297 \t--- Loss: 0.321\n",
      "Iteration: 298 \t--- Loss: 0.375\n",
      "Iteration: 299 \t--- Loss: 0.355\n",
      "Iteration: 300 \t--- Loss: 0.427\n",
      "Iteration: 301 \t--- Loss: 0.311\n",
      "Iteration: 302 \t--- Loss: 0.385\n",
      "Iteration: 303 \t--- Loss: 0.315\n",
      "Iteration: 304 \t--- Loss: 0.328\n",
      "Iteration: 305 \t--- Loss: 0.308\n",
      "Iteration: 306 \t--- Loss: 0.290\n",
      "Iteration: 307 \t--- Loss: 0.319\n",
      "Iteration: 308 \t--- Loss: 0.309\n",
      "Iteration: 309 \t--- Loss: 0.305\n",
      "Iteration: 310 \t--- Loss: 0.343\n",
      "Iteration: 311 \t--- Loss: 0.312\n",
      "Iteration: 312 \t--- Loss: 0.312\n",
      "Iteration: 313 \t--- Loss: 0.305\n",
      "Iteration: 314 \t--- Loss: 0.315\n",
      "Iteration: 315 \t--- Loss: 0.311\n",
      "Iteration: 316 \t--- Loss: 0.377\n",
      "Iteration: 317 \t--- Loss: 0.330\n",
      "Iteration: 318 \t--- Loss: 0.370\n",
      "Iteration: 319 \t--- Loss: 0.330\n",
      "Iteration: 320 \t--- Loss: 0.335\n",
      "Iteration: 321 \t--- Loss: 0.299\n",
      "Iteration: 322 \t--- Loss: 0.322\n",
      "Iteration: 323 \t--- Loss: 0.326\n",
      "Iteration: 324 \t--- Loss: 0.320\n",
      "Iteration: 325 \t--- Loss: 0.301\n",
      "Iteration: 326 \t--- Loss: 0.293\n",
      "Iteration: 327 \t--- Loss: 0.363\n",
      "Iteration: 328 \t--- Loss: 0.335\n",
      "Iteration: 329 \t--- Loss: 0.358\n",
      "Iteration: 330 \t--- Loss: 0.305\n",
      "Iteration: 331 \t--- Loss: 0.302\n",
      "Iteration: 332 \t--- Loss: 0.318\n",
      "Iteration: 333 \t--- Loss: 0.299\n",
      "Iteration: 334 \t--- Loss: 0.306\n",
      "Iteration: 335 \t--- Loss: 0.339\n",
      "Iteration: 336 \t--- Loss: 0.316\n",
      "Iteration: 337 \t--- Loss: 0.355\n",
      "Iteration: 338 \t--- Loss: 0.298\n",
      "Iteration: 339 \t--- Loss: 0.366\n",
      "Iteration: 340 \t--- Loss: 0.321\n",
      "Iteration: 341 \t--- Loss: 0.354\n",
      "Iteration: 342 \t--- Loss: 0.336\n",
      "Iteration: 343 \t--- Loss: 0.336\n",
      "Iteration: 344 \t--- Loss: 0.327\n",
      "Iteration: 345 \t--- Loss: 0.354\n",
      "Iteration: 346 \t--- Loss: 0.322\n",
      "Iteration: 347 \t--- Loss: 0.395\n",
      "Iteration: 348 \t--- Loss: 0.330\n",
      "Iteration: 349 \t--- Loss: 0.306\n",
      "Iteration: 350 \t--- Loss: 0.278\n",
      "Iteration: 351 \t--- Loss: 0.296\n",
      "Iteration: 352 \t--- Loss: 0.308\n",
      "Iteration: 353 \t--- Loss: 0.341\n",
      "Iteration: 354 \t--- Loss: 0.282\n",
      "Iteration: 355 \t--- Loss: 0.300\n",
      "Iteration: 356 \t--- Loss: 0.289\n",
      "Iteration: 357 \t--- Loss: 0.309\n",
      "Iteration: 358 \t--- Loss: 0.322\n",
      "Iteration: 359 \t--- Loss: 0.293\n",
      "Iteration: 360 \t--- Loss: 0.325\n",
      "Iteration: 361 \t--- Loss: 0.397\n",
      "Iteration: 362 \t--- Loss: 0.353\n",
      "Iteration: 363 \t--- Loss: 0.440\n",
      "Iteration: 364 \t--- Loss: 0.319\n",
      "Iteration: 365 \t--- Loss: 0.319\n",
      "Iteration: 366 \t--- Loss: 0.307\n",
      "Iteration: 367 \t--- Loss: 0.322\n",
      "Iteration: 368 \t--- Loss: 0.323\n",
      "Iteration: 369 \t--- Loss: 0.360\n",
      "Iteration: 370 \t--- Loss: 0.312\n",
      "Iteration: 371 \t--- Loss: 0.308\n",
      "Iteration: 372 \t--- Loss: 0.323\n",
      "Iteration: 373 \t--- Loss: 0.334\n",
      "Iteration: 374 \t--- Loss: 0.315\n",
      "Iteration: 375 \t--- Loss: 0.397\n",
      "Iteration: 376 \t--- Loss: 0.308\n",
      "Iteration: 377 \t--- Loss: 0.337\n",
      "Iteration: 378 \t--- Loss: 0.284\n",
      "Iteration: 379 \t--- Loss: 0.313\n",
      "Iteration: 380 \t--- Loss: 0.302\n",
      "Iteration: 381 \t--- Loss: 0.332\n",
      "Iteration: 382 \t--- Loss: 0.317\n",
      "Iteration: 383 \t--- Loss: 0.317\n",
      "Iteration: 384 \t--- Loss: 0.294\n",
      "Iteration: 385 \t--- Loss: 0.312\n",
      "Iteration: 386 \t--- Loss: 0.298\n",
      "Iteration: 387 \t--- Loss: 0.332\n",
      "Iteration: 388 \t--- Loss: 0.347\n",
      "Iteration: 389 \t--- Loss: 0.442\n",
      "Iteration: 390 \t--- Loss: 0.313\n",
      "Iteration: 391 \t--- Loss: 0.330\n",
      "Iteration: 392 \t--- Loss: 0.304\n",
      "Iteration: 393 \t--- Loss: 0.302\n",
      "Iteration: 394 \t--- Loss: 0.333\n",
      "Iteration: 395 \t--- Loss: 0.313\n",
      "Iteration: 396 \t--- Loss: 0.294\n",
      "Iteration: 397 \t--- Loss: 0.303\n",
      "Iteration: 398 \t--- Loss: 0.312\n",
      "Iteration: 399 \t--- Loss: 0.309\n",
      "Iteration: 400 \t--- Loss: 0.308\n",
      "Iteration: 401 \t--- Loss: 0.319\n",
      "Iteration: 402 \t--- Loss: 0.325\n",
      "Iteration: 403 \t--- Loss: 0.336\n",
      "Iteration: 404 \t--- Loss: 0.361\n",
      "Iteration: 405 \t--- Loss: 0.304\n",
      "Iteration: 406 \t--- Loss: 0.303\n",
      "Iteration: 407 \t--- Loss: 0.291\n",
      "Iteration: 408 \t--- Loss: 0.317\n",
      "Iteration: 409 \t--- Loss: 0.311\n",
      "Iteration: 410 \t--- Loss: 0.294\n",
      "Iteration: 411 \t--- Loss: 0.339\n",
      "Iteration: 412 \t--- Loss: 0.324\n",
      "Iteration: 413 \t--- Loss: 0.316\n",
      "Iteration: 414 \t--- Loss: 0.300\n",
      "Iteration: 415 \t--- Loss: 0.341\n",
      "Iteration: 416 \t--- Loss: 0.330\n",
      "Iteration: 417 \t--- Loss: 0.354\n",
      "Iteration: 418 \t--- Loss: 0.317\n",
      "Iteration: 419 \t--- Loss: 0.301\n",
      "Iteration: 420 \t--- Loss: 0.298\n",
      "Iteration: 421 \t--- Loss: 0.316\n",
      "Iteration: 422 \t--- Loss: 0.301\n",
      "Iteration: 423 \t--- Loss: 0.305\n",
      "Iteration: 424 \t--- Loss: 0.332\n",
      "Iteration: 425 \t--- Loss: 0.344\n",
      "Iteration: 426 \t--- Loss: 0.343\n",
      "Iteration: 427 \t--- Loss: 0.370\n",
      "Iteration: 428 \t--- Loss: 0.298\n",
      "Iteration: 429 \t--- Loss: 0.297\n",
      "Iteration: 430 \t--- Loss: 0.303\n",
      "Iteration: 431 \t--- Loss: 0.314\n",
      "Iteration: 432 \t--- Loss: 0.316\n",
      "Iteration: 433 \t--- Loss: 0.328\n",
      "Iteration: 434 \t--- Loss: 0.306\n",
      "Iteration: 435 \t--- Loss: 0.318\n",
      "Iteration: 436 \t--- Loss: 0.300\n",
      "Iteration: 437 \t--- Loss: 0.325\n",
      "Iteration: 438 \t--- Loss: 0.331\n",
      "Iteration: 439 \t--- Loss: 0.352\n",
      "Iteration: 440 \t--- Loss: 0.305\n",
      "Iteration: 441 \t--- Loss: 0.327\n",
      "Iteration: 442 \t--- Loss: 0.327\n",
      "Iteration: 443 \t--- Loss: 0.305\n",
      "Iteration: 444 \t--- Loss: 0.320\n",
      "Iteration: 445 \t--- Loss: 0.348\n",
      "Iteration: 446 \t--- Loss: 0.315\n",
      "Iteration: 447 \t--- Loss: 0.358\n",
      "Iteration: 448 \t--- Loss: 0.331\n",
      "Iteration: 449 \t--- Loss: 0.389\n",
      "Iteration: 450 \t--- Loss: 0.323\n",
      "Iteration: 451 \t--- Loss: 0.332\n",
      "Iteration: 452 \t--- Loss: 0.289\n",
      "Iteration: 453 \t--- Loss: 0.378\n",
      "Iteration: 454 \t--- Loss: 0.297\n",
      "Iteration: 455 \t--- Loss: 0.333\n",
      "Iteration: 456 \t--- Loss: 0.308\n",
      "Iteration: 457 \t--- Loss: 0.336\n",
      "Iteration: 458 \t--- Loss: 0.301\n",
      "Iteration: 459 \t--- Loss: 0.299\n",
      "Iteration: 460 \t--- Loss: 0.313\n",
      "Iteration: 461 \t--- Loss: 0.288\n",
      "Iteration: 462 \t--- Loss: 0.328\n",
      "Iteration: 463 \t--- Loss: 0.308\n",
      "Iteration: 464 \t--- Loss: 0.344\n",
      "Iteration: 465 \t--- Loss: 0.298\n",
      "Iteration: 466 \t--- Loss: 0.358\n",
      "Iteration: 467 \t--- Loss: 0.323\n",
      "Iteration: 468 \t--- Loss: 0.313\n",
      "Iteration: 469 \t--- Loss: 0.331\n",
      "Iteration: 470 \t--- Loss: 0.273\n",
      "Iteration: 471 \t--- Loss: 0.311\n",
      "Iteration: 472 \t--- Loss: 0.322\n",
      "Iteration: 473 \t--- Loss: 0.318\n",
      "Iteration: 474 \t--- Loss: 0.332\n",
      "Iteration: 475 \t--- Loss: 0.335\n",
      "Iteration: 476 \t--- Loss: 0.351\n",
      "Iteration: 477 \t--- Loss: 0.444\n",
      "Iteration: 478 \t--- Loss: 0.319\n",
      "Iteration: 479 \t--- Loss: 0.322\n",
      "Iteration: 480 \t--- Loss: 0.318\n",
      "Iteration: 481 \t--- Loss: 0.306\n",
      "Iteration: 482 \t--- Loss: 0.294\n",
      "Iteration: 483 \t--- Loss: 0.297\n",
      "Iteration: 484 \t--- Loss: 0.290\n",
      "Iteration: 485 \t--- Loss: 0.296\n",
      "Iteration: 486 \t--- Loss: 0.285\n",
      "Iteration: 487 \t--- Loss: 0.313\n",
      "Iteration: 488 \t--- Loss: 0.318\n",
      "Iteration: 489 \t--- Loss: 0.317\n",
      "Iteration: 490 \t--- Loss: 0.304\n",
      "Iteration: 491 \t--- Loss: 0.315\n",
      "Iteration: 492 \t--- Loss: 0.378\n",
      "Iteration: 493 \t--- Loss: 0.314\n",
      "Iteration: 494 \t--- Loss: 0.318\n",
      "Iteration: 495 \t--- Loss: 0.299\n",
      "Iteration: 496 \t--- Loss: 0.340\n",
      "Iteration: 497 \t--- Loss: 0.290\n",
      "Iteration: 498 \t--- Loss: 0.275\n",
      "Iteration: 499 \t--- Loss: 0.296\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.55s/it][Parallel(n_jobs=5)]: Done  13 tasks      | elapsed:  5.3min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.301\n",
      "Iteration: 1 \t--- Loss: 0.294\n",
      "Iteration: 2 \t--- Loss: 0.306\n",
      "Iteration: 3 \t--- Loss: 0.301\n",
      "Iteration: 4 \t--- Loss: 0.296\n",
      "Iteration: 5 \t--- Loss: 0.280\n",
      "Iteration: 6 \t--- Loss: 0.295\n",
      "Iteration: 7 \t--- Loss: 0.250\n",
      "Iteration: 8 \t--- Loss: 0.239\n",
      "Iteration: 9 \t--- Loss: 0.283\n",
      "Iteration: 10 \t--- Loss: 0.239\n",
      "Iteration: 11 \t--- Loss: 0.256\n",
      "Iteration: 12 \t--- Loss: 0.241\n",
      "Iteration: 13 \t--- Loss: 0.243\n",
      "Iteration: 14 \t--- Loss: 0.216\n",
      "Iteration: 15 \t--- Loss: 0.233\n",
      "Iteration: 16 \t--- Loss: 0.217\n",
      "Iteration: 17 \t--- Loss: 0.216\n",
      "Iteration: 18 \t--- Loss: 0.215\n",
      "Iteration: 19 \t--- Loss: 0.209\n",
      "Iteration: 20 \t--- Loss: 0.205\n",
      "Iteration: 21 \t--- Loss: 0.200\n",
      "Iteration: 22 \t--- Loss: 0.219\n",
      "Iteration: 23 \t--- Loss: 0.197\n",
      "Iteration: 24 \t--- Loss: 0.193\n",
      "Iteration: 25 \t--- Loss: 0.206\n",
      "Iteration: 26 \t--- Loss: 0.195\n",
      "Iteration: 27 \t--- Loss: 0.191\n",
      "Iteration: 28 \t--- Loss: 0.163\n",
      "Iteration: 29 \t--- Loss: 0.144\n",
      "Iteration: 30 \t--- Loss: 0.165\n",
      "Iteration: 31 \t--- Loss: 0.146\n",
      "Iteration: 32 \t--- Loss: 0.165\n",
      "Iteration: 33 \t--- Loss: 0.139\n",
      "Iteration: 34 \t--- Loss: 0.141\n",
      "Iteration: 35 \t--- Loss: 0.143\n",
      "Iteration: 36 \t--- Loss: 0.227\n",
      "Iteration: 37 \t--- Loss: 0.226\n",
      "Iteration: 38 \t--- Loss: 0.243\n",
      "Iteration: 39 \t--- Loss: 0.236\n",
      "Iteration: 40 \t--- Loss: 0.203\n",
      "Iteration: 41 \t--- Loss: 0.220\n",
      "Iteration: 42 \t--- Loss: 0.237\n",
      "Iteration: 43 \t--- Loss: 0.230\n",
      "Iteration: 44 \t--- Loss: 0.208\n",
      "Iteration: 45 \t--- Loss: 0.213\n",
      "Iteration: 46 \t--- Loss: 0.213\n",
      "Iteration: 47 \t--- Loss: 0.204\n",
      "Iteration: 48 \t--- Loss: 0.214\n",
      "Iteration: 49 \t--- Loss: 0.211\n",
      "Iteration: 50 \t--- Loss: 0.207\n",
      "Iteration: 51 \t--- Loss: 0.213\n",
      "Iteration: 52 \t--- Loss: 0.207\n",
      "Iteration: 53 \t--- Loss: 0.186\n",
      "Iteration: 54 \t--- Loss: 0.181\n",
      "Iteration: 55 \t--- Loss: 0.158\n",
      "Iteration: 56 \t--- Loss: 0.133\n",
      "Iteration: 57 \t--- Loss: 0.141\n",
      "Iteration: 58 \t--- Loss: 0.126\n",
      "Iteration: 59 \t--- Loss: 0.117\n",
      "Iteration: 60 \t--- Loss: 0.138\n",
      "Iteration: 61 \t--- Loss: 0.163\n",
      "Iteration: 62 \t--- Loss: 0.198\n",
      "Iteration: 63 \t--- Loss: 0.146\n",
      "Iteration: 64 \t--- Loss: 0.109\n",
      "Iteration: 65 \t--- Loss: 0.112\n",
      "Iteration: 66 \t--- Loss: 0.180\n",
      "Iteration: 67 \t--- Loss: 0.202\n",
      "Iteration: 68 \t--- Loss: 0.198\n",
      "Iteration: 69 \t--- Loss: 0.215\n",
      "Iteration: 70 \t--- Loss: 0.214\n",
      "Iteration: 71 \t--- Loss: 0.205\n",
      "Iteration: 72 \t--- Loss: 0.198\n",
      "Iteration: 73 \t--- Loss: 0.192\n",
      "Iteration: 74 \t--- Loss: 0.215\n",
      "Iteration: 75 \t--- Loss: 0.199\n",
      "Iteration: 76 \t--- Loss: 0.182\n",
      "Iteration: 77 \t--- Loss: 0.193\n",
      "Iteration: 78 \t--- Loss: 0.172\n",
      "Iteration: 79 \t--- Loss: 0.165\n",
      "Iteration: 80 \t--- Loss: 0.173\n",
      "Iteration: 81 \t--- Loss: 0.156\n",
      "Iteration: 82 \t--- Loss: 0.170\n",
      "Iteration: 83 \t--- Loss: 0.167\n",
      "Iteration: 84 \t--- Loss: 0.157\n",
      "Iteration: 85 \t--- Loss: 0.135\n",
      "Iteration: 86 \t--- Loss: 0.117\n",
      "Iteration: 87 \t--- Loss: 0.108\n",
      "Iteration: 88 \t--- Loss: 0.100\n",
      "Iteration: 89 \t--- Loss: 0.078\n",
      "Iteration: 90 \t--- Loss: 0.096\n",
      "Iteration: 91 \t--- Loss: 0.086\n",
      "Iteration: 92 \t--- Loss: 0.090\n",
      "Iteration: 93 \t--- Loss: 0.079\n",
      "Iteration: 94 \t--- Loss: 0.077\n",
      "Iteration: 95 \t--- Loss: 0.081\n",
      "Iteration: 96 \t--- Loss: 0.076\n",
      "Iteration: 97 \t--- Loss: 0.071\n",
      "Iteration: 98 \t--- Loss: 0.076\n",
      "Iteration: 99 \t--- Loss: 0.078\n",
      "Iteration: 100 \t--- Loss: 0.092\n",
      "Iteration: 101 \t--- Loss: 0.072\n",
      "Iteration: 102 \t--- Loss: 0.081\n",
      "Iteration: 103 \t--- Loss: 0.073\n",
      "Iteration: 104 \t--- Loss: 0.092\n",
      "Iteration: 105 \t--- Loss: 0.084\n",
      "Iteration: 106 \t--- Loss: 0.109\n",
      "Iteration: 107 \t--- Loss: 0.136\n",
      "Iteration: 108 \t--- Loss: 0.082\n",
      "Iteration: 109 \t--- Loss: 0.083\n",
      "Iteration: 110 \t--- Loss: 0.076\n",
      "Iteration: 111 \t--- Loss: 0.140\n",
      "Iteration: 112 \t--- Loss: 0.171\n",
      "Iteration: 113 \t--- Loss: 0.159\n",
      "Iteration: 114 \t--- Loss: 0.116\n",
      "Iteration: 115 \t--- Loss: 0.057\n",
      "Iteration: 116 \t--- Loss: 0.061\n",
      "Iteration: 117 \t--- Loss: 0.072\n",
      "Iteration: 118 \t--- Loss: 0.080\n",
      "Iteration: 119 \t--- Loss: 0.082\n",
      "Iteration: 120 \t--- Loss: 0.094\n",
      "Iteration: 121 \t--- Loss: 0.126\n",
      "Iteration: 122 \t--- Loss: 0.081\n",
      "Iteration: 123 \t--- Loss: 0.059\n",
      "Iteration: 124 \t--- Loss: 0.063\n",
      "Iteration: 125 \t--- Loss: 0.075\n",
      "Iteration: 126 \t--- Loss: 0.078\n",
      "Iteration: 127 \t--- Loss: 0.073\n",
      "Iteration: 128 \t--- Loss: 0.083\n",
      "Iteration: 129 \t--- Loss: 0.051\n",
      "Iteration: 130 \t--- Loss: 0.057\n",
      "Iteration: 131 \t--- Loss: 0.053\n",
      "Iteration: 132 \t--- Loss: 0.052\n",
      "Iteration: 133 \t--- Loss: 0.060\n",
      "Iteration: 134 \t--- Loss: 0.051\n",
      "Iteration: 135 \t--- Loss: 0.066\n",
      "Iteration: 136 \t--- Loss: 0.091\n",
      "Iteration: 137 \t--- Loss: 0.092\n",
      "Iteration: 138 \t--- Loss: 0.139\n",
      "Iteration: 139 \t--- Loss: 0.087\n",
      "Iteration: 140 \t--- Loss: 0.041\n",
      "Iteration: 141 \t--- Loss: 0.047\n",
      "Iteration: 142 \t--- Loss: 0.049\n",
      "Iteration: 143 \t--- Loss: 0.057\n",
      "Iteration: 144 \t--- Loss: 0.044\n",
      "Iteration: 145 \t--- Loss: 0.063\n",
      "Iteration: 146 \t--- Loss: 0.078\n",
      "Iteration: 147 \t--- Loss: 0.046\n",
      "Iteration: 148 \t--- Loss: 0.046\n",
      "Iteration: 149 \t--- Loss: 0.067\n",
      "Iteration: 150 \t--- Loss: 0.093\n",
      "Iteration: 151 \t--- Loss: 0.049\n",
      "Iteration: 152 \t--- Loss: 0.049\n",
      "Iteration: 153 \t--- Loss: 0.064\n",
      "Iteration: 154 \t--- Loss: 0.039\n",
      "Iteration: 155 \t--- Loss: 0.038\n",
      "Iteration: 156 \t--- Loss: 0.037\n",
      "Iteration: 157 \t--- Loss: 0.037\n",
      "Iteration: 158 \t--- Loss: 0.041\n",
      "Iteration: 159 \t--- Loss: 0.040\n",
      "Iteration: 160 \t--- Loss: 0.051\n",
      "Iteration: 161 \t--- Loss: 0.056\n",
      "Iteration: 162 \t--- Loss: 0.037\n",
      "Iteration: 163 \t--- Loss: 0.038\n",
      "Iteration: 164 \t--- Loss: 0.041\n",
      "Iteration: 165 \t--- Loss: 0.042\n",
      "Iteration: 166 \t--- Loss: 0.042\n",
      "Iteration: 167 \t--- Loss: 0.042\n",
      "Iteration: 168 \t--- Loss: 0.039\n",
      "Iteration: 169 \t--- Loss: 0.044\n",
      "Iteration: 170 \t--- Loss: 0.040\n",
      "Iteration: 171 \t--- Loss: 0.048\n",
      "Iteration: 172 \t--- Loss: 0.033\n",
      "Iteration: 173 \t--- Loss: 0.034\n",
      "Iteration: 174 \t--- Loss: 0.033\n",
      "Iteration: 175 \t--- Loss: 0.034\n",
      "Iteration: 176 \t--- Loss: 0.040\n",
      "Iteration: 177 \t--- Loss: 0.045\n",
      "Iteration: 178 \t--- Loss: 0.031\n",
      "Iteration: 179 \t--- Loss: 0.038\n",
      "Iteration: 180 \t--- Loss: 0.036\n",
      "Iteration: 181 \t--- Loss: 0.041\n",
      "Iteration: 182 \t--- Loss: 0.035\n",
      "Iteration: 183 \t--- Loss: 0.034\n",
      "Iteration: 184 \t--- Loss: 0.040\n",
      "Iteration: 185 \t--- Loss: 0.041\n",
      "Iteration: 186 \t--- Loss: 0.029\n",
      "Iteration: 187 \t--- Loss: 0.031\n",
      "Iteration: 188 \t--- Loss: 0.029\n",
      "Iteration: 189 \t--- Loss: 0.028\n",
      "Iteration: 190 \t--- Loss: 0.027\n",
      "Iteration: 191 \t--- Loss: 0.029\n",
      "Iteration: 192 \t--- Loss: 0.031\n",
      "Iteration: 193 \t--- Loss: 0.032\n",
      "Iteration: 194 \t--- Loss: 0.035\n",
      "Iteration: 195 \t--- Loss: 0.038\n",
      "Iteration: 196 \t--- Loss: 0.023\n",
      "Iteration: 197 \t--- Loss: 0.024\n",
      "Iteration: 198 \t--- Loss: 0.030\n",
      "Iteration: 199 \t--- Loss: 0.028\n",
      "Iteration: 200 \t--- Loss: 0.027\n",
      "Iteration: 201 \t--- Loss: 0.024\n",
      "Iteration: 202 \t--- Loss: 0.031\n",
      "Iteration: 203 \t--- Loss: 0.035\n",
      "Iteration: 204 \t--- Loss: 0.034\n",
      "Iteration: 205 \t--- Loss: 0.035\n",
      "Iteration: 206 \t--- Loss: 0.027\n",
      "Iteration: 207 \t--- Loss: 0.024\n",
      "Iteration: 208 \t--- Loss: 0.029\n",
      "Iteration: 209 \t--- Loss: 0.032\n",
      "Iteration: 210 \t--- Loss: 0.026\n",
      "Iteration: 211 \t--- Loss: 0.023\n",
      "Iteration: 212 \t--- Loss: 0.024\n",
      "Iteration: 213 \t--- Loss: 0.028\n",
      "Iteration: 214 \t--- Loss: 0.028\n",
      "Iteration: 215 \t--- Loss: 0.027\n",
      "Iteration: 216 \t--- Loss: 0.027\n",
      "Iteration: 217 \t--- Loss: 0.024\n",
      "Iteration: 218 \t--- Loss: 0.026\n",
      "Iteration: 219 \t--- Loss: 0.028\n",
      "Iteration: 220 \t--- Loss: 0.025\n",
      "Iteration: 221 \t--- Loss: 0.027\n",
      "Iteration: 222 \t--- Loss: 0.031\n",
      "Iteration: 223 \t--- Loss: 0.036\n",
      "Iteration: 224 \t--- Loss: 0.022\n",
      "Iteration: 225 \t--- Loss: 0.023\n",
      "Iteration: 226 \t--- Loss: 0.024\n",
      "Iteration: 227 \t--- Loss: 0.024\n",
      "Iteration: 228 \t--- Loss: 0.026\n",
      "Iteration: 229 \t--- Loss: 0.023\n",
      "Iteration: 230 \t--- Loss: 0.029\n",
      "Iteration: 231 \t--- Loss: 0.034\n",
      "Iteration: 232 \t--- Loss: 0.027\n",
      "Iteration: 233 \t--- Loss: 0.025\n",
      "Iteration: 234 \t--- Loss: 0.026\n",
      "Iteration: 235 \t--- Loss: 0.025\n",
      "Iteration: 236 \t--- Loss: 0.023\n",
      "Iteration: 237 \t--- Loss: 0.026\n",
      "Iteration: 238 \t--- Loss: 0.021\n",
      "Iteration: 239 \t--- Loss: 0.020\n",
      "Iteration: 240 \t--- Loss: 0.021\n",
      "Iteration: 241 \t--- Loss: 0.023\n",
      "Iteration: 242 \t--- Loss: 0.023\n",
      "Iteration: 243 \t--- Loss: 0.021\n",
      "Iteration: 244 \t--- Loss: 0.026\n",
      "Iteration: 245 \t--- Loss: 0.022\n",
      "Iteration: 246 \t--- Loss: 0.022\n",
      "Iteration: 247 \t--- Loss: 0.022\n",
      "Iteration: 248 \t--- Loss: 0.026\n",
      "Iteration: 249 \t--- Loss: 0.026\n",
      "Iteration: 250 \t--- Loss: 0.019\n",
      "Iteration: 251 \t--- Loss: 0.021\n",
      "Iteration: 252 \t--- Loss: 0.020\n",
      "Iteration: 253 \t--- Loss: 0.021\n",
      "Iteration: 254 \t--- Loss: 0.021\n",
      "Iteration: 255 \t--- Loss: 0.018\n",
      "Iteration: 256 \t--- Loss: 0.020\n",
      "Iteration: 257 \t--- Loss: 0.021\n",
      "Iteration: 258 \t--- Loss: 0.023\n",
      "Iteration: 259 \t--- Loss: 0.023Iteration: 0 \t--- Loss: 1.257\n",
      "Iteration: 1 \t--- Loss: 1.230\n",
      "Iteration: 2 \t--- Loss: 1.072\n",
      "Iteration: 3 \t--- Loss: 1.047\n",
      "Iteration: 4 \t--- Loss: 0.965\n",
      "Iteration: 5 \t--- Loss: 0.911\n",
      "Iteration: 6 \t--- Loss: 0.875\n",
      "Iteration: 7 \t--- Loss: 0.861\n",
      "Iteration: 8 \t--- Loss: 0.855\n",
      "Iteration: 9 \t--- Loss: 0.842\n",
      "Iteration: 10 \t--- Loss: 0.827\n",
      "Iteration: 11 \t--- Loss: 0.800\n",
      "Iteration: 12 \t--- Loss: 0.795\n",
      "Iteration: 13 \t--- Loss: 0.781\n",
      "Iteration: 14 \t--- Loss: 0.788\n",
      "Iteration: 15 \t--- Loss: 0.800\n",
      "Iteration: 16 \t--- Loss: 0.783\n",
      "Iteration: 17 \t--- Loss: 0.787\n",
      "Iteration: 18 \t--- Loss: 0.774\n",
      "Iteration: 19 \t--- Loss: 0.777\n",
      "Iteration: 20 \t--- Loss: 0.759\n",
      "Iteration: 21 \t--- Loss: 0.774\n",
      "Iteration: 22 \t--- Loss: 0.766\n",
      "Iteration: 23 \t--- Loss: 0.781\n",
      "Iteration: 24 \t--- Loss: 0.770\n",
      "Iteration: 25 \t--- Loss: 0.771\n",
      "Iteration: 26 \t--- Loss: 0.787\n",
      "Iteration: 27 \t--- Loss: 0.771\n",
      "Iteration: 28 \t--- Loss: 0.756\n",
      "Iteration: 29 \t--- Loss: 0.802\n",
      "Iteration: 30 \t--- Loss: 0.748\n",
      "Iteration: 31 \t--- Loss: 0.748\n",
      "Iteration: 32 \t--- Loss: 0.791\n",
      "Iteration: 33 \t--- Loss: 0.783\n",
      "Iteration: 34 \t--- Loss: 0.772\n",
      "Iteration: 35 \t--- Loss: 0.789\n",
      "Iteration: 36 \t--- Loss: 0.762\n",
      "Iteration: 37 \t--- Loss: 0.772\n",
      "Iteration: 38 \t--- Loss: 0.758\n",
      "Iteration: 39 \t--- Loss: 0.761\n",
      "Iteration: 40 \t--- Loss: 0.768\n",
      "Iteration: 41 \t--- Loss: 0.760\n",
      "Iteration: 42 \t--- Loss: 0.777\n",
      "Iteration: 43 \t--- Loss: 0.789\n",
      "Iteration: 44 \t--- Loss: 0.760\n",
      "Iteration: 45 \t--- Loss: 0.761\n",
      "Iteration: 46 \t--- Loss: 0.778\n",
      "Iteration: 47 \t--- Loss: 0.746\n",
      "Iteration: 48 \t--- Loss: 0.751\n",
      "Iteration: 49 \t--- Loss: 0.773\n",
      "Iteration: 50 \t--- Loss: 0.758\n",
      "Iteration: 51 \t--- Loss: 0.785\n",
      "Iteration: 52 \t--- Loss: 0.755\n",
      "Iteration: 53 \t--- Loss: 0.783\n",
      "Iteration: 54 \t--- Loss: 0.771\n",
      "Iteration: 55 \t--- Loss: 0.760\n",
      "Iteration: 56 \t--- Loss: 0.777\n",
      "Iteration: 57 \t--- Loss: 0.778\n",
      "Iteration: 58 \t--- Loss: 0.753\n",
      "Iteration: 59 \t--- Loss: 0.787\n",
      "Iteration: 60 \t--- Loss: 0.753\n",
      "Iteration: 61 \t--- Loss: 0.759\n",
      "Iteration: 62 \t--- Loss: 0.770\n",
      "Iteration: 63 \t--- Loss: 0.762\n",
      "Iteration: 64 \t--- Loss: 0.764\n",
      "Iteration: 65 \t--- Loss: 0.784\n",
      "Iteration: 66 \t--- Loss: 0.767\n",
      "Iteration: 67 \t--- Loss: 0.766\n",
      "Iteration: 68 \t--- Loss: 0.758\n",
      "Iteration: 69 \t--- Loss: 0.759\n",
      "Iteration: 70 \t--- Loss: 0.779\n",
      "Iteration: 71 \t--- Loss: 0.765\n",
      "Iteration: 72 \t--- Loss: 0.763\n",
      "Iteration: 73 \t--- Loss: 0.785\n",
      "Iteration: 74 \t--- Loss: 0.767\n",
      "Iteration: 75 \t--- Loss: 0.763\n",
      "Iteration: 76 \t--- Loss: 0.751\n",
      "Iteration: 77 \t--- Loss: 0.779\n",
      "Iteration: 78 \t--- Loss: 0.759\n",
      "Iteration: 79 \t--- Loss: 0.764\n",
      "Iteration: 80 \t--- Loss: 0.773\n",
      "Iteration: 81 \t--- Loss: 0.750\n",
      "Iteration: 82 \t--- Loss: 0.782\n",
      "Iteration: 83 \t--- Loss: 0.767\n",
      "Iteration: 84 \t--- Loss: 0.773\n",
      "Iteration: 85 \t--- Loss: 0.776\n",
      "Iteration: 86 \t--- Loss: 0.762\n",
      "Iteration: 87 \t--- Loss: 0.765\n",
      "Iteration: 88 \t--- Loss: 0.762\n",
      "Iteration: 89 \t--- Loss: 0.769\n",
      "Iteration: 90 \t--- Loss: 0.759\n",
      "Iteration: 91 \t--- Loss: 0.755\n",
      "Iteration: 92 \t--- Loss: 0.764\n",
      "Iteration: 93 \t--- Loss: 0.763\n",
      "Iteration: 94 \t--- Loss: 0.777\n",
      "Iteration: 95 \t--- Loss: 0.753\n",
      "Iteration: 96 \t--- Loss: 0.767\n",
      "Iteration: 97 \t--- Loss: 0.777\n",
      "Iteration: 98 \t--- Loss: 0.793\n",
      "Iteration: 99 \t--- Loss: 0.765\n",
      "Iteration: 100 \t--- Loss: 0.770\n",
      "Iteration: 101 \t--- Loss: 0.768\n",
      "Iteration: 102 \t--- Loss: 0.776\n",
      "Iteration: 103 \t--- Loss: 0.761\n",
      "Iteration: 104 \t--- Loss: 0.753\n",
      "Iteration: 105 \t--- Loss: 0.754\n",
      "Iteration: 106 \t--- Loss: 0.765\n",
      "Iteration: 107 \t--- Loss: 0.751\n",
      "Iteration: 108 \t--- Loss: 0.765\n",
      "Iteration: 109 \t--- Loss: 0.763\n",
      "Iteration: 110 \t--- Loss: 0.766\n",
      "Iteration: 111 \t--- Loss: 0.754\n",
      "Iteration: 112 \t--- Loss: 0.760\n",
      "Iteration: 113 \t--- Loss: 0.768\n",
      "Iteration: 114 \t--- Loss: 0.772\n",
      "Iteration: 115 \t--- Loss: 0.757\n",
      "Iteration: 116 \t--- Loss: 0.751\n",
      "Iteration: 117 \t--- Loss: 0.776\n",
      "Iteration: 118 \t--- Loss: 0.757\n",
      "Iteration: 119 \t--- Loss: 0.766\n",
      "Iteration: 120 \t--- Loss: 0.772\n",
      "Iteration: 121 \t--- Loss: 0.765\n",
      "Iteration: 122 \t--- Loss: 0.762\n",
      "Iteration: 123 \t--- Loss: 0.770\n",
      "Iteration: 124 \t--- Loss: 0.755\n",
      "Iteration: 125 \t--- Loss: 0.771\n",
      "Iteration: 126 \t--- Loss: 0.767\n",
      "Iteration: 127 \t--- Loss: 0.769\n",
      "Iteration: 128 \t--- Loss: 0.751\n",
      "Iteration: 129 \t--- Loss: 0.755\n",
      "Iteration: 130 \t--- Loss: 0.750\n",
      "Iteration: 131 \t--- Loss: 0.773\n",
      "Iteration: 132 \t--- Loss: 0.768\n",
      "Iteration: 133 \t--- Loss: 0.780\n",
      "Iteration: 134 \t--- Loss: 0.762\n",
      "Iteration: 135 \t--- Loss: 0.749\n",
      "Iteration: 136 \t--- Loss: 0.758\n",
      "Iteration: 137 \t--- Loss: 0.774\n",
      "Iteration: 138 \t--- Loss: 0.758\n",
      "Iteration: 139 \t--- Loss: 0.762\n",
      "Iteration: 140 \t--- Loss: 0.757\n",
      "Iteration: 141 \t--- Loss: 0.764\n",
      "Iteration: 142 \t--- Loss: 0.763\n",
      "Iteration: 143 \t--- Loss: 0.775\n",
      "Iteration: 144 \t--- Loss: 0.777\n",
      "Iteration: 145 \t--- Loss: 0.777\n",
      "Iteration: 146 \t--- Loss: 0.749\n",
      "Iteration: 147 \t--- Loss: 0.781\n",
      "Iteration: 148 \t--- Loss: 0.777\n",
      "Iteration: 149 \t--- Loss: 0.763\n",
      "Iteration: 150 \t--- Loss: 0.773\n",
      "Iteration: 151 \t--- Loss: 0.774\n",
      "Iteration: 152 \t--- Loss: 0.767\n",
      "Iteration: 153 \t--- Loss: 0.744\n",
      "Iteration: 154 \t--- Loss: 0.777\n",
      "Iteration: 155 \t--- Loss: 0.757\n",
      "Iteration: 156 \t--- Loss: 0.771\n",
      "Iteration: 157 \t--- Loss: 0.765\n",
      "Iteration: 158 \t--- Loss: 0.778\n",
      "Iteration: 159 \t--- Loss: 0.769\n",
      "Iteration: 160 \t--- Loss: 0.773\n",
      "Iteration: 161 \t--- Loss: 0.756\n",
      "Iteration: 162 \t--- Loss: 0.753\n",
      "Iteration: 163 \t--- Loss: 0.788\n",
      "Iteration: 164 \t--- Loss: 0.774\n",
      "Iteration: 165 \t--- Loss: 0.774\n",
      "Iteration: 166 \t--- Loss: 0.736\n",
      "Iteration: 167 \t--- Loss: 0.759\n",
      "Iteration: 168 \t--- Loss: 0.781\n",
      "Iteration: 169 \t--- Loss: 0.773\n",
      "Iteration: 170 \t--- Loss: 0.761\n",
      "Iteration: 171 \t--- Loss: 0.788\n",
      "Iteration: 172 \t--- Loss: 0.763\n",
      "Iteration: 173 \t--- Loss: 0.748\n",
      "Iteration: 174 \t--- Loss: 0.747\n",
      "Iteration: 175 \t--- Loss: 0.764\n",
      "Iteration: 176 \t--- Loss: 0.772\n",
      "Iteration: 177 \t--- Loss: 0.751\n",
      "Iteration: 178 \t--- Loss: 0.775\n",
      "Iteration: 179 \t--- Loss: 0.754\n",
      "Iteration: 180 \t--- Loss: 0.764\n",
      "Iteration: 181 \t--- Loss: 0.772\n",
      "Iteration: 182 \t--- Loss: 0.771\n",
      "Iteration: 183 \t--- Loss: 0.766\n",
      "Iteration: 184 \t--- Loss: 0.766\n",
      "Iteration: 185 \t--- Loss: 0.773\n",
      "Iteration: 186 \t--- Loss: 0.755\n",
      "Iteration: 187 \t--- Loss: 0.762\n",
      "Iteration: 188 \t--- Loss: 0.779\n",
      "Iteration: 189 \t--- Loss: 0.765\n",
      "Iteration: 190 \t--- Loss: 0.764\n",
      "Iteration: 191 \t--- Loss: 0.751\n",
      "Iteration: 192 \t--- Loss: 0.762\n",
      "Iteration: 193 \t--- Loss: 0.775\n",
      "Iteration: 194 \t--- Loss: 0.768\n",
      "Iteration: 195 \t--- Loss: 0.751\n",
      "Iteration: 196 \t--- Loss: 0.751\n",
      "Iteration: 197 \t--- Loss: 0.740\n",
      "Iteration: 198 \t--- Loss: 0.769\n",
      "Iteration: 199 \t--- Loss: 0.762\n",
      "Iteration: 200 \t--- Loss: 0.748\n",
      "Iteration: 201 \t--- Loss: 0.771\n",
      "Iteration: 202 \t--- Loss: 0.755\n",
      "Iteration: 203 \t--- Loss: 0.760\n",
      "Iteration: 204 \t--- Loss: 0.771\n",
      "Iteration: 205 \t--- Loss: 0.775\n",
      "Iteration: 206 \t--- Loss: 0.769\n",
      "Iteration: 207 \t--- Loss: 0.755\n",
      "Iteration: 208 \t--- Loss: 0.751\n",
      "Iteration: 209 \t--- Loss: 0.760\n",
      "Iteration: 210 \t--- Loss: 0.774\n",
      "Iteration: 211 \t--- Loss: 0.768\n",
      "Iteration: 212 \t--- Loss: 0.779\n",
      "Iteration: 213 \t--- Loss: 0.760\n",
      "Iteration: 214 \t--- Loss: 0.756\n",
      "Iteration: 215 \t--- Loss: 0.761\n",
      "Iteration: 216 \t--- Loss: 0.760\n",
      "Iteration: 217 \t--- Loss: 0.774\n",
      "Iteration: 218 \t--- Loss: 0.769\n",
      "Iteration: 219 \t--- Loss: 0.768\n",
      "Iteration: 220 \t--- Loss: 0.774\n",
      "Iteration: 221 \t--- Loss: 0.752\n",
      "Iteration: 222 \t--- Loss: 0.763\n",
      "Iteration: 223 \t--- Loss: 0.774\n",
      "Iteration: 224 \t--- Loss: 0.761\n",
      "Iteration: 225 \t--- Loss: 0.747\n",
      "Iteration: 226 \t--- Loss: 0.777\n",
      "Iteration: 227 \t--- Loss: 0.757\n",
      "Iteration: 228 \t--- Loss: 0.772\n",
      "Iteration: 229 \t--- Loss: 0.787\n",
      "Iteration: 230 \t--- Loss: 0.770\n",
      "Iteration: 231 \t--- Loss: 0.779\n",
      "Iteration: 232 \t--- Loss: 0.772\n",
      "Iteration: 233 \t--- Loss: 0.777\n",
      "Iteration: 234 \t--- Loss: 0.764\n",
      "Iteration: 235 \t--- Loss: 0.754\n",
      "Iteration: 236 \t--- Loss: 0.771\n",
      "Iteration: 237 \t--- Loss: 0.778\n",
      "Iteration: 238 \t--- Loss: 0.762\n",
      "Iteration: 239 \t--- Loss: 0.769\n",
      "Iteration: 240 \t--- Loss: 0.785\n",
      "Iteration: 241 \t--- Loss: 0.762\n",
      "Iteration: 242 \t--- Loss: 0.771\n",
      "Iteration: 243 \t--- Loss: 0.746\n",
      "Iteration: 244 \t--- Loss: 0.777\n",
      "Iteration: 245 \t--- Loss: 0.751\n",
      "Iteration: 246 \t--- Loss: 0.763\n",
      "Iteration: 247 \t--- Loss: 0.775\n",
      "Iteration: 248 \t--- Loss: 0.773\n",
      "Iteration: 249 \t--- Loss: 0.764\n",
      "Iteration: 250 \t--- Loss: 0.773\n",
      "Iteration: 251 \t--- Loss: 0.769\n",
      "Iteration: 252 \t--- Loss: 0.764\n",
      "Iteration: 253 \t--- Loss: 0.770\n",
      "Iteration: 254 \t--- Loss: 0.775\n",
      "Iteration: 255 \t--- Loss: 0.742\n",
      "Iteration: 256 \t--- Loss: 0.755\n",
      "Iteration: 257 \t--- Loss: 0.737\n",
      "Iteration: 258 \t--- Loss: 0.772\n",
      "Iteration: 259 \t--- Loss: 0.789Iteration: 0 \t--- Loss: 0.121\n",
      "Iteration: 1 \t--- Loss: 0.117\n",
      "Iteration: 2 \t--- Loss: 0.117\n",
      "Iteration: 3 \t--- Loss: 0.109\n",
      "Iteration: 4 \t--- Loss: 0.107\n",
      "Iteration: 5 \t--- Loss: 0.102\n",
      "Iteration: 6 \t--- Loss: 0.102\n",
      "Iteration: 7 \t--- Loss: 0.103\n",
      "Iteration: 8 \t--- Loss: 0.093\n",
      "Iteration: 9 \t--- Loss: 0.097\n",
      "Iteration: 10 \t--- Loss: 0.086\n",
      "Iteration: 11 \t--- Loss: 0.093\n",
      "Iteration: 12 \t--- Loss: 0.091\n",
      "Iteration: 13 \t--- Loss: 0.094\n",
      "Iteration: 14 \t--- Loss: 0.084\n",
      "Iteration: 15 \t--- Loss: 0.101\n",
      "Iteration: 16 \t--- Loss: 0.084\n",
      "Iteration: 17 \t--- Loss: 0.089\n",
      "Iteration: 18 \t--- Loss: 0.086\n",
      "Iteration: 19 \t--- Loss: 0.089\n",
      "Iteration: 20 \t--- Loss: 0.087\n",
      "Iteration: 21 \t--- Loss: 0.082\n",
      "Iteration: 22 \t--- Loss: 0.087\n",
      "Iteration: 23 \t--- Loss: 0.082\n",
      "Iteration: 24 \t--- Loss: 0.093\n",
      "Iteration: 25 \t--- Loss: 0.078\n",
      "Iteration: 26 \t--- Loss: 0.085\n",
      "Iteration: 27 \t--- Loss: 0.088\n",
      "Iteration: 28 \t--- Loss: 0.084\n",
      "Iteration: 29 \t--- Loss: 0.083\n",
      "Iteration: 30 \t--- Loss: 0.089\n",
      "Iteration: 31 \t--- Loss: 0.086\n",
      "Iteration: 32 \t--- Loss: 0.085\n",
      "Iteration: 33 \t--- Loss: 0.078\n",
      "Iteration: 34 \t--- Loss: 0.081\n",
      "Iteration: 35 \t--- Loss: 0.084\n",
      "Iteration: 36 \t--- Loss: 0.085\n",
      "Iteration: 37 \t--- Loss: 0.082\n",
      "Iteration: 38 \t--- Loss: 0.083\n",
      "Iteration: 39 \t--- Loss: 0.087\n",
      "Iteration: 40 \t--- Loss: 0.078\n",
      "Iteration: 41 \t--- Loss: 0.082\n",
      "Iteration: 42 \t--- Loss: 0.083\n",
      "Iteration: 43 \t--- Loss: 0.076\n",
      "Iteration: 44 \t--- Loss: 0.083\n",
      "Iteration: 45 \t--- Loss: 0.087\n",
      "Iteration: 46 \t--- Loss: 0.085\n",
      "Iteration: 47 \t--- Loss: 0.078\n",
      "Iteration: 48 \t--- Loss: 0.086\n",
      "Iteration: 49 \t--- Loss: 0.081\n",
      "Iteration: 50 \t--- Loss: 0.082\n",
      "Iteration: 51 \t--- Loss: 0.078\n",
      "Iteration: 52 \t--- Loss: 0.089\n",
      "Iteration: 53 \t--- Loss: 0.081\n",
      "Iteration: 54 \t--- Loss: 0.084\n",
      "Iteration: 55 \t--- Loss: 0.083\n",
      "Iteration: 56 \t--- Loss: 0.082\n",
      "Iteration: 57 \t--- Loss: 0.083\n",
      "Iteration: 58 \t--- Loss: 0.078\n",
      "Iteration: 59 \t--- Loss: 0.081\n",
      "Iteration: 60 \t--- Loss: 0.083\n",
      "Iteration: 61 \t--- Loss: 0.081\n",
      "Iteration: 62 \t--- Loss: 0.081\n",
      "Iteration: 63 \t--- Loss: 0.077\n",
      "Iteration: 64 \t--- Loss: 0.084\n",
      "Iteration: 65 \t--- Loss: 0.079\n",
      "Iteration: 66 \t--- Loss: 0.087\n",
      "Iteration: 67 \t--- Loss: 0.082\n",
      "Iteration: 68 \t--- Loss: 0.080\n",
      "Iteration: 69 \t--- Loss: 0.082\n",
      "Iteration: 70 \t--- Loss: 0.083\n",
      "Iteration: 71 \t--- Loss: 0.080\n",
      "Iteration: 72 \t--- Loss: 0.085\n",
      "Iteration: 73 \t--- Loss: 0.088\n",
      "Iteration: 74 \t--- Loss: 0.079\n",
      "Iteration: 75 \t--- Loss: 0.084\n",
      "Iteration: 76 \t--- Loss: 0.080\n",
      "Iteration: 77 \t--- Loss: 0.077\n",
      "Iteration: 78 \t--- Loss: 0.075\n",
      "Iteration: 79 \t--- Loss: 0.079\n",
      "Iteration: 80 \t--- Loss: 0.085\n",
      "Iteration: 81 \t--- Loss: 0.076\n",
      "Iteration: 82 \t--- Loss: 0.080\n",
      "Iteration: 83 \t--- Loss: 0.075\n",
      "Iteration: 84 \t--- Loss: 0.079\n",
      "Iteration: 85 \t--- Loss: 0.081\n",
      "Iteration: 86 \t--- Loss: 0.080\n",
      "Iteration: 87 \t--- Loss: 0.080\n",
      "Iteration: 88 \t--- Loss: 0.077\n",
      "Iteration: 89 \t--- Loss: 0.079\n",
      "Iteration: 90 \t--- Loss: 0.077\n",
      "Iteration: 91 \t--- Loss: 0.081\n",
      "Iteration: 92 \t--- Loss: 0.083\n",
      "Iteration: 93 \t--- Loss: 0.083\n",
      "Iteration: 94 \t--- Loss: 0.085\n",
      "Iteration: 95 \t--- Loss: 0.080\n",
      "Iteration: 96 \t--- Loss: 0.085\n",
      "Iteration: 97 \t--- Loss: 0.081\n",
      "Iteration: 98 \t--- Loss: 0.084\n",
      "Iteration: 99 \t--- Loss: 0.082\n",
      "Iteration: 100 \t--- Loss: 0.076\n",
      "Iteration: 101 \t--- Loss: 0.083\n",
      "Iteration: 102 \t--- Loss: 0.083\n",
      "Iteration: 103 \t--- Loss: 0.083\n",
      "Iteration: 104 \t--- Loss: 0.079\n",
      "Iteration: 105 \t--- Loss: 0.080\n",
      "Iteration: 106 \t--- Loss: 0.077\n",
      "Iteration: 107 \t--- Loss: 0.081\n",
      "Iteration: 108 \t--- Loss: 0.088\n",
      "Iteration: 109 \t--- Loss: 0.079\n",
      "Iteration: 110 \t--- Loss: 0.082\n",
      "Iteration: 111 \t--- Loss: 0.090\n",
      "Iteration: 112 \t--- Loss: 0.080\n",
      "Iteration: 113 \t--- Loss: 0.081\n",
      "Iteration: 114 \t--- Loss: 0.077\n",
      "Iteration: 115 \t--- Loss: 0.079\n",
      "Iteration: 116 \t--- Loss: 0.073\n",
      "Iteration: 117 \t--- Loss: 0.078\n",
      "Iteration: 118 \t--- Loss: 0.087\n",
      "Iteration: 119 \t--- Loss: 0.082\n",
      "Iteration: 120 \t--- Loss: 0.078\n",
      "Iteration: 121 \t--- Loss: 0.085\n",
      "Iteration: 122 \t--- Loss: 0.083\n",
      "Iteration: 123 \t--- Loss: 0.085\n",
      "Iteration: 124 \t--- Loss: 0.080\n",
      "Iteration: 125 \t--- Loss: 0.084\n",
      "Iteration: 126 \t--- Loss: 0.085\n",
      "Iteration: 127 \t--- Loss: 0.081\n",
      "Iteration: 128 \t--- Loss: 0.080\n",
      "Iteration: 129 \t--- Loss: 0.084\n",
      "Iteration: 130 \t--- Loss: 0.085\n",
      "Iteration: 131 \t--- Loss: 0.085\n",
      "Iteration: 132 \t--- Loss: 0.086\n",
      "Iteration: 133 \t--- Loss: 0.087\n",
      "Iteration: 134 \t--- Loss: 0.083\n",
      "Iteration: 135 \t--- Loss: 0.081\n",
      "Iteration: 136 \t--- Loss: 0.081\n",
      "Iteration: 137 \t--- Loss: 0.077\n",
      "Iteration: 138 \t--- Loss: 0.088\n",
      "Iteration: 139 \t--- Loss: 0.081\n",
      "Iteration: 140 \t--- Loss: 0.080\n",
      "Iteration: 141 \t--- Loss: 0.088\n",
      "Iteration: 142 \t--- Loss: 0.082\n",
      "Iteration: 143 \t--- Loss: 0.086\n",
      "Iteration: 144 \t--- Loss: 0.080\n",
      "Iteration: 145 \t--- Loss: 0.082\n",
      "Iteration: 146 \t--- Loss: 0.080\n",
      "Iteration: 147 \t--- Loss: 0.084\n",
      "Iteration: 148 \t--- Loss: 0.081\n",
      "Iteration: 149 \t--- Loss: 0.084\n",
      "Iteration: 150 \t--- Loss: 0.078\n",
      "Iteration: 151 \t--- Loss: 0.084\n",
      "Iteration: 152 \t--- Loss: 0.080\n",
      "Iteration: 153 \t--- Loss: 0.080\n",
      "Iteration: 154 \t--- Loss: 0.079\n",
      "Iteration: 155 \t--- Loss: 0.082\n",
      "Iteration: 156 \t--- Loss: 0.078\n",
      "Iteration: 157 \t--- Loss: 0.079\n",
      "Iteration: 158 \t--- Loss: 0.083\n",
      "Iteration: 159 \t--- Loss: 0.083\n",
      "Iteration: 160 \t--- Loss: 0.083\n",
      "Iteration: 161 \t--- Loss: 0.082\n",
      "Iteration: 162 \t--- Loss: 0.083\n",
      "Iteration: 163 \t--- Loss: 0.082\n",
      "Iteration: 164 \t--- Loss: 0.080\n",
      "Iteration: 165 \t--- Loss: 0.083\n",
      "Iteration: 166 \t--- Loss: 0.076\n",
      "Iteration: 167 \t--- Loss: 0.084\n",
      "Iteration: 168 \t--- Loss: 0.076\n",
      "Iteration: 169 \t--- Loss: 0.080\n",
      "Iteration: 170 \t--- Loss: 0.084\n",
      "Iteration: 171 \t--- Loss: 0.087\n",
      "Iteration: 172 \t--- Loss: 0.076\n",
      "Iteration: 173 \t--- Loss: 0.085\n",
      "Iteration: 174 \t--- Loss: 0.083\n",
      "Iteration: 175 \t--- Loss: 0.081\n",
      "Iteration: 176 \t--- Loss: 0.076\n",
      "Iteration: 177 \t--- Loss: 0.079\n",
      "Iteration: 178 \t--- Loss: 0.081\n",
      "Iteration: 179 \t--- Loss: 0.085\n",
      "Iteration: 180 \t--- Loss: 0.086\n",
      "Iteration: 181 \t--- Loss: 0.080\n",
      "Iteration: 182 \t--- Loss: 0.084\n",
      "Iteration: 183 \t--- Loss: 0.085\n",
      "Iteration: 184 \t--- Loss: 0.075\n",
      "Iteration: 185 \t--- Loss: 0.086\n",
      "Iteration: 186 \t--- Loss: 0.086\n",
      "Iteration: 187 \t--- Loss: 0.084\n",
      "Iteration: 188 \t--- Loss: 0.087\n",
      "Iteration: 189 \t--- Loss: 0.084\n",
      "Iteration: 190 \t--- Loss: 0.084\n",
      "Iteration: 191 \t--- Loss: 0.084\n",
      "Iteration: 192 \t--- Loss: 0.083\n",
      "Iteration: 193 \t--- Loss: 0.084\n",
      "Iteration: 194 \t--- Loss: 0.078\n",
      "Iteration: 195 \t--- Loss: 0.079\n",
      "Iteration: 196 \t--- Loss: 0.079\n",
      "Iteration: 197 \t--- Loss: 0.077\n",
      "Iteration: 198 \t--- Loss: 0.087\n",
      "Iteration: 199 \t--- Loss: 0.088\n",
      "Iteration: 200 \t--- Loss: 0.083\n",
      "Iteration: 201 \t--- Loss: 0.084\n",
      "Iteration: 202 \t--- Loss: 0.085\n",
      "Iteration: 203 \t--- Loss: 0.082\n",
      "Iteration: 204 \t--- Loss: 0.084\n",
      "Iteration: 205 \t--- Loss: 0.084\n",
      "Iteration: 206 \t--- Loss: 0.081\n",
      "Iteration: 207 \t--- Loss: 0.084\n",
      "Iteration: 208 \t--- Loss: 0.078\n",
      "Iteration: 209 \t--- Loss: 0.083\n",
      "Iteration: 210 \t--- Loss: 0.084\n",
      "Iteration: 211 \t--- Loss: 0.084\n",
      "Iteration: 212 \t--- Loss: 0.078\n",
      "Iteration: 213 \t--- Loss: 0.079\n",
      "Iteration: 214 \t--- Loss: 0.083\n",
      "Iteration: 215 \t--- Loss: 0.076\n",
      "Iteration: 216 \t--- Loss: 0.081\n",
      "Iteration: 217 \t--- Loss: 0.085\n",
      "Iteration: 218 \t--- Loss: 0.082\n",
      "Iteration: 219 \t--- Loss: 0.081\n",
      "Iteration: 220 \t--- Loss: 0.082\n",
      "Iteration: 221 \t--- Loss: 0.081\n",
      "Iteration: 222 \t--- Loss: 0.084\n",
      "Iteration: 223 \t--- Loss: 0.081\n",
      "Iteration: 224 \t--- Loss: 0.083\n",
      "Iteration: 225 \t--- Loss: 0.087\n",
      "Iteration: 226 \t--- Loss: 0.083\n",
      "Iteration: 227 \t--- Loss: 0.080\n",
      "Iteration: 228 \t--- Loss: 0.080\n",
      "Iteration: 229 \t--- Loss: 0.087\n",
      "Iteration: 230 \t--- Loss: 0.081\n",
      "Iteration: 231 \t--- Loss: 0.086\n",
      "Iteration: 232 \t--- Loss: 0.083\n",
      "Iteration: 233 \t--- Loss: 0.082\n",
      "Iteration: 234 \t--- Loss: 0.078\n",
      "Iteration: 235 \t--- Loss: 0.084\n",
      "Iteration: 236 \t--- Loss: 0.080\n",
      "Iteration: 237 \t--- Loss: 0.086\n",
      "Iteration: 238 \t--- Loss: 0.079\n",
      "Iteration: 239 \t--- Loss: 0.079\n",
      "Iteration: 240 \t--- Loss: 0.083\n",
      "Iteration: 241 \t--- Loss: 0.076\n",
      "Iteration: 242 \t--- Loss: 0.073\n",
      "Iteration: 243 \t--- Loss: 0.082\n",
      "Iteration: 244 \t--- Loss: 0.073\n",
      "Iteration: 245 \t--- Loss: 0.085\n",
      "Iteration: 246 \t--- Loss: 0.083\n",
      "Iteration: 247 \t--- Loss: 0.084\n",
      "Iteration: 248 \t--- Loss: 0.078\n",
      "Iteration: 249 \t--- Loss: 0.079\n",
      "Iteration: 250 \t--- Loss: 0.081\n",
      "Iteration: 251 \t--- Loss: 0.084\n",
      "Iteration: 252 \t--- Loss: 0.080\n",
      "Iteration: 253 \t--- Loss: 0.078\n",
      "Iteration: 254 \t--- Loss: 0.086\n",
      "Iteration: 255 \t--- Loss: 0.081\n",
      "Iteration: 256 \t--- Loss: 0.082\n",
      "Iteration: 257 \t--- Loss: 0.080\n",
      "Iteration: 258 \t--- Loss: 0.078\n",
      "Iteration: 259 \t--- Loss: 0.080"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:35<00:00, 95.31s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.776\n",
      "Iteration: 261 \t--- Loss: 0.760\n",
      "Iteration: 262 \t--- Loss: 0.771\n",
      "Iteration: 263 \t--- Loss: 0.765\n",
      "Iteration: 264 \t--- Loss: 0.757\n",
      "Iteration: 265 \t--- Loss: 0.775\n",
      "Iteration: 266 \t--- Loss: 0.763\n",
      "Iteration: 267 \t--- Loss: 0.760\n",
      "Iteration: 268 \t--- Loss: 0.765\n",
      "Iteration: 269 \t--- Loss: 0.752\n",
      "Iteration: 270 \t--- Loss: 0.767\n",
      "Iteration: 271 \t--- Loss: 0.770\n",
      "Iteration: 272 \t--- Loss: 0.758\n",
      "Iteration: 273 \t--- Loss: 0.756\n",
      "Iteration: 274 \t--- Loss: 0.786\n",
      "Iteration: 275 \t--- Loss: 0.766\n",
      "Iteration: 276 \t--- Loss: 0.785\n",
      "Iteration: 277 \t--- Loss: 0.773\n",
      "Iteration: 278 \t--- Loss: 0.768\n",
      "Iteration: 279 \t--- Loss: 0.752\n",
      "Iteration: 280 \t--- Loss: 0.781\n",
      "Iteration: 281 \t--- Loss: 0.777\n",
      "Iteration: 282 \t--- Loss: 0.766\n",
      "Iteration: 283 \t--- Loss: 0.785\n",
      "Iteration: 284 \t--- Loss: 0.775\n",
      "Iteration: 285 \t--- Loss: 0.781\n",
      "Iteration: 286 \t--- Loss: 0.774\n",
      "Iteration: 287 \t--- Loss: 0.771\n",
      "Iteration: 288 \t--- Loss: 0.767\n",
      "Iteration: 289 \t--- Loss: 0.765\n",
      "Iteration: 290 \t--- Loss: 0.777\n",
      "Iteration: 291 \t--- Loss: 0.757\n",
      "Iteration: 292 \t--- Loss: 0.763\n",
      "Iteration: 293 \t--- Loss: 0.744\n",
      "Iteration: 294 \t--- Loss: 0.784\n",
      "Iteration: 295 \t--- Loss: 0.785\n",
      "Iteration: 296 \t--- Loss: 0.782\n",
      "Iteration: 297 \t--- Loss: 0.780\n",
      "Iteration: 298 \t--- Loss: 0.759\n",
      "Iteration: 299 \t--- Loss: 0.766\n",
      "Iteration: 300 \t--- Loss: 0.745\n",
      "Iteration: 301 \t--- Loss: 0.747\n",
      "Iteration: 302 \t--- Loss: 0.768\n",
      "Iteration: 303 \t--- Loss: 0.766\n",
      "Iteration: 304 \t--- Loss: 0.756\n",
      "Iteration: 305 \t--- Loss: 0.772\n",
      "Iteration: 306 \t--- Loss: 0.774\n",
      "Iteration: 307 \t--- Loss: 0.766\n",
      "Iteration: 308 \t--- Loss: 0.764\n",
      "Iteration: 309 \t--- Loss: 0.759\n",
      "Iteration: 310 \t--- Loss: 0.765\n",
      "Iteration: 311 \t--- Loss: 0.758\n",
      "Iteration: 312 \t--- Loss: 0.781\n",
      "Iteration: 313 \t--- Loss: 0.785\n",
      "Iteration: 314 \t--- Loss: 0.774\n",
      "Iteration: 315 \t--- Loss: 0.786\n",
      "Iteration: 316 \t--- Loss: 0.753\n",
      "Iteration: 317 \t--- Loss: 0.776\n",
      "Iteration: 318 \t--- Loss: 0.766\n",
      "Iteration: 319 \t--- Loss: 0.775\n",
      "Iteration: 320 \t--- Loss: 0.764\n",
      "Iteration: 321 \t--- Loss: 0.756\n",
      "Iteration: 322 \t--- Loss: 0.753\n",
      "Iteration: 323 \t--- Loss: 0.771\n",
      "Iteration: 324 \t--- Loss: 0.768\n",
      "Iteration: 325 \t--- Loss: 0.756\n",
      "Iteration: 326 \t--- Loss: 0.776\n",
      "Iteration: 327 \t--- Loss: 0.757\n",
      "Iteration: 328 \t--- Loss: 0.749\n",
      "Iteration: 329 \t--- Loss: 0.778\n",
      "Iteration: 330 \t--- Loss: 0.772\n",
      "Iteration: 331 \t--- Loss: 0.753\n",
      "Iteration: 332 \t--- Loss: 0.768\n",
      "Iteration: 333 \t--- Loss: 0.776\n",
      "Iteration: 334 \t--- Loss: 0.769\n",
      "Iteration: 335 \t--- Loss: 0.756\n",
      "Iteration: 336 \t--- Loss: 0.782\n",
      "Iteration: 337 \t--- Loss: 0.757\n",
      "Iteration: 338 \t--- Loss: 0.774\n",
      "Iteration: 339 \t--- Loss: 0.776\n",
      "Iteration: 340 \t--- Loss: 0.762\n",
      "Iteration: 341 \t--- Loss: 0.781\n",
      "Iteration: 342 \t--- Loss: 0.747\n",
      "Iteration: 343 \t--- Loss: 0.774\n",
      "Iteration: 344 \t--- Loss: 0.756\n",
      "Iteration: 345 \t--- Loss: 0.772\n",
      "Iteration: 346 \t--- Loss: 0.772\n",
      "Iteration: 347 \t--- Loss: 0.775\n",
      "Iteration: 348 \t--- Loss: 0.755\n",
      "Iteration: 349 \t--- Loss: 0.748\n",
      "Iteration: 350 \t--- Loss: 0.766\n",
      "Iteration: 351 \t--- Loss: 0.785\n",
      "Iteration: 352 \t--- Loss: 0.759\n",
      "Iteration: 353 \t--- Loss: 0.764\n",
      "Iteration: 354 \t--- Loss: 0.754\n",
      "Iteration: 355 \t--- Loss: 0.771\n",
      "Iteration: 356 \t--- Loss: 0.759\n",
      "Iteration: 357 \t--- Loss: 0.765\n",
      "Iteration: 358 \t--- Loss: 0.783\n",
      "Iteration: 359 \t--- Loss: 0.778\n",
      "Iteration: 360 \t--- Loss: 0.769\n",
      "Iteration: 361 \t--- Loss: 0.783\n",
      "Iteration: 362 \t--- Loss: 0.768\n",
      "Iteration: 363 \t--- Loss: 0.764\n",
      "Iteration: 364 \t--- Loss: 0.767\n",
      "Iteration: 365 \t--- Loss: 0.766\n",
      "Iteration: 366 \t--- Loss: 0.780\n",
      "Iteration: 367 \t--- Loss: 0.761\n",
      "Iteration: 368 \t--- Loss: 0.757\n",
      "Iteration: 369 \t--- Loss: 0.769\n",
      "Iteration: 370 \t--- Loss: 0.775\n",
      "Iteration: 371 \t--- Loss: 0.763\n",
      "Iteration: 372 \t--- Loss: 0.771\n",
      "Iteration: 373 \t--- Loss: 0.785\n",
      "Iteration: 374 \t--- Loss: 0.773\n",
      "Iteration: 375 \t--- Loss: 0.747\n",
      "Iteration: 376 \t--- Loss: 0.745\n",
      "Iteration: 377 \t--- Loss: 0.780\n",
      "Iteration: 378 \t--- Loss: 0.766\n",
      "Iteration: 379 \t--- Loss: 0.759\n",
      "Iteration: 380 \t--- Loss: 0.766\n",
      "Iteration: 381 \t--- Loss: 0.777\n",
      "Iteration: 382 \t--- Loss: 0.761\n",
      "Iteration: 383 \t--- Loss: 0.768\n",
      "Iteration: 384 \t--- Loss: 0.767\n",
      "Iteration: 385 \t--- Loss: 0.779\n",
      "Iteration: 386 \t--- Loss: 0.759\n",
      "Iteration: 387 \t--- Loss: 0.775\n",
      "Iteration: 388 \t--- Loss: 0.754\n",
      "Iteration: 389 \t--- Loss: 0.780\n",
      "Iteration: 390 \t--- Loss: 0.771\n",
      "Iteration: 391 \t--- Loss: 0.772\n",
      "Iteration: 392 \t--- Loss: 0.773\n",
      "Iteration: 393 \t--- Loss: 0.773\n",
      "Iteration: 394 \t--- Loss: 0.753\n",
      "Iteration: 395 \t--- Loss: 0.776\n",
      "Iteration: 396 \t--- Loss: 0.759\n",
      "Iteration: 397 \t--- Loss: 0.758\n",
      "Iteration: 398 \t--- Loss: 0.773\n",
      "Iteration: 399 \t--- Loss: 0.757\n",
      "Iteration: 400 \t--- Loss: 0.784\n",
      "Iteration: 401 \t--- Loss: 0.773\n",
      "Iteration: 402 \t--- Loss: 0.784\n",
      "Iteration: 403 \t--- Loss: 0.763\n",
      "Iteration: 404 \t--- Loss: 0.765\n",
      "Iteration: 405 \t--- Loss: 0.772\n",
      "Iteration: 406 \t--- Loss: 0.756\n",
      "Iteration: 407 \t--- Loss: 0.761\n",
      "Iteration: 408 \t--- Loss: 0.770\n",
      "Iteration: 409 \t--- Loss: 0.773\n",
      "Iteration: 410 \t--- Loss: 0.769\n",
      "Iteration: 411 \t--- Loss: 0.766\n",
      "Iteration: 412 \t--- Loss: 0.763\n",
      "Iteration: 413 \t--- Loss: 0.764\n",
      "Iteration: 414 \t--- Loss: 0.754\n",
      "Iteration: 415 \t--- Loss: 0.763\n",
      "Iteration: 416 \t--- Loss: 0.764\n",
      "Iteration: 417 \t--- Loss: 0.773\n",
      "Iteration: 418 \t--- Loss: 0.763\n",
      "Iteration: 419 \t--- Loss: 0.777\n",
      "Iteration: 420 \t--- Loss: 0.777\n",
      "Iteration: 421 \t--- Loss: 0.761\n",
      "Iteration: 422 \t--- Loss: 0.782\n",
      "Iteration: 423 \t--- Loss: 0.765\n",
      "Iteration: 424 \t--- Loss: 0.763\n",
      "Iteration: 425 \t--- Loss: 0.759\n",
      "Iteration: 426 \t--- Loss: 0.761\n",
      "Iteration: 427 \t--- Loss: 0.773\n",
      "Iteration: 428 \t--- Loss: 0.745\n",
      "Iteration: 429 \t--- Loss: 0.762\n",
      "Iteration: 430 \t--- Loss: 0.775\n",
      "Iteration: 431 \t--- Loss: 0.743\n",
      "Iteration: 432 \t--- Loss: 0.767\n",
      "Iteration: 433 \t--- Loss: 0.774\n",
      "Iteration: 434 \t--- Loss: 0.784\n",
      "Iteration: 435 \t--- Loss: 0.761\n",
      "Iteration: 436 \t--- Loss: 0.765\n",
      "Iteration: 437 \t--- Loss: 0.768\n",
      "Iteration: 438 \t--- Loss: 0.770\n",
      "Iteration: 439 \t--- Loss: 0.774\n",
      "Iteration: 440 \t--- Loss: 0.776\n",
      "Iteration: 441 \t--- Loss: 0.780\n",
      "Iteration: 442 \t--- Loss: 0.780\n",
      "Iteration: 443 \t--- Loss: 0.771\n",
      "Iteration: 444 \t--- Loss: 0.764\n",
      "Iteration: 445 \t--- Loss: 0.750\n",
      "Iteration: 446 \t--- Loss: 0.772\n",
      "Iteration: 447 \t--- Loss: 0.765\n",
      "Iteration: 448 \t--- Loss: 0.758\n",
      "Iteration: 449 \t--- Loss: 0.751\n",
      "Iteration: 450 \t--- Loss: 0.761\n",
      "Iteration: 451 \t--- Loss: 0.793\n",
      "Iteration: 452 \t--- Loss: 0.763\n",
      "Iteration: 453 \t--- Loss: 0.766\n",
      "Iteration: 454 \t--- Loss: 0.765\n",
      "Iteration: 455 \t--- Loss: 0.786\n",
      "Iteration: 456 \t--- Loss: 0.770\n",
      "Iteration: 457 \t--- Loss: 0.749\n",
      "Iteration: 458 \t--- Loss: 0.757\n",
      "Iteration: 459 \t--- Loss: 0.783\n",
      "Iteration: 460 \t--- Loss: 0.790\n",
      "Iteration: 461 \t--- Loss: 0.758\n",
      "Iteration: 462 \t--- Loss: 0.775\n",
      "Iteration: 463 \t--- Loss: 0.768\n",
      "Iteration: 464 \t--- Loss: 0.784\n",
      "Iteration: 465 \t--- Loss: 0.773\n",
      "Iteration: 466 \t--- Loss: 0.773\n",
      "Iteration: 467 \t--- Loss: 0.772\n",
      "Iteration: 468 \t--- Loss: 0.768\n",
      "Iteration: 469 \t--- Loss: 0.758\n",
      "Iteration: 470 \t--- Loss: 0.777\n",
      "Iteration: 471 \t--- Loss: 0.752\n",
      "Iteration: 472 \t--- Loss: 0.780\n",
      "Iteration: 473 \t--- Loss: 0.786\n",
      "Iteration: 474 \t--- Loss: 0.777\n",
      "Iteration: 475 \t--- Loss: 0.769\n",
      "Iteration: 476 \t--- Loss: 0.778\n",
      "Iteration: 477 \t--- Loss: 0.773\n",
      "Iteration: 478 \t--- Loss: 0.784\n",
      "Iteration: 479 \t--- Loss: 0.760\n",
      "Iteration: 480 \t--- Loss: 0.748\n",
      "Iteration: 481 \t--- Loss: 0.761\n",
      "Iteration: 482 \t--- Loss: 0.773\n",
      "Iteration: 483 \t--- Loss: 0.761\n",
      "Iteration: 484 \t--- Loss: 0.778\n",
      "Iteration: 485 \t--- Loss: 0.760\n",
      "Iteration: 486 \t--- Loss: 0.770\n",
      "Iteration: 487 \t--- Loss: 0.778\n",
      "Iteration: 488 \t--- Loss: 0.769\n",
      "Iteration: 489 \t--- Loss: 0.766\n",
      "Iteration: 490 \t--- Loss: 0.770\n",
      "Iteration: 491 \t--- Loss: 0.766\n",
      "Iteration: 492 \t--- Loss: 0.763\n",
      "Iteration: 493 \t--- Loss: 0.782\n",
      "Iteration: 494 \t--- Loss: 0.783\n",
      "Iteration: 495 \t--- Loss: 0.781\n",
      "Iteration: 496 \t--- Loss: 0.774\n",
      "Iteration: 497 \t--- Loss: 0.768\n",
      "Iteration: 498 \t--- Loss: 0.766\n",
      "Iteration: 499 \t--- Loss: 0.763\n",
      "----  Optimizing the metamodel  ----\n",
      "Iteration: 0 \t--- Loss: 0.236\n",
      "Iteration: 1 \t--- Loss: 0.239\n",
      "Iteration: 2 \t--- Loss: 0.232\n",
      "Iteration: 3 \t--- Loss: 0.222\n",
      "Iteration: 4 \t--- Loss: 0.214\n",
      "Iteration: 5 \t--- Loss: 0.203\n",
      "Iteration: 6 \t--- Loss: 0.204\n",
      "Iteration: 7 \t--- Loss: 0.212\n",
      "Iteration: 8 \t--- Loss: 0.205\n",
      "Iteration: 9 \t--- Loss: 0.211\n",
      "Iteration: 10 \t--- Loss: 0.197\n",
      "Iteration: 11 \t--- Loss: 0.184\n",
      "Iteration: 12 \t--- Loss: 0.177\n",
      "Iteration: 13 \t--- Loss: 0.189\n",
      "Iteration: 14 \t--- Loss: 0.183\n",
      "Iteration: 15 \t--- Loss: 0.200\n",
      "Iteration: 16 \t--- Loss: 0.181\n",
      "Iteration: 17 \t--- Loss: 0.185\n",
      "Iteration: 18 \t--- Loss: 0.183\n",
      "Iteration: 19 \t--- Loss: 0.170\n",
      "Iteration: 20 \t--- Loss: 0.188\n",
      "Iteration: 21 \t--- Loss: 0.169\n",
      "Iteration: 22 \t--- Loss: 0.177\n",
      "Iteration: 23 \t--- Loss: 0.174\n",
      "Iteration: 24 \t--- Loss: 0.183\n",
      "Iteration: 25 \t--- Loss: 0.171\n",
      "Iteration: 26 \t--- Loss: 0.175\n",
      "Iteration: 27 \t--- Loss: 0.159\n",
      "Iteration: 28 \t--- Loss: 0.160\n",
      "Iteration: 29 \t--- Loss: 0.156\n",
      "Iteration: 30 \t--- Loss: 0.137\n",
      "Iteration: 31 \t--- Loss: 0.120\n",
      "Iteration: 32 \t--- Loss: 0.117\n",
      "Iteration: 33 \t--- Loss: 0.118\n",
      "Iteration: 34 \t--- Loss: 0.122\n",
      "Iteration: 35 \t--- Loss: 0.116\n",
      "Iteration: 36 \t--- Loss: 0.129\n",
      "Iteration: 37 \t--- Loss: 0.134\n",
      "Iteration: 38 \t--- Loss: 0.162\n",
      "Iteration: 39 \t--- Loss: 0.194\n",
      "Iteration: 40 \t--- Loss: 0.173\n",
      "Iteration: 41 \t--- Loss: 0.164\n",
      "Iteration: 42 \t--- Loss: 0.126\n",
      "Iteration: 43 \t--- Loss: 0.119\n",
      "Iteration: 44 \t--- Loss: 0.133\n",
      "Iteration: 45 \t--- Loss: 0.147\n",
      "Iteration: 46 \t--- Loss: 0.173\n",
      "Iteration: 47 \t--- Loss: 0.136\n",
      "Iteration: 48 \t--- Loss: 0.096\n",
      "Iteration: 49 \t--- Loss: 0.105\n",
      "Iteration: 50 \t--- Loss: 0.126\n",
      "Iteration: 51 \t--- Loss: 0.175\n",
      "Iteration: 52 \t--- Loss: 0.159\n",
      "Iteration: 53 \t--- Loss: 0.162\n",
      "Iteration: 54 \t--- Loss: 0.156\n",
      "Iteration: 55 \t--- Loss: 0.159\n",
      "Iteration: 56 \t--- Loss: 0.164\n",
      "Iteration: 57 \t--- Loss: 0.153\n",
      "Iteration: 58 \t--- Loss: 0.145\n",
      "Iteration: 59 \t--- Loss: 0.147\n",
      "Iteration: 60 \t--- Loss: 0.144\n",
      "Iteration: 61 \t--- Loss: 0.136\n",
      "Iteration: 62 \t--- Loss: 0.146\n",
      "Iteration: 63 \t--- Loss: 0.141\n",
      "Iteration: 64 \t--- Loss: 0.139\n",
      "Iteration: 65 \t--- Loss: 0.116\n",
      "Iteration: 66 \t--- Loss: 0.102\n",
      "Iteration: 67 \t--- Loss: 0.077\n",
      "Iteration: 68 \t--- Loss: 0.085\n",
      "Iteration: 69 \t--- Loss: 0.085\n",
      "Iteration: 70 \t--- Loss: 0.072\n",
      "Iteration: 71 \t--- Loss: 0.075\n",
      "Iteration: 72 \t--- Loss: 0.082\n",
      "Iteration: 73 \t--- Loss: 0.074\n",
      "Iteration: 74 \t--- Loss: 0.078\n",
      "Iteration: 75 \t--- Loss: 0.071\n",
      "Iteration: 76 \t--- Loss: 0.096\n",
      "Iteration: 77 \t--- Loss: 0.144\n",
      "Iteration: 78 \t--- Loss: 0.077\n",
      "Iteration: 79 \t--- Loss: 0.069\n",
      "Iteration: 80 \t--- Loss: 0.075\n",
      "Iteration: 81 \t--- Loss: 0.081\n",
      "Iteration: 82 \t--- Loss: 0.092\n",
      "Iteration: 83 \t--- Loss: 0.146\n",
      "Iteration: 84 \t--- Loss: 0.151\n",
      "Iteration: 85 \t--- Loss: 0.148\n",
      "Iteration: 86 \t--- Loss: 0.145\n",
      "Iteration: 87 \t--- Loss: 0.145\n",
      "Iteration: 88 \t--- Loss: 0.135\n",
      "Iteration: 89 \t--- Loss: 0.140\n",
      "Iteration: 90 \t--- Loss: 0.142\n",
      "Iteration: 91 \t--- Loss: 0.135\n",
      "Iteration: 92 \t--- Loss: 0.099\n",
      "Iteration: 93 \t--- Loss: 0.061\n",
      "Iteration: 94 \t--- Loss: 0.056\n",
      "Iteration: 95 \t--- Loss: 0.048\n",
      "Iteration: 96 \t--- Loss: 0.053\n",
      "Iteration: 97 \t--- Loss: 0.048\n",
      "Iteration: 98 \t--- Loss: 0.053\n",
      "Iteration: 99 \t--- Loss: 0.049\n",
      "Iteration: 100 \t--- Loss: 0.054\n",
      "Iteration: 101 \t--- Loss: 0.052\n",
      "Iteration: 102 \t--- Loss: 0.059\n",
      "Iteration: 103 \t--- Loss: 0.053\n",
      "Iteration: 104 \t--- Loss: 0.076\n",
      "Iteration: 105 \t--- Loss: 0.101\n",
      "Iteration: 106 \t--- Loss: 0.043\n",
      "Iteration: 107 \t--- Loss: 0.045\n",
      "Iteration: 108 \t--- Loss: 0.048\n",
      "Iteration: 109 \t--- Loss: 0.061\n",
      "Iteration: 110 \t--- Loss: 0.087\n",
      "Iteration: 111 \t--- Loss: 0.069\n",
      "Iteration: 112 \t--- Loss: 0.108\n",
      "Iteration: 113 \t--- Loss: 0.044\n",
      "Iteration: 114 \t--- Loss: 0.059\n",
      "Iteration: 115 \t--- Loss: 0.066\n",
      "Iteration: 116 \t--- Loss: 0.050\n",
      "Iteration: 117 \t--- Loss: 0.061\n",
      "Iteration: 118 \t--- Loss: 0.053\n",
      "Iteration: 119 \t--- Loss: 0.059\n",
      "Iteration: 120 \t--- Loss: 0.053\n",
      "Iteration: 121 \t--- Loss: 0.063\n",
      "Iteration: 122 \t--- Loss: 0.034\n",
      "Iteration: 123 \t--- Loss: 0.036\n",
      "Iteration: 124 \t--- Loss: 0.045\n",
      "Iteration: 125 \t--- Loss: 0.045\n",
      "Iteration: 126 \t--- Loss: 0.062\n",
      "Iteration: 127 \t--- Loss: 0.095\n",
      "Iteration: 128 \t--- Loss: 0.043\n",
      "Iteration: 129 \t--- Loss: 0.039\n",
      "Iteration: 130 \t--- Loss: 0.036\n",
      "Iteration: 131 \t--- Loss: 0.032\n",
      "Iteration: 132 \t--- Loss: 0.032\n",
      "Iteration: 133 \t--- Loss: 0.035\n",
      "Iteration: 134 \t--- Loss: 0.037\n",
      "Iteration: 135 \t--- Loss: 0.035\n",
      "Iteration: 136 \t--- Loss: 0.037\n",
      "Iteration: 137 \t--- Loss: 0.038\n",
      "Iteration: 138 \t--- Loss: 0.037\n",
      "Iteration: 139 \t--- Loss: 0.031\n",
      "Iteration: 140 \t--- Loss: 0.036\n",
      "Iteration: 141 \t--- Loss: 0.032\n",
      "Iteration: 142 \t--- Loss: 0.032\n",
      "Iteration: 143 \t--- Loss: 0.032\n",
      "Iteration: 144 \t--- Loss: 0.036\n",
      "Iteration: 145 \t--- Loss: 0.031\n",
      "Iteration: 146 \t--- Loss: 0.034\n",
      "Iteration: 147 \t--- Loss: 0.039\n",
      "Iteration: 148 \t--- Loss: 0.044\n",
      "Iteration: 149 \t--- Loss: 0.027\n",
      "Iteration: 150 \t--- Loss: 0.023\n",
      "Iteration: 151 \t--- Loss: 0.025\n",
      "Iteration: 152 \t--- Loss: 0.023\n",
      "Iteration: 153 \t--- Loss: 0.026\n",
      "Iteration: 154 \t--- Loss: 0.028\n",
      "Iteration: 155 \t--- Loss: 0.033\n",
      "Iteration: 156 \t--- Loss: 0.031\n",
      "Iteration: 157 \t--- Loss: 0.031\n",
      "Iteration: 158 \t--- Loss: 0.031\n",
      "Iteration: 159 \t--- Loss: 0.027\n",
      "Iteration: 160 \t--- Loss: 0.026\n",
      "Iteration: 161 \t--- Loss: 0.031\n",
      "Iteration: 162 \t--- Loss: 0.037\n",
      "Iteration: 163 \t--- Loss: 0.025\n",
      "Iteration: 164 \t--- Loss: 0.021\n",
      "Iteration: 165 \t--- Loss: 0.025\n",
      "Iteration: 166 \t--- Loss: 0.027\n",
      "Iteration: 167 \t--- Loss: 0.024\n",
      "Iteration: 168 \t--- Loss: 0.023\n",
      "Iteration: 169 \t--- Loss: 0.022\n",
      "Iteration: 170 \t--- Loss: 0.023\n",
      "Iteration: 171 \t--- Loss: 0.022\n",
      "Iteration: 172 \t--- Loss: 0.021\n",
      "Iteration: 173 \t--- Loss: 0.022\n",
      "Iteration: 174 \t--- Loss: 0.021\n",
      "Iteration: 175 \t--- Loss: 0.025\n",
      "Iteration: 176 \t--- Loss: 0.024\n",
      "Iteration: 177 \t--- Loss: 0.019\n",
      "Iteration: 178 \t--- Loss: 0.017\n",
      "Iteration: 179 \t--- Loss: 0.019\n",
      "Iteration: 180 \t--- Loss: 0.017\n",
      "Iteration: 181 \t--- Loss: 0.021\n",
      "Iteration: 182 \t--- Loss: 0.021\n",
      "Iteration: 183 \t--- Loss: 0.020\n",
      "Iteration: 184 \t--- Loss: 0.017\n",
      "Iteration: 185 \t--- Loss: 0.019\n",
      "Iteration: 186 \t--- Loss: 0.022\n",
      "Iteration: 187 \t--- Loss: 0.021\n",
      "Iteration: 188 \t--- Loss: 0.025\n",
      "Iteration: 189 \t--- Loss: 0.019\n",
      "Iteration: 190 \t--- Loss: 0.018\n",
      "Iteration: 191 \t--- Loss: 0.017\n",
      "Iteration: 192 \t--- Loss: 0.015\n",
      "Iteration: 193 \t--- Loss: 0.016\n",
      "Iteration: 194 \t--- Loss: 0.016\n",
      "Iteration: 195 \t--- Loss: 0.016\n",
      "Iteration: 196 \t--- Loss: 0.015\n",
      "Iteration: 197 \t--- Loss: 0.017\n",
      "Iteration: 198 \t--- Loss: 0.014\n",
      "Iteration: 199 \t--- Loss: 0.015\n",
      "Iteration: 200 \t--- Loss: 0.013\n",
      "Iteration: 201 \t--- Loss: 0.015\n",
      "Iteration: 202 \t--- Loss: 0.015\n",
      "Iteration: 203 \t--- Loss: 0.016\n",
      "Iteration: 204 \t--- Loss: 0.014\n",
      "Iteration: 205 \t--- Loss: 0.016\n",
      "Iteration: 206 \t--- Loss: 0.017\n",
      "Iteration: 207 \t--- Loss: 0.016\n",
      "Iteration: 208 \t--- Loss: 0.015\n",
      "Iteration: 209 \t--- Loss: 0.015\n",
      "Iteration: 210 \t--- Loss: 0.015\n",
      "Iteration: 211 \t--- Loss: 0.015\n",
      "Iteration: 212 \t--- Loss: 0.014\n",
      "Iteration: 213 \t--- Loss: 0.014\n",
      "Iteration: 214 \t--- Loss: 0.014\n",
      "Iteration: 215 \t--- Loss: 0.015\n",
      "Iteration: 216 \t--- Loss: 0.017\n",
      "Iteration: 217 \t--- Loss: 0.016\n",
      "Iteration: 218 \t--- Loss: 0.018\n",
      "Iteration: 219 \t--- Loss: 0.014\n",
      "Iteration: 220 \t--- Loss: 0.015\n",
      "Iteration: 221 \t--- Loss: 0.013\n",
      "Iteration: 222 \t--- Loss: 0.012\n",
      "Iteration: 223 \t--- Loss: 0.012\n",
      "Iteration: 224 \t--- Loss: 0.012\n",
      "Iteration: 225 \t--- Loss: 0.011\n",
      "Iteration: 226 \t--- Loss: 0.012\n",
      "Iteration: 227 \t--- Loss: 0.011\n",
      "Iteration: 228 \t--- Loss: 0.010\n",
      "Iteration: 229 \t--- Loss: 0.011\n",
      "Iteration: 230 \t--- Loss: 0.011\n",
      "Iteration: 231 \t--- Loss: 0.011\n",
      "Iteration: 232 \t--- Loss: 0.011\n",
      "Iteration: 233 \t--- Loss: 0.010\n",
      "Iteration: 234 \t--- Loss: 0.010\n",
      "Iteration: 235 \t--- Loss: 0.012\n",
      "Iteration: 236 \t--- Loss: 0.011\n",
      "Iteration: 237 \t--- Loss: 0.010\n",
      "Iteration: 238 \t--- Loss: 0.010\n",
      "Iteration: 239 \t--- Loss: 0.011\n",
      "Iteration: 240 \t--- Loss: 0.010\n",
      "Iteration: 241 \t--- Loss: 0.011\n",
      "Iteration: 242 \t--- Loss: 0.011\n",
      "Iteration: 243 \t--- Loss: 0.011\n",
      "Iteration: 244 \t--- Loss: 0.011\n",
      "Iteration: 245 \t--- Loss: 0.012\n",
      "Iteration: 246 \t--- Loss: 0.012\n",
      "Iteration: 247 \t--- Loss: 0.013\n",
      "Iteration: 248 \t--- Loss: 0.012\n",
      "Iteration: 249 \t--- Loss: 0.012\n",
      "Iteration: 250 \t--- Loss: 0.012\n",
      "Iteration: 251 \t--- Loss: 0.012\n",
      "Iteration: 252 \t--- Loss: 0.011\n",
      "Iteration: 253 \t--- Loss: 0.012\n",
      "Iteration: 254 \t--- Loss: 0.011\n",
      "Iteration: 255 \t--- Loss: 0.012\n",
      "Iteration: 256 \t--- Loss: 0.012\n",
      "Iteration: 257 \t--- Loss: 0.011\n",
      "Iteration: 258 \t--- Loss: 0.009\n",
      "Iteration: 259 \t--- Loss: 0.009"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.50s/it][Parallel(n_jobs=5)]: Done  14 tasks      | elapsed:  6.8min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:42<00:00, 102.93s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.081\n",
      "Iteration: 261 \t--- Loss: 0.089\n",
      "Iteration: 262 \t--- Loss: 0.084\n",
      "Iteration: 263 \t--- Loss: 0.081\n",
      "Iteration: 264 \t--- Loss: 0.085\n",
      "Iteration: 265 \t--- Loss: 0.084\n",
      "Iteration: 266 \t--- Loss: 0.083\n",
      "Iteration: 267 \t--- Loss: 0.079\n",
      "Iteration: 268 \t--- Loss: 0.084\n",
      "Iteration: 269 \t--- Loss: 0.079\n",
      "Iteration: 270 \t--- Loss: 0.084\n",
      "Iteration: 271 \t--- Loss: 0.084\n",
      "Iteration: 272 \t--- Loss: 0.078\n",
      "Iteration: 273 \t--- Loss: 0.085\n",
      "Iteration: 274 \t--- Loss: 0.084\n",
      "Iteration: 275 \t--- Loss: 0.080\n",
      "Iteration: 276 \t--- Loss: 0.085\n",
      "Iteration: 277 \t--- Loss: 0.090\n",
      "Iteration: 278 \t--- Loss: 0.077\n",
      "Iteration: 279 \t--- Loss: 0.077\n",
      "Iteration: 280 \t--- Loss: 0.086\n",
      "Iteration: 281 \t--- Loss: 0.079\n",
      "Iteration: 282 \t--- Loss: 0.084\n",
      "Iteration: 283 \t--- Loss: 0.083\n",
      "Iteration: 284 \t--- Loss: 0.081\n",
      "Iteration: 285 \t--- Loss: 0.083\n",
      "Iteration: 286 \t--- Loss: 0.076\n",
      "Iteration: 287 \t--- Loss: 0.085\n",
      "Iteration: 288 \t--- Loss: 0.084\n",
      "Iteration: 289 \t--- Loss: 0.079\n",
      "Iteration: 290 \t--- Loss: 0.078\n",
      "Iteration: 291 \t--- Loss: 0.086\n",
      "Iteration: 292 \t--- Loss: 0.082\n",
      "Iteration: 293 \t--- Loss: 0.080\n",
      "Iteration: 294 \t--- Loss: 0.081\n",
      "Iteration: 295 \t--- Loss: 0.086\n",
      "Iteration: 296 \t--- Loss: 0.081\n",
      "Iteration: 297 \t--- Loss: 0.087\n",
      "Iteration: 298 \t--- Loss: 0.082\n",
      "Iteration: 299 \t--- Loss: 0.087\n",
      "Iteration: 300 \t--- Loss: 0.084\n",
      "Iteration: 301 \t--- Loss: 0.084\n",
      "Iteration: 302 \t--- Loss: 0.081\n",
      "Iteration: 303 \t--- Loss: 0.078\n",
      "Iteration: 304 \t--- Loss: 0.080\n",
      "Iteration: 305 \t--- Loss: 0.083\n",
      "Iteration: 306 \t--- Loss: 0.084\n",
      "Iteration: 307 \t--- Loss: 0.086\n",
      "Iteration: 308 \t--- Loss: 0.078\n",
      "Iteration: 309 \t--- Loss: 0.079\n",
      "Iteration: 310 \t--- Loss: 0.079\n",
      "Iteration: 311 \t--- Loss: 0.074\n",
      "Iteration: 312 \t--- Loss: 0.080\n",
      "Iteration: 313 \t--- Loss: 0.086\n",
      "Iteration: 314 \t--- Loss: 0.083\n",
      "Iteration: 315 \t--- Loss: 0.080\n",
      "Iteration: 316 \t--- Loss: 0.080\n",
      "Iteration: 317 \t--- Loss: 0.085\n",
      "Iteration: 318 \t--- Loss: 0.079\n",
      "Iteration: 319 \t--- Loss: 0.078\n",
      "Iteration: 320 \t--- Loss: 0.083\n",
      "Iteration: 321 \t--- Loss: 0.079\n",
      "Iteration: 322 \t--- Loss: 0.084\n",
      "Iteration: 323 \t--- Loss: 0.081\n",
      "Iteration: 324 \t--- Loss: 0.082\n",
      "Iteration: 325 \t--- Loss: 0.080\n",
      "Iteration: 326 \t--- Loss: 0.076\n",
      "Iteration: 327 \t--- Loss: 0.078\n",
      "Iteration: 328 \t--- Loss: 0.080\n",
      "Iteration: 329 \t--- Loss: 0.087\n",
      "Iteration: 330 \t--- Loss: 0.085\n",
      "Iteration: 331 \t--- Loss: 0.081\n",
      "Iteration: 332 \t--- Loss: 0.078\n",
      "Iteration: 333 \t--- Loss: 0.078\n",
      "Iteration: 334 \t--- Loss: 0.081\n",
      "Iteration: 335 \t--- Loss: 0.076\n",
      "Iteration: 336 \t--- Loss: 0.083\n",
      "Iteration: 337 \t--- Loss: 0.085\n",
      "Iteration: 338 \t--- Loss: 0.079\n",
      "Iteration: 339 \t--- Loss: 0.081\n",
      "Iteration: 340 \t--- Loss: 0.079\n",
      "Iteration: 341 \t--- Loss: 0.076\n",
      "Iteration: 342 \t--- Loss: 0.081\n",
      "Iteration: 343 \t--- Loss: 0.081\n",
      "Iteration: 344 \t--- Loss: 0.084\n",
      "Iteration: 345 \t--- Loss: 0.075\n",
      "Iteration: 346 \t--- Loss: 0.086\n",
      "Iteration: 347 \t--- Loss: 0.080\n",
      "Iteration: 348 \t--- Loss: 0.079\n",
      "Iteration: 349 \t--- Loss: 0.083\n",
      "Iteration: 350 \t--- Loss: 0.080\n",
      "Iteration: 351 \t--- Loss: 0.086\n",
      "Iteration: 352 \t--- Loss: 0.078\n",
      "Iteration: 353 \t--- Loss: 0.078\n",
      "Iteration: 354 \t--- Loss: 0.082\n",
      "Iteration: 355 \t--- Loss: 0.079\n",
      "Iteration: 356 \t--- Loss: 0.079\n",
      "Iteration: 357 \t--- Loss: 0.085\n",
      "Iteration: 358 \t--- Loss: 0.082\n",
      "Iteration: 359 \t--- Loss: 0.082\n",
      "Iteration: 360 \t--- Loss: 0.075\n",
      "Iteration: 361 \t--- Loss: 0.082\n",
      "Iteration: 362 \t--- Loss: 0.077\n",
      "Iteration: 363 \t--- Loss: 0.087\n",
      "Iteration: 364 \t--- Loss: 0.086\n",
      "Iteration: 365 \t--- Loss: 0.079\n",
      "Iteration: 366 \t--- Loss: 0.082\n",
      "Iteration: 367 \t--- Loss: 0.080\n",
      "Iteration: 368 \t--- Loss: 0.083\n",
      "Iteration: 369 \t--- Loss: 0.080\n",
      "Iteration: 370 \t--- Loss: 0.080\n",
      "Iteration: 371 \t--- Loss: 0.082\n",
      "Iteration: 372 \t--- Loss: 0.085\n",
      "Iteration: 373 \t--- Loss: 0.082\n",
      "Iteration: 374 \t--- Loss: 0.081\n",
      "Iteration: 375 \t--- Loss: 0.082\n",
      "Iteration: 376 \t--- Loss: 0.082\n",
      "Iteration: 377 \t--- Loss: 0.087\n",
      "Iteration: 378 \t--- Loss: 0.080\n",
      "Iteration: 379 \t--- Loss: 0.079\n",
      "Iteration: 380 \t--- Loss: 0.079\n",
      "Iteration: 381 \t--- Loss: 0.078\n",
      "Iteration: 382 \t--- Loss: 0.076\n",
      "Iteration: 383 \t--- Loss: 0.083\n",
      "Iteration: 384 \t--- Loss: 0.080\n",
      "Iteration: 385 \t--- Loss: 0.078\n",
      "Iteration: 386 \t--- Loss: 0.083\n",
      "Iteration: 387 \t--- Loss: 0.078\n",
      "Iteration: 388 \t--- Loss: 0.084\n",
      "Iteration: 389 \t--- Loss: 0.080\n",
      "Iteration: 390 \t--- Loss: 0.077\n",
      "Iteration: 391 \t--- Loss: 0.080\n",
      "Iteration: 392 \t--- Loss: 0.077\n",
      "Iteration: 393 \t--- Loss: 0.083\n",
      "Iteration: 394 \t--- Loss: 0.082\n",
      "Iteration: 395 \t--- Loss: 0.083\n",
      "Iteration: 396 \t--- Loss: 0.080\n",
      "Iteration: 397 \t--- Loss: 0.086\n",
      "Iteration: 398 \t--- Loss: 0.084\n",
      "Iteration: 399 \t--- Loss: 0.081\n",
      "Iteration: 400 \t--- Loss: 0.083\n",
      "Iteration: 401 \t--- Loss: 0.083\n",
      "Iteration: 402 \t--- Loss: 0.085\n",
      "Iteration: 403 \t--- Loss: 0.087\n",
      "Iteration: 404 \t--- Loss: 0.083\n",
      "Iteration: 405 \t--- Loss: 0.074\n",
      "Iteration: 406 \t--- Loss: 0.088\n",
      "Iteration: 407 \t--- Loss: 0.080\n",
      "Iteration: 408 \t--- Loss: 0.076\n",
      "Iteration: 409 \t--- Loss: 0.083\n",
      "Iteration: 410 \t--- Loss: 0.081\n",
      "Iteration: 411 \t--- Loss: 0.083\n",
      "Iteration: 412 \t--- Loss: 0.073\n",
      "Iteration: 413 \t--- Loss: 0.079\n",
      "Iteration: 414 \t--- Loss: 0.077\n",
      "Iteration: 415 \t--- Loss: 0.080\n",
      "Iteration: 416 \t--- Loss: 0.078\n",
      "Iteration: 417 \t--- Loss: 0.079\n",
      "Iteration: 418 \t--- Loss: 0.085\n",
      "Iteration: 419 \t--- Loss: 0.079\n",
      "Iteration: 420 \t--- Loss: 0.078\n",
      "Iteration: 421 \t--- Loss: 0.079\n",
      "Iteration: 422 \t--- Loss: 0.082\n",
      "Iteration: 423 \t--- Loss: 0.081\n",
      "Iteration: 424 \t--- Loss: 0.080\n",
      "Iteration: 425 \t--- Loss: 0.086\n",
      "Iteration: 426 \t--- Loss: 0.078\n",
      "Iteration: 427 \t--- Loss: 0.081\n",
      "Iteration: 428 \t--- Loss: 0.083\n",
      "Iteration: 429 \t--- Loss: 0.082\n",
      "Iteration: 430 \t--- Loss: 0.085\n",
      "Iteration: 431 \t--- Loss: 0.081\n",
      "Iteration: 432 \t--- Loss: 0.079\n",
      "Iteration: 433 \t--- Loss: 0.077\n",
      "Iteration: 434 \t--- Loss: 0.085\n",
      "Iteration: 435 \t--- Loss: 0.080\n",
      "Iteration: 436 \t--- Loss: 0.079\n",
      "Iteration: 437 \t--- Loss: 0.079\n",
      "Iteration: 438 \t--- Loss: 0.080\n",
      "Iteration: 439 \t--- Loss: 0.080\n",
      "Iteration: 440 \t--- Loss: 0.075\n",
      "Iteration: 441 \t--- Loss: 0.080\n",
      "Iteration: 442 \t--- Loss: 0.079\n",
      "Iteration: 443 \t--- Loss: 0.077\n",
      "Iteration: 444 \t--- Loss: 0.079\n",
      "Iteration: 445 \t--- Loss: 0.080\n",
      "Iteration: 446 \t--- Loss: 0.077\n",
      "Iteration: 447 \t--- Loss: 0.078\n",
      "Iteration: 448 \t--- Loss: 0.081\n",
      "Iteration: 449 \t--- Loss: 0.075\n",
      "Iteration: 450 \t--- Loss: 0.082\n",
      "Iteration: 451 \t--- Loss: 0.079\n",
      "Iteration: 452 \t--- Loss: 0.075\n",
      "Iteration: 453 \t--- Loss: 0.077\n",
      "Iteration: 454 \t--- Loss: 0.087\n",
      "Iteration: 455 \t--- Loss: 0.081\n",
      "Iteration: 456 \t--- Loss: 0.078\n",
      "Iteration: 457 \t--- Loss: 0.080\n",
      "Iteration: 458 \t--- Loss: 0.080\n",
      "Iteration: 459 \t--- Loss: 0.084\n",
      "Iteration: 460 \t--- Loss: 0.079\n",
      "Iteration: 461 \t--- Loss: 0.077\n",
      "Iteration: 462 \t--- Loss: 0.079\n",
      "Iteration: 463 \t--- Loss: 0.082\n",
      "Iteration: 464 \t--- Loss: 0.083\n",
      "Iteration: 465 \t--- Loss: 0.081\n",
      "Iteration: 466 \t--- Loss: 0.081\n",
      "Iteration: 467 \t--- Loss: 0.082\n",
      "Iteration: 468 \t--- Loss: 0.078\n",
      "Iteration: 469 \t--- Loss: 0.082\n",
      "Iteration: 470 \t--- Loss: 0.079\n",
      "Iteration: 471 \t--- Loss: 0.081\n",
      "Iteration: 472 \t--- Loss: 0.077\n",
      "Iteration: 473 \t--- Loss: 0.086\n",
      "Iteration: 474 \t--- Loss: 0.079\n",
      "Iteration: 475 \t--- Loss: 0.081\n",
      "Iteration: 476 \t--- Loss: 0.082\n",
      "Iteration: 477 \t--- Loss: 0.083\n",
      "Iteration: 478 \t--- Loss: 0.079\n",
      "Iteration: 479 \t--- Loss: 0.081\n",
      "Iteration: 480 \t--- Loss: 0.073\n",
      "Iteration: 481 \t--- Loss: 0.083\n",
      "Iteration: 482 \t--- Loss: 0.080\n",
      "Iteration: 483 \t--- Loss: 0.078\n",
      "Iteration: 484 \t--- Loss: 0.080\n",
      "Iteration: 485 \t--- Loss: 0.076\n",
      "Iteration: 486 \t--- Loss: 0.079\n",
      "Iteration: 487 \t--- Loss: 0.085\n",
      "Iteration: 488 \t--- Loss: 0.081\n",
      "Iteration: 489 \t--- Loss: 0.079\n",
      "Iteration: 490 \t--- Loss: 0.084\n",
      "Iteration: 491 \t--- Loss: 0.079\n",
      "Iteration: 492 \t--- Loss: 0.081\n",
      "Iteration: 493 \t--- Loss: 0.076\n",
      "Iteration: 494 \t--- Loss: 0.085\n",
      "Iteration: 495 \t--- Loss: 0.085\n",
      "Iteration: 496 \t--- Loss: 0.085\n",
      "Iteration: 497 \t--- Loss: 0.078\n",
      "Iteration: 498 \t--- Loss: 0.082\n",
      "Iteration: 499 \t--- Loss: 0.086\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.59s/it][Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  7.3min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.58s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.787\n",
      "Iteration: 1 \t--- Loss: 0.700\n",
      "Iteration: 2 \t--- Loss: 0.643\n",
      "Iteration: 3 \t--- Loss: 0.601\n",
      "Iteration: 4 \t--- Loss: 0.550\n",
      "Iteration: 5 \t--- Loss: 0.519\n",
      "Iteration: 6 \t--- Loss: 0.494\n",
      "Iteration: 7 \t--- Loss: 0.478\n",
      "Iteration: 8 \t--- Loss: 0.462\n",
      "Iteration: 9 \t--- Loss: 0.446\n",
      "Iteration: 10 \t--- Loss: 0.428\n",
      "Iteration: 11 \t--- Loss: 0.421\n",
      "Iteration: 12 \t--- Loss: 0.421\n",
      "Iteration: 13 \t--- Loss: 0.409\n",
      "Iteration: 14 \t--- Loss: 0.408\n",
      "Iteration: 15 \t--- Loss: 0.414\n",
      "Iteration: 16 \t--- Loss: 0.385\n",
      "Iteration: 17 \t--- Loss: 0.391\n",
      "Iteration: 18 \t--- Loss: 0.385\n",
      "Iteration: 19 \t--- Loss: 0.383\n",
      "Iteration: 20 \t--- Loss: 0.378\n",
      "Iteration: 21 \t--- Loss: 0.380\n",
      "Iteration: 22 \t--- Loss: 0.380\n",
      "Iteration: 23 \t--- Loss: 0.384\n",
      "Iteration: 24 \t--- Loss: 0.375\n",
      "Iteration: 25 \t--- Loss: 0.378\n",
      "Iteration: 26 \t--- Loss: 0.374\n",
      "Iteration: 27 \t--- Loss: 0.368\n",
      "Iteration: 28 \t--- Loss: 0.377\n",
      "Iteration: 29 \t--- Loss: 0.365\n",
      "Iteration: 30 \t--- Loss: 0.370\n",
      "Iteration: 31 \t--- Loss: 0.384\n",
      "Iteration: 32 \t--- Loss: 0.380\n",
      "Iteration: 33 \t--- Loss: 0.365\n",
      "Iteration: 34 \t--- Loss: 0.364\n",
      "Iteration: 35 \t--- Loss: 0.373\n",
      "Iteration: 36 \t--- Loss: 0.366\n",
      "Iteration: 37 \t--- Loss: 0.375\n",
      "Iteration: 38 \t--- Loss: 0.372\n",
      "Iteration: 39 \t--- Loss: 0.388\n",
      "Iteration: 40 \t--- Loss: 0.370\n",
      "Iteration: 41 \t--- Loss: 0.370\n",
      "Iteration: 42 \t--- Loss: 0.373\n",
      "Iteration: 43 \t--- Loss: 0.361\n",
      "Iteration: 44 \t--- Loss: 0.373\n",
      "Iteration: 45 \t--- Loss: 0.369\n",
      "Iteration: 46 \t--- Loss: 0.380\n",
      "Iteration: 47 \t--- Loss: 0.377\n",
      "Iteration: 48 \t--- Loss: 0.361\n",
      "Iteration: 49 \t--- Loss: 0.374\n",
      "Iteration: 50 \t--- Loss: 0.370\n",
      "Iteration: 51 \t--- Loss: 0.363\n",
      "Iteration: 52 \t--- Loss: 0.371\n",
      "Iteration: 53 \t--- Loss: 0.361\n",
      "Iteration: 54 \t--- Loss: 0.364\n",
      "Iteration: 55 \t--- Loss: 0.373\n",
      "Iteration: 56 \t--- Loss: 0.367\n",
      "Iteration: 57 \t--- Loss: 0.368\n",
      "Iteration: 58 \t--- Loss: 0.361\n",
      "Iteration: 59 \t--- Loss: 0.373\n",
      "Iteration: 60 \t--- Loss: 0.380\n",
      "Iteration: 61 \t--- Loss: 0.382\n",
      "Iteration: 62 \t--- Loss: 0.373\n",
      "Iteration: 63 \t--- Loss: 0.369\n",
      "Iteration: 64 \t--- Loss: 0.388\n",
      "Iteration: 65 \t--- Loss: 0.362\n",
      "Iteration: 66 \t--- Loss: 0.370\n",
      "Iteration: 67 \t--- Loss: 0.360\n",
      "Iteration: 68 \t--- Loss: 0.363\n",
      "Iteration: 69 \t--- Loss: 0.370\n",
      "Iteration: 70 \t--- Loss: 0.367\n",
      "Iteration: 71 \t--- Loss: 0.377\n",
      "Iteration: 72 \t--- Loss: 0.374\n",
      "Iteration: 73 \t--- Loss: 0.365\n",
      "Iteration: 74 \t--- Loss: 0.367\n",
      "Iteration: 75 \t--- Loss: 0.366\n",
      "Iteration: 76 \t--- Loss: 0.361\n",
      "Iteration: 77 \t--- Loss: 0.372\n",
      "Iteration: 78 \t--- Loss: 0.368\n",
      "Iteration: 79 \t--- Loss: 0.368\n",
      "Iteration: 80 \t--- Loss: 0.375\n",
      "Iteration: 81 \t--- Loss: 0.379\n",
      "Iteration: 82 \t--- Loss: 0.368\n",
      "Iteration: 83 \t--- Loss: 0.373\n",
      "Iteration: 84 \t--- Loss: 0.368\n",
      "Iteration: 85 \t--- Loss: 0.369\n",
      "Iteration: 86 \t--- Loss: 0.365\n",
      "Iteration: 87 \t--- Loss: 0.364\n",
      "Iteration: 88 \t--- Loss: 0.368\n",
      "Iteration: 89 \t--- Loss: 0.369\n",
      "Iteration: 90 \t--- Loss: 0.370\n",
      "Iteration: 91 \t--- Loss: 0.374\n",
      "Iteration: 92 \t--- Loss: 0.373\n",
      "Iteration: 93 \t--- Loss: 0.373\n",
      "Iteration: 94 \t--- Loss: 0.357\n",
      "Iteration: 95 \t--- Loss: 0.365\n",
      "Iteration: 96 \t--- Loss: 0.368\n",
      "Iteration: 97 \t--- Loss: 0.357\n",
      "Iteration: 98 \t--- Loss: 0.370\n",
      "Iteration: 99 \t--- Loss: 0.367\n",
      "Iteration: 100 \t--- Loss: 0.369\n",
      "Iteration: 101 \t--- Loss: 0.368\n",
      "Iteration: 102 \t--- Loss: 0.376\n",
      "Iteration: 103 \t--- Loss: 0.370\n",
      "Iteration: 104 \t--- Loss: 0.373\n",
      "Iteration: 105 \t--- Loss: 0.368\n",
      "Iteration: 106 \t--- Loss: 0.367\n",
      "Iteration: 107 \t--- Loss: 0.378\n",
      "Iteration: 108 \t--- Loss: 0.365\n",
      "Iteration: 109 \t--- Loss: 0.358\n",
      "Iteration: 110 \t--- Loss: 0.360\n",
      "Iteration: 111 \t--- Loss: 0.371\n",
      "Iteration: 112 \t--- Loss: 0.370\n",
      "Iteration: 113 \t--- Loss: 0.376\n",
      "Iteration: 114 \t--- Loss: 0.370\n",
      "Iteration: 115 \t--- Loss: 0.372\n",
      "Iteration: 116 \t--- Loss: 0.366\n",
      "Iteration: 117 \t--- Loss: 0.375\n",
      "Iteration: 118 \t--- Loss: 0.373\n",
      "Iteration: 119 \t--- Loss: 0.376\n",
      "Iteration: 120 \t--- Loss: 0.365\n",
      "Iteration: 121 \t--- Loss: 0.373\n",
      "Iteration: 122 \t--- Loss: 0.384\n",
      "Iteration: 123 \t--- Loss: 0.364\n",
      "Iteration: 124 \t--- Loss: 0.361\n",
      "Iteration: 125 \t--- Loss: 0.373\n",
      "Iteration: 126 \t--- Loss: 0.360\n",
      "Iteration: 127 \t--- Loss: 0.370\n",
      "Iteration: 128 \t--- Loss: 0.365\n",
      "Iteration: 129 \t--- Loss: 0.370\n",
      "Iteration: 130 \t--- Loss: 0.387\n",
      "Iteration: 131 \t--- Loss: 0.371\n",
      "Iteration: 132 \t--- Loss: 0.377\n",
      "Iteration: 133 \t--- Loss: 0.363\n",
      "Iteration: 134 \t--- Loss: 0.375\n",
      "Iteration: 135 \t--- Loss: 0.366\n",
      "Iteration: 136 \t--- Loss: 0.358\n",
      "Iteration: 137 \t--- Loss: 0.372\n",
      "Iteration: 138 \t--- Loss: 0.360\n",
      "Iteration: 139 \t--- Loss: 0.374\n",
      "Iteration: 140 \t--- Loss: 0.371\n",
      "Iteration: 141 \t--- Loss: 0.359\n",
      "Iteration: 142 \t--- Loss: 0.365\n",
      "Iteration: 143 \t--- Loss: 0.368\n",
      "Iteration: 144 \t--- Loss: 0.373\n",
      "Iteration: 145 \t--- Loss: 0.372\n",
      "Iteration: 146 \t--- Loss: 0.367\n",
      "Iteration: 147 \t--- Loss: 0.371\n",
      "Iteration: 148 \t--- Loss: 0.369\n",
      "Iteration: 149 \t--- Loss: 0.370\n",
      "Iteration: 150 \t--- Loss: 0.360\n",
      "Iteration: 151 \t--- Loss: 0.363\n",
      "Iteration: 152 \t--- Loss: 0.364\n",
      "Iteration: 153 \t--- Loss: 0.366\n",
      "Iteration: 154 \t--- Loss: 0.362\n",
      "Iteration: 155 \t--- Loss: 0.371\n",
      "Iteration: 156 \t--- Loss: 0.370\n",
      "Iteration: 157 \t--- Loss: 0.363\n",
      "Iteration: 158 \t--- Loss: 0.370\n",
      "Iteration: 159 \t--- Loss: 0.368\n",
      "Iteration: 160 \t--- Loss: 0.377\n",
      "Iteration: 161 \t--- Loss: 0.364\n",
      "Iteration: 162 \t--- Loss: 0.378\n",
      "Iteration: 163 \t--- Loss: 0.366\n",
      "Iteration: 164 \t--- Loss: 0.365\n",
      "Iteration: 165 \t--- Loss: 0.372\n",
      "Iteration: 166 \t--- Loss: 0.365\n",
      "Iteration: 167 \t--- Loss: 0.365\n",
      "Iteration: 168 \t--- Loss: 0.372\n",
      "Iteration: 169 \t--- Loss: 0.361\n",
      "Iteration: 170 \t--- Loss: 0.383\n",
      "Iteration: 171 \t--- Loss: 0.374\n",
      "Iteration: 172 \t--- Loss: 0.370\n",
      "Iteration: 173 \t--- Loss: 0.365\n",
      "Iteration: 174 \t--- Loss: 0.379\n",
      "Iteration: 175 \t--- Loss: 0.378\n",
      "Iteration: 176 \t--- Loss: 0.358\n",
      "Iteration: 177 \t--- Loss: 0.379\n",
      "Iteration: 178 \t--- Loss: 0.375\n",
      "Iteration: 179 \t--- Loss: 0.365\n",
      "Iteration: 180 \t--- Loss: 0.370\n",
      "Iteration: 181 \t--- Loss: 0.373\n",
      "Iteration: 182 \t--- Loss: 0.374\n",
      "Iteration: 183 \t--- Loss: 0.358\n",
      "Iteration: 184 \t--- Loss: 0.373\n",
      "Iteration: 185 \t--- Loss: 0.361\n",
      "Iteration: 186 \t--- Loss: 0.377\n",
      "Iteration: 187 \t--- Loss: 0.354\n",
      "Iteration: 188 \t--- Loss: 0.367\n",
      "Iteration: 189 \t--- Loss: 0.377\n",
      "Iteration: 190 \t--- Loss: 0.367\n",
      "Iteration: 191 \t--- Loss: 0.358\n",
      "Iteration: 192 \t--- Loss: 0.356\n",
      "Iteration: 193 \t--- Loss: 0.373\n",
      "Iteration: 194 \t--- Loss: 0.358\n",
      "Iteration: 195 \t--- Loss: 0.374\n",
      "Iteration: 196 \t--- Loss: 0.355\n",
      "Iteration: 197 \t--- Loss: 0.372\n",
      "Iteration: 198 \t--- Loss: 0.382\n",
      "Iteration: 199 \t--- Loss: 0.377\n",
      "Iteration: 200 \t--- Loss: 0.370\n",
      "Iteration: 201 \t--- Loss: 0.382\n",
      "Iteration: 202 \t--- Loss: 0.365\n",
      "Iteration: 203 \t--- Loss: 0.357\n",
      "Iteration: 204 \t--- Loss: 0.378\n",
      "Iteration: 205 \t--- Loss: 0.371\n",
      "Iteration: 206 \t--- Loss: 0.367\n",
      "Iteration: 207 \t--- Loss: 0.386\n",
      "Iteration: 208 \t--- Loss: 0.369\n",
      "Iteration: 209 \t--- Loss: 0.367\n",
      "Iteration: 210 \t--- Loss: 0.366\n",
      "Iteration: 211 \t--- Loss: 0.369\n",
      "Iteration: 212 \t--- Loss: 0.372\n",
      "Iteration: 213 \t--- Loss: 0.365\n",
      "Iteration: 214 \t--- Loss: 0.359\n",
      "Iteration: 215 \t--- Loss: 0.365\n",
      "Iteration: 216 \t--- Loss: 0.374\n",
      "Iteration: 217 \t--- Loss: 0.380\n",
      "Iteration: 218 \t--- Loss: 0.368\n",
      "Iteration: 219 \t--- Loss: 0.374\n",
      "Iteration: 220 \t--- Loss: 0.373\n",
      "Iteration: 221 \t--- Loss: 0.365\n",
      "Iteration: 222 \t--- Loss: 0.381\n",
      "Iteration: 223 \t--- Loss: 0.376\n",
      "Iteration: 224 \t--- Loss: 0.367\n",
      "Iteration: 225 \t--- Loss: 0.372\n",
      "Iteration: 226 \t--- Loss: 0.367\n",
      "Iteration: 227 \t--- Loss: 0.371\n",
      "Iteration: 228 \t--- Loss: 0.357\n",
      "Iteration: 229 \t--- Loss: 0.375\n",
      "Iteration: 230 \t--- Loss: 0.363\n",
      "Iteration: 231 \t--- Loss: 0.367\n",
      "Iteration: 232 \t--- Loss: 0.373\n",
      "Iteration: 233 \t--- Loss: 0.359\n",
      "Iteration: 234 \t--- Loss: 0.359\n",
      "Iteration: 235 \t--- Loss: 0.368\n",
      "Iteration: 236 \t--- Loss: 0.359\n",
      "Iteration: 237 \t--- Loss: 0.357\n",
      "Iteration: 238 \t--- Loss: 0.366\n",
      "Iteration: 239 \t--- Loss: 0.367\n",
      "Iteration: 240 \t--- Loss: 0.385\n",
      "Iteration: 241 \t--- Loss: 0.371\n",
      "Iteration: 242 \t--- Loss: 0.375\n",
      "Iteration: 243 \t--- Loss: 0.376\n",
      "Iteration: 244 \t--- Loss: 0.373\n",
      "Iteration: 245 \t--- Loss: 0.368\n",
      "Iteration: 246 \t--- Loss: 0.365\n",
      "Iteration: 247 \t--- Loss: 0.367\n",
      "Iteration: 248 \t--- Loss: 0.374\n",
      "Iteration: 249 \t--- Loss: 0.369\n",
      "Iteration: 250 \t--- Loss: 0.371\n",
      "Iteration: 251 \t--- Loss: 0.363\n",
      "Iteration: 252 \t--- Loss: 0.376\n",
      "Iteration: 253 \t--- Loss: 0.363\n",
      "Iteration: 254 \t--- Loss: 0.359\n",
      "Iteration: 255 \t--- Loss: 0.367\n",
      "Iteration: 256 \t--- Loss: 0.384\n",
      "Iteration: 257 \t--- Loss: 0.359\n",
      "Iteration: 258 \t--- Loss: 0.371\n",
      "Iteration: 259 \t--- Loss: 0.368Iteration: 0 \t--- Loss: 1.334\n",
      "Iteration: 1 \t--- Loss: 1.156\n",
      "Iteration: 2 \t--- Loss: 1.137\n",
      "Iteration: 3 \t--- Loss: 1.037\n",
      "Iteration: 4 \t--- Loss: 0.956\n",
      "Iteration: 5 \t--- Loss: 0.875\n",
      "Iteration: 6 \t--- Loss: 0.932\n",
      "Iteration: 7 \t--- Loss: 0.882\n",
      "Iteration: 8 \t--- Loss: 0.888\n",
      "Iteration: 9 \t--- Loss: 0.819\n",
      "Iteration: 10 \t--- Loss: 0.830\n",
      "Iteration: 11 \t--- Loss: 0.793\n",
      "Iteration: 12 \t--- Loss: 0.836\n",
      "Iteration: 13 \t--- Loss: 0.796\n",
      "Iteration: 14 \t--- Loss: 0.802\n",
      "Iteration: 15 \t--- Loss: 0.845\n",
      "Iteration: 16 \t--- Loss: 0.783\n",
      "Iteration: 17 \t--- Loss: 0.839\n",
      "Iteration: 18 \t--- Loss: 0.810\n",
      "Iteration: 19 \t--- Loss: 0.767\n",
      "Iteration: 20 \t--- Loss: 0.777\n",
      "Iteration: 21 \t--- Loss: 0.819\n",
      "Iteration: 22 \t--- Loss: 0.780\n",
      "Iteration: 23 \t--- Loss: 0.795\n",
      "Iteration: 24 \t--- Loss: 0.789\n",
      "Iteration: 25 \t--- Loss: 0.783\n",
      "Iteration: 26 \t--- Loss: 0.779\n",
      "Iteration: 27 \t--- Loss: 0.795\n",
      "Iteration: 28 \t--- Loss: 0.777\n",
      "Iteration: 29 \t--- Loss: 0.785\n",
      "Iteration: 30 \t--- Loss: 0.763\n",
      "Iteration: 31 \t--- Loss: 0.777\n",
      "Iteration: 32 \t--- Loss: 0.804\n",
      "Iteration: 33 \t--- Loss: 0.771\n",
      "Iteration: 34 \t--- Loss: 0.781\n",
      "Iteration: 35 \t--- Loss: 0.761\n",
      "Iteration: 36 \t--- Loss: 0.805\n",
      "Iteration: 37 \t--- Loss: 0.783\n",
      "Iteration: 38 \t--- Loss: 0.794\n",
      "Iteration: 39 \t--- Loss: 0.774\n",
      "Iteration: 40 \t--- Loss: 0.789\n",
      "Iteration: 41 \t--- Loss: 0.766\n",
      "Iteration: 42 \t--- Loss: 0.762\n",
      "Iteration: 43 \t--- Loss: 0.761\n",
      "Iteration: 44 \t--- Loss: 0.778\n",
      "Iteration: 45 \t--- Loss: 0.789\n",
      "Iteration: 46 \t--- Loss: 0.814\n",
      "Iteration: 47 \t--- Loss: 0.788\n",
      "Iteration: 48 \t--- Loss: 0.749\n",
      "Iteration: 49 \t--- Loss: 0.777\n",
      "Iteration: 50 \t--- Loss: 0.772\n",
      "Iteration: 51 \t--- Loss: 0.801\n",
      "Iteration: 52 \t--- Loss: 0.779\n",
      "Iteration: 53 \t--- Loss: 0.803\n",
      "Iteration: 54 \t--- Loss: 0.762\n",
      "Iteration: 55 \t--- Loss: 0.786\n",
      "Iteration: 56 \t--- Loss: 0.746\n",
      "Iteration: 57 \t--- Loss: 0.794\n",
      "Iteration: 58 \t--- Loss: 0.740\n",
      "Iteration: 59 \t--- Loss: 0.796\n",
      "Iteration: 60 \t--- Loss: 0.769\n",
      "Iteration: 61 \t--- Loss: 0.784\n",
      "Iteration: 62 \t--- Loss: 0.769\n",
      "Iteration: 63 \t--- Loss: 0.743\n",
      "Iteration: 64 \t--- Loss: 0.750\n",
      "Iteration: 65 \t--- Loss: 0.782\n",
      "Iteration: 66 \t--- Loss: 0.761\n",
      "Iteration: 67 \t--- Loss: 0.778\n",
      "Iteration: 68 \t--- Loss: 0.771\n",
      "Iteration: 69 \t--- Loss: 0.788\n",
      "Iteration: 70 \t--- Loss: 0.789\n",
      "Iteration: 71 \t--- Loss: 0.784\n",
      "Iteration: 72 \t--- Loss: 0.794\n",
      "Iteration: 73 \t--- Loss: 0.780\n",
      "Iteration: 74 \t--- Loss: 0.780\n",
      "Iteration: 75 \t--- Loss: 0.781\n",
      "Iteration: 76 \t--- Loss: 0.768\n",
      "Iteration: 77 \t--- Loss: 0.770\n",
      "Iteration: 78 \t--- Loss: 0.764\n",
      "Iteration: 79 \t--- Loss: 0.811\n",
      "Iteration: 80 \t--- Loss: 0.798\n",
      "Iteration: 81 \t--- Loss: 0.775\n",
      "Iteration: 82 \t--- Loss: 0.806\n",
      "Iteration: 83 \t--- Loss: 0.783\n",
      "Iteration: 84 \t--- Loss: 0.781\n",
      "Iteration: 85 \t--- Loss: 0.780\n",
      "Iteration: 86 \t--- Loss: 0.777\n",
      "Iteration: 87 \t--- Loss: 0.771\n",
      "Iteration: 88 \t--- Loss: 0.808\n",
      "Iteration: 89 \t--- Loss: 0.814\n",
      "Iteration: 90 \t--- Loss: 0.785\n",
      "Iteration: 91 \t--- Loss: 0.791\n",
      "Iteration: 92 \t--- Loss: 0.779\n",
      "Iteration: 93 \t--- Loss: 0.777\n",
      "Iteration: 94 \t--- Loss: 0.804\n",
      "Iteration: 95 \t--- Loss: 0.778\n",
      "Iteration: 96 \t--- Loss: 0.771\n",
      "Iteration: 97 \t--- Loss: 0.787\n",
      "Iteration: 98 \t--- Loss: 0.772\n",
      "Iteration: 99 \t--- Loss: 0.784\n",
      "Iteration: 100 \t--- Loss: 0.791\n",
      "Iteration: 101 \t--- Loss: 0.800\n",
      "Iteration: 102 \t--- Loss: 0.785\n",
      "Iteration: 103 \t--- Loss: 0.786\n",
      "Iteration: 104 \t--- Loss: 0.757\n",
      "Iteration: 105 \t--- Loss: 0.802\n",
      "Iteration: 106 \t--- Loss: 0.791\n",
      "Iteration: 107 \t--- Loss: 0.769\n",
      "Iteration: 108 \t--- Loss: 0.793\n",
      "Iteration: 109 \t--- Loss: 0.768\n",
      "Iteration: 110 \t--- Loss: 0.763\n",
      "Iteration: 111 \t--- Loss: 0.765\n",
      "Iteration: 112 \t--- Loss: 0.806\n",
      "Iteration: 113 \t--- Loss: 0.785\n",
      "Iteration: 114 \t--- Loss: 0.794\n",
      "Iteration: 115 \t--- Loss: 0.775\n",
      "Iteration: 116 \t--- Loss: 0.770\n",
      "Iteration: 117 \t--- Loss: 0.762\n",
      "Iteration: 118 \t--- Loss: 0.769\n",
      "Iteration: 119 \t--- Loss: 0.777\n",
      "Iteration: 120 \t--- Loss: 0.797\n",
      "Iteration: 121 \t--- Loss: 0.793\n",
      "Iteration: 122 \t--- Loss: 0.797\n",
      "Iteration: 123 \t--- Loss: 0.783\n",
      "Iteration: 124 \t--- Loss: 0.801\n",
      "Iteration: 125 \t--- Loss: 0.762\n",
      "Iteration: 126 \t--- Loss: 0.824\n",
      "Iteration: 127 \t--- Loss: 0.791\n",
      "Iteration: 128 \t--- Loss: 0.756\n",
      "Iteration: 129 \t--- Loss: 0.771\n",
      "Iteration: 130 \t--- Loss: 0.755\n",
      "Iteration: 131 \t--- Loss: 0.793\n",
      "Iteration: 132 \t--- Loss: 0.779\n",
      "Iteration: 133 \t--- Loss: 0.775\n",
      "Iteration: 134 \t--- Loss: 0.788\n",
      "Iteration: 135 \t--- Loss: 0.765\n",
      "Iteration: 136 \t--- Loss: 0.760\n",
      "Iteration: 137 \t--- Loss: 0.792\n",
      "Iteration: 138 \t--- Loss: 0.786\n",
      "Iteration: 139 \t--- Loss: 0.766\n",
      "Iteration: 140 \t--- Loss: 0.793\n",
      "Iteration: 141 \t--- Loss: 0.777\n",
      "Iteration: 142 \t--- Loss: 0.798\n",
      "Iteration: 143 \t--- Loss: 0.754\n",
      "Iteration: 144 \t--- Loss: 0.775\n",
      "Iteration: 145 \t--- Loss: 0.792\n",
      "Iteration: 146 \t--- Loss: 0.791\n",
      "Iteration: 147 \t--- Loss: 0.790\n",
      "Iteration: 148 \t--- Loss: 0.787\n",
      "Iteration: 149 \t--- Loss: 0.784\n",
      "Iteration: 150 \t--- Loss: 0.775\n",
      "Iteration: 151 \t--- Loss: 0.772\n",
      "Iteration: 152 \t--- Loss: 0.792\n",
      "Iteration: 153 \t--- Loss: 0.762\n",
      "Iteration: 154 \t--- Loss: 0.795\n",
      "Iteration: 155 \t--- Loss: 0.774\n",
      "Iteration: 156 \t--- Loss: 0.804\n",
      "Iteration: 157 \t--- Loss: 0.791\n",
      "Iteration: 158 \t--- Loss: 0.775\n",
      "Iteration: 159 \t--- Loss: 0.766\n",
      "Iteration: 160 \t--- Loss: 0.776\n",
      "Iteration: 161 \t--- Loss: 0.802\n",
      "Iteration: 162 \t--- Loss: 0.786\n",
      "Iteration: 163 \t--- Loss: 0.768\n",
      "Iteration: 164 \t--- Loss: 0.794\n",
      "Iteration: 165 \t--- Loss: 0.791\n",
      "Iteration: 166 \t--- Loss: 0.787\n",
      "Iteration: 167 \t--- Loss: 0.771\n",
      "Iteration: 168 \t--- Loss: 0.779\n",
      "Iteration: 169 \t--- Loss: 0.790\n",
      "Iteration: 170 \t--- Loss: 0.748\n",
      "Iteration: 171 \t--- Loss: 0.797\n",
      "Iteration: 172 \t--- Loss: 0.786\n",
      "Iteration: 173 \t--- Loss: 0.768\n",
      "Iteration: 174 \t--- Loss: 0.771\n",
      "Iteration: 175 \t--- Loss: 0.775\n",
      "Iteration: 176 \t--- Loss: 0.778\n",
      "Iteration: 177 \t--- Loss: 0.791\n",
      "Iteration: 178 \t--- Loss: 0.764\n",
      "Iteration: 179 \t--- Loss: 0.782\n",
      "Iteration: 180 \t--- Loss: 0.771\n",
      "Iteration: 181 \t--- Loss: 0.789\n",
      "Iteration: 182 \t--- Loss: 0.763\n",
      "Iteration: 183 \t--- Loss: 0.800\n",
      "Iteration: 184 \t--- Loss: 0.768\n",
      "Iteration: 185 \t--- Loss: 0.792\n",
      "Iteration: 186 \t--- Loss: 0.767\n",
      "Iteration: 187 \t--- Loss: 0.792\n",
      "Iteration: 188 \t--- Loss: 0.800\n",
      "Iteration: 189 \t--- Loss: 0.802\n",
      "Iteration: 190 \t--- Loss: 0.766\n",
      "Iteration: 191 \t--- Loss: 0.762\n",
      "Iteration: 192 \t--- Loss: 0.784\n",
      "Iteration: 193 \t--- Loss: 0.784\n",
      "Iteration: 194 \t--- Loss: 0.780\n",
      "Iteration: 195 \t--- Loss: 0.784\n",
      "Iteration: 196 \t--- Loss: 0.786\n",
      "Iteration: 197 \t--- Loss: 0.799\n",
      "Iteration: 198 \t--- Loss: 0.766\n",
      "Iteration: 199 \t--- Loss: 0.767\n",
      "Iteration: 200 \t--- Loss: 0.744\n",
      "Iteration: 201 \t--- Loss: 0.782\n",
      "Iteration: 202 \t--- Loss: 0.780\n",
      "Iteration: 203 \t--- Loss: 0.752\n",
      "Iteration: 204 \t--- Loss: 0.783\n",
      "Iteration: 205 \t--- Loss: 0.773\n",
      "Iteration: 206 \t--- Loss: 0.822\n",
      "Iteration: 207 \t--- Loss: 0.775\n",
      "Iteration: 208 \t--- Loss: 0.793\n",
      "Iteration: 209 \t--- Loss: 0.784\n",
      "Iteration: 210 \t--- Loss: 0.766\n",
      "Iteration: 211 \t--- Loss: 0.792\n",
      "Iteration: 212 \t--- Loss: 0.775\n",
      "Iteration: 213 \t--- Loss: 0.795\n",
      "Iteration: 214 \t--- Loss: 0.770\n",
      "Iteration: 215 \t--- Loss: 0.782\n",
      "Iteration: 216 \t--- Loss: 0.810\n",
      "Iteration: 217 \t--- Loss: 0.767\n",
      "Iteration: 218 \t--- Loss: 0.793\n",
      "Iteration: 219 \t--- Loss: 0.776\n",
      "Iteration: 220 \t--- Loss: 0.788\n",
      "Iteration: 221 \t--- Loss: 0.791\n",
      "Iteration: 222 \t--- Loss: 0.793\n",
      "Iteration: 223 \t--- Loss: 0.774\n",
      "Iteration: 224 \t--- Loss: 0.774\n",
      "Iteration: 225 \t--- Loss: 0.781\n",
      "Iteration: 226 \t--- Loss: 0.791\n",
      "Iteration: 227 \t--- Loss: 0.796\n",
      "Iteration: 228 \t--- Loss: 0.774\n",
      "Iteration: 229 \t--- Loss: 0.776\n",
      "Iteration: 230 \t--- Loss: 0.784\n",
      "Iteration: 231 \t--- Loss: 0.779\n",
      "Iteration: 232 \t--- Loss: 0.819\n",
      "Iteration: 233 \t--- Loss: 0.827\n",
      "Iteration: 234 \t--- Loss: 0.801\n",
      "Iteration: 235 \t--- Loss: 0.792\n",
      "Iteration: 236 \t--- Loss: 0.786\n",
      "Iteration: 237 \t--- Loss: 0.758\n",
      "Iteration: 238 \t--- Loss: 0.770\n",
      "Iteration: 239 \t--- Loss: 0.775\n",
      "Iteration: 240 \t--- Loss: 0.793\n",
      "Iteration: 241 \t--- Loss: 0.788\n",
      "Iteration: 242 \t--- Loss: 0.802\n",
      "Iteration: 243 \t--- Loss: 0.804\n",
      "Iteration: 244 \t--- Loss: 0.759\n",
      "Iteration: 245 \t--- Loss: 0.773\n",
      "Iteration: 246 \t--- Loss: 0.774\n",
      "Iteration: 247 \t--- Loss: 0.801\n",
      "Iteration: 248 \t--- Loss: 0.779\n",
      "Iteration: 249 \t--- Loss: 0.779\n",
      "Iteration: 250 \t--- Loss: 0.772\n",
      "Iteration: 251 \t--- Loss: 0.793\n",
      "Iteration: 252 \t--- Loss: 0.791\n",
      "Iteration: 253 \t--- Loss: 0.795\n",
      "Iteration: 254 \t--- Loss: 0.756\n",
      "Iteration: 255 \t--- Loss: 0.777\n",
      "Iteration: 256 \t--- Loss: 0.768\n",
      "Iteration: 257 \t--- Loss: 0.780\n",
      "Iteration: 258 \t--- Loss: 0.743\n",
      "Iteration: 259 \t--- Loss: 0.766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:29<00:00, 89.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.371\n",
      "Iteration: 261 \t--- Loss: 0.380\n",
      "Iteration: 262 \t--- Loss: 0.386\n",
      "Iteration: 263 \t--- Loss: 0.363\n",
      "Iteration: 264 \t--- Loss: 0.374\n",
      "Iteration: 265 \t--- Loss: 0.375\n",
      "Iteration: 266 \t--- Loss: 0.359\n",
      "Iteration: 267 \t--- Loss: 0.360\n",
      "Iteration: 268 \t--- Loss: 0.382\n",
      "Iteration: 269 \t--- Loss: 0.371\n",
      "Iteration: 270 \t--- Loss: 0.365\n",
      "Iteration: 271 \t--- Loss: 0.351\n",
      "Iteration: 272 \t--- Loss: 0.366\n",
      "Iteration: 273 \t--- Loss: 0.380\n",
      "Iteration: 274 \t--- Loss: 0.362\n",
      "Iteration: 275 \t--- Loss: 0.369\n",
      "Iteration: 276 \t--- Loss: 0.363\n",
      "Iteration: 277 \t--- Loss: 0.375\n",
      "Iteration: 278 \t--- Loss: 0.370\n",
      "Iteration: 279 \t--- Loss: 0.384\n",
      "Iteration: 280 \t--- Loss: 0.369\n",
      "Iteration: 281 \t--- Loss: 0.369\n",
      "Iteration: 282 \t--- Loss: 0.366\n",
      "Iteration: 283 \t--- Loss: 0.376\n",
      "Iteration: 284 \t--- Loss: 0.376\n",
      "Iteration: 285 \t--- Loss: 0.362\n",
      "Iteration: 286 \t--- Loss: 0.371\n",
      "Iteration: 287 \t--- Loss: 0.355\n",
      "Iteration: 288 \t--- Loss: 0.367\n",
      "Iteration: 289 \t--- Loss: 0.361\n",
      "Iteration: 290 \t--- Loss: 0.362\n",
      "Iteration: 291 \t--- Loss: 0.366\n",
      "Iteration: 292 \t--- Loss: 0.374\n",
      "Iteration: 293 \t--- Loss: 0.358\n",
      "Iteration: 294 \t--- Loss: 0.375\n",
      "Iteration: 295 \t--- Loss: 0.374\n",
      "Iteration: 296 \t--- Loss: 0.372\n",
      "Iteration: 297 \t--- Loss: 0.371\n",
      "Iteration: 298 \t--- Loss: 0.353\n",
      "Iteration: 299 \t--- Loss: 0.386\n",
      "Iteration: 300 \t--- Loss: 0.360\n",
      "Iteration: 301 \t--- Loss: 0.363\n",
      "Iteration: 302 \t--- Loss: 0.365\n",
      "Iteration: 303 \t--- Loss: 0.362\n",
      "Iteration: 304 \t--- Loss: 0.363\n",
      "Iteration: 305 \t--- Loss: 0.368\n",
      "Iteration: 306 \t--- Loss: 0.361\n",
      "Iteration: 307 \t--- Loss: 0.367\n",
      "Iteration: 308 \t--- Loss: 0.381\n",
      "Iteration: 309 \t--- Loss: 0.367\n",
      "Iteration: 310 \t--- Loss: 0.369\n",
      "Iteration: 311 \t--- Loss: 0.363\n",
      "Iteration: 312 \t--- Loss: 0.362\n",
      "Iteration: 313 \t--- Loss: 0.375\n",
      "Iteration: 314 \t--- Loss: 0.378\n",
      "Iteration: 315 \t--- Loss: 0.361\n",
      "Iteration: 316 \t--- Loss: 0.358\n",
      "Iteration: 317 \t--- Loss: 0.365\n",
      "Iteration: 318 \t--- Loss: 0.372\n",
      "Iteration: 319 \t--- Loss: 0.381\n",
      "Iteration: 320 \t--- Loss: 0.368\n",
      "Iteration: 321 \t--- Loss: 0.372\n",
      "Iteration: 322 \t--- Loss: 0.368\n",
      "Iteration: 323 \t--- Loss: 0.365\n",
      "Iteration: 324 \t--- Loss: 0.370\n",
      "Iteration: 325 \t--- Loss: 0.367\n",
      "Iteration: 326 \t--- Loss: 0.367\n",
      "Iteration: 327 \t--- Loss: 0.385\n",
      "Iteration: 328 \t--- Loss: 0.381\n",
      "Iteration: 329 \t--- Loss: 0.374\n",
      "Iteration: 330 \t--- Loss: 0.370\n",
      "Iteration: 331 \t--- Loss: 0.373\n",
      "Iteration: 332 \t--- Loss: 0.373\n",
      "Iteration: 333 \t--- Loss: 0.376\n",
      "Iteration: 334 \t--- Loss: 0.362\n",
      "Iteration: 335 \t--- Loss: 0.360\n",
      "Iteration: 336 \t--- Loss: 0.378\n",
      "Iteration: 337 \t--- Loss: 0.363\n",
      "Iteration: 338 \t--- Loss: 0.356\n",
      "Iteration: 339 \t--- Loss: 0.363\n",
      "Iteration: 340 \t--- Loss: 0.375\n",
      "Iteration: 341 \t--- Loss: 0.368\n",
      "Iteration: 342 \t--- Loss: 0.359\n",
      "Iteration: 343 \t--- Loss: 0.378\n",
      "Iteration: 344 \t--- Loss: 0.359\n",
      "Iteration: 345 \t--- Loss: 0.372\n",
      "Iteration: 346 \t--- Loss: 0.362\n",
      "Iteration: 347 \t--- Loss: 0.374\n",
      "Iteration: 348 \t--- Loss: 0.351\n",
      "Iteration: 349 \t--- Loss: 0.378\n",
      "Iteration: 350 \t--- Loss: 0.370\n",
      "Iteration: 351 \t--- Loss: 0.371\n",
      "Iteration: 352 \t--- Loss: 0.371\n",
      "Iteration: 353 \t--- Loss: 0.375\n",
      "Iteration: 354 \t--- Loss: 0.372\n",
      "Iteration: 355 \t--- Loss: 0.366\n",
      "Iteration: 356 \t--- Loss: 0.363\n",
      "Iteration: 357 \t--- Loss: 0.368\n",
      "Iteration: 358 \t--- Loss: 0.366\n",
      "Iteration: 359 \t--- Loss: 0.376\n",
      "Iteration: 360 \t--- Loss: 0.365\n",
      "Iteration: 361 \t--- Loss: 0.365\n",
      "Iteration: 362 \t--- Loss: 0.364\n",
      "Iteration: 363 \t--- Loss: 0.356\n",
      "Iteration: 364 \t--- Loss: 0.366\n",
      "Iteration: 365 \t--- Loss: 0.365\n",
      "Iteration: 366 \t--- Loss: 0.371\n",
      "Iteration: 367 \t--- Loss: 0.371\n",
      "Iteration: 368 \t--- Loss: 0.364\n",
      "Iteration: 369 \t--- Loss: 0.377\n",
      "Iteration: 370 \t--- Loss: 0.367\n",
      "Iteration: 371 \t--- Loss: 0.368\n",
      "Iteration: 372 \t--- Loss: 0.373\n",
      "Iteration: 373 \t--- Loss: 0.373\n",
      "Iteration: 374 \t--- Loss: 0.376\n",
      "Iteration: 375 \t--- Loss: 0.374\n",
      "Iteration: 376 \t--- Loss: 0.360\n",
      "Iteration: 377 \t--- Loss: 0.365\n",
      "Iteration: 378 \t--- Loss: 0.376\n",
      "Iteration: 379 \t--- Loss: 0.366\n",
      "Iteration: 380 \t--- Loss: 0.355\n",
      "Iteration: 381 \t--- Loss: 0.376\n",
      "Iteration: 382 \t--- Loss: 0.384\n",
      "Iteration: 383 \t--- Loss: 0.373\n",
      "Iteration: 384 \t--- Loss: 0.367\n",
      "Iteration: 385 \t--- Loss: 0.355\n",
      "Iteration: 386 \t--- Loss: 0.367\n",
      "Iteration: 387 \t--- Loss: 0.367\n",
      "Iteration: 388 \t--- Loss: 0.368\n",
      "Iteration: 389 \t--- Loss: 0.375\n",
      "Iteration: 390 \t--- Loss: 0.365\n",
      "Iteration: 391 \t--- Loss: 0.371\n",
      "Iteration: 392 \t--- Loss: 0.357\n",
      "Iteration: 393 \t--- Loss: 0.369\n",
      "Iteration: 394 \t--- Loss: 0.369\n",
      "Iteration: 395 \t--- Loss: 0.378\n",
      "Iteration: 396 \t--- Loss: 0.386\n",
      "Iteration: 397 \t--- Loss: 0.357\n",
      "Iteration: 398 \t--- Loss: 0.387\n",
      "Iteration: 399 \t--- Loss: 0.355\n",
      "Iteration: 400 \t--- Loss: 0.363\n",
      "Iteration: 401 \t--- Loss: 0.367\n",
      "Iteration: 402 \t--- Loss: 0.367\n",
      "Iteration: 403 \t--- Loss: 0.376\n",
      "Iteration: 404 \t--- Loss: 0.370\n",
      "Iteration: 405 \t--- Loss: 0.366\n",
      "Iteration: 406 \t--- Loss: 0.362\n",
      "Iteration: 407 \t--- Loss: 0.370\n",
      "Iteration: 408 \t--- Loss: 0.370\n",
      "Iteration: 409 \t--- Loss: 0.372\n",
      "Iteration: 410 \t--- Loss: 0.365\n",
      "Iteration: 411 \t--- Loss: 0.365\n",
      "Iteration: 412 \t--- Loss: 0.377\n",
      "Iteration: 413 \t--- Loss: 0.357\n",
      "Iteration: 414 \t--- Loss: 0.379\n",
      "Iteration: 415 \t--- Loss: 0.362\n",
      "Iteration: 416 \t--- Loss: 0.363\n",
      "Iteration: 417 \t--- Loss: 0.362\n",
      "Iteration: 418 \t--- Loss: 0.362\n",
      "Iteration: 419 \t--- Loss: 0.371\n",
      "Iteration: 420 \t--- Loss: 0.369\n",
      "Iteration: 421 \t--- Loss: 0.369\n",
      "Iteration: 422 \t--- Loss: 0.357\n",
      "Iteration: 423 \t--- Loss: 0.381\n",
      "Iteration: 424 \t--- Loss: 0.363\n",
      "Iteration: 425 \t--- Loss: 0.368\n",
      "Iteration: 426 \t--- Loss: 0.382\n",
      "Iteration: 427 \t--- Loss: 0.361\n",
      "Iteration: 428 \t--- Loss: 0.381\n",
      "Iteration: 429 \t--- Loss: 0.376\n",
      "Iteration: 430 \t--- Loss: 0.375\n",
      "Iteration: 431 \t--- Loss: 0.365\n",
      "Iteration: 432 \t--- Loss: 0.368\n",
      "Iteration: 433 \t--- Loss: 0.385\n",
      "Iteration: 434 \t--- Loss: 0.370\n",
      "Iteration: 435 \t--- Loss: 0.371\n",
      "Iteration: 436 \t--- Loss: 0.370\n",
      "Iteration: 437 \t--- Loss: 0.371\n",
      "Iteration: 438 \t--- Loss: 0.366\n",
      "Iteration: 439 \t--- Loss: 0.367\n",
      "Iteration: 440 \t--- Loss: 0.378\n",
      "Iteration: 441 \t--- Loss: 0.371\n",
      "Iteration: 442 \t--- Loss: 0.374\n",
      "Iteration: 443 \t--- Loss: 0.381\n",
      "Iteration: 444 \t--- Loss: 0.375\n",
      "Iteration: 445 \t--- Loss: 0.372\n",
      "Iteration: 446 \t--- Loss: 0.362\n",
      "Iteration: 447 \t--- Loss: 0.374\n",
      "Iteration: 448 \t--- Loss: 0.375\n",
      "Iteration: 449 \t--- Loss: 0.365\n",
      "Iteration: 450 \t--- Loss: 0.373\n",
      "Iteration: 451 \t--- Loss: 0.385\n",
      "Iteration: 452 \t--- Loss: 0.364\n",
      "Iteration: 453 \t--- Loss: 0.367\n",
      "Iteration: 454 \t--- Loss: 0.369\n",
      "Iteration: 455 \t--- Loss: 0.368\n",
      "Iteration: 456 \t--- Loss: 0.373\n",
      "Iteration: 457 \t--- Loss: 0.358\n",
      "Iteration: 458 \t--- Loss: 0.365\n",
      "Iteration: 459 \t--- Loss: 0.370\n",
      "Iteration: 460 \t--- Loss: 0.368\n",
      "Iteration: 461 \t--- Loss: 0.377\n",
      "Iteration: 462 \t--- Loss: 0.369\n",
      "Iteration: 463 \t--- Loss: 0.380\n",
      "Iteration: 464 \t--- Loss: 0.367\n",
      "Iteration: 465 \t--- Loss: 0.370\n",
      "Iteration: 466 \t--- Loss: 0.372\n",
      "Iteration: 467 \t--- Loss: 0.362\n",
      "Iteration: 468 \t--- Loss: 0.368\n",
      "Iteration: 469 \t--- Loss: 0.380\n",
      "Iteration: 470 \t--- Loss: 0.362\n",
      "Iteration: 471 \t--- Loss: 0.364\n",
      "Iteration: 472 \t--- Loss: 0.371\n",
      "Iteration: 473 \t--- Loss: 0.366\n",
      "Iteration: 474 \t--- Loss: 0.377\n",
      "Iteration: 475 \t--- Loss: 0.374\n",
      "Iteration: 476 \t--- Loss: 0.363\n",
      "Iteration: 477 \t--- Loss: 0.369\n",
      "Iteration: 478 \t--- Loss: 0.364\n",
      "Iteration: 479 \t--- Loss: 0.366\n",
      "Iteration: 480 \t--- Loss: 0.368\n",
      "Iteration: 481 \t--- Loss: 0.360\n",
      "Iteration: 482 \t--- Loss: 0.384\n",
      "Iteration: 483 \t--- Loss: 0.374\n",
      "Iteration: 484 \t--- Loss: 0.380\n",
      "Iteration: 485 \t--- Loss: 0.361\n",
      "Iteration: 486 \t--- Loss: 0.365\n",
      "Iteration: 487 \t--- Loss: 0.380\n",
      "Iteration: 488 \t--- Loss: 0.360\n",
      "Iteration: 489 \t--- Loss: 0.383\n",
      "Iteration: 490 \t--- Loss: 0.355\n",
      "Iteration: 491 \t--- Loss: 0.389\n",
      "Iteration: 492 \t--- Loss: 0.377\n",
      "Iteration: 493 \t--- Loss: 0.373\n",
      "Iteration: 494 \t--- Loss: 0.386\n",
      "Iteration: 495 \t--- Loss: 0.369\n",
      "Iteration: 496 \t--- Loss: 0.368\n",
      "Iteration: 497 \t--- Loss: 0.363\n",
      "Iteration: 498 \t--- Loss: 0.377\n",
      "Iteration: 499 \t--- Loss: 0.377\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.57s/it][Parallel(n_jobs=5)]: Done  16 tasks      | elapsed:  8.6min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:28<00:00, 88.43s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.764\n",
      "Iteration: 261 \t--- Loss: 0.798\n",
      "Iteration: 262 \t--- Loss: 0.782\n",
      "Iteration: 263 \t--- Loss: 0.753\n",
      "Iteration: 264 \t--- Loss: 0.762\n",
      "Iteration: 265 \t--- Loss: 0.803\n",
      "Iteration: 266 \t--- Loss: 0.790\n",
      "Iteration: 267 \t--- Loss: 0.801\n",
      "Iteration: 268 \t--- Loss: 0.766\n",
      "Iteration: 269 \t--- Loss: 0.772\n",
      "Iteration: 270 \t--- Loss: 0.785\n",
      "Iteration: 271 \t--- Loss: 0.782\n",
      "Iteration: 272 \t--- Loss: 0.766\n",
      "Iteration: 273 \t--- Loss: 0.757\n",
      "Iteration: 274 \t--- Loss: 0.799\n",
      "Iteration: 275 \t--- Loss: 0.784\n",
      "Iteration: 276 \t--- Loss: 0.758\n",
      "Iteration: 277 \t--- Loss: 0.777\n",
      "Iteration: 278 \t--- Loss: 0.795\n",
      "Iteration: 279 \t--- Loss: 0.800\n",
      "Iteration: 280 \t--- Loss: 0.790\n",
      "Iteration: 281 \t--- Loss: 0.766\n",
      "Iteration: 282 \t--- Loss: 0.778\n",
      "Iteration: 283 \t--- Loss: 0.801\n",
      "Iteration: 284 \t--- Loss: 0.780\n",
      "Iteration: 285 \t--- Loss: 0.783\n",
      "Iteration: 286 \t--- Loss: 0.789\n",
      "Iteration: 287 \t--- Loss: 0.778\n",
      "Iteration: 288 \t--- Loss: 0.771\n",
      "Iteration: 289 \t--- Loss: 0.778\n",
      "Iteration: 290 \t--- Loss: 0.782\n",
      "Iteration: 291 \t--- Loss: 0.795\n",
      "Iteration: 292 \t--- Loss: 0.777\n",
      "Iteration: 293 \t--- Loss: 0.763\n",
      "Iteration: 294 \t--- Loss: 0.794\n",
      "Iteration: 295 \t--- Loss: 0.785\n",
      "Iteration: 296 \t--- Loss: 0.809\n",
      "Iteration: 297 \t--- Loss: 0.808\n",
      "Iteration: 298 \t--- Loss: 0.805\n",
      "Iteration: 299 \t--- Loss: 0.795\n",
      "Iteration: 300 \t--- Loss: 0.758\n",
      "Iteration: 301 \t--- Loss: 0.801\n",
      "Iteration: 302 \t--- Loss: 0.767\n",
      "Iteration: 303 \t--- Loss: 0.784\n",
      "Iteration: 304 \t--- Loss: 0.774\n",
      "Iteration: 305 \t--- Loss: 0.763\n",
      "Iteration: 306 \t--- Loss: 0.797\n",
      "Iteration: 307 \t--- Loss: 0.812\n",
      "Iteration: 308 \t--- Loss: 0.806\n",
      "Iteration: 309 \t--- Loss: 0.763\n",
      "Iteration: 310 \t--- Loss: 0.797\n",
      "Iteration: 311 \t--- Loss: 0.801\n",
      "Iteration: 312 \t--- Loss: 0.751\n",
      "Iteration: 313 \t--- Loss: 0.794\n",
      "Iteration: 314 \t--- Loss: 0.734\n",
      "Iteration: 315 \t--- Loss: 0.782\n",
      "Iteration: 316 \t--- Loss: 0.789\n",
      "Iteration: 317 \t--- Loss: 0.789\n",
      "Iteration: 318 \t--- Loss: 0.823\n",
      "Iteration: 319 \t--- Loss: 0.803\n",
      "Iteration: 320 \t--- Loss: 0.770\n",
      "Iteration: 321 \t--- Loss: 0.763\n",
      "Iteration: 322 \t--- Loss: 0.796\n",
      "Iteration: 323 \t--- Loss: 0.792\n",
      "Iteration: 324 \t--- Loss: 0.780\n",
      "Iteration: 325 \t--- Loss: 0.782\n",
      "Iteration: 326 \t--- Loss: 0.794\n",
      "Iteration: 327 \t--- Loss: 0.781\n",
      "Iteration: 328 \t--- Loss: 0.824\n",
      "Iteration: 329 \t--- Loss: 0.776\n",
      "Iteration: 330 \t--- Loss: 0.780\n",
      "Iteration: 331 \t--- Loss: 0.780\n",
      "Iteration: 332 \t--- Loss: 0.771\n",
      "Iteration: 333 \t--- Loss: 0.821\n",
      "Iteration: 334 \t--- Loss: 0.778\n",
      "Iteration: 335 \t--- Loss: 0.769\n",
      "Iteration: 336 \t--- Loss: 0.785\n",
      "Iteration: 337 \t--- Loss: 0.809\n",
      "Iteration: 338 \t--- Loss: 0.810\n",
      "Iteration: 339 \t--- Loss: 0.790\n",
      "Iteration: 340 \t--- Loss: 0.777\n",
      "Iteration: 341 \t--- Loss: 0.775\n",
      "Iteration: 342 \t--- Loss: 0.771\n",
      "Iteration: 343 \t--- Loss: 0.797\n",
      "Iteration: 344 \t--- Loss: 0.774\n",
      "Iteration: 345 \t--- Loss: 0.781\n",
      "Iteration: 346 \t--- Loss: 0.798\n",
      "Iteration: 347 \t--- Loss: 0.774\n",
      "Iteration: 348 \t--- Loss: 0.802\n",
      "Iteration: 349 \t--- Loss: 0.772\n",
      "Iteration: 350 \t--- Loss: 0.786\n",
      "Iteration: 351 \t--- Loss: 0.809\n",
      "Iteration: 352 \t--- Loss: 0.740\n",
      "Iteration: 353 \t--- Loss: 0.772\n",
      "Iteration: 354 \t--- Loss: 0.801\n",
      "Iteration: 355 \t--- Loss: 0.782\n",
      "Iteration: 356 \t--- Loss: 0.767\n",
      "Iteration: 357 \t--- Loss: 0.770\n",
      "Iteration: 358 \t--- Loss: 0.770\n",
      "Iteration: 359 \t--- Loss: 0.783\n",
      "Iteration: 360 \t--- Loss: 0.742\n",
      "Iteration: 361 \t--- Loss: 0.798\n",
      "Iteration: 362 \t--- Loss: 0.774\n",
      "Iteration: 363 \t--- Loss: 0.807\n",
      "Iteration: 364 \t--- Loss: 0.782\n",
      "Iteration: 365 \t--- Loss: 0.806\n",
      "Iteration: 366 \t--- Loss: 0.764\n",
      "Iteration: 367 \t--- Loss: 0.753\n",
      "Iteration: 368 \t--- Loss: 0.764\n",
      "Iteration: 369 \t--- Loss: 0.783\n",
      "Iteration: 370 \t--- Loss: 0.765\n",
      "Iteration: 371 \t--- Loss: 0.776\n",
      "Iteration: 372 \t--- Loss: 0.785\n",
      "Iteration: 373 \t--- Loss: 0.772\n",
      "Iteration: 374 \t--- Loss: 0.792\n",
      "Iteration: 375 \t--- Loss: 0.762\n",
      "Iteration: 376 \t--- Loss: 0.791\n",
      "Iteration: 377 \t--- Loss: 0.772\n",
      "Iteration: 378 \t--- Loss: 0.793\n",
      "Iteration: 379 \t--- Loss: 0.782\n",
      "Iteration: 380 \t--- Loss: 0.786\n",
      "Iteration: 381 \t--- Loss: 0.782\n",
      "Iteration: 382 \t--- Loss: 0.810\n",
      "Iteration: 383 \t--- Loss: 0.803\n",
      "Iteration: 384 \t--- Loss: 0.778\n",
      "Iteration: 385 \t--- Loss: 0.794\n",
      "Iteration: 386 \t--- Loss: 0.780\n",
      "Iteration: 387 \t--- Loss: 0.779\n",
      "Iteration: 388 \t--- Loss: 0.752\n",
      "Iteration: 389 \t--- Loss: 0.803\n",
      "Iteration: 390 \t--- Loss: 0.834\n",
      "Iteration: 391 \t--- Loss: 0.758\n",
      "Iteration: 392 \t--- Loss: 0.776\n",
      "Iteration: 393 \t--- Loss: 0.789\n",
      "Iteration: 394 \t--- Loss: 0.796\n",
      "Iteration: 395 \t--- Loss: 0.790\n",
      "Iteration: 396 \t--- Loss: 0.786\n",
      "Iteration: 397 \t--- Loss: 0.783\n",
      "Iteration: 398 \t--- Loss: 0.801\n",
      "Iteration: 399 \t--- Loss: 0.806\n",
      "Iteration: 400 \t--- Loss: 0.779\n",
      "Iteration: 401 \t--- Loss: 0.769\n",
      "Iteration: 402 \t--- Loss: 0.814\n",
      "Iteration: 403 \t--- Loss: 0.807\n",
      "Iteration: 404 \t--- Loss: 0.811\n",
      "Iteration: 405 \t--- Loss: 0.784\n",
      "Iteration: 406 \t--- Loss: 0.765\n",
      "Iteration: 407 \t--- Loss: 0.818\n",
      "Iteration: 408 \t--- Loss: 0.809\n",
      "Iteration: 409 \t--- Loss: 0.786\n",
      "Iteration: 410 \t--- Loss: 0.772\n",
      "Iteration: 411 \t--- Loss: 0.800\n",
      "Iteration: 412 \t--- Loss: 0.793\n",
      "Iteration: 413 \t--- Loss: 0.804\n",
      "Iteration: 414 \t--- Loss: 0.785\n",
      "Iteration: 415 \t--- Loss: 0.775\n",
      "Iteration: 416 \t--- Loss: 0.777\n",
      "Iteration: 417 \t--- Loss: 0.777\n",
      "Iteration: 418 \t--- Loss: 0.789\n",
      "Iteration: 419 \t--- Loss: 0.800\n",
      "Iteration: 420 \t--- Loss: 0.804\n",
      "Iteration: 421 \t--- Loss: 0.773\n",
      "Iteration: 422 \t--- Loss: 0.803\n",
      "Iteration: 423 \t--- Loss: 0.786\n",
      "Iteration: 424 \t--- Loss: 0.790\n",
      "Iteration: 425 \t--- Loss: 0.796\n",
      "Iteration: 426 \t--- Loss: 0.772\n",
      "Iteration: 427 \t--- Loss: 0.778\n",
      "Iteration: 428 \t--- Loss: 0.770\n",
      "Iteration: 429 \t--- Loss: 0.773\n",
      "Iteration: 430 \t--- Loss: 0.771\n",
      "Iteration: 431 \t--- Loss: 0.810\n",
      "Iteration: 432 \t--- Loss: 0.797\n",
      "Iteration: 433 \t--- Loss: 0.769\n",
      "Iteration: 434 \t--- Loss: 0.791\n",
      "Iteration: 435 \t--- Loss: 0.794\n",
      "Iteration: 436 \t--- Loss: 0.798\n",
      "Iteration: 437 \t--- Loss: 0.758\n",
      "Iteration: 438 \t--- Loss: 0.808\n",
      "Iteration: 439 \t--- Loss: 0.758\n",
      "Iteration: 440 \t--- Loss: 0.790\n",
      "Iteration: 441 \t--- Loss: 0.784\n",
      "Iteration: 442 \t--- Loss: 0.799\n",
      "Iteration: 443 \t--- Loss: 0.774\n",
      "Iteration: 444 \t--- Loss: 0.778\n",
      "Iteration: 445 \t--- Loss: 0.795\n",
      "Iteration: 446 \t--- Loss: 0.787\n",
      "Iteration: 447 \t--- Loss: 0.779\n",
      "Iteration: 448 \t--- Loss: 0.768\n",
      "Iteration: 449 \t--- Loss: 0.766\n",
      "Iteration: 450 \t--- Loss: 0.790\n",
      "Iteration: 451 \t--- Loss: 0.810\n",
      "Iteration: 452 \t--- Loss: 0.778\n",
      "Iteration: 453 \t--- Loss: 0.781\n",
      "Iteration: 454 \t--- Loss: 0.763\n",
      "Iteration: 455 \t--- Loss: 0.786\n",
      "Iteration: 456 \t--- Loss: 0.815\n",
      "Iteration: 457 \t--- Loss: 0.778\n",
      "Iteration: 458 \t--- Loss: 0.765\n",
      "Iteration: 459 \t--- Loss: 0.797\n",
      "Iteration: 460 \t--- Loss: 0.789\n",
      "Iteration: 461 \t--- Loss: 0.749\n",
      "Iteration: 462 \t--- Loss: 0.759\n",
      "Iteration: 463 \t--- Loss: 0.792\n",
      "Iteration: 464 \t--- Loss: 0.789\n",
      "Iteration: 465 \t--- Loss: 0.757\n",
      "Iteration: 466 \t--- Loss: 0.743\n",
      "Iteration: 467 \t--- Loss: 0.759\n",
      "Iteration: 468 \t--- Loss: 0.784\n",
      "Iteration: 469 \t--- Loss: 0.771\n",
      "Iteration: 470 \t--- Loss: 0.758\n",
      "Iteration: 471 \t--- Loss: 0.778\n",
      "Iteration: 472 \t--- Loss: 0.787\n",
      "Iteration: 473 \t--- Loss: 0.801\n",
      "Iteration: 474 \t--- Loss: 0.752\n",
      "Iteration: 475 \t--- Loss: 0.770\n",
      "Iteration: 476 \t--- Loss: 0.786\n",
      "Iteration: 477 \t--- Loss: 0.827\n",
      "Iteration: 478 \t--- Loss: 0.775\n",
      "Iteration: 479 \t--- Loss: 0.765\n",
      "Iteration: 480 \t--- Loss: 0.785\n",
      "Iteration: 481 \t--- Loss: 0.775\n",
      "Iteration: 482 \t--- Loss: 0.792\n",
      "Iteration: 483 \t--- Loss: 0.768\n",
      "Iteration: 484 \t--- Loss: 0.795\n",
      "Iteration: 485 \t--- Loss: 0.798\n",
      "Iteration: 486 \t--- Loss: 0.748\n",
      "Iteration: 487 \t--- Loss: 0.811\n",
      "Iteration: 488 \t--- Loss: 0.790\n",
      "Iteration: 489 \t--- Loss: 0.745\n",
      "Iteration: 490 \t--- Loss: 0.778\n",
      "Iteration: 491 \t--- Loss: 0.768\n",
      "Iteration: 492 \t--- Loss: 0.783\n",
      "Iteration: 493 \t--- Loss: 0.813\n",
      "Iteration: 494 \t--- Loss: 0.809\n",
      "Iteration: 495 \t--- Loss: 0.781\n",
      "Iteration: 496 \t--- Loss: 0.783\n",
      "Iteration: 497 \t--- Loss: 0.785\n",
      "Iteration: 498 \t--- Loss: 0.751\n",
      "Iteration: 499 \t--- Loss: 0.792\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.58s/it][Parallel(n_jobs=5)]: Done  17 tasks      | elapsed:  9.0min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.159\n",
      "Iteration: 1 \t--- Loss: 0.158\n",
      "Iteration: 2 \t--- Loss: 0.124\n",
      "Iteration: 3 \t--- Loss: 0.141\n",
      "Iteration: 4 \t--- Loss: 0.142\n",
      "Iteration: 5 \t--- Loss: 0.120\n",
      "Iteration: 6 \t--- Loss: 0.110\n",
      "Iteration: 7 \t--- Loss: 0.083\n",
      "Iteration: 8 \t--- Loss: 0.090\n",
      "Iteration: 9 \t--- Loss: 0.083\n",
      "Iteration: 10 \t--- Loss: 0.074\n",
      "Iteration: 11 \t--- Loss: 0.066\n",
      "Iteration: 12 \t--- Loss: 0.068\n",
      "Iteration: 13 \t--- Loss: 0.060\n",
      "Iteration: 14 \t--- Loss: 0.063\n",
      "Iteration: 15 \t--- Loss: 0.060\n",
      "Iteration: 16 \t--- Loss: 0.054\n",
      "Iteration: 17 \t--- Loss: 0.053\n",
      "Iteration: 18 \t--- Loss: 0.054\n",
      "Iteration: 19 \t--- Loss: 0.053\n",
      "Iteration: 20 \t--- Loss: 0.047\n",
      "Iteration: 21 \t--- Loss: 0.043\n",
      "Iteration: 22 \t--- Loss: 0.043\n",
      "Iteration: 23 \t--- Loss: 0.048\n",
      "Iteration: 24 \t--- Loss: 0.046\n",
      "Iteration: 25 \t--- Loss: 0.039\n",
      "Iteration: 26 \t--- Loss: 0.042\n",
      "Iteration: 27 \t--- Loss: 0.041\n",
      "Iteration: 28 \t--- Loss: 0.038\n",
      "Iteration: 29 \t--- Loss: 0.039\n",
      "Iteration: 30 \t--- Loss: 0.039\n",
      "Iteration: 31 \t--- Loss: 0.036\n",
      "Iteration: 32 \t--- Loss: 0.039\n",
      "Iteration: 33 \t--- Loss: 0.035\n",
      "Iteration: 34 \t--- Loss: 0.038\n",
      "Iteration: 35 \t--- Loss: 0.035\n",
      "Iteration: 36 \t--- Loss: 0.034\n",
      "Iteration: 37 \t--- Loss: 0.035\n",
      "Iteration: 38 \t--- Loss: 0.031\n",
      "Iteration: 39 \t--- Loss: 0.033\n",
      "Iteration: 40 \t--- Loss: 0.035\n",
      "Iteration: 41 \t--- Loss: 0.033\n",
      "Iteration: 42 \t--- Loss: 0.031\n",
      "Iteration: 43 \t--- Loss: 0.030\n",
      "Iteration: 44 \t--- Loss: 0.032\n",
      "Iteration: 45 \t--- Loss: 0.032\n",
      "Iteration: 46 \t--- Loss: 0.030\n",
      "Iteration: 47 \t--- Loss: 0.030\n",
      "Iteration: 48 \t--- Loss: 0.029\n",
      "Iteration: 49 \t--- Loss: 0.030\n",
      "Iteration: 50 \t--- Loss: 0.028\n",
      "Iteration: 51 \t--- Loss: 0.028\n",
      "Iteration: 52 \t--- Loss: 0.031\n",
      "Iteration: 53 \t--- Loss: 0.030\n",
      "Iteration: 54 \t--- Loss: 0.029\n",
      "Iteration: 55 \t--- Loss: 0.028\n",
      "Iteration: 56 \t--- Loss: 0.029\n",
      "Iteration: 57 \t--- Loss: 0.030\n",
      "Iteration: 58 \t--- Loss: 0.027\n",
      "Iteration: 59 \t--- Loss: 0.029\n",
      "Iteration: 60 \t--- Loss: 0.028\n",
      "Iteration: 61 \t--- Loss: 0.029\n",
      "Iteration: 62 \t--- Loss: 0.030\n",
      "Iteration: 63 \t--- Loss: 0.031\n",
      "Iteration: 64 \t--- Loss: 0.028\n",
      "Iteration: 65 \t--- Loss: 0.026\n",
      "Iteration: 66 \t--- Loss: 0.028\n",
      "Iteration: 67 \t--- Loss: 0.028\n",
      "Iteration: 68 \t--- Loss: 0.028\n",
      "Iteration: 69 \t--- Loss: 0.027\n",
      "Iteration: 70 \t--- Loss: 0.026\n",
      "Iteration: 71 \t--- Loss: 0.027\n",
      "Iteration: 72 \t--- Loss: 0.028\n",
      "Iteration: 73 \t--- Loss: 0.026\n",
      "Iteration: 74 \t--- Loss: 0.027\n",
      "Iteration: 75 \t--- Loss: 0.027\n",
      "Iteration: 76 \t--- Loss: 0.025\n",
      "Iteration: 77 \t--- Loss: 0.026\n",
      "Iteration: 78 \t--- Loss: 0.029\n",
      "Iteration: 79 \t--- Loss: 0.028\n",
      "Iteration: 80 \t--- Loss: 0.025\n",
      "Iteration: 81 \t--- Loss: 0.028\n",
      "Iteration: 82 \t--- Loss: 0.025\n",
      "Iteration: 83 \t--- Loss: 0.027\n",
      "Iteration: 84 \t--- Loss: 0.028\n",
      "Iteration: 85 \t--- Loss: 0.028\n",
      "Iteration: 86 \t--- Loss: 0.024\n",
      "Iteration: 87 \t--- Loss: 0.026\n",
      "Iteration: 88 \t--- Loss: 0.026\n",
      "Iteration: 89 \t--- Loss: 0.028\n",
      "Iteration: 90 \t--- Loss: 0.027\n",
      "Iteration: 91 \t--- Loss: 0.026\n",
      "Iteration: 92 \t--- Loss: 0.026\n",
      "Iteration: 93 \t--- Loss: 0.029\n",
      "Iteration: 94 \t--- Loss: 0.025\n",
      "Iteration: 95 \t--- Loss: 0.026\n",
      "Iteration: 96 \t--- Loss: 0.026\n",
      "Iteration: 97 \t--- Loss: 0.026\n",
      "Iteration: 98 \t--- Loss: 0.025\n",
      "Iteration: 99 \t--- Loss: 0.026\n",
      "Iteration: 100 \t--- Loss: 0.026\n",
      "Iteration: 101 \t--- Loss: 0.027\n",
      "Iteration: 102 \t--- Loss: 0.025\n",
      "Iteration: 103 \t--- Loss: 0.028\n",
      "Iteration: 104 \t--- Loss: 0.026\n",
      "Iteration: 105 \t--- Loss: 0.026\n",
      "Iteration: 106 \t--- Loss: 0.027\n",
      "Iteration: 107 \t--- Loss: 0.026\n",
      "Iteration: 108 \t--- Loss: 0.026\n",
      "Iteration: 109 \t--- Loss: 0.024\n",
      "Iteration: 110 \t--- Loss: 0.026\n",
      "Iteration: 111 \t--- Loss: 0.026\n",
      "Iteration: 112 \t--- Loss: 0.024\n",
      "Iteration: 113 \t--- Loss: 0.025\n",
      "Iteration: 114 \t--- Loss: 0.024\n",
      "Iteration: 115 \t--- Loss: 0.026\n",
      "Iteration: 116 \t--- Loss: 0.028\n",
      "Iteration: 117 \t--- Loss: 0.026\n",
      "Iteration: 118 \t--- Loss: 0.024\n",
      "Iteration: 119 \t--- Loss: 0.026\n",
      "Iteration: 120 \t--- Loss: 0.025\n",
      "Iteration: 121 \t--- Loss: 0.023\n",
      "Iteration: 122 \t--- Loss: 0.024\n",
      "Iteration: 123 \t--- Loss: 0.024\n",
      "Iteration: 124 \t--- Loss: 0.025\n",
      "Iteration: 125 \t--- Loss: 0.025\n",
      "Iteration: 126 \t--- Loss: 0.025\n",
      "Iteration: 127 \t--- Loss: 0.024\n",
      "Iteration: 128 \t--- Loss: 0.025\n",
      "Iteration: 129 \t--- Loss: 0.026\n",
      "Iteration: 130 \t--- Loss: 0.026\n",
      "Iteration: 131 \t--- Loss: 0.026\n",
      "Iteration: 132 \t--- Loss: 0.027\n",
      "Iteration: 133 \t--- Loss: 0.025\n",
      "Iteration: 134 \t--- Loss: 0.023\n",
      "Iteration: 135 \t--- Loss: 0.023\n",
      "Iteration: 136 \t--- Loss: 0.026\n",
      "Iteration: 137 \t--- Loss: 0.023\n",
      "Iteration: 138 \t--- Loss: 0.024\n",
      "Iteration: 139 \t--- Loss: 0.026\n",
      "Iteration: 140 \t--- Loss: 0.026\n",
      "Iteration: 141 \t--- Loss: 0.026\n",
      "Iteration: 142 \t--- Loss: 0.025\n",
      "Iteration: 143 \t--- Loss: 0.026\n",
      "Iteration: 144 \t--- Loss: 0.025\n",
      "Iteration: 145 \t--- Loss: 0.022\n",
      "Iteration: 146 \t--- Loss: 0.025\n",
      "Iteration: 147 \t--- Loss: 0.026\n",
      "Iteration: 148 \t--- Loss: 0.023\n",
      "Iteration: 149 \t--- Loss: 0.025\n",
      "Iteration: 150 \t--- Loss: 0.026\n",
      "Iteration: 151 \t--- Loss: 0.024\n",
      "Iteration: 152 \t--- Loss: 0.024\n",
      "Iteration: 153 \t--- Loss: 0.024\n",
      "Iteration: 154 \t--- Loss: 0.026\n",
      "Iteration: 155 \t--- Loss: 0.026\n",
      "Iteration: 156 \t--- Loss: 0.022\n",
      "Iteration: 157 \t--- Loss: 0.026\n",
      "Iteration: 158 \t--- Loss: 0.023\n",
      "Iteration: 159 \t--- Loss: 0.023\n",
      "Iteration: 160 \t--- Loss: 0.025\n",
      "Iteration: 161 \t--- Loss: 0.025\n",
      "Iteration: 162 \t--- Loss: 0.028\n",
      "Iteration: 163 \t--- Loss: 0.024\n",
      "Iteration: 164 \t--- Loss: 0.024\n",
      "Iteration: 165 \t--- Loss: 0.022\n",
      "Iteration: 166 \t--- Loss: 0.025\n",
      "Iteration: 167 \t--- Loss: 0.024\n",
      "Iteration: 168 \t--- Loss: 0.026\n",
      "Iteration: 169 \t--- Loss: 0.024\n",
      "Iteration: 170 \t--- Loss: 0.025\n",
      "Iteration: 171 \t--- Loss: 0.025\n",
      "Iteration: 172 \t--- Loss: 0.025\n",
      "Iteration: 173 \t--- Loss: 0.025\n",
      "Iteration: 174 \t--- Loss: 0.027\n",
      "Iteration: 175 \t--- Loss: 0.024\n",
      "Iteration: 176 \t--- Loss: 0.025\n",
      "Iteration: 177 \t--- Loss: 0.025\n",
      "Iteration: 178 \t--- Loss: 0.024\n",
      "Iteration: 179 \t--- Loss: 0.024\n",
      "Iteration: 180 \t--- Loss: 0.025\n",
      "Iteration: 181 \t--- Loss: 0.024\n",
      "Iteration: 182 \t--- Loss: 0.026\n",
      "Iteration: 183 \t--- Loss: 0.023\n",
      "Iteration: 184 \t--- Loss: 0.025\n",
      "Iteration: 185 \t--- Loss: 0.024\n",
      "Iteration: 186 \t--- Loss: 0.023\n",
      "Iteration: 187 \t--- Loss: 0.024\n",
      "Iteration: 188 \t--- Loss: 0.024\n",
      "Iteration: 189 \t--- Loss: 0.025\n",
      "Iteration: 190 \t--- Loss: 0.023\n",
      "Iteration: 191 \t--- Loss: 0.025\n",
      "Iteration: 192 \t--- Loss: 0.024\n",
      "Iteration: 193 \t--- Loss: 0.025\n",
      "Iteration: 194 \t--- Loss: 0.026\n",
      "Iteration: 195 \t--- Loss: 0.025\n",
      "Iteration: 196 \t--- Loss: 0.024\n",
      "Iteration: 197 \t--- Loss: 0.025\n",
      "Iteration: 198 \t--- Loss: 0.025\n",
      "Iteration: 199 \t--- Loss: 0.025\n",
      "Iteration: 200 \t--- Loss: 0.025\n",
      "Iteration: 201 \t--- Loss: 0.024\n",
      "Iteration: 202 \t--- Loss: 0.026\n",
      "Iteration: 203 \t--- Loss: 0.022\n",
      "Iteration: 204 \t--- Loss: 0.025\n",
      "Iteration: 205 \t--- Loss: 0.025\n",
      "Iteration: 206 \t--- Loss: 0.025\n",
      "Iteration: 207 \t--- Loss: 0.024\n",
      "Iteration: 208 \t--- Loss: 0.024\n",
      "Iteration: 209 \t--- Loss: 0.024\n",
      "Iteration: 210 \t--- Loss: 0.024\n",
      "Iteration: 211 \t--- Loss: 0.025\n",
      "Iteration: 212 \t--- Loss: 0.028\n",
      "Iteration: 213 \t--- Loss: 0.025\n",
      "Iteration: 214 \t--- Loss: 0.023\n",
      "Iteration: 215 \t--- Loss: 0.026\n",
      "Iteration: 216 \t--- Loss: 0.025\n",
      "Iteration: 217 \t--- Loss: 0.023\n",
      "Iteration: 218 \t--- Loss: 0.024\n",
      "Iteration: 219 \t--- Loss: 0.027\n",
      "Iteration: 220 \t--- Loss: 0.023\n",
      "Iteration: 221 \t--- Loss: 0.024\n",
      "Iteration: 222 \t--- Loss: 0.028\n",
      "Iteration: 223 \t--- Loss: 0.027\n",
      "Iteration: 224 \t--- Loss: 0.023\n",
      "Iteration: 225 \t--- Loss: 0.025\n",
      "Iteration: 226 \t--- Loss: 0.024\n",
      "Iteration: 227 \t--- Loss: 0.025\n",
      "Iteration: 228 \t--- Loss: 0.024\n",
      "Iteration: 229 \t--- Loss: 0.028\n",
      "Iteration: 230 \t--- Loss: 0.024\n",
      "Iteration: 231 \t--- Loss: 0.027\n",
      "Iteration: 232 \t--- Loss: 0.026\n",
      "Iteration: 233 \t--- Loss: 0.026\n",
      "Iteration: 234 \t--- Loss: 0.024\n",
      "Iteration: 235 \t--- Loss: 0.023\n",
      "Iteration: 236 \t--- Loss: 0.023\n",
      "Iteration: 237 \t--- Loss: 0.023\n",
      "Iteration: 238 \t--- Loss: 0.024\n",
      "Iteration: 239 \t--- Loss: 0.026\n",
      "Iteration: 240 \t--- Loss: 0.024\n",
      "Iteration: 241 \t--- Loss: 0.024\n",
      "Iteration: 242 \t--- Loss: 0.025\n",
      "Iteration: 243 \t--- Loss: 0.025\n",
      "Iteration: 244 \t--- Loss: 0.025\n",
      "Iteration: 245 \t--- Loss: 0.026\n",
      "Iteration: 246 \t--- Loss: 0.022\n",
      "Iteration: 247 \t--- Loss: 0.026\n",
      "Iteration: 248 \t--- Loss: 0.025\n",
      "Iteration: 249 \t--- Loss: 0.026\n",
      "Iteration: 250 \t--- Loss: 0.023\n",
      "Iteration: 251 \t--- Loss: 0.024\n",
      "Iteration: 252 \t--- Loss: 0.024\n",
      "Iteration: 253 \t--- Loss: 0.023\n",
      "Iteration: 254 \t--- Loss: 0.024\n",
      "Iteration: 255 \t--- Loss: 0.024\n",
      "Iteration: 256 \t--- Loss: 0.024\n",
      "Iteration: 257 \t--- Loss: 0.025\n",
      "Iteration: 258 \t--- Loss: 0.024\n",
      "Iteration: 259 \t--- Loss: 0.024Iteration: 0 \t--- Loss: 0.171\n",
      "Iteration: 1 \t--- Loss: 0.158\n",
      "Iteration: 2 \t--- Loss: 0.153\n",
      "Iteration: 3 \t--- Loss: 0.130\n",
      "Iteration: 4 \t--- Loss: 0.136\n",
      "Iteration: 5 \t--- Loss: 0.122\n",
      "Iteration: 6 \t--- Loss: 0.117\n",
      "Iteration: 7 \t--- Loss: 0.106\n",
      "Iteration: 8 \t--- Loss: 0.107\n",
      "Iteration: 9 \t--- Loss: 0.105\n",
      "Iteration: 10 \t--- Loss: 0.096\n",
      "Iteration: 11 \t--- Loss: 0.094\n",
      "Iteration: 12 \t--- Loss: 0.093\n",
      "Iteration: 13 \t--- Loss: 0.088\n",
      "Iteration: 14 \t--- Loss: 0.084\n",
      "Iteration: 15 \t--- Loss: 0.082\n",
      "Iteration: 16 \t--- Loss: 0.075\n",
      "Iteration: 17 \t--- Loss: 0.076\n",
      "Iteration: 18 \t--- Loss: 0.077\n",
      "Iteration: 19 \t--- Loss: 0.081\n",
      "Iteration: 20 \t--- Loss: 0.078\n",
      "Iteration: 21 \t--- Loss: 0.071\n",
      "Iteration: 22 \t--- Loss: 0.075\n",
      "Iteration: 23 \t--- Loss: 0.071\n",
      "Iteration: 24 \t--- Loss: 0.073\n",
      "Iteration: 25 \t--- Loss: 0.068\n",
      "Iteration: 26 \t--- Loss: 0.070\n",
      "Iteration: 27 \t--- Loss: 0.071\n",
      "Iteration: 28 \t--- Loss: 0.063\n",
      "Iteration: 29 \t--- Loss: 0.068\n",
      "Iteration: 30 \t--- Loss: 0.066\n",
      "Iteration: 31 \t--- Loss: 0.065\n",
      "Iteration: 32 \t--- Loss: 0.065\n",
      "Iteration: 33 \t--- Loss: 0.062\n",
      "Iteration: 34 \t--- Loss: 0.062\n",
      "Iteration: 35 \t--- Loss: 0.061\n",
      "Iteration: 36 \t--- Loss: 0.063\n",
      "Iteration: 37 \t--- Loss: 0.066\n",
      "Iteration: 38 \t--- Loss: 0.061\n",
      "Iteration: 39 \t--- Loss: 0.061\n",
      "Iteration: 40 \t--- Loss: 0.066\n",
      "Iteration: 41 \t--- Loss: 0.062\n",
      "Iteration: 42 \t--- Loss: 0.062\n",
      "Iteration: 43 \t--- Loss: 0.065\n",
      "Iteration: 44 \t--- Loss: 0.062\n",
      "Iteration: 45 \t--- Loss: 0.058\n",
      "Iteration: 46 \t--- Loss: 0.060\n",
      "Iteration: 47 \t--- Loss: 0.062\n",
      "Iteration: 48 \t--- Loss: 0.059\n",
      "Iteration: 49 \t--- Loss: 0.058\n",
      "Iteration: 50 \t--- Loss: 0.061\n",
      "Iteration: 51 \t--- Loss: 0.064\n",
      "Iteration: 52 \t--- Loss: 0.057\n",
      "Iteration: 53 \t--- Loss: 0.058\n",
      "Iteration: 54 \t--- Loss: 0.061\n",
      "Iteration: 55 \t--- Loss: 0.059\n",
      "Iteration: 56 \t--- Loss: 0.062\n",
      "Iteration: 57 \t--- Loss: 0.065\n",
      "Iteration: 58 \t--- Loss: 0.061\n",
      "Iteration: 59 \t--- Loss: 0.059\n",
      "Iteration: 60 \t--- Loss: 0.061\n",
      "Iteration: 61 \t--- Loss: 0.061\n",
      "Iteration: 62 \t--- Loss: 0.061\n",
      "Iteration: 63 \t--- Loss: 0.060\n",
      "Iteration: 64 \t--- Loss: 0.056\n",
      "Iteration: 65 \t--- Loss: 0.062\n",
      "Iteration: 66 \t--- Loss: 0.060\n",
      "Iteration: 67 \t--- Loss: 0.062\n",
      "Iteration: 68 \t--- Loss: 0.056\n",
      "Iteration: 69 \t--- Loss: 0.059\n",
      "Iteration: 70 \t--- Loss: 0.057\n",
      "Iteration: 71 \t--- Loss: 0.061\n",
      "Iteration: 72 \t--- Loss: 0.057\n",
      "Iteration: 73 \t--- Loss: 0.057\n",
      "Iteration: 74 \t--- Loss: 0.059\n",
      "Iteration: 75 \t--- Loss: 0.053\n",
      "Iteration: 76 \t--- Loss: 0.057\n",
      "Iteration: 77 \t--- Loss: 0.056\n",
      "Iteration: 78 \t--- Loss: 0.056\n",
      "Iteration: 79 \t--- Loss: 0.055\n",
      "Iteration: 80 \t--- Loss: 0.060\n",
      "Iteration: 81 \t--- Loss: 0.055\n",
      "Iteration: 82 \t--- Loss: 0.059\n",
      "Iteration: 83 \t--- Loss: 0.058\n",
      "Iteration: 84 \t--- Loss: 0.059\n",
      "Iteration: 85 \t--- Loss: 0.057\n",
      "Iteration: 86 \t--- Loss: 0.054\n",
      "Iteration: 87 \t--- Loss: 0.060\n",
      "Iteration: 88 \t--- Loss: 0.057\n",
      "Iteration: 89 \t--- Loss: 0.060\n",
      "Iteration: 90 \t--- Loss: 0.060\n",
      "Iteration: 91 \t--- Loss: 0.057\n",
      "Iteration: 92 \t--- Loss: 0.056\n",
      "Iteration: 93 \t--- Loss: 0.056\n",
      "Iteration: 94 \t--- Loss: 0.058\n",
      "Iteration: 95 \t--- Loss: 0.058\n",
      "Iteration: 96 \t--- Loss: 0.056\n",
      "Iteration: 97 \t--- Loss: 0.055\n",
      "Iteration: 98 \t--- Loss: 0.058\n",
      "Iteration: 99 \t--- Loss: 0.058\n",
      "Iteration: 100 \t--- Loss: 0.057\n",
      "Iteration: 101 \t--- Loss: 0.060\n",
      "Iteration: 102 \t--- Loss: 0.058\n",
      "Iteration: 103 \t--- Loss: 0.056\n",
      "Iteration: 104 \t--- Loss: 0.058\n",
      "Iteration: 105 \t--- Loss: 0.058\n",
      "Iteration: 106 \t--- Loss: 0.060\n",
      "Iteration: 107 \t--- Loss: 0.060\n",
      "Iteration: 108 \t--- Loss: 0.058\n",
      "Iteration: 109 \t--- Loss: 0.059\n",
      "Iteration: 110 \t--- Loss: 0.054\n",
      "Iteration: 111 \t--- Loss: 0.054\n",
      "Iteration: 112 \t--- Loss: 0.062\n",
      "Iteration: 113 \t--- Loss: 0.054\n",
      "Iteration: 114 \t--- Loss: 0.061\n",
      "Iteration: 115 \t--- Loss: 0.060\n",
      "Iteration: 116 \t--- Loss: 0.057\n",
      "Iteration: 117 \t--- Loss: 0.060\n",
      "Iteration: 118 \t--- Loss: 0.058\n",
      "Iteration: 119 \t--- Loss: 0.059\n",
      "Iteration: 120 \t--- Loss: 0.059\n",
      "Iteration: 121 \t--- Loss: 0.058\n",
      "Iteration: 122 \t--- Loss: 0.060\n",
      "Iteration: 123 \t--- Loss: 0.056\n",
      "Iteration: 124 \t--- Loss: 0.057\n",
      "Iteration: 125 \t--- Loss: 0.054\n",
      "Iteration: 126 \t--- Loss: 0.058\n",
      "Iteration: 127 \t--- Loss: 0.056\n",
      "Iteration: 128 \t--- Loss: 0.061\n",
      "Iteration: 129 \t--- Loss: 0.058\n",
      "Iteration: 130 \t--- Loss: 0.058\n",
      "Iteration: 131 \t--- Loss: 0.056\n",
      "Iteration: 132 \t--- Loss: 0.058\n",
      "Iteration: 133 \t--- Loss: 0.053\n",
      "Iteration: 134 \t--- Loss: 0.059\n",
      "Iteration: 135 \t--- Loss: 0.059\n",
      "Iteration: 136 \t--- Loss: 0.053\n",
      "Iteration: 137 \t--- Loss: 0.058\n",
      "Iteration: 138 \t--- Loss: 0.058\n",
      "Iteration: 139 \t--- Loss: 0.056\n",
      "Iteration: 140 \t--- Loss: 0.056\n",
      "Iteration: 141 \t--- Loss: 0.060\n",
      "Iteration: 142 \t--- Loss: 0.061\n",
      "Iteration: 143 \t--- Loss: 0.059\n",
      "Iteration: 144 \t--- Loss: 0.057\n",
      "Iteration: 145 \t--- Loss: 0.054\n",
      "Iteration: 146 \t--- Loss: 0.059\n",
      "Iteration: 147 \t--- Loss: 0.057\n",
      "Iteration: 148 \t--- Loss: 0.053\n",
      "Iteration: 149 \t--- Loss: 0.055\n",
      "Iteration: 150 \t--- Loss: 0.064\n",
      "Iteration: 151 \t--- Loss: 0.059\n",
      "Iteration: 152 \t--- Loss: 0.055\n",
      "Iteration: 153 \t--- Loss: 0.056\n",
      "Iteration: 154 \t--- Loss: 0.052\n",
      "Iteration: 155 \t--- Loss: 0.053\n",
      "Iteration: 156 \t--- Loss: 0.056\n",
      "Iteration: 157 \t--- Loss: 0.060\n",
      "Iteration: 158 \t--- Loss: 0.055\n",
      "Iteration: 159 \t--- Loss: 0.060\n",
      "Iteration: 160 \t--- Loss: 0.057\n",
      "Iteration: 161 \t--- Loss: 0.057\n",
      "Iteration: 162 \t--- Loss: 0.054\n",
      "Iteration: 163 \t--- Loss: 0.057\n",
      "Iteration: 164 \t--- Loss: 0.053\n",
      "Iteration: 165 \t--- Loss: 0.057\n",
      "Iteration: 166 \t--- Loss: 0.062\n",
      "Iteration: 167 \t--- Loss: 0.056\n",
      "Iteration: 168 \t--- Loss: 0.052\n",
      "Iteration: 169 \t--- Loss: 0.060\n",
      "Iteration: 170 \t--- Loss: 0.058\n",
      "Iteration: 171 \t--- Loss: 0.053\n",
      "Iteration: 172 \t--- Loss: 0.057\n",
      "Iteration: 173 \t--- Loss: 0.057\n",
      "Iteration: 174 \t--- Loss: 0.056\n",
      "Iteration: 175 \t--- Loss: 0.056\n",
      "Iteration: 176 \t--- Loss: 0.055\n",
      "Iteration: 177 \t--- Loss: 0.058\n",
      "Iteration: 178 \t--- Loss: 0.058\n",
      "Iteration: 179 \t--- Loss: 0.058\n",
      "Iteration: 180 \t--- Loss: 0.052\n",
      "Iteration: 181 \t--- Loss: 0.053\n",
      "Iteration: 182 \t--- Loss: 0.054\n",
      "Iteration: 183 \t--- Loss: 0.056\n",
      "Iteration: 184 \t--- Loss: 0.055\n",
      "Iteration: 185 \t--- Loss: 0.055\n",
      "Iteration: 186 \t--- Loss: 0.058\n",
      "Iteration: 187 \t--- Loss: 0.061\n",
      "Iteration: 188 \t--- Loss: 0.056\n",
      "Iteration: 189 \t--- Loss: 0.056\n",
      "Iteration: 190 \t--- Loss: 0.056\n",
      "Iteration: 191 \t--- Loss: 0.058\n",
      "Iteration: 192 \t--- Loss: 0.053\n",
      "Iteration: 193 \t--- Loss: 0.055\n",
      "Iteration: 194 \t--- Loss: 0.054\n",
      "Iteration: 195 \t--- Loss: 0.057\n",
      "Iteration: 196 \t--- Loss: 0.056\n",
      "Iteration: 197 \t--- Loss: 0.059\n",
      "Iteration: 198 \t--- Loss: 0.061\n",
      "Iteration: 199 \t--- Loss: 0.053\n",
      "Iteration: 200 \t--- Loss: 0.055\n",
      "Iteration: 201 \t--- Loss: 0.056\n",
      "Iteration: 202 \t--- Loss: 0.061\n",
      "Iteration: 203 \t--- Loss: 0.059\n",
      "Iteration: 204 \t--- Loss: 0.058\n",
      "Iteration: 205 \t--- Loss: 0.056\n",
      "Iteration: 206 \t--- Loss: 0.054\n",
      "Iteration: 207 \t--- Loss: 0.057\n",
      "Iteration: 208 \t--- Loss: 0.056\n",
      "Iteration: 209 \t--- Loss: 0.059\n",
      "Iteration: 210 \t--- Loss: 0.059\n",
      "Iteration: 211 \t--- Loss: 0.057\n",
      "Iteration: 212 \t--- Loss: 0.058\n",
      "Iteration: 213 \t--- Loss: 0.056\n",
      "Iteration: 214 \t--- Loss: 0.057\n",
      "Iteration: 215 \t--- Loss: 0.059\n",
      "Iteration: 216 \t--- Loss: 0.058\n",
      "Iteration: 217 \t--- Loss: 0.057\n",
      "Iteration: 218 \t--- Loss: 0.056\n",
      "Iteration: 219 \t--- Loss: 0.053\n",
      "Iteration: 220 \t--- Loss: 0.058\n",
      "Iteration: 221 \t--- Loss: 0.060\n",
      "Iteration: 222 \t--- Loss: 0.057\n",
      "Iteration: 223 \t--- Loss: 0.055\n",
      "Iteration: 224 \t--- Loss: 0.056\n",
      "Iteration: 225 \t--- Loss: 0.057\n",
      "Iteration: 226 \t--- Loss: 0.059\n",
      "Iteration: 227 \t--- Loss: 0.055\n",
      "Iteration: 228 \t--- Loss: 0.056\n",
      "Iteration: 229 \t--- Loss: 0.058\n",
      "Iteration: 230 \t--- Loss: 0.054\n",
      "Iteration: 231 \t--- Loss: 0.060\n",
      "Iteration: 232 \t--- Loss: 0.056\n",
      "Iteration: 233 \t--- Loss: 0.056\n",
      "Iteration: 234 \t--- Loss: 0.057\n",
      "Iteration: 235 \t--- Loss: 0.059\n",
      "Iteration: 236 \t--- Loss: 0.058\n",
      "Iteration: 237 \t--- Loss: 0.054\n",
      "Iteration: 238 \t--- Loss: 0.059\n",
      "Iteration: 239 \t--- Loss: 0.056\n",
      "Iteration: 240 \t--- Loss: 0.057\n",
      "Iteration: 241 \t--- Loss: 0.053\n",
      "Iteration: 242 \t--- Loss: 0.058\n",
      "Iteration: 243 \t--- Loss: 0.053\n",
      "Iteration: 244 \t--- Loss: 0.054\n",
      "Iteration: 245 \t--- Loss: 0.055\n",
      "Iteration: 246 \t--- Loss: 0.056\n",
      "Iteration: 247 \t--- Loss: 0.055\n",
      "Iteration: 248 \t--- Loss: 0.055\n",
      "Iteration: 249 \t--- Loss: 0.059\n",
      "Iteration: 250 \t--- Loss: 0.058\n",
      "Iteration: 251 \t--- Loss: 0.057\n",
      "Iteration: 252 \t--- Loss: 0.050\n",
      "Iteration: 253 \t--- Loss: 0.057\n",
      "Iteration: 254 \t--- Loss: 0.055\n",
      "Iteration: 255 \t--- Loss: 0.058\n",
      "Iteration: 256 \t--- Loss: 0.061\n",
      "Iteration: 257 \t--- Loss: 0.058\n",
      "Iteration: 258 \t--- Loss: 0.060\n",
      "Iteration: 259 \t--- Loss: 0.055"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:30<00:00, 90.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.022\n",
      "Iteration: 261 \t--- Loss: 0.023\n",
      "Iteration: 262 \t--- Loss: 0.023\n",
      "Iteration: 263 \t--- Loss: 0.024\n",
      "Iteration: 264 \t--- Loss: 0.023\n",
      "Iteration: 265 \t--- Loss: 0.022\n",
      "Iteration: 266 \t--- Loss: 0.024\n",
      "Iteration: 267 \t--- Loss: 0.025\n",
      "Iteration: 268 \t--- Loss: 0.024\n",
      "Iteration: 269 \t--- Loss: 0.025\n",
      "Iteration: 270 \t--- Loss: 0.026\n",
      "Iteration: 271 \t--- Loss: 0.025\n",
      "Iteration: 272 \t--- Loss: 0.022\n",
      "Iteration: 273 \t--- Loss: 0.024\n",
      "Iteration: 274 \t--- Loss: 0.022\n",
      "Iteration: 275 \t--- Loss: 0.025\n",
      "Iteration: 276 \t--- Loss: 0.024\n",
      "Iteration: 277 \t--- Loss: 0.026\n",
      "Iteration: 278 \t--- Loss: 0.025\n",
      "Iteration: 279 \t--- Loss: 0.025\n",
      "Iteration: 280 \t--- Loss: 0.025\n",
      "Iteration: 281 \t--- Loss: 0.026\n",
      "Iteration: 282 \t--- Loss: 0.026\n",
      "Iteration: 283 \t--- Loss: 0.023\n",
      "Iteration: 284 \t--- Loss: 0.026\n",
      "Iteration: 285 \t--- Loss: 0.025\n",
      "Iteration: 286 \t--- Loss: 0.025\n",
      "Iteration: 287 \t--- Loss: 0.024\n",
      "Iteration: 288 \t--- Loss: 0.024\n",
      "Iteration: 289 \t--- Loss: 0.024\n",
      "Iteration: 290 \t--- Loss: 0.024\n",
      "Iteration: 291 \t--- Loss: 0.024\n",
      "Iteration: 292 \t--- Loss: 0.023\n",
      "Iteration: 293 \t--- Loss: 0.024\n",
      "Iteration: 294 \t--- Loss: 0.026\n",
      "Iteration: 295 \t--- Loss: 0.021\n",
      "Iteration: 296 \t--- Loss: 0.025\n",
      "Iteration: 297 \t--- Loss: 0.023\n",
      "Iteration: 298 \t--- Loss: 0.026\n",
      "Iteration: 299 \t--- Loss: 0.025\n",
      "Iteration: 300 \t--- Loss: 0.022\n",
      "Iteration: 301 \t--- Loss: 0.024\n",
      "Iteration: 302 \t--- Loss: 0.023\n",
      "Iteration: 303 \t--- Loss: 0.024\n",
      "Iteration: 304 \t--- Loss: 0.025\n",
      "Iteration: 305 \t--- Loss: 0.025\n",
      "Iteration: 306 \t--- Loss: 0.023\n",
      "Iteration: 307 \t--- Loss: 0.024\n",
      "Iteration: 308 \t--- Loss: 0.025\n",
      "Iteration: 309 \t--- Loss: 0.022\n",
      "Iteration: 310 \t--- Loss: 0.026\n",
      "Iteration: 311 \t--- Loss: 0.025\n",
      "Iteration: 312 \t--- Loss: 0.025\n",
      "Iteration: 313 \t--- Loss: 0.024\n",
      "Iteration: 314 \t--- Loss: 0.023\n",
      "Iteration: 315 \t--- Loss: 0.026\n",
      "Iteration: 316 \t--- Loss: 0.023\n",
      "Iteration: 317 \t--- Loss: 0.025\n",
      "Iteration: 318 \t--- Loss: 0.027\n",
      "Iteration: 319 \t--- Loss: 0.026\n",
      "Iteration: 320 \t--- Loss: 0.024\n",
      "Iteration: 321 \t--- Loss: 0.024\n",
      "Iteration: 322 \t--- Loss: 0.024\n",
      "Iteration: 323 \t--- Loss: 0.024\n",
      "Iteration: 324 \t--- Loss: 0.025\n",
      "Iteration: 325 \t--- Loss: 0.027\n",
      "Iteration: 326 \t--- Loss: 0.026\n",
      "Iteration: 327 \t--- Loss: 0.024\n",
      "Iteration: 328 \t--- Loss: 0.024\n",
      "Iteration: 329 \t--- Loss: 0.022\n",
      "Iteration: 330 \t--- Loss: 0.026\n",
      "Iteration: 331 \t--- Loss: 0.023\n",
      "Iteration: 332 \t--- Loss: 0.023\n",
      "Iteration: 333 \t--- Loss: 0.024\n",
      "Iteration: 334 \t--- Loss: 0.024\n",
      "Iteration: 335 \t--- Loss: 0.025\n",
      "Iteration: 336 \t--- Loss: 0.024\n",
      "Iteration: 337 \t--- Loss: 0.024\n",
      "Iteration: 338 \t--- Loss: 0.025\n",
      "Iteration: 339 \t--- Loss: 0.024\n",
      "Iteration: 340 \t--- Loss: 0.024\n",
      "Iteration: 341 \t--- Loss: 0.024\n",
      "Iteration: 342 \t--- Loss: 0.024\n",
      "Iteration: 343 \t--- Loss: 0.025\n",
      "Iteration: 344 \t--- Loss: 0.025\n",
      "Iteration: 345 \t--- Loss: 0.024\n",
      "Iteration: 346 \t--- Loss: 0.024\n",
      "Iteration: 347 \t--- Loss: 0.026\n",
      "Iteration: 348 \t--- Loss: 0.024\n",
      "Iteration: 349 \t--- Loss: 0.026\n",
      "Iteration: 350 \t--- Loss: 0.024\n",
      "Iteration: 351 \t--- Loss: 0.025\n",
      "Iteration: 352 \t--- Loss: 0.025\n",
      "Iteration: 353 \t--- Loss: 0.025\n",
      "Iteration: 354 \t--- Loss: 0.025\n",
      "Iteration: 355 \t--- Loss: 0.023\n",
      "Iteration: 356 \t--- Loss: 0.025\n",
      "Iteration: 357 \t--- Loss: 0.022\n",
      "Iteration: 358 \t--- Loss: 0.025\n",
      "Iteration: 359 \t--- Loss: 0.024\n",
      "Iteration: 360 \t--- Loss: 0.024\n",
      "Iteration: 361 \t--- Loss: 0.022\n",
      "Iteration: 362 \t--- Loss: 0.025\n",
      "Iteration: 363 \t--- Loss: 0.025\n",
      "Iteration: 364 \t--- Loss: 0.022\n",
      "Iteration: 365 \t--- Loss: 0.025\n",
      "Iteration: 366 \t--- Loss: 0.024\n",
      "Iteration: 367 \t--- Loss: 0.024\n",
      "Iteration: 368 \t--- Loss: 0.024\n",
      "Iteration: 369 \t--- Loss: 0.025\n",
      "Iteration: 370 \t--- Loss: 0.023\n",
      "Iteration: 371 \t--- Loss: 0.024\n",
      "Iteration: 372 \t--- Loss: 0.024\n",
      "Iteration: 373 \t--- Loss: 0.022\n",
      "Iteration: 374 \t--- Loss: 0.025\n",
      "Iteration: 375 \t--- Loss: 0.024\n",
      "Iteration: 376 \t--- Loss: 0.023\n",
      "Iteration: 377 \t--- Loss: 0.024\n",
      "Iteration: 378 \t--- Loss: 0.025\n",
      "Iteration: 379 \t--- Loss: 0.026\n",
      "Iteration: 380 \t--- Loss: 0.025\n",
      "Iteration: 381 \t--- Loss: 0.025\n",
      "Iteration: 382 \t--- Loss: 0.025\n",
      "Iteration: 383 \t--- Loss: 0.022\n",
      "Iteration: 384 \t--- Loss: 0.024\n",
      "Iteration: 385 \t--- Loss: 0.025\n",
      "Iteration: 386 \t--- Loss: 0.024\n",
      "Iteration: 387 \t--- Loss: 0.022\n",
      "Iteration: 388 \t--- Loss: 0.023\n",
      "Iteration: 389 \t--- Loss: 0.024\n",
      "Iteration: 390 \t--- Loss: 0.025\n",
      "Iteration: 391 \t--- Loss: 0.023\n",
      "Iteration: 392 \t--- Loss: 0.026\n",
      "Iteration: 393 \t--- Loss: 0.024\n",
      "Iteration: 394 \t--- Loss: 0.023\n",
      "Iteration: 395 \t--- Loss: 0.026\n",
      "Iteration: 396 \t--- Loss: 0.024\n",
      "Iteration: 397 \t--- Loss: 0.025\n",
      "Iteration: 398 \t--- Loss: 0.023\n",
      "Iteration: 399 \t--- Loss: 0.026\n",
      "Iteration: 400 \t--- Loss: 0.024\n",
      "Iteration: 401 \t--- Loss: 0.025\n",
      "Iteration: 402 \t--- Loss: 0.022\n",
      "Iteration: 403 \t--- Loss: 0.023\n",
      "Iteration: 404 \t--- Loss: 0.023\n",
      "Iteration: 405 \t--- Loss: 0.024\n",
      "Iteration: 406 \t--- Loss: 0.022\n",
      "Iteration: 407 \t--- Loss: 0.023\n",
      "Iteration: 408 \t--- Loss: 0.024\n",
      "Iteration: 409 \t--- Loss: 0.025\n",
      "Iteration: 410 \t--- Loss: 0.023\n",
      "Iteration: 411 \t--- Loss: 0.024\n",
      "Iteration: 412 \t--- Loss: 0.024\n",
      "Iteration: 413 \t--- Loss: 0.024\n",
      "Iteration: 414 \t--- Loss: 0.026\n",
      "Iteration: 415 \t--- Loss: 0.025\n",
      "Iteration: 416 \t--- Loss: 0.026\n",
      "Iteration: 417 \t--- Loss: 0.025\n",
      "Iteration: 418 \t--- Loss: 0.026\n",
      "Iteration: 419 \t--- Loss: 0.025\n",
      "Iteration: 420 \t--- Loss: 0.024\n",
      "Iteration: 421 \t--- Loss: 0.025\n",
      "Iteration: 422 \t--- Loss: 0.022\n",
      "Iteration: 423 \t--- Loss: 0.025\n",
      "Iteration: 424 \t--- Loss: 0.025\n",
      "Iteration: 425 \t--- Loss: 0.022\n",
      "Iteration: 426 \t--- Loss: 0.023\n",
      "Iteration: 427 \t--- Loss: 0.024\n",
      "Iteration: 428 \t--- Loss: 0.023\n",
      "Iteration: 429 \t--- Loss: 0.022\n",
      "Iteration: 430 \t--- Loss: 0.023\n",
      "Iteration: 431 \t--- Loss: 0.023\n",
      "Iteration: 432 \t--- Loss: 0.023\n",
      "Iteration: 433 \t--- Loss: 0.027\n",
      "Iteration: 434 \t--- Loss: 0.024\n",
      "Iteration: 435 \t--- Loss: 0.023\n",
      "Iteration: 436 \t--- Loss: 0.024\n",
      "Iteration: 437 \t--- Loss: 0.023\n",
      "Iteration: 438 \t--- Loss: 0.025\n",
      "Iteration: 439 \t--- Loss: 0.025\n",
      "Iteration: 440 \t--- Loss: 0.024\n",
      "Iteration: 441 \t--- Loss: 0.022\n",
      "Iteration: 442 \t--- Loss: 0.023\n",
      "Iteration: 443 \t--- Loss: 0.023\n",
      "Iteration: 444 \t--- Loss: 0.025\n",
      "Iteration: 445 \t--- Loss: 0.024\n",
      "Iteration: 446 \t--- Loss: 0.023\n",
      "Iteration: 447 \t--- Loss: 0.025\n",
      "Iteration: 448 \t--- Loss: 0.025\n",
      "Iteration: 449 \t--- Loss: 0.025\n",
      "Iteration: 450 \t--- Loss: 0.023\n",
      "Iteration: 451 \t--- Loss: 0.024\n",
      "Iteration: 452 \t--- Loss: 0.026\n",
      "Iteration: 453 \t--- Loss: 0.024\n",
      "Iteration: 454 \t--- Loss: 0.023\n",
      "Iteration: 455 \t--- Loss: 0.023\n",
      "Iteration: 456 \t--- Loss: 0.025\n",
      "Iteration: 457 \t--- Loss: 0.023\n",
      "Iteration: 458 \t--- Loss: 0.024\n",
      "Iteration: 459 \t--- Loss: 0.025\n",
      "Iteration: 460 \t--- Loss: 0.024\n",
      "Iteration: 461 \t--- Loss: 0.024\n",
      "Iteration: 462 \t--- Loss: 0.026\n",
      "Iteration: 463 \t--- Loss: 0.023\n",
      "Iteration: 464 \t--- Loss: 0.024\n",
      "Iteration: 465 \t--- Loss: 0.026\n",
      "Iteration: 466 \t--- Loss: 0.024\n",
      "Iteration: 467 \t--- Loss: 0.022\n",
      "Iteration: 468 \t--- Loss: 0.021\n",
      "Iteration: 469 \t--- Loss: 0.026\n",
      "Iteration: 470 \t--- Loss: 0.022\n",
      "Iteration: 471 \t--- Loss: 0.023\n",
      "Iteration: 472 \t--- Loss: 0.023\n",
      "Iteration: 473 \t--- Loss: 0.023\n",
      "Iteration: 474 \t--- Loss: 0.026\n",
      "Iteration: 475 \t--- Loss: 0.022\n",
      "Iteration: 476 \t--- Loss: 0.023\n",
      "Iteration: 477 \t--- Loss: 0.023\n",
      "Iteration: 478 \t--- Loss: 0.024\n",
      "Iteration: 479 \t--- Loss: 0.022\n",
      "Iteration: 480 \t--- Loss: 0.028\n",
      "Iteration: 481 \t--- Loss: 0.022\n",
      "Iteration: 482 \t--- Loss: 0.023\n",
      "Iteration: 483 \t--- Loss: 0.023\n",
      "Iteration: 484 \t--- Loss: 0.024\n",
      "Iteration: 485 \t--- Loss: 0.025\n",
      "Iteration: 486 \t--- Loss: 0.024\n",
      "Iteration: 487 \t--- Loss: 0.024\n",
      "Iteration: 488 \t--- Loss: 0.022\n",
      "Iteration: 489 \t--- Loss: 0.024\n",
      "Iteration: 490 \t--- Loss: 0.025\n",
      "Iteration: 491 \t--- Loss: 0.024\n",
      "Iteration: 492 \t--- Loss: 0.023\n",
      "Iteration: 493 \t--- Loss: 0.023\n",
      "Iteration: 494 \t--- Loss: 0.023\n",
      "Iteration: 495 \t--- Loss: 0.025\n",
      "Iteration: 496 \t--- Loss: 0.023\n",
      "Iteration: 497 \t--- Loss: 0.022\n",
      "Iteration: 498 \t--- Loss: 0.023\n",
      "Iteration: 499 \t--- Loss: 0.025\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.57s/it][Parallel(n_jobs=5)]: Done  18 tasks      | elapsed: 10.3min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "\n",
      "Iteration: 260 \t--- Loss: 0.020\n",
      "Iteration: 261 \t--- Loss: 0.021\n",
      "Iteration: 262 \t--- Loss: 0.024\n",
      "Iteration: 263 \t--- Loss: 0.024\n",
      "Iteration: 264 \t--- Loss: 0.024\n",
      "Iteration: 265 \t--- Loss: 0.024\n",
      "Iteration: 266 \t--- Loss: 0.027\n",
      "Iteration: 267 \t--- Loss: 0.031\n",
      "Iteration: 268 \t--- Loss: 0.017\n",
      "Iteration: 269 \t--- Loss: 0.018\n",
      "Iteration: 270 \t--- Loss: 0.020\n",
      "Iteration: 271 \t--- Loss: 0.021\n",
      "Iteration: 272 \t--- Loss: 0.017\n",
      "Iteration: 273 \t--- Loss: 0.019\n",
      "Iteration: 274 \t--- Loss: 0.022\n",
      "Iteration: 275 \t--- Loss: 0.020\n",
      "Iteration: 276 \t--- Loss: 0.020\n",
      "Iteration: 277 \t--- Loss: 0.021\n",
      "Iteration: 278 \t--- Loss: 0.022\n",
      "Iteration: 279 \t--- Loss: 0.025\n",
      "Iteration: 280 \t--- Loss: 0.023\n",
      "Iteration: 281 \t--- Loss: 0.023\n",
      "Iteration: 282 \t--- Loss: 0.024\n",
      "Iteration: 283 \t--- Loss: 0.021\n",
      "Iteration: 284 \t--- Loss: 0.027\n",
      "Iteration: 285 \t--- Loss: 0.026\n",
      "Iteration: 286 \t--- Loss: 0.022\n",
      "Iteration: 287 \t--- Loss: 0.021\n",
      "Iteration: 288 \t--- Loss: 0.020\n",
      "Iteration: 289 \t--- Loss: 0.023\n",
      "Iteration: 290 \t--- Loss: 0.020\n",
      "Iteration: 291 \t--- Loss: 0.019\n",
      "Iteration: 292 \t--- Loss: 0.018\n",
      "Iteration: 293 \t--- Loss: 0.019\n",
      "Iteration: 294 \t--- Loss: 0.017\n",
      "Iteration: 295 \t--- Loss: 0.015\n",
      "Iteration: 296 \t--- Loss: 0.021\n",
      "Iteration: 297 \t--- Loss: 0.021\n",
      "Iteration: 298 \t--- Loss: 0.022\n",
      "Iteration: 299 \t--- Loss: 0.020\n",
      "Iteration: 300 \t--- Loss: 0.017\n",
      "Iteration: 301 \t--- Loss: 0.018\n",
      "Iteration: 302 \t--- Loss: 0.019\n",
      "Iteration: 303 \t--- Loss: 0.017\n",
      "Iteration: 304 \t--- Loss: 0.022\n",
      "Iteration: 305 \t--- Loss: 0.024\n",
      "Iteration: 306 \t--- Loss: 0.018\n",
      "Iteration: 307 \t--- Loss: 0.019\n",
      "Iteration: 308 \t--- Loss: 0.021\n",
      "Iteration: 309 \t--- Loss: 0.019\n",
      "Iteration: 310 \t--- Loss: 0.022\n",
      "Iteration: 311 \t--- Loss: 0.022\n",
      "Iteration: 312 \t--- Loss: 0.016\n",
      "Iteration: 313 \t--- Loss: 0.015\n",
      "Iteration: 314 \t--- Loss: 0.017\n",
      "Iteration: 315 \t--- Loss: 0.015\n",
      "Iteration: 316 \t--- Loss: 0.015\n",
      "Iteration: 317 \t--- Loss: 0.017\n",
      "Iteration: 318 \t--- Loss: 0.015\n",
      "Iteration: 319 \t--- Loss: 0.017\n",
      "Iteration: 320 \t--- Loss: 0.017\n",
      "Iteration: 321 \t--- Loss: 0.016\n",
      "Iteration: 322 \t--- Loss: 0.018\n",
      "Iteration: 323 \t--- Loss: 0.017\n",
      "Iteration: 324 \t--- Loss: 0.020\n",
      "Iteration: 325 \t--- Loss: 0.021\n",
      "Iteration: 326 \t--- Loss: 0.026\n",
      "Iteration: 327 \t--- Loss: 0.030\n",
      "Iteration: 328 \t--- Loss: 0.015\n",
      "Iteration: 329 \t--- Loss: 0.015\n",
      "Iteration: 330 \t--- Loss: 0.015\n",
      "Iteration: 331 \t--- Loss: 0.016\n",
      "Iteration: 332 \t--- Loss: 0.016\n",
      "Iteration: 333 \t--- Loss: 0.013\n",
      "Iteration: 334 \t--- Loss: 0.015\n",
      "Iteration: 335 \t--- Loss: 0.016\n",
      "Iteration: 336 \t--- Loss: 0.015\n",
      "Iteration: 337 \t--- Loss: 0.013\n",
      "Iteration: 338 \t--- Loss: 0.017\n",
      "Iteration: 339 \t--- Loss: 0.017\n",
      "Iteration: 340 \t--- Loss: 0.015\n",
      "Iteration: 341 \t--- Loss: 0.019\n",
      "Iteration: 342 \t--- Loss: 0.016\n",
      "Iteration: 343 \t--- Loss: 0.020\n",
      "Iteration: 344 \t--- Loss: 0.014\n",
      "Iteration: 345 \t--- Loss: 0.016\n",
      "Iteration: 346 \t--- Loss: 0.013\n",
      "Iteration: 347 \t--- Loss: 0.018\n",
      "Iteration: 348 \t--- Loss: 0.016\n",
      "Iteration: 349 \t--- Loss: 0.018\n",
      "Iteration: 350 \t--- Loss: 0.021\n",
      "Iteration: 351 \t--- Loss: 0.020\n",
      "Iteration: 352 \t--- Loss: 0.022\n",
      "Iteration: 353 \t--- Loss: 0.017\n",
      "Iteration: 354 \t--- Loss: 0.020\n",
      "Iteration: 355 \t--- Loss: 0.021\n",
      "Iteration: 356 \t--- Loss: 0.025\n",
      "Iteration: 357 \t--- Loss: 0.016\n",
      "Iteration: 358 \t--- Loss: 0.017\n",
      "Iteration: 359 \t--- Loss: 0.015\n",
      "Iteration: 360 \t--- Loss: 0.015\n",
      "Iteration: 361 \t--- Loss: 0.022\n",
      "Iteration: 362 \t--- Loss: 0.022\n",
      "Iteration: 363 \t--- Loss: 0.018\n",
      "Iteration: 364 \t--- Loss: 0.018\n",
      "Iteration: 365 \t--- Loss: 0.020\n",
      "Iteration: 366 \t--- Loss: 0.019\n",
      "Iteration: 367 \t--- Loss: 0.015\n",
      "Iteration: 368 \t--- Loss: 0.015\n",
      "Iteration: 369 \t--- Loss: 0.016\n",
      "Iteration: 370 \t--- Loss: 0.015\n",
      "Iteration: 371 \t--- Loss: 0.015\n",
      "Iteration: 372 \t--- Loss: 0.015\n",
      "Iteration: 373 \t--- Loss: 0.012\n",
      "Iteration: 374 \t--- Loss: 0.014\n",
      "Iteration: 375 \t--- Loss: 0.014\n",
      "Iteration: 376 \t--- Loss: 0.015\n",
      "Iteration: 377 \t--- Loss: 0.013\n",
      "Iteration: 378 \t--- Loss: 0.013\n",
      "Iteration: 379 \t--- Loss: 0.015\n",
      "Iteration: 380 \t--- Loss: 0.014\n",
      "Iteration: 381 \t--- Loss: 0.014\n",
      "Iteration: 382 \t--- Loss: 0.013\n",
      "Iteration: 383 \t--- Loss: 0.015\n",
      "Iteration: 384 \t--- Loss: 0.015\n",
      "Iteration: 385 \t--- Loss: 0.014\n",
      "Iteration: 386 \t--- Loss: 0.015\n",
      "Iteration: 387 \t--- Loss: 0.014\n",
      "Iteration: 388 \t--- Loss: 0.016\n",
      "Iteration: 389 \t--- Loss: 0.015\n",
      "Iteration: 390 \t--- Loss: 0.014\n",
      "Iteration: 391 \t--- Loss: 0.014\n",
      "Iteration: 392 \t--- Loss: 0.015\n",
      "Iteration: 393 \t--- Loss: 0.016\n",
      "Iteration: 394 \t--- Loss: 0.016\n",
      "Iteration: 395 \t--- Loss: 0.017\n",
      "Iteration: 396 \t--- Loss: 0.015\n",
      "Iteration: 397 \t--- Loss: 0.017\n",
      "Iteration: 398 \t--- Loss: 0.022\n",
      "Iteration: 399 \t--- Loss: 0.018\n",
      "Iteration: 400 \t--- Loss: 0.023\n",
      "Iteration: 401 \t--- Loss: 0.014\n",
      "Iteration: 402 \t--- Loss: 0.015\n",
      "Iteration: 403 \t--- Loss: 0.016\n",
      "Iteration: 404 \t--- Loss: 0.016\n",
      "Iteration: 405 \t--- Loss: 0.016\n",
      "Iteration: 406 \t--- Loss: 0.017\n",
      "Iteration: 407 \t--- Loss: 0.018\n",
      "Iteration: 408 \t--- Loss: 0.017\n",
      "Iteration: 409 \t--- Loss: 0.016\n",
      "Iteration: 410 \t--- Loss: 0.016\n",
      "Iteration: 411 \t--- Loss: 0.014\n",
      "Iteration: 412 \t--- Loss: 0.014\n",
      "Iteration: 413 \t--- Loss: 0.012\n",
      "Iteration: 414 \t--- Loss: 0.014\n",
      "Iteration: 415 \t--- Loss: 0.015\n",
      "Iteration: 416 \t--- Loss: 0.014\n",
      "Iteration: 417 \t--- Loss: 0.015\n",
      "Iteration: 418 \t--- Loss: 0.013\n",
      "Iteration: 419 \t--- Loss: 0.015\n",
      "Iteration: 420 \t--- Loss: 0.015\n",
      "Iteration: 421 \t--- Loss: 0.015\n",
      "Iteration: 422 \t--- Loss: 0.013\n",
      "Iteration: 423 \t--- Loss: 0.014\n",
      "Iteration: 424 \t--- Loss: 0.013\n",
      "Iteration: 425 \t--- Loss: 0.014\n",
      "Iteration: 426 \t--- Loss: 0.014\n",
      "Iteration: 427 \t--- Loss: 0.015\n",
      "Iteration: 428 \t--- Loss: 0.016\n",
      "Iteration: 429 \t--- Loss: 0.014\n",
      "Iteration: 430 \t--- Loss: 0.013\n",
      "Iteration: 431 \t--- Loss: 0.014\n",
      "Iteration: 432 \t--- Loss: 0.012\n",
      "Iteration: 433 \t--- Loss: 0.014\n",
      "Iteration: 434 \t--- Loss: 0.015\n",
      "Iteration: 435 \t--- Loss: 0.019\n",
      "Iteration: 436 \t--- Loss: 0.020\n",
      "Iteration: 437 \t--- Loss: 0.015\n",
      "Iteration: 438 \t--- Loss: 0.015\n",
      "Iteration: 439 \t--- Loss: 0.013\n",
      "Iteration: 440 \t--- Loss: 0.014\n",
      "Iteration: 441 \t--- Loss: 0.015\n",
      "Iteration: 442 \t--- Loss: 0.015\n",
      "Iteration: 443 \t--- Loss: 0.014\n",
      "Iteration: 444 \t--- Loss: 0.016\n",
      "Iteration: 445 \t--- Loss: 0.014\n",
      "Iteration: 446 \t--- Loss: 0.013\n",
      "Iteration: 447 \t--- Loss: 0.014\n",
      "Iteration: 448 \t--- Loss: 0.013\n",
      "Iteration: 449 \t--- Loss: 0.014\n",
      "Iteration: 450 \t--- Loss: 0.013\n",
      "Iteration: 451 \t--- Loss: 0.014\n",
      "Iteration: 452 \t--- Loss: 0.012\n",
      "Iteration: 453 \t--- Loss: 0.012\n",
      "Iteration: 454 \t--- Loss: 0.013\n",
      "Iteration: 455 \t--- Loss: 0.012\n",
      "Iteration: 456 \t--- Loss: 0.014\n",
      "Iteration: 457 \t--- Loss: 0.013\n",
      "Iteration: 458 \t--- Loss: 0.014\n",
      "Iteration: 459 \t--- Loss: 0.013\n",
      "Iteration: 460 \t--- Loss: 0.013\n",
      "Iteration: 461 \t--- Loss: 0.013\n",
      "Iteration: 462 \t--- Loss: 0.014\n",
      "Iteration: 463 \t--- Loss: 0.014\n",
      "Iteration: 464 \t--- Loss: 0.015\n",
      "Iteration: 465 \t--- Loss: 0.016\n",
      "Iteration: 466 \t--- Loss: 0.014\n",
      "Iteration: 467 \t--- Loss: 0.013\n",
      "Iteration: 468 \t--- Loss: 0.013\n",
      "Iteration: 469 \t--- Loss: 0.014\n",
      "Iteration: 470 \t--- Loss: 0.014\n",
      "Iteration: 471 \t--- Loss: 0.015\n",
      "Iteration: 472 \t--- Loss: 0.014\n",
      "Iteration: 473 \t--- Loss: 0.015\n",
      "Iteration: 474 \t--- Loss: 0.020\n",
      "Iteration: 475 \t--- Loss: 0.021\n",
      "Iteration: 476 \t--- Loss: 0.014\n",
      "Iteration: 477 \t--- Loss: 0.012\n",
      "Iteration: 478 \t--- Loss: 0.012\n",
      "Iteration: 479 \t--- Loss: 0.012\n",
      "Iteration: 480 \t--- Loss: 0.012\n",
      "Iteration: 481 \t--- Loss: 0.014\n",
      "Iteration: 482 \t--- Loss: 0.012\n",
      "Iteration: 483 \t--- Loss: 0.014\n",
      "Iteration: 484 \t--- Loss: 0.014\n",
      "Iteration: 485 \t--- Loss: 0.012\n",
      "Iteration: 486 \t--- Loss: 0.011\n",
      "Iteration: 487 \t--- Loss: 0.012\n",
      "Iteration: 488 \t--- Loss: 0.012\n",
      "Iteration: 489 \t--- Loss: 0.014\n",
      "Iteration: 490 \t--- Loss: 0.013\n",
      "Iteration: 491 \t--- Loss: 0.013\n",
      "Iteration: 492 \t--- Loss: 0.014\n",
      "Iteration: 493 \t--- Loss: 0.014\n",
      "Iteration: 494 \t--- Loss: 0.015\n",
      "Iteration: 495 \t--- Loss: 0.014\n",
      "Iteration: 496 \t--- Loss: 0.015\n",
      "Iteration: 497 \t--- Loss: 0.015\n",
      "Iteration: 498 \t--- Loss: 0.016\n",
      "Iteration: 499 \t--- Loss: 0.016\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:31<00:00, 91.35s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.054\n",
      "Iteration: 261 \t--- Loss: 0.056\n",
      "Iteration: 262 \t--- Loss: 0.056\n",
      "Iteration: 263 \t--- Loss: 0.055\n",
      "Iteration: 264 \t--- Loss: 0.058\n",
      "Iteration: 265 \t--- Loss: 0.054\n",
      "Iteration: 266 \t--- Loss: 0.056\n",
      "Iteration: 267 \t--- Loss: 0.055\n",
      "Iteration: 268 \t--- Loss: 0.057\n",
      "Iteration: 269 \t--- Loss: 0.057\n",
      "Iteration: 270 \t--- Loss: 0.056\n",
      "Iteration: 271 \t--- Loss: 0.057\n",
      "Iteration: 272 \t--- Loss: 0.056\n",
      "Iteration: 273 \t--- Loss: 0.057\n",
      "Iteration: 274 \t--- Loss: 0.059\n",
      "Iteration: 275 \t--- Loss: 0.056\n",
      "Iteration: 276 \t--- Loss: 0.053\n",
      "Iteration: 277 \t--- Loss: 0.057\n",
      "Iteration: 278 \t--- Loss: 0.053\n",
      "Iteration: 279 \t--- Loss: 0.058\n",
      "Iteration: 280 \t--- Loss: 0.055\n",
      "Iteration: 281 \t--- Loss: 0.056\n",
      "Iteration: 282 \t--- Loss: 0.057\n",
      "Iteration: 283 \t--- Loss: 0.059\n",
      "Iteration: 284 \t--- Loss: 0.058\n",
      "Iteration: 285 \t--- Loss: 0.057\n",
      "Iteration: 286 \t--- Loss: 0.058\n",
      "Iteration: 287 \t--- Loss: 0.054\n",
      "Iteration: 288 \t--- Loss: 0.054\n",
      "Iteration: 289 \t--- Loss: 0.057\n",
      "Iteration: 290 \t--- Loss: 0.055\n",
      "Iteration: 291 \t--- Loss: 0.058\n",
      "Iteration: 292 \t--- Loss: 0.051\n",
      "Iteration: 293 \t--- Loss: 0.056\n",
      "Iteration: 294 \t--- Loss: 0.054\n",
      "Iteration: 295 \t--- Loss: 0.056\n",
      "Iteration: 296 \t--- Loss: 0.057\n",
      "Iteration: 297 \t--- Loss: 0.057\n",
      "Iteration: 298 \t--- Loss: 0.058\n",
      "Iteration: 299 \t--- Loss: 0.055\n",
      "Iteration: 300 \t--- Loss: 0.058\n",
      "Iteration: 301 \t--- Loss: 0.056\n",
      "Iteration: 302 \t--- Loss: 0.058\n",
      "Iteration: 303 \t--- Loss: 0.058\n",
      "Iteration: 304 \t--- Loss: 0.052\n",
      "Iteration: 305 \t--- Loss: 0.058\n",
      "Iteration: 306 \t--- Loss: 0.052\n",
      "Iteration: 307 \t--- Loss: 0.059\n",
      "Iteration: 308 \t--- Loss: 0.054\n",
      "Iteration: 309 \t--- Loss: 0.058\n",
      "Iteration: 310 \t--- Loss: 0.058\n",
      "Iteration: 311 \t--- Loss: 0.052\n",
      "Iteration: 312 \t--- Loss: 0.058\n",
      "Iteration: 313 \t--- Loss: 0.057\n",
      "Iteration: 314 \t--- Loss: 0.056\n",
      "Iteration: 315 \t--- Loss: 0.053\n",
      "Iteration: 316 \t--- Loss: 0.055\n",
      "Iteration: 317 \t--- Loss: 0.055\n",
      "Iteration: 318 \t--- Loss: 0.055\n",
      "Iteration: 319 \t--- Loss: 0.059\n",
      "Iteration: 320 \t--- Loss: 0.058\n",
      "Iteration: 321 \t--- Loss: 0.055\n",
      "Iteration: 322 \t--- Loss: 0.054\n",
      "Iteration: 323 \t--- Loss: 0.059\n",
      "Iteration: 324 \t--- Loss: 0.056\n",
      "Iteration: 325 \t--- Loss: 0.056\n",
      "Iteration: 326 \t--- Loss: 0.060\n",
      "Iteration: 327 \t--- Loss: 0.055\n",
      "Iteration: 328 \t--- Loss: 0.056\n",
      "Iteration: 329 \t--- Loss: 0.054\n",
      "Iteration: 330 \t--- Loss: 0.054\n",
      "Iteration: 331 \t--- Loss: 0.052\n",
      "Iteration: 332 \t--- Loss: 0.058\n",
      "Iteration: 333 \t--- Loss: 0.053\n",
      "Iteration: 334 \t--- Loss: 0.056\n",
      "Iteration: 335 \t--- Loss: 0.059\n",
      "Iteration: 336 \t--- Loss: 0.057\n",
      "Iteration: 337 \t--- Loss: 0.055\n",
      "Iteration: 338 \t--- Loss: 0.053\n",
      "Iteration: 339 \t--- Loss: 0.053\n",
      "Iteration: 340 \t--- Loss: 0.054\n",
      "Iteration: 341 \t--- Loss: 0.056\n",
      "Iteration: 342 \t--- Loss: 0.056\n",
      "Iteration: 343 \t--- Loss: 0.055\n",
      "Iteration: 344 \t--- Loss: 0.053\n",
      "Iteration: 345 \t--- Loss: 0.061\n",
      "Iteration: 346 \t--- Loss: 0.053\n",
      "Iteration: 347 \t--- Loss: 0.053\n",
      "Iteration: 348 \t--- Loss: 0.053\n",
      "Iteration: 349 \t--- Loss: 0.059\n",
      "Iteration: 350 \t--- Loss: 0.058\n",
      "Iteration: 351 \t--- Loss: 0.055\n",
      "Iteration: 352 \t--- Loss: 0.056\n",
      "Iteration: 353 \t--- Loss: 0.057\n",
      "Iteration: 354 \t--- Loss: 0.057\n",
      "Iteration: 355 \t--- Loss: 0.058\n",
      "Iteration: 356 \t--- Loss: 0.056\n",
      "Iteration: 357 \t--- Loss: 0.058\n",
      "Iteration: 358 \t--- Loss: 0.057\n",
      "Iteration: 359 \t--- Loss: 0.055\n",
      "Iteration: 360 \t--- Loss: 0.056\n",
      "Iteration: 361 \t--- Loss: 0.059\n",
      "Iteration: 362 \t--- Loss: 0.056\n",
      "Iteration: 363 \t--- Loss: 0.055\n",
      "Iteration: 364 \t--- Loss: 0.057\n",
      "Iteration: 365 \t--- Loss: 0.059\n",
      "Iteration: 366 \t--- Loss: 0.058\n",
      "Iteration: 367 \t--- Loss: 0.059\n",
      "Iteration: 368 \t--- Loss: 0.053\n",
      "Iteration: 369 \t--- Loss: 0.055\n",
      "Iteration: 370 \t--- Loss: 0.054\n",
      "Iteration: 371 \t--- Loss: 0.063\n",
      "Iteration: 372 \t--- Loss: 0.057\n",
      "Iteration: 373 \t--- Loss: 0.057\n",
      "Iteration: 374 \t--- Loss: 0.055\n",
      "Iteration: 375 \t--- Loss: 0.054\n",
      "Iteration: 376 \t--- Loss: 0.058\n",
      "Iteration: 377 \t--- Loss: 0.057\n",
      "Iteration: 378 \t--- Loss: 0.054\n",
      "Iteration: 379 \t--- Loss: 0.057\n",
      "Iteration: 380 \t--- Loss: 0.058\n",
      "Iteration: 381 \t--- Loss: 0.053\n",
      "Iteration: 382 \t--- Loss: 0.061\n",
      "Iteration: 383 \t--- Loss: 0.057\n",
      "Iteration: 384 \t--- Loss: 0.056\n",
      "Iteration: 385 \t--- Loss: 0.057\n",
      "Iteration: 386 \t--- Loss: 0.055\n",
      "Iteration: 387 \t--- Loss: 0.058\n",
      "Iteration: 388 \t--- Loss: 0.053\n",
      "Iteration: 389 \t--- Loss: 0.053\n",
      "Iteration: 390 \t--- Loss: 0.054\n",
      "Iteration: 391 \t--- Loss: 0.053\n",
      "Iteration: 392 \t--- Loss: 0.054\n",
      "Iteration: 393 \t--- Loss: 0.059\n",
      "Iteration: 394 \t--- Loss: 0.058\n",
      "Iteration: 395 \t--- Loss: 0.056\n",
      "Iteration: 396 \t--- Loss: 0.057\n",
      "Iteration: 397 \t--- Loss: 0.056\n",
      "Iteration: 398 \t--- Loss: 0.056\n",
      "Iteration: 399 \t--- Loss: 0.054\n",
      "Iteration: 400 \t--- Loss: 0.055\n",
      "Iteration: 401 \t--- Loss: 0.058\n",
      "Iteration: 402 \t--- Loss: 0.055\n",
      "Iteration: 403 \t--- Loss: 0.055\n",
      "Iteration: 404 \t--- Loss: 0.055\n",
      "Iteration: 405 \t--- Loss: 0.056\n",
      "Iteration: 406 \t--- Loss: 0.059\n",
      "Iteration: 407 \t--- Loss: 0.054\n",
      "Iteration: 408 \t--- Loss: 0.054\n",
      "Iteration: 409 \t--- Loss: 0.055\n",
      "Iteration: 410 \t--- Loss: 0.057\n",
      "Iteration: 411 \t--- Loss: 0.053\n",
      "Iteration: 412 \t--- Loss: 0.058\n",
      "Iteration: 413 \t--- Loss: 0.055\n",
      "Iteration: 414 \t--- Loss: 0.058\n",
      "Iteration: 415 \t--- Loss: 0.052\n",
      "Iteration: 416 \t--- Loss: 0.055\n",
      "Iteration: 417 \t--- Loss: 0.059\n",
      "Iteration: 418 \t--- Loss: 0.056\n",
      "Iteration: 419 \t--- Loss: 0.057\n",
      "Iteration: 420 \t--- Loss: 0.054\n",
      "Iteration: 421 \t--- Loss: 0.053\n",
      "Iteration: 422 \t--- Loss: 0.054\n",
      "Iteration: 423 \t--- Loss: 0.058\n",
      "Iteration: 424 \t--- Loss: 0.054\n",
      "Iteration: 425 \t--- Loss: 0.059\n",
      "Iteration: 426 \t--- Loss: 0.055\n",
      "Iteration: 427 \t--- Loss: 0.054\n",
      "Iteration: 428 \t--- Loss: 0.053\n",
      "Iteration: 429 \t--- Loss: 0.056\n",
      "Iteration: 430 \t--- Loss: 0.057\n",
      "Iteration: 431 \t--- Loss: 0.053\n",
      "Iteration: 432 \t--- Loss: 0.057\n",
      "Iteration: 433 \t--- Loss: 0.054\n",
      "Iteration: 434 \t--- Loss: 0.056\n",
      "Iteration: 435 \t--- Loss: 0.056\n",
      "Iteration: 436 \t--- Loss: 0.057\n",
      "Iteration: 437 \t--- Loss: 0.061\n",
      "Iteration: 438 \t--- Loss: 0.051\n",
      "Iteration: 439 \t--- Loss: 0.054\n",
      "Iteration: 440 \t--- Loss: 0.056\n",
      "Iteration: 441 \t--- Loss: 0.056\n",
      "Iteration: 442 \t--- Loss: 0.061\n",
      "Iteration: 443 \t--- Loss: 0.056\n",
      "Iteration: 444 \t--- Loss: 0.059\n",
      "Iteration: 445 \t--- Loss: 0.063\n",
      "Iteration: 446 \t--- Loss: 0.060\n",
      "Iteration: 447 \t--- Loss: 0.057\n",
      "Iteration: 448 \t--- Loss: 0.062\n",
      "Iteration: 449 \t--- Loss: 0.053\n",
      "Iteration: 450 \t--- Loss: 0.055\n",
      "Iteration: 451 \t--- Loss: 0.055\n",
      "Iteration: 452 \t--- Loss: 0.052\n",
      "Iteration: 453 \t--- Loss: 0.054\n",
      "Iteration: 454 \t--- Loss: 0.060\n",
      "Iteration: 455 \t--- Loss: 0.056\n",
      "Iteration: 456 \t--- Loss: 0.060\n",
      "Iteration: 457 \t--- Loss: 0.055\n",
      "Iteration: 458 \t--- Loss: 0.057\n",
      "Iteration: 459 \t--- Loss: 0.056\n",
      "Iteration: 460 \t--- Loss: 0.055\n",
      "Iteration: 461 \t--- Loss: 0.051\n",
      "Iteration: 462 \t--- Loss: 0.056\n",
      "Iteration: 463 \t--- Loss: 0.053\n",
      "Iteration: 464 \t--- Loss: 0.053\n",
      "Iteration: 465 \t--- Loss: 0.061\n",
      "Iteration: 466 \t--- Loss: 0.058\n",
      "Iteration: 467 \t--- Loss: 0.054\n",
      "Iteration: 468 \t--- Loss: 0.058\n",
      "Iteration: 469 \t--- Loss: 0.056\n",
      "Iteration: 470 \t--- Loss: 0.057\n",
      "Iteration: 471 \t--- Loss: 0.054\n",
      "Iteration: 472 \t--- Loss: 0.054\n",
      "Iteration: 473 \t--- Loss: 0.057\n",
      "Iteration: 474 \t--- Loss: 0.055\n",
      "Iteration: 475 \t--- Loss: 0.060\n",
      "Iteration: 476 \t--- Loss: 0.055\n",
      "Iteration: 477 \t--- Loss: 0.056\n",
      "Iteration: 478 \t--- Loss: 0.058\n",
      "Iteration: 479 \t--- Loss: 0.058\n",
      "Iteration: 480 \t--- Loss: 0.054\n",
      "Iteration: 481 \t--- Loss: 0.058\n",
      "Iteration: 482 \t--- Loss: 0.057\n",
      "Iteration: 483 \t--- Loss: 0.058\n",
      "Iteration: 484 \t--- Loss: 0.056\n",
      "Iteration: 485 \t--- Loss: 0.054\n",
      "Iteration: 486 \t--- Loss: 0.059\n",
      "Iteration: 487 \t--- Loss: 0.052\n",
      "Iteration: 488 \t--- Loss: 0.056\n",
      "Iteration: 489 \t--- Loss: 0.057\n",
      "Iteration: 490 \t--- Loss: 0.057\n",
      "Iteration: 491 \t--- Loss: 0.055\n",
      "Iteration: 492 \t--- Loss: 0.058\n",
      "Iteration: 493 \t--- Loss: 0.056\n",
      "Iteration: 494 \t--- Loss: 0.056\n",
      "Iteration: 495 \t--- Loss: 0.055\n",
      "Iteration: 496 \t--- Loss: 0.057\n",
      "Iteration: 497 \t--- Loss: 0.056\n",
      "Iteration: 498 \t--- Loss: 0.056\n",
      "Iteration: 499 \t--- Loss: 0.057\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.71s/it][Parallel(n_jobs=5)]: Done  19 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=5)]: Done  20 tasks      | elapsed: 10.8min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.50s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 3.367\n",
      "Iteration: 1 \t--- Loss: 3.489\n",
      "Iteration: 2 \t--- Loss: 3.615\n",
      "Iteration: 3 \t--- Loss: 3.013\n",
      "Iteration: 4 \t--- Loss: 3.058\n",
      "Iteration: 5 \t--- Loss: 3.022\n",
      "Iteration: 6 \t--- Loss: 2.978\n",
      "Iteration: 7 \t--- Loss: 2.952\n",
      "Iteration: 8 \t--- Loss: 2.985\n",
      "Iteration: 9 \t--- Loss: 2.985\n",
      "Iteration: 10 \t--- Loss: 2.907\n",
      "Iteration: 11 \t--- Loss: 2.842\n",
      "Iteration: 12 \t--- Loss: 2.697\n",
      "Iteration: 13 \t--- Loss: 2.660\n",
      "Iteration: 14 \t--- Loss: 2.806\n",
      "Iteration: 15 \t--- Loss: 2.995\n",
      "Iteration: 16 \t--- Loss: 2.792\n",
      "Iteration: 17 \t--- Loss: 2.818\n",
      "Iteration: 18 \t--- Loss: 2.805\n",
      "Iteration: 19 \t--- Loss: 2.584\n",
      "Iteration: 20 \t--- Loss: 2.695\n",
      "Iteration: 21 \t--- Loss: 2.928\n",
      "Iteration: 22 \t--- Loss: 2.830\n",
      "Iteration: 23 \t--- Loss: 2.848\n",
      "Iteration: 24 \t--- Loss: 2.750\n",
      "Iteration: 25 \t--- Loss: 2.781\n",
      "Iteration: 26 \t--- Loss: 2.739\n",
      "Iteration: 27 \t--- Loss: 2.801\n",
      "Iteration: 28 \t--- Loss: 2.942\n",
      "Iteration: 29 \t--- Loss: 2.833\n",
      "Iteration: 30 \t--- Loss: 2.842\n",
      "Iteration: 31 \t--- Loss: 2.890\n",
      "Iteration: 32 \t--- Loss: 2.867\n",
      "Iteration: 33 \t--- Loss: 2.994\n",
      "Iteration: 34 \t--- Loss: 2.945\n",
      "Iteration: 35 \t--- Loss: 2.780\n",
      "Iteration: 36 \t--- Loss: 2.803\n",
      "Iteration: 37 \t--- Loss: 2.884\n",
      "Iteration: 38 \t--- Loss: 2.778\n",
      "Iteration: 39 \t--- Loss: 2.884\n",
      "Iteration: 40 \t--- Loss: 2.746\n",
      "Iteration: 41 \t--- Loss: 2.817\n",
      "Iteration: 42 \t--- Loss: 3.001\n",
      "Iteration: 43 \t--- Loss: 2.816\n",
      "Iteration: 44 \t--- Loss: 2.931\n",
      "Iteration: 45 \t--- Loss: 2.922\n",
      "Iteration: 46 \t--- Loss: 2.954\n",
      "Iteration: 47 \t--- Loss: 2.724\n",
      "Iteration: 48 \t--- Loss: 2.781\n",
      "Iteration: 49 \t--- Loss: 2.739\n",
      "Iteration: 50 \t--- Loss: 2.919\n",
      "Iteration: 51 \t--- Loss: 2.870\n",
      "Iteration: 52 \t--- Loss: 2.697\n",
      "Iteration: 53 \t--- Loss: 2.762\n",
      "Iteration: 54 \t--- Loss: 2.728\n",
      "Iteration: 55 \t--- Loss: 2.949\n",
      "Iteration: 56 \t--- Loss: 2.894\n",
      "Iteration: 57 \t--- Loss: 2.701\n",
      "Iteration: 58 \t--- Loss: 2.981\n",
      "Iteration: 59 \t--- Loss: 2.751\n",
      "Iteration: 60 \t--- Loss: 2.853\n",
      "Iteration: 61 \t--- Loss: 2.796\n",
      "Iteration: 62 \t--- Loss: 2.644\n",
      "Iteration: 63 \t--- Loss: 2.818\n",
      "Iteration: 64 \t--- Loss: 2.823\n",
      "Iteration: 65 \t--- Loss: 2.861\n",
      "Iteration: 66 \t--- Loss: 2.810\n",
      "Iteration: 67 \t--- Loss: 2.881\n",
      "Iteration: 68 \t--- Loss: 2.695\n",
      "Iteration: 69 \t--- Loss: 2.941\n",
      "Iteration: 70 \t--- Loss: 2.809\n",
      "Iteration: 71 \t--- Loss: 2.909\n",
      "Iteration: 72 \t--- Loss: 3.053\n",
      "Iteration: 73 \t--- Loss: 2.836\n",
      "Iteration: 74 \t--- Loss: 2.851\n",
      "Iteration: 75 \t--- Loss: 2.652\n",
      "Iteration: 76 \t--- Loss: 2.849\n",
      "Iteration: 77 \t--- Loss: 2.831\n",
      "Iteration: 78 \t--- Loss: 2.837\n",
      "Iteration: 79 \t--- Loss: 2.969\n",
      "Iteration: 80 \t--- Loss: 2.954\n",
      "Iteration: 81 \t--- Loss: 2.913\n",
      "Iteration: 82 \t--- Loss: 2.760\n",
      "Iteration: 83 \t--- Loss: 2.861\n",
      "Iteration: 84 \t--- Loss: 2.749\n",
      "Iteration: 85 \t--- Loss: 2.987\n",
      "Iteration: 86 \t--- Loss: 2.743\n",
      "Iteration: 87 \t--- Loss: 2.851\n",
      "Iteration: 88 \t--- Loss: 2.846\n",
      "Iteration: 89 \t--- Loss: 2.844\n",
      "Iteration: 90 \t--- Loss: 2.890\n",
      "Iteration: 91 \t--- Loss: 2.906\n",
      "Iteration: 92 \t--- Loss: 2.925\n",
      "Iteration: 93 \t--- Loss: 2.686\n",
      "Iteration: 94 \t--- Loss: 2.815\n",
      "Iteration: 95 \t--- Loss: 2.868\n",
      "Iteration: 96 \t--- Loss: 2.905\n",
      "Iteration: 97 \t--- Loss: 2.778\n",
      "Iteration: 98 \t--- Loss: 2.895\n",
      "Iteration: 99 \t--- Loss: 2.797\n",
      "Iteration: 100 \t--- Loss: 2.771\n",
      "Iteration: 101 \t--- Loss: 2.793\n",
      "Iteration: 102 \t--- Loss: 2.749\n",
      "Iteration: 103 \t--- Loss: 2.888\n",
      "Iteration: 104 \t--- Loss: 2.800\n",
      "Iteration: 105 \t--- Loss: 2.724\n",
      "Iteration: 106 \t--- Loss: 2.729\n",
      "Iteration: 107 \t--- Loss: 2.876\n",
      "Iteration: 108 \t--- Loss: 2.849\n",
      "Iteration: 109 \t--- Loss: 2.797\n",
      "Iteration: 110 \t--- Loss: 2.803\n",
      "Iteration: 111 \t--- Loss: 2.903\n",
      "Iteration: 112 \t--- Loss: 2.704\n",
      "Iteration: 113 \t--- Loss: 2.871\n",
      "Iteration: 114 \t--- Loss: 2.944\n",
      "Iteration: 115 \t--- Loss: 2.742\n",
      "Iteration: 116 \t--- Loss: 2.845\n",
      "Iteration: 117 \t--- Loss: 2.970\n",
      "Iteration: 118 \t--- Loss: 2.847\n",
      "Iteration: 119 \t--- Loss: 2.903\n",
      "Iteration: 120 \t--- Loss: 2.824\n",
      "Iteration: 121 \t--- Loss: 2.870\n",
      "Iteration: 122 \t--- Loss: 2.779\n",
      "Iteration: 123 \t--- Loss: 2.898\n",
      "Iteration: 124 \t--- Loss: 2.756\n",
      "Iteration: 125 \t--- Loss: 2.653\n",
      "Iteration: 126 \t--- Loss: 2.867\n",
      "Iteration: 127 \t--- Loss: 2.950\n",
      "Iteration: 128 \t--- Loss: 2.886\n",
      "Iteration: 129 \t--- Loss: 2.811\n",
      "Iteration: 130 \t--- Loss: 2.743\n",
      "Iteration: 131 \t--- Loss: 2.868\n",
      "Iteration: 132 \t--- Loss: 2.918\n",
      "Iteration: 133 \t--- Loss: 2.887\n",
      "Iteration: 134 \t--- Loss: 2.725\n",
      "Iteration: 135 \t--- Loss: 2.919\n",
      "Iteration: 136 \t--- Loss: 2.888\n",
      "Iteration: 137 \t--- Loss: 2.788\n",
      "Iteration: 138 \t--- Loss: 2.849\n",
      "Iteration: 139 \t--- Loss: 2.847\n",
      "Iteration: 140 \t--- Loss: 2.920\n",
      "Iteration: 141 \t--- Loss: 2.827\n",
      "Iteration: 142 \t--- Loss: 2.808\n",
      "Iteration: 143 \t--- Loss: 2.923\n",
      "Iteration: 144 \t--- Loss: 2.784\n",
      "Iteration: 145 \t--- Loss: 2.623\n",
      "Iteration: 146 \t--- Loss: 2.824\n",
      "Iteration: 147 \t--- Loss: 2.822\n",
      "Iteration: 148 \t--- Loss: 2.846\n",
      "Iteration: 149 \t--- Loss: 2.888\n",
      "Iteration: 150 \t--- Loss: 2.862\n",
      "Iteration: 151 \t--- Loss: 2.768\n",
      "Iteration: 152 \t--- Loss: 2.704\n",
      "Iteration: 153 \t--- Loss: 2.823\n",
      "Iteration: 154 \t--- Loss: 2.774\n",
      "Iteration: 155 \t--- Loss: 2.897\n",
      "Iteration: 156 \t--- Loss: 2.866\n",
      "Iteration: 157 \t--- Loss: 2.769\n",
      "Iteration: 158 \t--- Loss: 2.889\n",
      "Iteration: 159 \t--- Loss: 2.927\n",
      "Iteration: 160 \t--- Loss: 2.752\n",
      "Iteration: 161 \t--- Loss: 2.867\n",
      "Iteration: 162 \t--- Loss: 2.786\n",
      "Iteration: 163 \t--- Loss: 2.968\n",
      "Iteration: 164 \t--- Loss: 2.789\n",
      "Iteration: 165 \t--- Loss: 2.800\n",
      "Iteration: 166 \t--- Loss: 2.737\n",
      "Iteration: 167 \t--- Loss: 2.757\n",
      "Iteration: 168 \t--- Loss: 2.883\n",
      "Iteration: 169 \t--- Loss: 3.041\n",
      "Iteration: 170 \t--- Loss: 2.901\n",
      "Iteration: 171 \t--- Loss: 2.721\n",
      "Iteration: 172 \t--- Loss: 2.954\n",
      "Iteration: 173 \t--- Loss: 2.730\n",
      "Iteration: 174 \t--- Loss: 2.968\n",
      "Iteration: 175 \t--- Loss: 2.770\n",
      "Iteration: 176 \t--- Loss: 2.820\n",
      "Iteration: 177 \t--- Loss: 2.727\n",
      "Iteration: 178 \t--- Loss: 2.873\n",
      "Iteration: 179 \t--- Loss: 2.950\n",
      "Iteration: 180 \t--- Loss: 2.894\n",
      "Iteration: 181 \t--- Loss: 2.796\n",
      "Iteration: 182 \t--- Loss: 2.858\n",
      "Iteration: 183 \t--- Loss: 2.771\n",
      "Iteration: 184 \t--- Loss: 2.779\n",
      "Iteration: 185 \t--- Loss: 2.705\n",
      "Iteration: 186 \t--- Loss: 2.809\n",
      "Iteration: 187 \t--- Loss: 2.833\n",
      "Iteration: 188 \t--- Loss: 2.865\n",
      "Iteration: 189 \t--- Loss: 2.717\n",
      "Iteration: 190 \t--- Loss: 2.751\n",
      "Iteration: 191 \t--- Loss: 2.857\n",
      "Iteration: 192 \t--- Loss: 2.859\n",
      "Iteration: 193 \t--- Loss: 2.904\n",
      "Iteration: 194 \t--- Loss: 2.947\n",
      "Iteration: 195 \t--- Loss: 2.898\n",
      "Iteration: 196 \t--- Loss: 2.831\n",
      "Iteration: 197 \t--- Loss: 2.688\n",
      "Iteration: 198 \t--- Loss: 2.863\n",
      "Iteration: 199 \t--- Loss: 2.964\n",
      "Iteration: 200 \t--- Loss: 2.897\n",
      "Iteration: 201 \t--- Loss: 2.729\n",
      "Iteration: 202 \t--- Loss: 2.897\n",
      "Iteration: 203 \t--- Loss: 2.820\n",
      "Iteration: 204 \t--- Loss: 2.797\n",
      "Iteration: 205 \t--- Loss: 2.766\n",
      "Iteration: 206 \t--- Loss: 2.956\n",
      "Iteration: 207 \t--- Loss: 2.856\n",
      "Iteration: 208 \t--- Loss: 2.847\n",
      "Iteration: 209 \t--- Loss: 2.807\n",
      "Iteration: 210 \t--- Loss: 2.928\n",
      "Iteration: 211 \t--- Loss: 2.783\n",
      "Iteration: 212 \t--- Loss: 2.789\n",
      "Iteration: 213 \t--- Loss: 2.829\n",
      "Iteration: 214 \t--- Loss: 2.959\n",
      "Iteration: 215 \t--- Loss: 2.857\n",
      "Iteration: 216 \t--- Loss: 2.942\n",
      "Iteration: 217 \t--- Loss: 2.844\n",
      "Iteration: 218 \t--- Loss: 2.884\n",
      "Iteration: 219 \t--- Loss: 2.808\n",
      "Iteration: 220 \t--- Loss: 2.769\n",
      "Iteration: 221 \t--- Loss: 2.921\n",
      "Iteration: 222 \t--- Loss: 3.116\n",
      "Iteration: 223 \t--- Loss: 2.675\n",
      "Iteration: 224 \t--- Loss: 2.908\n",
      "Iteration: 225 \t--- Loss: 2.844\n",
      "Iteration: 226 \t--- Loss: 2.828\n",
      "Iteration: 227 \t--- Loss: 2.862\n",
      "Iteration: 228 \t--- Loss: 2.924\n",
      "Iteration: 229 \t--- Loss: 2.757\n",
      "Iteration: 230 \t--- Loss: 2.799\n",
      "Iteration: 231 \t--- Loss: 2.851\n",
      "Iteration: 232 \t--- Loss: 2.754\n",
      "Iteration: 233 \t--- Loss: 2.855\n",
      "Iteration: 234 \t--- Loss: 2.894\n",
      "Iteration: 235 \t--- Loss: 2.711\n",
      "Iteration: 236 \t--- Loss: 2.812\n",
      "Iteration: 237 \t--- Loss: 2.868\n",
      "Iteration: 238 \t--- Loss: 2.898\n",
      "Iteration: 239 \t--- Loss: 2.894\n",
      "Iteration: 240 \t--- Loss: 2.864\n",
      "Iteration: 241 \t--- Loss: 2.877\n",
      "Iteration: 242 \t--- Loss: 2.916\n",
      "Iteration: 243 \t--- Loss: 2.909\n",
      "Iteration: 244 \t--- Loss: 2.829\n",
      "Iteration: 245 \t--- Loss: 2.909\n",
      "Iteration: 246 \t--- Loss: 2.910\n",
      "Iteration: 247 \t--- Loss: 2.762\n",
      "Iteration: 248 \t--- Loss: 2.759\n",
      "Iteration: 249 \t--- Loss: 2.889\n",
      "Iteration: 250 \t--- Loss: 2.794\n",
      "Iteration: 251 \t--- Loss: 2.828\n",
      "Iteration: 252 \t--- Loss: 2.819\n",
      "Iteration: 253 \t--- Loss: 3.006\n",
      "Iteration: 254 \t--- Loss: 2.827\n",
      "Iteration: 255 \t--- Loss: 2.756\n",
      "Iteration: 256 \t--- Loss: 2.930\n",
      "Iteration: 257 \t--- Loss: 2.871\n",
      "Iteration: 258 \t--- Loss: 2.866\n",
      "Iteration: 259 \t--- Loss: 2.829Iteration: 0 \t--- Loss: 0.600\n",
      "Iteration: 1 \t--- Loss: 0.551\n",
      "Iteration: 2 \t--- Loss: 0.525\n",
      "Iteration: 3 \t--- Loss: 0.475\n",
      "Iteration: 4 \t--- Loss: 0.436\n",
      "Iteration: 5 \t--- Loss: 0.403\n",
      "Iteration: 6 \t--- Loss: 0.388\n",
      "Iteration: 7 \t--- Loss: 0.360\n",
      "Iteration: 8 \t--- Loss: 0.348\n",
      "Iteration: 9 \t--- Loss: 0.339\n",
      "Iteration: 10 \t--- Loss: 0.324\n",
      "Iteration: 11 \t--- Loss: 0.317\n",
      "Iteration: 12 \t--- Loss: 0.311\n",
      "Iteration: 13 \t--- Loss: 0.303\n",
      "Iteration: 14 \t--- Loss: 0.297\n",
      "Iteration: 15 \t--- Loss: 0.293\n",
      "Iteration: 16 \t--- Loss: 0.291\n",
      "Iteration: 17 \t--- Loss: 0.286\n",
      "Iteration: 18 \t--- Loss: 0.283\n",
      "Iteration: 19 \t--- Loss: 0.280\n",
      "Iteration: 20 \t--- Loss: 0.278\n",
      "Iteration: 21 \t--- Loss: 0.276\n",
      "Iteration: 22 \t--- Loss: 0.275\n",
      "Iteration: 23 \t--- Loss: 0.273\n",
      "Iteration: 24 \t--- Loss: 0.272\n",
      "Iteration: 25 \t--- Loss: 0.270\n",
      "Iteration: 26 \t--- Loss: 0.269\n",
      "Iteration: 27 \t--- Loss: 0.268\n",
      "Iteration: 28 \t--- Loss: 0.267\n",
      "Iteration: 29 \t--- Loss: 0.266\n",
      "Iteration: 30 \t--- Loss: 0.266\n",
      "Iteration: 31 \t--- Loss: 0.264\n",
      "Iteration: 32 \t--- Loss: 0.264\n",
      "Iteration: 33 \t--- Loss: 0.263\n",
      "Iteration: 34 \t--- Loss: 0.263\n",
      "Iteration: 35 \t--- Loss: 0.262\n",
      "Iteration: 36 \t--- Loss: 0.263\n",
      "Iteration: 37 \t--- Loss: 0.263\n",
      "Iteration: 38 \t--- Loss: 0.262\n",
      "Iteration: 39 \t--- Loss: 0.263\n",
      "Iteration: 40 \t--- Loss: 0.262\n",
      "Iteration: 41 \t--- Loss: 0.260\n",
      "Iteration: 42 \t--- Loss: 0.261\n",
      "Iteration: 43 \t--- Loss: 0.260\n",
      "Iteration: 44 \t--- Loss: 0.261\n",
      "Iteration: 45 \t--- Loss: 0.261\n",
      "Iteration: 46 \t--- Loss: 0.260\n",
      "Iteration: 47 \t--- Loss: 0.260\n",
      "Iteration: 48 \t--- Loss: 0.260\n",
      "Iteration: 49 \t--- Loss: 0.261\n",
      "Iteration: 50 \t--- Loss: 0.260\n",
      "Iteration: 51 \t--- Loss: 0.259\n",
      "Iteration: 52 \t--- Loss: 0.259\n",
      "Iteration: 53 \t--- Loss: 0.260\n",
      "Iteration: 54 \t--- Loss: 0.259\n",
      "Iteration: 55 \t--- Loss: 0.260\n",
      "Iteration: 56 \t--- Loss: 0.259\n",
      "Iteration: 57 \t--- Loss: 0.259\n",
      "Iteration: 58 \t--- Loss: 0.260\n",
      "Iteration: 59 \t--- Loss: 0.260\n",
      "Iteration: 60 \t--- Loss: 0.258\n",
      "Iteration: 61 \t--- Loss: 0.260\n",
      "Iteration: 62 \t--- Loss: 0.259\n",
      "Iteration: 63 \t--- Loss: 0.258\n",
      "Iteration: 64 \t--- Loss: 0.259\n",
      "Iteration: 65 \t--- Loss: 0.259\n",
      "Iteration: 66 \t--- Loss: 0.258\n",
      "Iteration: 67 \t--- Loss: 0.257\n",
      "Iteration: 68 \t--- Loss: 0.259\n",
      "Iteration: 69 \t--- Loss: 0.258\n",
      "Iteration: 70 \t--- Loss: 0.258\n",
      "Iteration: 71 \t--- Loss: 0.257\n",
      "Iteration: 72 \t--- Loss: 0.258\n",
      "Iteration: 73 \t--- Loss: 0.258\n",
      "Iteration: 74 \t--- Loss: 0.259\n",
      "Iteration: 75 \t--- Loss: 0.258\n",
      "Iteration: 76 \t--- Loss: 0.258\n",
      "Iteration: 77 \t--- Loss: 0.258\n",
      "Iteration: 78 \t--- Loss: 0.259\n",
      "Iteration: 79 \t--- Loss: 0.258\n",
      "Iteration: 80 \t--- Loss: 0.258\n",
      "Iteration: 81 \t--- Loss: 0.258\n",
      "Iteration: 82 \t--- Loss: 0.260\n",
      "Iteration: 83 \t--- Loss: 0.259\n",
      "Iteration: 84 \t--- Loss: 0.258\n",
      "Iteration: 85 \t--- Loss: 0.259\n",
      "Iteration: 86 \t--- Loss: 0.259\n",
      "Iteration: 87 \t--- Loss: 0.258\n",
      "Iteration: 88 \t--- Loss: 0.259\n",
      "Iteration: 89 \t--- Loss: 0.259\n",
      "Iteration: 90 \t--- Loss: 0.257\n",
      "Iteration: 91 \t--- Loss: 0.259\n",
      "Iteration: 92 \t--- Loss: 0.258\n",
      "Iteration: 93 \t--- Loss: 0.257\n",
      "Iteration: 94 \t--- Loss: 0.259\n",
      "Iteration: 95 \t--- Loss: 0.257\n",
      "Iteration: 96 \t--- Loss: 0.258\n",
      "Iteration: 97 \t--- Loss: 0.258\n",
      "Iteration: 98 \t--- Loss: 0.256\n",
      "Iteration: 99 \t--- Loss: 0.257\n",
      "Iteration: 100 \t--- Loss: 0.257\n",
      "Iteration: 101 \t--- Loss: 0.258\n",
      "Iteration: 102 \t--- Loss: 0.258\n",
      "Iteration: 103 \t--- Loss: 0.259\n",
      "Iteration: 104 \t--- Loss: 0.259\n",
      "Iteration: 105 \t--- Loss: 0.258\n",
      "Iteration: 106 \t--- Loss: 0.258\n",
      "Iteration: 107 \t--- Loss: 0.258\n",
      "Iteration: 108 \t--- Loss: 0.258\n",
      "Iteration: 109 \t--- Loss: 0.257\n",
      "Iteration: 110 \t--- Loss: 0.258\n",
      "Iteration: 111 \t--- Loss: 0.258\n",
      "Iteration: 112 \t--- Loss: 0.257\n",
      "Iteration: 113 \t--- Loss: 0.259\n",
      "Iteration: 114 \t--- Loss: 0.258\n",
      "Iteration: 115 \t--- Loss: 0.258\n",
      "Iteration: 116 \t--- Loss: 0.258\n",
      "Iteration: 117 \t--- Loss: 0.256\n",
      "Iteration: 118 \t--- Loss: 0.259\n",
      "Iteration: 119 \t--- Loss: 0.258\n",
      "Iteration: 120 \t--- Loss: 0.257\n",
      "Iteration: 121 \t--- Loss: 0.259\n",
      "Iteration: 122 \t--- Loss: 0.258\n",
      "Iteration: 123 \t--- Loss: 0.257\n",
      "Iteration: 124 \t--- Loss: 0.258\n",
      "Iteration: 125 \t--- Loss: 0.257\n",
      "Iteration: 126 \t--- Loss: 0.260\n",
      "Iteration: 127 \t--- Loss: 0.258\n",
      "Iteration: 128 \t--- Loss: 0.259\n",
      "Iteration: 129 \t--- Loss: 0.258\n",
      "Iteration: 130 \t--- Loss: 0.258\n",
      "Iteration: 131 \t--- Loss: 0.259\n",
      "Iteration: 132 \t--- Loss: 0.258\n",
      "Iteration: 133 \t--- Loss: 0.256\n",
      "Iteration: 134 \t--- Loss: 0.257\n",
      "Iteration: 135 \t--- Loss: 0.258\n",
      "Iteration: 136 \t--- Loss: 0.259\n",
      "Iteration: 137 \t--- Loss: 0.257\n",
      "Iteration: 138 \t--- Loss: 0.259\n",
      "Iteration: 139 \t--- Loss: 0.257\n",
      "Iteration: 140 \t--- Loss: 0.257\n",
      "Iteration: 141 \t--- Loss: 0.258\n",
      "Iteration: 142 \t--- Loss: 0.259\n",
      "Iteration: 143 \t--- Loss: 0.257\n",
      "Iteration: 144 \t--- Loss: 0.258\n",
      "Iteration: 145 \t--- Loss: 0.258\n",
      "Iteration: 146 \t--- Loss: 0.259\n",
      "Iteration: 147 \t--- Loss: 0.257\n",
      "Iteration: 148 \t--- Loss: 0.258\n",
      "Iteration: 149 \t--- Loss: 0.258\n",
      "Iteration: 150 \t--- Loss: 0.258\n",
      "Iteration: 151 \t--- Loss: 0.257\n",
      "Iteration: 152 \t--- Loss: 0.258\n",
      "Iteration: 153 \t--- Loss: 0.257\n",
      "Iteration: 154 \t--- Loss: 0.257\n",
      "Iteration: 155 \t--- Loss: 0.257\n",
      "Iteration: 156 \t--- Loss: 0.259\n",
      "Iteration: 157 \t--- Loss: 0.259\n",
      "Iteration: 158 \t--- Loss: 0.259\n",
      "Iteration: 159 \t--- Loss: 0.257\n",
      "Iteration: 160 \t--- Loss: 0.257\n",
      "Iteration: 161 \t--- Loss: 0.258\n",
      "Iteration: 162 \t--- Loss: 0.258\n",
      "Iteration: 163 \t--- Loss: 0.257\n",
      "Iteration: 164 \t--- Loss: 0.257\n",
      "Iteration: 165 \t--- Loss: 0.258\n",
      "Iteration: 166 \t--- Loss: 0.259\n",
      "Iteration: 167 \t--- Loss: 0.258\n",
      "Iteration: 168 \t--- Loss: 0.259\n",
      "Iteration: 169 \t--- Loss: 0.258\n",
      "Iteration: 170 \t--- Loss: 0.257\n",
      "Iteration: 171 \t--- Loss: 0.258\n",
      "Iteration: 172 \t--- Loss: 0.258\n",
      "Iteration: 173 \t--- Loss: 0.258\n",
      "Iteration: 174 \t--- Loss: 0.259\n",
      "Iteration: 175 \t--- Loss: 0.259\n",
      "Iteration: 176 \t--- Loss: 0.257\n",
      "Iteration: 177 \t--- Loss: 0.258\n",
      "Iteration: 178 \t--- Loss: 0.258\n",
      "Iteration: 179 \t--- Loss: 0.257\n",
      "Iteration: 180 \t--- Loss: 0.258\n",
      "Iteration: 181 \t--- Loss: 0.259\n",
      "Iteration: 182 \t--- Loss: 0.257\n",
      "Iteration: 183 \t--- Loss: 0.258\n",
      "Iteration: 184 \t--- Loss: 0.257\n",
      "Iteration: 185 \t--- Loss: 0.257\n",
      "Iteration: 186 \t--- Loss: 0.259\n",
      "Iteration: 187 \t--- Loss: 0.258\n",
      "Iteration: 188 \t--- Loss: 0.258\n",
      "Iteration: 189 \t--- Loss: 0.258\n",
      "Iteration: 190 \t--- Loss: 0.258\n",
      "Iteration: 191 \t--- Loss: 0.256\n",
      "Iteration: 192 \t--- Loss: 0.258\n",
      "Iteration: 193 \t--- Loss: 0.258\n",
      "Iteration: 194 \t--- Loss: 0.257\n",
      "Iteration: 195 \t--- Loss: 0.258\n",
      "Iteration: 196 \t--- Loss: 0.257\n",
      "Iteration: 197 \t--- Loss: 0.258\n",
      "Iteration: 198 \t--- Loss: 0.258\n",
      "Iteration: 199 \t--- Loss: 0.258\n",
      "Iteration: 200 \t--- Loss: 0.258\n",
      "Iteration: 201 \t--- Loss: 0.259\n",
      "Iteration: 202 \t--- Loss: 0.257\n",
      "Iteration: 203 \t--- Loss: 0.258\n",
      "Iteration: 204 \t--- Loss: 0.257\n",
      "Iteration: 205 \t--- Loss: 0.257\n",
      "Iteration: 206 \t--- Loss: 0.258\n",
      "Iteration: 207 \t--- Loss: 0.258\n",
      "Iteration: 208 \t--- Loss: 0.258\n",
      "Iteration: 209 \t--- Loss: 0.259\n",
      "Iteration: 210 \t--- Loss: 0.258\n",
      "Iteration: 211 \t--- Loss: 0.258\n",
      "Iteration: 212 \t--- Loss: 0.258\n",
      "Iteration: 213 \t--- Loss: 0.257\n",
      "Iteration: 214 \t--- Loss: 0.258\n",
      "Iteration: 215 \t--- Loss: 0.258\n",
      "Iteration: 216 \t--- Loss: 0.258\n",
      "Iteration: 217 \t--- Loss: 0.258\n",
      "Iteration: 218 \t--- Loss: 0.257\n",
      "Iteration: 219 \t--- Loss: 0.259\n",
      "Iteration: 220 \t--- Loss: 0.257\n",
      "Iteration: 221 \t--- Loss: 0.258\n",
      "Iteration: 222 \t--- Loss: 0.258\n",
      "Iteration: 223 \t--- Loss: 0.258\n",
      "Iteration: 224 \t--- Loss: 0.258\n",
      "Iteration: 225 \t--- Loss: 0.259\n",
      "Iteration: 226 \t--- Loss: 0.258\n",
      "Iteration: 227 \t--- Loss: 0.259\n",
      "Iteration: 228 \t--- Loss: 0.258\n",
      "Iteration: 229 \t--- Loss: 0.257\n",
      "Iteration: 230 \t--- Loss: 0.257\n",
      "Iteration: 231 \t--- Loss: 0.257\n",
      "Iteration: 232 \t--- Loss: 0.259\n",
      "Iteration: 233 \t--- Loss: 0.258\n",
      "Iteration: 234 \t--- Loss: 0.258\n",
      "Iteration: 235 \t--- Loss: 0.257\n",
      "Iteration: 236 \t--- Loss: 0.258\n",
      "Iteration: 237 \t--- Loss: 0.258\n",
      "Iteration: 238 \t--- Loss: 0.257\n",
      "Iteration: 239 \t--- Loss: 0.257\n",
      "Iteration: 240 \t--- Loss: 0.258\n",
      "Iteration: 241 \t--- Loss: 0.258\n",
      "Iteration: 242 \t--- Loss: 0.257\n",
      "Iteration: 243 \t--- Loss: 0.258\n",
      "Iteration: 244 \t--- Loss: 0.259\n",
      "Iteration: 245 \t--- Loss: 0.257\n",
      "Iteration: 246 \t--- Loss: 0.257\n",
      "Iteration: 247 \t--- Loss: 0.257\n",
      "Iteration: 248 \t--- Loss: 0.256\n",
      "Iteration: 249 \t--- Loss: 0.257\n",
      "Iteration: 250 \t--- Loss: 0.259\n",
      "Iteration: 251 \t--- Loss: 0.257\n",
      "Iteration: 252 \t--- Loss: 0.259\n",
      "Iteration: 253 \t--- Loss: 0.258\n",
      "Iteration: 254 \t--- Loss: 0.258\n",
      "Iteration: 255 \t--- Loss: 0.258\n",
      "Iteration: 256 \t--- Loss: 0.258\n",
      "Iteration: 257 \t--- Loss: 0.259\n",
      "Iteration: 258 \t--- Loss: 0.258\n",
      "Iteration: 259 \t--- Loss: 0.258Iteration: 0 \t--- Loss: 0.875\n",
      "Iteration: 1 \t--- Loss: 0.864\n",
      "Iteration: 2 \t--- Loss: 0.857\n",
      "Iteration: 3 \t--- Loss: 0.697\n",
      "Iteration: 4 \t--- Loss: 0.688\n",
      "Iteration: 5 \t--- Loss: 0.661\n",
      "Iteration: 6 \t--- Loss: 0.555\n",
      "Iteration: 7 \t--- Loss: 0.586\n",
      "Iteration: 8 \t--- Loss: 0.559\n",
      "Iteration: 9 \t--- Loss: 0.557\n",
      "Iteration: 10 \t--- Loss: 0.559\n",
      "Iteration: 11 \t--- Loss: 0.553\n",
      "Iteration: 12 \t--- Loss: 0.515\n",
      "Iteration: 13 \t--- Loss: 0.508\n",
      "Iteration: 14 \t--- Loss: 0.512\n",
      "Iteration: 15 \t--- Loss: 0.482\n",
      "Iteration: 16 \t--- Loss: 0.518\n",
      "Iteration: 17 \t--- Loss: 0.479\n",
      "Iteration: 18 \t--- Loss: 0.497\n",
      "Iteration: 19 \t--- Loss: 0.511\n",
      "Iteration: 20 \t--- Loss: 0.513\n",
      "Iteration: 21 \t--- Loss: 0.489\n",
      "Iteration: 22 \t--- Loss: 0.506\n",
      "Iteration: 23 \t--- Loss: 0.502\n",
      "Iteration: 24 \t--- Loss: 0.472\n",
      "Iteration: 25 \t--- Loss: 0.501\n",
      "Iteration: 26 \t--- Loss: 0.475\n",
      "Iteration: 27 \t--- Loss: 0.499\n",
      "Iteration: 28 \t--- Loss: 0.478\n",
      "Iteration: 29 \t--- Loss: 0.493\n",
      "Iteration: 30 \t--- Loss: 0.477\n",
      "Iteration: 31 \t--- Loss: 0.476\n",
      "Iteration: 32 \t--- Loss: 0.499\n",
      "Iteration: 33 \t--- Loss: 0.501\n",
      "Iteration: 34 \t--- Loss: 0.493\n",
      "Iteration: 35 \t--- Loss: 0.486\n",
      "Iteration: 36 \t--- Loss: 0.480\n",
      "Iteration: 37 \t--- Loss: 0.502\n",
      "Iteration: 38 \t--- Loss: 0.493\n",
      "Iteration: 39 \t--- Loss: 0.486\n",
      "Iteration: 40 \t--- Loss: 0.471\n",
      "Iteration: 41 \t--- Loss: 0.486\n",
      "Iteration: 42 \t--- Loss: 0.494\n",
      "Iteration: 43 \t--- Loss: 0.484\n",
      "Iteration: 44 \t--- Loss: 0.490\n",
      "Iteration: 45 \t--- Loss: 0.506\n",
      "Iteration: 46 \t--- Loss: 0.475\n",
      "Iteration: 47 \t--- Loss: 0.502\n",
      "Iteration: 48 \t--- Loss: 0.468\n",
      "Iteration: 49 \t--- Loss: 0.484\n",
      "Iteration: 50 \t--- Loss: 0.469\n",
      "Iteration: 51 \t--- Loss: 0.466\n",
      "Iteration: 52 \t--- Loss: 0.487\n",
      "Iteration: 53 \t--- Loss: 0.490\n",
      "Iteration: 54 \t--- Loss: 0.482\n",
      "Iteration: 55 \t--- Loss: 0.481\n",
      "Iteration: 56 \t--- Loss: 0.485\n",
      "Iteration: 57 \t--- Loss: 0.463\n",
      "Iteration: 58 \t--- Loss: 0.481\n",
      "Iteration: 59 \t--- Loss: 0.476\n",
      "Iteration: 60 \t--- Loss: 0.489\n",
      "Iteration: 61 \t--- Loss: 0.482\n",
      "Iteration: 62 \t--- Loss: 0.480\n",
      "Iteration: 63 \t--- Loss: 0.476\n",
      "Iteration: 64 \t--- Loss: 0.479\n",
      "Iteration: 65 \t--- Loss: 0.473\n",
      "Iteration: 66 \t--- Loss: 0.469\n",
      "Iteration: 67 \t--- Loss: 0.472\n",
      "Iteration: 68 \t--- Loss: 0.470\n",
      "Iteration: 69 \t--- Loss: 0.474\n",
      "Iteration: 70 \t--- Loss: 0.495\n",
      "Iteration: 71 \t--- Loss: 0.506\n",
      "Iteration: 72 \t--- Loss: 0.491\n",
      "Iteration: 73 \t--- Loss: 0.476\n",
      "Iteration: 74 \t--- Loss: 0.483\n",
      "Iteration: 75 \t--- Loss: 0.485\n",
      "Iteration: 76 \t--- Loss: 0.480\n",
      "Iteration: 77 \t--- Loss: 0.485\n",
      "Iteration: 78 \t--- Loss: 0.473\n",
      "Iteration: 79 \t--- Loss: 0.496\n",
      "Iteration: 80 \t--- Loss: 0.471\n",
      "Iteration: 81 \t--- Loss: 0.480\n",
      "Iteration: 82 \t--- Loss: 0.470\n",
      "Iteration: 83 \t--- Loss: 0.469\n",
      "Iteration: 84 \t--- Loss: 0.486\n",
      "Iteration: 85 \t--- Loss: 0.488\n",
      "Iteration: 86 \t--- Loss: 0.481\n",
      "Iteration: 87 \t--- Loss: 0.479\n",
      "Iteration: 88 \t--- Loss: 0.507\n",
      "Iteration: 89 \t--- Loss: 0.489\n",
      "Iteration: 90 \t--- Loss: 0.465\n",
      "Iteration: 91 \t--- Loss: 0.480\n",
      "Iteration: 92 \t--- Loss: 0.476\n",
      "Iteration: 93 \t--- Loss: 0.496\n",
      "Iteration: 94 \t--- Loss: 0.489\n",
      "Iteration: 95 \t--- Loss: 0.479\n",
      "Iteration: 96 \t--- Loss: 0.476\n",
      "Iteration: 97 \t--- Loss: 0.482\n",
      "Iteration: 98 \t--- Loss: 0.488\n",
      "Iteration: 99 \t--- Loss: 0.467\n",
      "Iteration: 100 \t--- Loss: 0.497\n",
      "Iteration: 101 \t--- Loss: 0.475\n",
      "Iteration: 102 \t--- Loss: 0.473\n",
      "Iteration: 103 \t--- Loss: 0.477\n",
      "Iteration: 104 \t--- Loss: 0.488\n",
      "Iteration: 105 \t--- Loss: 0.476\n",
      "Iteration: 106 \t--- Loss: 0.485\n",
      "Iteration: 107 \t--- Loss: 0.478\n",
      "Iteration: 108 \t--- Loss: 0.483\n",
      "Iteration: 109 \t--- Loss: 0.489\n",
      "Iteration: 110 \t--- Loss: 0.466\n",
      "Iteration: 111 \t--- Loss: 0.488\n",
      "Iteration: 112 \t--- Loss: 0.490\n",
      "Iteration: 113 \t--- Loss: 0.481\n",
      "Iteration: 114 \t--- Loss: 0.485\n",
      "Iteration: 115 \t--- Loss: 0.499\n",
      "Iteration: 116 \t--- Loss: 0.474\n",
      "Iteration: 117 \t--- Loss: 0.467\n",
      "Iteration: 118 \t--- Loss: 0.474\n",
      "Iteration: 119 \t--- Loss: 0.488\n",
      "Iteration: 120 \t--- Loss: 0.497\n",
      "Iteration: 121 \t--- Loss: 0.476\n",
      "Iteration: 122 \t--- Loss: 0.485\n",
      "Iteration: 123 \t--- Loss: 0.488\n",
      "Iteration: 124 \t--- Loss: 0.477\n",
      "Iteration: 125 \t--- Loss: 0.479\n",
      "Iteration: 126 \t--- Loss: 0.472\n",
      "Iteration: 127 \t--- Loss: 0.481\n",
      "Iteration: 128 \t--- Loss: 0.488\n",
      "Iteration: 129 \t--- Loss: 0.478\n",
      "Iteration: 130 \t--- Loss: 0.476\n",
      "Iteration: 131 \t--- Loss: 0.469\n",
      "Iteration: 132 \t--- Loss: 0.490\n",
      "Iteration: 133 \t--- Loss: 0.468\n",
      "Iteration: 134 \t--- Loss: 0.478\n",
      "Iteration: 135 \t--- Loss: 0.477\n",
      "Iteration: 136 \t--- Loss: 0.502\n",
      "Iteration: 137 \t--- Loss: 0.471\n",
      "Iteration: 138 \t--- Loss: 0.477\n",
      "Iteration: 139 \t--- Loss: 0.483\n",
      "Iteration: 140 \t--- Loss: 0.481\n",
      "Iteration: 141 \t--- Loss: 0.504\n",
      "Iteration: 142 \t--- Loss: 0.476\n",
      "Iteration: 143 \t--- Loss: 0.485\n",
      "Iteration: 144 \t--- Loss: 0.476\n",
      "Iteration: 145 \t--- Loss: 0.479\n",
      "Iteration: 146 \t--- Loss: 0.486\n",
      "Iteration: 147 \t--- Loss: 0.476\n",
      "Iteration: 148 \t--- Loss: 0.483\n",
      "Iteration: 149 \t--- Loss: 0.466\n",
      "Iteration: 150 \t--- Loss: 0.487\n",
      "Iteration: 151 \t--- Loss: 0.493\n",
      "Iteration: 152 \t--- Loss: 0.499\n",
      "Iteration: 153 \t--- Loss: 0.498\n",
      "Iteration: 154 \t--- Loss: 0.496\n",
      "Iteration: 155 \t--- Loss: 0.464\n",
      "Iteration: 156 \t--- Loss: 0.469\n",
      "Iteration: 157 \t--- Loss: 0.509\n",
      "Iteration: 158 \t--- Loss: 0.482\n",
      "Iteration: 159 \t--- Loss: 0.473\n",
      "Iteration: 160 \t--- Loss: 0.492\n",
      "Iteration: 161 \t--- Loss: 0.485\n",
      "Iteration: 162 \t--- Loss: 0.464\n",
      "Iteration: 163 \t--- Loss: 0.467\n",
      "Iteration: 164 \t--- Loss: 0.495\n",
      "Iteration: 165 \t--- Loss: 0.482\n",
      "Iteration: 166 \t--- Loss: 0.498\n",
      "Iteration: 167 \t--- Loss: 0.473\n",
      "Iteration: 168 \t--- Loss: 0.488\n",
      "Iteration: 169 \t--- Loss: 0.480\n",
      "Iteration: 170 \t--- Loss: 0.489\n",
      "Iteration: 171 \t--- Loss: 0.487\n",
      "Iteration: 172 \t--- Loss: 0.493\n",
      "Iteration: 173 \t--- Loss: 0.474\n",
      "Iteration: 174 \t--- Loss: 0.487\n",
      "Iteration: 175 \t--- Loss: 0.494\n",
      "Iteration: 176 \t--- Loss: 0.507\n",
      "Iteration: 177 \t--- Loss: 0.467\n",
      "Iteration: 178 \t--- Loss: 0.477\n",
      "Iteration: 179 \t--- Loss: 0.478\n",
      "Iteration: 180 \t--- Loss: 0.479\n",
      "Iteration: 181 \t--- Loss: 0.485\n",
      "Iteration: 182 \t--- Loss: 0.493\n",
      "Iteration: 183 \t--- Loss: 0.475\n",
      "Iteration: 184 \t--- Loss: 0.469\n",
      "Iteration: 185 \t--- Loss: 0.486\n",
      "Iteration: 186 \t--- Loss: 0.472\n",
      "Iteration: 187 \t--- Loss: 0.494\n",
      "Iteration: 188 \t--- Loss: 0.471\n",
      "Iteration: 189 \t--- Loss: 0.473\n",
      "Iteration: 190 \t--- Loss: 0.475\n",
      "Iteration: 191 \t--- Loss: 0.478\n",
      "Iteration: 192 \t--- Loss: 0.500\n",
      "Iteration: 193 \t--- Loss: 0.468\n",
      "Iteration: 194 \t--- Loss: 0.471\n",
      "Iteration: 195 \t--- Loss: 0.471\n",
      "Iteration: 196 \t--- Loss: 0.495\n",
      "Iteration: 197 \t--- Loss: 0.492\n",
      "Iteration: 198 \t--- Loss: 0.493\n",
      "Iteration: 199 \t--- Loss: 0.468\n",
      "Iteration: 200 \t--- Loss: 0.472\n",
      "Iteration: 201 \t--- Loss: 0.488\n",
      "Iteration: 202 \t--- Loss: 0.472\n",
      "Iteration: 203 \t--- Loss: 0.480\n",
      "Iteration: 204 \t--- Loss: 0.487\n",
      "Iteration: 205 \t--- Loss: 0.470\n",
      "Iteration: 206 \t--- Loss: 0.476\n",
      "Iteration: 207 \t--- Loss: 0.497\n",
      "Iteration: 208 \t--- Loss: 0.464\n",
      "Iteration: 209 \t--- Loss: 0.486\n",
      "Iteration: 210 \t--- Loss: 0.472\n",
      "Iteration: 211 \t--- Loss: 0.474\n",
      "Iteration: 212 \t--- Loss: 0.488\n",
      "Iteration: 213 \t--- Loss: 0.487\n",
      "Iteration: 214 \t--- Loss: 0.499\n",
      "Iteration: 215 \t--- Loss: 0.483\n",
      "Iteration: 216 \t--- Loss: 0.474\n",
      "Iteration: 217 \t--- Loss: 0.492\n",
      "Iteration: 218 \t--- Loss: 0.476\n",
      "Iteration: 219 \t--- Loss: 0.483\n",
      "Iteration: 220 \t--- Loss: 0.471\n",
      "Iteration: 221 \t--- Loss: 0.473\n",
      "Iteration: 222 \t--- Loss: 0.481\n",
      "Iteration: 223 \t--- Loss: 0.494\n",
      "Iteration: 224 \t--- Loss: 0.465\n",
      "Iteration: 225 \t--- Loss: 0.484\n",
      "Iteration: 226 \t--- Loss: 0.473\n",
      "Iteration: 227 \t--- Loss: 0.474\n",
      "Iteration: 228 \t--- Loss: 0.472\n",
      "Iteration: 229 \t--- Loss: 0.481\n",
      "Iteration: 230 \t--- Loss: 0.471\n",
      "Iteration: 231 \t--- Loss: 0.484\n",
      "Iteration: 232 \t--- Loss: 0.479\n",
      "Iteration: 233 \t--- Loss: 0.478\n",
      "Iteration: 234 \t--- Loss: 0.502\n",
      "Iteration: 235 \t--- Loss: 0.485\n",
      "Iteration: 236 \t--- Loss: 0.487\n",
      "Iteration: 237 \t--- Loss: 0.491\n",
      "Iteration: 238 \t--- Loss: 0.476\n",
      "Iteration: 239 \t--- Loss: 0.473\n",
      "Iteration: 240 \t--- Loss: 0.490\n",
      "Iteration: 241 \t--- Loss: 0.479\n",
      "Iteration: 242 \t--- Loss: 0.476\n",
      "Iteration: 243 \t--- Loss: 0.462\n",
      "Iteration: 244 \t--- Loss: 0.483\n",
      "Iteration: 245 \t--- Loss: 0.475\n",
      "Iteration: 246 \t--- Loss: 0.476\n",
      "Iteration: 247 \t--- Loss: 0.469\n",
      "Iteration: 248 \t--- Loss: 0.490\n",
      "Iteration: 249 \t--- Loss: 0.485\n",
      "Iteration: 250 \t--- Loss: 0.471\n",
      "Iteration: 251 \t--- Loss: 0.497\n",
      "Iteration: 252 \t--- Loss: 0.470\n",
      "Iteration: 253 \t--- Loss: 0.482\n",
      "Iteration: 254 \t--- Loss: 0.469\n",
      "Iteration: 255 \t--- Loss: 0.478\n",
      "Iteration: 256 \t--- Loss: 0.475\n",
      "Iteration: 257 \t--- Loss: 0.480\n",
      "Iteration: 258 \t--- Loss: 0.499\n",
      "Iteration: 259 \t--- Loss: 0.477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:27<00:00, 87.07s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 2.945\n",
      "Iteration: 261 \t--- Loss: 2.712\n",
      "Iteration: 262 \t--- Loss: 2.691\n",
      "Iteration: 263 \t--- Loss: 2.734\n",
      "Iteration: 264 \t--- Loss: 2.886\n",
      "Iteration: 265 \t--- Loss: 2.759\n",
      "Iteration: 266 \t--- Loss: 2.780\n",
      "Iteration: 267 \t--- Loss: 2.730\n",
      "Iteration: 268 \t--- Loss: 2.679\n",
      "Iteration: 269 \t--- Loss: 2.754\n",
      "Iteration: 270 \t--- Loss: 2.822\n",
      "Iteration: 271 \t--- Loss: 2.828\n",
      "Iteration: 272 \t--- Loss: 2.988\n",
      "Iteration: 273 \t--- Loss: 2.876\n",
      "Iteration: 274 \t--- Loss: 2.793\n",
      "Iteration: 275 \t--- Loss: 2.739\n",
      "Iteration: 276 \t--- Loss: 2.725\n",
      "Iteration: 277 \t--- Loss: 2.773\n",
      "Iteration: 278 \t--- Loss: 2.685\n",
      "Iteration: 279 \t--- Loss: 2.765\n",
      "Iteration: 280 \t--- Loss: 2.911\n",
      "Iteration: 281 \t--- Loss: 2.723\n",
      "Iteration: 282 \t--- Loss: 2.693\n",
      "Iteration: 283 \t--- Loss: 2.746\n",
      "Iteration: 284 \t--- Loss: 2.789\n",
      "Iteration: 285 \t--- Loss: 2.825\n",
      "Iteration: 286 \t--- Loss: 2.808\n",
      "Iteration: 287 \t--- Loss: 2.782\n",
      "Iteration: 288 \t--- Loss: 2.880\n",
      "Iteration: 289 \t--- Loss: 2.848\n",
      "Iteration: 290 \t--- Loss: 2.835\n",
      "Iteration: 291 \t--- Loss: 2.904\n",
      "Iteration: 292 \t--- Loss: 2.972\n",
      "Iteration: 293 \t--- Loss: 2.739\n",
      "Iteration: 294 \t--- Loss: 2.780\n",
      "Iteration: 295 \t--- Loss: 2.859\n",
      "Iteration: 296 \t--- Loss: 2.963\n",
      "Iteration: 297 \t--- Loss: 2.620\n",
      "Iteration: 298 \t--- Loss: 2.846\n",
      "Iteration: 299 \t--- Loss: 2.785\n",
      "Iteration: 300 \t--- Loss: 2.807\n",
      "Iteration: 301 \t--- Loss: 2.958\n",
      "Iteration: 302 \t--- Loss: 2.837\n",
      "Iteration: 303 \t--- Loss: 2.679\n",
      "Iteration: 304 \t--- Loss: 2.790\n",
      "Iteration: 305 \t--- Loss: 2.964\n",
      "Iteration: 306 \t--- Loss: 2.796\n",
      "Iteration: 307 \t--- Loss: 2.907\n",
      "Iteration: 308 \t--- Loss: 2.719\n",
      "Iteration: 309 \t--- Loss: 2.684\n",
      "Iteration: 310 \t--- Loss: 2.789\n",
      "Iteration: 311 \t--- Loss: 2.848\n",
      "Iteration: 312 \t--- Loss: 2.806\n",
      "Iteration: 313 \t--- Loss: 2.910\n",
      "Iteration: 314 \t--- Loss: 2.835\n",
      "Iteration: 315 \t--- Loss: 2.762\n",
      "Iteration: 316 \t--- Loss: 2.836\n",
      "Iteration: 317 \t--- Loss: 2.895\n",
      "Iteration: 318 \t--- Loss: 2.857\n",
      "Iteration: 319 \t--- Loss: 2.885\n",
      "Iteration: 320 \t--- Loss: 2.739\n",
      "Iteration: 321 \t--- Loss: 3.034\n",
      "Iteration: 322 \t--- Loss: 2.887\n",
      "Iteration: 323 \t--- Loss: 2.979\n",
      "Iteration: 324 \t--- Loss: 2.719\n",
      "Iteration: 325 \t--- Loss: 2.818\n",
      "Iteration: 326 \t--- Loss: 2.840\n",
      "Iteration: 327 \t--- Loss: 2.830\n",
      "Iteration: 328 \t--- Loss: 2.769\n",
      "Iteration: 329 \t--- Loss: 2.843\n",
      "Iteration: 330 \t--- Loss: 2.785\n",
      "Iteration: 331 \t--- Loss: 2.882\n",
      "Iteration: 332 \t--- Loss: 2.958\n",
      "Iteration: 333 \t--- Loss: 2.634\n",
      "Iteration: 334 \t--- Loss: 2.994\n",
      "Iteration: 335 \t--- Loss: 2.987\n",
      "Iteration: 336 \t--- Loss: 2.880\n",
      "Iteration: 337 \t--- Loss: 2.856\n",
      "Iteration: 338 \t--- Loss: 2.980\n",
      "Iteration: 339 \t--- Loss: 2.873\n",
      "Iteration: 340 \t--- Loss: 2.994\n",
      "Iteration: 341 \t--- Loss: 2.823\n",
      "Iteration: 342 \t--- Loss: 2.677\n",
      "Iteration: 343 \t--- Loss: 2.891\n",
      "Iteration: 344 \t--- Loss: 2.736\n",
      "Iteration: 345 \t--- Loss: 2.958\n",
      "Iteration: 346 \t--- Loss: 2.821\n",
      "Iteration: 347 \t--- Loss: 2.800\n",
      "Iteration: 348 \t--- Loss: 2.895\n",
      "Iteration: 349 \t--- Loss: 2.883\n",
      "Iteration: 350 \t--- Loss: 2.769\n",
      "Iteration: 351 \t--- Loss: 2.910\n",
      "Iteration: 352 \t--- Loss: 2.632\n",
      "Iteration: 353 \t--- Loss: 2.891\n",
      "Iteration: 354 \t--- Loss: 2.712\n",
      "Iteration: 355 \t--- Loss: 2.802\n",
      "Iteration: 356 \t--- Loss: 2.959\n",
      "Iteration: 357 \t--- Loss: 2.809\n",
      "Iteration: 358 \t--- Loss: 2.858\n",
      "Iteration: 359 \t--- Loss: 2.793\n",
      "Iteration: 360 \t--- Loss: 2.716\n",
      "Iteration: 361 \t--- Loss: 2.899\n",
      "Iteration: 362 \t--- Loss: 2.767\n",
      "Iteration: 363 \t--- Loss: 2.981\n",
      "Iteration: 364 \t--- Loss: 2.974\n",
      "Iteration: 365 \t--- Loss: 2.818\n",
      "Iteration: 366 \t--- Loss: 2.772\n",
      "Iteration: 367 \t--- Loss: 3.005\n",
      "Iteration: 368 \t--- Loss: 3.093\n",
      "Iteration: 369 \t--- Loss: 2.734\n",
      "Iteration: 370 \t--- Loss: 2.832\n",
      "Iteration: 371 \t--- Loss: 2.924\n",
      "Iteration: 372 \t--- Loss: 2.798\n",
      "Iteration: 373 \t--- Loss: 2.742\n",
      "Iteration: 374 \t--- Loss: 2.866\n",
      "Iteration: 375 \t--- Loss: 2.845\n",
      "Iteration: 376 \t--- Loss: 2.824\n",
      "Iteration: 377 \t--- Loss: 2.749\n",
      "Iteration: 378 \t--- Loss: 2.845\n",
      "Iteration: 379 \t--- Loss: 2.817\n",
      "Iteration: 380 \t--- Loss: 2.727\n",
      "Iteration: 381 \t--- Loss: 2.964\n",
      "Iteration: 382 \t--- Loss: 2.859\n",
      "Iteration: 383 \t--- Loss: 2.773\n",
      "Iteration: 384 \t--- Loss: 2.722\n",
      "Iteration: 385 \t--- Loss: 2.846\n",
      "Iteration: 386 \t--- Loss: 2.794\n",
      "Iteration: 387 \t--- Loss: 2.882\n",
      "Iteration: 388 \t--- Loss: 3.025\n",
      "Iteration: 389 \t--- Loss: 2.796\n",
      "Iteration: 390 \t--- Loss: 2.905\n",
      "Iteration: 391 \t--- Loss: 2.845\n",
      "Iteration: 392 \t--- Loss: 2.731\n",
      "Iteration: 393 \t--- Loss: 2.745\n",
      "Iteration: 394 \t--- Loss: 2.698\n",
      "Iteration: 395 \t--- Loss: 2.872\n",
      "Iteration: 396 \t--- Loss: 2.785\n",
      "Iteration: 397 \t--- Loss: 3.021\n",
      "Iteration: 398 \t--- Loss: 2.945\n",
      "Iteration: 399 \t--- Loss: 2.837\n",
      "Iteration: 400 \t--- Loss: 2.901\n",
      "Iteration: 401 \t--- Loss: 2.789\n",
      "Iteration: 402 \t--- Loss: 2.802\n",
      "Iteration: 403 \t--- Loss: 2.884\n",
      "Iteration: 404 \t--- Loss: 3.005\n",
      "Iteration: 405 \t--- Loss: 2.785\n",
      "Iteration: 406 \t--- Loss: 2.982\n",
      "Iteration: 407 \t--- Loss: 2.936\n",
      "Iteration: 408 \t--- Loss: 3.057\n",
      "Iteration: 409 \t--- Loss: 2.862\n",
      "Iteration: 410 \t--- Loss: 2.914\n",
      "Iteration: 411 \t--- Loss: 2.750\n",
      "Iteration: 412 \t--- Loss: 2.997\n",
      "Iteration: 413 \t--- Loss: 3.016\n",
      "Iteration: 414 \t--- Loss: 2.933\n",
      "Iteration: 415 \t--- Loss: 2.782\n",
      "Iteration: 416 \t--- Loss: 2.865\n",
      "Iteration: 417 \t--- Loss: 2.846\n",
      "Iteration: 418 \t--- Loss: 2.808\n",
      "Iteration: 419 \t--- Loss: 2.846\n",
      "Iteration: 420 \t--- Loss: 2.892\n",
      "Iteration: 421 \t--- Loss: 2.837\n",
      "Iteration: 422 \t--- Loss: 2.752\n",
      "Iteration: 423 \t--- Loss: 2.983\n",
      "Iteration: 424 \t--- Loss: 2.751\n",
      "Iteration: 425 \t--- Loss: 2.791\n",
      "Iteration: 426 \t--- Loss: 2.926\n",
      "Iteration: 427 \t--- Loss: 2.696\n",
      "Iteration: 428 \t--- Loss: 2.820\n",
      "Iteration: 429 \t--- Loss: 2.759\n",
      "Iteration: 430 \t--- Loss: 2.820\n",
      "Iteration: 431 \t--- Loss: 2.848\n",
      "Iteration: 432 \t--- Loss: 2.685\n",
      "Iteration: 433 \t--- Loss: 2.998\n",
      "Iteration: 434 \t--- Loss: 2.890\n",
      "Iteration: 435 \t--- Loss: 2.950\n",
      "Iteration: 436 \t--- Loss: 2.831\n",
      "Iteration: 437 \t--- Loss: 2.907\n",
      "Iteration: 438 \t--- Loss: 2.719\n",
      "Iteration: 439 \t--- Loss: 2.813\n",
      "Iteration: 440 \t--- Loss: 2.746\n",
      "Iteration: 441 \t--- Loss: 2.768\n",
      "Iteration: 442 \t--- Loss: 2.823\n",
      "Iteration: 443 \t--- Loss: 2.838\n",
      "Iteration: 444 \t--- Loss: 2.786\n",
      "Iteration: 445 \t--- Loss: 2.872\n",
      "Iteration: 446 \t--- Loss: 2.816\n",
      "Iteration: 447 \t--- Loss: 2.826\n",
      "Iteration: 448 \t--- Loss: 2.862\n",
      "Iteration: 449 \t--- Loss: 2.790\n",
      "Iteration: 450 \t--- Loss: 2.839\n",
      "Iteration: 451 \t--- Loss: 2.768\n",
      "Iteration: 452 \t--- Loss: 2.864\n",
      "Iteration: 453 \t--- Loss: 2.801\n",
      "Iteration: 454 \t--- Loss: 2.912\n",
      "Iteration: 455 \t--- Loss: 2.686\n",
      "Iteration: 456 \t--- Loss: 2.822\n",
      "Iteration: 457 \t--- Loss: 3.002\n",
      "Iteration: 458 \t--- Loss: 2.871\n",
      "Iteration: 459 \t--- Loss: 2.917\n",
      "Iteration: 460 \t--- Loss: 2.906\n",
      "Iteration: 461 \t--- Loss: 2.798\n",
      "Iteration: 462 \t--- Loss: 2.701\n",
      "Iteration: 463 \t--- Loss: 2.763\n",
      "Iteration: 464 \t--- Loss: 2.735\n",
      "Iteration: 465 \t--- Loss: 2.965\n",
      "Iteration: 466 \t--- Loss: 2.784\n",
      "Iteration: 467 \t--- Loss: 2.737\n",
      "Iteration: 468 \t--- Loss: 2.825\n",
      "Iteration: 469 \t--- Loss: 2.806\n",
      "Iteration: 470 \t--- Loss: 2.928\n",
      "Iteration: 471 \t--- Loss: 2.920\n",
      "Iteration: 472 \t--- Loss: 2.749\n",
      "Iteration: 473 \t--- Loss: 2.878\n",
      "Iteration: 474 \t--- Loss: 2.920\n",
      "Iteration: 475 \t--- Loss: 2.962\n",
      "Iteration: 476 \t--- Loss: 2.857\n",
      "Iteration: 477 \t--- Loss: 2.808\n",
      "Iteration: 478 \t--- Loss: 2.855\n",
      "Iteration: 479 \t--- Loss: 2.883\n",
      "Iteration: 480 \t--- Loss: 2.928\n",
      "Iteration: 481 \t--- Loss: 2.832\n",
      "Iteration: 482 \t--- Loss: 2.935\n",
      "Iteration: 483 \t--- Loss: 2.914\n",
      "Iteration: 484 \t--- Loss: 3.041\n",
      "Iteration: 485 \t--- Loss: 2.766\n",
      "Iteration: 486 \t--- Loss: 2.956\n",
      "Iteration: 487 \t--- Loss: 2.677\n",
      "Iteration: 488 \t--- Loss: 2.833\n",
      "Iteration: 489 \t--- Loss: 2.684\n",
      "Iteration: 490 \t--- Loss: 2.832\n",
      "Iteration: 491 \t--- Loss: 2.946\n",
      "Iteration: 492 \t--- Loss: 2.750\n",
      "Iteration: 493 \t--- Loss: 2.895\n",
      "Iteration: 494 \t--- Loss: 2.801\n",
      "Iteration: 495 \t--- Loss: 2.719\n",
      "Iteration: 496 \t--- Loss: 2.803\n",
      "Iteration: 497 \t--- Loss: 2.835\n",
      "Iteration: 498 \t--- Loss: 2.865\n",
      "Iteration: 499 \t--- Loss: 2.882\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:09,  1.41s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.010\n",
      "Iteration: 261 \t--- Loss: 0.011\n",
      "Iteration: 262 \t--- Loss: 0.011\n",
      "Iteration: 263 \t--- Loss: 0.011\n",
      "Iteration: 264 \t--- Loss: 0.011\n",
      "Iteration: 265 \t--- Loss: 0.010\n",
      "Iteration: 266 \t--- Loss: 0.011\n",
      "Iteration: 267 \t--- Loss: 0.011\n",
      "Iteration: 268 \t--- Loss: 0.009\n",
      "Iteration: 269 \t--- Loss: 0.009\n",
      "Iteration: 270 \t--- Loss: 0.011\n",
      "Iteration: 271 \t--- Loss: 0.010\n",
      "Iteration: 272 \t--- Loss: 0.009\n",
      "Iteration: 273 \t--- Loss: 0.009\n",
      "Iteration: 274 \t--- Loss: 0.009\n",
      "Iteration: 275 \t--- Loss: 0.009\n",
      "Iteration: 276 \t--- Loss: 0.010\n",
      "Iteration: 277 \t--- Loss: 0.009\n",
      "Iteration: 278 \t--- Loss: 0.010\n",
      "Iteration: 279 \t--- Loss: 0.009\n",
      "Iteration: 280 \t--- Loss: 0.009\n",
      "Iteration: 281 \t--- Loss: 0.009\n",
      "Iteration: 282 \t--- Loss: 0.009\n",
      "Iteration: 283 \t--- Loss: 0.010\n",
      "Iteration: 284 \t--- Loss: 0.008\n",
      "Iteration: 285 \t--- Loss: 0.009\n",
      "Iteration: 286 \t--- Loss: 0.008\n",
      "Iteration: 287 \t--- Loss: 0.009\n",
      "Iteration: 288 \t--- Loss: 0.008\n",
      "Iteration: 289 \t--- Loss: 0.007\n",
      "Iteration: 290 \t--- Loss: 0.009\n",
      "Iteration: 291 \t--- Loss: 0.008\n",
      "Iteration: 292 \t--- Loss: 0.008\n",
      "Iteration: 293 \t--- Loss: 0.009\n",
      "Iteration: 294 \t--- Loss: 0.009\n",
      "Iteration: 295 \t--- Loss: 0.008\n",
      "Iteration: 296 \t--- Loss: 0.008\n",
      "Iteration: 297 \t--- Loss: 0.008\n",
      "Iteration: 298 \t--- Loss: 0.009\n",
      "Iteration: 299 \t--- Loss: 0.008\n",
      "Iteration: 300 \t--- Loss: 0.008\n",
      "Iteration: 301 \t--- Loss: 0.008\n",
      "Iteration: 302 \t--- Loss: 0.008\n",
      "Iteration: 303 \t--- Loss: 0.008\n",
      "Iteration: 304 \t--- Loss: 0.009\n",
      "Iteration: 305 \t--- Loss: 0.009\n",
      "Iteration: 306 \t--- Loss: 0.009\n",
      "Iteration: 307 \t--- Loss: 0.009\n",
      "Iteration: 308 \t--- Loss: 0.008\n",
      "Iteration: 309 \t--- Loss: 0.008\n",
      "Iteration: 310 \t--- Loss: 0.009\n",
      "Iteration: 311 \t--- Loss: 0.010\n",
      "Iteration: 312 \t--- Loss: 0.009\n",
      "Iteration: 313 \t--- Loss: 0.009\n",
      "Iteration: 314 \t--- Loss: 0.012\n",
      "Iteration: 315 \t--- Loss: 0.013\n",
      "Iteration: 316 \t--- Loss: 0.014\n",
      "Iteration: 317 \t--- Loss: 0.011\n",
      "Iteration: 318 \t--- Loss: 0.010\n",
      "Iteration: 319 \t--- Loss: 0.010\n",
      "Iteration: 320 \t--- Loss: 0.009\n",
      "Iteration: 321 \t--- Loss: 0.010\n",
      "Iteration: 322 \t--- Loss: 0.010\n",
      "Iteration: 323 \t--- Loss: 0.009\n",
      "Iteration: 324 \t--- Loss: 0.009\n",
      "Iteration: 325 \t--- Loss: 0.009\n",
      "Iteration: 326 \t--- Loss: 0.008\n",
      "Iteration: 327 \t--- Loss: 0.008\n",
      "Iteration: 328 \t--- Loss: 0.008\n",
      "Iteration: 329 \t--- Loss: 0.008\n",
      "Iteration: 330 \t--- Loss: 0.007\n",
      "Iteration: 331 \t--- Loss: 0.008\n",
      "Iteration: 332 \t--- Loss: 0.007\n",
      "Iteration: 333 \t--- Loss: 0.007\n",
      "Iteration: 334 \t--- Loss: 0.007\n",
      "Iteration: 335 \t--- Loss: 0.008\n",
      "Iteration: 336 \t--- Loss: 0.008\n",
      "Iteration: 337 \t--- Loss: 0.008\n",
      "Iteration: 338 \t--- Loss: 0.008\n",
      "Iteration: 339 \t--- Loss: 0.006\n",
      "Iteration: 340 \t--- Loss: 0.007\n",
      "Iteration: 341 \t--- Loss: 0.007\n",
      "Iteration: 342 \t--- Loss: 0.008\n",
      "Iteration: 343 \t--- Loss: 0.007\n",
      "Iteration: 344 \t--- Loss: 0.008\n",
      "Iteration: 345 \t--- Loss: 0.007\n",
      "Iteration: 346 \t--- Loss: 0.007\n",
      "Iteration: 347 \t--- Loss: 0.008\n",
      "Iteration: 348 \t--- Loss: 0.007\n",
      "Iteration: 349 \t--- Loss: 0.007\n",
      "Iteration: 350 \t--- Loss: 0.007\n",
      "Iteration: 351 \t--- Loss: 0.007\n",
      "Iteration: 352 \t--- Loss: 0.007\n",
      "Iteration: 353 \t--- Loss: 0.007\n",
      "Iteration: 354 \t--- Loss: 0.007\n",
      "Iteration: 355 \t--- Loss: 0.007\n",
      "Iteration: 356 \t--- Loss: 0.007\n",
      "Iteration: 357 \t--- Loss: 0.007\n",
      "Iteration: 358 \t--- Loss: 0.007\n",
      "Iteration: 359 \t--- Loss: 0.007\n",
      "Iteration: 360 \t--- Loss: 0.007\n",
      "Iteration: 361 \t--- Loss: 0.007\n",
      "Iteration: 362 \t--- Loss: 0.006\n",
      "Iteration: 363 \t--- Loss: 0.008\n",
      "Iteration: 364 \t--- Loss: 0.008\n",
      "Iteration: 365 \t--- Loss: 0.006\n",
      "Iteration: 366 \t--- Loss: 0.006\n",
      "Iteration: 367 \t--- Loss: 0.007\n",
      "Iteration: 368 \t--- Loss: 0.007\n",
      "Iteration: 369 \t--- Loss: 0.008\n",
      "Iteration: 370 \t--- Loss: 0.007\n",
      "Iteration: 371 \t--- Loss: 0.007\n",
      "Iteration: 372 \t--- Loss: 0.007\n",
      "Iteration: 373 \t--- Loss: 0.007\n",
      "Iteration: 374 \t--- Loss: 0.008\n",
      "Iteration: 375 \t--- Loss: 0.007\n",
      "Iteration: 376 \t--- Loss: 0.007\n",
      "Iteration: 377 \t--- Loss: 0.006\n",
      "Iteration: 378 \t--- Loss: 0.006\n",
      "Iteration: 379 \t--- Loss: 0.007\n",
      "Iteration: 380 \t--- Loss: 0.007\n",
      "Iteration: 381 \t--- Loss: 0.006\n",
      "Iteration: 382 \t--- Loss: 0.007\n",
      "Iteration: 383 \t--- Loss: 0.007\n",
      "Iteration: 384 \t--- Loss: 0.007\n",
      "Iteration: 385 \t--- Loss: 0.006\n",
      "Iteration: 386 \t--- Loss: 0.006\n",
      "Iteration: 387 \t--- Loss: 0.006\n",
      "Iteration: 388 \t--- Loss: 0.007\n",
      "Iteration: 389 \t--- Loss: 0.007\n",
      "Iteration: 390 \t--- Loss: 0.007\n",
      "Iteration: 391 \t--- Loss: 0.007\n",
      "Iteration: 392 \t--- Loss: 0.007\n",
      "Iteration: 393 \t--- Loss: 0.007\n",
      "Iteration: 394 \t--- Loss: 0.007\n",
      "Iteration: 395 \t--- Loss: 0.006\n",
      "Iteration: 396 \t--- Loss: 0.006\n",
      "Iteration: 397 \t--- Loss: 0.006\n",
      "Iteration: 398 \t--- Loss: 0.007\n",
      "Iteration: 399 \t--- Loss: 0.006\n",
      "Iteration: 400 \t--- Loss: 0.006\n",
      "Iteration: 401 \t--- Loss: 0.006\n",
      "Iteration: 402 \t--- Loss: 0.007\n",
      "Iteration: 403 \t--- Loss: 0.007\n",
      "Iteration: 404 \t--- Loss: 0.007\n",
      "Iteration: 405 \t--- Loss: 0.007\n",
      "Iteration: 406 \t--- Loss: 0.006\n",
      "Iteration: 407 \t--- Loss: 0.006\n",
      "Iteration: 408 \t--- Loss: 0.007\n",
      "Iteration: 409 \t--- Loss: 0.007\n",
      "Iteration: 410 \t--- Loss: 0.007\n",
      "Iteration: 411 \t--- Loss: 0.007\n",
      "Iteration: 412 \t--- Loss: 0.007\n",
      "Iteration: 413 \t--- Loss: 0.007\n",
      "Iteration: 414 \t--- Loss: 0.008\n",
      "Iteration: 415 \t--- Loss: 0.007\n",
      "Iteration: 416 \t--- Loss: 0.006\n",
      "Iteration: 417 \t--- Loss: 0.007\n",
      "Iteration: 418 \t--- Loss: 0.007\n",
      "Iteration: 419 \t--- Loss: 0.006\n",
      "Iteration: 420 \t--- Loss: 0.006\n",
      "Iteration: 421 \t--- Loss: 0.006\n",
      "Iteration: 422 \t--- Loss: 0.007\n",
      "Iteration: 423 \t--- Loss: 0.006\n",
      "Iteration: 424 \t--- Loss: 0.007\n",
      "Iteration: 425 \t--- Loss: 0.006\n",
      "Iteration: 426 \t--- Loss: 0.006\n",
      "Iteration: 427 \t--- Loss: 0.006\n",
      "Iteration: 428 \t--- Loss: 0.006\n",
      "Iteration: 429 \t--- Loss: 0.006\n",
      "Iteration: 430 \t--- Loss: 0.006\n",
      "Iteration: 431 \t--- Loss: 0.007\n",
      "Iteration: 432 \t--- Loss: 0.006\n",
      "Iteration: 433 \t--- Loss: 0.006\n",
      "Iteration: 434 \t--- Loss: 0.006\n",
      "Iteration: 435 \t--- Loss: 0.006\n",
      "Iteration: 436 \t--- Loss: 0.006\n",
      "Iteration: 437 \t--- Loss: 0.006\n",
      "Iteration: 438 \t--- Loss: 0.006\n",
      "Iteration: 439 \t--- Loss: 0.006\n",
      "Iteration: 440 \t--- Loss: 0.006\n",
      "Iteration: 441 \t--- Loss: 0.006\n",
      "Iteration: 442 \t--- Loss: 0.006\n",
      "Iteration: 443 \t--- Loss: 0.006\n",
      "Iteration: 444 \t--- Loss: 0.007\n",
      "Iteration: 445 \t--- Loss: 0.006\n",
      "Iteration: 446 \t--- Loss: 0.006\n",
      "Iteration: 447 \t--- Loss: 0.006\n",
      "Iteration: 448 \t--- Loss: 0.006\n",
      "Iteration: 449 \t--- Loss: 0.006\n",
      "Iteration: 450 \t--- Loss: 0.006\n",
      "Iteration: 451 \t--- Loss: 0.006\n",
      "Iteration: 452 \t--- Loss: 0.006\n",
      "Iteration: 453 \t--- Loss: 0.006\n",
      "Iteration: 454 \t--- Loss: 0.006\n",
      "Iteration: 455 \t--- Loss: 0.006\n",
      "Iteration: 456 \t--- Loss: 0.005\n",
      "Iteration: 457 \t--- Loss: 0.006\n",
      "Iteration: 458 \t--- Loss: 0.006\n",
      "Iteration: 459 \t--- Loss: 0.006\n",
      "Iteration: 460 \t--- Loss: 0.005\n",
      "Iteration: 461 \t--- Loss: 0.006\n",
      "Iteration: 462 \t--- Loss: 0.005\n",
      "Iteration: 463 \t--- Loss: 0.006\n",
      "Iteration: 464 \t--- Loss: 0.005\n",
      "Iteration: 465 \t--- Loss: 0.006\n",
      "Iteration: 466 \t--- Loss: 0.006\n",
      "Iteration: 467 \t--- Loss: 0.005\n",
      "Iteration: 468 \t--- Loss: 0.005\n",
      "Iteration: 469 \t--- Loss: 0.006\n",
      "Iteration: 470 \t--- Loss: 0.006\n",
      "Iteration: 471 \t--- Loss: 0.005\n",
      "Iteration: 472 \t--- Loss: 0.006\n",
      "Iteration: 473 \t--- Loss: 0.006\n",
      "Iteration: 474 \t--- Loss: 0.005\n",
      "Iteration: 475 \t--- Loss: 0.005\n",
      "Iteration: 476 \t--- Loss: 0.006\n",
      "Iteration: 477 \t--- Loss: 0.005\n",
      "Iteration: 478 \t--- Loss: 0.005\n",
      "Iteration: 479 \t--- Loss: 0.005\n",
      "Iteration: 480 \t--- Loss: 0.006\n",
      "Iteration: 481 \t--- Loss: 0.005\n",
      "Iteration: 482 \t--- Loss: 0.005\n",
      "Iteration: 483 \t--- Loss: 0.005\n",
      "Iteration: 484 \t--- Loss: 0.005\n",
      "Iteration: 485 \t--- Loss: 0.005\n",
      "Iteration: 486 \t--- Loss: 0.005\n",
      "Iteration: 487 \t--- Loss: 0.005\n",
      "Iteration: 488 \t--- Loss: 0.005\n",
      "Iteration: 489 \t--- Loss: 0.005\n",
      "Iteration: 490 \t--- Loss: 0.005\n",
      "Iteration: 491 \t--- Loss: 0.005\n",
      "Iteration: 492 \t--- Loss: 0.005\n",
      "Iteration: 493 \t--- Loss: 0.005\n",
      "Iteration: 494 \t--- Loss: 0.005\n",
      "Iteration: 495 \t--- Loss: 0.005\n",
      "Iteration: 496 \t--- Loss: 0.005\n",
      "Iteration: 497 \t--- Loss: 0.004\n",
      "Iteration: 498 \t--- Loss: 0.005\n",
      "Iteration: 499 \t--- Loss: 0.005\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.53s/it][Parallel(n_jobs=5)]: Done  21 tasks      | elapsed: 12.0min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 5/10 [00:14<00:15,  3.12s/it][Parallel(n_jobs=5)]: Done  22 tasks      | elapsed: 12.2min\n",
      "  0%|          | 0/1 [09:59<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 2.080\n",
      "Iteration: 1 \t--- Loss: 1.965\n",
      "Iteration: 2 \t--- Loss: 1.902\n",
      "Iteration: 3 \t--- Loss: 1.743\n",
      "Iteration: 4 \t--- Loss: 1.750\n",
      "Iteration: 5 \t--- Loss: 1.639\n",
      "Iteration: 6 \t--- Loss: 1.590\n",
      "Iteration: 7 \t--- Loss: 1.555\n",
      "Iteration: 8 \t--- Loss: 1.698\n",
      "Iteration: 9 \t--- Loss: 1.667\n",
      "Iteration: 10 \t--- Loss: 1.524\n",
      "Iteration: 11 \t--- Loss: 1.525\n",
      "Iteration: 12 \t--- Loss: 1.652\n",
      "Iteration: 13 \t--- Loss: 1.582\n",
      "Iteration: 14 \t--- Loss: 1.592\n",
      "Iteration: 15 \t--- Loss: 1.498\n",
      "Iteration: 16 \t--- Loss: 1.547\n",
      "Iteration: 17 \t--- Loss: 1.524\n",
      "Iteration: 18 \t--- Loss: 1.455\n",
      "Iteration: 19 \t--- Loss: 1.408\n",
      "Iteration: 20 \t--- Loss: 1.474\n",
      "Iteration: 21 \t--- Loss: 1.475\n",
      "Iteration: 22 \t--- Loss: 1.516\n",
      "Iteration: 23 \t--- Loss: 1.462\n",
      "Iteration: 24 \t--- Loss: 1.468\n",
      "Iteration: 25 \t--- Loss: 1.413\n",
      "Iteration: 26 \t--- Loss: 1.482\n",
      "Iteration: 27 \t--- Loss: 1.414\n",
      "Iteration: 28 \t--- Loss: 1.396\n",
      "Iteration: 29 \t--- Loss: 1.335\n",
      "Iteration: 30 \t--- Loss: 1.382\n",
      "Iteration: 31 \t--- Loss: 1.400\n",
      "Iteration: 32 \t--- Loss: 1.366\n",
      "Iteration: 33 \t--- Loss: 1.419\n",
      "Iteration: 34 \t--- Loss: 1.353\n",
      "Iteration: 35 \t--- Loss: 1.442\n",
      "Iteration: 36 \t--- Loss: 1.395\n",
      "Iteration: 37 \t--- Loss: 1.430\n",
      "Iteration: 38 \t--- Loss: 1.477\n",
      "Iteration: 39 \t--- Loss: 1.361\n",
      "Iteration: 40 \t--- Loss: 1.397\n",
      "Iteration: 41 \t--- Loss: 1.381\n",
      "Iteration: 42 \t--- Loss: 1.344\n",
      "Iteration: 43 \t--- Loss: 1.356\n",
      "Iteration: 44 \t--- Loss: 1.345\n",
      "Iteration: 45 \t--- Loss: 1.331\n",
      "Iteration: 46 \t--- Loss: 1.334\n",
      "Iteration: 47 \t--- Loss: 1.310\n",
      "Iteration: 48 \t--- Loss: 1.274\n",
      "Iteration: 49 \t--- Loss: 1.294\n",
      "Iteration: 50 \t--- Loss: 1.251\n",
      "Iteration: 51 \t--- Loss: 1.224\n",
      "Iteration: 52 \t--- Loss: 1.333\n",
      "Iteration: 53 \t--- Loss: 1.363\n",
      "Iteration: 54 \t--- Loss: 1.306\n",
      "Iteration: 55 \t--- Loss: 1.323\n",
      "Iteration: 56 \t--- Loss: 1.302\n",
      "Iteration: 57 \t--- Loss: 1.161\n",
      "Iteration: 58 \t--- Loss: 1.163\n",
      "Iteration: 59 \t--- Loss: 1.261\n",
      "Iteration: 60 \t--- Loss: 1.176\n",
      "Iteration: 61 \t--- Loss: 1.088\n",
      "Iteration: 62 \t--- Loss: 1.099\n",
      "Iteration: 63 \t--- Loss: 1.030\n",
      "Iteration: 64 \t--- Loss: 1.082\n",
      "Iteration: 65 \t--- Loss: 1.187\n",
      "Iteration: 66 \t--- Loss: 1.615\n",
      "Iteration: 67 \t--- Loss: 2.587\n",
      "\n",
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:21<00:09,  3.10s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.258\n",
      "Iteration: 261 \t--- Loss: 0.257\n",
      "Iteration: 262 \t--- Loss: 0.259\n",
      "Iteration: 263 \t--- Loss: 0.259\n",
      "Iteration: 264 \t--- Loss: 0.258\n",
      "Iteration: 265 \t--- Loss: 0.259\n",
      "Iteration: 266 \t--- Loss: 0.259\n",
      "Iteration: 267 \t--- Loss: 0.257\n",
      "Iteration: 268 \t--- Loss: 0.256\n",
      "Iteration: 269 \t--- Loss: 0.257\n",
      "Iteration: 270 \t--- Loss: 0.257\n",
      "Iteration: 271 \t--- Loss: 0.257\n",
      "Iteration: 272 \t--- Loss: 0.257\n",
      "Iteration: 273 \t--- Loss: 0.258\n",
      "Iteration: 274 \t--- Loss: 0.258\n",
      "Iteration: 275 \t--- Loss: 0.258\n",
      "Iteration: 276 \t--- Loss: 0.258\n",
      "Iteration: 277 \t--- Loss: 0.258\n",
      "Iteration: 278 \t--- Loss: 0.259\n",
      "Iteration: 279 \t--- Loss: 0.258\n",
      "Iteration: 280 \t--- Loss: 0.258\n",
      "Iteration: 281 \t--- Loss: 0.258\n",
      "Iteration: 282 \t--- Loss: 0.257\n",
      "Iteration: 283 \t--- Loss: 0.257\n",
      "Iteration: 284 \t--- Loss: 0.259\n",
      "Iteration: 285 \t--- Loss: 0.259\n",
      "Iteration: 286 \t--- Loss: 0.259\n",
      "Iteration: 287 \t--- Loss: 0.257\n",
      "Iteration: 288 \t--- Loss: 0.259\n",
      "Iteration: 289 \t--- Loss: 0.258\n",
      "Iteration: 290 \t--- Loss: 0.258\n",
      "Iteration: 291 \t--- Loss: 0.259\n",
      "Iteration: 292 \t--- Loss: 0.257\n",
      "Iteration: 293 \t--- Loss: 0.258\n",
      "Iteration: 294 \t--- Loss: 0.258\n",
      "Iteration: 295 \t--- Loss: 0.258\n",
      "Iteration: 296 \t--- Loss: 0.258\n",
      "Iteration: 297 \t--- Loss: 0.259\n",
      "Iteration: 298 \t--- Loss: 0.257\n",
      "Iteration: 299 \t--- Loss: 0.258\n",
      "Iteration: 300 \t--- Loss: 0.258\n",
      "Iteration: 301 \t--- Loss: 0.258\n",
      "Iteration: 302 \t--- Loss: 0.258\n",
      "Iteration: 303 \t--- Loss: 0.258\n",
      "Iteration: 304 \t--- Loss: 0.258\n",
      "Iteration: 305 \t--- Loss: 0.258\n",
      "Iteration: 306 \t--- Loss: 0.258\n",
      "Iteration: 307 \t--- Loss: 0.258\n",
      "Iteration: 308 \t--- Loss: 0.257\n",
      "Iteration: 309 \t--- Loss: 0.258\n",
      "Iteration: 310 \t--- Loss: 0.257\n",
      "Iteration: 311 \t--- Loss: 0.258\n",
      "Iteration: 312 \t--- Loss: 0.256\n",
      "Iteration: 313 \t--- Loss: 0.257\n",
      "Iteration: 314 \t--- Loss: 0.257\n",
      "Iteration: 315 \t--- Loss: 0.258\n",
      "Iteration: 316 \t--- Loss: 0.258\n",
      "Iteration: 317 \t--- Loss: 0.258\n",
      "Iteration: 318 \t--- Loss: 0.258\n",
      "Iteration: 319 \t--- Loss: 0.258\n",
      "Iteration: 320 \t--- Loss: 0.259\n",
      "Iteration: 321 \t--- Loss: 0.258\n",
      "Iteration: 322 \t--- Loss: 0.258\n",
      "Iteration: 323 \t--- Loss: 0.258\n",
      "Iteration: 324 \t--- Loss: 0.258\n",
      "Iteration: 325 \t--- Loss: 0.258\n",
      "Iteration: 326 \t--- Loss: 0.257\n",
      "Iteration: 327 \t--- Loss: 0.259\n",
      "Iteration: 328 \t--- Loss: 0.259\n",
      "Iteration: 329 \t--- Loss: 0.258\n",
      "Iteration: 330 \t--- Loss: 0.257\n",
      "Iteration: 331 \t--- Loss: 0.258\n",
      "Iteration: 332 \t--- Loss: 0.259\n",
      "Iteration: 333 \t--- Loss: 0.258\n",
      "Iteration: 334 \t--- Loss: 0.257\n",
      "Iteration: 335 \t--- Loss: 0.257\n",
      "Iteration: 336 \t--- Loss: 0.257\n",
      "Iteration: 337 \t--- Loss: 0.260\n",
      "Iteration: 338 \t--- Loss: 0.258\n",
      "Iteration: 339 \t--- Loss: 0.258\n",
      "Iteration: 340 \t--- Loss: 0.258\n",
      "Iteration: 341 \t--- Loss: 0.257\n",
      "Iteration: 342 \t--- Loss: 0.258\n",
      "Iteration: 343 \t--- Loss: 0.258\n",
      "Iteration: 344 \t--- Loss: 0.258\n",
      "Iteration: 345 \t--- Loss: 0.257\n",
      "Iteration: 346 \t--- Loss: 0.257\n",
      "Iteration: 347 \t--- Loss: 0.258\n",
      "Iteration: 348 \t--- Loss: 0.257\n",
      "Iteration: 349 \t--- Loss: 0.257\n",
      "Iteration: 350 \t--- Loss: 0.258\n",
      "Iteration: 351 \t--- Loss: 0.258\n",
      "Iteration: 352 \t--- Loss: 0.259\n",
      "Iteration: 353 \t--- Loss: 0.258\n",
      "Iteration: 354 \t--- Loss: 0.258\n",
      "Iteration: 355 \t--- Loss: 0.258\n",
      "Iteration: 356 \t--- Loss: 0.259\n",
      "Iteration: 357 \t--- Loss: 0.258\n",
      "Iteration: 358 \t--- Loss: 0.257\n",
      "Iteration: 359 \t--- Loss: 0.257\n",
      "Iteration: 360 \t--- Loss: 0.259\n",
      "Iteration: 361 \t--- Loss: 0.257\n",
      "Iteration: 362 \t--- Loss: 0.258\n",
      "Iteration: 363 \t--- Loss: 0.258\n",
      "Iteration: 364 \t--- Loss: 0.259\n",
      "Iteration: 365 \t--- Loss: 0.258\n",
      "Iteration: 366 \t--- Loss: 0.258\n",
      "Iteration: 367 \t--- Loss: 0.258\n",
      "Iteration: 368 \t--- Loss: 0.258\n",
      "Iteration: 369 \t--- Loss: 0.259\n",
      "Iteration: 370 \t--- Loss: 0.258\n",
      "Iteration: 371 \t--- Loss: 0.258\n",
      "Iteration: 372 \t--- Loss: 0.259\n",
      "Iteration: 373 \t--- Loss: 0.258\n",
      "Iteration: 374 \t--- Loss: 0.259\n",
      "Iteration: 375 \t--- Loss: 0.258\n",
      "Iteration: 376 \t--- Loss: 0.259\n",
      "Iteration: 377 \t--- Loss: 0.258\n",
      "Iteration: 378 \t--- Loss: 0.257\n",
      "Iteration: 379 \t--- Loss: 0.257\n",
      "Iteration: 380 \t--- Loss: 0.258\n",
      "Iteration: 381 \t--- Loss: 0.259\n",
      "Iteration: 382 \t--- Loss: 0.257\n",
      "Iteration: 383 \t--- Loss: 0.258\n",
      "Iteration: 384 \t--- Loss: 0.257\n",
      "Iteration: 385 \t--- Loss: 0.258\n",
      "Iteration: 386 \t--- Loss: 0.257\n",
      "Iteration: 387 \t--- Loss: 0.258\n",
      "Iteration: 388 \t--- Loss: 0.259\n",
      "Iteration: 389 \t--- Loss: 0.258\n",
      "Iteration: 390 \t--- Loss: 0.258\n",
      "Iteration: 391 \t--- Loss: 0.258\n",
      "Iteration: 392 \t--- Loss: 0.257\n",
      "Iteration: 393 \t--- Loss: 0.258\n",
      "Iteration: 394 \t--- Loss: 0.258\n",
      "Iteration: 395 \t--- Loss: 0.259\n",
      "Iteration: 396 \t--- Loss: 0.258\n",
      "Iteration: 397 \t--- Loss: 0.257\n",
      "Iteration: 398 \t--- Loss: 0.257\n",
      "Iteration: 399 \t--- Loss: 0.258\n",
      "Iteration: 400 \t--- Loss: 0.257\n",
      "Iteration: 401 \t--- Loss: 0.258\n",
      "Iteration: 402 \t--- Loss: 0.258\n",
      "Iteration: 403 \t--- Loss: 0.257\n",
      "Iteration: 404 \t--- Loss: 0.257\n",
      "Iteration: 405 \t--- Loss: 0.258\n",
      "Iteration: 406 \t--- Loss: 0.257\n",
      "Iteration: 407 \t--- Loss: 0.257\n",
      "Iteration: 408 \t--- Loss: 0.258\n",
      "Iteration: 409 \t--- Loss: 0.258\n",
      "Iteration: 410 \t--- Loss: 0.258\n",
      "Iteration: 411 \t--- Loss: 0.257\n",
      "Iteration: 412 \t--- Loss: 0.258\n",
      "Iteration: 413 \t--- Loss: 0.259\n",
      "Iteration: 414 \t--- Loss: 0.257\n",
      "Iteration: 415 \t--- Loss: 0.259\n",
      "Iteration: 416 \t--- Loss: 0.258\n",
      "Iteration: 417 \t--- Loss: 0.258\n",
      "Iteration: 418 \t--- Loss: 0.258\n",
      "Iteration: 419 \t--- Loss: 0.257\n",
      "Iteration: 420 \t--- Loss: 0.258\n",
      "Iteration: 421 \t--- Loss: 0.259\n",
      "Iteration: 422 \t--- Loss: 0.257\n",
      "Iteration: 423 \t--- Loss: 0.258\n",
      "Iteration: 424 \t--- Loss: 0.258\n",
      "Iteration: 425 \t--- Loss: 0.258\n",
      "Iteration: 426 \t--- Loss: 0.257\n",
      "Iteration: 427 \t--- Loss: 0.257\n",
      "Iteration: 428 \t--- Loss: 0.257\n",
      "Iteration: 429 \t--- Loss: 0.257\n",
      "Iteration: 430 \t--- Loss: 0.258\n",
      "Iteration: 431 \t--- Loss: 0.258\n",
      "Iteration: 432 \t--- Loss: 0.257\n",
      "Iteration: 433 \t--- Loss: 0.258\n",
      "Iteration: 434 \t--- Loss: 0.259\n",
      "Iteration: 435 \t--- Loss: 0.257\n",
      "Iteration: 436 \t--- Loss: 0.257\n",
      "Iteration: 437 \t--- Loss: 0.258\n",
      "Iteration: 438 \t--- Loss: 0.257\n",
      "Iteration: 439 \t--- Loss: 0.257\n",
      "Iteration: 440 \t--- Loss: 0.257\n",
      "Iteration: 441 \t--- Loss: 0.258\n",
      "Iteration: 442 \t--- Loss: 0.259\n",
      "Iteration: 443 \t--- Loss: 0.257\n",
      "Iteration: 444 \t--- Loss: 0.258\n",
      "Iteration: 445 \t--- Loss: 0.258\n",
      "Iteration: 446 \t--- Loss: 0.259\n",
      "Iteration: 447 \t--- Loss: 0.257\n",
      "Iteration: 448 \t--- Loss: 0.260\n",
      "Iteration: 449 \t--- Loss: 0.258\n",
      "Iteration: 450 \t--- Loss: 0.257\n",
      "Iteration: 451 \t--- Loss: 0.258\n",
      "Iteration: 452 \t--- Loss: 0.259\n",
      "Iteration: 453 \t--- Loss: 0.257\n",
      "Iteration: 454 \t--- Loss: 0.257\n",
      "Iteration: 455 \t--- Loss: 0.258\n",
      "Iteration: 456 \t--- Loss: 0.258\n",
      "Iteration: 457 \t--- Loss: 0.259\n",
      "Iteration: 458 \t--- Loss: 0.258\n",
      "Iteration: 459 \t--- Loss: 0.257\n",
      "Iteration: 460 \t--- Loss: 0.257\n",
      "Iteration: 461 \t--- Loss: 0.258\n",
      "Iteration: 462 \t--- Loss: 0.258\n",
      "Iteration: 463 \t--- Loss: 0.259\n",
      "Iteration: 464 \t--- Loss: 0.258\n",
      "Iteration: 465 \t--- Loss: 0.258\n",
      "Iteration: 466 \t--- Loss: 0.258\n",
      "Iteration: 467 \t--- Loss: 0.258\n",
      "Iteration: 468 \t--- Loss: 0.257\n",
      "Iteration: 469 \t--- Loss: 0.258\n",
      "Iteration: 470 \t--- Loss: 0.258\n",
      "Iteration: 471 \t--- Loss: 0.256\n",
      "Iteration: 472 \t--- Loss: 0.258\n",
      "Iteration: 473 \t--- Loss: 0.258\n",
      "Iteration: 474 \t--- Loss: 0.257\n",
      "Iteration: 475 \t--- Loss: 0.258\n",
      "Iteration: 476 \t--- Loss: 0.257\n",
      "Iteration: 477 \t--- Loss: 0.256\n",
      "Iteration: 478 \t--- Loss: 0.258\n",
      "Iteration: 479 \t--- Loss: 0.258\n",
      "Iteration: 480 \t--- Loss: 0.257\n",
      "Iteration: 481 \t--- Loss: 0.258\n",
      "Iteration: 482 \t--- Loss: 0.257\n",
      "Iteration: 483 \t--- Loss: 0.257\n",
      "Iteration: 484 \t--- Loss: 0.258\n",
      "Iteration: 485 \t--- Loss: 0.258\n",
      "Iteration: 486 \t--- Loss: 0.258\n",
      "Iteration: 487 \t--- Loss: 0.257\n",
      "Iteration: 488 \t--- Loss: 0.259\n",
      "Iteration: 489 \t--- Loss: 0.258\n",
      "Iteration: 490 \t--- Loss: 0.258\n",
      "Iteration: 491 \t--- Loss: 0.258\n",
      "Iteration: 492 \t--- Loss: 0.257\n",
      "Iteration: 493 \t--- Loss: 0.256\n",
      "Iteration: 494 \t--- Loss: 0.258\n",
      "Iteration: 495 \t--- Loss: 0.258\n",
      "Iteration: 496 \t--- Loss: 0.258\n",
      "Iteration: 497 \t--- Loss: 0.259\n",
      "Iteration: 498 \t--- Loss: 0.259\n",
      "Iteration: 499 \t--- Loss: 0.258\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:08<00:06,  1.53s/it][Parallel(n_jobs=5)]: Done  23 tasks      | elapsed: 12.4min\n",
      "100%|██████████| 10/10 [00:30<00:00,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [01:35<00:00, 95.64s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.483\n",
      "Iteration: 261 \t--- Loss: 0.481\n",
      "Iteration: 262 \t--- Loss: 0.490\n",
      "Iteration: 263 \t--- Loss: 0.490\n",
      "Iteration: 264 \t--- Loss: 0.505\n",
      "Iteration: 265 \t--- Loss: 0.477\n",
      "Iteration: 266 \t--- Loss: 0.491\n",
      "Iteration: 267 \t--- Loss: 0.479\n",
      "Iteration: 268 \t--- Loss: 0.480\n",
      "Iteration: 269 \t--- Loss: 0.489\n",
      "Iteration: 270 \t--- Loss: 0.468\n",
      "Iteration: 271 \t--- Loss: 0.488\n",
      "Iteration: 272 \t--- Loss: 0.477\n",
      "Iteration: 273 \t--- Loss: 0.489\n",
      "Iteration: 274 \t--- Loss: 0.488\n",
      "Iteration: 275 \t--- Loss: 0.480\n",
      "Iteration: 276 \t--- Loss: 0.487\n",
      "Iteration: 277 \t--- Loss: 0.497\n",
      "Iteration: 278 \t--- Loss: 0.494\n",
      "Iteration: 279 \t--- Loss: 0.486\n",
      "Iteration: 280 \t--- Loss: 0.464\n",
      "Iteration: 281 \t--- Loss: 0.488\n",
      "Iteration: 282 \t--- Loss: 0.468\n",
      "Iteration: 283 \t--- Loss: 0.483\n",
      "Iteration: 284 \t--- Loss: 0.458\n",
      "Iteration: 285 \t--- Loss: 0.450\n",
      "Iteration: 286 \t--- Loss: 0.474\n",
      "Iteration: 287 \t--- Loss: 0.482\n",
      "Iteration: 288 \t--- Loss: 0.480\n",
      "Iteration: 289 \t--- Loss: 0.488\n",
      "Iteration: 290 \t--- Loss: 0.482\n",
      "Iteration: 291 \t--- Loss: 0.477\n",
      "Iteration: 292 \t--- Loss: 0.493\n",
      "Iteration: 293 \t--- Loss: 0.471\n",
      "Iteration: 294 \t--- Loss: 0.481\n",
      "Iteration: 295 \t--- Loss: 0.475\n",
      "Iteration: 296 \t--- Loss: 0.476\n",
      "Iteration: 297 \t--- Loss: 0.484\n",
      "Iteration: 298 \t--- Loss: 0.491\n",
      "Iteration: 299 \t--- Loss: 0.470\n",
      "Iteration: 300 \t--- Loss: 0.497\n",
      "Iteration: 301 \t--- Loss: 0.467\n",
      "Iteration: 302 \t--- Loss: 0.476\n",
      "Iteration: 303 \t--- Loss: 0.493\n",
      "Iteration: 304 \t--- Loss: 0.486\n",
      "Iteration: 305 \t--- Loss: 0.475\n",
      "Iteration: 306 \t--- Loss: 0.494\n",
      "Iteration: 307 \t--- Loss: 0.490\n",
      "Iteration: 308 \t--- Loss: 0.484\n",
      "Iteration: 309 \t--- Loss: 0.484\n",
      "Iteration: 310 \t--- Loss: 0.473\n",
      "Iteration: 311 \t--- Loss: 0.485\n",
      "Iteration: 312 \t--- Loss: 0.479\n",
      "Iteration: 313 \t--- Loss: 0.492\n",
      "Iteration: 314 \t--- Loss: 0.476\n",
      "Iteration: 315 \t--- Loss: 0.481\n",
      "Iteration: 316 \t--- Loss: 0.481\n",
      "Iteration: 317 \t--- Loss: 0.458\n",
      "Iteration: 318 \t--- Loss: 0.462\n",
      "Iteration: 319 \t--- Loss: 0.487\n",
      "Iteration: 320 \t--- Loss: 0.481\n",
      "Iteration: 321 \t--- Loss: 0.479\n",
      "Iteration: 322 \t--- Loss: 0.482\n",
      "Iteration: 323 \t--- Loss: 0.481\n",
      "Iteration: 324 \t--- Loss: 0.471\n",
      "Iteration: 325 \t--- Loss: 0.492\n",
      "Iteration: 326 \t--- Loss: 0.480\n",
      "Iteration: 327 \t--- Loss: 0.480\n",
      "Iteration: 328 \t--- Loss: 0.474\n",
      "Iteration: 329 \t--- Loss: 0.477\n",
      "Iteration: 330 \t--- Loss: 0.462\n",
      "Iteration: 331 \t--- Loss: 0.469\n",
      "Iteration: 332 \t--- Loss: 0.498\n",
      "Iteration: 333 \t--- Loss: 0.460\n",
      "Iteration: 334 \t--- Loss: 0.497\n",
      "Iteration: 335 \t--- Loss: 0.461\n",
      "Iteration: 336 \t--- Loss: 0.489\n",
      "Iteration: 337 \t--- Loss: 0.467\n",
      "Iteration: 338 \t--- Loss: 0.494\n",
      "Iteration: 339 \t--- Loss: 0.501\n",
      "Iteration: 340 \t--- Loss: 0.477\n",
      "Iteration: 341 \t--- Loss: 0.468\n",
      "Iteration: 342 \t--- Loss: 0.490\n",
      "Iteration: 343 \t--- Loss: 0.479\n",
      "Iteration: 344 \t--- Loss: 0.484\n",
      "Iteration: 345 \t--- Loss: 0.463\n",
      "Iteration: 346 \t--- Loss: 0.481\n",
      "Iteration: 347 \t--- Loss: 0.497\n",
      "Iteration: 348 \t--- Loss: 0.461\n",
      "Iteration: 349 \t--- Loss: 0.474\n",
      "Iteration: 350 \t--- Loss: 0.521\n",
      "Iteration: 351 \t--- Loss: 0.507\n",
      "Iteration: 352 \t--- Loss: 0.456\n",
      "Iteration: 353 \t--- Loss: 0.463\n",
      "Iteration: 354 \t--- Loss: 0.470\n",
      "Iteration: 355 \t--- Loss: 0.474\n",
      "Iteration: 356 \t--- Loss: 0.492\n",
      "Iteration: 357 \t--- Loss: 0.463\n",
      "Iteration: 358 \t--- Loss: 0.492\n",
      "Iteration: 359 \t--- Loss: 0.464\n",
      "Iteration: 360 \t--- Loss: 0.483\n",
      "Iteration: 361 \t--- Loss: 0.487\n",
      "Iteration: 362 \t--- Loss: 0.494\n",
      "Iteration: 363 \t--- Loss: 0.491\n",
      "Iteration: 364 \t--- Loss: 0.463\n",
      "Iteration: 365 \t--- Loss: 0.474\n",
      "Iteration: 366 \t--- Loss: 0.501\n",
      "Iteration: 367 \t--- Loss: 0.454\n",
      "Iteration: 368 \t--- Loss: 0.458\n",
      "Iteration: 369 \t--- Loss: 0.460\n",
      "Iteration: 370 \t--- Loss: 0.502\n",
      "Iteration: 371 \t--- Loss: 0.467\n",
      "Iteration: 372 \t--- Loss: 0.493\n",
      "Iteration: 373 \t--- Loss: 0.487\n",
      "Iteration: 374 \t--- Loss: 0.478\n",
      "Iteration: 375 \t--- Loss: 0.470\n",
      "Iteration: 376 \t--- Loss: 0.480\n",
      "Iteration: 377 \t--- Loss: 0.466\n",
      "Iteration: 378 \t--- Loss: 0.492\n",
      "Iteration: 379 \t--- Loss: 0.474\n",
      "Iteration: 380 \t--- Loss: 0.481\n",
      "Iteration: 381 \t--- Loss: 0.485\n",
      "Iteration: 382 \t--- Loss: 0.476\n",
      "Iteration: 383 \t--- Loss: 0.479\n",
      "Iteration: 384 \t--- Loss: 0.481\n",
      "Iteration: 385 \t--- Loss: 0.479\n",
      "Iteration: 386 \t--- Loss: 0.471\n",
      "Iteration: 387 \t--- Loss: 0.487\n",
      "Iteration: 388 \t--- Loss: 0.492\n",
      "Iteration: 389 \t--- Loss: 0.475\n",
      "Iteration: 390 \t--- Loss: 0.474\n",
      "Iteration: 391 \t--- Loss: 0.474\n",
      "Iteration: 392 \t--- Loss: 0.489\n",
      "Iteration: 393 \t--- Loss: 0.477\n",
      "Iteration: 394 \t--- Loss: 0.498\n",
      "Iteration: 395 \t--- Loss: 0.480\n",
      "Iteration: 396 \t--- Loss: 0.493\n",
      "Iteration: 397 \t--- Loss: 0.480\n",
      "Iteration: 398 \t--- Loss: 0.477\n",
      "Iteration: 399 \t--- Loss: 0.474\n",
      "Iteration: 400 \t--- Loss: 0.476\n",
      "Iteration: 401 \t--- Loss: 0.472\n",
      "Iteration: 402 \t--- Loss: 0.478\n",
      "Iteration: 403 \t--- Loss: 0.502\n",
      "Iteration: 404 \t--- Loss: 0.480\n",
      "Iteration: 405 \t--- Loss: 0.479\n",
      "Iteration: 406 \t--- Loss: 0.498\n",
      "Iteration: 407 \t--- Loss: 0.481\n",
      "Iteration: 408 \t--- Loss: 0.475\n",
      "Iteration: 409 \t--- Loss: 0.476\n",
      "Iteration: 410 \t--- Loss: 0.491\n",
      "Iteration: 411 \t--- Loss: 0.475\n",
      "Iteration: 412 \t--- Loss: 0.480\n",
      "Iteration: 413 \t--- Loss: 0.493\n",
      "Iteration: 414 \t--- Loss: 0.491\n",
      "Iteration: 415 \t--- Loss: 0.479\n",
      "Iteration: 416 \t--- Loss: 0.466\n",
      "Iteration: 417 \t--- Loss: 0.485\n",
      "Iteration: 418 \t--- Loss: 0.478\n",
      "Iteration: 419 \t--- Loss: 0.470\n",
      "Iteration: 420 \t--- Loss: 0.490\n",
      "Iteration: 421 \t--- Loss: 0.474\n",
      "Iteration: 422 \t--- Loss: 0.480\n",
      "Iteration: 423 \t--- Loss: 0.479\n",
      "Iteration: 424 \t--- Loss: 0.493\n",
      "Iteration: 425 \t--- Loss: 0.476\n",
      "Iteration: 426 \t--- Loss: 0.490\n",
      "Iteration: 427 \t--- Loss: 0.482\n",
      "Iteration: 428 \t--- Loss: 0.470\n",
      "Iteration: 429 \t--- Loss: 0.484\n",
      "Iteration: 430 \t--- Loss: 0.485\n",
      "Iteration: 431 \t--- Loss: 0.474\n",
      "Iteration: 432 \t--- Loss: 0.467\n",
      "Iteration: 433 \t--- Loss: 0.492\n",
      "Iteration: 434 \t--- Loss: 0.477\n",
      "Iteration: 435 \t--- Loss: 0.489\n",
      "Iteration: 436 \t--- Loss: 0.495\n",
      "Iteration: 437 \t--- Loss: 0.486\n",
      "Iteration: 438 \t--- Loss: 0.481\n",
      "Iteration: 439 \t--- Loss: 0.499\n",
      "Iteration: 440 \t--- Loss: 0.470\n",
      "Iteration: 441 \t--- Loss: 0.482\n",
      "Iteration: 442 \t--- Loss: 0.471\n",
      "Iteration: 443 \t--- Loss: 0.475\n",
      "Iteration: 444 \t--- Loss: 0.477\n",
      "Iteration: 445 \t--- Loss: 0.481\n",
      "Iteration: 446 \t--- Loss: 0.500\n",
      "Iteration: 447 \t--- Loss: 0.488\n",
      "Iteration: 448 \t--- Loss: 0.475\n",
      "Iteration: 449 \t--- Loss: 0.483\n",
      "Iteration: 450 \t--- Loss: 0.485\n",
      "Iteration: 451 \t--- Loss: 0.479\n",
      "Iteration: 452 \t--- Loss: 0.500\n",
      "Iteration: 453 \t--- Loss: 0.465\n",
      "Iteration: 454 \t--- Loss: 0.504\n",
      "Iteration: 455 \t--- Loss: 0.481\n",
      "Iteration: 456 \t--- Loss: 0.493\n",
      "Iteration: 457 \t--- Loss: 0.465\n",
      "Iteration: 458 \t--- Loss: 0.465\n",
      "Iteration: 459 \t--- Loss: 0.469\n",
      "Iteration: 460 \t--- Loss: 0.496\n",
      "Iteration: 461 \t--- Loss: 0.489\n",
      "Iteration: 462 \t--- Loss: 0.470\n",
      "Iteration: 463 \t--- Loss: 0.474\n",
      "Iteration: 464 \t--- Loss: 0.478\n",
      "Iteration: 465 \t--- Loss: 0.490\n",
      "Iteration: 466 \t--- Loss: 0.482\n",
      "Iteration: 467 \t--- Loss: 0.468\n",
      "Iteration: 468 \t--- Loss: 0.486\n",
      "Iteration: 469 \t--- Loss: 0.489\n",
      "Iteration: 470 \t--- Loss: 0.476\n",
      "Iteration: 471 \t--- Loss: 0.478\n",
      "Iteration: 472 \t--- Loss: 0.479\n",
      "Iteration: 473 \t--- Loss: 0.461\n",
      "Iteration: 474 \t--- Loss: 0.470\n",
      "Iteration: 475 \t--- Loss: 0.491\n",
      "Iteration: 476 \t--- Loss: 0.474\n",
      "Iteration: 477 \t--- Loss: 0.460\n",
      "Iteration: 478 \t--- Loss: 0.501\n",
      "Iteration: 479 \t--- Loss: 0.471\n",
      "Iteration: 480 \t--- Loss: 0.494\n",
      "Iteration: 481 \t--- Loss: 0.516\n",
      "Iteration: 482 \t--- Loss: 0.459\n",
      "Iteration: 483 \t--- Loss: 0.490\n",
      "Iteration: 484 \t--- Loss: 0.464\n",
      "Iteration: 485 \t--- Loss: 0.471\n",
      "Iteration: 486 \t--- Loss: 0.466\n",
      "Iteration: 487 \t--- Loss: 0.489\n",
      "Iteration: 488 \t--- Loss: 0.491\n",
      "Iteration: 489 \t--- Loss: 0.492\n",
      "Iteration: 490 \t--- Loss: 0.477\n",
      "Iteration: 491 \t--- Loss: 0.492\n",
      "Iteration: 492 \t--- Loss: 0.482\n",
      "Iteration: 493 \t--- Loss: 0.473\n",
      "Iteration: 494 \t--- Loss: 0.506\n",
      "Iteration: 495 \t--- Loss: 0.504\n",
      "Iteration: 496 \t--- Loss: 0.470\n",
      "Iteration: 497 \t--- Loss: 0.481\n",
      "Iteration: 498 \t--- Loss: 0.488\n",
      "Iteration: 499 \t--- Loss: 0.482\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it][Parallel(n_jobs=5)]: Done  24 tasks      | elapsed: 12.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.62s/it][Parallel(n_jobs=5)]: Done  25 tasks      | elapsed: 12.7min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.947\n",
      "Iteration: 1 \t--- Loss: 0.843\n",
      "Iteration: 2 \t--- Loss: 0.812\n",
      "Iteration: 3 \t--- Loss: 0.736\n",
      "Iteration: 4 \t--- Loss: 0.694\n",
      "Iteration: 5 \t--- Loss: 0.663\n",
      "Iteration: 6 \t--- Loss: 0.632\n",
      "Iteration: 7 \t--- Loss: 0.593\n",
      "Iteration: 8 \t--- Loss: 0.620\n",
      "Iteration: 9 \t--- Loss: 0.571\n",
      "Iteration: 10 \t--- Loss: 0.572\n",
      "Iteration: 11 \t--- Loss: 0.566\n",
      "Iteration: 12 \t--- Loss: 0.554\n",
      "Iteration: 13 \t--- Loss: 0.532\n",
      "Iteration: 14 \t--- Loss: 0.540\n",
      "Iteration: 15 \t--- Loss: 0.529\n",
      "Iteration: 16 \t--- Loss: 0.550\n",
      "Iteration: 17 \t--- Loss: 0.524\n",
      "Iteration: 18 \t--- Loss: 0.542\n",
      "Iteration: 19 \t--- Loss: 0.521\n",
      "Iteration: 20 \t--- Loss: 0.508\n",
      "Iteration: 21 \t--- Loss: 0.519\n",
      "Iteration: 22 \t--- Loss: 0.505\n",
      "Iteration: 23 \t--- Loss: 0.512\n",
      "Iteration: 24 \t--- Loss: 0.500\n",
      "Iteration: 25 \t--- Loss: 0.513\n",
      "Iteration: 26 \t--- Loss: 0.509\n",
      "Iteration: 27 \t--- Loss: 0.522\n",
      "Iteration: 28 \t--- Loss: 0.518\n",
      "Iteration: 29 \t--- Loss: 0.506\n",
      "Iteration: 30 \t--- Loss: 0.514\n",
      "Iteration: 31 \t--- Loss: 0.500\n",
      "Iteration: 32 \t--- Loss: 0.521\n",
      "Iteration: 33 \t--- Loss: 0.502\n",
      "Iteration: 34 \t--- Loss: 0.515\n",
      "Iteration: 35 \t--- Loss: 0.518\n",
      "Iteration: 36 \t--- Loss: 0.510\n",
      "Iteration: 37 \t--- Loss: 0.498\n",
      "Iteration: 38 \t--- Loss: 0.498\n",
      "Iteration: 39 \t--- Loss: 0.521\n",
      "Iteration: 40 \t--- Loss: 0.507\n",
      "Iteration: 41 \t--- Loss: 0.501\n",
      "Iteration: 42 \t--- Loss: 0.490\n",
      "Iteration: 43 \t--- Loss: 0.527\n",
      "Iteration: 44 \t--- Loss: 0.513\n",
      "Iteration: 45 \t--- Loss: 0.492\n",
      "Iteration: 46 \t--- Loss: 0.498\n",
      "Iteration: 47 \t--- Loss: 0.493\n",
      "Iteration: 48 \t--- Loss: 0.494\n",
      "Iteration: 49 \t--- Loss: 0.496\n",
      "Iteration: 50 \t--- Loss: 0.495\n",
      "Iteration: 51 \t--- Loss: 0.498\n",
      "Iteration: 52 \t--- Loss: 0.510\n",
      "Iteration: 53 \t--- Loss: 0.513\n",
      "Iteration: 54 \t--- Loss: 0.530\n",
      "Iteration: 55 \t--- Loss: 0.501\n",
      "Iteration: 56 \t--- Loss: 0.520\n",
      "Iteration: 57 \t--- Loss: 0.508\n",
      "Iteration: 58 \t--- Loss: 0.520\n",
      "Iteration: 59 \t--- Loss: 0.533\n",
      "Iteration: 60 \t--- Loss: 0.532\n",
      "Iteration: 61 \t--- Loss: 0.495\n",
      "Iteration: 62 \t--- Loss: 0.512\n",
      "Iteration: 63 \t--- Loss: 0.516\n",
      "Iteration: 64 \t--- Loss: 0.511\n",
      "Iteration: 65 \t--- Loss: 0.516\n",
      "Iteration: 66 \t--- Loss: 0.499\n",
      "Iteration: 67 \t--- Loss: 0.512\n",
      "Iteration: 68 \t--- Loss: 0.486\n",
      "Iteration: 69 \t--- Loss: 0.506\n",
      "Iteration: 70 \t--- Loss: 0.506\n",
      "Iteration: 71 \t--- Loss: 0.491\n",
      "Iteration: 72 \t--- Loss: 0.502\n",
      "Iteration: 73 \t--- Loss: 0.502\n",
      "Iteration: 74 \t--- Loss: 0.496\n",
      "Iteration: 75 \t--- Loss: 0.495\n",
      "Iteration: 76 \t--- Loss: 0.527\n",
      "Iteration: 77 \t--- Loss: 0.499\n",
      "Iteration: 78 \t--- Loss: 0.501\n",
      "Iteration: 79 \t--- Loss: 0.495\n",
      "Iteration: 80 \t--- Loss: 0.517\n",
      "Iteration: 81 \t--- Loss: 0.506\n",
      "Iteration: 82 \t--- Loss: 0.496\n",
      "Iteration: 83 \t--- Loss: 0.494\n",
      "Iteration: 84 \t--- Loss: 0.520\n",
      "Iteration: 85 \t--- Loss: 0.517\n",
      "Iteration: 86 \t--- Loss: 0.511\n",
      "Iteration: 87 \t--- Loss: 0.491\n",
      "Iteration: 88 \t--- Loss: 0.502\n",
      "Iteration: 89 \t--- Loss: 0.495\n",
      "Iteration: 90 \t--- Loss: 0.516\n",
      "Iteration: 91 \t--- Loss: 0.519\n",
      "Iteration: 92 \t--- Loss: 0.493\n",
      "Iteration: 93 \t--- Loss: 0.493\n",
      "Iteration: 94 \t--- Loss: 0.487\n",
      "Iteration: 95 \t--- Loss: 0.536\n",
      "Iteration: 96 \t--- Loss: 0.499\n",
      "Iteration: 97 \t--- Loss: 0.490\n",
      "Iteration: 98 \t--- Loss: 0.507\n",
      "Iteration: 99 \t--- Loss: 0.525\n",
      "Iteration: 100 \t--- Loss: 0.503\n",
      "Iteration: 101 \t--- Loss: 0.521\n",
      "Iteration: 102 \t--- Loss: 0.505\n",
      "Iteration: 103 \t--- Loss: 0.510\n",
      "Iteration: 104 \t--- Loss: 0.515\n",
      "Iteration: 105 \t--- Loss: 0.512\n",
      "Iteration: 106 \t--- Loss: 0.488\n",
      "Iteration: 107 \t--- Loss: 0.513\n",
      "Iteration: 108 \t--- Loss: 0.512\n",
      "Iteration: 109 \t--- Loss: 0.491\n",
      "Iteration: 110 \t--- Loss: 0.528\n",
      "Iteration: 111 \t--- Loss: 0.532\n",
      "Iteration: 112 \t--- Loss: 0.523\n",
      "Iteration: 113 \t--- Loss: 0.508\n",
      "Iteration: 114 \t--- Loss: 0.497\n",
      "Iteration: 115 \t--- Loss: 0.491\n",
      "Iteration: 116 \t--- Loss: 0.492\n",
      "Iteration: 117 \t--- Loss: 0.506\n",
      "Iteration: 118 \t--- Loss: 0.503\n",
      "Iteration: 119 \t--- Loss: 0.503\n",
      "Iteration: 120 \t--- Loss: 0.516\n",
      "Iteration: 121 \t--- Loss: 0.502\n",
      "Iteration: 122 \t--- Loss: 0.530\n",
      "Iteration: 123 \t--- Loss: 0.518\n",
      "Iteration: 124 \t--- Loss: 0.497\n",
      "Iteration: 125 \t--- Loss: 0.508\n",
      "Iteration: 126 \t--- Loss: 0.511\n",
      "Iteration: 127 \t--- Loss: 0.518\n",
      "Iteration: 128 \t--- Loss: 0.526\n",
      "Iteration: 129 \t--- Loss: 0.500\n",
      "Iteration: 130 \t--- Loss: 0.508\n",
      "Iteration: 131 \t--- Loss: 0.509\n",
      "Iteration: 132 \t--- Loss: 0.495\n",
      "Iteration: 133 \t--- Loss: 0.496\n",
      "Iteration: 134 \t--- Loss: 0.499\n",
      "Iteration: 135 \t--- Loss: 0.486\n",
      "Iteration: 136 \t--- Loss: 0.497\n",
      "Iteration: 137 \t--- Loss: 0.508\n",
      "Iteration: 138 \t--- Loss: 0.497\n",
      "Iteration: 139 \t--- Loss: 0.496\n",
      "Iteration: 140 \t--- Loss: 0.507\n",
      "Iteration: 141 \t--- Loss: 0.502\n",
      "Iteration: 142 \t--- Loss: 0.504\n",
      "Iteration: 143 \t--- Loss: 0.515\n",
      "Iteration: 144 \t--- Loss: 0.515\n",
      "Iteration: 145 \t--- Loss: 0.512\n",
      "Iteration: 146 \t--- Loss: 0.498\n",
      "Iteration: 147 \t--- Loss: 0.517\n",
      "Iteration: 148 \t--- Loss: 0.512\n",
      "Iteration: 149 \t--- Loss: 0.488\n",
      "Iteration: 150 \t--- Loss: 0.501\n",
      "Iteration: 151 \t--- Loss: 0.494\n",
      "Iteration: 152 \t--- Loss: 0.534\n",
      "Iteration: 153 \t--- Loss: 0.502\n",
      "Iteration: 154 \t--- Loss: 0.511\n",
      "Iteration: 155 \t--- Loss: 0.507\n",
      "Iteration: 156 \t--- Loss: 0.529\n",
      "Iteration: 157 \t--- Loss: 0.504\n",
      "Iteration: 158 \t--- Loss: 0.508\n",
      "Iteration: 159 \t--- Loss: 0.530\n",
      "Iteration: 160 \t--- Loss: 0.493\n",
      "Iteration: 161 \t--- Loss: 0.521\n",
      "Iteration: 162 \t--- Loss: 0.507\n",
      "Iteration: 163 \t--- Loss: 0.527\n",
      "Iteration: 164 \t--- Loss: 0.507\n",
      "Iteration: 165 \t--- Loss: 0.512\n",
      "Iteration: 166 \t--- Loss: 0.503\n",
      "Iteration: 167 \t--- Loss: 0.514\n",
      "Iteration: 168 \t--- Loss: 0.490\n",
      "Iteration: 169 \t--- Loss: 0.540\n",
      "Iteration: 170 \t--- Loss: 0.496\n",
      "Iteration: 171 \t--- Loss: 0.507\n",
      "Iteration: 172 \t--- Loss: 0.511\n",
      "Iteration: 173 \t--- Loss: 0.483\n",
      "Iteration: 174 \t--- Loss: 0.531\n",
      "Iteration: 175 \t--- Loss: 0.496\n",
      "Iteration: 176 \t--- Loss: 0.523\n",
      "Iteration: 177 \t--- Loss: 0.499\n",
      "Iteration: 178 \t--- Loss: 0.496\n",
      "Iteration: 179 \t--- Loss: 0.510\n",
      "Iteration: 180 \t--- Loss: 0.503\n",
      "Iteration: 181 \t--- Loss: 0.510\n",
      "Iteration: 182 \t--- Loss: 0.502\n",
      "Iteration: 183 \t--- Loss: 0.509\n",
      "Iteration: 184 \t--- Loss: 0.502\n",
      "Iteration: 185 \t--- Loss: 0.529\n",
      "Iteration: 186 \t--- Loss: 0.514\n",
      "Iteration: 187 \t--- Loss: 0.493\n",
      "Iteration: 188 \t--- Loss: 0.485\n",
      "Iteration: 189 \t--- Loss: 0.504\n",
      "Iteration: 190 \t--- Loss: 0.503\n",
      "Iteration: 191 \t--- Loss: 0.501\n",
      "Iteration: 192 \t--- Loss: 0.509\n",
      "Iteration: 193 \t--- Loss: 0.507\n",
      "Iteration: 194 \t--- Loss: 0.530\n",
      "Iteration: 195 \t--- Loss: 0.508\n",
      "Iteration: 196 \t--- Loss: 0.516\n",
      "Iteration: 197 \t--- Loss: 0.525\n",
      "Iteration: 198 \t--- Loss: 0.500\n",
      "Iteration: 199 \t--- Loss: 0.520\n",
      "Iteration: 200 \t--- Loss: 0.488\n",
      "Iteration: 201 \t--- Loss: 0.509\n",
      "Iteration: 202 \t--- Loss: 0.479\n",
      "Iteration: 203 \t--- Loss: 0.514\n",
      "Iteration: 204 \t--- Loss: 0.532\n",
      "Iteration: 205 \t--- Loss: 0.504\n",
      "Iteration: 206 \t--- Loss: 0.505\n",
      "Iteration: 207 \t--- Loss: 0.496\n",
      "Iteration: 208 \t--- Loss: 0.514\n",
      "Iteration: 209 \t--- Loss: 0.503\n",
      "Iteration: 210 \t--- Loss: 0.496\n",
      "Iteration: 211 \t--- Loss: 0.516\n",
      "Iteration: 212 \t--- Loss: 0.498\n",
      "Iteration: 213 \t--- Loss: 0.488\n",
      "Iteration: 214 \t--- Loss: 0.519\n",
      "Iteration: 215 \t--- Loss: 0.518\n",
      "Iteration: 216 \t--- Loss: 0.525\n",
      "Iteration: 217 \t--- Loss: 0.500\n",
      "Iteration: 218 \t--- Loss: 0.507\n",
      "Iteration: 219 \t--- Loss: 0.524\n",
      "Iteration: 220 \t--- Loss: 0.528\n",
      "Iteration: 221 \t--- Loss: 0.501\n",
      "Iteration: 222 \t--- Loss: 0.503\n",
      "Iteration: 223 \t--- Loss: 0.504\n",
      "Iteration: 224 \t--- Loss: 0.512\n",
      "Iteration: 225 \t--- Loss: 0.512\n",
      "Iteration: 226 \t--- Loss: 0.516\n",
      "Iteration: 227 \t--- Loss: 0.478\n",
      "Iteration: 228 \t--- Loss: 0.493\n",
      "Iteration: 229 \t--- Loss: 0.500\n",
      "Iteration: 230 \t--- Loss: 0.482\n",
      "Iteration: 231 \t--- Loss: 0.507\n",
      "Iteration: 232 \t--- Loss: 0.508\n",
      "Iteration: 233 \t--- Loss: 0.502\n",
      "Iteration: 234 \t--- Loss: 0.500\n",
      "Iteration: 235 \t--- Loss: 0.491\n",
      "Iteration: 236 \t--- Loss: 0.516\n",
      "Iteration: 237 \t--- Loss: 0.512\n",
      "Iteration: 238 \t--- Loss: 0.521\n",
      "Iteration: 239 \t--- Loss: 0.516\n",
      "Iteration: 240 \t--- Loss: 0.508\n",
      "Iteration: 241 \t--- Loss: 0.483\n",
      "Iteration: 242 \t--- Loss: 0.497\n",
      "Iteration: 243 \t--- Loss: 0.505\n",
      "Iteration: 244 \t--- Loss: 0.485\n",
      "Iteration: 245 \t--- Loss: 0.512\n",
      "Iteration: 246 \t--- Loss: 0.512\n",
      "Iteration: 247 \t--- Loss: 0.497\n",
      "Iteration: 248 \t--- Loss: 0.521\n",
      "Iteration: 249 \t--- Loss: 0.507\n",
      "Iteration: 250 \t--- Loss: 0.495\n",
      "Iteration: 251 \t--- Loss: 0.511\n",
      "Iteration: 252 \t--- Loss: 0.522\n",
      "Iteration: 253 \t--- Loss: 0.500\n",
      "Iteration: 254 \t--- Loss: 0.519\n",
      "Iteration: 255 \t--- Loss: 0.507\n",
      "Iteration: 256 \t--- Loss: 0.483\n",
      "Iteration: 257 \t--- Loss: 0.519\n",
      "Iteration: 258 \t--- Loss: 0.489\n",
      "Iteration: 259 \t--- Loss: 0.541Iteration: 0 \t--- Loss: 1.105\n",
      "Iteration: 1 \t--- Loss: 1.075\n",
      "Iteration: 2 \t--- Loss: 0.966\n",
      "Iteration: 3 \t--- Loss: 0.893\n",
      "Iteration: 4 \t--- Loss: 0.847\n",
      "Iteration: 5 \t--- Loss: 0.803\n",
      "Iteration: 6 \t--- Loss: 0.753\n",
      "Iteration: 7 \t--- Loss: 0.737\n",
      "Iteration: 8 \t--- Loss: 0.722\n",
      "Iteration: 9 \t--- Loss: 0.706\n",
      "Iteration: 10 \t--- Loss: 0.695\n",
      "Iteration: 11 \t--- Loss: 0.678\n",
      "Iteration: 12 \t--- Loss: 0.682\n",
      "Iteration: 13 \t--- Loss: 0.672\n",
      "Iteration: 14 \t--- Loss: 0.667\n",
      "Iteration: 15 \t--- Loss: 0.663\n",
      "Iteration: 16 \t--- Loss: 0.657\n",
      "Iteration: 17 \t--- Loss: 0.656\n",
      "Iteration: 18 \t--- Loss: 0.657\n",
      "Iteration: 19 \t--- Loss: 0.653\n",
      "Iteration: 20 \t--- Loss: 0.650\n",
      "Iteration: 21 \t--- Loss: 0.650\n",
      "Iteration: 22 \t--- Loss: 0.646\n",
      "Iteration: 23 \t--- Loss: 0.648\n",
      "Iteration: 24 \t--- Loss: 0.646\n",
      "Iteration: 25 \t--- Loss: 0.646\n",
      "Iteration: 26 \t--- Loss: 0.640\n",
      "Iteration: 27 \t--- Loss: 0.641\n",
      "Iteration: 28 \t--- Loss: 0.641\n",
      "Iteration: 29 \t--- Loss: 0.640\n",
      "Iteration: 30 \t--- Loss: 0.643\n",
      "Iteration: 31 \t--- Loss: 0.640\n",
      "Iteration: 32 \t--- Loss: 0.639\n",
      "Iteration: 33 \t--- Loss: 0.635\n",
      "Iteration: 34 \t--- Loss: 0.639\n",
      "Iteration: 35 \t--- Loss: 0.642\n",
      "Iteration: 36 \t--- Loss: 0.637\n",
      "Iteration: 37 \t--- Loss: 0.641\n",
      "Iteration: 38 \t--- Loss: 0.641\n",
      "Iteration: 39 \t--- Loss: 0.639\n",
      "Iteration: 40 \t--- Loss: 0.638\n",
      "Iteration: 41 \t--- Loss: 0.637\n",
      "Iteration: 42 \t--- Loss: 0.639\n",
      "Iteration: 43 \t--- Loss: 0.639\n",
      "Iteration: 44 \t--- Loss: 0.637\n",
      "Iteration: 45 \t--- Loss: 0.638\n",
      "Iteration: 46 \t--- Loss: 0.640\n",
      "Iteration: 47 \t--- Loss: 0.639\n",
      "Iteration: 48 \t--- Loss: 0.636\n",
      "Iteration: 49 \t--- Loss: 0.640\n",
      "Iteration: 50 \t--- Loss: 0.636\n",
      "Iteration: 51 \t--- Loss: 0.639\n",
      "Iteration: 52 \t--- Loss: 0.638\n",
      "Iteration: 53 \t--- Loss: 0.637\n",
      "Iteration: 54 \t--- Loss: 0.637\n",
      "Iteration: 55 \t--- Loss: 0.638\n",
      "Iteration: 56 \t--- Loss: 0.640\n",
      "Iteration: 57 \t--- Loss: 0.638\n",
      "Iteration: 58 \t--- Loss: 0.638\n",
      "Iteration: 59 \t--- Loss: 0.637\n",
      "Iteration: 60 \t--- Loss: 0.636\n",
      "Iteration: 61 \t--- Loss: 0.637\n",
      "Iteration: 62 \t--- Loss: 0.637\n",
      "Iteration: 63 \t--- Loss: 0.639\n",
      "Iteration: 64 \t--- Loss: 0.638\n",
      "Iteration: 65 \t--- Loss: 0.636\n",
      "Iteration: 66 \t--- Loss: 0.636\n",
      "Iteration: 67 \t--- Loss: 0.639\n",
      "Iteration: 68 \t--- Loss: 0.638\n",
      "Iteration: 69 \t--- Loss: 0.636\n",
      "Iteration: 70 \t--- Loss: 0.640\n",
      "Iteration: 71 \t--- Loss: 0.637\n",
      "Iteration: 72 \t--- Loss: 0.638\n",
      "Iteration: 73 \t--- Loss: 0.636\n",
      "Iteration: 74 \t--- Loss: 0.637\n",
      "Iteration: 75 \t--- Loss: 0.638\n",
      "Iteration: 76 \t--- Loss: 0.636\n",
      "Iteration: 77 \t--- Loss: 0.638\n",
      "Iteration: 78 \t--- Loss: 0.637\n",
      "Iteration: 79 \t--- Loss: 0.637\n",
      "Iteration: 80 \t--- Loss: 0.637\n",
      "Iteration: 81 \t--- Loss: 0.637\n",
      "Iteration: 82 \t--- Loss: 0.637\n",
      "Iteration: 83 \t--- Loss: 0.638\n",
      "Iteration: 84 \t--- Loss: 0.636\n",
      "Iteration: 85 \t--- Loss: 0.636\n",
      "Iteration: 86 \t--- Loss: 0.637\n",
      "Iteration: 87 \t--- Loss: 0.637\n",
      "Iteration: 88 \t--- Loss: 0.636\n",
      "Iteration: 89 \t--- Loss: 0.638\n",
      "Iteration: 90 \t--- Loss: 0.637\n",
      "Iteration: 91 \t--- Loss: 0.635\n",
      "Iteration: 92 \t--- Loss: 0.637\n",
      "Iteration: 93 \t--- Loss: 0.640\n",
      "Iteration: 94 \t--- Loss: 0.634\n",
      "Iteration: 95 \t--- Loss: 0.637\n",
      "Iteration: 96 \t--- Loss: 0.637\n",
      "Iteration: 97 \t--- Loss: 0.639\n",
      "Iteration: 98 \t--- Loss: 0.637\n",
      "Iteration: 99 \t--- Loss: 0.637\n",
      "Iteration: 100 \t--- Loss: 0.639\n",
      "Iteration: 101 \t--- Loss: 0.635\n",
      "Iteration: 102 \t--- Loss: 0.635\n",
      "Iteration: 103 \t--- Loss: 0.639\n",
      "Iteration: 104 \t--- Loss: 0.638\n",
      "Iteration: 105 \t--- Loss: 0.637\n",
      "Iteration: 106 \t--- Loss: 0.638\n",
      "Iteration: 107 \t--- Loss: 0.636\n",
      "Iteration: 108 \t--- Loss: 0.636\n",
      "Iteration: 109 \t--- Loss: 0.637\n",
      "Iteration: 110 \t--- Loss: 0.638\n",
      "Iteration: 111 \t--- Loss: 0.635\n",
      "Iteration: 112 \t--- Loss: 0.638\n",
      "Iteration: 113 \t--- Loss: 0.637\n",
      "Iteration: 114 \t--- Loss: 0.639\n",
      "Iteration: 115 \t--- Loss: 0.638\n",
      "Iteration: 116 \t--- Loss: 0.639\n",
      "Iteration: 117 \t--- Loss: 0.638\n",
      "Iteration: 118 \t--- Loss: 0.640\n",
      "Iteration: 119 \t--- Loss: 0.641\n",
      "Iteration: 120 \t--- Loss: 0.638\n",
      "Iteration: 121 \t--- Loss: 0.634\n",
      "Iteration: 122 \t--- Loss: 0.638\n",
      "Iteration: 123 \t--- Loss: 0.641\n",
      "Iteration: 124 \t--- Loss: 0.635\n",
      "Iteration: 125 \t--- Loss: 0.640\n",
      "Iteration: 126 \t--- Loss: 0.636\n",
      "Iteration: 127 \t--- Loss: 0.636\n",
      "Iteration: 128 \t--- Loss: 0.639\n",
      "Iteration: 129 \t--- Loss: 0.637\n",
      "Iteration: 130 \t--- Loss: 0.639\n",
      "Iteration: 131 \t--- Loss: 0.637\n",
      "Iteration: 132 \t--- Loss: 0.636\n",
      "Iteration: 133 \t--- Loss: 0.639\n",
      "Iteration: 134 \t--- Loss: 0.640\n",
      "Iteration: 135 \t--- Loss: 0.638\n",
      "Iteration: 136 \t--- Loss: 0.637\n",
      "Iteration: 137 \t--- Loss: 0.639\n",
      "Iteration: 138 \t--- Loss: 0.636\n",
      "Iteration: 139 \t--- Loss: 0.640\n",
      "Iteration: 140 \t--- Loss: 0.639\n",
      "Iteration: 141 \t--- Loss: 0.638\n",
      "Iteration: 142 \t--- Loss: 0.636\n",
      "Iteration: 143 \t--- Loss: 0.638\n",
      "Iteration: 144 \t--- Loss: 0.633\n",
      "Iteration: 145 \t--- Loss: 0.637\n",
      "Iteration: 146 \t--- Loss: 0.635\n",
      "Iteration: 147 \t--- Loss: 0.638\n",
      "Iteration: 148 \t--- Loss: 0.639\n",
      "Iteration: 149 \t--- Loss: 0.637\n",
      "Iteration: 150 \t--- Loss: 0.638\n",
      "Iteration: 151 \t--- Loss: 0.638\n",
      "Iteration: 152 \t--- Loss: 0.638\n",
      "Iteration: 153 \t--- Loss: 0.639\n",
      "Iteration: 154 \t--- Loss: 0.636\n",
      "Iteration: 155 \t--- Loss: 0.635\n",
      "Iteration: 156 \t--- Loss: 0.640\n",
      "Iteration: 157 \t--- Loss: 0.636\n",
      "Iteration: 158 \t--- Loss: 0.638\n",
      "Iteration: 159 \t--- Loss: 0.635\n",
      "Iteration: 160 \t--- Loss: 0.633\n",
      "Iteration: 161 \t--- Loss: 0.638\n",
      "Iteration: 162 \t--- Loss: 0.638\n",
      "Iteration: 163 \t--- Loss: 0.638\n",
      "Iteration: 164 \t--- Loss: 0.635\n",
      "Iteration: 165 \t--- Loss: 0.634\n",
      "Iteration: 166 \t--- Loss: 0.633\n",
      "Iteration: 167 \t--- Loss: 0.638\n",
      "Iteration: 168 \t--- Loss: 0.640\n",
      "Iteration: 169 \t--- Loss: 0.638\n",
      "Iteration: 170 \t--- Loss: 0.635\n",
      "Iteration: 171 \t--- Loss: 0.639\n",
      "Iteration: 172 \t--- Loss: 0.639\n",
      "Iteration: 173 \t--- Loss: 0.639\n",
      "Iteration: 174 \t--- Loss: 0.639\n",
      "Iteration: 175 \t--- Loss: 0.636\n",
      "Iteration: 176 \t--- Loss: 0.639\n",
      "Iteration: 177 \t--- Loss: 0.638\n",
      "Iteration: 178 \t--- Loss: 0.635\n",
      "Iteration: 179 \t--- Loss: 0.638\n",
      "Iteration: 180 \t--- Loss: 0.638\n",
      "Iteration: 181 \t--- Loss: 0.635\n",
      "Iteration: 182 \t--- Loss: 0.637\n",
      "Iteration: 183 \t--- Loss: 0.635\n",
      "Iteration: 184 \t--- Loss: 0.637\n",
      "Iteration: 185 \t--- Loss: 0.637\n",
      "Iteration: 186 \t--- Loss: 0.639\n",
      "Iteration: 187 \t--- Loss: 0.637\n",
      "Iteration: 188 \t--- Loss: 0.640\n",
      "Iteration: 189 \t--- Loss: 0.639\n",
      "Iteration: 190 \t--- Loss: 0.637\n",
      "Iteration: 191 \t--- Loss: 0.636\n",
      "Iteration: 192 \t--- Loss: 0.640\n",
      "Iteration: 193 \t--- Loss: 0.634\n",
      "Iteration: 194 \t--- Loss: 0.641\n",
      "Iteration: 195 \t--- Loss: 0.638\n",
      "Iteration: 196 \t--- Loss: 0.636\n",
      "Iteration: 197 \t--- Loss: 0.638\n",
      "Iteration: 198 \t--- Loss: 0.636\n",
      "Iteration: 199 \t--- Loss: 0.636\n",
      "Iteration: 200 \t--- Loss: 0.637\n",
      "Iteration: 201 \t--- Loss: 0.638\n",
      "Iteration: 202 \t--- Loss: 0.639\n",
      "Iteration: 203 \t--- Loss: 0.635\n",
      "Iteration: 204 \t--- Loss: 0.642\n",
      "Iteration: 205 \t--- Loss: 0.638\n",
      "Iteration: 206 \t--- Loss: 0.635\n",
      "Iteration: 207 \t--- Loss: 0.637\n",
      "Iteration: 208 \t--- Loss: 0.635\n",
      "Iteration: 209 \t--- Loss: 0.637\n",
      "Iteration: 210 \t--- Loss: 0.637\n",
      "Iteration: 211 \t--- Loss: 0.636\n",
      "Iteration: 212 \t--- Loss: 0.640\n",
      "Iteration: 213 \t--- Loss: 0.638\n",
      "Iteration: 214 \t--- Loss: 0.637\n",
      "Iteration: 215 \t--- Loss: 0.641\n",
      "Iteration: 216 \t--- Loss: 0.639\n",
      "Iteration: 217 \t--- Loss: 0.639\n",
      "Iteration: 218 \t--- Loss: 0.636\n",
      "Iteration: 219 \t--- Loss: 0.638\n",
      "Iteration: 220 \t--- Loss: 0.637\n",
      "Iteration: 221 \t--- Loss: 0.636\n",
      "Iteration: 222 \t--- Loss: 0.637\n",
      "Iteration: 223 \t--- Loss: 0.637\n",
      "Iteration: 224 \t--- Loss: 0.638\n",
      "Iteration: 225 \t--- Loss: 0.636\n",
      "Iteration: 226 \t--- Loss: 0.638\n",
      "Iteration: 227 \t--- Loss: 0.640\n",
      "Iteration: 228 \t--- Loss: 0.639\n",
      "Iteration: 229 \t--- Loss: 0.635\n",
      "Iteration: 230 \t--- Loss: 0.640\n",
      "Iteration: 231 \t--- Loss: 0.638\n",
      "Iteration: 232 \t--- Loss: 0.638\n",
      "Iteration: 233 \t--- Loss: 0.636\n",
      "Iteration: 234 \t--- Loss: 0.639\n",
      "Iteration: 235 \t--- Loss: 0.638\n",
      "Iteration: 236 \t--- Loss: 0.636\n",
      "Iteration: 237 \t--- Loss: 0.637\n",
      "Iteration: 238 \t--- Loss: 0.638\n",
      "Iteration: 239 \t--- Loss: 0.638\n",
      "Iteration: 240 \t--- Loss: 0.638\n",
      "Iteration: 241 \t--- Loss: 0.638\n",
      "Iteration: 242 \t--- Loss: 0.636\n",
      "Iteration: 243 \t--- Loss: 0.635\n",
      "Iteration: 244 \t--- Loss: 0.636\n",
      "Iteration: 245 \t--- Loss: 0.640\n",
      "Iteration: 246 \t--- Loss: 0.639\n",
      "Iteration: 247 \t--- Loss: 0.636\n",
      "Iteration: 248 \t--- Loss: 0.637\n",
      "Iteration: 249 \t--- Loss: 0.635\n",
      "Iteration: 250 \t--- Loss: 0.639\n",
      "Iteration: 251 \t--- Loss: 0.638\n",
      "Iteration: 252 \t--- Loss: 0.641\n",
      "Iteration: 253 \t--- Loss: 0.641\n",
      "Iteration: 254 \t--- Loss: 0.635\n",
      "Iteration: 255 \t--- Loss: 0.637\n",
      "Iteration: 256 \t--- Loss: 0.636\n",
      "Iteration: 257 \t--- Loss: 0.636\n",
      "Iteration: 258 \t--- Loss: 0.637\n",
      "Iteration: 259 \t--- Loss: 0.638Iteration: 0 \t--- Loss: 0.684\n",
      "Iteration: 1 \t--- Loss: 0.636\n",
      "Iteration: 2 \t--- Loss: 0.574\n",
      "Iteration: 3 \t--- Loss: 0.540\n",
      "Iteration: 4 \t--- Loss: 0.509\n",
      "Iteration: 5 \t--- Loss: 0.474\n",
      "Iteration: 6 \t--- Loss: 0.454\n",
      "Iteration: 7 \t--- Loss: 0.426\n",
      "Iteration: 8 \t--- Loss: 0.419\n",
      "Iteration: 9 \t--- Loss: 0.402\n",
      "Iteration: 10 \t--- Loss: 0.390\n",
      "Iteration: 11 \t--- Loss: 0.376\n",
      "Iteration: 12 \t--- Loss: 0.366\n",
      "Iteration: 13 \t--- Loss: 0.357\n",
      "Iteration: 14 \t--- Loss: 0.360\n",
      "Iteration: 15 \t--- Loss: 0.347\n",
      "Iteration: 16 \t--- Loss: 0.354\n",
      "Iteration: 17 \t--- Loss: 0.349\n",
      "Iteration: 18 \t--- Loss: 0.352\n",
      "Iteration: 19 \t--- Loss: 0.343\n",
      "Iteration: 20 \t--- Loss: 0.336\n",
      "Iteration: 21 \t--- Loss: 0.341\n",
      "Iteration: 22 \t--- Loss: 0.344\n",
      "Iteration: 23 \t--- Loss: 0.339\n",
      "Iteration: 24 \t--- Loss: 0.335\n",
      "Iteration: 25 \t--- Loss: 0.331\n",
      "Iteration: 26 \t--- Loss: 0.324\n",
      "Iteration: 27 \t--- Loss: 0.332\n",
      "Iteration: 28 \t--- Loss: 0.322\n",
      "Iteration: 29 \t--- Loss: 0.332\n",
      "Iteration: 30 \t--- Loss: 0.323\n",
      "Iteration: 31 \t--- Loss: 0.332\n",
      "Iteration: 32 \t--- Loss: 0.329\n",
      "Iteration: 33 \t--- Loss: 0.341\n",
      "Iteration: 34 \t--- Loss: 0.326\n",
      "Iteration: 35 \t--- Loss: 0.323\n",
      "Iteration: 36 \t--- Loss: 0.321\n",
      "Iteration: 37 \t--- Loss: 0.333\n",
      "Iteration: 38 \t--- Loss: 0.329\n",
      "Iteration: 39 \t--- Loss: 0.319\n",
      "Iteration: 40 \t--- Loss: 0.323\n",
      "Iteration: 41 \t--- Loss: 0.335\n",
      "Iteration: 42 \t--- Loss: 0.319\n",
      "Iteration: 43 \t--- Loss: 0.328\n",
      "Iteration: 44 \t--- Loss: 0.318\n",
      "Iteration: 45 \t--- Loss: 0.324\n",
      "Iteration: 46 \t--- Loss: 0.323\n",
      "Iteration: 47 \t--- Loss: 0.335\n",
      "Iteration: 48 \t--- Loss: 0.332\n",
      "Iteration: 49 \t--- Loss: 0.312\n",
      "Iteration: 50 \t--- Loss: 0.315\n",
      "Iteration: 51 \t--- Loss: 0.326\n",
      "Iteration: 52 \t--- Loss: 0.328\n",
      "Iteration: 53 \t--- Loss: 0.315\n",
      "Iteration: 54 \t--- Loss: 0.330\n",
      "Iteration: 55 \t--- Loss: 0.331\n",
      "Iteration: 56 \t--- Loss: 0.330\n",
      "Iteration: 57 \t--- Loss: 0.322\n",
      "Iteration: 58 \t--- Loss: 0.318\n",
      "Iteration: 59 \t--- Loss: 0.329\n",
      "Iteration: 60 \t--- Loss: 0.327\n",
      "Iteration: 61 \t--- Loss: 0.326\n",
      "Iteration: 62 \t--- Loss: 0.317\n",
      "Iteration: 63 \t--- Loss: 0.306\n",
      "Iteration: 64 \t--- Loss: 0.316\n",
      "Iteration: 65 \t--- Loss: 0.317\n",
      "Iteration: 66 \t--- Loss: 0.306\n",
      "Iteration: 67 \t--- Loss: 0.320\n",
      "Iteration: 68 \t--- Loss: 0.318\n",
      "Iteration: 69 \t--- Loss: 0.322\n",
      "Iteration: 70 \t--- Loss: 0.327\n",
      "Iteration: 71 \t--- Loss: 0.315\n",
      "Iteration: 72 \t--- Loss: 0.338\n",
      "Iteration: 73 \t--- Loss: 0.325\n",
      "Iteration: 74 \t--- Loss: 0.322\n",
      "Iteration: 75 \t--- Loss: 0.327\n",
      "Iteration: 76 \t--- Loss: 0.323\n",
      "Iteration: 77 \t--- Loss: 0.326\n",
      "Iteration: 78 \t--- Loss: 0.321\n",
      "Iteration: 79 \t--- Loss: 0.319\n",
      "Iteration: 80 \t--- Loss: 0.330\n",
      "Iteration: 81 \t--- Loss: 0.316\n",
      "Iteration: 82 \t--- Loss: 0.317\n",
      "Iteration: 83 \t--- Loss: 0.316\n",
      "Iteration: 84 \t--- Loss: 0.318\n",
      "Iteration: 85 \t--- Loss: 0.328\n",
      "Iteration: 86 \t--- Loss: 0.315\n",
      "Iteration: 87 \t--- Loss: 0.309\n",
      "Iteration: 88 \t--- Loss: 0.336\n",
      "Iteration: 89 \t--- Loss: 0.321\n",
      "Iteration: 90 \t--- Loss: 0.317\n",
      "Iteration: 91 \t--- Loss: 0.308\n",
      "Iteration: 92 \t--- Loss: 0.329\n",
      "Iteration: 93 \t--- Loss: 0.316\n",
      "Iteration: 94 \t--- Loss: 0.311\n",
      "Iteration: 95 \t--- Loss: 0.315\n",
      "Iteration: 96 \t--- Loss: 0.307\n",
      "Iteration: 97 \t--- Loss: 0.331\n",
      "Iteration: 98 \t--- Loss: 0.324\n",
      "Iteration: 99 \t--- Loss: 0.309\n",
      "Iteration: 100 \t--- Loss: 0.327\n",
      "Iteration: 101 \t--- Loss: 0.328\n",
      "Iteration: 102 \t--- Loss: 0.324\n",
      "Iteration: 103 \t--- Loss: 0.321\n",
      "Iteration: 104 \t--- Loss: 0.309\n",
      "Iteration: 105 \t--- Loss: 0.310\n",
      "Iteration: 106 \t--- Loss: 0.314\n",
      "Iteration: 107 \t--- Loss: 0.313\n",
      "Iteration: 108 \t--- Loss: 0.324\n",
      "Iteration: 109 \t--- Loss: 0.319\n",
      "Iteration: 110 \t--- Loss: 0.316\n",
      "Iteration: 111 \t--- Loss: 0.314\n",
      "Iteration: 112 \t--- Loss: 0.324\n",
      "Iteration: 113 \t--- Loss: 0.315\n",
      "Iteration: 114 \t--- Loss: 0.326\n",
      "Iteration: 115 \t--- Loss: 0.320\n",
      "Iteration: 116 \t--- Loss: 0.321\n",
      "Iteration: 117 \t--- Loss: 0.315\n",
      "Iteration: 118 \t--- Loss: 0.310\n",
      "Iteration: 119 \t--- Loss: 0.338\n",
      "Iteration: 120 \t--- Loss: 0.309\n",
      "Iteration: 121 \t--- Loss: 0.310\n",
      "Iteration: 122 \t--- Loss: 0.315\n",
      "Iteration: 123 \t--- Loss: 0.328\n",
      "Iteration: 124 \t--- Loss: 0.315\n",
      "Iteration: 125 \t--- Loss: 0.320\n",
      "Iteration: 126 \t--- Loss: 0.330\n",
      "Iteration: 127 \t--- Loss: 0.317\n",
      "Iteration: 128 \t--- Loss: 0.319\n",
      "Iteration: 129 \t--- Loss: 0.321\n",
      "Iteration: 130 \t--- Loss: 0.321\n",
      "Iteration: 131 \t--- Loss: 0.324\n",
      "Iteration: 132 \t--- Loss: 0.320\n",
      "Iteration: 133 \t--- Loss: 0.318\n",
      "Iteration: 134 \t--- Loss: 0.312\n",
      "Iteration: 135 \t--- Loss: 0.314\n",
      "Iteration: 136 \t--- Loss: 0.326\n",
      "Iteration: 137 \t--- Loss: 0.319\n",
      "Iteration: 138 \t--- Loss: 0.325\n",
      "Iteration: 139 \t--- Loss: 0.325\n",
      "Iteration: 140 \t--- Loss: 0.318\n",
      "Iteration: 141 \t--- Loss: 0.310\n",
      "Iteration: 142 \t--- Loss: 0.325\n",
      "Iteration: 143 \t--- Loss: 0.316\n",
      "Iteration: 144 \t--- Loss: 0.326\n",
      "Iteration: 145 \t--- Loss: 0.319\n",
      "Iteration: 146 \t--- Loss: 0.326\n",
      "Iteration: 147 \t--- Loss: 0.326\n",
      "Iteration: 148 \t--- Loss: 0.323\n",
      "Iteration: 149 \t--- Loss: 0.321\n",
      "Iteration: 150 \t--- Loss: 0.325\n",
      "Iteration: 151 \t--- Loss: 0.320\n",
      "Iteration: 152 \t--- Loss: 0.319\n",
      "Iteration: 153 \t--- Loss: 0.328\n",
      "Iteration: 154 \t--- Loss: 0.321\n",
      "Iteration: 155 \t--- Loss: 0.335\n",
      "Iteration: 156 \t--- Loss: 0.312\n",
      "Iteration: 157 \t--- Loss: 0.311\n",
      "Iteration: 158 \t--- Loss: 0.319\n",
      "Iteration: 159 \t--- Loss: 0.312\n",
      "Iteration: 160 \t--- Loss: 0.327\n",
      "Iteration: 161 \t--- Loss: 0.313\n",
      "Iteration: 162 \t--- Loss: 0.322\n",
      "Iteration: 163 \t--- Loss: 0.326\n",
      "Iteration: 164 \t--- Loss: 0.317\n",
      "Iteration: 165 \t--- Loss: 0.322\n",
      "Iteration: 166 \t--- Loss: 0.312\n",
      "Iteration: 167 \t--- Loss: 0.326\n",
      "Iteration: 168 \t--- Loss: 0.324\n",
      "Iteration: 169 \t--- Loss: 0.334\n",
      "Iteration: 170 \t--- Loss: 0.345\n",
      "Iteration: 171 \t--- Loss: 0.327\n",
      "Iteration: 172 \t--- Loss: 0.316\n",
      "Iteration: 173 \t--- Loss: 0.317\n",
      "Iteration: 174 \t--- Loss: 0.322\n",
      "Iteration: 175 \t--- Loss: 0.322\n",
      "Iteration: 176 \t--- Loss: 0.310\n",
      "Iteration: 177 \t--- Loss: 0.323\n",
      "Iteration: 178 \t--- Loss: 0.314\n",
      "Iteration: 179 \t--- Loss: 0.320\n",
      "Iteration: 180 \t--- Loss: 0.314\n",
      "Iteration: 181 \t--- Loss: 0.324\n",
      "Iteration: 182 \t--- Loss: 0.319\n",
      "Iteration: 183 \t--- Loss: 0.322\n",
      "Iteration: 184 \t--- Loss: 0.312\n",
      "Iteration: 185 \t--- Loss: 0.332\n",
      "Iteration: 186 \t--- Loss: 0.324\n",
      "Iteration: 187 \t--- Loss: 0.333\n",
      "Iteration: 188 \t--- Loss: 0.318\n",
      "Iteration: 189 \t--- Loss: 0.323\n",
      "Iteration: 190 \t--- Loss: 0.319\n",
      "Iteration: 191 \t--- Loss: 0.319\n",
      "Iteration: 192 \t--- Loss: 0.333\n",
      "Iteration: 193 \t--- Loss: 0.317\n",
      "Iteration: 194 \t--- Loss: 0.317\n",
      "Iteration: 195 \t--- Loss: 0.318\n",
      "Iteration: 196 \t--- Loss: 0.327\n",
      "Iteration: 197 \t--- Loss: 0.305\n",
      "Iteration: 198 \t--- Loss: 0.323\n",
      "Iteration: 199 \t--- Loss: 0.324\n",
      "Iteration: 200 \t--- Loss: 0.315\n",
      "Iteration: 201 \t--- Loss: 0.327\n",
      "Iteration: 202 \t--- Loss: 0.312\n",
      "Iteration: 203 \t--- Loss: 0.302\n",
      "Iteration: 204 \t--- Loss: 0.312\n",
      "Iteration: 205 \t--- Loss: 0.326\n",
      "Iteration: 206 \t--- Loss: 0.317\n",
      "Iteration: 207 \t--- Loss: 0.312\n",
      "Iteration: 208 \t--- Loss: 0.310\n",
      "Iteration: 209 \t--- Loss: 0.312\n",
      "Iteration: 210 \t--- Loss: 0.319\n",
      "Iteration: 211 \t--- Loss: 0.324\n",
      "Iteration: 212 \t--- Loss: 0.320\n",
      "Iteration: 213 \t--- Loss: 0.319\n",
      "Iteration: 214 \t--- Loss: 0.316\n",
      "Iteration: 215 \t--- Loss: 0.321\n",
      "Iteration: 216 \t--- Loss: 0.319\n",
      "Iteration: 217 \t--- Loss: 0.323\n",
      "Iteration: 218 \t--- Loss: 0.331\n",
      "Iteration: 219 \t--- Loss: 0.330\n",
      "Iteration: 220 \t--- Loss: 0.326\n",
      "Iteration: 221 \t--- Loss: 0.317\n",
      "Iteration: 222 \t--- Loss: 0.324\n",
      "Iteration: 223 \t--- Loss: 0.328\n",
      "Iteration: 224 \t--- Loss: 0.319\n",
      "Iteration: 225 \t--- Loss: 0.307\n",
      "Iteration: 226 \t--- Loss: 0.309\n",
      "Iteration: 227 \t--- Loss: 0.321\n",
      "Iteration: 228 \t--- Loss: 0.325\n",
      "Iteration: 229 \t--- Loss: 0.318\n",
      "Iteration: 230 \t--- Loss: 0.323\n",
      "Iteration: 231 \t--- Loss: 0.323\n",
      "Iteration: 232 \t--- Loss: 0.320\n",
      "Iteration: 233 \t--- Loss: 0.330\n",
      "Iteration: 234 \t--- Loss: 0.329\n",
      "Iteration: 235 \t--- Loss: 0.318\n",
      "Iteration: 236 \t--- Loss: 0.322\n",
      "Iteration: 237 \t--- Loss: 0.319\n",
      "Iteration: 238 \t--- Loss: 0.303\n",
      "Iteration: 239 \t--- Loss: 0.318\n",
      "Iteration: 240 \t--- Loss: 0.322\n",
      "Iteration: 241 \t--- Loss: 0.321\n",
      "Iteration: 242 \t--- Loss: 0.338\n",
      "Iteration: 243 \t--- Loss: 0.318\n",
      "Iteration: 244 \t--- Loss: 0.323\n",
      "Iteration: 245 \t--- Loss: 0.334\n",
      "Iteration: 246 \t--- Loss: 0.308\n",
      "Iteration: 247 \t--- Loss: 0.316\n",
      "Iteration: 248 \t--- Loss: 0.335\n",
      "Iteration: 249 \t--- Loss: 0.329\n",
      "Iteration: 250 \t--- Loss: 0.321\n",
      "Iteration: 251 \t--- Loss: 0.317\n",
      "Iteration: 252 \t--- Loss: 0.328\n",
      "Iteration: 253 \t--- Loss: 0.330\n",
      "Iteration: 254 \t--- Loss: 0.316\n",
      "Iteration: 255 \t--- Loss: 0.320\n",
      "Iteration: 256 \t--- Loss: 0.323\n",
      "Iteration: 257 \t--- Loss: 0.320\n",
      "Iteration: 258 \t--- Loss: 0.313\n",
      "Iteration: 259 \t--- Loss: 0.320"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:31<00:00, 91.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.504\n",
      "Iteration: 261 \t--- Loss: 0.526\n",
      "Iteration: 262 \t--- Loss: 0.512\n",
      "Iteration: 263 \t--- Loss: 0.496\n",
      "Iteration: 264 \t--- Loss: 0.515\n",
      "Iteration: 265 \t--- Loss: 0.492\n",
      "Iteration: 266 \t--- Loss: 0.515\n",
      "Iteration: 267 \t--- Loss: 0.514\n",
      "Iteration: 268 \t--- Loss: 0.535\n",
      "Iteration: 269 \t--- Loss: 0.504\n",
      "Iteration: 270 \t--- Loss: 0.500\n",
      "Iteration: 271 \t--- Loss: 0.488\n",
      "Iteration: 272 \t--- Loss: 0.506\n",
      "Iteration: 273 \t--- Loss: 0.508\n",
      "Iteration: 274 \t--- Loss: 0.491\n",
      "Iteration: 275 \t--- Loss: 0.487\n",
      "Iteration: 276 \t--- Loss: 0.520\n",
      "Iteration: 277 \t--- Loss: 0.528\n",
      "Iteration: 278 \t--- Loss: 0.525\n",
      "Iteration: 279 \t--- Loss: 0.505\n",
      "Iteration: 280 \t--- Loss: 0.503\n",
      "Iteration: 281 \t--- Loss: 0.496\n",
      "Iteration: 282 \t--- Loss: 0.517\n",
      "Iteration: 283 \t--- Loss: 0.484\n",
      "Iteration: 284 \t--- Loss: 0.490\n",
      "Iteration: 285 \t--- Loss: 0.503\n",
      "Iteration: 286 \t--- Loss: 0.521\n",
      "Iteration: 287 \t--- Loss: 0.508\n",
      "Iteration: 288 \t--- Loss: 0.510\n",
      "Iteration: 289 \t--- Loss: 0.492\n",
      "Iteration: 290 \t--- Loss: 0.523\n",
      "Iteration: 291 \t--- Loss: 0.518\n",
      "Iteration: 292 \t--- Loss: 0.497\n",
      "Iteration: 293 \t--- Loss: 0.503\n",
      "Iteration: 294 \t--- Loss: 0.503\n",
      "Iteration: 295 \t--- Loss: 0.494\n",
      "Iteration: 296 \t--- Loss: 0.507\n",
      "Iteration: 297 \t--- Loss: 0.493\n",
      "Iteration: 298 \t--- Loss: 0.480\n",
      "Iteration: 299 \t--- Loss: 0.522\n",
      "Iteration: 300 \t--- Loss: 0.515\n",
      "Iteration: 301 \t--- Loss: 0.520\n",
      "Iteration: 302 \t--- Loss: 0.508\n",
      "Iteration: 303 \t--- Loss: 0.506\n",
      "Iteration: 304 \t--- Loss: 0.515\n",
      "Iteration: 305 \t--- Loss: 0.527\n",
      "Iteration: 306 \t--- Loss: 0.515\n",
      "Iteration: 307 \t--- Loss: 0.505\n",
      "Iteration: 308 \t--- Loss: 0.505\n",
      "Iteration: 309 \t--- Loss: 0.503\n",
      "Iteration: 310 \t--- Loss: 0.503\n",
      "Iteration: 311 \t--- Loss: 0.481\n",
      "Iteration: 312 \t--- Loss: 0.491\n",
      "Iteration: 313 \t--- Loss: 0.509\n",
      "Iteration: 314 \t--- Loss: 0.507\n",
      "Iteration: 315 \t--- Loss: 0.539\n",
      "Iteration: 316 \t--- Loss: 0.519\n",
      "Iteration: 317 \t--- Loss: 0.508\n",
      "Iteration: 318 \t--- Loss: 0.495\n",
      "Iteration: 319 \t--- Loss: 0.496\n",
      "Iteration: 320 \t--- Loss: 0.490\n",
      "Iteration: 321 \t--- Loss: 0.510\n",
      "Iteration: 322 \t--- Loss: 0.491\n",
      "Iteration: 323 \t--- Loss: 0.501\n",
      "Iteration: 324 \t--- Loss: 0.503\n",
      "Iteration: 325 \t--- Loss: 0.517\n",
      "Iteration: 326 \t--- Loss: 0.503\n",
      "Iteration: 327 \t--- Loss: 0.523\n",
      "Iteration: 328 \t--- Loss: 0.522\n",
      "Iteration: 329 \t--- Loss: 0.492\n",
      "Iteration: 330 \t--- Loss: 0.506\n",
      "Iteration: 331 \t--- Loss: 0.495\n",
      "Iteration: 332 \t--- Loss: 0.502\n",
      "Iteration: 333 \t--- Loss: 0.537\n",
      "Iteration: 334 \t--- Loss: 0.498\n",
      "Iteration: 335 \t--- Loss: 0.499\n",
      "Iteration: 336 \t--- Loss: 0.530\n",
      "Iteration: 337 \t--- Loss: 0.509\n",
      "Iteration: 338 \t--- Loss: 0.517\n",
      "Iteration: 339 \t--- Loss: 0.491\n",
      "Iteration: 340 \t--- Loss: 0.512\n",
      "Iteration: 341 \t--- Loss: 0.521\n",
      "Iteration: 342 \t--- Loss: 0.496\n",
      "Iteration: 343 \t--- Loss: 0.497\n",
      "Iteration: 344 \t--- Loss: 0.499\n",
      "Iteration: 345 \t--- Loss: 0.505\n",
      "Iteration: 346 \t--- Loss: 0.516\n",
      "Iteration: 347 \t--- Loss: 0.501\n",
      "Iteration: 348 \t--- Loss: 0.499\n",
      "Iteration: 349 \t--- Loss: 0.511\n",
      "Iteration: 350 \t--- Loss: 0.526\n",
      "Iteration: 351 \t--- Loss: 0.508\n",
      "Iteration: 352 \t--- Loss: 0.504\n",
      "Iteration: 353 \t--- Loss: 0.490\n",
      "Iteration: 354 \t--- Loss: 0.500\n",
      "Iteration: 355 \t--- Loss: 0.506\n",
      "Iteration: 356 \t--- Loss: 0.487\n",
      "Iteration: 357 \t--- Loss: 0.494\n",
      "Iteration: 358 \t--- Loss: 0.514\n",
      "Iteration: 359 \t--- Loss: 0.508\n",
      "Iteration: 360 \t--- Loss: 0.508\n",
      "Iteration: 361 \t--- Loss: 0.516\n",
      "Iteration: 362 \t--- Loss: 0.519\n",
      "Iteration: 363 \t--- Loss: 0.507\n",
      "Iteration: 364 \t--- Loss: 0.495\n",
      "Iteration: 365 \t--- Loss: 0.525\n",
      "Iteration: 366 \t--- Loss: 0.504\n",
      "Iteration: 367 \t--- Loss: 0.501\n",
      "Iteration: 368 \t--- Loss: 0.513\n",
      "Iteration: 369 \t--- Loss: 0.523\n",
      "Iteration: 370 \t--- Loss: 0.511\n",
      "Iteration: 371 \t--- Loss: 0.521\n",
      "Iteration: 372 \t--- Loss: 0.525\n",
      "Iteration: 373 \t--- Loss: 0.509\n",
      "Iteration: 374 \t--- Loss: 0.514\n",
      "Iteration: 375 \t--- Loss: 0.518\n",
      "Iteration: 376 \t--- Loss: 0.485\n",
      "Iteration: 377 \t--- Loss: 0.512\n",
      "Iteration: 378 \t--- Loss: 0.473\n",
      "Iteration: 379 \t--- Loss: 0.500\n",
      "Iteration: 380 \t--- Loss: 0.495\n",
      "Iteration: 381 \t--- Loss: 0.501\n",
      "Iteration: 382 \t--- Loss: 0.529\n",
      "Iteration: 383 \t--- Loss: 0.503\n",
      "Iteration: 384 \t--- Loss: 0.515\n",
      "Iteration: 385 \t--- Loss: 0.511\n",
      "Iteration: 386 \t--- Loss: 0.513\n",
      "Iteration: 387 \t--- Loss: 0.480\n",
      "Iteration: 388 \t--- Loss: 0.508\n",
      "Iteration: 389 \t--- Loss: 0.477\n",
      "Iteration: 390 \t--- Loss: 0.503\n",
      "Iteration: 391 \t--- Loss: 0.520\n",
      "Iteration: 392 \t--- Loss: 0.479\n",
      "Iteration: 393 \t--- Loss: 0.488\n",
      "Iteration: 394 \t--- Loss: 0.500\n",
      "Iteration: 395 \t--- Loss: 0.500\n",
      "Iteration: 396 \t--- Loss: 0.504\n",
      "Iteration: 397 \t--- Loss: 0.504\n",
      "Iteration: 398 \t--- Loss: 0.540\n",
      "Iteration: 399 \t--- Loss: 0.478\n",
      "Iteration: 400 \t--- Loss: 0.515\n",
      "Iteration: 401 \t--- Loss: 0.501\n",
      "Iteration: 402 \t--- Loss: 0.531\n",
      "Iteration: 403 \t--- Loss: 0.495\n",
      "Iteration: 404 \t--- Loss: 0.527\n",
      "Iteration: 405 \t--- Loss: 0.500\n",
      "Iteration: 406 \t--- Loss: 0.510\n",
      "Iteration: 407 \t--- Loss: 0.495\n",
      "Iteration: 408 \t--- Loss: 0.491\n",
      "Iteration: 409 \t--- Loss: 0.528\n",
      "Iteration: 410 \t--- Loss: 0.499\n",
      "Iteration: 411 \t--- Loss: 0.514\n",
      "Iteration: 412 \t--- Loss: 0.513\n",
      "Iteration: 413 \t--- Loss: 0.510\n",
      "Iteration: 414 \t--- Loss: 0.512\n",
      "Iteration: 415 \t--- Loss: 0.497\n",
      "Iteration: 416 \t--- Loss: 0.509\n",
      "Iteration: 417 \t--- Loss: 0.509\n",
      "Iteration: 418 \t--- Loss: 0.521\n",
      "Iteration: 419 \t--- Loss: 0.507\n",
      "Iteration: 420 \t--- Loss: 0.503\n",
      "Iteration: 421 \t--- Loss: 0.508\n",
      "Iteration: 422 \t--- Loss: 0.517\n",
      "Iteration: 423 \t--- Loss: 0.517\n",
      "Iteration: 424 \t--- Loss: 0.502\n",
      "Iteration: 425 \t--- Loss: 0.496\n",
      "Iteration: 426 \t--- Loss: 0.487\n",
      "Iteration: 427 \t--- Loss: 0.485\n",
      "Iteration: 428 \t--- Loss: 0.512\n",
      "Iteration: 429 \t--- Loss: 0.518\n",
      "Iteration: 430 \t--- Loss: 0.532\n",
      "Iteration: 431 \t--- Loss: 0.494\n",
      "Iteration: 432 \t--- Loss: 0.484\n",
      "Iteration: 433 \t--- Loss: 0.462\n",
      "Iteration: 434 \t--- Loss: 0.520\n",
      "Iteration: 435 \t--- Loss: 0.525\n",
      "Iteration: 436 \t--- Loss: 0.522\n",
      "Iteration: 437 \t--- Loss: 0.509\n",
      "Iteration: 438 \t--- Loss: 0.485\n",
      "Iteration: 439 \t--- Loss: 0.531\n",
      "Iteration: 440 \t--- Loss: 0.520\n",
      "Iteration: 441 \t--- Loss: 0.522\n",
      "Iteration: 442 \t--- Loss: 0.516\n",
      "Iteration: 443 \t--- Loss: 0.531\n",
      "Iteration: 444 \t--- Loss: 0.495\n",
      "Iteration: 445 \t--- Loss: 0.500\n",
      "Iteration: 446 \t--- Loss: 0.497\n",
      "Iteration: 447 \t--- Loss: 0.502\n",
      "Iteration: 448 \t--- Loss: 0.500\n",
      "Iteration: 449 \t--- Loss: 0.518\n",
      "Iteration: 450 \t--- Loss: 0.489\n",
      "Iteration: 451 \t--- Loss: 0.516\n",
      "Iteration: 452 \t--- Loss: 0.503\n",
      "Iteration: 453 \t--- Loss: 0.486\n",
      "Iteration: 454 \t--- Loss: 0.508\n",
      "Iteration: 455 \t--- Loss: 0.522\n",
      "Iteration: 456 \t--- Loss: 0.476\n",
      "Iteration: 457 \t--- Loss: 0.512\n",
      "Iteration: 458 \t--- Loss: 0.501\n",
      "Iteration: 459 \t--- Loss: 0.512\n",
      "Iteration: 460 \t--- Loss: 0.522\n",
      "Iteration: 461 \t--- Loss: 0.494\n",
      "Iteration: 462 \t--- Loss: 0.499\n",
      "Iteration: 463 \t--- Loss: 0.494\n",
      "Iteration: 464 \t--- Loss: 0.491\n",
      "Iteration: 465 \t--- Loss: 0.528\n",
      "Iteration: 466 \t--- Loss: 0.500\n",
      "Iteration: 467 \t--- Loss: 0.503\n",
      "Iteration: 468 \t--- Loss: 0.511\n",
      "Iteration: 469 \t--- Loss: 0.504\n",
      "Iteration: 470 \t--- Loss: 0.520\n",
      "Iteration: 471 \t--- Loss: 0.508\n",
      "Iteration: 472 \t--- Loss: 0.520\n",
      "Iteration: 473 \t--- Loss: 0.502\n",
      "Iteration: 474 \t--- Loss: 0.506\n",
      "Iteration: 475 \t--- Loss: 0.507\n",
      "Iteration: 476 \t--- Loss: 0.494\n",
      "Iteration: 477 \t--- Loss: 0.510\n",
      "Iteration: 478 \t--- Loss: 0.514\n",
      "Iteration: 479 \t--- Loss: 0.489\n",
      "Iteration: 480 \t--- Loss: 0.501\n",
      "Iteration: 481 \t--- Loss: 0.491\n",
      "Iteration: 482 \t--- Loss: 0.495\n",
      "Iteration: 483 \t--- Loss: 0.495\n",
      "Iteration: 484 \t--- Loss: 0.513\n",
      "Iteration: 485 \t--- Loss: 0.516\n",
      "Iteration: 486 \t--- Loss: 0.503\n",
      "Iteration: 487 \t--- Loss: 0.516\n",
      "Iteration: 488 \t--- Loss: 0.500\n",
      "Iteration: 489 \t--- Loss: 0.502\n",
      "Iteration: 490 \t--- Loss: 0.512\n",
      "Iteration: 491 \t--- Loss: 0.488\n",
      "Iteration: 492 \t--- Loss: 0.506\n",
      "Iteration: 493 \t--- Loss: 0.496\n",
      "Iteration: 494 \t--- Loss: 0.502\n",
      "Iteration: 495 \t--- Loss: 0.499\n",
      "Iteration: 496 \t--- Loss: 0.494\n",
      "Iteration: 497 \t--- Loss: 0.519\n",
      "Iteration: 498 \t--- Loss: 0.504\n",
      "Iteration: 499 \t--- Loss: 0.522\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:10,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.823\n",
      "Iteration: 1 \t--- Loss: 0.794\n",
      "Iteration: 2 \t--- Loss: 0.747\n",
      "Iteration: 3 \t--- Loss: 0.720\n",
      "Iteration: 4 \t--- Loss: 0.710\n",
      "Iteration: 5 \t--- Loss: 0.678\n",
      "Iteration: 6 \t--- Loss: 0.670\n",
      "Iteration: 7 \t--- Loss: 0.657\n",
      "Iteration: 8 \t--- Loss: 0.642\n",
      "Iteration: 9 \t--- Loss: 0.605\n",
      "Iteration: 10 \t--- Loss: 0.628\n",
      "Iteration: 11 \t--- Loss: 0.582\n",
      "Iteration: 12 \t--- Loss: 0.590\n",
      "Iteration: 13 \t--- Loss: 0.566\n",
      "Iteration: 14 \t--- Loss: 0.585\n",
      "Iteration: 15 \t--- Loss: 0.563\n",
      "Iteration: 16 \t--- Loss: 0.569\n",
      "Iteration: 17 \t--- Loss: 0.547\n",
      "Iteration: 18 \t--- Loss: 0.550\n",
      "Iteration: 19 \t--- Loss: 0.542\n",
      "Iteration: 20 \t--- Loss: 0.527\n",
      "Iteration: 21 \t--- Loss: 0.531\n",
      "Iteration: 22 \t--- Loss: 0.516\n",
      "Iteration: 23 \t--- Loss: 0.523\n",
      "Iteration: 24 \t--- Loss: 0.511\n",
      "Iteration: 25 \t--- Loss: 0.497\n",
      "Iteration: 26 \t--- Loss: 0.447\n",
      "Iteration: 27 \t--- Loss: 0.455\n",
      "Iteration: 28 \t--- Loss: 0.441\n",
      "Iteration: 29 \t--- Loss: 0.429\n",
      "Iteration: 30 \t--- Loss: 0.402\n",
      "Iteration: 31 \t--- Loss: 0.400\n",
      "Iteration: 32 \t--- Loss: 0.412\n",
      "Iteration: 33 \t--- Loss: 0.426\n",
      "Iteration: 34 \t--- Loss: 0.410\n",
      "Iteration: 35 \t--- Loss: 0.482\n",
      "Iteration: 36 \t--- Loss: 0.815\n",
      "Iteration: 37 \t--- Loss: 0.788\n",
      "Iteration: 38 \t--- Loss: 0.750\n",
      "Iteration: 39 \t--- Loss: 0.724\n",
      "Iteration: 40 \t--- Loss: 0.695\n",
      "Iteration: 41 \t--- Loss: 0.663\n",
      "Iteration: 42 \t--- Loss: 0.638\n",
      "Iteration: 43 \t--- Loss: 0.627\n",
      "Iteration: 44 \t--- Loss: 0.614\n",
      "Iteration: 45 \t--- Loss: 0.599\n",
      "Iteration: 46 \t--- Loss: 0.589\n",
      "Iteration: 47 \t--- Loss: 0.575\n",
      "Iteration: 48 \t--- Loss: 0.562\n",
      "Iteration: 49 \t--- Loss: 0.552\n",
      "Iteration: 50 \t--- Loss: 0.531\n",
      "Iteration: 51 \t--- Loss: 0.528\n",
      "Iteration: 52 \t--- Loss: 0.545\n",
      "Iteration: 53 \t--- Loss: 0.507\n",
      "Iteration: 54 \t--- Loss: 0.480\n",
      "Iteration: 55 \t--- Loss: 0.474\n",
      "Iteration: 56 \t--- Loss: 0.442\n",
      "Iteration: 57 \t--- Loss: 0.456\n",
      "Iteration: 58 \t--- Loss: 0.453\n",
      "Iteration: 59 \t--- Loss: 0.405\n",
      "Iteration: 60 \t--- Loss: 0.401\n",
      "Iteration: 61 \t--- Loss: 0.368\n",
      "Iteration: 62 \t--- Loss: 0.374\n",
      "Iteration: 63 \t--- Loss: 0.369\n",
      "Iteration: 64 \t--- Loss: 0.393\n",
      "Iteration: 65 \t--- Loss: 0.459\n",
      "Iteration: 66 \t--- Loss: 0.406\n",
      "Iteration: 67 \t--- Loss: 0.383\n",
      "Iteration: 68 \t--- Loss: 0.449\n",
      "Iteration: 69 \t--- Loss: 0.445\n",
      "Iteration: 70 \t--- Loss: 0.425\n",
      "Iteration: 71 \t--- Loss: 0.384\n",
      "Iteration: 72 \t--- Loss: 0.332\n",
      "Iteration: 73 \t--- Loss: 0.319\n",
      "Iteration: 74 \t--- Loss: 0.312\n",
      "Iteration: 75 \t--- Loss: 0.354\n",
      "Iteration: 76 \t--- Loss: 0.426\n",
      "Iteration: 77 \t--- Loss: 0.413\n",
      "Iteration: 78 \t--- Loss: 0.367\n",
      "Iteration: 79 \t--- Loss: 0.292\n",
      "Iteration: 80 \t--- Loss: 0.301\n",
      "Iteration: 81 \t--- Loss: 0.300\n",
      "Iteration: 82 \t--- Loss: 0.382\n",
      "Iteration: 83 \t--- Loss: 0.540\n",
      "Iteration: 84 \t--- Loss: 0.545\n",
      "Iteration: 85 \t--- Loss: 0.520\n",
      "Iteration: 86 \t--- Loss: 0.512\n",
      "Iteration: 87 \t--- Loss: 0.490\n",
      "Iteration: 88 \t--- Loss: 0.477\n",
      "Iteration: 89 \t--- Loss: 0.495\n",
      "Iteration: 90 \t--- Loss: 0.468\n",
      "Iteration: 91 \t--- Loss: 0.482\n",
      "Iteration: 92 \t--- Loss: 0.464\n",
      "Iteration: 93 \t--- Loss: 0.437\n",
      "Iteration: 94 \t--- Loss: 0.432\n",
      "Iteration: 95 \t--- Loss: 0.420\n",
      "Iteration: 96 \t--- Loss: 0.414\n",
      "Iteration: 97 \t--- Loss: 0.409\n",
      "Iteration: 98 \t--- Loss: 0.419\n",
      "Iteration: 99 \t--- Loss: 0.374\n",
      "Iteration: 100 \t--- Loss: 0.362\n",
      "Iteration: 101 \t--- Loss: 0.375\n",
      "Iteration: 102 \t--- Loss: 0.364\n",
      "Iteration: 103 \t--- Loss: 0.329\n",
      "Iteration: 104 \t--- Loss: 0.340\n",
      "Iteration: 105 \t--- Loss: 0.338\n",
      "Iteration: 106 \t--- Loss: 0.331\n",
      "Iteration: 107 \t--- Loss: 0.291\n",
      "Iteration: 108 \t--- Loss: 0.306\n",
      "Iteration: 109 \t--- Loss: 0.295\n",
      "Iteration: 110 \t--- Loss: 0.314\n",
      "Iteration: 111 \t--- Loss: 0.306\n",
      "Iteration: 112 \t--- Loss: 0.286\n",
      "Iteration: 113 \t--- Loss: 0.289\n",
      "Iteration: 114 \t--- Loss: 0.288\n",
      "Iteration: 115 \t--- Loss: 0.293\n",
      "Iteration: 116 \t--- Loss: 0.296\n",
      "Iteration: 117 \t--- Loss: 0.290\n",
      "Iteration: 118 \t--- Loss: 0.288\n",
      "Iteration: 119 \t--- Loss: 0.282\n",
      "Iteration: 120 \t--- Loss: 0.279\n",
      "Iteration: 121 \t--- Loss: 0.307\n",
      "Iteration: 122 \t--- Loss: 0.320\n",
      "Iteration: 123 \t--- Loss: 0.315\n",
      "Iteration: 124 \t--- Loss: 0.284\n",
      "Iteration: 125 \t--- Loss: 0.306\n",
      "Iteration: 126 \t--- Loss: 0.295\n",
      "Iteration: 127 \t--- Loss: 0.287\n",
      "Iteration: 128 \t--- Loss: 0.291\n",
      "Iteration: 129 \t--- Loss: 0.316\n",
      "Iteration: 130 \t--- Loss: 0.307\n",
      "Iteration: 131 \t--- Loss: 0.297\n",
      "Iteration: 132 \t--- Loss: 0.305\n",
      "Iteration: 133 \t--- Loss: 0.292\n",
      "Iteration: 134 \t--- Loss: 0.313\n",
      "Iteration: 135 \t--- Loss: 0.314\n",
      "Iteration: 136 \t--- Loss: 0.298\n",
      "Iteration: 137 \t--- Loss: 0.320\n",
      "Iteration: 138 \t--- Loss: 0.318\n",
      "Iteration: 139 \t--- Loss: 0.288\n",
      "Iteration: 140 \t--- Loss: 0.338\n",
      "Iteration: 141 \t--- Loss: 0.344\n",
      "Iteration: 142 \t--- Loss: 0.315\n",
      "Iteration: 143 \t--- Loss: 0.335\n",
      "Iteration: 144 \t--- Loss: 0.334\n",
      "Iteration: 145 \t--- Loss: 0.329\n",
      "Iteration: 146 \t--- Loss: 0.312\n",
      "Iteration: 147 \t--- Loss: 0.336\n",
      "Iteration: 148 \t--- Loss: 0.360\n",
      "Iteration: 149 \t--- Loss: 0.326\n",
      "Iteration: 150 \t--- Loss: 0.332\n",
      "Iteration: 151 \t--- Loss: 0.347\n",
      "Iteration: 152 \t--- Loss: 0.352\n",
      "Iteration: 153 \t--- Loss: 0.335\n",
      "Iteration: 154 \t--- Loss: 0.342\n",
      "Iteration: 155 \t--- Loss: 0.341\n",
      "Iteration: 156 \t--- Loss: 0.353\n",
      "Iteration: 157 \t--- Loss: 0.331\n",
      "Iteration: 158 \t--- Loss: 0.330\n",
      "Iteration: 159 \t--- Loss: 0.345\n",
      "Iteration: 160 \t--- Loss: 0.329\n",
      "Iteration: 161 \t--- Loss: 0.339\n",
      "Iteration: 162 \t--- Loss: 0.313\n",
      "Iteration: 163 \t--- Loss: 0.334\n",
      "Iteration: 164 \t--- Loss: 0.324\n",
      "Iteration: 165 \t--- Loss: 0.349\n",
      "Iteration: 166 \t--- Loss: 0.332\n",
      "Iteration: 167 \t--- Loss: 0.325\n",
      "Iteration: 168 \t--- Loss: 0.300\n",
      "Iteration: 169 \t--- Loss: 0.292\n",
      "Iteration: 170 \t--- Loss: 0.274\n",
      "Iteration: 171 \t--- Loss: 0.287\n",
      "Iteration: 172 \t--- Loss: 0.305\n",
      "Iteration: 173 \t--- Loss: 0.288\n",
      "Iteration: 174 \t--- Loss: 0.260\n",
      "Iteration: 175 \t--- Loss: 0.267\n",
      "Iteration: 176 \t--- Loss: 0.258\n",
      "Iteration: 177 \t--- Loss: 0.262\n",
      "Iteration: 178 \t--- Loss: 0.277\n",
      "Iteration: 179 \t--- Loss: 0.251\n",
      "Iteration: 180 \t--- Loss: 0.222\n",
      "Iteration: 181 \t--- Loss: 0.240\n",
      "Iteration: 182 \t--- Loss: 0.208\n",
      "Iteration: 183 \t--- Loss: 0.227\n",
      "Iteration: 184 \t--- Loss: 0.200\n",
      "Iteration: 185 \t--- Loss: 0.208\n",
      "Iteration: 186 \t--- Loss: 0.222\n",
      "Iteration: 187 \t--- Loss: 0.176\n",
      "Iteration: 188 \t--- Loss: 0.163\n",
      "Iteration: 189 \t--- Loss: 0.175\n",
      "Iteration: 190 \t--- Loss: 0.184\n",
      "Iteration: 191 \t--- Loss: 0.158\n",
      "Iteration: 192 \t--- Loss: 0.156\n",
      "Iteration: 193 \t--- Loss: 0.169\n",
      "Iteration: 194 \t--- Loss: 0.150\n",
      "Iteration: 195 \t--- Loss: 0.140\n",
      "Iteration: 196 \t--- Loss: 0.155\n",
      "Iteration: 197 \t--- Loss: 0.149\n",
      "Iteration: 198 \t--- Loss: 0.156\n",
      "Iteration: 199 \t--- Loss: 0.143\n",
      "Iteration: 200 \t--- Loss: 0.149\n",
      "Iteration: 201 \t--- Loss: 0.136\n",
      "Iteration: 202 \t--- Loss: 0.129\n",
      "Iteration: 203 \t--- Loss: 0.133\n",
      "Iteration: 204 \t--- Loss: 0.128\n",
      "Iteration: 205 \t--- Loss: 0.124\n",
      "Iteration: 206 \t--- Loss: 0.132\n",
      "Iteration: 207 \t--- Loss: 0.139\n",
      "Iteration: 208 \t--- Loss: 0.134\n",
      "Iteration: 209 \t--- Loss: 0.130\n",
      "Iteration: 210 \t--- Loss: 0.142\n",
      "Iteration: 211 \t--- Loss: 0.127\n",
      "Iteration: 212 \t--- Loss: 0.107\n",
      "Iteration: 213 \t--- Loss: 0.121\n",
      "Iteration: 214 \t--- Loss: 0.118\n",
      "Iteration: 215 \t--- Loss: 0.116\n",
      "Iteration: 216 \t--- Loss: 0.122\n",
      "Iteration: 217 \t--- Loss: 0.122\n",
      "Iteration: 218 \t--- Loss: 0.134\n",
      "Iteration: 219 \t--- Loss: 0.120\n",
      "Iteration: 220 \t--- Loss: 0.117\n",
      "Iteration: 221 \t--- Loss: 0.122\n",
      "Iteration: 222 \t--- Loss: 0.128\n",
      "Iteration: 223 \t--- Loss: 0.111\n",
      "Iteration: 224 \t--- Loss: 0.114\n",
      "Iteration: 225 \t--- Loss: 0.114\n",
      "Iteration: 226 \t--- Loss: 0.120\n",
      "Iteration: 227 \t--- Loss: 0.116\n",
      "Iteration: 228 \t--- Loss: 0.116\n",
      "Iteration: 229 \t--- Loss: 0.109\n",
      "Iteration: 230 \t--- Loss: 0.121\n",
      "Iteration: 231 \t--- Loss: 0.116\n",
      "Iteration: 232 \t--- Loss: 0.106\n",
      "Iteration: 233 \t--- Loss: 0.116\n",
      "Iteration: 234 \t--- Loss: 0.105\n",
      "Iteration: 235 \t--- Loss: 0.102\n",
      "Iteration: 236 \t--- Loss: 0.114\n",
      "Iteration: 237 \t--- Loss: 0.125\n",
      "Iteration: 238 \t--- Loss: 0.109\n",
      "Iteration: 239 \t--- Loss: 0.109\n",
      "Iteration: 240 \t--- Loss: 0.109\n",
      "Iteration: 241 \t--- Loss: 0.102\n",
      "Iteration: 242 \t--- Loss: 0.108\n",
      "Iteration: 243 \t--- Loss: 0.101\n",
      "Iteration: 244 \t--- Loss: 0.115\n",
      "Iteration: 245 \t--- Loss: 0.110\n",
      "Iteration: 246 \t--- Loss: 0.104\n",
      "Iteration: 247 \t--- Loss: 0.104\n",
      "Iteration: 248 \t--- Loss: 0.105\n",
      "Iteration: 249 \t--- Loss: 0.105\n",
      "Iteration: 250 \t--- Loss: 0.115\n",
      "Iteration: 251 \t--- Loss: 0.095\n",
      "Iteration: 252 \t--- Loss: 0.099\n",
      "Iteration: 253 \t--- Loss: 0.103\n",
      "Iteration: 254 \t--- Loss: 0.107\n",
      "Iteration: 255 \t--- Loss: 0.104\n",
      "Iteration: 256 \t--- Loss: 0.106\n",
      "Iteration: 257 \t--- Loss: 0.098\n",
      "Iteration: 258 \t--- Loss: 0.097\n",
      "Iteration: 259 \t--- Loss: 0.101"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:25<00:00, 85.16s/it]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.636\n",
      "Iteration: 261 \t--- Loss: 0.637\n",
      "Iteration: 262 \t--- Loss: 0.639\n",
      "Iteration: 263 \t--- Loss: 0.637\n",
      "Iteration: 264 \t--- Loss: 0.637\n",
      "Iteration: 265 \t--- Loss: 0.639\n",
      "Iteration: 266 \t--- Loss: 0.637\n",
      "Iteration: 267 \t--- Loss: 0.638\n",
      "Iteration: 268 \t--- Loss: 0.638\n",
      "Iteration: 269 \t--- Loss: 0.639\n",
      "Iteration: 270 \t--- Loss: 0.638\n",
      "Iteration: 271 \t--- Loss: 0.635\n",
      "Iteration: 272 \t--- Loss: 0.640\n",
      "Iteration: 273 \t--- Loss: 0.640\n",
      "Iteration: 274 \t--- Loss: 0.637\n",
      "Iteration: 275 \t--- Loss: 0.635\n",
      "Iteration: 276 \t--- Loss: 0.636\n",
      "Iteration: 277 \t--- Loss: 0.636\n",
      "Iteration: 278 \t--- Loss: 0.638\n",
      "Iteration: 279 \t--- Loss: 0.636\n",
      "Iteration: 280 \t--- Loss: 0.636\n",
      "Iteration: 281 \t--- Loss: 0.636\n",
      "Iteration: 282 \t--- Loss: 0.639\n",
      "Iteration: 283 \t--- Loss: 0.641\n",
      "Iteration: 284 \t--- Loss: 0.637\n",
      "Iteration: 285 \t--- Loss: 0.636\n",
      "Iteration: 286 \t--- Loss: 0.639\n",
      "Iteration: 287 \t--- Loss: 0.634\n",
      "Iteration: 288 \t--- Loss: 0.637\n",
      "Iteration: 289 \t--- Loss: 0.640\n",
      "Iteration: 290 \t--- Loss: 0.636\n",
      "Iteration: 291 \t--- Loss: 0.639\n",
      "Iteration: 292 \t--- Loss: 0.639\n",
      "Iteration: 293 \t--- Loss: 0.639\n",
      "Iteration: 294 \t--- Loss: 0.638\n",
      "Iteration: 295 \t--- Loss: 0.638\n",
      "Iteration: 296 \t--- Loss: 0.636\n",
      "Iteration: 297 \t--- Loss: 0.637\n",
      "Iteration: 298 \t--- Loss: 0.639\n",
      "Iteration: 299 \t--- Loss: 0.636\n",
      "Iteration: 300 \t--- Loss: 0.638\n",
      "Iteration: 301 \t--- Loss: 0.637\n",
      "Iteration: 302 \t--- Loss: 0.636\n",
      "Iteration: 303 \t--- Loss: 0.635\n",
      "Iteration: 304 \t--- Loss: 0.639\n",
      "Iteration: 305 \t--- Loss: 0.637\n",
      "Iteration: 306 \t--- Loss: 0.639\n",
      "Iteration: 307 \t--- Loss: 0.638\n",
      "Iteration: 308 \t--- Loss: 0.641\n",
      "Iteration: 309 \t--- Loss: 0.635\n",
      "Iteration: 310 \t--- Loss: 0.639\n",
      "Iteration: 311 \t--- Loss: 0.638\n",
      "Iteration: 312 \t--- Loss: 0.637\n",
      "Iteration: 313 \t--- Loss: 0.641\n",
      "Iteration: 314 \t--- Loss: 0.638\n",
      "Iteration: 315 \t--- Loss: 0.637\n",
      "Iteration: 316 \t--- Loss: 0.639\n",
      "Iteration: 317 \t--- Loss: 0.639\n",
      "Iteration: 318 \t--- Loss: 0.639\n",
      "Iteration: 319 \t--- Loss: 0.638\n",
      "Iteration: 320 \t--- Loss: 0.637\n",
      "Iteration: 321 \t--- Loss: 0.637\n",
      "Iteration: 322 \t--- Loss: 0.639\n",
      "Iteration: 323 \t--- Loss: 0.637\n",
      "Iteration: 324 \t--- Loss: 0.632\n",
      "Iteration: 325 \t--- Loss: 0.638\n",
      "Iteration: 326 \t--- Loss: 0.637\n",
      "Iteration: 327 \t--- Loss: 0.639\n",
      "Iteration: 328 \t--- Loss: 0.639\n",
      "Iteration: 329 \t--- Loss: 0.634\n",
      "Iteration: 330 \t--- Loss: 0.635\n",
      "Iteration: 331 \t--- Loss: 0.638\n",
      "Iteration: 332 \t--- Loss: 0.636\n",
      "Iteration: 333 \t--- Loss: 0.640\n",
      "Iteration: 334 \t--- Loss: 0.635\n",
      "Iteration: 335 \t--- Loss: 0.636\n",
      "Iteration: 336 \t--- Loss: 0.637\n",
      "Iteration: 337 \t--- Loss: 0.636\n",
      "Iteration: 338 \t--- Loss: 0.634\n",
      "Iteration: 339 \t--- Loss: 0.636\n",
      "Iteration: 340 \t--- Loss: 0.637\n",
      "Iteration: 341 \t--- Loss: 0.639\n",
      "Iteration: 342 \t--- Loss: 0.637\n",
      "Iteration: 343 \t--- Loss: 0.636\n",
      "Iteration: 344 \t--- Loss: 0.637\n",
      "Iteration: 345 \t--- Loss: 0.638\n",
      "Iteration: 346 \t--- Loss: 0.636\n",
      "Iteration: 347 \t--- Loss: 0.639\n",
      "Iteration: 348 \t--- Loss: 0.638\n",
      "Iteration: 349 \t--- Loss: 0.637\n",
      "Iteration: 350 \t--- Loss: 0.638\n",
      "Iteration: 351 \t--- Loss: 0.637\n",
      "Iteration: 352 \t--- Loss: 0.635\n",
      "Iteration: 353 \t--- Loss: 0.638\n",
      "Iteration: 354 \t--- Loss: 0.635\n",
      "Iteration: 355 \t--- Loss: 0.637\n",
      "Iteration: 356 \t--- Loss: 0.640\n",
      "Iteration: 357 \t--- Loss: 0.639\n",
      "Iteration: 358 \t--- Loss: 0.639\n",
      "Iteration: 359 \t--- Loss: 0.636\n",
      "Iteration: 360 \t--- Loss: 0.636\n",
      "Iteration: 361 \t--- Loss: 0.641\n",
      "Iteration: 362 \t--- Loss: 0.637\n",
      "Iteration: 363 \t--- Loss: 0.635\n",
      "Iteration: 364 \t--- Loss: 0.639\n",
      "Iteration: 365 \t--- Loss: 0.641\n",
      "Iteration: 366 \t--- Loss: 0.637\n",
      "Iteration: 367 \t--- Loss: 0.637\n",
      "Iteration: 368 \t--- Loss: 0.637\n",
      "Iteration: 369 \t--- Loss: 0.635\n",
      "Iteration: 370 \t--- Loss: 0.637\n",
      "Iteration: 371 \t--- Loss: 0.635\n",
      "Iteration: 372 \t--- Loss: 0.635\n",
      "Iteration: 373 \t--- Loss: 0.638\n",
      "Iteration: 374 \t--- Loss: 0.637\n",
      "Iteration: 375 \t--- Loss: 0.637\n",
      "Iteration: 376 \t--- Loss: 0.638\n",
      "Iteration: 377 \t--- Loss: 0.638\n",
      "Iteration: 378 \t--- Loss: 0.638\n",
      "Iteration: 379 \t--- Loss: 0.637\n",
      "Iteration: 380 \t--- Loss: 0.636\n",
      "Iteration: 381 \t--- Loss: 0.639\n",
      "Iteration: 382 \t--- Loss: 0.639\n",
      "Iteration: 383 \t--- Loss: 0.638\n",
      "Iteration: 384 \t--- Loss: 0.638\n",
      "Iteration: 385 \t--- Loss: 0.637\n",
      "Iteration: 386 \t--- Loss: 0.637\n",
      "Iteration: 387 \t--- Loss: 0.635\n",
      "Iteration: 388 \t--- Loss: 0.640\n",
      "Iteration: 389 \t--- Loss: 0.637\n",
      "Iteration: 390 \t--- Loss: 0.636\n",
      "Iteration: 391 \t--- Loss: 0.634\n",
      "Iteration: 392 \t--- Loss: 0.637\n",
      "Iteration: 393 \t--- Loss: 0.638\n",
      "Iteration: 394 \t--- Loss: 0.640\n",
      "Iteration: 395 \t--- Loss: 0.634\n",
      "Iteration: 396 \t--- Loss: 0.638\n",
      "Iteration: 397 \t--- Loss: 0.635\n",
      "Iteration: 398 \t--- Loss: 0.638\n",
      "Iteration: 399 \t--- Loss: 0.638\n",
      "Iteration: 400 \t--- Loss: 0.635\n",
      "Iteration: 401 \t--- Loss: 0.637\n",
      "Iteration: 402 \t--- Loss: 0.640\n",
      "Iteration: 403 \t--- Loss: 0.639\n",
      "Iteration: 404 \t--- Loss: 0.639\n",
      "Iteration: 405 \t--- Loss: 0.636\n",
      "Iteration: 406 \t--- Loss: 0.639\n",
      "Iteration: 407 \t--- Loss: 0.640\n",
      "Iteration: 408 \t--- Loss: 0.638\n",
      "Iteration: 409 \t--- Loss: 0.634\n",
      "Iteration: 410 \t--- Loss: 0.639\n",
      "Iteration: 411 \t--- Loss: 0.637\n",
      "Iteration: 412 \t--- Loss: 0.636\n",
      "Iteration: 413 \t--- Loss: 0.636\n",
      "Iteration: 414 \t--- Loss: 0.639\n",
      "Iteration: 415 \t--- Loss: 0.639\n",
      "Iteration: 416 \t--- Loss: 0.635\n",
      "Iteration: 417 \t--- Loss: 0.635\n",
      "Iteration: 418 \t--- Loss: 0.635\n",
      "Iteration: 419 \t--- Loss: 0.636\n",
      "Iteration: 420 \t--- Loss: 0.638\n",
      "Iteration: 421 \t--- Loss: 0.638\n",
      "Iteration: 422 \t--- Loss: 0.635\n",
      "Iteration: 423 \t--- Loss: 0.638\n",
      "Iteration: 424 \t--- Loss: 0.640\n",
      "Iteration: 425 \t--- Loss: 0.636\n",
      "Iteration: 426 \t--- Loss: 0.638\n",
      "Iteration: 427 \t--- Loss: 0.639\n",
      "Iteration: 428 \t--- Loss: 0.641\n",
      "Iteration: 429 \t--- Loss: 0.637\n",
      "Iteration: 430 \t--- Loss: 0.638\n",
      "Iteration: 431 \t--- Loss: 0.637\n",
      "Iteration: 432 \t--- Loss: 0.637\n",
      "Iteration: 433 \t--- Loss: 0.636\n",
      "Iteration: 434 \t--- Loss: 0.637\n",
      "Iteration: 435 \t--- Loss: 0.640\n",
      "Iteration: 436 \t--- Loss: 0.638\n",
      "Iteration: 437 \t--- Loss: 0.639\n",
      "Iteration: 438 \t--- Loss: 0.639\n",
      "Iteration: 439 \t--- Loss: 0.636\n",
      "Iteration: 440 \t--- Loss: 0.635\n",
      "Iteration: 441 \t--- Loss: 0.637\n",
      "Iteration: 442 \t--- Loss: 0.636\n",
      "Iteration: 443 \t--- Loss: 0.639\n",
      "Iteration: 444 \t--- Loss: 0.640\n",
      "Iteration: 445 \t--- Loss: 0.636\n",
      "Iteration: 446 \t--- Loss: 0.636\n",
      "Iteration: 447 \t--- Loss: 0.637\n",
      "Iteration: 448 \t--- Loss: 0.637\n",
      "Iteration: 449 \t--- Loss: 0.636\n",
      "Iteration: 450 \t--- Loss: 0.636\n",
      "Iteration: 451 \t--- Loss: 0.637\n",
      "Iteration: 452 \t--- Loss: 0.642\n",
      "Iteration: 453 \t--- Loss: 0.637\n",
      "Iteration: 454 \t--- Loss: 0.636\n",
      "Iteration: 455 \t--- Loss: 0.636\n",
      "Iteration: 456 \t--- Loss: 0.637\n",
      "Iteration: 457 \t--- Loss: 0.638\n",
      "Iteration: 458 \t--- Loss: 0.638\n",
      "Iteration: 459 \t--- Loss: 0.639\n",
      "Iteration: 460 \t--- Loss: 0.637\n",
      "Iteration: 461 \t--- Loss: 0.637\n",
      "Iteration: 462 \t--- Loss: 0.639\n",
      "Iteration: 463 \t--- Loss: 0.637\n",
      "Iteration: 464 \t--- Loss: 0.638\n",
      "Iteration: 465 \t--- Loss: 0.637\n",
      "Iteration: 466 \t--- Loss: 0.638\n",
      "Iteration: 467 \t--- Loss: 0.635\n",
      "Iteration: 468 \t--- Loss: 0.639\n",
      "Iteration: 469 \t--- Loss: 0.641\n",
      "Iteration: 470 \t--- Loss: 0.635\n",
      "Iteration: 471 \t--- Loss: 0.637\n",
      "Iteration: 472 \t--- Loss: 0.638\n",
      "Iteration: 473 \t--- Loss: 0.637\n",
      "Iteration: 474 \t--- Loss: 0.636\n",
      "Iteration: 475 \t--- Loss: 0.635\n",
      "Iteration: 476 \t--- Loss: 0.637\n",
      "Iteration: 477 \t--- Loss: 0.637\n",
      "Iteration: 478 \t--- Loss: 0.639\n",
      "Iteration: 479 \t--- Loss: 0.635\n",
      "Iteration: 480 \t--- Loss: 0.637\n",
      "Iteration: 481 \t--- Loss: 0.639\n",
      "Iteration: 482 \t--- Loss: 0.635\n",
      "Iteration: 483 \t--- Loss: 0.636\n",
      "Iteration: 484 \t--- Loss: 0.637\n",
      "Iteration: 485 \t--- Loss: 0.639\n",
      "Iteration: 486 \t--- Loss: 0.637\n",
      "Iteration: 487 \t--- Loss: 0.638\n",
      "Iteration: 488 \t--- Loss: 0.636\n",
      "Iteration: 489 \t--- Loss: 0.635\n",
      "Iteration: 490 \t--- Loss: 0.636\n",
      "Iteration: 491 \t--- Loss: 0.639\n",
      "Iteration: 492 \t--- Loss: 0.638\n",
      "Iteration: 493 \t--- Loss: 0.638\n",
      "Iteration: 494 \t--- Loss: 0.635\n",
      "Iteration: 495 \t--- Loss: 0.635\n",
      "Iteration: 496 \t--- Loss: 0.638\n",
      "Iteration: 497 \t--- Loss: 0.636\n",
      "Iteration: 498 \t--- Loss: 0.637\n",
      "Iteration: 499 \t--- Loss: 0.639\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/it][Parallel(n_jobs=5)]: Done  26 tasks      | elapsed: 13.8min\n",
      " 10%|█         | 1/10 [00:00<00:07,  1.17it/s]]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.50s/it][Parallel(n_jobs=5)]: Done  27 tasks      | elapsed: 14.1min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:34<00:00, 94.60s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.322\n",
      "Iteration: 261 \t--- Loss: 0.326\n",
      "Iteration: 262 \t--- Loss: 0.312\n",
      "Iteration: 263 \t--- Loss: 0.333\n",
      "Iteration: 264 \t--- Loss: 0.325\n",
      "Iteration: 265 \t--- Loss: 0.331\n",
      "Iteration: 266 \t--- Loss: 0.332\n",
      "Iteration: 267 \t--- Loss: 0.325\n",
      "Iteration: 268 \t--- Loss: 0.327\n",
      "Iteration: 269 \t--- Loss: 0.316\n",
      "Iteration: 270 \t--- Loss: 0.336\n",
      "Iteration: 271 \t--- Loss: 0.310\n",
      "Iteration: 272 \t--- Loss: 0.322\n",
      "Iteration: 273 \t--- Loss: 0.322\n",
      "Iteration: 274 \t--- Loss: 0.325\n",
      "Iteration: 275 \t--- Loss: 0.310\n",
      "Iteration: 276 \t--- Loss: 0.317\n",
      "Iteration: 277 \t--- Loss: 0.329\n",
      "Iteration: 278 \t--- Loss: 0.308\n",
      "Iteration: 279 \t--- Loss: 0.317\n",
      "Iteration: 280 \t--- Loss: 0.326\n",
      "Iteration: 281 \t--- Loss: 0.325\n",
      "Iteration: 282 \t--- Loss: 0.335\n",
      "Iteration: 283 \t--- Loss: 0.317\n",
      "Iteration: 284 \t--- Loss: 0.321\n",
      "Iteration: 285 \t--- Loss: 0.301\n",
      "Iteration: 286 \t--- Loss: 0.320\n",
      "Iteration: 287 \t--- Loss: 0.321\n",
      "Iteration: 288 \t--- Loss: 0.315\n",
      "Iteration: 289 \t--- Loss: 0.321\n",
      "Iteration: 290 \t--- Loss: 0.318\n",
      "Iteration: 291 \t--- Loss: 0.327\n",
      "Iteration: 292 \t--- Loss: 0.319\n",
      "Iteration: 293 \t--- Loss: 0.318\n",
      "Iteration: 294 \t--- Loss: 0.314\n",
      "Iteration: 295 \t--- Loss: 0.326\n",
      "Iteration: 296 \t--- Loss: 0.317\n",
      "Iteration: 297 \t--- Loss: 0.325\n",
      "Iteration: 298 \t--- Loss: 0.322\n",
      "Iteration: 299 \t--- Loss: 0.313\n",
      "Iteration: 300 \t--- Loss: 0.324\n",
      "Iteration: 301 \t--- Loss: 0.318\n",
      "Iteration: 302 \t--- Loss: 0.317\n",
      "Iteration: 303 \t--- Loss: 0.322\n",
      "Iteration: 304 \t--- Loss: 0.324\n",
      "Iteration: 305 \t--- Loss: 0.325\n",
      "Iteration: 306 \t--- Loss: 0.312\n",
      "Iteration: 307 \t--- Loss: 0.312\n",
      "Iteration: 308 \t--- Loss: 0.315\n",
      "Iteration: 309 \t--- Loss: 0.320\n",
      "Iteration: 310 \t--- Loss: 0.321\n",
      "Iteration: 311 \t--- Loss: 0.309\n",
      "Iteration: 312 \t--- Loss: 0.326\n",
      "Iteration: 313 \t--- Loss: 0.313\n",
      "Iteration: 314 \t--- Loss: 0.324\n",
      "Iteration: 315 \t--- Loss: 0.329\n",
      "Iteration: 316 \t--- Loss: 0.318\n",
      "Iteration: 317 \t--- Loss: 0.318\n",
      "Iteration: 318 \t--- Loss: 0.317\n",
      "Iteration: 319 \t--- Loss: 0.302\n",
      "Iteration: 320 \t--- Loss: 0.307\n",
      "Iteration: 321 \t--- Loss: 0.304\n",
      "Iteration: 322 \t--- Loss: 0.333\n",
      "Iteration: 323 \t--- Loss: 0.328\n",
      "Iteration: 324 \t--- Loss: 0.322\n",
      "Iteration: 325 \t--- Loss: 0.313\n",
      "Iteration: 326 \t--- Loss: 0.311\n",
      "Iteration: 327 \t--- Loss: 0.322\n",
      "Iteration: 328 \t--- Loss: 0.319\n",
      "Iteration: 329 \t--- Loss: 0.327\n",
      "Iteration: 330 \t--- Loss: 0.335\n",
      "Iteration: 331 \t--- Loss: 0.308\n",
      "Iteration: 332 \t--- Loss: 0.326\n",
      "Iteration: 333 \t--- Loss: 0.318\n",
      "Iteration: 334 \t--- Loss: 0.302\n",
      "Iteration: 335 \t--- Loss: 0.331\n",
      "Iteration: 336 \t--- Loss: 0.324\n",
      "Iteration: 337 \t--- Loss: 0.327\n",
      "Iteration: 338 \t--- Loss: 0.308\n",
      "Iteration: 339 \t--- Loss: 0.320\n",
      "Iteration: 340 \t--- Loss: 0.314\n",
      "Iteration: 341 \t--- Loss: 0.323\n",
      "Iteration: 342 \t--- Loss: 0.321\n",
      "Iteration: 343 \t--- Loss: 0.321\n",
      "Iteration: 344 \t--- Loss: 0.296\n",
      "Iteration: 345 \t--- Loss: 0.321\n",
      "Iteration: 346 \t--- Loss: 0.313\n",
      "Iteration: 347 \t--- Loss: 0.320\n",
      "Iteration: 348 \t--- Loss: 0.316\n",
      "Iteration: 349 \t--- Loss: 0.321\n",
      "Iteration: 350 \t--- Loss: 0.317\n",
      "Iteration: 351 \t--- Loss: 0.324\n",
      "Iteration: 352 \t--- Loss: 0.322\n",
      "Iteration: 353 \t--- Loss: 0.319\n",
      "Iteration: 354 \t--- Loss: 0.335\n",
      "Iteration: 355 \t--- Loss: 0.334\n",
      "Iteration: 356 \t--- Loss: 0.326\n",
      "Iteration: 357 \t--- Loss: 0.323\n",
      "Iteration: 358 \t--- Loss: 0.327\n",
      "Iteration: 359 \t--- Loss: 0.324\n",
      "Iteration: 360 \t--- Loss: 0.313\n",
      "Iteration: 361 \t--- Loss: 0.316\n",
      "Iteration: 362 \t--- Loss: 0.319\n",
      "Iteration: 363 \t--- Loss: 0.314\n",
      "Iteration: 364 \t--- Loss: 0.316\n",
      "Iteration: 365 \t--- Loss: 0.320\n",
      "Iteration: 366 \t--- Loss: 0.328\n",
      "Iteration: 367 \t--- Loss: 0.316\n",
      "Iteration: 368 \t--- Loss: 0.310\n",
      "Iteration: 369 \t--- Loss: 0.326\n",
      "Iteration: 370 \t--- Loss: 0.320\n",
      "Iteration: 371 \t--- Loss: 0.319\n",
      "Iteration: 372 \t--- Loss: 0.330\n",
      "Iteration: 373 \t--- Loss: 0.335\n",
      "Iteration: 374 \t--- Loss: 0.327\n",
      "Iteration: 375 \t--- Loss: 0.329\n",
      "Iteration: 376 \t--- Loss: 0.332\n",
      "Iteration: 377 \t--- Loss: 0.328\n",
      "Iteration: 378 \t--- Loss: 0.323\n",
      "Iteration: 379 \t--- Loss: 0.313\n",
      "Iteration: 380 \t--- Loss: 0.322\n",
      "Iteration: 381 \t--- Loss: 0.334\n",
      "Iteration: 382 \t--- Loss: 0.326\n",
      "Iteration: 383 \t--- Loss: 0.318\n",
      "Iteration: 384 \t--- Loss: 0.318\n",
      "Iteration: 385 \t--- Loss: 0.321\n",
      "Iteration: 386 \t--- Loss: 0.326\n",
      "Iteration: 387 \t--- Loss: 0.322\n",
      "Iteration: 388 \t--- Loss: 0.333\n",
      "Iteration: 389 \t--- Loss: 0.316\n",
      "Iteration: 390 \t--- Loss: 0.316\n",
      "Iteration: 391 \t--- Loss: 0.325\n",
      "Iteration: 392 \t--- Loss: 0.334\n",
      "Iteration: 393 \t--- Loss: 0.318\n",
      "Iteration: 394 \t--- Loss: 0.328\n",
      "Iteration: 395 \t--- Loss: 0.315\n",
      "Iteration: 396 \t--- Loss: 0.310\n",
      "Iteration: 397 \t--- Loss: 0.327\n",
      "Iteration: 398 \t--- Loss: 0.328\n",
      "Iteration: 399 \t--- Loss: 0.327\n",
      "Iteration: 400 \t--- Loss: 0.317\n",
      "Iteration: 401 \t--- Loss: 0.323\n",
      "Iteration: 402 \t--- Loss: 0.324\n",
      "Iteration: 403 \t--- Loss: 0.327\n",
      "Iteration: 404 \t--- Loss: 0.313\n",
      "Iteration: 405 \t--- Loss: 0.328\n",
      "Iteration: 406 \t--- Loss: 0.331\n",
      "Iteration: 407 \t--- Loss: 0.314\n",
      "Iteration: 408 \t--- Loss: 0.318\n",
      "Iteration: 409 \t--- Loss: 0.330\n",
      "Iteration: 410 \t--- Loss: 0.311\n",
      "Iteration: 411 \t--- Loss: 0.322\n",
      "Iteration: 412 \t--- Loss: 0.323\n",
      "Iteration: 413 \t--- Loss: 0.323\n",
      "Iteration: 414 \t--- Loss: 0.305\n",
      "Iteration: 415 \t--- Loss: 0.331\n",
      "Iteration: 416 \t--- Loss: 0.337\n",
      "Iteration: 417 \t--- Loss: 0.321\n",
      "Iteration: 418 \t--- Loss: 0.307\n",
      "Iteration: 419 \t--- Loss: 0.321\n",
      "Iteration: 420 \t--- Loss: 0.322\n",
      "Iteration: 421 \t--- Loss: 0.327\n",
      "Iteration: 422 \t--- Loss: 0.315\n",
      "Iteration: 423 \t--- Loss: 0.319\n",
      "Iteration: 424 \t--- Loss: 0.310\n",
      "Iteration: 425 \t--- Loss: 0.330\n",
      "Iteration: 426 \t--- Loss: 0.318\n",
      "Iteration: 427 \t--- Loss: 0.334\n",
      "Iteration: 428 \t--- Loss: 0.316\n",
      "Iteration: 429 \t--- Loss: 0.328\n",
      "Iteration: 430 \t--- Loss: 0.309\n",
      "Iteration: 431 \t--- Loss: 0.309\n",
      "Iteration: 432 \t--- Loss: 0.317\n",
      "Iteration: 433 \t--- Loss: 0.313\n",
      "Iteration: 434 \t--- Loss: 0.324\n",
      "Iteration: 435 \t--- Loss: 0.324\n",
      "Iteration: 436 \t--- Loss: 0.320\n",
      "Iteration: 437 \t--- Loss: 0.319\n",
      "Iteration: 438 \t--- Loss: 0.330\n",
      "Iteration: 439 \t--- Loss: 0.316\n",
      "Iteration: 440 \t--- Loss: 0.327\n",
      "Iteration: 441 \t--- Loss: 0.328\n",
      "Iteration: 442 \t--- Loss: 0.316\n",
      "Iteration: 443 \t--- Loss: 0.329\n",
      "Iteration: 444 \t--- Loss: 0.317\n",
      "Iteration: 445 \t--- Loss: 0.325\n",
      "Iteration: 446 \t--- Loss: 0.333\n",
      "Iteration: 447 \t--- Loss: 0.308\n",
      "Iteration: 448 \t--- Loss: 0.320\n",
      "Iteration: 449 \t--- Loss: 0.318\n",
      "Iteration: 450 \t--- Loss: 0.323\n",
      "Iteration: 451 \t--- Loss: 0.332\n",
      "Iteration: 452 \t--- Loss: 0.325\n",
      "Iteration: 453 \t--- Loss: 0.326\n",
      "Iteration: 454 \t--- Loss: 0.319\n",
      "Iteration: 455 \t--- Loss: 0.311\n",
      "Iteration: 456 \t--- Loss: 0.320\n",
      "Iteration: 457 \t--- Loss: 0.326\n",
      "Iteration: 458 \t--- Loss: 0.324\n",
      "Iteration: 459 \t--- Loss: 0.323\n",
      "Iteration: 460 \t--- Loss: 0.316\n",
      "Iteration: 461 \t--- Loss: 0.321\n",
      "Iteration: 462 \t--- Loss: 0.318\n",
      "Iteration: 463 \t--- Loss: 0.322\n",
      "Iteration: 464 \t--- Loss: 0.313\n",
      "Iteration: 465 \t--- Loss: 0.322\n",
      "Iteration: 466 \t--- Loss: 0.320\n",
      "Iteration: 467 \t--- Loss: 0.323\n",
      "Iteration: 468 \t--- Loss: 0.313\n",
      "Iteration: 469 \t--- Loss: 0.332\n",
      "Iteration: 470 \t--- Loss: 0.333\n",
      "Iteration: 471 \t--- Loss: 0.312\n",
      "Iteration: 472 \t--- Loss: 0.328\n",
      "Iteration: 473 \t--- Loss: 0.318\n",
      "Iteration: 474 \t--- Loss: 0.311\n",
      "Iteration: 475 \t--- Loss: 0.312\n",
      "Iteration: 476 \t--- Loss: 0.322\n",
      "Iteration: 477 \t--- Loss: 0.310\n",
      "Iteration: 478 \t--- Loss: 0.329\n",
      "Iteration: 479 \t--- Loss: 0.311\n",
      "Iteration: 480 \t--- Loss: 0.328\n",
      "Iteration: 481 \t--- Loss: 0.338\n",
      "Iteration: 482 \t--- Loss: 0.329\n",
      "Iteration: 483 \t--- Loss: 0.317\n",
      "Iteration: 484 \t--- Loss: 0.329\n",
      "Iteration: 485 \t--- Loss: 0.317\n",
      "Iteration: 486 \t--- Loss: 0.332\n",
      "Iteration: 487 \t--- Loss: 0.325\n",
      "Iteration: 488 \t--- Loss: 0.320\n",
      "Iteration: 489 \t--- Loss: 0.328\n",
      "Iteration: 490 \t--- Loss: 0.322\n",
      "Iteration: 491 \t--- Loss: 0.323\n",
      "Iteration: 492 \t--- Loss: 0.320\n",
      "Iteration: 493 \t--- Loss: 0.331\n",
      "Iteration: 494 \t--- Loss: 0.331\n",
      "Iteration: 495 \t--- Loss: 0.335\n",
      "Iteration: 496 \t--- Loss: 0.334\n",
      "Iteration: 497 \t--- Loss: 0.322\n",
      "Iteration: 498 \t--- Loss: 0.320\n",
      "Iteration: 499 \t--- Loss: 0.328\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:07<00:07,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.103\n",
      "Iteration: 261 \t--- Loss: 0.107\n",
      "Iteration: 262 \t--- Loss: 0.104\n",
      "Iteration: 263 \t--- Loss: 0.105\n",
      "Iteration: 264 \t--- Loss: 0.104\n",
      "Iteration: 265 \t--- Loss: 0.108\n",
      "Iteration: 266 \t--- Loss: 0.108\n",
      "Iteration: 267 \t--- Loss: 0.106\n",
      "Iteration: 268 \t--- Loss: 0.102\n",
      "Iteration: 269 \t--- Loss: 0.120\n",
      "Iteration: 270 \t--- Loss: 0.105\n",
      "Iteration: 271 \t--- Loss: 0.101\n",
      "Iteration: 272 \t--- Loss: 0.108\n",
      "Iteration: 273 \t--- Loss: 0.096\n",
      "Iteration: 274 \t--- Loss: 0.103\n",
      "Iteration: 275 \t--- Loss: 0.098\n",
      "Iteration: 276 \t--- Loss: 0.101\n",
      "Iteration: 277 \t--- Loss: 0.094\n",
      "Iteration: 278 \t--- Loss: 0.105\n",
      "Iteration: 279 \t--- Loss: 0.098\n",
      "Iteration: 280 \t--- Loss: 0.111\n",
      "Iteration: 281 \t--- Loss: 0.095\n",
      "Iteration: 282 \t--- Loss: 0.106\n",
      "Iteration: 283 \t--- Loss: 0.096\n",
      "Iteration: 284 \t--- Loss: 0.105\n",
      "Iteration: 285 \t--- Loss: 0.104\n",
      "Iteration: 286 \t--- Loss: 0.098\n",
      "Iteration: 287 \t--- Loss: 0.098\n",
      "Iteration: 288 \t--- Loss: 0.107\n",
      "Iteration: 289 \t--- Loss: 0.099\n",
      "Iteration: 290 \t--- Loss: 0.098\n",
      "Iteration: 291 \t--- Loss: 0.091\n",
      "Iteration: 292 \t--- Loss: 0.101\n",
      "Iteration: 293 \t--- Loss: 0.102\n",
      "Iteration: 294 \t--- Loss: 0.101\n",
      "Iteration: 295 \t--- Loss: 0.099\n",
      "Iteration: 296 \t--- Loss: 0.112\n",
      "Iteration: 297 \t--- Loss: 0.109\n",
      "Iteration: 298 \t--- Loss: 0.097\n",
      "Iteration: 299 \t--- Loss: 0.097\n",
      "Iteration: 300 \t--- Loss: 0.098\n",
      "Iteration: 301 \t--- Loss: 0.098\n",
      "Iteration: 302 \t--- Loss: 0.097\n",
      "Iteration: 303 \t--- Loss: 0.100\n",
      "Iteration: 304 \t--- Loss: 0.100\n",
      "Iteration: 305 \t--- Loss: 0.101\n",
      "Iteration: 306 \t--- Loss: 0.096\n",
      "Iteration: 307 \t--- Loss: 0.089\n",
      "Iteration: 308 \t--- Loss: 0.095\n",
      "Iteration: 309 \t--- Loss: 0.098\n",
      "Iteration: 310 \t--- Loss: 0.100\n",
      "Iteration: 311 \t--- Loss: 0.096\n",
      "Iteration: 312 \t--- Loss: 0.105\n",
      "Iteration: 313 \t--- Loss: 0.090\n",
      "Iteration: 314 \t--- Loss: 0.096\n",
      "Iteration: 315 \t--- Loss: 0.096\n",
      "Iteration: 316 \t--- Loss: 0.102\n",
      "Iteration: 317 \t--- Loss: 0.098\n",
      "Iteration: 318 \t--- Loss: 0.094\n",
      "Iteration: 319 \t--- Loss: 0.100\n",
      "Iteration: 320 \t--- Loss: 0.098\n",
      "Iteration: 321 \t--- Loss: 0.095\n",
      "Iteration: 322 \t--- Loss: 0.101\n",
      "Iteration: 323 \t--- Loss: 0.099\n",
      "Iteration: 324 \t--- Loss: 0.104\n",
      "Iteration: 325 \t--- Loss: 0.104\n",
      "Iteration: 326 \t--- Loss: 0.099\n",
      "Iteration: 327 \t--- Loss: 0.093\n",
      "Iteration: 328 \t--- Loss: 0.097\n",
      "Iteration: 329 \t--- Loss: 0.091\n",
      "Iteration: 330 \t--- Loss: 0.105\n",
      "Iteration: 331 \t--- Loss: 0.098\n",
      "Iteration: 332 \t--- Loss: 0.101\n",
      "Iteration: 333 \t--- Loss: 0.099\n",
      "Iteration: 334 \t--- Loss: 0.114\n",
      "Iteration: 335 \t--- Loss: 0.111\n",
      "Iteration: 336 \t--- Loss: 0.117\n",
      "Iteration: 337 \t--- Loss: 0.100\n",
      "Iteration: 338 \t--- Loss: 0.096\n",
      "Iteration: 339 \t--- Loss: 0.098\n",
      "Iteration: 340 \t--- Loss: 0.102\n",
      "Iteration: 341 \t--- Loss: 0.103\n",
      "Iteration: 342 \t--- Loss: 0.101\n",
      "Iteration: 343 \t--- Loss: 0.100\n",
      "Iteration: 344 \t--- Loss: 0.091\n",
      "Iteration: 345 \t--- Loss: 0.098\n",
      "Iteration: 346 \t--- Loss: 0.102\n",
      "Iteration: 347 \t--- Loss: 0.095\n",
      "Iteration: 348 \t--- Loss: 0.091\n",
      "Iteration: 349 \t--- Loss: 0.101\n",
      "Iteration: 350 \t--- Loss: 0.100\n",
      "Iteration: 351 \t--- Loss: 0.095\n",
      "Iteration: 352 \t--- Loss: 0.094\n",
      "Iteration: 353 \t--- Loss: 0.099\n",
      "Iteration: 354 \t--- Loss: 0.093\n",
      "Iteration: 355 \t--- Loss: 0.100\n",
      "Iteration: 356 \t--- Loss: 0.097\n",
      "Iteration: 357 \t--- Loss: 0.095\n",
      "Iteration: 358 \t--- Loss: 0.092\n",
      "Iteration: 359 \t--- Loss: 0.103\n",
      "Iteration: 360 \t--- Loss: 0.094\n",
      "Iteration: 361 \t--- Loss: 0.097\n",
      "Iteration: 362 \t--- Loss: 0.092\n",
      "Iteration: 363 \t--- Loss: 0.105\n",
      "Iteration: 364 \t--- Loss: 0.104\n",
      "Iteration: 365 \t--- Loss: 0.105\n",
      "Iteration: 366 \t--- Loss: 0.103\n",
      "Iteration: 367 \t--- Loss: 0.107\n",
      "Iteration: 368 \t--- Loss: 0.105\n",
      "Iteration: 369 \t--- Loss: 0.108\n",
      "Iteration: 370 \t--- Loss: 0.102\n",
      "Iteration: 371 \t--- Loss: 0.108\n",
      "Iteration: 372 \t--- Loss: 0.102\n",
      "Iteration: 373 \t--- Loss: 0.106\n",
      "Iteration: 374 \t--- Loss: 0.098\n",
      "Iteration: 375 \t--- Loss: 0.098\n",
      "Iteration: 376 \t--- Loss: 0.097\n",
      "Iteration: 377 \t--- Loss: 0.092\n",
      "Iteration: 378 \t--- Loss: 0.092\n",
      "Iteration: 379 \t--- Loss: 0.097\n",
      "Iteration: 380 \t--- Loss: 0.100\n",
      "Iteration: 381 \t--- Loss: 0.100\n",
      "Iteration: 382 \t--- Loss: 0.093\n",
      "Iteration: 383 \t--- Loss: 0.100\n",
      "Iteration: 384 \t--- Loss: 0.099\n",
      "Iteration: 385 \t--- Loss: 0.106\n",
      "Iteration: 386 \t--- Loss: 0.103\n",
      "Iteration: 387 \t--- Loss: 0.101\n",
      "Iteration: 388 \t--- Loss: 0.099\n",
      "Iteration: 389 \t--- Loss: 0.097\n",
      "Iteration: 390 \t--- Loss: 0.088\n",
      "Iteration: 391 \t--- Loss: 0.110\n",
      "Iteration: 392 \t--- Loss: 0.099\n",
      "Iteration: 393 \t--- Loss: 0.101\n",
      "Iteration: 394 \t--- Loss: 0.098\n",
      "Iteration: 395 \t--- Loss: 0.129\n",
      "Iteration: 396 \t--- Loss: 0.110\n",
      "Iteration: 397 \t--- Loss: 0.095\n",
      "Iteration: 398 \t--- Loss: 0.098\n",
      "Iteration: 399 \t--- Loss: 0.087\n",
      "Iteration: 400 \t--- Loss: 0.097\n",
      "Iteration: 401 \t--- Loss: 0.090\n",
      "Iteration: 402 \t--- Loss: 0.104\n",
      "Iteration: 403 \t--- Loss: 0.093\n",
      "Iteration: 404 \t--- Loss: 0.091\n",
      "Iteration: 405 \t--- Loss: 0.096\n",
      "Iteration: 406 \t--- Loss: 0.106\n",
      "Iteration: 407 \t--- Loss: 0.100\n",
      "Iteration: 408 \t--- Loss: 0.095\n",
      "Iteration: 409 \t--- Loss: 0.097\n",
      "Iteration: 410 \t--- Loss: 0.092\n",
      "Iteration: 411 \t--- Loss: 0.097\n",
      "Iteration: 412 \t--- Loss: 0.093\n",
      "Iteration: 413 \t--- Loss: 0.098\n",
      "Iteration: 414 \t--- Loss: 0.097\n",
      "Iteration: 415 \t--- Loss: 0.102\n",
      "Iteration: 416 \t--- Loss: 0.096\n",
      "Iteration: 417 \t--- Loss: 0.101\n",
      "Iteration: 418 \t--- Loss: 0.104\n",
      "Iteration: 419 \t--- Loss: 0.110\n",
      "Iteration: 420 \t--- Loss: 0.096\n",
      "Iteration: 421 \t--- Loss: 0.098\n",
      "Iteration: 422 \t--- Loss: 0.097\n",
      "Iteration: 423 \t--- Loss: 0.102\n",
      "Iteration: 424 \t--- Loss: 0.092\n",
      "Iteration: 425 \t--- Loss: 0.093\n",
      "Iteration: 426 \t--- Loss: 0.116\n",
      "Iteration: 427 \t--- Loss: 0.108\n",
      "Iteration: 428 \t--- Loss: 0.115\n",
      "Iteration: 429 \t--- Loss: 0.100\n",
      "Iteration: 430 \t--- Loss: 0.091\n",
      "Iteration: 431 \t--- Loss: 0.103\n",
      "Iteration: 432 \t--- Loss: 0.103\n",
      "Iteration: 433 \t--- Loss: 0.099\n",
      "Iteration: 434 \t--- Loss: 0.108\n",
      "Iteration: 435 \t--- Loss: 0.110\n",
      "Iteration: 436 \t--- Loss: 0.091\n",
      "Iteration: 437 \t--- Loss: 0.107\n",
      "Iteration: 438 \t--- Loss: 0.098\n",
      "Iteration: 439 \t--- Loss: 0.091\n",
      "Iteration: 440 \t--- Loss: 0.099\n",
      "Iteration: 441 \t--- Loss: 0.109\n",
      "Iteration: 442 \t--- Loss: 0.094\n",
      "Iteration: 443 \t--- Loss: 0.091\n",
      "Iteration: 444 \t--- Loss: 0.090\n",
      "Iteration: 445 \t--- Loss: 0.088\n",
      "Iteration: 446 \t--- Loss: 0.103\n",
      "Iteration: 447 \t--- Loss: 0.100\n",
      "Iteration: 448 \t--- Loss: 0.114\n",
      "Iteration: 449 \t--- Loss: 0.102\n",
      "Iteration: 450 \t--- Loss: 0.098\n",
      "Iteration: 451 \t--- Loss: 0.098\n",
      "Iteration: 452 \t--- Loss: 0.110\n",
      "Iteration: 453 \t--- Loss: 0.106\n",
      "Iteration: 454 \t--- Loss: 0.134\n",
      "Iteration: 455 \t--- Loss: 0.110\n",
      "Iteration: 456 \t--- Loss: 0.125\n",
      "Iteration: 457 \t--- Loss: 0.100\n",
      "Iteration: 458 \t--- Loss: 0.119\n",
      "Iteration: 459 \t--- Loss: 0.107\n",
      "Iteration: 460 \t--- Loss: 0.102\n",
      "Iteration: 461 \t--- Loss: 0.091\n",
      "Iteration: 462 \t--- Loss: 0.107\n",
      "Iteration: 463 \t--- Loss: 0.097\n",
      "Iteration: 464 \t--- Loss: 0.107\n",
      "Iteration: 465 \t--- Loss: 0.096\n",
      "Iteration: 466 \t--- Loss: 0.091\n",
      "Iteration: 467 \t--- Loss: 0.097\n",
      "Iteration: 468 \t--- Loss: 0.104\n",
      "Iteration: 469 \t--- Loss: 0.108\n",
      "Iteration: 470 \t--- Loss: 0.097\n",
      "Iteration: 471 \t--- Loss: 0.099\n",
      "Iteration: 472 \t--- Loss: 0.105\n",
      "Iteration: 473 \t--- Loss: 0.089\n",
      "Iteration: 474 \t--- Loss: 0.097\n",
      "Iteration: 475 \t--- Loss: 0.087\n",
      "Iteration: 476 \t--- Loss: 0.094\n",
      "Iteration: 477 \t--- Loss: 0.100\n",
      "Iteration: 478 \t--- Loss: 0.097\n",
      "Iteration: 479 \t--- Loss: 0.097\n",
      "Iteration: 480 \t--- Loss: 0.098\n",
      "Iteration: 481 \t--- Loss: 0.090\n",
      "Iteration: 482 \t--- Loss: 0.103\n",
      "Iteration: 483 \t--- Loss: 0.097\n",
      "Iteration: 484 \t--- Loss: 0.093\n",
      "Iteration: 485 \t--- Loss: 0.096\n",
      "Iteration: 486 \t--- Loss: 0.109\n",
      "Iteration: 487 \t--- Loss: 0.105\n",
      "Iteration: 488 \t--- Loss: 0.101\n",
      "Iteration: 489 \t--- Loss: 0.103\n",
      "Iteration: 490 \t--- Loss: 0.093\n",
      "Iteration: 491 \t--- Loss: 0.100\n",
      "Iteration: 492 \t--- Loss: 0.093\n",
      "Iteration: 493 \t--- Loss: 0.101\n",
      "Iteration: 494 \t--- Loss: 0.095\n",
      "Iteration: 495 \t--- Loss: 0.113\n",
      "Iteration: 496 \t--- Loss: 0.105\n",
      "Iteration: 497 \t--- Loss: 0.099\n",
      "Iteration: 498 \t--- Loss: 0.100\n",
      "Iteration: 499 \t--- Loss: 0.101\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.59s/it][Parallel(n_jobs=5)]: Done  28 tasks      | elapsed: 14.5min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it][Parallel(n_jobs=5)]: Done  29 tasks      | elapsed: 14.6min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.341\n",
      "Iteration: 1 \t--- Loss: 0.333\n",
      "Iteration: 2 \t--- Loss: 0.322\n",
      "Iteration: 3 \t--- Loss: 0.314\n",
      "Iteration: 4 \t--- Loss: 0.309\n",
      "Iteration: 5 \t--- Loss: 0.306\n",
      "Iteration: 6 \t--- Loss: 0.307\n",
      "Iteration: 7 \t--- Loss: 0.324\n",
      "Iteration: 8 \t--- Loss: 0.295\n",
      "Iteration: 9 \t--- Loss: 0.282\n",
      "Iteration: 10 \t--- Loss: 0.295\n",
      "Iteration: 11 \t--- Loss: 0.269\n",
      "Iteration: 12 \t--- Loss: 0.300\n",
      "Iteration: 13 \t--- Loss: 0.269\n",
      "Iteration: 14 \t--- Loss: 0.287\n",
      "Iteration: 15 \t--- Loss: 0.287\n",
      "Iteration: 16 \t--- Loss: 0.306\n",
      "Iteration: 17 \t--- Loss: 0.263\n",
      "Iteration: 18 \t--- Loss: 0.257\n",
      "Iteration: 19 \t--- Loss: 0.247\n",
      "Iteration: 20 \t--- Loss: 0.285\n",
      "Iteration: 21 \t--- Loss: 0.283\n",
      "Iteration: 22 \t--- Loss: 0.258\n",
      "Iteration: 23 \t--- Loss: 0.253\n",
      "Iteration: 24 \t--- Loss: 0.255\n",
      "Iteration: 25 \t--- Loss: 0.275\n",
      "Iteration: 26 \t--- Loss: 0.268\n",
      "Iteration: 27 \t--- Loss: 0.294\n",
      "Iteration: 28 \t--- Loss: 0.260\n",
      "Iteration: 29 \t--- Loss: 0.274\n",
      "Iteration: 30 \t--- Loss: 0.270\n",
      "Iteration: 31 \t--- Loss: 0.270\n",
      "Iteration: 32 \t--- Loss: 0.280\n",
      "Iteration: 33 \t--- Loss: 0.276\n",
      "Iteration: 34 \t--- Loss: 0.273\n",
      "Iteration: 35 \t--- Loss: 0.272\n",
      "Iteration: 36 \t--- Loss: 0.266\n",
      "Iteration: 37 \t--- Loss: 0.250\n",
      "Iteration: 38 \t--- Loss: 0.263\n",
      "Iteration: 39 \t--- Loss: 0.274\n",
      "Iteration: 40 \t--- Loss: 0.245\n",
      "Iteration: 41 \t--- Loss: 0.255\n",
      "Iteration: 42 \t--- Loss: 0.255\n",
      "Iteration: 43 \t--- Loss: 0.269\n",
      "Iteration: 44 \t--- Loss: 0.262\n",
      "Iteration: 45 \t--- Loss: 0.252\n",
      "Iteration: 46 \t--- Loss: 0.241\n",
      "Iteration: 47 \t--- Loss: 0.258\n",
      "Iteration: 48 \t--- Loss: 0.244\n",
      "Iteration: 49 \t--- Loss: 0.254\n",
      "Iteration: 50 \t--- Loss: 0.277\n",
      "Iteration: 51 \t--- Loss: 0.265\n",
      "Iteration: 52 \t--- Loss: 0.266\n",
      "Iteration: 53 \t--- Loss: 0.264\n",
      "Iteration: 54 \t--- Loss: 0.244\n",
      "Iteration: 55 \t--- Loss: 0.268\n",
      "Iteration: 56 \t--- Loss: 0.239\n",
      "Iteration: 57 \t--- Loss: 0.276\n",
      "Iteration: 58 \t--- Loss: 0.263\n",
      "Iteration: 59 \t--- Loss: 0.250\n",
      "Iteration: 60 \t--- Loss: 0.273\n",
      "Iteration: 61 \t--- Loss: 0.255\n",
      "Iteration: 62 \t--- Loss: 0.262\n",
      "Iteration: 63 \t--- Loss: 0.271\n",
      "Iteration: 64 \t--- Loss: 0.270\n",
      "Iteration: 65 \t--- Loss: 0.250\n",
      "Iteration: 66 \t--- Loss: 0.254\n",
      "Iteration: 67 \t--- Loss: 0.252\n",
      "Iteration: 68 \t--- Loss: 0.245\n",
      "Iteration: 69 \t--- Loss: 0.247\n",
      "Iteration: 70 \t--- Loss: 0.247\n",
      "Iteration: 71 \t--- Loss: 0.243\n",
      "Iteration: 72 \t--- Loss: 0.251\n",
      "Iteration: 73 \t--- Loss: 0.251\n",
      "Iteration: 74 \t--- Loss: 0.258\n",
      "Iteration: 75 \t--- Loss: 0.255\n",
      "Iteration: 76 \t--- Loss: 0.245\n",
      "Iteration: 77 \t--- Loss: 0.260\n",
      "Iteration: 78 \t--- Loss: 0.263\n",
      "Iteration: 79 \t--- Loss: 0.273\n",
      "Iteration: 80 \t--- Loss: 0.244\n",
      "Iteration: 81 \t--- Loss: 0.272\n",
      "Iteration: 82 \t--- Loss: 0.261\n",
      "Iteration: 83 \t--- Loss: 0.269\n",
      "Iteration: 84 \t--- Loss: 0.254\n",
      "Iteration: 85 \t--- Loss: 0.249\n",
      "Iteration: 86 \t--- Loss: 0.253\n",
      "Iteration: 87 \t--- Loss: 0.236\n",
      "Iteration: 88 \t--- Loss: 0.251\n",
      "Iteration: 89 \t--- Loss: 0.255\n",
      "Iteration: 90 \t--- Loss: 0.236\n",
      "Iteration: 91 \t--- Loss: 0.254\n",
      "Iteration: 92 \t--- Loss: 0.265\n",
      "Iteration: 93 \t--- Loss: 0.238\n",
      "Iteration: 94 \t--- Loss: 0.252\n",
      "Iteration: 95 \t--- Loss: 0.266\n",
      "Iteration: 96 \t--- Loss: 0.252\n",
      "Iteration: 97 \t--- Loss: 0.284\n",
      "Iteration: 98 \t--- Loss: 0.243\n",
      "Iteration: 99 \t--- Loss: 0.267\n",
      "Iteration: 100 \t--- Loss: 0.266\n",
      "Iteration: 101 \t--- Loss: 0.235\n",
      "Iteration: 102 \t--- Loss: 0.241\n",
      "Iteration: 103 \t--- Loss: 0.232\n",
      "Iteration: 104 \t--- Loss: 0.275\n",
      "Iteration: 105 \t--- Loss: 0.283\n",
      "Iteration: 106 \t--- Loss: 0.245\n",
      "Iteration: 107 \t--- Loss: 0.256\n",
      "Iteration: 108 \t--- Loss: 0.242\n",
      "Iteration: 109 \t--- Loss: 0.242\n",
      "Iteration: 110 \t--- Loss: 0.263\n",
      "Iteration: 111 \t--- Loss: 0.233\n",
      "Iteration: 112 \t--- Loss: 0.270\n",
      "Iteration: 113 \t--- Loss: 0.244\n",
      "Iteration: 114 \t--- Loss: 0.250\n",
      "Iteration: 115 \t--- Loss: 0.268\n",
      "Iteration: 116 \t--- Loss: 0.269\n",
      "Iteration: 117 \t--- Loss: 0.262\n",
      "Iteration: 118 \t--- Loss: 0.276\n",
      "Iteration: 119 \t--- Loss: 0.239\n",
      "Iteration: 120 \t--- Loss: 0.239\n",
      "Iteration: 121 \t--- Loss: 0.251\n",
      "Iteration: 122 \t--- Loss: 0.241\n",
      "Iteration: 123 \t--- Loss: 0.279\n",
      "Iteration: 124 \t--- Loss: 0.262\n",
      "Iteration: 125 \t--- Loss: 0.288\n",
      "Iteration: 126 \t--- Loss: 0.274\n",
      "Iteration: 127 \t--- Loss: 0.259\n",
      "Iteration: 128 \t--- Loss: 0.239\n",
      "Iteration: 129 \t--- Loss: 0.250\n",
      "Iteration: 130 \t--- Loss: 0.263\n",
      "Iteration: 131 \t--- Loss: 0.252\n",
      "Iteration: 132 \t--- Loss: 0.249\n",
      "Iteration: 133 \t--- Loss: 0.270\n",
      "Iteration: 134 \t--- Loss: 0.238\n",
      "Iteration: 135 \t--- Loss: 0.261\n",
      "Iteration: 136 \t--- Loss: 0.256\n",
      "Iteration: 137 \t--- Loss: 0.285\n",
      "Iteration: 138 \t--- Loss: 0.248\n",
      "Iteration: 139 \t--- Loss: 0.243\n",
      "Iteration: 140 \t--- Loss: 0.234\n",
      "Iteration: 141 \t--- Loss: 0.267\n",
      "Iteration: 142 \t--- Loss: 0.255\n",
      "Iteration: 143 \t--- Loss: 0.260\n",
      "Iteration: 144 \t--- Loss: 0.259\n",
      "Iteration: 145 \t--- Loss: 0.236\n",
      "Iteration: 146 \t--- Loss: 0.268\n",
      "Iteration: 147 \t--- Loss: 0.278\n",
      "Iteration: 148 \t--- Loss: 0.273\n",
      "Iteration: 149 \t--- Loss: 0.251\n",
      "Iteration: 150 \t--- Loss: 0.228\n",
      "Iteration: 151 \t--- Loss: 0.246\n",
      "Iteration: 152 \t--- Loss: 0.270\n",
      "Iteration: 153 \t--- Loss: 0.249\n",
      "Iteration: 154 \t--- Loss: 0.257\n",
      "Iteration: 155 \t--- Loss: 0.243\n",
      "Iteration: 156 \t--- Loss: 0.268\n",
      "Iteration: 157 \t--- Loss: 0.253\n",
      "Iteration: 158 \t--- Loss: 0.266\n",
      "Iteration: 159 \t--- Loss: 0.241\n",
      "Iteration: 160 \t--- Loss: 0.243\n",
      "Iteration: 161 \t--- Loss: 0.260\n",
      "Iteration: 162 \t--- Loss: 0.265\n",
      "Iteration: 163 \t--- Loss: 0.252\n",
      "Iteration: 164 \t--- Loss: 0.241\n",
      "Iteration: 165 \t--- Loss: 0.261\n",
      "Iteration: 166 \t--- Loss: 0.247\n",
      "Iteration: 167 \t--- Loss: 0.272\n",
      "Iteration: 168 \t--- Loss: 0.238\n",
      "Iteration: 169 \t--- Loss: 0.257\n",
      "Iteration: 170 \t--- Loss: 0.244\n",
      "Iteration: 171 \t--- Loss: 0.267\n",
      "Iteration: 172 \t--- Loss: 0.251\n",
      "Iteration: 173 \t--- Loss: 0.248\n",
      "Iteration: 174 \t--- Loss: 0.246\n",
      "Iteration: 175 \t--- Loss: 0.265\n",
      "Iteration: 176 \t--- Loss: 0.224\n",
      "Iteration: 177 \t--- Loss: 0.270\n",
      "Iteration: 178 \t--- Loss: 0.240\n",
      "Iteration: 179 \t--- Loss: 0.272\n",
      "Iteration: 180 \t--- Loss: 0.244\n",
      "Iteration: 181 \t--- Loss: 0.253\n",
      "Iteration: 182 \t--- Loss: 0.250\n",
      "Iteration: 183 \t--- Loss: 0.267\n",
      "Iteration: 184 \t--- Loss: 0.286\n",
      "Iteration: 185 \t--- Loss: 0.227\n",
      "Iteration: 186 \t--- Loss: 0.257\n",
      "Iteration: 187 \t--- Loss: 0.258\n",
      "Iteration: 188 \t--- Loss: 0.251\n",
      "Iteration: 189 \t--- Loss: 0.250\n",
      "Iteration: 190 \t--- Loss: 0.260\n",
      "Iteration: 191 \t--- Loss: 0.250\n",
      "Iteration: 192 \t--- Loss: 0.260\n",
      "Iteration: 193 \t--- Loss: 0.277\n",
      "Iteration: 194 \t--- Loss: 0.253\n",
      "Iteration: 195 \t--- Loss: 0.246\n",
      "Iteration: 196 \t--- Loss: 0.237\n",
      "Iteration: 197 \t--- Loss: 0.241\n",
      "Iteration: 198 \t--- Loss: 0.248\n",
      "Iteration: 199 \t--- Loss: 0.249\n",
      "Iteration: 200 \t--- Loss: 0.234\n",
      "Iteration: 201 \t--- Loss: 0.260\n",
      "Iteration: 202 \t--- Loss: 0.257\n",
      "Iteration: 203 \t--- Loss: 0.254\n",
      "Iteration: 204 \t--- Loss: 0.246\n",
      "Iteration: 205 \t--- Loss: 0.264\n",
      "Iteration: 206 \t--- Loss: 0.247\n",
      "Iteration: 207 \t--- Loss: 0.249\n",
      "Iteration: 208 \t--- Loss: 0.239\n",
      "Iteration: 209 \t--- Loss: 0.267\n",
      "Iteration: 210 \t--- Loss: 0.260\n",
      "Iteration: 211 \t--- Loss: 0.244\n",
      "Iteration: 212 \t--- Loss: 0.245\n",
      "Iteration: 213 \t--- Loss: 0.242\n",
      "Iteration: 214 \t--- Loss: 0.260\n",
      "Iteration: 215 \t--- Loss: 0.248\n",
      "Iteration: 216 \t--- Loss: 0.236\n",
      "Iteration: 217 \t--- Loss: 0.245\n",
      "Iteration: 218 \t--- Loss: 0.275\n",
      "Iteration: 219 \t--- Loss: 0.265\n",
      "Iteration: 220 \t--- Loss: 0.247\n",
      "Iteration: 221 \t--- Loss: 0.270\n",
      "Iteration: 222 \t--- Loss: 0.262\n",
      "Iteration: 223 \t--- Loss: 0.250\n",
      "Iteration: 224 \t--- Loss: 0.259\n",
      "Iteration: 225 \t--- Loss: 0.222\n",
      "Iteration: 226 \t--- Loss: 0.256\n",
      "Iteration: 227 \t--- Loss: 0.268\n",
      "Iteration: 228 \t--- Loss: 0.268\n",
      "Iteration: 229 \t--- Loss: 0.262\n",
      "Iteration: 230 \t--- Loss: 0.248\n",
      "Iteration: 231 \t--- Loss: 0.248\n",
      "Iteration: 232 \t--- Loss: 0.256\n",
      "Iteration: 233 \t--- Loss: 0.256\n",
      "Iteration: 234 \t--- Loss: 0.262\n",
      "Iteration: 235 \t--- Loss: 0.244\n",
      "Iteration: 236 \t--- Loss: 0.244\n",
      "Iteration: 237 \t--- Loss: 0.251\n",
      "Iteration: 238 \t--- Loss: 0.263\n",
      "Iteration: 239 \t--- Loss: 0.241\n",
      "Iteration: 240 \t--- Loss: 0.257\n",
      "Iteration: 241 \t--- Loss: 0.253\n",
      "Iteration: 242 \t--- Loss: 0.254\n",
      "Iteration: 243 \t--- Loss: 0.258\n",
      "Iteration: 244 \t--- Loss: 0.259\n",
      "Iteration: 245 \t--- Loss: 0.258\n",
      "Iteration: 246 \t--- Loss: 0.255\n",
      "Iteration: 247 \t--- Loss: 0.254\n",
      "Iteration: 248 \t--- Loss: 0.249\n",
      "Iteration: 249 \t--- Loss: 0.249\n",
      "Iteration: 250 \t--- Loss: 0.244\n",
      "Iteration: 251 \t--- Loss: 0.251\n",
      "Iteration: 252 \t--- Loss: 0.274\n",
      "Iteration: 253 \t--- Loss: 0.235\n",
      "Iteration: 254 \t--- Loss: 0.249\n",
      "Iteration: 255 \t--- Loss: 0.237\n",
      "Iteration: 256 \t--- Loss: 0.250\n",
      "Iteration: 257 \t--- Loss: 0.297\n",
      "Iteration: 258 \t--- Loss: 0.246\n",
      "Iteration: 259 \t--- Loss: 0.253Iteration: 0 \t--- Loss: 1.048\n",
      "Iteration: 1 \t--- Loss: 0.996\n",
      "Iteration: 2 \t--- Loss: 0.882\n",
      "Iteration: 3 \t--- Loss: 0.857\n",
      "Iteration: 4 \t--- Loss: 0.771\n",
      "Iteration: 5 \t--- Loss: 0.726\n",
      "Iteration: 6 \t--- Loss: 0.699\n",
      "Iteration: 7 \t--- Loss: 0.672\n",
      "Iteration: 8 \t--- Loss: 0.650\n",
      "Iteration: 9 \t--- Loss: 0.638\n",
      "Iteration: 10 \t--- Loss: 0.624\n",
      "Iteration: 11 \t--- Loss: 0.632\n",
      "Iteration: 12 \t--- Loss: 0.629\n",
      "Iteration: 13 \t--- Loss: 0.617\n",
      "Iteration: 14 \t--- Loss: 0.597\n",
      "Iteration: 15 \t--- Loss: 0.602\n",
      "Iteration: 16 \t--- Loss: 0.591\n",
      "Iteration: 17 \t--- Loss: 0.596\n",
      "Iteration: 18 \t--- Loss: 0.582\n",
      "Iteration: 19 \t--- Loss: 0.588\n",
      "Iteration: 20 \t--- Loss: 0.577\n",
      "Iteration: 21 \t--- Loss: 0.582\n",
      "Iteration: 22 \t--- Loss: 0.577\n",
      "Iteration: 23 \t--- Loss: 0.582\n",
      "Iteration: 24 \t--- Loss: 0.578\n",
      "Iteration: 25 \t--- Loss: 0.577\n",
      "Iteration: 26 \t--- Loss: 0.588\n",
      "Iteration: 27 \t--- Loss: 0.584\n",
      "Iteration: 28 \t--- Loss: 0.580\n",
      "Iteration: 29 \t--- Loss: 0.576\n",
      "Iteration: 30 \t--- Loss: 0.579\n",
      "Iteration: 31 \t--- Loss: 0.570\n",
      "Iteration: 32 \t--- Loss: 0.573\n",
      "Iteration: 33 \t--- Loss: 0.574\n",
      "Iteration: 34 \t--- Loss: 0.574\n",
      "Iteration: 35 \t--- Loss: 0.568\n",
      "Iteration: 36 \t--- Loss: 0.564\n",
      "Iteration: 37 \t--- Loss: 0.576\n",
      "Iteration: 38 \t--- Loss: 0.572\n",
      "Iteration: 39 \t--- Loss: 0.582\n",
      "Iteration: 40 \t--- Loss: 0.568\n",
      "Iteration: 41 \t--- Loss: 0.578\n",
      "Iteration: 42 \t--- Loss: 0.571\n",
      "Iteration: 43 \t--- Loss: 0.566\n",
      "Iteration: 44 \t--- Loss: 0.568\n",
      "Iteration: 45 \t--- Loss: 0.577\n",
      "Iteration: 46 \t--- Loss: 0.581\n",
      "Iteration: 47 \t--- Loss: 0.580\n",
      "Iteration: 48 \t--- Loss: 0.576\n",
      "Iteration: 49 \t--- Loss: 0.595\n",
      "Iteration: 50 \t--- Loss: 0.571\n",
      "Iteration: 51 \t--- Loss: 0.571\n",
      "Iteration: 52 \t--- Loss: 0.575\n",
      "Iteration: 53 \t--- Loss: 0.571\n",
      "Iteration: 54 \t--- Loss: 0.571\n",
      "Iteration: 55 \t--- Loss: 0.578\n",
      "Iteration: 56 \t--- Loss: 0.569\n",
      "Iteration: 57 \t--- Loss: 0.571\n",
      "Iteration: 58 \t--- Loss: 0.567\n",
      "Iteration: 59 \t--- Loss: 0.568\n",
      "Iteration: 60 \t--- Loss: 0.573\n",
      "Iteration: 61 \t--- Loss: 0.571\n",
      "Iteration: 62 \t--- Loss: 0.570\n",
      "Iteration: 63 \t--- Loss: 0.574\n",
      "Iteration: 64 \t--- Loss: 0.570\n",
      "Iteration: 65 \t--- Loss: 0.566\n",
      "Iteration: 66 \t--- Loss: 0.582\n",
      "Iteration: 67 \t--- Loss: 0.571\n",
      "Iteration: 68 \t--- Loss: 0.577\n",
      "Iteration: 69 \t--- Loss: 0.578\n",
      "Iteration: 70 \t--- Loss: 0.563\n",
      "Iteration: 71 \t--- Loss: 0.565\n",
      "Iteration: 72 \t--- Loss: 0.572\n",
      "Iteration: 73 \t--- Loss: 0.576\n",
      "Iteration: 74 \t--- Loss: 0.572\n",
      "Iteration: 75 \t--- Loss: 0.567\n",
      "Iteration: 76 \t--- Loss: 0.570\n",
      "Iteration: 77 \t--- Loss: 0.573\n",
      "Iteration: 78 \t--- Loss: 0.565\n",
      "Iteration: 79 \t--- Loss: 0.566\n",
      "Iteration: 80 \t--- Loss: 0.569\n",
      "Iteration: 81 \t--- Loss: 0.571\n",
      "Iteration: 82 \t--- Loss: 0.570\n",
      "Iteration: 83 \t--- Loss: 0.575\n",
      "Iteration: 84 \t--- Loss: 0.571\n",
      "Iteration: 85 \t--- Loss: 0.578\n",
      "Iteration: 86 \t--- Loss: 0.567\n",
      "Iteration: 87 \t--- Loss: 0.580\n",
      "Iteration: 88 \t--- Loss: 0.576\n",
      "Iteration: 89 \t--- Loss: 0.568\n",
      "Iteration: 90 \t--- Loss: 0.569\n",
      "Iteration: 91 \t--- Loss: 0.569\n",
      "Iteration: 92 \t--- Loss: 0.574\n",
      "Iteration: 93 \t--- Loss: 0.565\n",
      "Iteration: 94 \t--- Loss: 0.568\n",
      "Iteration: 95 \t--- Loss: 0.570\n",
      "Iteration: 96 \t--- Loss: 0.578\n",
      "Iteration: 97 \t--- Loss: 0.575\n",
      "Iteration: 98 \t--- Loss: 0.564\n",
      "Iteration: 99 \t--- Loss: 0.577\n",
      "Iteration: 100 \t--- Loss: 0.571\n",
      "Iteration: 101 \t--- Loss: 0.571\n",
      "Iteration: 102 \t--- Loss: 0.569\n",
      "Iteration: 103 \t--- Loss: 0.572\n",
      "Iteration: 104 \t--- Loss: 0.573\n",
      "Iteration: 105 \t--- Loss: 0.572\n",
      "Iteration: 106 \t--- Loss: 0.584\n",
      "Iteration: 107 \t--- Loss: 0.567\n",
      "Iteration: 108 \t--- Loss: 0.568\n",
      "Iteration: 109 \t--- Loss: 0.564\n",
      "Iteration: 110 \t--- Loss: 0.577\n",
      "Iteration: 111 \t--- Loss: 0.580\n",
      "Iteration: 112 \t--- Loss: 0.577\n",
      "Iteration: 113 \t--- Loss: 0.568\n",
      "Iteration: 114 \t--- Loss: 0.572\n",
      "Iteration: 115 \t--- Loss: 0.574\n",
      "Iteration: 116 \t--- Loss: 0.564\n",
      "Iteration: 117 \t--- Loss: 0.582\n",
      "Iteration: 118 \t--- Loss: 0.565\n",
      "Iteration: 119 \t--- Loss: 0.564\n",
      "Iteration: 120 \t--- Loss: 0.570\n",
      "Iteration: 121 \t--- Loss: 0.570\n",
      "Iteration: 122 \t--- Loss: 0.566\n",
      "Iteration: 123 \t--- Loss: 0.566\n",
      "Iteration: 124 \t--- Loss: 0.569\n",
      "Iteration: 125 \t--- Loss: 0.576\n",
      "Iteration: 126 \t--- Loss: 0.574\n",
      "Iteration: 127 \t--- Loss: 0.568\n",
      "Iteration: 128 \t--- Loss: 0.575\n",
      "Iteration: 129 \t--- Loss: 0.565\n",
      "Iteration: 130 \t--- Loss: 0.575\n",
      "Iteration: 131 \t--- Loss: 0.570\n",
      "Iteration: 132 \t--- Loss: 0.569\n",
      "Iteration: 133 \t--- Loss: 0.577\n",
      "Iteration: 134 \t--- Loss: 0.566\n",
      "Iteration: 135 \t--- Loss: 0.566\n",
      "Iteration: 136 \t--- Loss: 0.571\n",
      "Iteration: 137 \t--- Loss: 0.569\n",
      "Iteration: 138 \t--- Loss: 0.576\n",
      "Iteration: 139 \t--- Loss: 0.578\n",
      "Iteration: 140 \t--- Loss: 0.568\n",
      "Iteration: 141 \t--- Loss: 0.564\n",
      "Iteration: 142 \t--- Loss: 0.575\n",
      "Iteration: 143 \t--- Loss: 0.576\n",
      "Iteration: 144 \t--- Loss: 0.567\n",
      "Iteration: 145 \t--- Loss: 0.574\n",
      "Iteration: 146 \t--- Loss: 0.570\n",
      "Iteration: 147 \t--- Loss: 0.567\n",
      "Iteration: 148 \t--- Loss: 0.574\n",
      "Iteration: 149 \t--- Loss: 0.575\n",
      "Iteration: 150 \t--- Loss: 0.567\n",
      "Iteration: 151 \t--- Loss: 0.562\n",
      "Iteration: 152 \t--- Loss: 0.574\n",
      "Iteration: 153 \t--- Loss: 0.563\n",
      "Iteration: 154 \t--- Loss: 0.575\n",
      "Iteration: 155 \t--- Loss: 0.572\n",
      "Iteration: 156 \t--- Loss: 0.573\n",
      "Iteration: 157 \t--- Loss: 0.561\n",
      "Iteration: 158 \t--- Loss: 0.569\n",
      "Iteration: 159 \t--- Loss: 0.574\n",
      "Iteration: 160 \t--- Loss: 0.582\n",
      "Iteration: 161 \t--- Loss: 0.570\n",
      "Iteration: 162 \t--- Loss: 0.572\n",
      "Iteration: 163 \t--- Loss: 0.569\n",
      "Iteration: 164 \t--- Loss: 0.567\n",
      "Iteration: 165 \t--- Loss: 0.583\n",
      "Iteration: 166 \t--- Loss: 0.576\n",
      "Iteration: 167 \t--- Loss: 0.561\n",
      "Iteration: 168 \t--- Loss: 0.555\n",
      "Iteration: 169 \t--- Loss: 0.567\n",
      "Iteration: 170 \t--- Loss: 0.569\n",
      "Iteration: 171 \t--- Loss: 0.560\n",
      "Iteration: 172 \t--- Loss: 0.585\n",
      "Iteration: 173 \t--- Loss: 0.575\n",
      "Iteration: 174 \t--- Loss: 0.555\n",
      "Iteration: 175 \t--- Loss: 0.567\n",
      "Iteration: 176 \t--- Loss: 0.577\n",
      "Iteration: 177 \t--- Loss: 0.571\n",
      "Iteration: 178 \t--- Loss: 0.575\n",
      "Iteration: 179 \t--- Loss: 0.574\n",
      "Iteration: 180 \t--- Loss: 0.565\n",
      "Iteration: 181 \t--- Loss: 0.566\n",
      "Iteration: 182 \t--- Loss: 0.570\n",
      "Iteration: 183 \t--- Loss: 0.575\n",
      "Iteration: 184 \t--- Loss: 0.572\n",
      "Iteration: 185 \t--- Loss: 0.576\n",
      "Iteration: 186 \t--- Loss: 0.571\n",
      "Iteration: 187 \t--- Loss: 0.574\n",
      "Iteration: 188 \t--- Loss: 0.577\n",
      "Iteration: 189 \t--- Loss: 0.576\n",
      "Iteration: 190 \t--- Loss: 0.569\n",
      "Iteration: 191 \t--- Loss: 0.571\n",
      "Iteration: 192 \t--- Loss: 0.568\n",
      "Iteration: 193 \t--- Loss: 0.559\n",
      "Iteration: 194 \t--- Loss: 0.569\n",
      "Iteration: 195 \t--- Loss: 0.573\n",
      "Iteration: 196 \t--- Loss: 0.566\n",
      "Iteration: 197 \t--- Loss: 0.574\n",
      "Iteration: 198 \t--- Loss: 0.570\n",
      "Iteration: 199 \t--- Loss: 0.575\n",
      "Iteration: 200 \t--- Loss: 0.570\n",
      "Iteration: 201 \t--- Loss: 0.572\n",
      "Iteration: 202 \t--- Loss: 0.565\n",
      "Iteration: 203 \t--- Loss: 0.571\n",
      "Iteration: 204 \t--- Loss: 0.566\n",
      "Iteration: 205 \t--- Loss: 0.576\n",
      "Iteration: 206 \t--- Loss: 0.568\n",
      "Iteration: 207 \t--- Loss: 0.572\n",
      "Iteration: 208 \t--- Loss: 0.560\n",
      "Iteration: 209 \t--- Loss: 0.573\n",
      "Iteration: 210 \t--- Loss: 0.576\n",
      "Iteration: 211 \t--- Loss: 0.573\n",
      "Iteration: 212 \t--- Loss: 0.580\n",
      "Iteration: 213 \t--- Loss: 0.560\n",
      "Iteration: 214 \t--- Loss: 0.567\n",
      "Iteration: 215 \t--- Loss: 0.575\n",
      "Iteration: 216 \t--- Loss: 0.564\n",
      "Iteration: 217 \t--- Loss: 0.571\n",
      "Iteration: 218 \t--- Loss: 0.571\n",
      "Iteration: 219 \t--- Loss: 0.568\n",
      "Iteration: 220 \t--- Loss: 0.572\n",
      "Iteration: 221 \t--- Loss: 0.571\n",
      "Iteration: 222 \t--- Loss: 0.565\n",
      "Iteration: 223 \t--- Loss: 0.564\n",
      "Iteration: 224 \t--- Loss: 0.579\n",
      "Iteration: 225 \t--- Loss: 0.570\n",
      "Iteration: 226 \t--- Loss: 0.566\n",
      "Iteration: 227 \t--- Loss: 0.562\n",
      "Iteration: 228 \t--- Loss: 0.574\n",
      "Iteration: 229 \t--- Loss: 0.572\n",
      "Iteration: 230 \t--- Loss: 0.580\n",
      "Iteration: 231 \t--- Loss: 0.561\n",
      "Iteration: 232 \t--- Loss: 0.573\n",
      "Iteration: 233 \t--- Loss: 0.572\n",
      "Iteration: 234 \t--- Loss: 0.564\n",
      "Iteration: 235 \t--- Loss: 0.567\n",
      "Iteration: 236 \t--- Loss: 0.574\n",
      "Iteration: 237 \t--- Loss: 0.563\n",
      "Iteration: 238 \t--- Loss: 0.578\n",
      "Iteration: 239 \t--- Loss: 0.566\n",
      "Iteration: 240 \t--- Loss: 0.572\n",
      "Iteration: 241 \t--- Loss: 0.564\n",
      "Iteration: 242 \t--- Loss: 0.566\n",
      "Iteration: 243 \t--- Loss: 0.575\n",
      "Iteration: 244 \t--- Loss: 0.558\n",
      "Iteration: 245 \t--- Loss: 0.570\n",
      "Iteration: 246 \t--- Loss: 0.557\n",
      "Iteration: 247 \t--- Loss: 0.565\n",
      "Iteration: 248 \t--- Loss: 0.575\n",
      "Iteration: 249 \t--- Loss: 0.576\n",
      "Iteration: 250 \t--- Loss: 0.561\n",
      "Iteration: 251 \t--- Loss: 0.581\n",
      "Iteration: 252 \t--- Loss: 0.565\n",
      "Iteration: 253 \t--- Loss: 0.571\n",
      "Iteration: 254 \t--- Loss: 0.562\n",
      "Iteration: 255 \t--- Loss: 0.562\n",
      "Iteration: 256 \t--- Loss: 0.578\n",
      "Iteration: 257 \t--- Loss: 0.571\n",
      "Iteration: 258 \t--- Loss: 0.557\n",
      "Iteration: 259 \t--- Loss: 0.565"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:37<00:00, 97.65s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.241\n",
      "Iteration: 261 \t--- Loss: 0.263\n",
      "Iteration: 262 \t--- Loss: 0.262\n",
      "Iteration: 263 \t--- Loss: 0.241\n",
      "Iteration: 264 \t--- Loss: 0.257\n",
      "Iteration: 265 \t--- Loss: 0.262\n",
      "Iteration: 266 \t--- Loss: 0.270\n",
      "Iteration: 267 \t--- Loss: 0.278\n",
      "Iteration: 268 \t--- Loss: 0.229\n",
      "Iteration: 269 \t--- Loss: 0.260\n",
      "Iteration: 270 \t--- Loss: 0.240\n",
      "Iteration: 271 \t--- Loss: 0.251\n",
      "Iteration: 272 \t--- Loss: 0.243\n",
      "Iteration: 273 \t--- Loss: 0.261\n",
      "Iteration: 274 \t--- Loss: 0.251\n",
      "Iteration: 275 \t--- Loss: 0.240\n",
      "Iteration: 276 \t--- Loss: 0.250\n",
      "Iteration: 277 \t--- Loss: 0.270\n",
      "Iteration: 278 \t--- Loss: 0.258\n",
      "Iteration: 279 \t--- Loss: 0.255\n",
      "Iteration: 280 \t--- Loss: 0.252\n",
      "Iteration: 281 \t--- Loss: 0.241\n",
      "Iteration: 282 \t--- Loss: 0.255\n",
      "Iteration: 283 \t--- Loss: 0.250\n",
      "Iteration: 284 \t--- Loss: 0.255\n",
      "Iteration: 285 \t--- Loss: 0.238\n",
      "Iteration: 286 \t--- Loss: 0.262\n",
      "Iteration: 287 \t--- Loss: 0.253\n",
      "Iteration: 288 \t--- Loss: 0.253\n",
      "Iteration: 289 \t--- Loss: 0.280\n",
      "Iteration: 290 \t--- Loss: 0.255\n",
      "Iteration: 291 \t--- Loss: 0.267\n",
      "Iteration: 292 \t--- Loss: 0.280\n",
      "Iteration: 293 \t--- Loss: 0.261\n",
      "Iteration: 294 \t--- Loss: 0.246\n",
      "Iteration: 295 \t--- Loss: 0.253\n",
      "Iteration: 296 \t--- Loss: 0.243\n",
      "Iteration: 297 \t--- Loss: 0.256\n",
      "Iteration: 298 \t--- Loss: 0.254\n",
      "Iteration: 299 \t--- Loss: 0.256\n",
      "Iteration: 300 \t--- Loss: 0.235\n",
      "Iteration: 301 \t--- Loss: 0.247\n",
      "Iteration: 302 \t--- Loss: 0.242\n",
      "Iteration: 303 \t--- Loss: 0.247\n",
      "Iteration: 304 \t--- Loss: 0.248\n",
      "Iteration: 305 \t--- Loss: 0.247\n",
      "Iteration: 306 \t--- Loss: 0.273\n",
      "Iteration: 307 \t--- Loss: 0.230\n",
      "Iteration: 308 \t--- Loss: 0.251\n",
      "Iteration: 309 \t--- Loss: 0.253\n",
      "Iteration: 310 \t--- Loss: 0.257\n",
      "Iteration: 311 \t--- Loss: 0.253\n",
      "Iteration: 312 \t--- Loss: 0.282\n",
      "Iteration: 313 \t--- Loss: 0.260\n",
      "Iteration: 314 \t--- Loss: 0.245\n",
      "Iteration: 315 \t--- Loss: 0.252\n",
      "Iteration: 316 \t--- Loss: 0.261\n",
      "Iteration: 317 \t--- Loss: 0.251\n",
      "Iteration: 318 \t--- Loss: 0.237\n",
      "Iteration: 319 \t--- Loss: 0.254\n",
      "Iteration: 320 \t--- Loss: 0.249\n",
      "Iteration: 321 \t--- Loss: 0.271\n",
      "Iteration: 322 \t--- Loss: 0.254\n",
      "Iteration: 323 \t--- Loss: 0.271\n",
      "Iteration: 324 \t--- Loss: 0.253\n",
      "Iteration: 325 \t--- Loss: 0.245\n",
      "Iteration: 326 \t--- Loss: 0.269\n",
      "Iteration: 327 \t--- Loss: 0.262\n",
      "Iteration: 328 \t--- Loss: 0.287\n",
      "Iteration: 329 \t--- Loss: 0.275\n",
      "Iteration: 330 \t--- Loss: 0.280\n",
      "Iteration: 331 \t--- Loss: 0.254\n",
      "Iteration: 332 \t--- Loss: 0.260\n",
      "Iteration: 333 \t--- Loss: 0.280\n",
      "Iteration: 334 \t--- Loss: 0.255\n",
      "Iteration: 335 \t--- Loss: 0.243\n",
      "Iteration: 336 \t--- Loss: 0.250\n",
      "Iteration: 337 \t--- Loss: 0.233\n",
      "Iteration: 338 \t--- Loss: 0.244\n",
      "Iteration: 339 \t--- Loss: 0.278\n",
      "Iteration: 340 \t--- Loss: 0.231\n",
      "Iteration: 341 \t--- Loss: 0.258\n",
      "Iteration: 342 \t--- Loss: 0.235\n",
      "Iteration: 343 \t--- Loss: 0.244\n",
      "Iteration: 344 \t--- Loss: 0.260\n",
      "Iteration: 345 \t--- Loss: 0.234\n",
      "Iteration: 346 \t--- Loss: 0.248\n",
      "Iteration: 347 \t--- Loss: 0.276\n",
      "Iteration: 348 \t--- Loss: 0.266\n",
      "Iteration: 349 \t--- Loss: 0.248\n",
      "Iteration: 350 \t--- Loss: 0.249\n",
      "Iteration: 351 \t--- Loss: 0.246\n",
      "Iteration: 352 \t--- Loss: 0.245\n",
      "Iteration: 353 \t--- Loss: 0.239\n",
      "Iteration: 354 \t--- Loss: 0.248\n",
      "Iteration: 355 \t--- Loss: 0.281\n",
      "Iteration: 356 \t--- Loss: 0.244\n",
      "Iteration: 357 \t--- Loss: 0.246\n",
      "Iteration: 358 \t--- Loss: 0.283\n",
      "Iteration: 359 \t--- Loss: 0.256\n",
      "Iteration: 360 \t--- Loss: 0.256\n",
      "Iteration: 361 \t--- Loss: 0.257\n",
      "Iteration: 362 \t--- Loss: 0.270\n",
      "Iteration: 363 \t--- Loss: 0.237\n",
      "Iteration: 364 \t--- Loss: 0.245\n",
      "Iteration: 365 \t--- Loss: 0.282\n",
      "Iteration: 366 \t--- Loss: 0.253\n",
      "Iteration: 367 \t--- Loss: 0.266\n",
      "Iteration: 368 \t--- Loss: 0.239\n",
      "Iteration: 369 \t--- Loss: 0.234\n",
      "Iteration: 370 \t--- Loss: 0.250\n",
      "Iteration: 371 \t--- Loss: 0.259\n",
      "Iteration: 372 \t--- Loss: 0.270\n",
      "Iteration: 373 \t--- Loss: 0.249\n",
      "Iteration: 374 \t--- Loss: 0.232\n",
      "Iteration: 375 \t--- Loss: 0.240\n",
      "Iteration: 376 \t--- Loss: 0.252\n",
      "Iteration: 377 \t--- Loss: 0.250\n",
      "Iteration: 378 \t--- Loss: 0.249\n",
      "Iteration: 379 \t--- Loss: 0.245\n",
      "Iteration: 380 \t--- Loss: 0.225\n",
      "Iteration: 381 \t--- Loss: 0.240\n",
      "Iteration: 382 \t--- Loss: 0.257\n",
      "Iteration: 383 \t--- Loss: 0.246\n",
      "Iteration: 384 \t--- Loss: 0.234\n",
      "Iteration: 385 \t--- Loss: 0.264\n",
      "Iteration: 386 \t--- Loss: 0.279\n",
      "Iteration: 387 \t--- Loss: 0.244\n",
      "Iteration: 388 \t--- Loss: 0.253\n",
      "Iteration: 389 \t--- Loss: 0.265\n",
      "Iteration: 390 \t--- Loss: 0.253\n",
      "Iteration: 391 \t--- Loss: 0.251\n",
      "Iteration: 392 \t--- Loss: 0.281\n",
      "Iteration: 393 \t--- Loss: 0.265\n",
      "Iteration: 394 \t--- Loss: 0.236\n",
      "Iteration: 395 \t--- Loss: 0.261\n",
      "Iteration: 396 \t--- Loss: 0.260\n",
      "Iteration: 397 \t--- Loss: 0.244\n",
      "Iteration: 398 \t--- Loss: 0.269\n",
      "Iteration: 399 \t--- Loss: 0.238\n",
      "Iteration: 400 \t--- Loss: 0.267\n",
      "Iteration: 401 \t--- Loss: 0.266\n",
      "Iteration: 402 \t--- Loss: 0.249\n",
      "Iteration: 403 \t--- Loss: 0.246\n",
      "Iteration: 404 \t--- Loss: 0.259\n",
      "Iteration: 405 \t--- Loss: 0.247\n",
      "Iteration: 406 \t--- Loss: 0.262\n",
      "Iteration: 407 \t--- Loss: 0.273\n",
      "Iteration: 408 \t--- Loss: 0.266\n",
      "Iteration: 409 \t--- Loss: 0.261\n",
      "Iteration: 410 \t--- Loss: 0.266\n",
      "Iteration: 411 \t--- Loss: 0.250\n",
      "Iteration: 412 \t--- Loss: 0.268\n",
      "Iteration: 413 \t--- Loss: 0.237\n",
      "Iteration: 414 \t--- Loss: 0.253\n",
      "Iteration: 415 \t--- Loss: 0.252\n",
      "Iteration: 416 \t--- Loss: 0.240\n",
      "Iteration: 417 \t--- Loss: 0.258\n",
      "Iteration: 418 \t--- Loss: 0.246\n",
      "Iteration: 419 \t--- Loss: 0.242\n",
      "Iteration: 420 \t--- Loss: 0.259\n",
      "Iteration: 421 \t--- Loss: 0.239\n",
      "Iteration: 422 \t--- Loss: 0.240\n",
      "Iteration: 423 \t--- Loss: 0.266\n",
      "Iteration: 424 \t--- Loss: 0.255\n",
      "Iteration: 425 \t--- Loss: 0.258\n",
      "Iteration: 426 \t--- Loss: 0.270\n",
      "Iteration: 427 \t--- Loss: 0.240\n",
      "Iteration: 428 \t--- Loss: 0.256\n",
      "Iteration: 429 \t--- Loss: 0.259\n",
      "Iteration: 430 \t--- Loss: 0.253\n",
      "Iteration: 431 \t--- Loss: 0.258\n",
      "Iteration: 432 \t--- Loss: 0.244\n",
      "Iteration: 433 \t--- Loss: 0.252\n",
      "Iteration: 434 \t--- Loss: 0.254\n",
      "Iteration: 435 \t--- Loss: 0.239\n",
      "Iteration: 436 \t--- Loss: 0.264\n",
      "Iteration: 437 \t--- Loss: 0.266\n",
      "Iteration: 438 \t--- Loss: 0.276\n",
      "Iteration: 439 \t--- Loss: 0.285\n",
      "Iteration: 440 \t--- Loss: 0.268\n",
      "Iteration: 441 \t--- Loss: 0.279\n",
      "Iteration: 442 \t--- Loss: 0.253\n",
      "Iteration: 443 \t--- Loss: 0.244\n",
      "Iteration: 444 \t--- Loss: 0.256\n",
      "Iteration: 445 \t--- Loss: 0.247\n",
      "Iteration: 446 \t--- Loss: 0.241\n",
      "Iteration: 447 \t--- Loss: 0.252\n",
      "Iteration: 448 \t--- Loss: 0.251\n",
      "Iteration: 449 \t--- Loss: 0.262\n",
      "Iteration: 450 \t--- Loss: 0.260\n",
      "Iteration: 451 \t--- Loss: 0.258\n",
      "Iteration: 452 \t--- Loss: 0.243\n",
      "Iteration: 453 \t--- Loss: 0.275\n",
      "Iteration: 454 \t--- Loss: 0.255\n",
      "Iteration: 455 \t--- Loss: 0.234\n",
      "Iteration: 456 \t--- Loss: 0.255\n",
      "Iteration: 457 \t--- Loss: 0.244\n",
      "Iteration: 458 \t--- Loss: 0.287\n",
      "Iteration: 459 \t--- Loss: 0.249\n",
      "Iteration: 460 \t--- Loss: 0.250\n",
      "Iteration: 461 \t--- Loss: 0.257\n",
      "Iteration: 462 \t--- Loss: 0.243\n",
      "Iteration: 463 \t--- Loss: 0.251\n",
      "Iteration: 464 \t--- Loss: 0.242\n",
      "Iteration: 465 \t--- Loss: 0.277\n",
      "Iteration: 466 \t--- Loss: 0.285\n",
      "Iteration: 467 \t--- Loss: 0.265\n",
      "Iteration: 468 \t--- Loss: 0.233\n",
      "Iteration: 469 \t--- Loss: 0.245\n",
      "Iteration: 470 \t--- Loss: 0.263\n",
      "Iteration: 471 \t--- Loss: 0.252\n",
      "Iteration: 472 \t--- Loss: 0.246\n",
      "Iteration: 473 \t--- Loss: 0.269\n",
      "Iteration: 474 \t--- Loss: 0.261\n",
      "Iteration: 475 \t--- Loss: 0.254\n",
      "Iteration: 476 \t--- Loss: 0.257\n",
      "Iteration: 477 \t--- Loss: 0.240\n",
      "Iteration: 478 \t--- Loss: 0.231\n",
      "Iteration: 479 \t--- Loss: 0.277\n",
      "Iteration: 480 \t--- Loss: 0.251\n",
      "Iteration: 481 \t--- Loss: 0.243\n",
      "Iteration: 482 \t--- Loss: 0.238\n",
      "Iteration: 483 \t--- Loss: 0.246\n",
      "Iteration: 484 \t--- Loss: 0.270\n",
      "Iteration: 485 \t--- Loss: 0.284\n",
      "Iteration: 486 \t--- Loss: 0.271\n",
      "Iteration: 487 \t--- Loss: 0.256\n",
      "Iteration: 488 \t--- Loss: 0.283\n",
      "Iteration: 489 \t--- Loss: 0.235\n",
      "Iteration: 490 \t--- Loss: 0.242\n",
      "Iteration: 491 \t--- Loss: 0.257\n",
      "Iteration: 492 \t--- Loss: 0.239\n",
      "Iteration: 493 \t--- Loss: 0.270\n",
      "Iteration: 494 \t--- Loss: 0.249\n",
      "Iteration: 495 \t--- Loss: 0.243\n",
      "Iteration: 496 \t--- Loss: 0.241\n",
      "Iteration: 497 \t--- Loss: 0.276\n",
      "Iteration: 498 \t--- Loss: 0.271\n",
      "Iteration: 499 \t--- Loss: 0.259\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:10,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.318\n",
      "Iteration: 1 \t--- Loss: 0.309\n",
      "Iteration: 2 \t--- Loss: 0.299\n",
      "Iteration: 3 \t--- Loss: 0.297\n",
      "Iteration: 4 \t--- Loss: 0.288\n",
      "Iteration: 5 \t--- Loss: 0.282\n",
      "Iteration: 6 \t--- Loss: 0.280\n",
      "Iteration: 7 \t--- Loss: 0.267\n",
      "Iteration: 8 \t--- Loss: 0.269\n",
      "Iteration: 9 \t--- Loss: 0.262\n",
      "Iteration: 10 \t--- Loss: 0.263\n",
      "Iteration: 11 \t--- Loss: 0.252\n",
      "Iteration: 12 \t--- Loss: 0.258\n",
      "Iteration: 13 \t--- Loss: 0.257\n",
      "Iteration: 14 \t--- Loss: 0.250\n",
      "Iteration: 15 \t--- Loss: 0.247\n",
      "Iteration: 16 \t--- Loss: 0.245\n",
      "Iteration: 17 \t--- Loss: 0.243\n",
      "Iteration: 18 \t--- Loss: 0.245\n",
      "Iteration: 19 \t--- Loss: 0.231\n",
      "Iteration: 20 \t--- Loss: 0.235\n",
      "Iteration: 21 \t--- Loss: 0.243\n",
      "Iteration: 22 \t--- Loss: 0.229\n",
      "Iteration: 23 \t--- Loss: 0.236\n",
      "Iteration: 24 \t--- Loss: 0.225\n",
      "Iteration: 25 \t--- Loss: 0.230\n",
      "Iteration: 26 \t--- Loss: 0.224\n",
      "Iteration: 27 \t--- Loss: 0.201\n",
      "Iteration: 28 \t--- Loss: 0.195\n",
      "Iteration: 29 \t--- Loss: 0.182\n",
      "Iteration: 30 \t--- Loss: 0.183\n",
      "Iteration: 31 \t--- Loss: 0.174\n",
      "Iteration: 32 \t--- Loss: 0.177\n",
      "Iteration: 33 \t--- Loss: 0.183\n",
      "Iteration: 34 \t--- Loss: 0.164\n",
      "Iteration: 35 \t--- Loss: 0.180\n",
      "Iteration: 36 \t--- Loss: 0.165\n",
      "Iteration: 37 \t--- Loss: 0.262\n",
      "Iteration: 38 \t--- Loss: 0.302\n",
      "Iteration: 39 \t--- Loss: 0.295\n",
      "Iteration: 40 \t--- Loss: 0.289\n",
      "Iteration: 41 \t--- Loss: 0.281\n",
      "Iteration: 42 \t--- Loss: 0.273\n",
      "Iteration: 43 \t--- Loss: 0.275\n",
      "Iteration: 44 \t--- Loss: 0.268\n",
      "Iteration: 45 \t--- Loss: 0.256\n",
      "Iteration: 46 \t--- Loss: 0.251\n",
      "Iteration: 47 \t--- Loss: 0.259\n",
      "Iteration: 48 \t--- Loss: 0.253\n",
      "Iteration: 49 \t--- Loss: 0.252\n",
      "Iteration: 50 \t--- Loss: 0.244\n",
      "Iteration: 51 \t--- Loss: 0.235\n",
      "Iteration: 52 \t--- Loss: 0.237\n",
      "Iteration: 53 \t--- Loss: 0.228\n",
      "Iteration: 54 \t--- Loss: 0.227\n",
      "Iteration: 55 \t--- Loss: 0.231\n",
      "Iteration: 56 \t--- Loss: 0.226\n",
      "Iteration: 57 \t--- Loss: 0.223\n",
      "Iteration: 58 \t--- Loss: 0.208\n",
      "Iteration: 59 \t--- Loss: 0.222\n",
      "Iteration: 60 \t--- Loss: 0.224\n",
      "Iteration: 61 \t--- Loss: 0.215\n",
      "Iteration: 62 \t--- Loss: 0.205\n",
      "Iteration: 63 \t--- Loss: 0.204\n",
      "Iteration: 64 \t--- Loss: 0.205\n",
      "Iteration: 65 \t--- Loss: 0.211\n",
      "Iteration: 66 \t--- Loss: 0.182\n",
      "Iteration: 67 \t--- Loss: 0.184\n",
      "Iteration: 68 \t--- Loss: 0.189\n",
      "Iteration: 69 \t--- Loss: 0.149\n",
      "Iteration: 70 \t--- Loss: 0.136\n",
      "Iteration: 71 \t--- Loss: 0.143\n",
      "Iteration: 72 \t--- Loss: 0.137\n",
      "Iteration: 73 \t--- Loss: 0.136\n",
      "Iteration: 74 \t--- Loss: 0.141\n",
      "Iteration: 75 \t--- Loss: 0.134\n",
      "Iteration: 76 \t--- Loss: 0.135\n",
      "Iteration: 77 \t--- Loss: 0.125\n",
      "Iteration: 78 \t--- Loss: 0.129\n",
      "Iteration: 79 \t--- Loss: 0.134\n",
      "Iteration: 80 \t--- Loss: 0.126\n",
      "Iteration: 81 \t--- Loss: 0.134\n",
      "Iteration: 82 \t--- Loss: 0.140\n",
      "Iteration: 83 \t--- Loss: 0.143\n",
      "Iteration: 84 \t--- Loss: 0.245\n",
      "Iteration: 85 \t--- Loss: 0.337\n",
      "Iteration: 86 \t--- Loss: 0.325\n",
      "Iteration: 87 \t--- Loss: 0.316\n",
      "Iteration: 88 \t--- Loss: 0.310\n",
      "Iteration: 89 \t--- Loss: 0.301\n",
      "Iteration: 90 \t--- Loss: 0.288\n",
      "Iteration: 91 \t--- Loss: 0.284\n",
      "Iteration: 92 \t--- Loss: 0.272\n",
      "Iteration: 93 \t--- Loss: 0.262\n",
      "Iteration: 94 \t--- Loss: 0.253\n",
      "Iteration: 95 \t--- Loss: 0.241\n",
      "Iteration: 96 \t--- Loss: 0.241\n",
      "Iteration: 97 \t--- Loss: 0.237\n",
      "Iteration: 98 \t--- Loss: 0.235\n",
      "Iteration: 99 \t--- Loss: 0.223\n",
      "Iteration: 100 \t--- Loss: 0.225\n",
      "Iteration: 101 \t--- Loss: 0.214\n",
      "Iteration: 102 \t--- Loss: 0.213\n",
      "Iteration: 103 \t--- Loss: 0.198\n",
      "Iteration: 104 \t--- Loss: 0.200\n",
      "Iteration: 105 \t--- Loss: 0.191\n",
      "Iteration: 106 \t--- Loss: 0.185\n",
      "Iteration: 107 \t--- Loss: 0.205\n",
      "Iteration: 108 \t--- Loss: 0.179\n",
      "Iteration: 109 \t--- Loss: 0.194\n",
      "Iteration: 110 \t--- Loss: 0.179\n",
      "Iteration: 111 \t--- Loss: 0.174\n",
      "Iteration: 112 \t--- Loss: 0.166\n",
      "Iteration: 113 \t--- Loss: 0.162\n",
      "Iteration: 114 \t--- Loss: 0.152\n",
      "Iteration: 115 \t--- Loss: 0.169\n",
      "Iteration: 116 \t--- Loss: 0.151\n",
      "Iteration: 117 \t--- Loss: 0.149\n",
      "Iteration: 118 \t--- Loss: 0.148\n",
      "Iteration: 119 \t--- Loss: 0.150\n",
      "Iteration: 120 \t--- Loss: 0.139\n",
      "Iteration: 121 \t--- Loss: 0.134\n",
      "Iteration: 122 \t--- Loss: 0.135\n",
      "Iteration: 123 \t--- Loss: 0.128\n",
      "Iteration: 124 \t--- Loss: 0.127\n",
      "Iteration: 125 \t--- Loss: 0.114\n",
      "Iteration: 126 \t--- Loss: 0.117\n",
      "Iteration: 127 \t--- Loss: 0.103\n",
      "Iteration: 128 \t--- Loss: 0.107\n",
      "Iteration: 129 \t--- Loss: 0.091\n",
      "Iteration: 130 \t--- Loss: 0.090\n",
      "Iteration: 131 \t--- Loss: 0.086\n",
      "Iteration: 132 \t--- Loss: 0.088\n",
      "Iteration: 133 \t--- Loss: 0.079\n",
      "Iteration: 134 \t--- Loss: 0.079\n",
      "Iteration: 135 \t--- Loss: 0.073\n",
      "Iteration: 136 \t--- Loss: 0.076\n",
      "Iteration: 137 \t--- Loss: 0.076\n",
      "Iteration: 138 \t--- Loss: 0.080\n",
      "Iteration: 139 \t--- Loss: 0.079\n",
      "Iteration: 140 \t--- Loss: 0.078\n",
      "Iteration: 141 \t--- Loss: 0.074\n",
      "Iteration: 142 \t--- Loss: 0.071\n",
      "Iteration: 143 \t--- Loss: 0.070\n",
      "Iteration: 144 \t--- Loss: 0.072\n",
      "Iteration: 145 \t--- Loss: 0.071\n",
      "Iteration: 146 \t--- Loss: 0.070\n",
      "Iteration: 147 \t--- Loss: 0.068\n",
      "Iteration: 148 \t--- Loss: 0.068\n",
      "Iteration: 149 \t--- Loss: 0.070\n",
      "Iteration: 150 \t--- Loss: 0.067\n",
      "Iteration: 151 \t--- Loss: 0.069\n",
      "Iteration: 152 \t--- Loss: 0.071\n",
      "Iteration: 153 \t--- Loss: 0.066\n",
      "Iteration: 154 \t--- Loss: 0.067\n",
      "Iteration: 155 \t--- Loss: 0.066\n",
      "Iteration: 156 \t--- Loss: 0.066\n",
      "Iteration: 157 \t--- Loss: 0.065\n",
      "Iteration: 158 \t--- Loss: 0.066\n",
      "Iteration: 159 \t--- Loss: 0.064\n",
      "Iteration: 160 \t--- Loss: 0.063\n",
      "Iteration: 161 \t--- Loss: 0.062\n",
      "Iteration: 162 \t--- Loss: 0.062\n",
      "Iteration: 163 \t--- Loss: 0.068\n",
      "Iteration: 164 \t--- Loss: 0.068\n",
      "Iteration: 165 \t--- Loss: 0.066\n",
      "Iteration: 166 \t--- Loss: 0.061\n",
      "Iteration: 167 \t--- Loss: 0.062\n",
      "Iteration: 168 \t--- Loss: 0.063\n",
      "Iteration: 169 \t--- Loss: 0.061\n",
      "Iteration: 170 \t--- Loss: 0.062\n",
      "Iteration: 171 \t--- Loss: 0.063\n",
      "Iteration: 172 \t--- Loss: 0.063\n",
      "Iteration: 173 \t--- Loss: 0.057\n",
      "Iteration: 174 \t--- Loss: 0.061\n",
      "Iteration: 175 \t--- Loss: 0.058\n",
      "Iteration: 176 \t--- Loss: 0.056\n",
      "Iteration: 177 \t--- Loss: 0.061\n",
      "Iteration: 178 \t--- Loss: 0.057\n",
      "Iteration: 179 \t--- Loss: 0.060\n",
      "Iteration: 180 \t--- Loss: 0.056\n",
      "Iteration: 181 \t--- Loss: 0.060\n",
      "Iteration: 182 \t--- Loss: 0.054\n",
      "Iteration: 183 \t--- Loss: 0.052\n",
      "Iteration: 184 \t--- Loss: 0.059\n",
      "Iteration: 185 \t--- Loss: 0.058\n",
      "Iteration: 186 \t--- Loss: 0.055\n",
      "Iteration: 187 \t--- Loss: 0.057\n",
      "Iteration: 188 \t--- Loss: 0.054\n",
      "Iteration: 189 \t--- Loss: 0.053\n",
      "Iteration: 190 \t--- Loss: 0.055\n",
      "Iteration: 191 \t--- Loss: 0.051\n",
      "Iteration: 192 \t--- Loss: 0.050\n",
      "Iteration: 193 \t--- Loss: 0.056\n",
      "Iteration: 194 \t--- Loss: 0.053\n",
      "Iteration: 195 \t--- Loss: 0.051\n",
      "Iteration: 196 \t--- Loss: 0.050\n",
      "Iteration: 197 \t--- Loss: 0.052\n",
      "Iteration: 198 \t--- Loss: 0.054\n",
      "Iteration: 199 \t--- Loss: 0.053\n",
      "Iteration: 200 \t--- Loss: 0.054\n",
      "Iteration: 201 \t--- Loss: 0.054\n",
      "Iteration: 202 \t--- Loss: 0.050\n",
      "Iteration: 203 \t--- Loss: 0.052\n",
      "Iteration: 204 \t--- Loss: 0.052\n",
      "Iteration: 205 \t--- Loss: 0.056\n",
      "Iteration: 206 \t--- Loss: 0.051\n",
      "Iteration: 207 \t--- Loss: 0.049\n",
      "Iteration: 208 \t--- Loss: 0.048\n",
      "Iteration: 209 \t--- Loss: 0.047\n",
      "Iteration: 210 \t--- Loss: 0.046\n",
      "Iteration: 211 \t--- Loss: 0.051\n",
      "Iteration: 212 \t--- Loss: 0.051\n",
      "Iteration: 213 \t--- Loss: 0.045\n",
      "Iteration: 214 \t--- Loss: 0.048\n",
      "Iteration: 215 \t--- Loss: 0.048\n",
      "Iteration: 216 \t--- Loss: 0.044\n",
      "Iteration: 217 \t--- Loss: 0.048\n",
      "Iteration: 218 \t--- Loss: 0.043\n",
      "Iteration: 219 \t--- Loss: 0.045\n",
      "Iteration: 220 \t--- Loss: 0.049\n",
      "Iteration: 221 \t--- Loss: 0.047\n",
      "Iteration: 222 \t--- Loss: 0.043\n",
      "Iteration: 223 \t--- Loss: 0.045\n",
      "Iteration: 224 \t--- Loss: 0.047\n",
      "Iteration: 225 \t--- Loss: 0.044\n",
      "Iteration: 226 \t--- Loss: 0.044\n",
      "Iteration: 227 \t--- Loss: 0.045\n",
      "Iteration: 228 \t--- Loss: 0.048\n",
      "Iteration: 229 \t--- Loss: 0.044\n",
      "Iteration: 230 \t--- Loss: 0.048\n",
      "Iteration: 231 \t--- Loss: 0.045\n",
      "Iteration: 232 \t--- Loss: 0.043\n",
      "Iteration: 233 \t--- Loss: 0.045\n",
      "Iteration: 234 \t--- Loss: 0.041\n",
      "Iteration: 235 \t--- Loss: 0.045\n",
      "Iteration: 236 \t--- Loss: 0.045\n",
      "Iteration: 237 \t--- Loss: 0.042\n",
      "Iteration: 238 \t--- Loss: 0.045\n",
      "Iteration: 239 \t--- Loss: 0.043\n",
      "Iteration: 240 \t--- Loss: 0.039\n",
      "Iteration: 241 \t--- Loss: 0.042\n",
      "Iteration: 242 \t--- Loss: 0.043\n",
      "Iteration: 243 \t--- Loss: 0.041\n",
      "Iteration: 244 \t--- Loss: 0.042\n",
      "Iteration: 245 \t--- Loss: 0.043\n",
      "Iteration: 246 \t--- Loss: 0.043\n",
      "Iteration: 247 \t--- Loss: 0.042\n",
      "Iteration: 248 \t--- Loss: 0.039\n",
      "Iteration: 249 \t--- Loss: 0.042\n",
      "Iteration: 250 \t--- Loss: 0.041\n",
      "Iteration: 251 \t--- Loss: 0.039\n",
      "Iteration: 252 \t--- Loss: 0.043\n",
      "Iteration: 253 \t--- Loss: 0.041\n",
      "Iteration: 254 \t--- Loss: 0.038\n",
      "Iteration: 255 \t--- Loss: 0.039\n",
      "Iteration: 256 \t--- Loss: 0.041\n",
      "Iteration: 257 \t--- Loss: 0.040\n",
      "Iteration: 258 \t--- Loss: 0.043\n",
      "Iteration: 259 \t--- Loss: 0.040"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it][Parallel(n_jobs=5)]: Done  30 tasks      | elapsed: 15.7min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:24<00:00, 84.73s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.564\n",
      "Iteration: 261 \t--- Loss: 0.570\n",
      "Iteration: 262 \t--- Loss: 0.577\n",
      "Iteration: 263 \t--- Loss: 0.587\n",
      "Iteration: 264 \t--- Loss: 0.573\n",
      "Iteration: 265 \t--- Loss: 0.571\n",
      "Iteration: 266 \t--- Loss: 0.562\n",
      "Iteration: 267 \t--- Loss: 0.566\n",
      "Iteration: 268 \t--- Loss: 0.566\n",
      "Iteration: 269 \t--- Loss: 0.559\n",
      "Iteration: 270 \t--- Loss: 0.579\n",
      "Iteration: 271 \t--- Loss: 0.576\n",
      "Iteration: 272 \t--- Loss: 0.574\n",
      "Iteration: 273 \t--- Loss: 0.571\n",
      "Iteration: 274 \t--- Loss: 0.569\n",
      "Iteration: 275 \t--- Loss: 0.569\n",
      "Iteration: 276 \t--- Loss: 0.581\n",
      "Iteration: 277 \t--- Loss: 0.577\n",
      "Iteration: 278 \t--- Loss: 0.559\n",
      "Iteration: 279 \t--- Loss: 0.567\n",
      "Iteration: 280 \t--- Loss: 0.579\n",
      "Iteration: 281 \t--- Loss: 0.573\n",
      "Iteration: 282 \t--- Loss: 0.573\n",
      "Iteration: 283 \t--- Loss: 0.582\n",
      "Iteration: 284 \t--- Loss: 0.571\n",
      "Iteration: 285 \t--- Loss: 0.581\n",
      "Iteration: 286 \t--- Loss: 0.577\n",
      "Iteration: 287 \t--- Loss: 0.572\n",
      "Iteration: 288 \t--- Loss: 0.575\n",
      "Iteration: 289 \t--- Loss: 0.572\n",
      "Iteration: 290 \t--- Loss: 0.572\n",
      "Iteration: 291 \t--- Loss: 0.572\n",
      "Iteration: 292 \t--- Loss: 0.572\n",
      "Iteration: 293 \t--- Loss: 0.574\n",
      "Iteration: 294 \t--- Loss: 0.570\n",
      "Iteration: 295 \t--- Loss: 0.575\n",
      "Iteration: 296 \t--- Loss: 0.572\n",
      "Iteration: 297 \t--- Loss: 0.567\n",
      "Iteration: 298 \t--- Loss: 0.563\n",
      "Iteration: 299 \t--- Loss: 0.569\n",
      "Iteration: 300 \t--- Loss: 0.566\n",
      "Iteration: 301 \t--- Loss: 0.570\n",
      "Iteration: 302 \t--- Loss: 0.571\n",
      "Iteration: 303 \t--- Loss: 0.581\n",
      "Iteration: 304 \t--- Loss: 0.566\n",
      "Iteration: 305 \t--- Loss: 0.567\n",
      "Iteration: 306 \t--- Loss: 0.571\n",
      "Iteration: 307 \t--- Loss: 0.570\n",
      "Iteration: 308 \t--- Loss: 0.578\n",
      "Iteration: 309 \t--- Loss: 0.583\n",
      "Iteration: 310 \t--- Loss: 0.576\n",
      "Iteration: 311 \t--- Loss: 0.569\n",
      "Iteration: 312 \t--- Loss: 0.587\n",
      "Iteration: 313 \t--- Loss: 0.576\n",
      "Iteration: 314 \t--- Loss: 0.568\n",
      "Iteration: 315 \t--- Loss: 0.569\n",
      "Iteration: 316 \t--- Loss: 0.575\n",
      "Iteration: 317 \t--- Loss: 0.570\n",
      "Iteration: 318 \t--- Loss: 0.578\n",
      "Iteration: 319 \t--- Loss: 0.574\n",
      "Iteration: 320 \t--- Loss: 0.576\n",
      "Iteration: 321 \t--- Loss: 0.570\n",
      "Iteration: 322 \t--- Loss: 0.567\n",
      "Iteration: 323 \t--- Loss: 0.574\n",
      "Iteration: 324 \t--- Loss: 0.563\n",
      "Iteration: 325 \t--- Loss: 0.566\n",
      "Iteration: 326 \t--- Loss: 0.563\n",
      "Iteration: 327 \t--- Loss: 0.565\n",
      "Iteration: 328 \t--- Loss: 0.584\n",
      "Iteration: 329 \t--- Loss: 0.573\n",
      "Iteration: 330 \t--- Loss: 0.567\n",
      "Iteration: 331 \t--- Loss: 0.568\n",
      "Iteration: 332 \t--- Loss: 0.573\n",
      "Iteration: 333 \t--- Loss: 0.571\n",
      "Iteration: 334 \t--- Loss: 0.572\n",
      "Iteration: 335 \t--- Loss: 0.566\n",
      "Iteration: 336 \t--- Loss: 0.578\n",
      "Iteration: 337 \t--- Loss: 0.576\n",
      "Iteration: 338 \t--- Loss: 0.565\n",
      "Iteration: 339 \t--- Loss: 0.567\n",
      "Iteration: 340 \t--- Loss: 0.576\n",
      "Iteration: 341 \t--- Loss: 0.575\n",
      "Iteration: 342 \t--- Loss: 0.579\n",
      "Iteration: 343 \t--- Loss: 0.563\n",
      "Iteration: 344 \t--- Loss: 0.574\n",
      "Iteration: 345 \t--- Loss: 0.576\n",
      "Iteration: 346 \t--- Loss: 0.576\n",
      "Iteration: 347 \t--- Loss: 0.575\n",
      "Iteration: 348 \t--- Loss: 0.577\n",
      "Iteration: 349 \t--- Loss: 0.567\n",
      "Iteration: 350 \t--- Loss: 0.575\n",
      "Iteration: 351 \t--- Loss: 0.575\n",
      "Iteration: 352 \t--- Loss: 0.579\n",
      "Iteration: 353 \t--- Loss: 0.574\n",
      "Iteration: 354 \t--- Loss: 0.573\n",
      "Iteration: 355 \t--- Loss: 0.580\n",
      "Iteration: 356 \t--- Loss: 0.582\n",
      "Iteration: 357 \t--- Loss: 0.581\n",
      "Iteration: 358 \t--- Loss: 0.571\n",
      "Iteration: 359 \t--- Loss: 0.572\n",
      "Iteration: 360 \t--- Loss: 0.571\n",
      "Iteration: 361 \t--- Loss: 0.573\n",
      "Iteration: 362 \t--- Loss: 0.573\n",
      "Iteration: 363 \t--- Loss: 0.576\n",
      "Iteration: 364 \t--- Loss: 0.569\n",
      "Iteration: 365 \t--- Loss: 0.572\n",
      "Iteration: 366 \t--- Loss: 0.564\n",
      "Iteration: 367 \t--- Loss: 0.572\n",
      "Iteration: 368 \t--- Loss: 0.563\n",
      "Iteration: 369 \t--- Loss: 0.562\n",
      "Iteration: 370 \t--- Loss: 0.573\n",
      "Iteration: 371 \t--- Loss: 0.580\n",
      "Iteration: 372 \t--- Loss: 0.564\n",
      "Iteration: 373 \t--- Loss: 0.574\n",
      "Iteration: 374 \t--- Loss: 0.580\n",
      "Iteration: 375 \t--- Loss: 0.563\n",
      "Iteration: 376 \t--- Loss: 0.568\n",
      "Iteration: 377 \t--- Loss: 0.573\n",
      "Iteration: 378 \t--- Loss: 0.576\n",
      "Iteration: 379 \t--- Loss: 0.572\n",
      "Iteration: 380 \t--- Loss: 0.566\n",
      "Iteration: 381 \t--- Loss: 0.564\n",
      "Iteration: 382 \t--- Loss: 0.574\n",
      "Iteration: 383 \t--- Loss: 0.568\n",
      "Iteration: 384 \t--- Loss: 0.572\n",
      "Iteration: 385 \t--- Loss: 0.571\n",
      "Iteration: 386 \t--- Loss: 0.566\n",
      "Iteration: 387 \t--- Loss: 0.575\n",
      "Iteration: 388 \t--- Loss: 0.572\n",
      "Iteration: 389 \t--- Loss: 0.567\n",
      "Iteration: 390 \t--- Loss: 0.567\n",
      "Iteration: 391 \t--- Loss: 0.569\n",
      "Iteration: 392 \t--- Loss: 0.571\n",
      "Iteration: 393 \t--- Loss: 0.575\n",
      "Iteration: 394 \t--- Loss: 0.566\n",
      "Iteration: 395 \t--- Loss: 0.568\n",
      "Iteration: 396 \t--- Loss: 0.566\n",
      "Iteration: 397 \t--- Loss: 0.571\n",
      "Iteration: 398 \t--- Loss: 0.577\n",
      "Iteration: 399 \t--- Loss: 0.565\n",
      "Iteration: 400 \t--- Loss: 0.572\n",
      "Iteration: 401 \t--- Loss: 0.570\n",
      "Iteration: 402 \t--- Loss: 0.575\n",
      "Iteration: 403 \t--- Loss: 0.573\n",
      "Iteration: 404 \t--- Loss: 0.561\n",
      "Iteration: 405 \t--- Loss: 0.574\n",
      "Iteration: 406 \t--- Loss: 0.565\n",
      "Iteration: 407 \t--- Loss: 0.572\n",
      "Iteration: 408 \t--- Loss: 0.571\n",
      "Iteration: 409 \t--- Loss: 0.577\n",
      "Iteration: 410 \t--- Loss: 0.577\n",
      "Iteration: 411 \t--- Loss: 0.575\n",
      "Iteration: 412 \t--- Loss: 0.574\n",
      "Iteration: 413 \t--- Loss: 0.579\n",
      "Iteration: 414 \t--- Loss: 0.566\n",
      "Iteration: 415 \t--- Loss: 0.564\n",
      "Iteration: 416 \t--- Loss: 0.575\n",
      "Iteration: 417 \t--- Loss: 0.573\n",
      "Iteration: 418 \t--- Loss: 0.574\n",
      "Iteration: 419 \t--- Loss: 0.569\n",
      "Iteration: 420 \t--- Loss: 0.569\n",
      "Iteration: 421 \t--- Loss: 0.562\n",
      "Iteration: 422 \t--- Loss: 0.577\n",
      "Iteration: 423 \t--- Loss: 0.564\n",
      "Iteration: 424 \t--- Loss: 0.572\n",
      "Iteration: 425 \t--- Loss: 0.572\n",
      "Iteration: 426 \t--- Loss: 0.570\n",
      "Iteration: 427 \t--- Loss: 0.577\n",
      "Iteration: 428 \t--- Loss: 0.576\n",
      "Iteration: 429 \t--- Loss: 0.566\n",
      "Iteration: 430 \t--- Loss: 0.571\n",
      "Iteration: 431 \t--- Loss: 0.571\n",
      "Iteration: 432 \t--- Loss: 0.576\n",
      "Iteration: 433 \t--- Loss: 0.584\n",
      "Iteration: 434 \t--- Loss: 0.558\n",
      "Iteration: 435 \t--- Loss: 0.582\n",
      "Iteration: 436 \t--- Loss: 0.578\n",
      "Iteration: 437 \t--- Loss: 0.560\n",
      "Iteration: 438 \t--- Loss: 0.566\n",
      "Iteration: 439 \t--- Loss: 0.579\n",
      "Iteration: 440 \t--- Loss: 0.575\n",
      "Iteration: 441 \t--- Loss: 0.576\n",
      "Iteration: 442 \t--- Loss: 0.564\n",
      "Iteration: 443 \t--- Loss: 0.581\n",
      "Iteration: 444 \t--- Loss: 0.566\n",
      "Iteration: 445 \t--- Loss: 0.570\n",
      "Iteration: 446 \t--- Loss: 0.568\n",
      "Iteration: 447 \t--- Loss: 0.567\n",
      "Iteration: 448 \t--- Loss: 0.572\n",
      "Iteration: 449 \t--- Loss: 0.576\n",
      "Iteration: 450 \t--- Loss: 0.583\n",
      "Iteration: 451 \t--- Loss: 0.569\n",
      "Iteration: 452 \t--- Loss: 0.568\n",
      "Iteration: 453 \t--- Loss: 0.577\n",
      "Iteration: 454 \t--- Loss: 0.581\n",
      "Iteration: 455 \t--- Loss: 0.567\n",
      "Iteration: 456 \t--- Loss: 0.581\n",
      "Iteration: 457 \t--- Loss: 0.574\n",
      "Iteration: 458 \t--- Loss: 0.580\n",
      "Iteration: 459 \t--- Loss: 0.568\n",
      "Iteration: 460 \t--- Loss: 0.573\n",
      "Iteration: 461 \t--- Loss: 0.578\n",
      "Iteration: 462 \t--- Loss: 0.575\n",
      "Iteration: 463 \t--- Loss: 0.572\n",
      "Iteration: 464 \t--- Loss: 0.573\n",
      "Iteration: 465 \t--- Loss: 0.578\n",
      "Iteration: 466 \t--- Loss: 0.573\n",
      "Iteration: 467 \t--- Loss: 0.569\n",
      "Iteration: 468 \t--- Loss: 0.580\n",
      "Iteration: 469 \t--- Loss: 0.561\n",
      "Iteration: 470 \t--- Loss: 0.575\n",
      "Iteration: 471 \t--- Loss: 0.574\n",
      "Iteration: 472 \t--- Loss: 0.576\n",
      "Iteration: 473 \t--- Loss: 0.562\n",
      "Iteration: 474 \t--- Loss: 0.568\n",
      "Iteration: 475 \t--- Loss: 0.573\n",
      "Iteration: 476 \t--- Loss: 0.578\n",
      "Iteration: 477 \t--- Loss: 0.571\n",
      "Iteration: 478 \t--- Loss: 0.581\n",
      "Iteration: 479 \t--- Loss: 0.579\n",
      "Iteration: 480 \t--- Loss: 0.573\n",
      "Iteration: 481 \t--- Loss: 0.567\n",
      "Iteration: 482 \t--- Loss: 0.568\n",
      "Iteration: 483 \t--- Loss: 0.567\n",
      "Iteration: 484 \t--- Loss: 0.557\n",
      "Iteration: 485 \t--- Loss: 0.578\n",
      "Iteration: 486 \t--- Loss: 0.571\n",
      "Iteration: 487 \t--- Loss: 0.558\n",
      "Iteration: 488 \t--- Loss: 0.575\n",
      "Iteration: 489 \t--- Loss: 0.574\n",
      "Iteration: 490 \t--- Loss: 0.570\n",
      "Iteration: 491 \t--- Loss: 0.583\n",
      "Iteration: 492 \t--- Loss: 0.573\n",
      "Iteration: 493 \t--- Loss: 0.577\n",
      "Iteration: 494 \t--- Loss: 0.573\n",
      "Iteration: 495 \t--- Loss: 0.581\n",
      "Iteration: 496 \t--- Loss: 0.577\n",
      "Iteration: 497 \t--- Loss: 0.575\n",
      "Iteration: 498 \t--- Loss: 0.568\n",
      "Iteration: 499 \t--- Loss: 0.570\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it][Parallel(n_jobs=5)]: Done  31 tasks      | elapsed: 16.3min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:46<00:00, 106.76s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.040\n",
      "Iteration: 261 \t--- Loss: 0.038\n",
      "Iteration: 262 \t--- Loss: 0.040\n",
      "Iteration: 263 \t--- Loss: 0.041\n",
      "Iteration: 264 \t--- Loss: 0.042\n",
      "Iteration: 265 \t--- Loss: 0.041\n",
      "Iteration: 266 \t--- Loss: 0.042\n",
      "Iteration: 267 \t--- Loss: 0.038\n",
      "Iteration: 268 \t--- Loss: 0.042\n",
      "Iteration: 269 \t--- Loss: 0.040\n",
      "Iteration: 270 \t--- Loss: 0.040\n",
      "Iteration: 271 \t--- Loss: 0.038\n",
      "Iteration: 272 \t--- Loss: 0.040\n",
      "Iteration: 273 \t--- Loss: 0.039\n",
      "Iteration: 274 \t--- Loss: 0.037\n",
      "Iteration: 275 \t--- Loss: 0.035\n",
      "Iteration: 276 \t--- Loss: 0.040\n",
      "Iteration: 277 \t--- Loss: 0.038\n",
      "Iteration: 278 \t--- Loss: 0.040\n",
      "Iteration: 279 \t--- Loss: 0.037\n",
      "Iteration: 280 \t--- Loss: 0.037\n",
      "Iteration: 281 \t--- Loss: 0.041\n",
      "Iteration: 282 \t--- Loss: 0.038\n",
      "Iteration: 283 \t--- Loss: 0.041\n",
      "Iteration: 284 \t--- Loss: 0.042\n",
      "Iteration: 285 \t--- Loss: 0.038\n",
      "Iteration: 286 \t--- Loss: 0.038\n",
      "Iteration: 287 \t--- Loss: 0.037\n",
      "Iteration: 288 \t--- Loss: 0.038\n",
      "Iteration: 289 \t--- Loss: 0.041\n",
      "Iteration: 290 \t--- Loss: 0.036\n",
      "Iteration: 291 \t--- Loss: 0.038\n",
      "Iteration: 292 \t--- Loss: 0.038\n",
      "Iteration: 293 \t--- Loss: 0.039\n",
      "Iteration: 294 \t--- Loss: 0.038\n",
      "Iteration: 295 \t--- Loss: 0.034\n",
      "Iteration: 296 \t--- Loss: 0.041\n",
      "Iteration: 297 \t--- Loss: 0.038\n",
      "Iteration: 298 \t--- Loss: 0.040\n",
      "Iteration: 299 \t--- Loss: 0.038\n",
      "Iteration: 300 \t--- Loss: 0.037\n",
      "Iteration: 301 \t--- Loss: 0.037\n",
      "Iteration: 302 \t--- Loss: 0.036\n",
      "Iteration: 303 \t--- Loss: 0.041\n",
      "Iteration: 304 \t--- Loss: 0.037\n",
      "Iteration: 305 \t--- Loss: 0.040\n",
      "Iteration: 306 \t--- Loss: 0.039\n",
      "Iteration: 307 \t--- Loss: 0.041\n",
      "Iteration: 308 \t--- Loss: 0.036\n",
      "Iteration: 309 \t--- Loss: 0.037\n",
      "Iteration: 310 \t--- Loss: 0.039\n",
      "Iteration: 311 \t--- Loss: 0.036\n",
      "Iteration: 312 \t--- Loss: 0.039\n",
      "Iteration: 313 \t--- Loss: 0.041\n",
      "Iteration: 314 \t--- Loss: 0.039\n",
      "Iteration: 315 \t--- Loss: 0.039\n",
      "Iteration: 316 \t--- Loss: 0.039\n",
      "Iteration: 317 \t--- Loss: 0.040\n",
      "Iteration: 318 \t--- Loss: 0.039\n",
      "Iteration: 319 \t--- Loss: 0.037\n",
      "Iteration: 320 \t--- Loss: 0.038\n",
      "Iteration: 321 \t--- Loss: 0.035\n",
      "Iteration: 322 \t--- Loss: 0.040\n",
      "Iteration: 323 \t--- Loss: 0.038\n",
      "Iteration: 324 \t--- Loss: 0.038\n",
      "Iteration: 325 \t--- Loss: 0.037\n",
      "Iteration: 326 \t--- Loss: 0.039\n",
      "Iteration: 327 \t--- Loss: 0.039\n",
      "Iteration: 328 \t--- Loss: 0.037\n",
      "Iteration: 329 \t--- Loss: 0.038\n",
      "Iteration: 330 \t--- Loss: 0.039\n",
      "Iteration: 331 \t--- Loss: 0.039\n",
      "Iteration: 332 \t--- Loss: 0.038\n",
      "Iteration: 333 \t--- Loss: 0.040\n",
      "Iteration: 334 \t--- Loss: 0.038\n",
      "Iteration: 335 \t--- Loss: 0.039\n",
      "Iteration: 336 \t--- Loss: 0.038\n",
      "Iteration: 337 \t--- Loss: 0.041\n",
      "Iteration: 338 \t--- Loss: 0.038\n",
      "Iteration: 339 \t--- Loss: 0.041\n",
      "Iteration: 340 \t--- Loss: 0.038\n",
      "Iteration: 341 \t--- Loss: 0.039\n",
      "Iteration: 342 \t--- Loss: 0.038\n",
      "Iteration: 343 \t--- Loss: 0.038\n",
      "Iteration: 344 \t--- Loss: 0.037\n",
      "Iteration: 345 \t--- Loss: 0.038\n",
      "Iteration: 346 \t--- Loss: 0.035\n",
      "Iteration: 347 \t--- Loss: 0.035\n",
      "Iteration: 348 \t--- Loss: 0.036\n",
      "Iteration: 349 \t--- Loss: 0.039\n",
      "Iteration: 350 \t--- Loss: 0.040\n",
      "Iteration: 351 \t--- Loss: 0.037\n",
      "Iteration: 352 \t--- Loss: 0.040\n",
      "Iteration: 353 \t--- Loss: 0.038\n",
      "Iteration: 354 \t--- Loss: 0.038\n",
      "Iteration: 355 \t--- Loss: 0.036\n",
      "Iteration: 356 \t--- Loss: 0.036\n",
      "Iteration: 357 \t--- Loss: 0.038\n",
      "Iteration: 358 \t--- Loss: 0.040\n",
      "Iteration: 359 \t--- Loss: 0.037\n",
      "Iteration: 360 \t--- Loss: 0.038\n",
      "Iteration: 361 \t--- Loss: 0.038\n",
      "Iteration: 362 \t--- Loss: 0.038\n",
      "Iteration: 363 \t--- Loss: 0.037\n",
      "Iteration: 364 \t--- Loss: 0.035\n",
      "Iteration: 365 \t--- Loss: 0.037\n",
      "Iteration: 366 \t--- Loss: 0.037\n",
      "Iteration: 367 \t--- Loss: 0.034\n",
      "Iteration: 368 \t--- Loss: 0.039\n",
      "Iteration: 369 \t--- Loss: 0.038\n",
      "Iteration: 370 \t--- Loss: 0.039\n",
      "Iteration: 371 \t--- Loss: 0.037\n",
      "Iteration: 372 \t--- Loss: 0.042\n",
      "Iteration: 373 \t--- Loss: 0.038\n",
      "Iteration: 374 \t--- Loss: 0.039\n",
      "Iteration: 375 \t--- Loss: 0.038\n",
      "Iteration: 376 \t--- Loss: 0.040\n",
      "Iteration: 377 \t--- Loss: 0.039\n",
      "Iteration: 378 \t--- Loss: 0.038\n",
      "Iteration: 379 \t--- Loss: 0.040\n",
      "Iteration: 380 \t--- Loss: 0.040\n",
      "Iteration: 381 \t--- Loss: 0.037\n",
      "Iteration: 382 \t--- Loss: 0.037\n",
      "Iteration: 383 \t--- Loss: 0.039\n",
      "Iteration: 384 \t--- Loss: 0.036\n",
      "Iteration: 385 \t--- Loss: 0.038\n",
      "Iteration: 386 \t--- Loss: 0.038\n",
      "Iteration: 387 \t--- Loss: 0.038\n",
      "Iteration: 388 \t--- Loss: 0.036\n",
      "Iteration: 389 \t--- Loss: 0.036\n",
      "Iteration: 390 \t--- Loss: 0.035\n",
      "Iteration: 391 \t--- Loss: 0.040\n",
      "Iteration: 392 \t--- Loss: 0.040\n",
      "Iteration: 393 \t--- Loss: 0.038\n",
      "Iteration: 394 \t--- Loss: 0.038\n",
      "Iteration: 395 \t--- Loss: 0.037\n",
      "Iteration: 396 \t--- Loss: 0.037\n",
      "Iteration: 397 \t--- Loss: 0.041\n",
      "Iteration: 398 \t--- Loss: 0.041\n",
      "Iteration: 399 \t--- Loss: 0.037\n",
      "Iteration: 400 \t--- Loss: 0.038\n",
      "Iteration: 401 \t--- Loss: 0.037\n",
      "Iteration: 402 \t--- Loss: 0.038\n",
      "Iteration: 403 \t--- Loss: 0.038\n",
      "Iteration: 404 \t--- Loss: 0.038\n",
      "Iteration: 405 \t--- Loss: 0.037\n",
      "Iteration: 406 \t--- Loss: 0.039\n",
      "Iteration: 407 \t--- Loss: 0.038\n",
      "Iteration: 408 \t--- Loss: 0.038\n",
      "Iteration: 409 \t--- Loss: 0.036\n",
      "Iteration: 410 \t--- Loss: 0.036\n",
      "Iteration: 411 \t--- Loss: 0.039\n",
      "Iteration: 412 \t--- Loss: 0.037\n",
      "Iteration: 413 \t--- Loss: 0.037\n",
      "Iteration: 414 \t--- Loss: 0.038\n",
      "Iteration: 415 \t--- Loss: 0.041\n",
      "Iteration: 416 \t--- Loss: 0.039\n",
      "Iteration: 417 \t--- Loss: 0.038\n",
      "Iteration: 418 \t--- Loss: 0.039\n",
      "Iteration: 419 \t--- Loss: 0.039\n",
      "Iteration: 420 \t--- Loss: 0.038\n",
      "Iteration: 421 \t--- Loss: 0.039\n",
      "Iteration: 422 \t--- Loss: 0.040\n",
      "Iteration: 423 \t--- Loss: 0.037\n",
      "Iteration: 424 \t--- Loss: 0.038\n",
      "Iteration: 425 \t--- Loss: 0.039\n",
      "Iteration: 426 \t--- Loss: 0.038\n",
      "Iteration: 427 \t--- Loss: 0.041\n",
      "Iteration: 428 \t--- Loss: 0.036\n",
      "Iteration: 429 \t--- Loss: 0.038\n",
      "Iteration: 430 \t--- Loss: 0.035\n",
      "Iteration: 431 \t--- Loss: 0.038\n",
      "Iteration: 432 \t--- Loss: 0.039\n",
      "Iteration: 433 \t--- Loss: 0.040\n",
      "Iteration: 434 \t--- Loss: 0.036\n",
      "Iteration: 435 \t--- Loss: 0.038\n",
      "Iteration: 436 \t--- Loss: 0.040\n",
      "Iteration: 437 \t--- Loss: 0.037\n",
      "Iteration: 438 \t--- Loss: 0.037\n",
      "Iteration: 439 \t--- Loss: 0.036\n",
      "Iteration: 440 \t--- Loss: 0.040\n",
      "Iteration: 441 \t--- Loss: 0.037\n",
      "Iteration: 442 \t--- Loss: 0.038\n",
      "Iteration: 443 \t--- Loss: 0.038\n",
      "Iteration: 444 \t--- Loss: 0.037\n",
      "Iteration: 445 \t--- Loss: 0.038\n",
      "Iteration: 446 \t--- Loss: 0.037\n",
      "Iteration: 447 \t--- Loss: 0.039\n",
      "Iteration: 448 \t--- Loss: 0.036\n",
      "Iteration: 449 \t--- Loss: 0.038\n",
      "Iteration: 450 \t--- Loss: 0.038\n",
      "Iteration: 451 \t--- Loss: 0.037\n",
      "Iteration: 452 \t--- Loss: 0.037\n",
      "Iteration: 453 \t--- Loss: 0.036\n",
      "Iteration: 454 \t--- Loss: 0.036\n",
      "Iteration: 455 \t--- Loss: 0.038\n",
      "Iteration: 456 \t--- Loss: 0.037\n",
      "Iteration: 457 \t--- Loss: 0.038\n",
      "Iteration: 458 \t--- Loss: 0.036\n",
      "Iteration: 459 \t--- Loss: 0.039\n",
      "Iteration: 460 \t--- Loss: 0.041\n",
      "Iteration: 461 \t--- Loss: 0.040\n",
      "Iteration: 462 \t--- Loss: 0.038\n",
      "Iteration: 463 \t--- Loss: 0.036\n",
      "Iteration: 464 \t--- Loss: 0.040\n",
      "Iteration: 465 \t--- Loss: 0.039\n",
      "Iteration: 466 \t--- Loss: 0.036\n",
      "Iteration: 467 \t--- Loss: 0.036\n",
      "Iteration: 468 \t--- Loss: 0.037\n",
      "Iteration: 469 \t--- Loss: 0.036\n",
      "Iteration: 470 \t--- Loss: 0.038\n",
      "Iteration: 471 \t--- Loss: 0.040\n",
      "Iteration: 472 \t--- Loss: 0.039\n",
      "Iteration: 473 \t--- Loss: 0.036\n",
      "Iteration: 474 \t--- Loss: 0.035\n",
      "Iteration: 475 \t--- Loss: 0.035\n",
      "Iteration: 476 \t--- Loss: 0.039\n",
      "Iteration: 477 \t--- Loss: 0.039\n",
      "Iteration: 478 \t--- Loss: 0.036\n",
      "Iteration: 479 \t--- Loss: 0.040\n",
      "Iteration: 480 \t--- Loss: 0.041\n",
      "Iteration: 481 \t--- Loss: 0.036\n",
      "Iteration: 482 \t--- Loss: 0.037\n",
      "Iteration: 483 \t--- Loss: 0.038\n",
      "Iteration: 484 \t--- Loss: 0.035\n",
      "Iteration: 485 \t--- Loss: 0.038\n",
      "Iteration: 486 \t--- Loss: 0.037\n",
      "Iteration: 487 \t--- Loss: 0.040\n",
      "Iteration: 488 \t--- Loss: 0.036\n",
      "Iteration: 489 \t--- Loss: 0.039\n",
      "Iteration: 490 \t--- Loss: 0.039\n",
      "Iteration: 491 \t--- Loss: 0.036\n",
      "Iteration: 492 \t--- Loss: 0.041\n",
      "Iteration: 493 \t--- Loss: 0.038\n",
      "Iteration: 494 \t--- Loss: 0.036\n",
      "Iteration: 495 \t--- Loss: 0.036\n",
      "Iteration: 496 \t--- Loss: 0.038\n",
      "Iteration: 497 \t--- Loss: 0.035\n",
      "Iteration: 498 \t--- Loss: 0.038\n",
      "Iteration: 499 \t--- Loss: 0.037\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.53s/it][Parallel(n_jobs=5)]: Done  32 tasks      | elapsed: 16.6min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.243\n",
      "Iteration: 1 \t--- Loss: 0.238\n",
      "Iteration: 2 \t--- Loss: 0.235\n",
      "Iteration: 3 \t--- Loss: 0.233\n",
      "Iteration: 4 \t--- Loss: 0.228\n",
      "Iteration: 5 \t--- Loss: 0.222\n",
      "Iteration: 6 \t--- Loss: 0.218\n",
      "Iteration: 7 \t--- Loss: 0.218\n",
      "Iteration: 8 \t--- Loss: 0.217\n",
      "Iteration: 9 \t--- Loss: 0.220\n",
      "Iteration: 10 \t--- Loss: 0.214\n",
      "Iteration: 11 \t--- Loss: 0.208\n",
      "Iteration: 12 \t--- Loss: 0.202\n",
      "Iteration: 13 \t--- Loss: 0.209\n",
      "Iteration: 14 \t--- Loss: 0.207\n",
      "Iteration: 15 \t--- Loss: 0.202\n",
      "Iteration: 16 \t--- Loss: 0.202\n",
      "Iteration: 17 \t--- Loss: 0.201\n",
      "Iteration: 18 \t--- Loss: 0.192\n",
      "Iteration: 19 \t--- Loss: 0.200\n",
      "Iteration: 20 \t--- Loss: 0.198\n",
      "Iteration: 21 \t--- Loss: 0.190\n",
      "Iteration: 22 \t--- Loss: 0.200\n",
      "Iteration: 23 \t--- Loss: 0.193\n",
      "Iteration: 24 \t--- Loss: 0.193\n",
      "Iteration: 25 \t--- Loss: 0.188\n",
      "Iteration: 26 \t--- Loss: 0.191\n",
      "Iteration: 27 \t--- Loss: 0.185\n",
      "Iteration: 28 \t--- Loss: 0.190\n",
      "Iteration: 29 \t--- Loss: 0.177\n",
      "Iteration: 30 \t--- Loss: 0.176\n",
      "Iteration: 31 \t--- Loss: 0.163\n",
      "Iteration: 32 \t--- Loss: 0.138\n",
      "Iteration: 33 \t--- Loss: 0.143\n",
      "Iteration: 34 \t--- Loss: 0.139\n",
      "Iteration: 35 \t--- Loss: 0.146\n",
      "Iteration: 36 \t--- Loss: 0.145\n",
      "Iteration: 37 \t--- Loss: 0.142\n",
      "Iteration: 38 \t--- Loss: 0.157\n",
      "Iteration: 39 \t--- Loss: 0.177\n",
      "Iteration: 40 \t--- Loss: 0.166\n",
      "Iteration: 41 \t--- Loss: 0.214\n",
      "Iteration: 42 \t--- Loss: 0.168\n",
      "Iteration: 43 \t--- Loss: 0.170\n",
      "Iteration: 44 \t--- Loss: 0.204\n",
      "Iteration: 45 \t--- Loss: 0.151\n",
      "Iteration: 46 \t--- Loss: 0.136\n",
      "Iteration: 47 \t--- Loss: 0.135\n",
      "Iteration: 48 \t--- Loss: 0.176\n",
      "Iteration: 49 \t--- Loss: 0.198\n",
      "Iteration: 50 \t--- Loss: 0.199\n",
      "Iteration: 51 \t--- Loss: 0.169\n",
      "Iteration: 52 \t--- Loss: 0.117\n",
      "Iteration: 53 \t--- Loss: 0.134\n",
      "Iteration: 54 \t--- Loss: 0.138\n",
      "Iteration: 55 \t--- Loss: 0.131\n",
      "Iteration: 56 \t--- Loss: 0.158\n",
      "Iteration: 57 \t--- Loss: 0.120\n",
      "Iteration: 58 \t--- Loss: 0.118\n",
      "Iteration: 59 \t--- Loss: 0.163\n",
      "Iteration: 60 \t--- Loss: 0.206\n",
      "Iteration: 61 \t--- Loss: 0.196\n",
      "Iteration: 62 \t--- Loss: 0.180\n",
      "Iteration: 63 \t--- Loss: 0.144\n",
      "Iteration: 64 \t--- Loss: 0.106\n",
      "Iteration: 65 \t--- Loss: 0.102\n",
      "Iteration: 66 \t--- Loss: 0.112\n",
      "Iteration: 67 \t--- Loss: 0.109\n",
      "Iteration: 68 \t--- Loss: 0.137\n",
      "Iteration: 69 \t--- Loss: 0.178\n",
      "Iteration: 70 \t--- Loss: 0.153\n",
      "Iteration: 71 \t--- Loss: 0.091\n",
      "Iteration: 72 \t--- Loss: 0.106\n",
      "Iteration: 73 \t--- Loss: 0.112\n",
      "Iteration: 74 \t--- Loss: 0.118\n",
      "Iteration: 75 \t--- Loss: 0.155\n",
      "Iteration: 76 \t--- Loss: 0.087\n",
      "Iteration: 77 \t--- Loss: 0.103\n",
      "Iteration: 78 \t--- Loss: 0.105\n",
      "Iteration: 79 \t--- Loss: 0.114\n",
      "Iteration: 80 \t--- Loss: 0.146\n",
      "Iteration: 81 \t--- Loss: 0.084\n",
      "Iteration: 82 \t--- Loss: 0.078\n",
      "Iteration: 83 \t--- Loss: 0.085\n",
      "Iteration: 84 \t--- Loss: 0.095\n",
      "Iteration: 85 \t--- Loss: 0.131\n",
      "Iteration: 86 \t--- Loss: 0.176\n",
      "Iteration: 87 \t--- Loss: 0.134\n",
      "Iteration: 88 \t--- Loss: 0.081\n",
      "Iteration: 89 \t--- Loss: 0.080\n",
      "Iteration: 90 \t--- Loss: 0.095\n",
      "Iteration: 91 \t--- Loss: 0.113\n",
      "Iteration: 92 \t--- Loss: 0.083\n",
      "Iteration: 93 \t--- Loss: 0.085\n",
      "Iteration: 94 \t--- Loss: 0.108\n",
      "Iteration: 95 \t--- Loss: 0.146\n",
      "Iteration: 96 \t--- Loss: 0.076\n",
      "Iteration: 97 \t--- Loss: 0.083\n",
      "Iteration: 98 \t--- Loss: 0.098\n",
      "Iteration: 99 \t--- Loss: 0.081\n",
      "Iteration: 100 \t--- Loss: 0.084\n",
      "Iteration: 101 \t--- Loss: 0.079\n",
      "Iteration: 102 \t--- Loss: 0.083\n",
      "Iteration: 103 \t--- Loss: 0.079\n",
      "Iteration: 104 \t--- Loss: 0.093\n",
      "Iteration: 105 \t--- Loss: 0.081\n",
      "Iteration: 106 \t--- Loss: 0.092\n",
      "Iteration: 107 \t--- Loss: 0.065\n",
      "Iteration: 108 \t--- Loss: 0.066\n",
      "Iteration: 109 \t--- Loss: 0.074\n",
      "Iteration: 110 \t--- Loss: 0.087\n",
      "Iteration: 111 \t--- Loss: 0.070\n",
      "Iteration: 112 \t--- Loss: 0.080\n",
      "Iteration: 113 \t--- Loss: 0.069\n",
      "Iteration: 114 \t--- Loss: 0.076\n",
      "Iteration: 115 \t--- Loss: 0.068\n",
      "Iteration: 116 \t--- Loss: 0.072\n",
      "Iteration: 117 \t--- Loss: 0.068\n",
      "Iteration: 118 \t--- Loss: 0.069\n",
      "Iteration: 119 \t--- Loss: 0.067\n",
      "Iteration: 120 \t--- Loss: 0.068\n",
      "Iteration: 121 \t--- Loss: 0.063\n",
      "Iteration: 122 \t--- Loss: 0.075\n",
      "Iteration: 123 \t--- Loss: 0.064\n",
      "Iteration: 124 \t--- Loss: 0.067\n",
      "Iteration: 125 \t--- Loss: 0.066\n",
      "Iteration: 126 \t--- Loss: 0.065\n",
      "Iteration: 127 \t--- Loss: 0.053\n",
      "Iteration: 128 \t--- Loss: 0.052\n",
      "Iteration: 129 \t--- Loss: 0.053\n",
      "Iteration: 130 \t--- Loss: 0.056\n",
      "Iteration: 131 \t--- Loss: 0.061\n",
      "Iteration: 132 \t--- Loss: 0.071\n",
      "Iteration: 133 \t--- Loss: 0.059\n",
      "Iteration: 134 \t--- Loss: 0.060\n",
      "Iteration: 135 \t--- Loss: 0.055\n",
      "Iteration: 136 \t--- Loss: 0.064\n",
      "Iteration: 137 \t--- Loss: 0.056\n",
      "Iteration: 138 \t--- Loss: 0.051\n",
      "Iteration: 139 \t--- Loss: 0.051\n",
      "Iteration: 140 \t--- Loss: 0.051\n",
      "Iteration: 141 \t--- Loss: 0.052\n",
      "Iteration: 142 \t--- Loss: 0.055\n",
      "Iteration: 143 \t--- Loss: 0.054\n",
      "Iteration: 144 \t--- Loss: 0.056\n",
      "Iteration: 145 \t--- Loss: 0.057\n",
      "Iteration: 146 \t--- Loss: 0.058\n",
      "Iteration: 147 \t--- Loss: 0.051\n",
      "Iteration: 148 \t--- Loss: 0.055\n",
      "Iteration: 149 \t--- Loss: 0.059\n",
      "Iteration: 150 \t--- Loss: 0.063\n",
      "Iteration: 151 \t--- Loss: 0.049\n",
      "Iteration: 152 \t--- Loss: 0.055\n",
      "Iteration: 153 \t--- Loss: 0.052\n",
      "Iteration: 154 \t--- Loss: 0.055\n",
      "Iteration: 155 \t--- Loss: 0.052\n",
      "Iteration: 156 \t--- Loss: 0.047\n",
      "Iteration: 157 \t--- Loss: 0.047\n",
      "Iteration: 158 \t--- Loss: 0.044\n",
      "Iteration: 159 \t--- Loss: 0.049\n",
      "Iteration: 160 \t--- Loss: 0.050\n",
      "Iteration: 161 \t--- Loss: 0.045\n",
      "Iteration: 162 \t--- Loss: 0.050\n",
      "Iteration: 163 \t--- Loss: 0.048\n",
      "Iteration: 164 \t--- Loss: 0.050\n",
      "Iteration: 165 \t--- Loss: 0.052\n",
      "Iteration: 166 \t--- Loss: 0.056\n",
      "Iteration: 167 \t--- Loss: 0.049\n",
      "Iteration: 168 \t--- Loss: 0.053\n",
      "Iteration: 169 \t--- Loss: 0.048\n",
      "Iteration: 170 \t--- Loss: 0.053\n",
      "Iteration: 171 \t--- Loss: 0.053\n",
      "Iteration: 172 \t--- Loss: 0.050\n",
      "Iteration: 173 \t--- Loss: 0.044\n",
      "Iteration: 174 \t--- Loss: 0.044\n",
      "Iteration: 175 \t--- Loss: 0.048\n",
      "Iteration: 176 \t--- Loss: 0.051\n",
      "Iteration: 177 \t--- Loss: 0.045\n",
      "Iteration: 178 \t--- Loss: 0.047\n",
      "Iteration: 179 \t--- Loss: 0.046\n",
      "Iteration: 180 \t--- Loss: 0.044\n",
      "Iteration: 181 \t--- Loss: 0.044\n",
      "Iteration: 182 \t--- Loss: 0.039\n",
      "Iteration: 183 \t--- Loss: 0.043\n",
      "Iteration: 184 \t--- Loss: 0.043\n",
      "Iteration: 185 \t--- Loss: 0.043\n",
      "Iteration: 186 \t--- Loss: 0.042\n",
      "Iteration: 187 \t--- Loss: 0.043\n",
      "Iteration: 188 \t--- Loss: 0.040\n",
      "Iteration: 189 \t--- Loss: 0.039\n",
      "Iteration: 190 \t--- Loss: 0.041\n",
      "Iteration: 191 \t--- Loss: 0.040\n",
      "Iteration: 192 \t--- Loss: 0.041\n",
      "Iteration: 193 \t--- Loss: 0.040\n",
      "Iteration: 194 \t--- Loss: 0.040\n",
      "Iteration: 195 \t--- Loss: 0.040\n",
      "Iteration: 196 \t--- Loss: 0.042\n",
      "Iteration: 197 \t--- Loss: 0.041\n",
      "Iteration: 198 \t--- Loss: 0.043\n",
      "Iteration: 199 \t--- Loss: 0.043\n",
      "Iteration: 200 \t--- Loss: 0.045\n",
      "Iteration: 201 \t--- Loss: 0.044\n",
      "Iteration: 202 \t--- Loss: 0.042\n",
      "Iteration: 203 \t--- Loss: 0.044\n",
      "Iteration: 204 \t--- Loss: 0.045\n",
      "Iteration: 205 \t--- Loss: 0.041\n",
      "Iteration: 206 \t--- Loss: 0.044\n",
      "Iteration: 207 \t--- Loss: 0.042\n",
      "Iteration: 208 \t--- Loss: 0.043\n",
      "Iteration: 209 \t--- Loss: 0.042\n",
      "Iteration: 210 \t--- Loss: 0.047\n",
      "Iteration: 211 \t--- Loss: 0.047\n",
      "Iteration: 212 \t--- Loss: 0.042\n",
      "Iteration: 213 \t--- Loss: 0.042\n",
      "Iteration: 214 \t--- Loss: 0.043\n",
      "Iteration: 215 \t--- Loss: 0.042\n",
      "Iteration: 216 \t--- Loss: 0.042\n",
      "Iteration: 217 \t--- Loss: 0.038\n",
      "Iteration: 218 \t--- Loss: 0.040\n",
      "Iteration: 219 \t--- Loss: 0.041\n",
      "Iteration: 220 \t--- Loss: 0.041\n",
      "Iteration: 221 \t--- Loss: 0.039\n",
      "Iteration: 222 \t--- Loss: 0.043\n",
      "Iteration: 223 \t--- Loss: 0.048\n",
      "Iteration: 224 \t--- Loss: 0.050\n",
      "Iteration: 225 \t--- Loss: 0.047\n",
      "Iteration: 226 \t--- Loss: 0.047\n",
      "Iteration: 227 \t--- Loss: 0.041\n",
      "Iteration: 228 \t--- Loss: 0.043\n",
      "Iteration: 229 \t--- Loss: 0.041\n",
      "Iteration: 230 \t--- Loss: 0.044\n",
      "Iteration: 231 \t--- Loss: 0.041\n",
      "Iteration: 232 \t--- Loss: 0.040\n",
      "Iteration: 233 \t--- Loss: 0.039\n",
      "Iteration: 234 \t--- Loss: 0.038\n",
      "Iteration: 235 \t--- Loss: 0.040\n",
      "Iteration: 236 \t--- Loss: 0.037\n",
      "Iteration: 237 \t--- Loss: 0.041\n",
      "Iteration: 238 \t--- Loss: 0.037\n",
      "Iteration: 239 \t--- Loss: 0.039\n",
      "Iteration: 240 \t--- Loss: 0.041\n",
      "Iteration: 241 \t--- Loss: 0.040\n",
      "Iteration: 242 \t--- Loss: 0.039\n",
      "Iteration: 243 \t--- Loss: 0.040\n",
      "Iteration: 244 \t--- Loss: 0.039\n",
      "Iteration: 245 \t--- Loss: 0.039\n",
      "Iteration: 246 \t--- Loss: 0.039\n",
      "Iteration: 247 \t--- Loss: 0.038\n",
      "Iteration: 248 \t--- Loss: 0.038\n",
      "Iteration: 249 \t--- Loss: 0.038\n",
      "Iteration: 250 \t--- Loss: 0.037\n",
      "Iteration: 251 \t--- Loss: 0.039\n",
      "Iteration: 252 \t--- Loss: 0.036\n",
      "Iteration: 253 \t--- Loss: 0.039\n",
      "Iteration: 254 \t--- Loss: 0.036\n",
      "Iteration: 255 \t--- Loss: 0.040\n",
      "Iteration: 256 \t--- Loss: 0.038\n",
      "Iteration: 257 \t--- Loss: 0.038\n",
      "Iteration: 258 \t--- Loss: 0.039\n",
      "Iteration: 259 \t--- Loss: 0.039Iteration: 0 \t--- Loss: 0.825\n",
      "Iteration: 1 \t--- Loss: 0.744\n",
      "Iteration: 2 \t--- Loss: 0.696\n",
      "Iteration: 3 \t--- Loss: 0.640\n",
      "Iteration: 4 \t--- Loss: 0.600\n",
      "Iteration: 5 \t--- Loss: 0.566\n",
      "Iteration: 6 \t--- Loss: 0.540\n",
      "Iteration: 7 \t--- Loss: 0.530\n",
      "Iteration: 8 \t--- Loss: 0.514\n",
      "Iteration: 9 \t--- Loss: 0.500\n",
      "Iteration: 10 \t--- Loss: 0.487\n",
      "Iteration: 11 \t--- Loss: 0.475\n",
      "Iteration: 12 \t--- Loss: 0.451\n",
      "Iteration: 13 \t--- Loss: 0.458\n",
      "Iteration: 14 \t--- Loss: 0.450\n",
      "Iteration: 15 \t--- Loss: 0.452\n",
      "Iteration: 16 \t--- Loss: 0.444\n",
      "Iteration: 17 \t--- Loss: 0.433\n",
      "Iteration: 18 \t--- Loss: 0.440\n",
      "Iteration: 19 \t--- Loss: 0.431\n",
      "Iteration: 20 \t--- Loss: 0.422\n",
      "Iteration: 21 \t--- Loss: 0.419\n",
      "Iteration: 22 \t--- Loss: 0.426\n",
      "Iteration: 23 \t--- Loss: 0.427\n",
      "Iteration: 24 \t--- Loss: 0.444\n",
      "Iteration: 25 \t--- Loss: 0.432\n",
      "Iteration: 26 \t--- Loss: 0.417\n",
      "Iteration: 27 \t--- Loss: 0.421\n",
      "Iteration: 28 \t--- Loss: 0.409\n",
      "Iteration: 29 \t--- Loss: 0.402\n",
      "Iteration: 30 \t--- Loss: 0.420\n",
      "Iteration: 31 \t--- Loss: 0.417\n",
      "Iteration: 32 \t--- Loss: 0.416\n",
      "Iteration: 33 \t--- Loss: 0.414\n",
      "Iteration: 34 \t--- Loss: 0.405\n",
      "Iteration: 35 \t--- Loss: 0.411\n",
      "Iteration: 36 \t--- Loss: 0.420\n",
      "Iteration: 37 \t--- Loss: 0.418\n",
      "Iteration: 38 \t--- Loss: 0.394\n",
      "Iteration: 39 \t--- Loss: 0.416\n",
      "Iteration: 40 \t--- Loss: 0.422\n",
      "Iteration: 41 \t--- Loss: 0.423\n",
      "Iteration: 42 \t--- Loss: 0.407\n",
      "Iteration: 43 \t--- Loss: 0.431\n",
      "Iteration: 44 \t--- Loss: 0.413\n",
      "Iteration: 45 \t--- Loss: 0.397\n",
      "Iteration: 46 \t--- Loss: 0.406\n",
      "Iteration: 47 \t--- Loss: 0.406\n",
      "Iteration: 48 \t--- Loss: 0.411\n",
      "Iteration: 49 \t--- Loss: 0.412\n",
      "Iteration: 50 \t--- Loss: 0.417\n",
      "Iteration: 51 \t--- Loss: 0.402\n",
      "Iteration: 52 \t--- Loss: 0.406\n",
      "Iteration: 53 \t--- Loss: 0.422\n",
      "Iteration: 54 \t--- Loss: 0.413\n",
      "Iteration: 55 \t--- Loss: 0.425\n",
      "Iteration: 56 \t--- Loss: 0.408\n",
      "Iteration: 57 \t--- Loss: 0.401\n",
      "Iteration: 58 \t--- Loss: 0.417\n",
      "Iteration: 59 \t--- Loss: 0.419\n",
      "Iteration: 60 \t--- Loss: 0.416\n",
      "Iteration: 61 \t--- Loss: 0.403\n",
      "Iteration: 62 \t--- Loss: 0.398\n",
      "Iteration: 63 \t--- Loss: 0.422\n",
      "Iteration: 64 \t--- Loss: 0.400\n",
      "Iteration: 65 \t--- Loss: 0.418\n",
      "Iteration: 66 \t--- Loss: 0.423\n",
      "Iteration: 67 \t--- Loss: 0.423\n",
      "Iteration: 68 \t--- Loss: 0.419\n",
      "Iteration: 69 \t--- Loss: 0.410\n",
      "Iteration: 70 \t--- Loss: 0.425\n",
      "Iteration: 71 \t--- Loss: 0.412\n",
      "Iteration: 72 \t--- Loss: 0.436\n",
      "Iteration: 73 \t--- Loss: 0.419\n",
      "Iteration: 74 \t--- Loss: 0.410\n",
      "Iteration: 75 \t--- Loss: 0.413\n",
      "Iteration: 76 \t--- Loss: 0.408\n",
      "Iteration: 77 \t--- Loss: 0.422\n",
      "Iteration: 78 \t--- Loss: 0.415\n",
      "Iteration: 79 \t--- Loss: 0.389\n",
      "Iteration: 80 \t--- Loss: 0.417\n",
      "Iteration: 81 \t--- Loss: 0.414\n",
      "Iteration: 82 \t--- Loss: 0.421\n",
      "Iteration: 83 \t--- Loss: 0.408\n",
      "Iteration: 84 \t--- Loss: 0.406\n",
      "Iteration: 85 \t--- Loss: 0.422\n",
      "Iteration: 86 \t--- Loss: 0.402\n",
      "Iteration: 87 \t--- Loss: 0.406\n",
      "Iteration: 88 \t--- Loss: 0.416\n",
      "Iteration: 89 \t--- Loss: 0.413\n",
      "Iteration: 90 \t--- Loss: 0.429\n",
      "Iteration: 91 \t--- Loss: 0.417\n",
      "Iteration: 92 \t--- Loss: 0.418\n",
      "Iteration: 93 \t--- Loss: 0.421\n",
      "Iteration: 94 \t--- Loss: 0.400\n",
      "Iteration: 95 \t--- Loss: 0.427\n",
      "Iteration: 96 \t--- Loss: 0.406\n",
      "Iteration: 97 \t--- Loss: 0.425\n",
      "Iteration: 98 \t--- Loss: 0.420\n",
      "Iteration: 99 \t--- Loss: 0.414\n",
      "Iteration: 100 \t--- Loss: 0.438\n",
      "Iteration: 101 \t--- Loss: 0.408\n",
      "Iteration: 102 \t--- Loss: 0.425\n",
      "Iteration: 103 \t--- Loss: 0.419\n",
      "Iteration: 104 \t--- Loss: 0.406\n",
      "Iteration: 105 \t--- Loss: 0.410\n",
      "Iteration: 106 \t--- Loss: 0.417\n",
      "Iteration: 107 \t--- Loss: 0.420\n",
      "Iteration: 108 \t--- Loss: 0.418\n",
      "Iteration: 109 \t--- Loss: 0.421\n",
      "Iteration: 110 \t--- Loss: 0.425\n",
      "Iteration: 111 \t--- Loss: 0.422\n",
      "Iteration: 112 \t--- Loss: 0.409\n",
      "Iteration: 113 \t--- Loss: 0.413\n",
      "Iteration: 114 \t--- Loss: 0.403\n",
      "Iteration: 115 \t--- Loss: 0.432\n",
      "Iteration: 116 \t--- Loss: 0.410\n",
      "Iteration: 117 \t--- Loss: 0.418\n",
      "Iteration: 118 \t--- Loss: 0.401\n",
      "Iteration: 119 \t--- Loss: 0.410\n",
      "Iteration: 120 \t--- Loss: 0.414\n",
      "Iteration: 121 \t--- Loss: 0.407\n",
      "Iteration: 122 \t--- Loss: 0.397\n",
      "Iteration: 123 \t--- Loss: 0.409\n",
      "Iteration: 124 \t--- Loss: 0.395\n",
      "Iteration: 125 \t--- Loss: 0.411\n",
      "Iteration: 126 \t--- Loss: 0.403\n",
      "Iteration: 127 \t--- Loss: 0.410\n",
      "Iteration: 128 \t--- Loss: 0.419\n",
      "Iteration: 129 \t--- Loss: 0.408\n",
      "Iteration: 130 \t--- Loss: 0.417\n",
      "Iteration: 131 \t--- Loss: 0.419\n",
      "Iteration: 132 \t--- Loss: 0.426\n",
      "Iteration: 133 \t--- Loss: 0.416\n",
      "Iteration: 134 \t--- Loss: 0.421\n",
      "Iteration: 135 \t--- Loss: 0.423\n",
      "Iteration: 136 \t--- Loss: 0.415\n",
      "Iteration: 137 \t--- Loss: 0.402\n",
      "Iteration: 138 \t--- Loss: 0.421\n",
      "Iteration: 139 \t--- Loss: 0.411\n",
      "Iteration: 140 \t--- Loss: 0.421\n",
      "Iteration: 141 \t--- Loss: 0.420\n",
      "Iteration: 142 \t--- Loss: 0.409\n",
      "Iteration: 143 \t--- Loss: 0.402\n",
      "Iteration: 144 \t--- Loss: 0.416\n",
      "Iteration: 145 \t--- Loss: 0.411\n",
      "Iteration: 146 \t--- Loss: 0.417\n",
      "Iteration: 147 \t--- Loss: 0.415\n",
      "Iteration: 148 \t--- Loss: 0.402\n",
      "Iteration: 149 \t--- Loss: 0.413\n",
      "Iteration: 150 \t--- Loss: 0.407\n",
      "Iteration: 151 \t--- Loss: 0.414\n",
      "Iteration: 152 \t--- Loss: 0.415\n",
      "Iteration: 153 \t--- Loss: 0.406\n",
      "Iteration: 154 \t--- Loss: 0.413\n",
      "Iteration: 155 \t--- Loss: 0.405\n",
      "Iteration: 156 \t--- Loss: 0.402\n",
      "Iteration: 157 \t--- Loss: 0.421\n",
      "Iteration: 158 \t--- Loss: 0.416\n",
      "Iteration: 159 \t--- Loss: 0.412\n",
      "Iteration: 160 \t--- Loss: 0.422\n",
      "Iteration: 161 \t--- Loss: 0.416\n",
      "Iteration: 162 \t--- Loss: 0.403\n",
      "Iteration: 163 \t--- Loss: 0.411\n",
      "Iteration: 164 \t--- Loss: 0.412\n",
      "Iteration: 165 \t--- Loss: 0.408\n",
      "Iteration: 166 \t--- Loss: 0.407\n",
      "Iteration: 167 \t--- Loss: 0.419\n",
      "Iteration: 168 \t--- Loss: 0.417\n",
      "Iteration: 169 \t--- Loss: 0.416\n",
      "Iteration: 170 \t--- Loss: 0.415\n",
      "Iteration: 171 \t--- Loss: 0.401\n",
      "Iteration: 172 \t--- Loss: 0.407\n",
      "Iteration: 173 \t--- Loss: 0.411\n",
      "Iteration: 174 \t--- Loss: 0.414\n",
      "Iteration: 175 \t--- Loss: 0.403\n",
      "Iteration: 176 \t--- Loss: 0.403\n",
      "Iteration: 177 \t--- Loss: 0.415\n",
      "Iteration: 178 \t--- Loss: 0.424\n",
      "Iteration: 179 \t--- Loss: 0.428\n",
      "Iteration: 180 \t--- Loss: 0.420\n",
      "Iteration: 181 \t--- Loss: 0.401\n",
      "Iteration: 182 \t--- Loss: 0.418\n",
      "Iteration: 183 \t--- Loss: 0.411\n",
      "Iteration: 184 \t--- Loss: 0.410\n",
      "Iteration: 185 \t--- Loss: 0.409\n",
      "Iteration: 186 \t--- Loss: 0.405\n",
      "Iteration: 187 \t--- Loss: 0.415\n",
      "Iteration: 188 \t--- Loss: 0.411\n",
      "Iteration: 189 \t--- Loss: 0.405\n",
      "Iteration: 190 \t--- Loss: 0.413\n",
      "Iteration: 191 \t--- Loss: 0.415\n",
      "Iteration: 192 \t--- Loss: 0.407\n",
      "Iteration: 193 \t--- Loss: 0.409\n",
      "Iteration: 194 \t--- Loss: 0.429\n",
      "Iteration: 195 \t--- Loss: 0.413\n",
      "Iteration: 196 \t--- Loss: 0.408\n",
      "Iteration: 197 \t--- Loss: 0.399\n",
      "Iteration: 198 \t--- Loss: 0.400\n",
      "Iteration: 199 \t--- Loss: 0.406\n",
      "Iteration: 200 \t--- Loss: 0.405\n",
      "Iteration: 201 \t--- Loss: 0.396\n",
      "Iteration: 202 \t--- Loss: 0.422\n",
      "Iteration: 203 \t--- Loss: 0.429\n",
      "Iteration: 204 \t--- Loss: 0.410\n",
      "Iteration: 205 \t--- Loss: 0.418\n",
      "Iteration: 206 \t--- Loss: 0.423\n",
      "Iteration: 207 \t--- Loss: 0.423\n",
      "Iteration: 208 \t--- Loss: 0.421\n",
      "Iteration: 209 \t--- Loss: 0.417\n",
      "Iteration: 210 \t--- Loss: 0.412\n",
      "Iteration: 211 \t--- Loss: 0.411\n",
      "Iteration: 212 \t--- Loss: 0.415\n",
      "Iteration: 213 \t--- Loss: 0.410\n",
      "Iteration: 214 \t--- Loss: 0.395\n",
      "Iteration: 215 \t--- Loss: 0.419\n",
      "Iteration: 216 \t--- Loss: 0.416\n",
      "Iteration: 217 \t--- Loss: 0.421\n",
      "Iteration: 218 \t--- Loss: 0.405\n",
      "Iteration: 219 \t--- Loss: 0.416\n",
      "Iteration: 220 \t--- Loss: 0.408\n",
      "Iteration: 221 \t--- Loss: 0.403\n",
      "Iteration: 222 \t--- Loss: 0.424\n",
      "Iteration: 223 \t--- Loss: 0.414\n",
      "Iteration: 224 \t--- Loss: 0.416\n",
      "Iteration: 225 \t--- Loss: 0.418\n",
      "Iteration: 226 \t--- Loss: 0.410\n",
      "Iteration: 227 \t--- Loss: 0.403\n",
      "Iteration: 228 \t--- Loss: 0.400\n",
      "Iteration: 229 \t--- Loss: 0.414\n",
      "Iteration: 230 \t--- Loss: 0.421\n",
      "Iteration: 231 \t--- Loss: 0.407\n",
      "Iteration: 232 \t--- Loss: 0.401\n",
      "Iteration: 233 \t--- Loss: 0.406\n",
      "Iteration: 234 \t--- Loss: 0.418\n",
      "Iteration: 235 \t--- Loss: 0.412\n",
      "Iteration: 236 \t--- Loss: 0.427\n",
      "Iteration: 237 \t--- Loss: 0.425\n",
      "Iteration: 238 \t--- Loss: 0.394\n",
      "Iteration: 239 \t--- Loss: 0.413\n",
      "Iteration: 240 \t--- Loss: 0.404\n",
      "Iteration: 241 \t--- Loss: 0.412\n",
      "Iteration: 242 \t--- Loss: 0.400\n",
      "Iteration: 243 \t--- Loss: 0.423\n",
      "Iteration: 244 \t--- Loss: 0.404\n",
      "Iteration: 245 \t--- Loss: 0.427\n",
      "Iteration: 246 \t--- Loss: 0.405\n",
      "Iteration: 247 \t--- Loss: 0.423\n",
      "Iteration: 248 \t--- Loss: 0.419\n",
      "Iteration: 249 \t--- Loss: 0.440\n",
      "Iteration: 250 \t--- Loss: 0.428\n",
      "Iteration: 251 \t--- Loss: 0.394\n",
      "Iteration: 252 \t--- Loss: 0.411\n",
      "Iteration: 253 \t--- Loss: 0.403\n",
      "Iteration: 254 \t--- Loss: 0.417\n",
      "Iteration: 255 \t--- Loss: 0.416\n",
      "Iteration: 256 \t--- Loss: 0.403\n",
      "Iteration: 257 \t--- Loss: 0.419\n",
      "Iteration: 258 \t--- Loss: 0.420\n",
      "Iteration: 259 \t--- Loss: 0.415Iteration: 0 \t--- Loss: 0.180\n",
      "Iteration: 1 \t--- Loss: 0.189\n",
      "Iteration: 2 \t--- Loss: 0.185\n",
      "Iteration: 3 \t--- Loss: 0.172\n",
      "Iteration: 4 \t--- Loss: 0.173\n",
      "Iteration: 5 \t--- Loss: 0.165\n",
      "Iteration: 6 \t--- Loss: 0.180\n",
      "Iteration: 7 \t--- Loss: 0.172\n",
      "Iteration: 8 \t--- Loss: 0.168\n",
      "Iteration: 9 \t--- Loss: 0.167\n",
      "Iteration: 10 \t--- Loss: 0.166\n",
      "Iteration: 11 \t--- Loss: 0.172\n",
      "Iteration: 12 \t--- Loss: 0.165\n",
      "Iteration: 13 \t--- Loss: 0.157\n",
      "Iteration: 14 \t--- Loss: 0.157\n",
      "Iteration: 15 \t--- Loss: 0.156\n",
      "Iteration: 16 \t--- Loss: 0.161\n",
      "Iteration: 17 \t--- Loss: 0.159\n",
      "Iteration: 18 \t--- Loss: 0.150\n",
      "Iteration: 19 \t--- Loss: 0.154\n",
      "Iteration: 20 \t--- Loss: 0.160\n",
      "Iteration: 21 \t--- Loss: 0.146\n",
      "Iteration: 22 \t--- Loss: 0.144\n",
      "Iteration: 23 \t--- Loss: 0.138\n",
      "Iteration: 24 \t--- Loss: 0.151\n",
      "Iteration: 25 \t--- Loss: 0.148\n",
      "Iteration: 26 \t--- Loss: 0.141\n",
      "Iteration: 27 \t--- Loss: 0.149\n",
      "Iteration: 28 \t--- Loss: 0.140\n",
      "Iteration: 29 \t--- Loss: 0.139\n",
      "Iteration: 30 \t--- Loss: 0.141\n",
      "Iteration: 31 \t--- Loss: 0.136\n",
      "Iteration: 32 \t--- Loss: 0.121\n",
      "Iteration: 33 \t--- Loss: 0.104\n",
      "Iteration: 34 \t--- Loss: 0.105\n",
      "Iteration: 35 \t--- Loss: 0.095\n",
      "Iteration: 36 \t--- Loss: 0.100\n",
      "Iteration: 37 \t--- Loss: 0.103\n",
      "Iteration: 38 \t--- Loss: 0.096\n",
      "Iteration: 39 \t--- Loss: 0.101\n",
      "Iteration: 40 \t--- Loss: 0.094\n",
      "Iteration: 41 \t--- Loss: 0.101\n",
      "Iteration: 42 \t--- Loss: 0.139\n",
      "Iteration: 43 \t--- Loss: 0.202\n",
      "Iteration: 44 \t--- Loss: 0.295\n",
      "Iteration: 45 \t--- Loss: 0.282\n",
      "Iteration: 46 \t--- Loss: 0.276\n",
      "Iteration: 47 \t--- Loss: 0.271\n",
      "Iteration: 48 \t--- Loss: 0.251\n",
      "Iteration: 49 \t--- Loss: 0.255\n",
      "Iteration: 50 \t--- Loss: 0.246\n",
      "Iteration: 51 \t--- Loss: 0.236\n",
      "Iteration: 52 \t--- Loss: 0.226\n",
      "Iteration: 53 \t--- Loss: 0.219\n",
      "Iteration: 54 \t--- Loss: 0.212\n",
      "Iteration: 55 \t--- Loss: 0.197\n",
      "Iteration: 56 \t--- Loss: 0.207\n",
      "Iteration: 57 \t--- Loss: 0.188\n",
      "Iteration: 58 \t--- Loss: 0.188\n",
      "Iteration: 59 \t--- Loss: 0.182\n",
      "Iteration: 60 \t--- Loss: 0.175\n",
      "Iteration: 61 \t--- Loss: 0.162\n",
      "Iteration: 62 \t--- Loss: 0.172\n",
      "Iteration: 63 \t--- Loss: 0.147\n",
      "Iteration: 64 \t--- Loss: 0.155\n",
      "Iteration: 65 \t--- Loss: 0.161\n",
      "Iteration: 66 \t--- Loss: 0.149\n",
      "Iteration: 67 \t--- Loss: 0.156\n",
      "Iteration: 68 \t--- Loss: 0.137\n",
      "Iteration: 69 \t--- Loss: 0.147\n",
      "Iteration: 70 \t--- Loss: 0.147\n",
      "Iteration: 71 \t--- Loss: 0.142\n",
      "Iteration: 72 \t--- Loss: 0.140\n",
      "Iteration: 73 \t--- Loss: 0.138\n",
      "Iteration: 74 \t--- Loss: 0.130\n",
      "Iteration: 75 \t--- Loss: 0.132\n",
      "Iteration: 76 \t--- Loss: 0.129\n",
      "Iteration: 77 \t--- Loss: 0.132\n",
      "Iteration: 78 \t--- Loss: 0.126\n",
      "Iteration: 79 \t--- Loss: 0.124\n",
      "Iteration: 80 \t--- Loss: 0.128\n",
      "Iteration: 81 \t--- Loss: 0.120\n",
      "Iteration: 82 \t--- Loss: 0.122\n",
      "Iteration: 83 \t--- Loss: 0.124\n",
      "Iteration: 84 \t--- Loss: 0.117\n",
      "Iteration: 85 \t--- Loss: 0.117\n",
      "Iteration: 86 \t--- Loss: 0.104\n",
      "Iteration: 87 \t--- Loss: 0.104\n",
      "Iteration: 88 \t--- Loss: 0.113\n",
      "Iteration: 89 \t--- Loss: 0.119\n",
      "Iteration: 90 \t--- Loss: 0.102\n",
      "Iteration: 91 \t--- Loss: 0.104\n",
      "Iteration: 92 \t--- Loss: 0.110\n",
      "Iteration: 93 \t--- Loss: 0.107\n",
      "Iteration: 94 \t--- Loss: 0.105\n",
      "Iteration: 95 \t--- Loss: 0.099\n",
      "Iteration: 96 \t--- Loss: 0.094\n",
      "Iteration: 97 \t--- Loss: 0.086\n",
      "Iteration: 98 \t--- Loss: 0.087\n",
      "Iteration: 99 \t--- Loss: 0.074\n",
      "Iteration: 100 \t--- Loss: 0.066\n",
      "Iteration: 101 \t--- Loss: 0.063\n",
      "Iteration: 102 \t--- Loss: 0.052\n",
      "Iteration: 103 \t--- Loss: 0.053\n",
      "Iteration: 104 \t--- Loss: 0.052\n",
      "Iteration: 105 \t--- Loss: 0.055\n",
      "Iteration: 106 \t--- Loss: 0.051\n",
      "Iteration: 107 \t--- Loss: 0.050\n",
      "Iteration: 108 \t--- Loss: 0.052\n",
      "Iteration: 109 \t--- Loss: 0.052\n",
      "Iteration: 110 \t--- Loss: 0.048\n",
      "Iteration: 111 \t--- Loss: 0.050\n",
      "Iteration: 112 \t--- Loss: 0.049\n",
      "Iteration: 113 \t--- Loss: 0.047\n",
      "Iteration: 114 \t--- Loss: 0.049\n",
      "Iteration: 115 \t--- Loss: 0.048\n",
      "Iteration: 116 \t--- Loss: 0.047\n",
      "Iteration: 117 \t--- Loss: 0.048\n",
      "Iteration: 118 \t--- Loss: 0.051\n",
      "Iteration: 119 \t--- Loss: 0.047\n",
      "Iteration: 120 \t--- Loss: 0.043\n",
      "Iteration: 121 \t--- Loss: 0.044\n",
      "Iteration: 122 \t--- Loss: 0.046\n",
      "Iteration: 123 \t--- Loss: 0.045\n",
      "Iteration: 124 \t--- Loss: 0.048\n",
      "Iteration: 125 \t--- Loss: 0.047\n",
      "Iteration: 126 \t--- Loss: 0.046\n",
      "Iteration: 127 \t--- Loss: 0.043\n",
      "Iteration: 128 \t--- Loss: 0.046\n",
      "Iteration: 129 \t--- Loss: 0.045\n",
      "Iteration: 130 \t--- Loss: 0.047\n",
      "Iteration: 131 \t--- Loss: 0.048\n",
      "Iteration: 132 \t--- Loss: 0.043\n",
      "Iteration: 133 \t--- Loss: 0.042\n",
      "Iteration: 134 \t--- Loss: 0.044\n",
      "Iteration: 135 \t--- Loss: 0.046\n",
      "Iteration: 136 \t--- Loss: 0.046\n",
      "Iteration: 137 \t--- Loss: 0.051\n",
      "Iteration: 138 \t--- Loss: 0.049\n",
      "Iteration: 139 \t--- Loss: 0.055\n",
      "Iteration: 140 \t--- Loss: 0.064\n",
      "Iteration: 141 \t--- Loss: 0.085\n",
      "Iteration: 142 \t--- Loss: 0.127\n",
      "Iteration: 143 \t--- Loss: 0.078\n",
      "Iteration: 144 \t--- Loss: 0.035\n",
      "Iteration: 145 \t--- Loss: 0.040\n",
      "Iteration: 146 \t--- Loss: 0.042\n",
      "Iteration: 147 \t--- Loss: 0.040\n",
      "Iteration: 148 \t--- Loss: 0.053\n",
      "Iteration: 149 \t--- Loss: 0.064\n",
      "Iteration: 150 \t--- Loss: 0.056\n",
      "Iteration: 151 \t--- Loss: 0.071\n",
      "Iteration: 152 \t--- Loss: 0.033\n",
      "Iteration: 153 \t--- Loss: 0.034\n",
      "Iteration: 154 \t--- Loss: 0.039\n",
      "Iteration: 155 \t--- Loss: 0.036\n",
      "Iteration: 156 \t--- Loss: 0.058\n",
      "Iteration: 157 \t--- Loss: 0.086\n",
      "Iteration: 158 \t--- Loss: 0.037\n",
      "Iteration: 159 \t--- Loss: 0.045\n",
      "Iteration: 160 \t--- Loss: 0.055\n",
      "Iteration: 161 \t--- Loss: 0.080\n",
      "Iteration: 162 \t--- Loss: 0.033\n",
      "Iteration: 163 \t--- Loss: 0.031\n",
      "Iteration: 164 \t--- Loss: 0.029\n",
      "Iteration: 165 \t--- Loss: 0.032\n",
      "Iteration: 166 \t--- Loss: 0.033\n",
      "Iteration: 167 \t--- Loss: 0.046\n",
      "Iteration: 168 \t--- Loss: 0.081\n",
      "Iteration: 169 \t--- Loss: 0.038\n",
      "Iteration: 170 \t--- Loss: 0.045\n",
      "Iteration: 171 \t--- Loss: 0.050\n",
      "Iteration: 172 \t--- Loss: 0.074\n",
      "Iteration: 173 \t--- Loss: 0.041\n",
      "Iteration: 174 \t--- Loss: 0.065\n",
      "Iteration: 175 \t--- Loss: 0.028\n",
      "Iteration: 176 \t--- Loss: 0.031\n",
      "Iteration: 177 \t--- Loss: 0.037\n",
      "Iteration: 178 \t--- Loss: 0.054\n",
      "Iteration: 179 \t--- Loss: 0.032\n",
      "Iteration: 180 \t--- Loss: 0.039\n",
      "Iteration: 181 \t--- Loss: 0.031\n",
      "Iteration: 182 \t--- Loss: 0.030\n",
      "Iteration: 183 \t--- Loss: 0.026\n",
      "Iteration: 184 \t--- Loss: 0.026\n",
      "Iteration: 185 \t--- Loss: 0.026\n",
      "Iteration: 186 \t--- Loss: 0.034\n",
      "Iteration: 187 \t--- Loss: 0.028\n",
      "Iteration: 188 \t--- Loss: 0.032\n",
      "Iteration: 189 \t--- Loss: 0.028\n",
      "Iteration: 190 \t--- Loss: 0.028\n",
      "Iteration: 191 \t--- Loss: 0.030\n",
      "Iteration: 192 \t--- Loss: 0.039\n",
      "Iteration: 193 \t--- Loss: 0.022\n",
      "Iteration: 194 \t--- Loss: 0.019\n",
      "Iteration: 195 \t--- Loss: 0.022\n",
      "Iteration: 196 \t--- Loss: 0.018\n",
      "Iteration: 197 \t--- Loss: 0.021\n",
      "Iteration: 198 \t--- Loss: 0.023\n",
      "Iteration: 199 \t--- Loss: 0.025\n",
      "Iteration: 200 \t--- Loss: 0.024\n",
      "Iteration: 201 \t--- Loss: 0.021\n",
      "Iteration: 202 \t--- Loss: 0.022\n",
      "Iteration: 203 \t--- Loss: 0.023\n",
      "Iteration: 204 \t--- Loss: 0.028\n",
      "Iteration: 205 \t--- Loss: 0.022\n",
      "Iteration: 206 \t--- Loss: 0.021\n",
      "Iteration: 207 \t--- Loss: 0.016\n",
      "Iteration: 208 \t--- Loss: 0.014\n",
      "Iteration: 209 \t--- Loss: 0.017\n",
      "Iteration: 210 \t--- Loss: 0.013\n",
      "Iteration: 211 \t--- Loss: 0.016\n",
      "Iteration: 212 \t--- Loss: 0.015\n",
      "Iteration: 213 \t--- Loss: 0.017\n",
      "Iteration: 214 \t--- Loss: 0.016\n",
      "Iteration: 215 \t--- Loss: 0.018\n",
      "Iteration: 216 \t--- Loss: 0.018\n",
      "Iteration: 217 \t--- Loss: 0.018\n",
      "Iteration: 218 \t--- Loss: 0.016\n",
      "Iteration: 219 \t--- Loss: 0.019\n",
      "Iteration: 220 \t--- Loss: 0.017\n",
      "Iteration: 221 \t--- Loss: 0.019\n",
      "Iteration: 222 \t--- Loss: 0.016\n",
      "Iteration: 223 \t--- Loss: 0.019\n",
      "Iteration: 224 \t--- Loss: 0.020\n",
      "Iteration: 225 \t--- Loss: 0.019\n",
      "Iteration: 226 \t--- Loss: 0.016\n",
      "Iteration: 227 \t--- Loss: 0.017\n",
      "Iteration: 228 \t--- Loss: 0.016\n",
      "Iteration: 229 \t--- Loss: 0.016\n",
      "Iteration: 230 \t--- Loss: 0.016\n",
      "Iteration: 231 \t--- Loss: 0.018\n",
      "Iteration: 232 \t--- Loss: 0.018\n",
      "Iteration: 233 \t--- Loss: 0.016\n",
      "Iteration: 234 \t--- Loss: 0.014\n",
      "Iteration: 235 \t--- Loss: 0.015\n",
      "Iteration: 236 \t--- Loss: 0.014\n",
      "Iteration: 237 \t--- Loss: 0.015\n",
      "Iteration: 238 \t--- Loss: 0.015\n",
      "Iteration: 239 \t--- Loss: 0.018\n",
      "Iteration: 240 \t--- Loss: 0.018\n",
      "Iteration: 241 \t--- Loss: 0.017\n",
      "Iteration: 242 \t--- Loss: 0.016\n",
      "Iteration: 243 \t--- Loss: 0.013\n",
      "Iteration: 244 \t--- Loss: 0.011\n",
      "Iteration: 245 \t--- Loss: 0.013\n",
      "Iteration: 246 \t--- Loss: 0.011\n",
      "Iteration: 247 \t--- Loss: 0.013\n",
      "Iteration: 248 \t--- Loss: 0.011\n",
      "Iteration: 249 \t--- Loss: 0.014\n",
      "Iteration: 250 \t--- Loss: 0.011\n",
      "Iteration: 251 \t--- Loss: 0.013\n",
      "Iteration: 252 \t--- Loss: 0.012\n",
      "Iteration: 253 \t--- Loss: 0.013\n",
      "Iteration: 254 \t--- Loss: 0.012\n",
      "Iteration: 255 \t--- Loss: 0.013\n",
      "Iteration: 256 \t--- Loss: 0.014\n",
      "Iteration: 257 \t--- Loss: 0.014\n",
      "Iteration: 258 \t--- Loss: 0.014\n",
      "Iteration: 259 \t--- Loss: 0.013"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:31<00:00, 91.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.420\n",
      "Iteration: 261 \t--- Loss: 0.419\n",
      "Iteration: 262 \t--- Loss: 0.412\n",
      "Iteration: 263 \t--- Loss: 0.417\n",
      "Iteration: 264 \t--- Loss: 0.402\n",
      "Iteration: 265 \t--- Loss: 0.422\n",
      "Iteration: 266 \t--- Loss: 0.403\n",
      "Iteration: 267 \t--- Loss: 0.399\n",
      "Iteration: 268 \t--- Loss: 0.416\n",
      "Iteration: 269 \t--- Loss: 0.413\n",
      "Iteration: 270 \t--- Loss: 0.424\n",
      "Iteration: 271 \t--- Loss: 0.423\n",
      "Iteration: 272 \t--- Loss: 0.416\n",
      "Iteration: 273 \t--- Loss: 0.400\n",
      "Iteration: 274 \t--- Loss: 0.412\n",
      "Iteration: 275 \t--- Loss: 0.411\n",
      "Iteration: 276 \t--- Loss: 0.410\n",
      "Iteration: 277 \t--- Loss: 0.418\n",
      "Iteration: 278 \t--- Loss: 0.399\n",
      "Iteration: 279 \t--- Loss: 0.412\n",
      "Iteration: 280 \t--- Loss: 0.413\n",
      "Iteration: 281 \t--- Loss: 0.407\n",
      "Iteration: 282 \t--- Loss: 0.400\n",
      "Iteration: 283 \t--- Loss: 0.415\n",
      "Iteration: 284 \t--- Loss: 0.410\n",
      "Iteration: 285 \t--- Loss: 0.404\n",
      "Iteration: 286 \t--- Loss: 0.419\n",
      "Iteration: 287 \t--- Loss: 0.418\n",
      "Iteration: 288 \t--- Loss: 0.408\n",
      "Iteration: 289 \t--- Loss: 0.415\n",
      "Iteration: 290 \t--- Loss: 0.425\n",
      "Iteration: 291 \t--- Loss: 0.401\n",
      "Iteration: 292 \t--- Loss: 0.401\n",
      "Iteration: 293 \t--- Loss: 0.414\n",
      "Iteration: 294 \t--- Loss: 0.411\n",
      "Iteration: 295 \t--- Loss: 0.417\n",
      "Iteration: 296 \t--- Loss: 0.407\n",
      "Iteration: 297 \t--- Loss: 0.421\n",
      "Iteration: 298 \t--- Loss: 0.423\n",
      "Iteration: 299 \t--- Loss: 0.408\n",
      "Iteration: 300 \t--- Loss: 0.411\n",
      "Iteration: 301 \t--- Loss: 0.419\n",
      "Iteration: 302 \t--- Loss: 0.410\n",
      "Iteration: 303 \t--- Loss: 0.421\n",
      "Iteration: 304 \t--- Loss: 0.408\n",
      "Iteration: 305 \t--- Loss: 0.407\n",
      "Iteration: 306 \t--- Loss: 0.414\n",
      "Iteration: 307 \t--- Loss: 0.415\n",
      "Iteration: 308 \t--- Loss: 0.405\n",
      "Iteration: 309 \t--- Loss: 0.426\n",
      "Iteration: 310 \t--- Loss: 0.428\n",
      "Iteration: 311 \t--- Loss: 0.397\n",
      "Iteration: 312 \t--- Loss: 0.414\n",
      "Iteration: 313 \t--- Loss: 0.426\n",
      "Iteration: 314 \t--- Loss: 0.416\n",
      "Iteration: 315 \t--- Loss: 0.407\n",
      "Iteration: 316 \t--- Loss: 0.398\n",
      "Iteration: 317 \t--- Loss: 0.416\n",
      "Iteration: 318 \t--- Loss: 0.419\n",
      "Iteration: 319 \t--- Loss: 0.421\n",
      "Iteration: 320 \t--- Loss: 0.400\n",
      "Iteration: 321 \t--- Loss: 0.429\n",
      "Iteration: 322 \t--- Loss: 0.432\n",
      "Iteration: 323 \t--- Loss: 0.408\n",
      "Iteration: 324 \t--- Loss: 0.405\n",
      "Iteration: 325 \t--- Loss: 0.401\n",
      "Iteration: 326 \t--- Loss: 0.400\n",
      "Iteration: 327 \t--- Loss: 0.424\n",
      "Iteration: 328 \t--- Loss: 0.412\n",
      "Iteration: 329 \t--- Loss: 0.424\n",
      "Iteration: 330 \t--- Loss: 0.415\n",
      "Iteration: 331 \t--- Loss: 0.419\n",
      "Iteration: 332 \t--- Loss: 0.409\n",
      "Iteration: 333 \t--- Loss: 0.418\n",
      "Iteration: 334 \t--- Loss: 0.399\n",
      "Iteration: 335 \t--- Loss: 0.410\n",
      "Iteration: 336 \t--- Loss: 0.417\n",
      "Iteration: 337 \t--- Loss: 0.415\n",
      "Iteration: 338 \t--- Loss: 0.421\n",
      "Iteration: 339 \t--- Loss: 0.409\n",
      "Iteration: 340 \t--- Loss: 0.423\n",
      "Iteration: 341 \t--- Loss: 0.410\n",
      "Iteration: 342 \t--- Loss: 0.408\n",
      "Iteration: 343 \t--- Loss: 0.426\n",
      "Iteration: 344 \t--- Loss: 0.413\n",
      "Iteration: 345 \t--- Loss: 0.409\n",
      "Iteration: 346 \t--- Loss: 0.401\n",
      "Iteration: 347 \t--- Loss: 0.408\n",
      "Iteration: 348 \t--- Loss: 0.405\n",
      "Iteration: 349 \t--- Loss: 0.419\n",
      "Iteration: 350 \t--- Loss: 0.418\n",
      "Iteration: 351 \t--- Loss: 0.413\n",
      "Iteration: 352 \t--- Loss: 0.412\n",
      "Iteration: 353 \t--- Loss: 0.407\n",
      "Iteration: 354 \t--- Loss: 0.416\n",
      "Iteration: 355 \t--- Loss: 0.423\n",
      "Iteration: 356 \t--- Loss: 0.414\n",
      "Iteration: 357 \t--- Loss: 0.415\n",
      "Iteration: 358 \t--- Loss: 0.413\n",
      "Iteration: 359 \t--- Loss: 0.414\n",
      "Iteration: 360 \t--- Loss: 0.406\n",
      "Iteration: 361 \t--- Loss: 0.415\n",
      "Iteration: 362 \t--- Loss: 0.397\n",
      "Iteration: 363 \t--- Loss: 0.418\n",
      "Iteration: 364 \t--- Loss: 0.403\n",
      "Iteration: 365 \t--- Loss: 0.397\n",
      "Iteration: 366 \t--- Loss: 0.409\n",
      "Iteration: 367 \t--- Loss: 0.395\n",
      "Iteration: 368 \t--- Loss: 0.417\n",
      "Iteration: 369 \t--- Loss: 0.431\n",
      "Iteration: 370 \t--- Loss: 0.412\n",
      "Iteration: 371 \t--- Loss: 0.402\n",
      "Iteration: 372 \t--- Loss: 0.418\n",
      "Iteration: 373 \t--- Loss: 0.411\n",
      "Iteration: 374 \t--- Loss: 0.404\n",
      "Iteration: 375 \t--- Loss: 0.396\n",
      "Iteration: 376 \t--- Loss: 0.406\n",
      "Iteration: 377 \t--- Loss: 0.409\n",
      "Iteration: 378 \t--- Loss: 0.414\n",
      "Iteration: 379 \t--- Loss: 0.421\n",
      "Iteration: 380 \t--- Loss: 0.420\n",
      "Iteration: 381 \t--- Loss: 0.411\n",
      "Iteration: 382 \t--- Loss: 0.413\n",
      "Iteration: 383 \t--- Loss: 0.415\n",
      "Iteration: 384 \t--- Loss: 0.422\n",
      "Iteration: 385 \t--- Loss: 0.415\n",
      "Iteration: 386 \t--- Loss: 0.421\n",
      "Iteration: 387 \t--- Loss: 0.412\n",
      "Iteration: 388 \t--- Loss: 0.427\n",
      "Iteration: 389 \t--- Loss: 0.407\n",
      "Iteration: 390 \t--- Loss: 0.405\n",
      "Iteration: 391 \t--- Loss: 0.419\n",
      "Iteration: 392 \t--- Loss: 0.426\n",
      "Iteration: 393 \t--- Loss: 0.414\n",
      "Iteration: 394 \t--- Loss: 0.412\n",
      "Iteration: 395 \t--- Loss: 0.412\n",
      "Iteration: 396 \t--- Loss: 0.406\n",
      "Iteration: 397 \t--- Loss: 0.420\n",
      "Iteration: 398 \t--- Loss: 0.414\n",
      "Iteration: 399 \t--- Loss: 0.413\n",
      "Iteration: 400 \t--- Loss: 0.408\n",
      "Iteration: 401 \t--- Loss: 0.421\n",
      "Iteration: 402 \t--- Loss: 0.403\n",
      "Iteration: 403 \t--- Loss: 0.403\n",
      "Iteration: 404 \t--- Loss: 0.408\n",
      "Iteration: 405 \t--- Loss: 0.409\n",
      "Iteration: 406 \t--- Loss: 0.405\n",
      "Iteration: 407 \t--- Loss: 0.420\n",
      "Iteration: 408 \t--- Loss: 0.397\n",
      "Iteration: 409 \t--- Loss: 0.417\n",
      "Iteration: 410 \t--- Loss: 0.421\n",
      "Iteration: 411 \t--- Loss: 0.421\n",
      "Iteration: 412 \t--- Loss: 0.429\n",
      "Iteration: 413 \t--- Loss: 0.427\n",
      "Iteration: 414 \t--- Loss: 0.413\n",
      "Iteration: 415 \t--- Loss: 0.421\n",
      "Iteration: 416 \t--- Loss: 0.414\n",
      "Iteration: 417 \t--- Loss: 0.421\n",
      "Iteration: 418 \t--- Loss: 0.411\n",
      "Iteration: 419 \t--- Loss: 0.415\n",
      "Iteration: 420 \t--- Loss: 0.423\n",
      "Iteration: 421 \t--- Loss: 0.415\n",
      "Iteration: 422 \t--- Loss: 0.421\n",
      "Iteration: 423 \t--- Loss: 0.397\n",
      "Iteration: 424 \t--- Loss: 0.416\n",
      "Iteration: 425 \t--- Loss: 0.401\n",
      "Iteration: 426 \t--- Loss: 0.407\n",
      "Iteration: 427 \t--- Loss: 0.407\n",
      "Iteration: 428 \t--- Loss: 0.416\n",
      "Iteration: 429 \t--- Loss: 0.418\n",
      "Iteration: 430 \t--- Loss: 0.405\n",
      "Iteration: 431 \t--- Loss: 0.417\n",
      "Iteration: 432 \t--- Loss: 0.412\n",
      "Iteration: 433 \t--- Loss: 0.419\n",
      "Iteration: 434 \t--- Loss: 0.419\n",
      "Iteration: 435 \t--- Loss: 0.403\n",
      "Iteration: 436 \t--- Loss: 0.405\n",
      "Iteration: 437 \t--- Loss: 0.406\n",
      "Iteration: 438 \t--- Loss: 0.412\n",
      "Iteration: 439 \t--- Loss: 0.401\n",
      "Iteration: 440 \t--- Loss: 0.403\n",
      "Iteration: 441 \t--- Loss: 0.412\n",
      "Iteration: 442 \t--- Loss: 0.404\n",
      "Iteration: 443 \t--- Loss: 0.422\n",
      "Iteration: 444 \t--- Loss: 0.419\n",
      "Iteration: 445 \t--- Loss: 0.424\n",
      "Iteration: 446 \t--- Loss: 0.398\n",
      "Iteration: 447 \t--- Loss: 0.415\n",
      "Iteration: 448 \t--- Loss: 0.399\n",
      "Iteration: 449 \t--- Loss: 0.425\n",
      "Iteration: 450 \t--- Loss: 0.419\n",
      "Iteration: 451 \t--- Loss: 0.399\n",
      "Iteration: 452 \t--- Loss: 0.413\n",
      "Iteration: 453 \t--- Loss: 0.411\n",
      "Iteration: 454 \t--- Loss: 0.420\n",
      "Iteration: 455 \t--- Loss: 0.420\n",
      "Iteration: 456 \t--- Loss: 0.409\n",
      "Iteration: 457 \t--- Loss: 0.413\n",
      "Iteration: 458 \t--- Loss: 0.428\n",
      "Iteration: 459 \t--- Loss: 0.413\n",
      "Iteration: 460 \t--- Loss: 0.402\n",
      "Iteration: 461 \t--- Loss: 0.403\n",
      "Iteration: 462 \t--- Loss: 0.406\n",
      "Iteration: 463 \t--- Loss: 0.398\n",
      "Iteration: 464 \t--- Loss: 0.426\n",
      "Iteration: 465 \t--- Loss: 0.411\n",
      "Iteration: 466 \t--- Loss: 0.411\n",
      "Iteration: 467 \t--- Loss: 0.412\n",
      "Iteration: 468 \t--- Loss: 0.411\n",
      "Iteration: 469 \t--- Loss: 0.425\n",
      "Iteration: 470 \t--- Loss: 0.411\n",
      "Iteration: 471 \t--- Loss: 0.405\n",
      "Iteration: 472 \t--- Loss: 0.428\n",
      "Iteration: 473 \t--- Loss: 0.409\n",
      "Iteration: 474 \t--- Loss: 0.415\n",
      "Iteration: 475 \t--- Loss: 0.400\n",
      "Iteration: 476 \t--- Loss: 0.411\n",
      "Iteration: 477 \t--- Loss: 0.418\n",
      "Iteration: 478 \t--- Loss: 0.419\n",
      "Iteration: 479 \t--- Loss: 0.420\n",
      "Iteration: 480 \t--- Loss: 0.414\n",
      "Iteration: 481 \t--- Loss: 0.411\n",
      "Iteration: 482 \t--- Loss: 0.414\n",
      "Iteration: 483 \t--- Loss: 0.417\n",
      "Iteration: 484 \t--- Loss: 0.420\n",
      "Iteration: 485 \t--- Loss: 0.405\n",
      "Iteration: 486 \t--- Loss: 0.419\n",
      "Iteration: 487 \t--- Loss: 0.414\n",
      "Iteration: 488 \t--- Loss: 0.419\n",
      "Iteration: 489 \t--- Loss: 0.412\n",
      "Iteration: 490 \t--- Loss: 0.402\n",
      "Iteration: 491 \t--- Loss: 0.419\n",
      "Iteration: 492 \t--- Loss: 0.423\n",
      "Iteration: 493 \t--- Loss: 0.414\n",
      "Iteration: 494 \t--- Loss: 0.410\n",
      "Iteration: 495 \t--- Loss: 0.422\n",
      "Iteration: 496 \t--- Loss: 0.413\n",
      "Iteration: 497 \t--- Loss: 0.415\n",
      "Iteration: 498 \t--- Loss: 0.408\n",
      "Iteration: 499 \t--- Loss: 0.407\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.57s/it][Parallel(n_jobs=5)]: Done  33 tasks      | elapsed: 18.4min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 1.744\n",
      "Iteration: 1 \t--- Loss: 1.642\n",
      "Iteration: 2 \t--- Loss: 1.476\n",
      "Iteration: 3 \t--- Loss: 1.452\n",
      "Iteration: 4 \t--- Loss: 1.316\n",
      "Iteration: 5 \t--- Loss: 1.324\n",
      "Iteration: 6 \t--- Loss: 1.299\n",
      "Iteration: 7 \t--- Loss: 1.313\n",
      "Iteration: 8 \t--- Loss: 1.224\n",
      "Iteration: 9 \t--- Loss: 1.211\n",
      "Iteration: 10 \t--- Loss: 1.222\n",
      "Iteration: 11 \t--- Loss: 1.199\n",
      "Iteration: 12 \t--- Loss: 1.239\n",
      "Iteration: 13 \t--- Loss: 1.144\n",
      "Iteration: 14 \t--- Loss: 1.132\n",
      "Iteration: 15 \t--- Loss: 1.178\n",
      "Iteration: 16 \t--- Loss: 1.187\n",
      "Iteration: 17 \t--- Loss: 1.189\n",
      "Iteration: 18 \t--- Loss: 1.205\n",
      "Iteration: 19 \t--- Loss: 1.129\n",
      "Iteration: 20 \t--- Loss: 1.168\n",
      "Iteration: 21 \t--- Loss: 1.205\n",
      "Iteration: 22 \t--- Loss: 1.206\n",
      "Iteration: 23 \t--- Loss: 1.167\n",
      "Iteration: 24 \t--- Loss: 1.142\n",
      "Iteration: 25 \t--- Loss: 1.171\n",
      "Iteration: 26 \t--- Loss: 1.150\n",
      "Iteration: 27 \t--- Loss: 1.154\n",
      "Iteration: 28 \t--- Loss: 1.178\n",
      "Iteration: 29 \t--- Loss: 1.161\n",
      "Iteration: 30 \t--- Loss: 1.204\n",
      "Iteration: 31 \t--- Loss: 1.207\n",
      "Iteration: 32 \t--- Loss: 1.143\n",
      "Iteration: 33 \t--- Loss: 1.195\n",
      "Iteration: 34 \t--- Loss: 1.158\n",
      "Iteration: 35 \t--- Loss: 1.156\n",
      "Iteration: 36 \t--- Loss: 1.166\n",
      "Iteration: 37 \t--- Loss: 1.156\n",
      "Iteration: 38 \t--- Loss: 1.197\n",
      "Iteration: 39 \t--- Loss: 1.202\n",
      "Iteration: 40 \t--- Loss: 1.147\n",
      "Iteration: 41 \t--- Loss: 1.156\n",
      "Iteration: 42 \t--- Loss: 1.144\n",
      "Iteration: 43 \t--- Loss: 1.146\n",
      "Iteration: 44 \t--- Loss: 1.179\n",
      "Iteration: 45 \t--- Loss: 1.153\n",
      "Iteration: 46 \t--- Loss: 1.168\n",
      "Iteration: 47 \t--- Loss: 1.214\n",
      "Iteration: 48 \t--- Loss: 1.170\n",
      "Iteration: 49 \t--- Loss: 1.185\n",
      "Iteration: 50 \t--- Loss: 1.144\n",
      "Iteration: 51 \t--- Loss: 1.176\n",
      "Iteration: 52 \t--- Loss: 1.183\n",
      "Iteration: 53 \t--- Loss: 1.149\n",
      "Iteration: 54 \t--- Loss: 1.164\n",
      "Iteration: 55 \t--- Loss: 1.191\n",
      "Iteration: 56 \t--- Loss: 1.134\n",
      "Iteration: 57 \t--- Loss: 1.147\n",
      "Iteration: 58 \t--- Loss: 1.180\n",
      "Iteration: 59 \t--- Loss: 1.161\n",
      "Iteration: 60 \t--- Loss: 1.118\n",
      "Iteration: 61 \t--- Loss: 1.139\n",
      "Iteration: 62 \t--- Loss: 1.184\n",
      "Iteration: 63 \t--- Loss: 1.177\n",
      "Iteration: 64 \t--- Loss: 1.177\n",
      "Iteration: 65 \t--- Loss: 1.154\n",
      "Iteration: 66 \t--- Loss: 1.109\n",
      "Iteration: 67 \t--- Loss: 1.160\n",
      "Iteration: 68 \t--- Loss: 1.180\n",
      "Iteration: 69 \t--- Loss: 1.159\n",
      "Iteration: 70 \t--- Loss: 1.181\n",
      "Iteration: 71 \t--- Loss: 1.177\n",
      "Iteration: 72 \t--- Loss: 1.188\n",
      "Iteration: 73 \t--- Loss: 1.193\n",
      "Iteration: 74 \t--- Loss: 1.171\n",
      "Iteration: 75 \t--- Loss: 1.153\n",
      "Iteration: 76 \t--- Loss: 1.176\n",
      "Iteration: 77 \t--- Loss: 1.151\n",
      "Iteration: 78 \t--- Loss: 1.170\n",
      "Iteration: 79 \t--- Loss: 1.194\n",
      "Iteration: 80 \t--- Loss: 1.170\n",
      "Iteration: 81 \t--- Loss: 1.201\n",
      "Iteration: 82 \t--- Loss: 1.209\n",
      "Iteration: 83 \t--- Loss: 1.162\n",
      "Iteration: 84 \t--- Loss: 1.175\n",
      "Iteration: 85 \t--- Loss: 1.165\n",
      "Iteration: 86 \t--- Loss: 1.156\n",
      "Iteration: 87 \t--- Loss: 1.170\n",
      "Iteration: 88 \t--- Loss: 1.186\n",
      "Iteration: 89 \t--- Loss: 1.171\n",
      "Iteration: 90 \t--- Loss: 1.212\n",
      "Iteration: 91 \t--- Loss: 1.187\n",
      "Iteration: 92 \t--- Loss: 1.152\n",
      "Iteration: 93 \t--- Loss: 1.211\n",
      "Iteration: 94 \t--- Loss: 1.199\n",
      "Iteration: 95 \t--- Loss: 1.163\n",
      "Iteration: 96 \t--- Loss: 1.168\n",
      "Iteration: 97 \t--- Loss: 1.185\n",
      "Iteration: 98 \t--- Loss: 1.162\n",
      "Iteration: 99 \t--- Loss: 1.171\n",
      "Iteration: 100 \t--- Loss: 1.174\n",
      "Iteration: 101 \t--- Loss: 1.168\n",
      "Iteration: 102 \t--- Loss: 1.164\n",
      "Iteration: 103 \t--- Loss: 1.174\n",
      "Iteration: 104 \t--- Loss: 1.141\n",
      "Iteration: 105 \t--- Loss: 1.160\n",
      "Iteration: 106 \t--- Loss: 1.109\n",
      "Iteration: 107 \t--- Loss: 1.172\n",
      "Iteration: 108 \t--- Loss: 1.208\n",
      "Iteration: 109 \t--- Loss: 1.164\n",
      "Iteration: 110 \t--- Loss: 1.164\n",
      "Iteration: 111 \t--- Loss: 1.143\n",
      "Iteration: 112 \t--- Loss: 1.175\n",
      "Iteration: 113 \t--- Loss: 1.175\n",
      "Iteration: 114 \t--- Loss: 1.158\n",
      "Iteration: 115 \t--- Loss: 1.148\n",
      "Iteration: 116 \t--- Loss: 1.163\n",
      "Iteration: 117 \t--- Loss: 1.221\n",
      "Iteration: 118 \t--- Loss: 1.136\n",
      "Iteration: 119 \t--- Loss: 1.178\n",
      "Iteration: 120 \t--- Loss: 1.157\n",
      "Iteration: 121 \t--- Loss: 1.208\n",
      "Iteration: 122 \t--- Loss: 1.144\n",
      "Iteration: 123 \t--- Loss: 1.181\n",
      "Iteration: 124 \t--- Loss: 1.140\n",
      "Iteration: 125 \t--- Loss: 1.149\n",
      "Iteration: 126 \t--- Loss: 1.171\n",
      "Iteration: 127 \t--- Loss: 1.164\n",
      "Iteration: 128 \t--- Loss: 1.140\n",
      "Iteration: 129 \t--- Loss: 1.156\n",
      "Iteration: 130 \t--- Loss: 1.188\n",
      "Iteration: 131 \t--- Loss: 1.210\n",
      "Iteration: 132 \t--- Loss: 1.227\n",
      "Iteration: 133 \t--- Loss: 1.173\n",
      "Iteration: 134 \t--- Loss: 1.181\n",
      "Iteration: 135 \t--- Loss: 1.133\n",
      "Iteration: 136 \t--- Loss: 1.166\n",
      "Iteration: 137 \t--- Loss: 1.149\n",
      "Iteration: 138 \t--- Loss: 1.172\n",
      "Iteration: 139 \t--- Loss: 1.180\n",
      "Iteration: 140 \t--- Loss: 1.147\n",
      "Iteration: 141 \t--- Loss: 1.182\n",
      "Iteration: 142 \t--- Loss: 1.164\n",
      "Iteration: 143 \t--- Loss: 1.179\n",
      "Iteration: 144 \t--- Loss: 1.158\n",
      "Iteration: 145 \t--- Loss: 1.177\n",
      "Iteration: 146 \t--- Loss: 1.175\n",
      "Iteration: 147 \t--- Loss: 1.138\n",
      "Iteration: 148 \t--- Loss: 1.174\n",
      "Iteration: 149 \t--- Loss: 1.166\n",
      "Iteration: 150 \t--- Loss: 1.162\n",
      "Iteration: 151 \t--- Loss: 1.185\n",
      "Iteration: 152 \t--- Loss: 1.186\n",
      "Iteration: 153 \t--- Loss: 1.154\n",
      "Iteration: 154 \t--- Loss: 1.161\n",
      "Iteration: 155 \t--- Loss: 1.211\n",
      "Iteration: 156 \t--- Loss: 1.159\n",
      "Iteration: 157 \t--- Loss: 1.188\n",
      "Iteration: 158 \t--- Loss: 1.194\n",
      "Iteration: 159 \t--- Loss: 1.199\n",
      "Iteration: 160 \t--- Loss: 1.180\n",
      "Iteration: 161 \t--- Loss: 1.142\n",
      "Iteration: 162 \t--- Loss: 1.102\n",
      "Iteration: 163 \t--- Loss: 1.144\n",
      "Iteration: 164 \t--- Loss: 1.185\n",
      "Iteration: 165 \t--- Loss: 1.202\n",
      "Iteration: 166 \t--- Loss: 1.133\n",
      "Iteration: 167 \t--- Loss: 1.163\n",
      "Iteration: 168 \t--- Loss: 1.162\n",
      "Iteration: 169 \t--- Loss: 1.112\n",
      "Iteration: 170 \t--- Loss: 1.182\n",
      "Iteration: 171 \t--- Loss: 1.166\n",
      "Iteration: 172 \t--- Loss: 1.162\n",
      "Iteration: 173 \t--- Loss: 1.145\n",
      "Iteration: 174 \t--- Loss: 1.143\n",
      "Iteration: 175 \t--- Loss: 1.172\n",
      "Iteration: 176 \t--- Loss: 1.122\n",
      "Iteration: 177 \t--- Loss: 1.159\n",
      "Iteration: 178 \t--- Loss: 1.167\n",
      "Iteration: 179 \t--- Loss: 1.179\n",
      "Iteration: 180 \t--- Loss: 1.191\n",
      "Iteration: 181 \t--- Loss: 1.163\n",
      "Iteration: 182 \t--- Loss: 1.170\n",
      "Iteration: 183 \t--- Loss: 1.159\n",
      "Iteration: 184 \t--- Loss: 1.165\n",
      "Iteration: 185 \t--- Loss: 1.150\n",
      "Iteration: 186 \t--- Loss: 1.190\n",
      "Iteration: 187 \t--- Loss: 1.122\n",
      "Iteration: 188 \t--- Loss: 1.184\n",
      "Iteration: 189 \t--- Loss: 1.183\n",
      "Iteration: 190 \t--- Loss: 1.152\n",
      "Iteration: 191 \t--- Loss: 1.152\n",
      "Iteration: 192 \t--- Loss: 1.169\n",
      "Iteration: 193 \t--- Loss: 1.131\n",
      "Iteration: 194 \t--- Loss: 1.157\n",
      "Iteration: 195 \t--- Loss: 1.187\n",
      "Iteration: 196 \t--- Loss: 1.159\n",
      "Iteration: 197 \t--- Loss: 1.189\n",
      "Iteration: 198 \t--- Loss: 1.180\n",
      "Iteration: 199 \t--- Loss: 1.131\n",
      "Iteration: 200 \t--- Loss: 1.123\n",
      "Iteration: 201 \t--- Loss: 1.191\n",
      "Iteration: 202 \t--- Loss: 1.154\n",
      "Iteration: 203 \t--- Loss: 1.189\n",
      "Iteration: 204 \t--- Loss: 1.195\n",
      "Iteration: 205 \t--- Loss: 1.144\n",
      "Iteration: 206 \t--- Loss: 1.130\n",
      "Iteration: 207 \t--- Loss: 1.159\n",
      "Iteration: 208 \t--- Loss: 1.197\n",
      "Iteration: 209 \t--- Loss: 1.167\n",
      "Iteration: 210 \t--- Loss: 1.144\n",
      "Iteration: 211 \t--- Loss: 1.148\n",
      "Iteration: 212 \t--- Loss: 1.190\n",
      "Iteration: 213 \t--- Loss: 1.178\n",
      "Iteration: 214 \t--- Loss: 1.157\n",
      "Iteration: 215 \t--- Loss: 1.171\n",
      "Iteration: 216 \t--- Loss: 1.160\n",
      "Iteration: 217 \t--- Loss: 1.137\n",
      "Iteration: 218 \t--- Loss: 1.163\n",
      "Iteration: 219 \t--- Loss: 1.179\n",
      "Iteration: 220 \t--- Loss: 1.183\n",
      "Iteration: 221 \t--- Loss: 1.164\n",
      "Iteration: 222 \t--- Loss: 1.149\n",
      "Iteration: 223 \t--- Loss: 1.189\n",
      "Iteration: 224 \t--- Loss: 1.197\n",
      "Iteration: 225 \t--- Loss: 1.179\n",
      "Iteration: 226 \t--- Loss: 1.192\n",
      "Iteration: 227 \t--- Loss: 1.158\n",
      "Iteration: 228 \t--- Loss: 1.157\n",
      "Iteration: 229 \t--- Loss: 1.182\n",
      "Iteration: 230 \t--- Loss: 1.169\n",
      "Iteration: 231 \t--- Loss: 1.132\n",
      "Iteration: 232 \t--- Loss: 1.207\n",
      "Iteration: 233 \t--- Loss: 1.165\n",
      "Iteration: 234 \t--- Loss: 1.194\n",
      "Iteration: 235 \t--- Loss: 1.172\n",
      "Iteration: 236 \t--- Loss: 1.177\n",
      "Iteration: 237 \t--- Loss: 1.173\n",
      "Iteration: 238 \t--- Loss: 1.164\n",
      "Iteration: 239 \t--- Loss: 1.201\n",
      "Iteration: 240 \t--- Loss: 1.169\n",
      "Iteration: 241 \t--- Loss: 1.191\n",
      "Iteration: 242 \t--- Loss: 1.206\n",
      "Iteration: 243 \t--- Loss: 1.193\n",
      "Iteration: 244 \t--- Loss: 1.154\n",
      "Iteration: 245 \t--- Loss: 1.177\n",
      "Iteration: 246 \t--- Loss: 1.191\n",
      "Iteration: 247 \t--- Loss: 1.226\n",
      "Iteration: 248 \t--- Loss: 1.151\n",
      "Iteration: 249 \t--- Loss: 1.179\n",
      "Iteration: 250 \t--- Loss: 1.176\n",
      "Iteration: 251 \t--- Loss: 1.174\n",
      "Iteration: 252 \t--- Loss: 1.187\n",
      "Iteration: 253 \t--- Loss: 1.187\n",
      "Iteration: 254 \t--- Loss: 1.200\n",
      "Iteration: 255 \t--- Loss: 1.187\n",
      "Iteration: 256 \t--- Loss: 1.169\n",
      "Iteration: 257 \t--- Loss: 1.155\n",
      "Iteration: 258 \t--- Loss: 1.160\n",
      "Iteration: 259 \t--- Loss: 1.202Iteration: 0 \t--- Loss: 0.203\n",
      "Iteration: 1 \t--- Loss: 0.198\n",
      "Iteration: 2 \t--- Loss: 0.225\n",
      "Iteration: 3 \t--- Loss: 0.199\n",
      "Iteration: 4 \t--- Loss: 0.188\n",
      "Iteration: 5 \t--- Loss: 0.204\n",
      "Iteration: 6 \t--- Loss: 0.213\n",
      "Iteration: 7 \t--- Loss: 0.194\n",
      "Iteration: 8 \t--- Loss: 0.214\n",
      "Iteration: 9 \t--- Loss: 0.194\n",
      "Iteration: 10 \t--- Loss: 0.174\n",
      "Iteration: 11 \t--- Loss: 0.171\n",
      "Iteration: 12 \t--- Loss: 0.188\n",
      "Iteration: 13 \t--- Loss: 0.189\n",
      "Iteration: 14 \t--- Loss: 0.194\n",
      "Iteration: 15 \t--- Loss: 0.193\n",
      "Iteration: 16 \t--- Loss: 0.193\n",
      "Iteration: 17 \t--- Loss: 0.172\n",
      "Iteration: 18 \t--- Loss: 0.191\n",
      "Iteration: 19 \t--- Loss: 0.180\n",
      "Iteration: 20 \t--- Loss: 0.187\n",
      "Iteration: 21 \t--- Loss: 0.171\n",
      "Iteration: 22 \t--- Loss: 0.160\n",
      "Iteration: 23 \t--- Loss: 0.197\n",
      "Iteration: 24 \t--- Loss: 0.186\n",
      "Iteration: 25 \t--- Loss: 0.170\n",
      "Iteration: 26 \t--- Loss: 0.184\n",
      "Iteration: 27 \t--- Loss: 0.171\n",
      "Iteration: 28 \t--- Loss: 0.186\n",
      "Iteration: 29 \t--- Loss: 0.179\n",
      "Iteration: 30 \t--- Loss: 0.171\n",
      "Iteration: 31 \t--- Loss: 0.172\n",
      "Iteration: 32 \t--- Loss: 0.189\n",
      "Iteration: 33 \t--- Loss: 0.178\n",
      "Iteration: 34 \t--- Loss: 0.165\n",
      "Iteration: 35 \t--- Loss: 0.140\n",
      "Iteration: 36 \t--- Loss: 0.136\n",
      "Iteration: 37 \t--- Loss: 0.125\n",
      "Iteration: 38 \t--- Loss: 0.135\n",
      "Iteration: 39 \t--- Loss: 0.128\n",
      "Iteration: 40 \t--- Loss: 0.150\n",
      "Iteration: 41 \t--- Loss: 0.146\n",
      "Iteration: 42 \t--- Loss: 0.139\n",
      "Iteration: 43 \t--- Loss: 0.133\n",
      "Iteration: 44 \t--- Loss: 0.148\n",
      "Iteration: 45 \t--- Loss: 0.195\n",
      "Iteration: 46 \t--- Loss: 0.126\n",
      "Iteration: 47 \t--- Loss: 0.121\n",
      "Iteration: 48 \t--- Loss: 0.130\n",
      "Iteration: 49 \t--- Loss: 0.134\n",
      "Iteration: 50 \t--- Loss: 0.231\n",
      "Iteration: 51 \t--- Loss: 0.219\n",
      "Iteration: 52 \t--- Loss: 0.203\n",
      "Iteration: 53 \t--- Loss: 0.223\n",
      "Iteration: 54 \t--- Loss: 0.203\n",
      "Iteration: 55 \t--- Loss: 0.225\n",
      "Iteration: 56 \t--- Loss: 0.193\n",
      "Iteration: 57 \t--- Loss: 0.177\n",
      "Iteration: 58 \t--- Loss: 0.159\n",
      "Iteration: 59 \t--- Loss: 0.182\n",
      "Iteration: 60 \t--- Loss: 0.188\n",
      "Iteration: 61 \t--- Loss: 0.199\n",
      "Iteration: 62 \t--- Loss: 0.167\n",
      "Iteration: 63 \t--- Loss: 0.179\n",
      "Iteration: 64 \t--- Loss: 0.191\n",
      "Iteration: 65 \t--- Loss: 0.190\n",
      "Iteration: 66 \t--- Loss: 0.182\n",
      "Iteration: 67 \t--- Loss: 0.171\n",
      "Iteration: 68 \t--- Loss: 0.176\n",
      "Iteration: 69 \t--- Loss: 0.182\n",
      "Iteration: 70 \t--- Loss: 0.174\n",
      "Iteration: 71 \t--- Loss: 0.172\n",
      "Iteration: 72 \t--- Loss: 0.169\n",
      "Iteration: 73 \t--- Loss: 0.163\n",
      "Iteration: 74 \t--- Loss: 0.157\n",
      "Iteration: 75 \t--- Loss: 0.161\n",
      "Iteration: 76 \t--- Loss: 0.164\n",
      "Iteration: 77 \t--- Loss: 0.173\n",
      "Iteration: 78 \t--- Loss: 0.176\n",
      "Iteration: 79 \t--- Loss: 0.167\n",
      "Iteration: 80 \t--- Loss: 0.175\n",
      "Iteration: 81 \t--- Loss: 0.158\n",
      "Iteration: 82 \t--- Loss: 0.162\n",
      "Iteration: 83 \t--- Loss: 0.165\n",
      "Iteration: 84 \t--- Loss: 0.155\n",
      "Iteration: 85 \t--- Loss: 0.164\n",
      "Iteration: 86 \t--- Loss: 0.164\n",
      "Iteration: 87 \t--- Loss: 0.162\n",
      "Iteration: 88 \t--- Loss: 0.180\n",
      "Iteration: 89 \t--- Loss: 0.159\n",
      "Iteration: 90 \t--- Loss: 0.161\n",
      "Iteration: 91 \t--- Loss: 0.148\n",
      "Iteration: 92 \t--- Loss: 0.151\n",
      "Iteration: 93 \t--- Loss: 0.147\n",
      "Iteration: 94 \t--- Loss: 0.140\n",
      "Iteration: 95 \t--- Loss: 0.131\n",
      "Iteration: 96 \t--- Loss: 0.121\n",
      "Iteration: 97 \t--- Loss: 0.109\n",
      "Iteration: 98 \t--- Loss: 0.099\n",
      "Iteration: 99 \t--- Loss: 0.098\n",
      "Iteration: 100 \t--- Loss: 0.110\n",
      "Iteration: 101 \t--- Loss: 0.094\n",
      "Iteration: 102 \t--- Loss: 0.107\n",
      "Iteration: 103 \t--- Loss: 0.094\n",
      "Iteration: 104 \t--- Loss: 0.096\n",
      "Iteration: 105 \t--- Loss: 0.099\n",
      "Iteration: 106 \t--- Loss: 0.098\n",
      "Iteration: 107 \t--- Loss: 0.095\n",
      "Iteration: 108 \t--- Loss: 0.090\n",
      "Iteration: 109 \t--- Loss: 0.098\n",
      "Iteration: 110 \t--- Loss: 0.086\n",
      "Iteration: 111 \t--- Loss: 0.097\n",
      "Iteration: 112 \t--- Loss: 0.091\n",
      "Iteration: 113 \t--- Loss: 0.097\n",
      "Iteration: 114 \t--- Loss: 0.092\n",
      "Iteration: 115 \t--- Loss: 0.103\n",
      "Iteration: 116 \t--- Loss: 0.105\n",
      "Iteration: 117 \t--- Loss: 0.115\n",
      "Iteration: 118 \t--- Loss: 0.097\n",
      "Iteration: 119 \t--- Loss: 0.109\n",
      "Iteration: 120 \t--- Loss: 0.116\n",
      "Iteration: 121 \t--- Loss: 0.097\n",
      "Iteration: 122 \t--- Loss: 0.108\n",
      "Iteration: 123 \t--- Loss: 0.140\n",
      "Iteration: 124 \t--- Loss: 0.178\n",
      "Iteration: 125 \t--- Loss: 0.115\n",
      "Iteration: 126 \t--- Loss: 0.093\n",
      "Iteration: 127 \t--- Loss: 0.089\n",
      "Iteration: 128 \t--- Loss: 0.098\n",
      "Iteration: 129 \t--- Loss: 0.084\n",
      "Iteration: 130 \t--- Loss: 0.083\n",
      "Iteration: 131 \t--- Loss: 0.090\n",
      "Iteration: 132 \t--- Loss: 0.093\n",
      "Iteration: 133 \t--- Loss: 0.095\n",
      "Iteration: 134 \t--- Loss: 0.123\n",
      "Iteration: 135 \t--- Loss: 0.110\n",
      "Iteration: 136 \t--- Loss: 0.156\n",
      "Iteration: 137 \t--- Loss: 0.086\n",
      "Iteration: 138 \t--- Loss: 0.083\n",
      "Iteration: 139 \t--- Loss: 0.089\n",
      "Iteration: 140 \t--- Loss: 0.095\n",
      "Iteration: 141 \t--- Loss: 0.108\n",
      "Iteration: 142 \t--- Loss: 0.166\n",
      "Iteration: 143 \t--- Loss: 0.084\n",
      "Iteration: 144 \t--- Loss: 0.084\n",
      "Iteration: 145 \t--- Loss: 0.079\n",
      "Iteration: 146 \t--- Loss: 0.082\n",
      "Iteration: 147 \t--- Loss: 0.096\n",
      "Iteration: 148 \t--- Loss: 0.106\n",
      "Iteration: 149 \t--- Loss: 0.140\n",
      "Iteration: 150 \t--- Loss: 0.078\n",
      "Iteration: 151 \t--- Loss: 0.079\n",
      "Iteration: 152 \t--- Loss: 0.079\n",
      "Iteration: 153 \t--- Loss: 0.135\n",
      "Iteration: 154 \t--- Loss: 0.172\n",
      "Iteration: 155 \t--- Loss: 0.154\n",
      "Iteration: 156 \t--- Loss: 0.099\n",
      "Iteration: 157 \t--- Loss: 0.075\n",
      "Iteration: 158 \t--- Loss: 0.084\n",
      "Iteration: 159 \t--- Loss: 0.069\n",
      "Iteration: 160 \t--- Loss: 0.072\n",
      "Iteration: 161 \t--- Loss: 0.070\n",
      "Iteration: 162 \t--- Loss: 0.064\n",
      "Iteration: 163 \t--- Loss: 0.076\n",
      "Iteration: 164 \t--- Loss: 0.069\n",
      "Iteration: 165 \t--- Loss: 0.077\n",
      "Iteration: 166 \t--- Loss: 0.083\n",
      "Iteration: 167 \t--- Loss: 0.077\n",
      "Iteration: 168 \t--- Loss: 0.075\n",
      "Iteration: 169 \t--- Loss: 0.072\n",
      "Iteration: 170 \t--- Loss: 0.071\n",
      "Iteration: 171 \t--- Loss: 0.067\n",
      "Iteration: 172 \t--- Loss: 0.072\n",
      "Iteration: 173 \t--- Loss: 0.077\n",
      "Iteration: 174 \t--- Loss: 0.072\n",
      "Iteration: 175 \t--- Loss: 0.070\n",
      "Iteration: 176 \t--- Loss: 0.072\n",
      "Iteration: 177 \t--- Loss: 0.081\n",
      "Iteration: 178 \t--- Loss: 0.087\n",
      "Iteration: 179 \t--- Loss: 0.063\n",
      "Iteration: 180 \t--- Loss: 0.065\n",
      "Iteration: 181 \t--- Loss: 0.071\n",
      "Iteration: 182 \t--- Loss: 0.068\n",
      "Iteration: 183 \t--- Loss: 0.068\n",
      "Iteration: 184 \t--- Loss: 0.068\n",
      "Iteration: 185 \t--- Loss: 0.071\n",
      "Iteration: 186 \t--- Loss: 0.075\n",
      "Iteration: 187 \t--- Loss: 0.077\n",
      "Iteration: 188 \t--- Loss: 0.077\n",
      "Iteration: 189 \t--- Loss: 0.066\n",
      "Iteration: 190 \t--- Loss: 0.069\n",
      "Iteration: 191 \t--- Loss: 0.072\n",
      "Iteration: 192 \t--- Loss: 0.067\n",
      "Iteration: 193 \t--- Loss: 0.063\n",
      "Iteration: 194 \t--- Loss: 0.063\n",
      "Iteration: 195 \t--- Loss: 0.059\n",
      "Iteration: 196 \t--- Loss: 0.060\n",
      "Iteration: 197 \t--- Loss: 0.056\n",
      "Iteration: 198 \t--- Loss: 0.055\n",
      "Iteration: 199 \t--- Loss: 0.061\n",
      "Iteration: 200 \t--- Loss: 0.057\n",
      "Iteration: 201 \t--- Loss: 0.065\n",
      "Iteration: 202 \t--- Loss: 0.058\n",
      "Iteration: 203 \t--- Loss: 0.059\n",
      "Iteration: 204 \t--- Loss: 0.071\n",
      "Iteration: 205 \t--- Loss: 0.065\n",
      "Iteration: 206 \t--- Loss: 0.071\n",
      "Iteration: 207 \t--- Loss: 0.067\n",
      "Iteration: 208 \t--- Loss: 0.062\n",
      "Iteration: 209 \t--- Loss: 0.058\n",
      "Iteration: 210 \t--- Loss: 0.068\n",
      "Iteration: 211 \t--- Loss: 0.064\n",
      "Iteration: 212 \t--- Loss: 0.062\n",
      "Iteration: 213 \t--- Loss: 0.063\n",
      "Iteration: 214 \t--- Loss: 0.057\n",
      "Iteration: 215 \t--- Loss: 0.058\n",
      "Iteration: 216 \t--- Loss: 0.054\n",
      "Iteration: 217 \t--- Loss: 0.060\n",
      "Iteration: 218 \t--- Loss: 0.057\n",
      "Iteration: 219 \t--- Loss: 0.060\n",
      "Iteration: 220 \t--- Loss: 0.060\n",
      "Iteration: 221 \t--- Loss: 0.058\n",
      "Iteration: 222 \t--- Loss: 0.062\n",
      "Iteration: 223 \t--- Loss: 0.058\n",
      "Iteration: 224 \t--- Loss: 0.057\n",
      "Iteration: 225 \t--- Loss: 0.058\n",
      "Iteration: 226 \t--- Loss: 0.064\n",
      "Iteration: 227 \t--- Loss: 0.055\n",
      "Iteration: 228 \t--- Loss: 0.059\n",
      "Iteration: 229 \t--- Loss: 0.057\n",
      "Iteration: 230 \t--- Loss: 0.067\n",
      "Iteration: 231 \t--- Loss: 0.069\n",
      "Iteration: 232 \t--- Loss: 0.071\n",
      "Iteration: 233 \t--- Loss: 0.078\n",
      "Iteration: 234 \t--- Loss: 0.076\n",
      "Iteration: 235 \t--- Loss: 0.084\n",
      "Iteration: 236 \t--- Loss: 0.064\n",
      "Iteration: 237 \t--- Loss: 0.070\n",
      "Iteration: 238 \t--- Loss: 0.065\n",
      "Iteration: 239 \t--- Loss: 0.069\n",
      "Iteration: 240 \t--- Loss: 0.064\n",
      "Iteration: 241 \t--- Loss: 0.059\n",
      "Iteration: 242 \t--- Loss: 0.061\n",
      "Iteration: 243 \t--- Loss: 0.059\n",
      "Iteration: 244 \t--- Loss: 0.052\n",
      "Iteration: 245 \t--- Loss: 0.053\n",
      "Iteration: 246 \t--- Loss: 0.056\n",
      "Iteration: 247 \t--- Loss: 0.055\n",
      "Iteration: 248 \t--- Loss: 0.057\n",
      "Iteration: 249 \t--- Loss: 0.055\n",
      "Iteration: 250 \t--- Loss: 0.052\n",
      "Iteration: 251 \t--- Loss: 0.053\n",
      "Iteration: 252 \t--- Loss: 0.050\n",
      "Iteration: 253 \t--- Loss: 0.052\n",
      "Iteration: 254 \t--- Loss: 0.059\n",
      "Iteration: 255 \t--- Loss: 0.055\n",
      "Iteration: 256 \t--- Loss: 0.050\n",
      "Iteration: 257 \t--- Loss: 0.055\n",
      "Iteration: 258 \t--- Loss: 0.053\n",
      "Iteration: 259 \t--- Loss: 0.057"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:27<00:00, 87.57s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 1.193\n",
      "Iteration: 261 \t--- Loss: 1.172\n",
      "Iteration: 262 \t--- Loss: 1.140\n",
      "Iteration: 263 \t--- Loss: 1.142\n",
      "Iteration: 264 \t--- Loss: 1.122\n",
      "Iteration: 265 \t--- Loss: 1.212\n",
      "Iteration: 266 \t--- Loss: 1.137\n",
      "Iteration: 267 \t--- Loss: 1.169\n",
      "Iteration: 268 \t--- Loss: 1.216\n",
      "Iteration: 269 \t--- Loss: 1.183\n",
      "Iteration: 270 \t--- Loss: 1.173\n",
      "Iteration: 271 \t--- Loss: 1.179\n",
      "Iteration: 272 \t--- Loss: 1.152\n",
      "Iteration: 273 \t--- Loss: 1.185\n",
      "Iteration: 274 \t--- Loss: 1.179\n",
      "Iteration: 275 \t--- Loss: 1.193\n",
      "Iteration: 276 \t--- Loss: 1.149\n",
      "Iteration: 277 \t--- Loss: 1.153\n",
      "Iteration: 278 \t--- Loss: 1.133\n",
      "Iteration: 279 \t--- Loss: 1.152\n",
      "Iteration: 280 \t--- Loss: 1.162\n",
      "Iteration: 281 \t--- Loss: 1.182\n",
      "Iteration: 282 \t--- Loss: 1.214\n",
      "Iteration: 283 \t--- Loss: 1.191\n",
      "Iteration: 284 \t--- Loss: 1.182\n",
      "Iteration: 285 \t--- Loss: 1.158\n",
      "Iteration: 286 \t--- Loss: 1.181\n",
      "Iteration: 287 \t--- Loss: 1.186\n",
      "Iteration: 288 \t--- Loss: 1.143\n",
      "Iteration: 289 \t--- Loss: 1.191\n",
      "Iteration: 290 \t--- Loss: 1.148\n",
      "Iteration: 291 \t--- Loss: 1.130\n",
      "Iteration: 292 \t--- Loss: 1.169\n",
      "Iteration: 293 \t--- Loss: 1.214\n",
      "Iteration: 294 \t--- Loss: 1.115\n",
      "Iteration: 295 \t--- Loss: 1.201\n",
      "Iteration: 296 \t--- Loss: 1.172\n",
      "Iteration: 297 \t--- Loss: 1.190\n",
      "Iteration: 298 \t--- Loss: 1.180\n",
      "Iteration: 299 \t--- Loss: 1.176\n",
      "Iteration: 300 \t--- Loss: 1.175\n",
      "Iteration: 301 \t--- Loss: 1.156\n",
      "Iteration: 302 \t--- Loss: 1.178\n",
      "Iteration: 303 \t--- Loss: 1.174\n",
      "Iteration: 304 \t--- Loss: 1.178\n",
      "Iteration: 305 \t--- Loss: 1.156\n",
      "Iteration: 306 \t--- Loss: 1.193\n",
      "Iteration: 307 \t--- Loss: 1.199\n",
      "Iteration: 308 \t--- Loss: 1.202\n",
      "Iteration: 309 \t--- Loss: 1.136\n",
      "Iteration: 310 \t--- Loss: 1.163\n",
      "Iteration: 311 \t--- Loss: 1.183\n",
      "Iteration: 312 \t--- Loss: 1.194\n",
      "Iteration: 313 \t--- Loss: 1.145\n",
      "Iteration: 314 \t--- Loss: 1.134\n",
      "Iteration: 315 \t--- Loss: 1.154\n",
      "Iteration: 316 \t--- Loss: 1.182\n",
      "Iteration: 317 \t--- Loss: 1.182\n",
      "Iteration: 318 \t--- Loss: 1.167\n",
      "Iteration: 319 \t--- Loss: 1.186\n",
      "Iteration: 320 \t--- Loss: 1.136\n",
      "Iteration: 321 \t--- Loss: 1.171\n",
      "Iteration: 322 \t--- Loss: 1.149\n",
      "Iteration: 323 \t--- Loss: 1.207\n",
      "Iteration: 324 \t--- Loss: 1.177\n",
      "Iteration: 325 \t--- Loss: 1.188\n",
      "Iteration: 326 \t--- Loss: 1.145\n",
      "Iteration: 327 \t--- Loss: 1.173\n",
      "Iteration: 328 \t--- Loss: 1.156\n",
      "Iteration: 329 \t--- Loss: 1.153\n",
      "Iteration: 330 \t--- Loss: 1.139\n",
      "Iteration: 331 \t--- Loss: 1.219\n",
      "Iteration: 332 \t--- Loss: 1.171\n",
      "Iteration: 333 \t--- Loss: 1.157\n",
      "Iteration: 334 \t--- Loss: 1.156\n",
      "Iteration: 335 \t--- Loss: 1.147\n",
      "Iteration: 336 \t--- Loss: 1.176\n",
      "Iteration: 337 \t--- Loss: 1.177\n",
      "Iteration: 338 \t--- Loss: 1.161\n",
      "Iteration: 339 \t--- Loss: 1.175\n",
      "Iteration: 340 \t--- Loss: 1.142\n",
      "Iteration: 341 \t--- Loss: 1.158\n",
      "Iteration: 342 \t--- Loss: 1.157\n",
      "Iteration: 343 \t--- Loss: 1.169\n",
      "Iteration: 344 \t--- Loss: 1.183\n",
      "Iteration: 345 \t--- Loss: 1.197\n",
      "Iteration: 346 \t--- Loss: 1.165\n",
      "Iteration: 347 \t--- Loss: 1.154\n",
      "Iteration: 348 \t--- Loss: 1.166\n",
      "Iteration: 349 \t--- Loss: 1.157\n",
      "Iteration: 350 \t--- Loss: 1.176\n",
      "Iteration: 351 \t--- Loss: 1.174\n",
      "Iteration: 352 \t--- Loss: 1.180\n",
      "Iteration: 353 \t--- Loss: 1.138\n",
      "Iteration: 354 \t--- Loss: 1.155\n",
      "Iteration: 355 \t--- Loss: 1.164\n",
      "Iteration: 356 \t--- Loss: 1.110\n",
      "Iteration: 357 \t--- Loss: 1.207\n",
      "Iteration: 358 \t--- Loss: 1.157\n",
      "Iteration: 359 \t--- Loss: 1.163\n",
      "Iteration: 360 \t--- Loss: 1.182\n",
      "Iteration: 361 \t--- Loss: 1.197\n",
      "Iteration: 362 \t--- Loss: 1.162\n",
      "Iteration: 363 \t--- Loss: 1.180\n",
      "Iteration: 364 \t--- Loss: 1.175\n",
      "Iteration: 365 \t--- Loss: 1.197\n",
      "Iteration: 366 \t--- Loss: 1.149\n",
      "Iteration: 367 \t--- Loss: 1.209\n",
      "Iteration: 368 \t--- Loss: 1.118\n",
      "Iteration: 369 \t--- Loss: 1.199\n",
      "Iteration: 370 \t--- Loss: 1.190\n",
      "Iteration: 371 \t--- Loss: 1.192\n",
      "Iteration: 372 \t--- Loss: 1.159\n",
      "Iteration: 373 \t--- Loss: 1.170\n",
      "Iteration: 374 \t--- Loss: 1.181\n",
      "Iteration: 375 \t--- Loss: 1.181\n",
      "Iteration: 376 \t--- Loss: 1.176\n",
      "Iteration: 377 \t--- Loss: 1.155\n",
      "Iteration: 378 \t--- Loss: 1.170\n",
      "Iteration: 379 \t--- Loss: 1.144\n",
      "Iteration: 380 \t--- Loss: 1.157\n",
      "Iteration: 381 \t--- Loss: 1.195\n",
      "Iteration: 382 \t--- Loss: 1.188\n",
      "Iteration: 383 \t--- Loss: 1.147\n",
      "Iteration: 384 \t--- Loss: 1.198\n",
      "Iteration: 385 \t--- Loss: 1.164\n",
      "Iteration: 386 \t--- Loss: 1.120\n",
      "Iteration: 387 \t--- Loss: 1.175\n",
      "Iteration: 388 \t--- Loss: 1.167\n",
      "Iteration: 389 \t--- Loss: 1.171\n",
      "Iteration: 390 \t--- Loss: 1.195\n",
      "Iteration: 391 \t--- Loss: 1.171\n",
      "Iteration: 392 \t--- Loss: 1.161\n",
      "Iteration: 393 \t--- Loss: 1.145\n",
      "Iteration: 394 \t--- Loss: 1.185\n",
      "Iteration: 395 \t--- Loss: 1.200\n",
      "Iteration: 396 \t--- Loss: 1.188\n",
      "Iteration: 397 \t--- Loss: 1.166\n",
      "Iteration: 398 \t--- Loss: 1.143\n",
      "Iteration: 399 \t--- Loss: 1.162\n",
      "Iteration: 400 \t--- Loss: 1.165\n",
      "Iteration: 401 \t--- Loss: 1.143\n",
      "Iteration: 402 \t--- Loss: 1.163\n",
      "Iteration: 403 \t--- Loss: 1.160\n",
      "Iteration: 404 \t--- Loss: 1.162\n",
      "Iteration: 405 \t--- Loss: 1.164\n",
      "Iteration: 406 \t--- Loss: 1.173\n",
      "Iteration: 407 \t--- Loss: 1.169\n",
      "Iteration: 408 \t--- Loss: 1.179\n",
      "Iteration: 409 \t--- Loss: 1.161\n",
      "Iteration: 410 \t--- Loss: 1.173\n",
      "Iteration: 411 \t--- Loss: 1.152\n",
      "Iteration: 412 \t--- Loss: 1.160\n",
      "Iteration: 413 \t--- Loss: 1.147\n",
      "Iteration: 414 \t--- Loss: 1.147\n",
      "Iteration: 415 \t--- Loss: 1.156\n",
      "Iteration: 416 \t--- Loss: 1.131\n",
      "Iteration: 417 \t--- Loss: 1.128\n",
      "Iteration: 418 \t--- Loss: 1.164\n",
      "Iteration: 419 \t--- Loss: 1.173\n",
      "Iteration: 420 \t--- Loss: 1.190\n",
      "Iteration: 421 \t--- Loss: 1.150\n",
      "Iteration: 422 \t--- Loss: 1.201\n",
      "Iteration: 423 \t--- Loss: 1.184\n",
      "Iteration: 424 \t--- Loss: 1.147\n",
      "Iteration: 425 \t--- Loss: 1.164\n",
      "Iteration: 426 \t--- Loss: 1.162\n",
      "Iteration: 427 \t--- Loss: 1.187\n",
      "Iteration: 428 \t--- Loss: 1.154\n",
      "Iteration: 429 \t--- Loss: 1.156\n",
      "Iteration: 430 \t--- Loss: 1.184\n",
      "Iteration: 431 \t--- Loss: 1.145\n",
      "Iteration: 432 \t--- Loss: 1.185\n",
      "Iteration: 433 \t--- Loss: 1.174\n",
      "Iteration: 434 \t--- Loss: 1.185\n",
      "Iteration: 435 \t--- Loss: 1.200\n",
      "Iteration: 436 \t--- Loss: 1.191\n",
      "Iteration: 437 \t--- Loss: 1.130\n",
      "Iteration: 438 \t--- Loss: 1.191\n",
      "Iteration: 439 \t--- Loss: 1.163\n",
      "Iteration: 440 \t--- Loss: 1.178\n",
      "Iteration: 441 \t--- Loss: 1.203\n",
      "Iteration: 442 \t--- Loss: 1.132\n",
      "Iteration: 443 \t--- Loss: 1.188\n",
      "Iteration: 444 \t--- Loss: 1.182\n",
      "Iteration: 445 \t--- Loss: 1.170\n",
      "Iteration: 446 \t--- Loss: 1.148\n",
      "Iteration: 447 \t--- Loss: 1.201\n",
      "Iteration: 448 \t--- Loss: 1.178\n",
      "Iteration: 449 \t--- Loss: 1.179\n",
      "Iteration: 450 \t--- Loss: 1.135\n",
      "Iteration: 451 \t--- Loss: 1.185\n",
      "Iteration: 452 \t--- Loss: 1.163\n",
      "Iteration: 453 \t--- Loss: 1.188\n",
      "Iteration: 454 \t--- Loss: 1.181\n",
      "Iteration: 455 \t--- Loss: 1.196\n",
      "Iteration: 456 \t--- Loss: 1.158\n",
      "Iteration: 457 \t--- Loss: 1.147\n",
      "Iteration: 458 \t--- Loss: 1.142\n",
      "Iteration: 459 \t--- Loss: 1.171\n",
      "Iteration: 460 \t--- Loss: 1.160\n",
      "Iteration: 461 \t--- Loss: 1.188\n",
      "Iteration: 462 \t--- Loss: 1.138\n",
      "Iteration: 463 \t--- Loss: 1.146\n",
      "Iteration: 464 \t--- Loss: 1.184\n",
      "Iteration: 465 \t--- Loss: 1.157\n",
      "Iteration: 466 \t--- Loss: 1.189\n",
      "Iteration: 467 \t--- Loss: 1.156\n",
      "Iteration: 468 \t--- Loss: 1.151\n",
      "Iteration: 469 \t--- Loss: 1.177\n",
      "Iteration: 470 \t--- Loss: 1.158\n",
      "Iteration: 471 \t--- Loss: 1.171\n",
      "Iteration: 472 \t--- Loss: 1.193\n",
      "Iteration: 473 \t--- Loss: 1.137\n",
      "Iteration: 474 \t--- Loss: 1.140\n",
      "Iteration: 475 \t--- Loss: 1.167\n",
      "Iteration: 476 \t--- Loss: 1.181\n",
      "Iteration: 477 \t--- Loss: 1.156\n",
      "Iteration: 478 \t--- Loss: 1.115\n",
      "Iteration: 479 \t--- Loss: 1.189\n",
      "Iteration: 480 \t--- Loss: 1.152\n",
      "Iteration: 481 \t--- Loss: 1.189\n",
      "Iteration: 482 \t--- Loss: 1.225\n",
      "Iteration: 483 \t--- Loss: 1.139\n",
      "Iteration: 484 \t--- Loss: 1.176\n",
      "Iteration: 485 \t--- Loss: 1.145\n",
      "Iteration: 486 \t--- Loss: 1.171\n",
      "Iteration: 487 \t--- Loss: 1.120\n",
      "Iteration: 488 \t--- Loss: 1.197\n",
      "Iteration: 489 \t--- Loss: 1.167\n",
      "Iteration: 490 \t--- Loss: 1.177\n",
      "Iteration: 491 \t--- Loss: 1.204\n",
      "Iteration: 492 \t--- Loss: 1.157\n",
      "Iteration: 493 \t--- Loss: 1.196\n",
      "Iteration: 494 \t--- Loss: 1.156\n",
      "Iteration: 495 \t--- Loss: 1.153\n",
      "Iteration: 496 \t--- Loss: 1.179\n",
      "Iteration: 497 \t--- Loss: 1.182\n",
      "Iteration: 498 \t--- Loss: 1.161\n",
      "Iteration: 499 \t--- Loss: 1.133\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.57s/it][Parallel(n_jobs=5)]: Done  34 tasks      | elapsed: 20.1min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.235\n",
      "Iteration: 1 \t--- Loss: 0.226\n",
      "Iteration: 2 \t--- Loss: 0.224\n",
      "Iteration: 3 \t--- Loss: 0.232\n",
      "Iteration: 4 \t--- Loss: 0.219\n",
      "Iteration: 5 \t--- Loss: 0.217\n",
      "Iteration: 6 \t--- Loss: 0.209\n",
      "Iteration: 7 \t--- Loss: 0.218\n",
      "Iteration: 8 \t--- Loss: 0.210\n",
      "Iteration: 9 \t--- Loss: 0.215\n",
      "Iteration: 10 \t--- Loss: 0.210\n",
      "Iteration: 11 \t--- Loss: 0.218\n",
      "Iteration: 12 \t--- Loss: 0.210\n",
      "Iteration: 13 \t--- Loss: 0.204\n",
      "Iteration: 14 \t--- Loss: 0.208\n",
      "Iteration: 15 \t--- Loss: 0.199\n",
      "Iteration: 16 \t--- Loss: 0.210\n",
      "Iteration: 17 \t--- Loss: 0.197\n",
      "Iteration: 18 \t--- Loss: 0.191\n",
      "Iteration: 19 \t--- Loss: 0.200\n",
      "Iteration: 20 \t--- Loss: 0.199\n",
      "Iteration: 21 \t--- Loss: 0.202\n",
      "Iteration: 22 \t--- Loss: 0.198\n",
      "Iteration: 23 \t--- Loss: 0.194\n",
      "Iteration: 24 \t--- Loss: 0.184\n",
      "Iteration: 25 \t--- Loss: 0.201\n",
      "Iteration: 26 \t--- Loss: 0.196\n",
      "Iteration: 27 \t--- Loss: 0.186\n",
      "Iteration: 28 \t--- Loss: 0.197\n",
      "Iteration: 29 \t--- Loss: 0.195\n",
      "Iteration: 30 \t--- Loss: 0.177\n",
      "Iteration: 31 \t--- Loss: 0.185\n",
      "Iteration: 32 \t--- Loss: 0.180\n",
      "Iteration: 33 \t--- Loss: 0.162\n",
      "Iteration: 34 \t--- Loss: 0.152\n",
      "Iteration: 35 \t--- Loss: 0.143\n",
      "Iteration: 36 \t--- Loss: 0.154\n",
      "Iteration: 37 \t--- Loss: 0.146\n",
      "Iteration: 38 \t--- Loss: 0.146\n",
      "Iteration: 39 \t--- Loss: 0.142\n",
      "Iteration: 40 \t--- Loss: 0.138\n",
      "Iteration: 41 \t--- Loss: 0.161\n",
      "Iteration: 42 \t--- Loss: 0.161\n",
      "Iteration: 43 \t--- Loss: 0.202\n",
      "Iteration: 44 \t--- Loss: 0.205\n",
      "Iteration: 45 \t--- Loss: 0.197\n",
      "Iteration: 46 \t--- Loss: 0.234\n",
      "Iteration: 47 \t--- Loss: 0.196\n",
      "Iteration: 48 \t--- Loss: 0.172\n",
      "Iteration: 49 \t--- Loss: 0.144\n",
      "Iteration: 50 \t--- Loss: 0.146\n",
      "Iteration: 51 \t--- Loss: 0.135\n",
      "Iteration: 52 \t--- Loss: 0.148\n",
      "Iteration: 53 \t--- Loss: 0.158\n",
      "Iteration: 54 \t--- Loss: 0.136\n",
      "Iteration: 55 \t--- Loss: 0.133\n",
      "Iteration: 56 \t--- Loss: 0.179\n",
      "Iteration: 57 \t--- Loss: 0.216\n",
      "Iteration: 58 \t--- Loss: 0.190\n",
      "Iteration: 59 \t--- Loss: 0.161\n",
      "Iteration: 60 \t--- Loss: 0.111\n",
      "Iteration: 61 \t--- Loss: 0.134\n",
      "Iteration: 62 \t--- Loss: 0.130\n",
      "Iteration: 63 \t--- Loss: 0.159\n",
      "Iteration: 64 \t--- Loss: 0.212\n",
      "Iteration: 65 \t--- Loss: 0.149\n",
      "Iteration: 66 \t--- Loss: 0.134\n",
      "Iteration: 67 \t--- Loss: 0.151\n",
      "Iteration: 68 \t--- Loss: 0.152\n",
      "Iteration: 69 \t--- Loss: 0.197\n",
      "Iteration: 70 \t--- Loss: 0.117\n",
      "Iteration: 71 \t--- Loss: 0.131\n",
      "Iteration: 72 \t--- Loss: 0.159\n",
      "Iteration: 73 \t--- Loss: 0.123\n",
      "Iteration: 74 \t--- Loss: 0.136\n",
      "Iteration: 75 \t--- Loss: 0.124\n",
      "Iteration: 76 \t--- Loss: 0.136\n",
      "Iteration: 77 \t--- Loss: 0.140\n",
      "Iteration: 78 \t--- Loss: 0.196\n",
      "Iteration: 79 \t--- Loss: 0.114\n",
      "Iteration: 80 \t--- Loss: 0.119\n",
      "Iteration: 81 \t--- Loss: 0.125\n",
      "Iteration: 82 \t--- Loss: 0.116\n",
      "Iteration: 83 \t--- Loss: 0.128\n",
      "Iteration: 84 \t--- Loss: 0.116\n",
      "Iteration: 85 \t--- Loss: 0.148\n",
      "Iteration: 86 \t--- Loss: 0.104\n",
      "Iteration: 87 \t--- Loss: 0.114\n",
      "Iteration: 88 \t--- Loss: 0.134\n",
      "Iteration: 89 \t--- Loss: 0.162\n",
      "Iteration: 90 \t--- Loss: 0.094\n",
      "Iteration: 91 \t--- Loss: 0.111\n",
      "Iteration: 92 \t--- Loss: 0.119\n",
      "Iteration: 93 \t--- Loss: 0.098\n",
      "Iteration: 94 \t--- Loss: 0.095\n",
      "Iteration: 95 \t--- Loss: 0.112\n",
      "Iteration: 96 \t--- Loss: 0.123\n",
      "Iteration: 97 \t--- Loss: 0.096\n",
      "Iteration: 98 \t--- Loss: 0.095\n",
      "Iteration: 99 \t--- Loss: 0.111\n",
      "Iteration: 100 \t--- Loss: 0.126\n",
      "Iteration: 101 \t--- Loss: 0.092\n",
      "Iteration: 102 \t--- Loss: 0.089\n",
      "Iteration: 103 \t--- Loss: 0.107\n",
      "Iteration: 104 \t--- Loss: 0.123\n",
      "Iteration: 105 \t--- Loss: 0.084\n",
      "Iteration: 106 \t--- Loss: 0.083\n",
      "Iteration: 107 \t--- Loss: 0.087\n",
      "Iteration: 108 \t--- Loss: 0.092\n",
      "Iteration: 109 \t--- Loss: 0.109\n",
      "Iteration: 110 \t--- Loss: 0.153\n",
      "Iteration: 111 \t--- Loss: 0.076\n",
      "Iteration: 112 \t--- Loss: 0.085\n",
      "Iteration: 113 \t--- Loss: 0.083\n",
      "Iteration: 114 \t--- Loss: 0.085\n",
      "Iteration: 115 \t--- Loss: 0.079\n",
      "Iteration: 116 \t--- Loss: 0.096\n",
      "Iteration: 117 \t--- Loss: 0.113\n",
      "Iteration: 118 \t--- Loss: 0.096\n",
      "Iteration: 119 \t--- Loss: 0.106\n",
      "Iteration: 120 \t--- Loss: 0.083\n",
      "Iteration: 121 \t--- Loss: 0.086\n",
      "Iteration: 122 \t--- Loss: 0.095\n",
      "Iteration: 123 \t--- Loss: 0.125\n",
      "Iteration: 124 \t--- Loss: 0.069\n",
      "Iteration: 125 \t--- Loss: 0.076\n",
      "Iteration: 126 \t--- Loss: 0.073\n",
      "Iteration: 127 \t--- Loss: 0.076\n",
      "Iteration: 128 \t--- Loss: 0.077\n",
      "Iteration: 129 \t--- Loss: 0.089\n",
      "Iteration: 130 \t--- Loss: 0.098\n",
      "Iteration: 131 \t--- Loss: 0.117\n",
      "Iteration: 132 \t--- Loss: 0.068\n",
      "Iteration: 133 \t--- Loss: 0.061\n",
      "Iteration: 134 \t--- Loss: 0.076\n",
      "Iteration: 135 \t--- Loss: 0.073\n",
      "Iteration: 136 \t--- Loss: 0.085\n",
      "Iteration: 137 \t--- Loss: 0.093\n",
      "Iteration: 138 \t--- Loss: 0.082\n",
      "Iteration: 139 \t--- Loss: 0.092\n",
      "Iteration: 140 \t--- Loss: 0.082\n",
      "Iteration: 141 \t--- Loss: 0.088\n",
      "Iteration: 142 \t--- Loss: 0.076\n",
      "Iteration: 143 \t--- Loss: 0.077\n",
      "Iteration: 144 \t--- Loss: 0.075\n",
      "Iteration: 145 \t--- Loss: 0.076\n",
      "Iteration: 146 \t--- Loss: 0.068\n",
      "Iteration: 147 \t--- Loss: 0.068\n",
      "Iteration: 148 \t--- Loss: 0.074\n",
      "Iteration: 149 \t--- Loss: 0.078\n",
      "Iteration: 150 \t--- Loss: 0.070\n",
      "Iteration: 151 \t--- Loss: 0.073\n",
      "Iteration: 152 \t--- Loss: 0.071\n",
      "Iteration: 153 \t--- Loss: 0.078\n",
      "Iteration: 154 \t--- Loss: 0.073\n",
      "Iteration: 155 \t--- Loss: 0.076\n",
      "Iteration: 156 \t--- Loss: 0.072\n",
      "Iteration: 157 \t--- Loss: 0.078\n",
      "Iteration: 158 \t--- Loss: 0.061\n",
      "Iteration: 159 \t--- Loss: 0.064\n",
      "Iteration: 160 \t--- Loss: 0.067\n",
      "Iteration: 161 \t--- Loss: 0.065\n",
      "Iteration: 162 \t--- Loss: 0.065\n",
      "Iteration: 163 \t--- Loss: 0.065\n",
      "Iteration: 164 \t--- Loss: 0.063\n",
      "Iteration: 165 \t--- Loss: 0.062\n",
      "Iteration: 166 \t--- Loss: 0.068\n",
      "Iteration: 167 \t--- Loss: 0.068\n",
      "Iteration: 168 \t--- Loss: 0.066\n",
      "Iteration: 169 \t--- Loss: 0.069\n",
      "Iteration: 170 \t--- Loss: 0.064\n",
      "Iteration: 171 \t--- Loss: 0.069\n",
      "Iteration: 172 \t--- Loss: 0.066\n",
      "Iteration: 173 \t--- Loss: 0.065\n",
      "Iteration: 174 \t--- Loss: 0.070\n",
      "Iteration: 175 \t--- Loss: 0.066\n",
      "Iteration: 176 \t--- Loss: 0.072\n",
      "Iteration: 177 \t--- Loss: 0.074\n",
      "Iteration: 178 \t--- Loss: 0.073\n",
      "Iteration: 179 \t--- Loss: 0.075\n",
      "Iteration: 180 \t--- Loss: 0.065\n",
      "Iteration: 181 \t--- Loss: 0.070\n",
      "Iteration: 182 \t--- Loss: 0.064\n",
      "Iteration: 183 \t--- Loss: 0.068\n",
      "Iteration: 184 \t--- Loss: 0.061\n",
      "Iteration: 185 \t--- Loss: 0.067\n",
      "Iteration: 186 \t--- Loss: 0.065\n",
      "Iteration: 187 \t--- Loss: 0.065\n",
      "Iteration: 188 \t--- Loss: 0.057\n",
      "Iteration: 189 \t--- Loss: 0.059\n",
      "Iteration: 190 \t--- Loss: 0.060\n",
      "Iteration: 191 \t--- Loss: 0.053\n",
      "Iteration: 192 \t--- Loss: 0.056\n",
      "Iteration: 193 \t--- Loss: 0.060\n",
      "Iteration: 194 \t--- Loss: 0.058\n",
      "Iteration: 195 \t--- Loss: 0.062\n",
      "Iteration: 196 \t--- Loss: 0.057\n",
      "Iteration: 197 \t--- Loss: 0.060\n",
      "Iteration: 198 \t--- Loss: 0.056\n",
      "Iteration: 199 \t--- Loss: 0.065\n",
      "Iteration: 200 \t--- Loss: 0.061\n",
      "Iteration: 201 \t--- Loss: 0.059\n",
      "Iteration: 202 \t--- Loss: 0.060\n",
      "Iteration: 203 \t--- Loss: 0.057\n",
      "Iteration: 204 \t--- Loss: 0.061\n",
      "Iteration: 205 \t--- Loss: 0.059\n",
      "Iteration: 206 \t--- Loss: 0.055\n",
      "Iteration: 207 \t--- Loss: 0.061\n",
      "Iteration: 208 \t--- Loss: 0.062\n",
      "Iteration: 209 \t--- Loss: 0.060\n",
      "Iteration: 210 \t--- Loss: 0.059\n",
      "Iteration: 211 \t--- Loss: 0.066\n",
      "Iteration: 212 \t--- Loss: 0.063\n",
      "Iteration: 213 \t--- Loss: 0.065\n",
      "Iteration: 214 \t--- Loss: 0.055\n",
      "Iteration: 215 \t--- Loss: 0.061\n",
      "Iteration: 216 \t--- Loss: 0.060\n",
      "Iteration: 217 \t--- Loss: 0.056\n",
      "Iteration: 218 \t--- Loss: 0.059\n",
      "Iteration: 219 \t--- Loss: 0.059\n",
      "Iteration: 220 \t--- Loss: 0.058\n",
      "Iteration: 221 \t--- Loss: 0.057\n",
      "Iteration: 222 \t--- Loss: 0.060\n",
      "Iteration: 223 \t--- Loss: 0.056\n",
      "Iteration: 224 \t--- Loss: 0.061\n",
      "Iteration: 225 \t--- Loss: 0.063\n",
      "Iteration: 226 \t--- Loss: 0.056\n",
      "Iteration: 227 \t--- Loss: 0.054\n",
      "Iteration: 228 \t--- Loss: 0.061\n",
      "Iteration: 229 \t--- Loss: 0.061\n",
      "Iteration: 230 \t--- Loss: 0.069\n",
      "Iteration: 231 \t--- Loss: 0.061\n",
      "Iteration: 232 \t--- Loss: 0.052\n",
      "Iteration: 233 \t--- Loss: 0.059\n",
      "Iteration: 234 \t--- Loss: 0.057\n",
      "Iteration: 235 \t--- Loss: 0.061\n",
      "Iteration: 236 \t--- Loss: 0.055\n",
      "Iteration: 237 \t--- Loss: 0.056\n",
      "Iteration: 238 \t--- Loss: 0.058\n",
      "Iteration: 239 \t--- Loss: 0.062\n",
      "Iteration: 240 \t--- Loss: 0.056\n",
      "Iteration: 241 \t--- Loss: 0.052\n",
      "Iteration: 242 \t--- Loss: 0.058\n",
      "Iteration: 243 \t--- Loss: 0.058\n",
      "Iteration: 244 \t--- Loss: 0.058\n",
      "Iteration: 245 \t--- Loss: 0.053\n",
      "Iteration: 246 \t--- Loss: 0.056\n",
      "Iteration: 247 \t--- Loss: 0.055\n",
      "Iteration: 248 \t--- Loss: 0.059\n",
      "Iteration: 249 \t--- Loss: 0.059\n",
      "Iteration: 250 \t--- Loss: 0.053\n",
      "Iteration: 251 \t--- Loss: 0.059\n",
      "Iteration: 252 \t--- Loss: 0.056\n",
      "Iteration: 253 \t--- Loss: 0.059\n",
      "Iteration: 254 \t--- Loss: 0.060\n",
      "Iteration: 255 \t--- Loss: 0.056\n",
      "Iteration: 256 \t--- Loss: 0.061\n",
      "Iteration: 257 \t--- Loss: 0.055\n",
      "Iteration: 258 \t--- Loss: 0.058\n",
      "Iteration: 259 \t--- Loss: 0.053Iteration: 0 \t--- Loss: 0.080\n",
      "Iteration: 1 \t--- Loss: 0.080\n",
      "Iteration: 2 \t--- Loss: 0.078\n",
      "Iteration: 3 \t--- Loss: 0.084\n",
      "Iteration: 4 \t--- Loss: 0.080\n",
      "Iteration: 5 \t--- Loss: 0.080\n",
      "Iteration: 6 \t--- Loss: 0.081\n",
      "Iteration: 7 \t--- Loss: 0.077\n",
      "Iteration: 8 \t--- Loss: 0.079\n",
      "Iteration: 9 \t--- Loss: 0.079\n",
      "Iteration: 10 \t--- Loss: 0.081\n",
      "Iteration: 11 \t--- Loss: 0.078\n",
      "Iteration: 12 \t--- Loss: 0.073\n",
      "Iteration: 13 \t--- Loss: 0.081\n",
      "Iteration: 14 \t--- Loss: 0.080\n",
      "Iteration: 15 \t--- Loss: 0.080\n",
      "Iteration: 16 \t--- Loss: 0.080\n",
      "Iteration: 17 \t--- Loss: 0.076\n",
      "Iteration: 18 \t--- Loss: 0.081\n",
      "Iteration: 19 \t--- Loss: 0.077\n",
      "Iteration: 20 \t--- Loss: 0.083\n",
      "Iteration: 21 \t--- Loss: 0.079\n",
      "Iteration: 22 \t--- Loss: 0.078\n",
      "Iteration: 23 \t--- Loss: 0.080\n",
      "Iteration: 24 \t--- Loss: 0.080\n",
      "Iteration: 25 \t--- Loss: 0.076\n",
      "Iteration: 26 \t--- Loss: 0.080\n",
      "Iteration: 27 \t--- Loss: 0.076\n",
      "Iteration: 28 \t--- Loss: 0.075\n",
      "Iteration: 29 \t--- Loss: 0.077\n",
      "Iteration: 30 \t--- Loss: 0.075\n",
      "Iteration: 31 \t--- Loss: 0.077\n",
      "Iteration: 32 \t--- Loss: 0.079\n",
      "Iteration: 33 \t--- Loss: 0.077\n",
      "Iteration: 34 \t--- Loss: 0.079\n",
      "Iteration: 35 \t--- Loss: 0.079\n",
      "Iteration: 36 \t--- Loss: 0.077\n",
      "Iteration: 37 \t--- Loss: 0.079\n",
      "Iteration: 38 \t--- Loss: 0.078\n",
      "Iteration: 39 \t--- Loss: 0.077\n",
      "Iteration: 40 \t--- Loss: 0.077\n",
      "Iteration: 41 \t--- Loss: 0.076\n",
      "Iteration: 42 \t--- Loss: 0.077\n",
      "Iteration: 43 \t--- Loss: 0.078\n",
      "Iteration: 44 \t--- Loss: 0.077\n",
      "Iteration: 45 \t--- Loss: 0.081\n",
      "Iteration: 46 \t--- Loss: 0.079\n",
      "Iteration: 47 \t--- Loss: 0.075\n",
      "Iteration: 48 \t--- Loss: 0.073\n",
      "Iteration: 49 \t--- Loss: 0.077\n",
      "Iteration: 50 \t--- Loss: 0.073\n",
      "Iteration: 51 \t--- Loss: 0.077\n",
      "Iteration: 52 \t--- Loss: 0.079\n",
      "Iteration: 53 \t--- Loss: 0.072\n",
      "Iteration: 54 \t--- Loss: 0.078\n",
      "Iteration: 55 \t--- Loss: 0.077\n",
      "Iteration: 56 \t--- Loss: 0.078\n",
      "Iteration: 57 \t--- Loss: 0.073\n",
      "Iteration: 58 \t--- Loss: 0.076\n",
      "Iteration: 59 \t--- Loss: 0.075\n",
      "Iteration: 60 \t--- Loss: 0.077\n",
      "Iteration: 61 \t--- Loss: 0.075\n",
      "Iteration: 62 \t--- Loss: 0.073\n",
      "Iteration: 63 \t--- Loss: 0.081\n",
      "Iteration: 64 \t--- Loss: 0.079\n",
      "Iteration: 65 \t--- Loss: 0.074\n",
      "Iteration: 66 \t--- Loss: 0.073\n",
      "Iteration: 67 \t--- Loss: 0.077\n",
      "Iteration: 68 \t--- Loss: 0.072\n",
      "Iteration: 69 \t--- Loss: 0.074\n",
      "Iteration: 70 \t--- Loss: 0.077\n",
      "Iteration: 71 \t--- Loss: 0.075\n",
      "Iteration: 72 \t--- Loss: 0.075\n",
      "Iteration: 73 \t--- Loss: 0.077\n",
      "Iteration: 74 \t--- Loss: 0.074\n",
      "Iteration: 75 \t--- Loss: 0.072\n",
      "Iteration: 76 \t--- Loss: 0.077\n",
      "Iteration: 77 \t--- Loss: 0.072\n",
      "Iteration: 78 \t--- Loss: 0.074\n",
      "Iteration: 79 \t--- Loss: 0.065\n",
      "Iteration: 80 \t--- Loss: 0.077\n",
      "Iteration: 81 \t--- Loss: 0.070\n",
      "Iteration: 82 \t--- Loss: 0.076\n",
      "Iteration: 83 \t--- Loss: 0.073\n",
      "Iteration: 84 \t--- Loss: 0.072\n",
      "Iteration: 85 \t--- Loss: 0.073\n",
      "Iteration: 86 \t--- Loss: 0.076\n",
      "Iteration: 87 \t--- Loss: 0.081\n",
      "Iteration: 88 \t--- Loss: 0.078\n",
      "Iteration: 89 \t--- Loss: 0.083\n",
      "Iteration: 90 \t--- Loss: 0.071\n",
      "Iteration: 91 \t--- Loss: 0.076\n",
      "Iteration: 92 \t--- Loss: 0.074\n",
      "Iteration: 93 \t--- Loss: 0.073\n",
      "Iteration: 94 \t--- Loss: 0.071\n",
      "Iteration: 95 \t--- Loss: 0.071\n",
      "Iteration: 96 \t--- Loss: 0.070\n",
      "Iteration: 97 \t--- Loss: 0.073\n",
      "Iteration: 98 \t--- Loss: 0.070\n",
      "Iteration: 99 \t--- Loss: 0.071\n",
      "Iteration: 100 \t--- Loss: 0.070\n",
      "Iteration: 101 \t--- Loss: 0.074\n",
      "Iteration: 102 \t--- Loss: 0.076\n",
      "Iteration: 103 \t--- Loss: 0.074\n",
      "Iteration: 104 \t--- Loss: 0.075\n",
      "Iteration: 105 \t--- Loss: 0.070\n",
      "Iteration: 106 \t--- Loss: 0.072\n",
      "Iteration: 107 \t--- Loss: 0.073\n",
      "Iteration: 108 \t--- Loss: 0.075\n",
      "Iteration: 109 \t--- Loss: 0.070\n",
      "Iteration: 110 \t--- Loss: 0.072\n",
      "Iteration: 111 \t--- Loss: 0.074\n",
      "Iteration: 112 \t--- Loss: 0.071\n",
      "Iteration: 113 \t--- Loss: 0.069\n",
      "Iteration: 114 \t--- Loss: 0.072\n",
      "Iteration: 115 \t--- Loss: 0.071\n",
      "Iteration: 116 \t--- Loss: 0.069\n",
      "Iteration: 117 \t--- Loss: 0.068\n",
      "Iteration: 118 \t--- Loss: 0.068\n",
      "Iteration: 119 \t--- Loss: 0.063\n",
      "Iteration: 120 \t--- Loss: 0.064\n",
      "Iteration: 121 \t--- Loss: 0.060\n",
      "Iteration: 122 \t--- Loss: 0.060\n",
      "Iteration: 123 \t--- Loss: 0.051\n",
      "Iteration: 124 \t--- Loss: 0.052\n",
      "Iteration: 125 \t--- Loss: 0.050\n",
      "Iteration: 126 \t--- Loss: 0.045\n",
      "Iteration: 127 \t--- Loss: 0.045\n",
      "Iteration: 128 \t--- Loss: 0.048\n",
      "Iteration: 129 \t--- Loss: 0.053\n",
      "Iteration: 130 \t--- Loss: 0.043\n",
      "Iteration: 131 \t--- Loss: 0.048\n",
      "Iteration: 132 \t--- Loss: 0.046\n",
      "Iteration: 133 \t--- Loss: 0.047\n",
      "Iteration: 134 \t--- Loss: 0.048\n",
      "Iteration: 135 \t--- Loss: 0.045\n",
      "Iteration: 136 \t--- Loss: 0.043\n",
      "Iteration: 137 \t--- Loss: 0.044\n",
      "Iteration: 138 \t--- Loss: 0.044\n",
      "Iteration: 139 \t--- Loss: 0.047\n",
      "Iteration: 140 \t--- Loss: 0.045\n",
      "Iteration: 141 \t--- Loss: 0.045\n",
      "Iteration: 142 \t--- Loss: 0.046\n",
      "Iteration: 143 \t--- Loss: 0.042\n",
      "Iteration: 144 \t--- Loss: 0.046\n",
      "Iteration: 145 \t--- Loss: 0.044\n",
      "Iteration: 146 \t--- Loss: 0.044\n",
      "Iteration: 147 \t--- Loss: 0.044\n",
      "Iteration: 148 \t--- Loss: 0.043\n",
      "Iteration: 149 \t--- Loss: 0.043\n",
      "Iteration: 150 \t--- Loss: 0.043\n",
      "Iteration: 151 \t--- Loss: 0.046\n",
      "Iteration: 152 \t--- Loss: 0.043\n",
      "Iteration: 153 \t--- Loss: 0.043\n",
      "Iteration: 154 \t--- Loss: 0.046\n",
      "Iteration: 155 \t--- Loss: 0.043\n",
      "Iteration: 156 \t--- Loss: 0.043\n",
      "Iteration: 157 \t--- Loss: 0.041\n",
      "Iteration: 158 \t--- Loss: 0.039\n",
      "Iteration: 159 \t--- Loss: 0.042\n",
      "Iteration: 160 \t--- Loss: 0.040\n",
      "Iteration: 161 \t--- Loss: 0.043\n",
      "Iteration: 162 \t--- Loss: 0.044\n",
      "Iteration: 163 \t--- Loss: 0.040\n",
      "Iteration: 164 \t--- Loss: 0.041\n",
      "Iteration: 165 \t--- Loss: 0.040\n",
      "Iteration: 166 \t--- Loss: 0.045\n",
      "Iteration: 167 \t--- Loss: 0.039\n",
      "Iteration: 168 \t--- Loss: 0.041\n",
      "Iteration: 169 \t--- Loss: 0.049\n",
      "Iteration: 170 \t--- Loss: 0.069\n",
      "Iteration: 171 \t--- Loss: 0.094\n",
      "Iteration: 172 \t--- Loss: 0.131\n",
      "Iteration: 173 \t--- Loss: 0.137\n",
      "Iteration: 174 \t--- Loss: 0.134\n",
      "Iteration: 175 \t--- Loss: 0.104\n",
      "Iteration: 176 \t--- Loss: 0.053\n",
      "Iteration: 177 \t--- Loss: 0.042\n",
      "Iteration: 178 \t--- Loss: 0.039\n",
      "Iteration: 179 \t--- Loss: 0.040\n",
      "Iteration: 180 \t--- Loss: 0.042\n",
      "Iteration: 181 \t--- Loss: 0.042\n",
      "Iteration: 182 \t--- Loss: 0.042\n",
      "Iteration: 183 \t--- Loss: 0.044\n",
      "Iteration: 184 \t--- Loss: 0.048\n",
      "Iteration: 185 \t--- Loss: 0.048\n",
      "Iteration: 186 \t--- Loss: 0.064\n",
      "Iteration: 187 \t--- Loss: 0.045\n",
      "Iteration: 188 \t--- Loss: 0.047\n",
      "Iteration: 189 \t--- Loss: 0.043\n",
      "Iteration: 190 \t--- Loss: 0.058\n",
      "Iteration: 191 \t--- Loss: 0.050\n",
      "Iteration: 192 \t--- Loss: 0.063\n",
      "Iteration: 193 \t--- Loss: 0.038\n",
      "Iteration: 194 \t--- Loss: 0.034\n",
      "Iteration: 195 \t--- Loss: 0.039\n",
      "Iteration: 196 \t--- Loss: 0.037\n",
      "Iteration: 197 \t--- Loss: 0.045\n",
      "Iteration: 198 \t--- Loss: 0.056\n",
      "Iteration: 199 \t--- Loss: 0.047\n",
      "Iteration: 200 \t--- Loss: 0.065\n",
      "Iteration: 201 \t--- Loss: 0.044\n",
      "Iteration: 202 \t--- Loss: 0.051\n",
      "Iteration: 203 \t--- Loss: 0.040\n",
      "Iteration: 204 \t--- Loss: 0.045\n",
      "Iteration: 205 \t--- Loss: 0.050\n",
      "Iteration: 206 \t--- Loss: 0.070\n",
      "Iteration: 207 \t--- Loss: 0.033\n",
      "Iteration: 208 \t--- Loss: 0.033\n",
      "Iteration: 209 \t--- Loss: 0.037\n",
      "Iteration: 210 \t--- Loss: 0.039\n",
      "Iteration: 211 \t--- Loss: 0.045\n",
      "Iteration: 212 \t--- Loss: 0.061\n",
      "Iteration: 213 \t--- Loss: 0.035\n",
      "Iteration: 214 \t--- Loss: 0.038\n",
      "Iteration: 215 \t--- Loss: 0.043\n",
      "Iteration: 216 \t--- Loss: 0.055\n",
      "Iteration: 217 \t--- Loss: 0.043\n",
      "Iteration: 218 \t--- Loss: 0.053\n",
      "Iteration: 219 \t--- Loss: 0.042\n",
      "Iteration: 220 \t--- Loss: 0.049\n",
      "Iteration: 221 \t--- Loss: 0.041\n",
      "Iteration: 222 \t--- Loss: 0.049\n",
      "Iteration: 223 \t--- Loss: 0.036\n",
      "Iteration: 224 \t--- Loss: 0.035\n",
      "Iteration: 225 \t--- Loss: 0.036\n",
      "Iteration: 226 \t--- Loss: 0.039\n",
      "Iteration: 227 \t--- Loss: 0.042\n",
      "Iteration: 228 \t--- Loss: 0.061\n",
      "Iteration: 229 \t--- Loss: 0.037\n",
      "Iteration: 230 \t--- Loss: 0.040\n",
      "Iteration: 231 \t--- Loss: 0.035\n",
      "Iteration: 232 \t--- Loss: 0.038\n",
      "Iteration: 233 \t--- Loss: 0.038\n",
      "Iteration: 234 \t--- Loss: 0.044\n",
      "Iteration: 235 \t--- Loss: 0.032\n",
      "Iteration: 236 \t--- Loss: 0.032\n",
      "Iteration: 237 \t--- Loss: 0.033\n",
      "Iteration: 238 \t--- Loss: 0.037\n",
      "Iteration: 239 \t--- Loss: 0.037\n",
      "Iteration: 240 \t--- Loss: 0.048\n",
      "Iteration: 241 \t--- Loss: 0.037\n",
      "Iteration: 242 \t--- Loss: 0.041\n",
      "Iteration: 243 \t--- Loss: 0.035\n",
      "Iteration: 244 \t--- Loss: 0.042\n",
      "Iteration: 245 \t--- Loss: 0.037\n",
      "Iteration: 246 \t--- Loss: 0.043\n",
      "Iteration: 247 \t--- Loss: 0.030\n",
      "Iteration: 248 \t--- Loss: 0.029\n",
      "Iteration: 249 \t--- Loss: 0.027\n",
      "Iteration: 250 \t--- Loss: 0.024\n",
      "Iteration: 251 \t--- Loss: 0.028\n",
      "Iteration: 252 \t--- Loss: 0.026\n",
      "Iteration: 253 \t--- Loss: 0.028\n",
      "Iteration: 254 \t--- Loss: 0.029\n",
      "Iteration: 255 \t--- Loss: 0.028\n",
      "Iteration: 256 \t--- Loss: 0.031\n",
      "Iteration: 257 \t--- Loss: 0.033\n",
      "Iteration: 258 \t--- Loss: 0.036\n",
      "Iteration: 259 \t--- Loss: 0.033"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:38<00:00, 98.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.034\n",
      "Iteration: 261 \t--- Loss: 0.031\n",
      "Iteration: 262 \t--- Loss: 0.030\n",
      "Iteration: 263 \t--- Loss: 0.030\n",
      "Iteration: 264 \t--- Loss: 0.033\n",
      "Iteration: 265 \t--- Loss: 0.031\n",
      "Iteration: 266 \t--- Loss: 0.031\n",
      "Iteration: 267 \t--- Loss: 0.031\n",
      "Iteration: 268 \t--- Loss: 0.037\n",
      "Iteration: 269 \t--- Loss: 0.028\n",
      "Iteration: 270 \t--- Loss: 0.029\n",
      "Iteration: 271 \t--- Loss: 0.027\n",
      "Iteration: 272 \t--- Loss: 0.028\n",
      "Iteration: 273 \t--- Loss: 0.029\n",
      "Iteration: 274 \t--- Loss: 0.033\n",
      "Iteration: 275 \t--- Loss: 0.030\n",
      "Iteration: 276 \t--- Loss: 0.035\n",
      "Iteration: 277 \t--- Loss: 0.034\n",
      "Iteration: 278 \t--- Loss: 0.045\n",
      "Iteration: 279 \t--- Loss: 0.028\n",
      "Iteration: 280 \t--- Loss: 0.027\n",
      "Iteration: 281 \t--- Loss: 0.025\n",
      "Iteration: 282 \t--- Loss: 0.025\n",
      "Iteration: 283 \t--- Loss: 0.026\n",
      "Iteration: 284 \t--- Loss: 0.027\n",
      "Iteration: 285 \t--- Loss: 0.026\n",
      "Iteration: 286 \t--- Loss: 0.027\n",
      "Iteration: 287 \t--- Loss: 0.024\n",
      "Iteration: 288 \t--- Loss: 0.024\n",
      "Iteration: 289 \t--- Loss: 0.024\n",
      "Iteration: 290 \t--- Loss: 0.025\n",
      "Iteration: 291 \t--- Loss: 0.024\n",
      "Iteration: 292 \t--- Loss: 0.025\n",
      "Iteration: 293 \t--- Loss: 0.024\n",
      "Iteration: 294 \t--- Loss: 0.022\n",
      "Iteration: 295 \t--- Loss: 0.022\n",
      "Iteration: 296 \t--- Loss: 0.022\n",
      "Iteration: 297 \t--- Loss: 0.025\n",
      "Iteration: 298 \t--- Loss: 0.024\n",
      "Iteration: 299 \t--- Loss: 0.027\n",
      "Iteration: 300 \t--- Loss: 0.031\n",
      "Iteration: 301 \t--- Loss: 0.027\n",
      "Iteration: 302 \t--- Loss: 0.028\n",
      "Iteration: 303 \t--- Loss: 0.025\n",
      "Iteration: 304 \t--- Loss: 0.028\n",
      "Iteration: 305 \t--- Loss: 0.028\n",
      "Iteration: 306 \t--- Loss: 0.031\n",
      "Iteration: 307 \t--- Loss: 0.022\n",
      "Iteration: 308 \t--- Loss: 0.023\n",
      "Iteration: 309 \t--- Loss: 0.022\n",
      "Iteration: 310 \t--- Loss: 0.020\n",
      "Iteration: 311 \t--- Loss: 0.022\n",
      "Iteration: 312 \t--- Loss: 0.021\n",
      "Iteration: 313 \t--- Loss: 0.022\n",
      "Iteration: 314 \t--- Loss: 0.020\n",
      "Iteration: 315 \t--- Loss: 0.020\n",
      "Iteration: 316 \t--- Loss: 0.020\n",
      "Iteration: 317 \t--- Loss: 0.021\n",
      "Iteration: 318 \t--- Loss: 0.019\n",
      "Iteration: 319 \t--- Loss: 0.021\n",
      "Iteration: 320 \t--- Loss: 0.022\n",
      "Iteration: 321 \t--- Loss: 0.023\n",
      "Iteration: 322 \t--- Loss: 0.022\n",
      "Iteration: 323 \t--- Loss: 0.023\n",
      "Iteration: 324 \t--- Loss: 0.025\n",
      "Iteration: 325 \t--- Loss: 0.023\n",
      "Iteration: 326 \t--- Loss: 0.023\n",
      "Iteration: 327 \t--- Loss: 0.025\n",
      "Iteration: 328 \t--- Loss: 0.026\n",
      "Iteration: 329 \t--- Loss: 0.027\n",
      "Iteration: 330 \t--- Loss: 0.034\n",
      "Iteration: 331 \t--- Loss: 0.028\n",
      "Iteration: 332 \t--- Loss: 0.033\n",
      "Iteration: 333 \t--- Loss: 0.024\n",
      "Iteration: 334 \t--- Loss: 0.026\n",
      "Iteration: 335 \t--- Loss: 0.022\n",
      "Iteration: 336 \t--- Loss: 0.022\n",
      "Iteration: 337 \t--- Loss: 0.019\n",
      "Iteration: 338 \t--- Loss: 0.020\n",
      "Iteration: 339 \t--- Loss: 0.021\n",
      "Iteration: 340 \t--- Loss: 0.019\n",
      "Iteration: 341 \t--- Loss: 0.021\n",
      "Iteration: 342 \t--- Loss: 0.021\n",
      "Iteration: 343 \t--- Loss: 0.021\n",
      "Iteration: 344 \t--- Loss: 0.018\n",
      "Iteration: 345 \t--- Loss: 0.022\n",
      "Iteration: 346 \t--- Loss: 0.021\n",
      "Iteration: 347 \t--- Loss: 0.019\n",
      "Iteration: 348 \t--- Loss: 0.021\n",
      "Iteration: 349 \t--- Loss: 0.021\n",
      "Iteration: 350 \t--- Loss: 0.019\n",
      "Iteration: 351 \t--- Loss: 0.019\n",
      "Iteration: 352 \t--- Loss: 0.019\n",
      "Iteration: 353 \t--- Loss: 0.019\n",
      "Iteration: 354 \t--- Loss: 0.020\n",
      "Iteration: 355 \t--- Loss: 0.020\n",
      "Iteration: 356 \t--- Loss: 0.019\n",
      "Iteration: 357 \t--- Loss: 0.020\n",
      "Iteration: 358 \t--- Loss: 0.019\n",
      "Iteration: 359 \t--- Loss: 0.019\n",
      "Iteration: 360 \t--- Loss: 0.020\n",
      "Iteration: 361 \t--- Loss: 0.021\n",
      "Iteration: 362 \t--- Loss: 0.020\n",
      "Iteration: 363 \t--- Loss: 0.020\n",
      "Iteration: 364 \t--- Loss: 0.021\n",
      "Iteration: 365 \t--- Loss: 0.019\n",
      "Iteration: 366 \t--- Loss: 0.019\n",
      "Iteration: 367 \t--- Loss: 0.019\n",
      "Iteration: 368 \t--- Loss: 0.018\n",
      "Iteration: 369 \t--- Loss: 0.018\n",
      "Iteration: 370 \t--- Loss: 0.020\n",
      "Iteration: 371 \t--- Loss: 0.019\n",
      "Iteration: 372 \t--- Loss: 0.020\n",
      "Iteration: 373 \t--- Loss: 0.019\n",
      "Iteration: 374 \t--- Loss: 0.019\n",
      "Iteration: 375 \t--- Loss: 0.019\n",
      "Iteration: 376 \t--- Loss: 0.018\n",
      "Iteration: 377 \t--- Loss: 0.020\n",
      "Iteration: 378 \t--- Loss: 0.019\n",
      "Iteration: 379 \t--- Loss: 0.019\n",
      "Iteration: 380 \t--- Loss: 0.018\n",
      "Iteration: 381 \t--- Loss: 0.018\n",
      "Iteration: 382 \t--- Loss: 0.019\n",
      "Iteration: 383 \t--- Loss: 0.018\n",
      "Iteration: 384 \t--- Loss: 0.020\n",
      "Iteration: 385 \t--- Loss: 0.020\n",
      "Iteration: 386 \t--- Loss: 0.021\n",
      "Iteration: 387 \t--- Loss: 0.020\n",
      "Iteration: 388 \t--- Loss: 0.019\n",
      "Iteration: 389 \t--- Loss: 0.021\n",
      "Iteration: 390 \t--- Loss: 0.023\n",
      "Iteration: 391 \t--- Loss: 0.022\n",
      "Iteration: 392 \t--- Loss: 0.021\n",
      "Iteration: 393 \t--- Loss: 0.021\n",
      "Iteration: 394 \t--- Loss: 0.020\n",
      "Iteration: 395 \t--- Loss: 0.019\n",
      "Iteration: 396 \t--- Loss: 0.021\n",
      "Iteration: 397 \t--- Loss: 0.020\n",
      "Iteration: 398 \t--- Loss: 0.020\n",
      "Iteration: 399 \t--- Loss: 0.020\n",
      "Iteration: 400 \t--- Loss: 0.021\n",
      "Iteration: 401 \t--- Loss: 0.020\n",
      "Iteration: 402 \t--- Loss: 0.021\n",
      "Iteration: 403 \t--- Loss: 0.022\n",
      "Iteration: 404 \t--- Loss: 0.019\n",
      "Iteration: 405 \t--- Loss: 0.021\n",
      "Iteration: 406 \t--- Loss: 0.021\n",
      "Iteration: 407 \t--- Loss: 0.021\n",
      "Iteration: 408 \t--- Loss: 0.022\n",
      "Iteration: 409 \t--- Loss: 0.020\n",
      "Iteration: 410 \t--- Loss: 0.018\n",
      "Iteration: 411 \t--- Loss: 0.019\n",
      "Iteration: 412 \t--- Loss: 0.020\n",
      "Iteration: 413 \t--- Loss: 0.018\n",
      "Iteration: 414 \t--- Loss: 0.021\n",
      "Iteration: 415 \t--- Loss: 0.021\n",
      "Iteration: 416 \t--- Loss: 0.019\n",
      "Iteration: 417 \t--- Loss: 0.019\n",
      "Iteration: 418 \t--- Loss: 0.019\n",
      "Iteration: 419 \t--- Loss: 0.019\n",
      "Iteration: 420 \t--- Loss: 0.019\n",
      "Iteration: 421 \t--- Loss: 0.021\n",
      "Iteration: 422 \t--- Loss: 0.020\n",
      "Iteration: 423 \t--- Loss: 0.017\n",
      "Iteration: 424 \t--- Loss: 0.018\n",
      "Iteration: 425 \t--- Loss: 0.020\n",
      "Iteration: 426 \t--- Loss: 0.020\n",
      "Iteration: 427 \t--- Loss: 0.020\n",
      "Iteration: 428 \t--- Loss: 0.019\n",
      "Iteration: 429 \t--- Loss: 0.019\n",
      "Iteration: 430 \t--- Loss: 0.019\n",
      "Iteration: 431 \t--- Loss: 0.019\n",
      "Iteration: 432 \t--- Loss: 0.019\n",
      "Iteration: 433 \t--- Loss: 0.018\n",
      "Iteration: 434 \t--- Loss: 0.020\n",
      "Iteration: 435 \t--- Loss: 0.020\n",
      "Iteration: 436 \t--- Loss: 0.018\n",
      "Iteration: 437 \t--- Loss: 0.019\n",
      "Iteration: 438 \t--- Loss: 0.018\n",
      "Iteration: 439 \t--- Loss: 0.019\n",
      "Iteration: 440 \t--- Loss: 0.020\n",
      "Iteration: 441 \t--- Loss: 0.020\n",
      "Iteration: 442 \t--- Loss: 0.018\n",
      "Iteration: 443 \t--- Loss: 0.017\n",
      "Iteration: 444 \t--- Loss: 0.019\n",
      "Iteration: 445 \t--- Loss: 0.020\n",
      "Iteration: 446 \t--- Loss: 0.019\n",
      "Iteration: 447 \t--- Loss: 0.018\n",
      "Iteration: 448 \t--- Loss: 0.019\n",
      "Iteration: 449 \t--- Loss: 0.019\n",
      "Iteration: 450 \t--- Loss: 0.019\n",
      "Iteration: 451 \t--- Loss: 0.018\n",
      "Iteration: 452 \t--- Loss: 0.019\n",
      "Iteration: 453 \t--- Loss: 0.020\n",
      "Iteration: 454 \t--- Loss: 0.019\n",
      "Iteration: 455 \t--- Loss: 0.020\n",
      "Iteration: 456 \t--- Loss: 0.017\n",
      "Iteration: 457 \t--- Loss: 0.018\n",
      "Iteration: 458 \t--- Loss: 0.020\n",
      "Iteration: 459 \t--- Loss: 0.019\n",
      "Iteration: 460 \t--- Loss: 0.018\n",
      "Iteration: 461 \t--- Loss: 0.018\n",
      "Iteration: 462 \t--- Loss: 0.019\n",
      "Iteration: 463 \t--- Loss: 0.018\n",
      "Iteration: 464 \t--- Loss: 0.019\n",
      "Iteration: 465 \t--- Loss: 0.019\n",
      "Iteration: 466 \t--- Loss: 0.019\n",
      "Iteration: 467 \t--- Loss: 0.018\n",
      "Iteration: 468 \t--- Loss: 0.019\n",
      "Iteration: 469 \t--- Loss: 0.020\n",
      "Iteration: 470 \t--- Loss: 0.020\n",
      "Iteration: 471 \t--- Loss: 0.019\n",
      "Iteration: 472 \t--- Loss: 0.020\n",
      "Iteration: 473 \t--- Loss: 0.020\n",
      "Iteration: 474 \t--- Loss: 0.020\n",
      "Iteration: 475 \t--- Loss: 0.018\n",
      "Iteration: 476 \t--- Loss: 0.018\n",
      "Iteration: 477 \t--- Loss: 0.018\n",
      "Iteration: 478 \t--- Loss: 0.019\n",
      "Iteration: 479 \t--- Loss: 0.020\n",
      "Iteration: 480 \t--- Loss: 0.018\n",
      "Iteration: 481 \t--- Loss: 0.017\n",
      "Iteration: 482 \t--- Loss: 0.018\n",
      "Iteration: 483 \t--- Loss: 0.018\n",
      "Iteration: 484 \t--- Loss: 0.019\n",
      "Iteration: 485 \t--- Loss: 0.019\n",
      "Iteration: 486 \t--- Loss: 0.018\n",
      "Iteration: 487 \t--- Loss: 0.017\n",
      "Iteration: 488 \t--- Loss: 0.019\n",
      "Iteration: 489 \t--- Loss: 0.019\n",
      "Iteration: 490 \t--- Loss: 0.018\n",
      "Iteration: 491 \t--- Loss: 0.019\n",
      "Iteration: 492 \t--- Loss: 0.019\n",
      "Iteration: 493 \t--- Loss: 0.018\n",
      "Iteration: 494 \t--- Loss: 0.018\n",
      "Iteration: 495 \t--- Loss: 0.018\n",
      "Iteration: 496 \t--- Loss: 0.019\n",
      "Iteration: 497 \t--- Loss: 0.018\n",
      "Iteration: 498 \t--- Loss: 0.018\n",
      "Iteration: 499 \t--- Loss: 0.018\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.60s/it][Parallel(n_jobs=5)]: Done  35 tasks      | elapsed: 22.0min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  36 tasks      | elapsed: 22.2min\n",
      "  0%|          | 0/1 [09:59<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.038\n",
      "Iteration: 261 \t--- Loss: 0.037\n",
      "Iteration: 262 \t--- Loss: 0.039\n",
      "Iteration: 263 \t--- Loss: 0.040\n",
      "Iteration: 264 \t--- Loss: 0.042\n",
      "Iteration: 265 \t--- Loss: 0.045\n",
      "Iteration: 266 \t--- Loss: 0.041\n",
      "Iteration: 267 \t--- Loss: 0.040\n",
      "Iteration: 268 \t--- Loss: 0.041\n",
      "Iteration: 269 \t--- Loss: 0.037\n",
      "Iteration: 270 \t--- Loss: 0.037\n",
      "Iteration: 271 \t--- Loss: 0.034\n",
      "Iteration: 272 \t--- Loss: 0.037\n",
      "Iteration: 273 \t--- Loss: 0.035\n",
      "Iteration: 274 \t--- Loss: 0.038\n",
      "Iteration: 275 \t--- Loss: 0.036\n",
      "Iteration: 276 \t--- Loss: 0.036\n",
      "Iteration: 277 \t--- Loss: 0.035\n",
      "Iteration: 278 \t--- Loss: 0.036\n",
      "Iteration: 279 \t--- Loss: 0.036\n",
      "Iteration: 280 \t--- Loss: 0.038\n",
      "Iteration: 281 \t--- Loss: 0.037\n",
      "Iteration: 282 \t--- Loss: 0.040\n",
      "Iteration: 283 \t--- Loss: 0.042\n",
      "Iteration: 284 \t--- Loss: 0.041\n",
      "Iteration: 285 \t--- Loss: 0.035\n",
      "Iteration: 286 \t--- Loss: 0.037\n",
      "Iteration: 287 \t--- Loss: 0.037\n",
      "Iteration: 288 \t--- Loss: 0.036\n",
      "Iteration: 289 \t--- Loss: 0.041\n",
      "Iteration: 290 \t--- Loss: 0.037\n",
      "Iteration: 291 \t--- Loss: 0.037\n",
      "Iteration: 292 \t--- Loss: 0.035\n",
      "Iteration: 293 \t--- Loss: 0.037\n",
      "Iteration: 294 \t--- Loss: 0.038\n",
      "Iteration: 295 \t--- Loss: 0.036\n",
      "Iteration: 296 \t--- Loss: 0.035\n",
      "Iteration: 297 \t--- Loss: 0.039\n",
      "Iteration: 298 \t--- Loss: 0.034\n",
      "Iteration: 299 \t--- Loss: 0.036\n",
      "Iteration: 300 \t--- Loss: 0.037\n",
      "Iteration: 301 \t--- Loss: 0.037\n",
      "Iteration: 302 \t--- Loss: 0.035\n",
      "Iteration: 303 \t--- Loss: 0.038\n",
      "Iteration: 304 \t--- Loss: 0.036\n",
      "Iteration: 305 \t--- Loss: 0.040\n",
      "Iteration: 306 \t--- Loss: 0.035\n",
      "Iteration: 307 \t--- Loss: 0.035\n",
      "Iteration: 308 \t--- Loss: 0.038\n",
      "Iteration: 309 \t--- Loss: 0.034\n",
      "Iteration: 310 \t--- Loss: 0.035\n",
      "Iteration: 311 \t--- Loss: 0.033\n",
      "Iteration: 312 \t--- Loss: 0.033\n",
      "Iteration: 313 \t--- Loss: 0.036\n",
      "Iteration: 314 \t--- Loss: 0.037\n",
      "Iteration: 315 \t--- Loss: 0.034\n",
      "Iteration: 316 \t--- Loss: 0.036\n",
      "Iteration: 317 \t--- Loss: 0.034\n",
      "Iteration: 318 \t--- Loss: 0.037\n",
      "Iteration: 319 \t--- Loss: 0.038\n",
      "Iteration: 320 \t--- Loss: 0.034\n",
      "Iteration: 321 \t--- Loss: 0.035\n",
      "Iteration: 322 \t--- Loss: 0.036\n",
      "Iteration: 323 \t--- Loss: 0.034\n",
      "Iteration: 324 \t--- Loss: 0.039\n",
      "Iteration: 325 \t--- Loss: 0.038\n",
      "Iteration: 326 \t--- Loss: 0.036\n",
      "Iteration: 327 \t--- Loss: 0.037\n",
      "Iteration: 328 \t--- Loss: 0.035\n",
      "Iteration: 329 \t--- Loss: 0.035\n",
      "Iteration: 330 \t--- Loss: 0.036\n",
      "Iteration: 331 \t--- Loss: 0.034\n",
      "Iteration: 332 \t--- Loss: 0.034\n",
      "Iteration: 333 \t--- Loss: 0.036\n",
      "Iteration: 334 \t--- Loss: 0.035\n",
      "Iteration: 335 \t--- Loss: 0.035\n",
      "Iteration: 336 \t--- Loss: 0.035\n",
      "Iteration: 337 \t--- Loss: 0.036\n",
      "Iteration: 338 \t--- Loss: 0.036\n",
      "Iteration: 339 \t--- Loss: 0.036\n",
      "Iteration: 340 \t--- Loss: 0.034\n",
      "Iteration: 341 \t--- Loss: 0.036\n",
      "Iteration: 342 \t--- Loss: 0.036\n",
      "Iteration: 343 \t--- Loss: 0.036\n",
      "Iteration: 344 \t--- Loss: 0.036\n",
      "Iteration: 345 \t--- Loss: 0.035\n",
      "Iteration: 346 \t--- Loss: 0.035\n",
      "Iteration: 347 \t--- Loss: 0.034\n",
      "Iteration: 348 \t--- Loss: 0.035\n",
      "Iteration: 349 \t--- Loss: 0.034\n",
      "Iteration: 350 \t--- Loss: 0.032\n",
      "Iteration: 351 \t--- Loss: 0.033\n",
      "Iteration: 352 \t--- Loss: 0.036\n",
      "Iteration: 353 \t--- Loss: 0.034\n",
      "Iteration: 354 \t--- Loss: 0.036\n",
      "Iteration: 355 \t--- Loss: 0.034\n",
      "Iteration: 356 \t--- Loss: 0.034\n",
      "Iteration: 357 \t--- Loss: 0.036\n",
      "Iteration: 358 \t--- Loss: 0.035\n",
      "Iteration: 359 \t--- Loss: 0.034\n",
      "Iteration: 360 \t--- Loss: 0.035\n",
      "Iteration: 361 \t--- Loss: 0.035\n",
      "Iteration: 362 \t--- Loss: 0.036\n",
      "Iteration: 363 \t--- Loss: 0.034\n",
      "Iteration: 364 \t--- Loss: 0.033\n",
      "Iteration: 365 \t--- Loss: 0.034\n",
      "Iteration: 366 \t--- Loss: 0.035\n",
      "Iteration: 367 \t--- Loss: 0.036\n",
      "Iteration: 368 \t--- Loss: 0.034\n",
      "Iteration: 369 \t--- Loss: 0.036\n",
      "Iteration: 370 \t--- Loss: 0.036\n",
      "Iteration: 371 \t--- Loss: 0.034\n",
      "Iteration: 372 \t--- Loss: 0.036\n",
      "Iteration: 373 \t--- Loss: 0.036\n",
      "Iteration: 374 \t--- Loss: 0.034\n",
      "Iteration: 375 \t--- Loss: 0.035\n",
      "Iteration: 376 \t--- Loss: 0.034\n",
      "Iteration: 377 \t--- Loss: 0.036\n",
      "Iteration: 378 \t--- Loss: 0.035\n",
      "Iteration: 379 \t--- Loss: 0.033\n",
      "Iteration: 380 \t--- Loss: 0.035\n",
      "Iteration: 381 \t--- Loss: 0.034\n",
      "Iteration: 382 \t--- Loss: 0.036\n",
      "Iteration: 383 \t--- Loss: 0.033\n",
      "Iteration: 384 \t--- Loss: 0.035\n",
      "Iteration: 385 \t--- Loss: 0.033\n",
      "Iteration: 386 \t--- Loss: 0.037\n",
      "Iteration: 387 \t--- Loss: 0.035\n",
      "Iteration: 388 \t--- Loss: 0.034\n",
      "Iteration: 389 \t--- Loss: 0.035\n",
      "Iteration: 390 \t--- Loss: 0.035\n",
      "Iteration: 391 \t--- Loss: 0.034\n",
      "Iteration: 392 \t--- Loss: 0.035\n",
      "Iteration: 393 \t--- Loss: 0.034\n",
      "Iteration: 394 \t--- Loss: 0.038\n",
      "Iteration: 395 \t--- Loss: 0.035\n",
      "Iteration: 396 \t--- Loss: 0.033\n",
      "Iteration: 397 \t--- Loss: 0.033\n",
      "Iteration: 398 \t--- Loss: 0.034\n",
      "Iteration: 399 \t--- Loss: 0.033\n",
      "Iteration: 400 \t--- Loss: 0.033\n",
      "Iteration: 401 \t--- Loss: 0.036\n",
      "Iteration: 402 \t--- Loss: 0.034\n",
      "Iteration: 403 \t--- Loss: 0.035\n",
      "Iteration: 404 \t--- Loss: 0.034\n",
      "Iteration: 405 \t--- Loss: 0.036\n",
      "Iteration: 406 \t--- Loss: 0.036\n",
      "Iteration: 407 \t--- Loss: 0.036\n",
      "Iteration: 408 \t--- Loss: 0.034\n",
      "Iteration: 409 \t--- Loss: 0.035\n",
      "Iteration: 410 \t--- Loss: 0.034\n",
      "Iteration: 411 \t--- Loss: 0.034\n",
      "Iteration: 412 \t--- Loss: 0.036\n",
      "Iteration: 413 \t--- Loss: 0.037\n",
      "Iteration: 414 \t--- Loss: 0.035\n",
      "Iteration: 415 \t--- Loss: 0.034\n",
      "Iteration: 416 \t--- Loss: 0.034\n",
      "Iteration: 417 \t--- Loss: 0.034\n",
      "Iteration: 418 \t--- Loss: 0.034\n",
      "Iteration: 419 \t--- Loss: 0.036\n",
      "Iteration: 420 \t--- Loss: 0.035\n",
      "Iteration: 421 \t--- Loss: 0.033\n",
      "Iteration: 422 \t--- Loss: 0.035\n",
      "Iteration: 423 \t--- Loss: 0.033\n",
      "Iteration: 424 \t--- Loss: 0.032\n",
      "Iteration: 425 \t--- Loss: 0.035\n",
      "Iteration: 426 \t--- Loss: 0.035\n",
      "Iteration: 427 \t--- Loss: 0.035\n",
      "Iteration: 428 \t--- Loss: 0.033\n",
      "Iteration: 429 \t--- Loss: 0.033\n",
      "Iteration: 430 \t--- Loss: 0.034\n",
      "Iteration: 431 \t--- Loss: 0.037\n",
      "Iteration: 432 \t--- Loss: 0.035\n",
      "Iteration: 433 \t--- Loss: 0.036\n",
      "Iteration: 434 \t--- Loss: 0.035\n",
      "Iteration: 435 \t--- Loss: 0.033\n",
      "Iteration: 436 \t--- Loss: 0.033\n",
      "Iteration: 437 \t--- Loss: 0.035\n",
      "Iteration: 438 \t--- Loss: 0.033\n",
      "Iteration: 439 \t--- Loss: 0.033\n",
      "Iteration: 440 \t--- Loss: 0.035\n",
      "Iteration: 441 \t--- Loss: 0.032\n",
      "Iteration: 442 \t--- Loss: 0.032\n",
      "Iteration: 443 \t--- Loss: 0.034\n",
      "Iteration: 444 \t--- Loss: 0.033\n",
      "Iteration: 445 \t--- Loss: 0.033\n",
      "Iteration: 446 \t--- Loss: 0.030\n",
      "Iteration: 447 \t--- Loss: 0.034\n",
      "Iteration: 448 \t--- Loss: 0.034\n",
      "Iteration: 449 \t--- Loss: 0.037\n",
      "Iteration: 450 \t--- Loss: 0.035\n",
      "Iteration: 451 \t--- Loss: 0.033\n",
      "Iteration: 452 \t--- Loss: 0.033\n",
      "Iteration: 453 \t--- Loss: 0.035\n",
      "Iteration: 454 \t--- Loss: 0.033\n",
      "Iteration: 455 \t--- Loss: 0.034\n",
      "Iteration: 456 \t--- Loss: 0.032\n",
      "Iteration: 457 \t--- Loss: 0.033\n",
      "Iteration: 458 \t--- Loss: 0.034\n",
      "Iteration: 459 \t--- Loss: 0.034\n",
      "Iteration: 460 \t--- Loss: 0.035\n",
      "Iteration: 461 \t--- Loss: 0.033\n",
      "Iteration: 462 \t--- Loss: 0.034\n",
      "Iteration: 463 \t--- Loss: 0.035\n",
      "Iteration: 464 \t--- Loss: 0.035\n",
      "Iteration: 465 \t--- Loss: 0.034\n",
      "Iteration: 466 \t--- Loss: 0.033\n",
      "Iteration: 467 \t--- Loss: 0.034\n",
      "Iteration: 468 \t--- Loss: 0.036\n",
      "Iteration: 469 \t--- Loss: 0.036\n",
      "\n",
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.059\n",
      "Iteration: 1 \t--- Loss: 0.059\n",
      "Iteration: 2 \t--- Loss: 0.052\n",
      "Iteration: 3 \t--- Loss: 0.061\n",
      "Iteration: 4 \t--- Loss: 0.056\n",
      "Iteration: 5 \t--- Loss: 0.057\n",
      "Iteration: 6 \t--- Loss: 0.058\n",
      "Iteration: 7 \t--- Loss: 0.057\n",
      "Iteration: 8 \t--- Loss: 0.056\n",
      "Iteration: 9 \t--- Loss: 0.055\n",
      "Iteration: 10 \t--- Loss: 0.054\n",
      "Iteration: 11 \t--- Loss: 0.054\n",
      "Iteration: 12 \t--- Loss: 0.058\n",
      "Iteration: 13 \t--- Loss: 0.054\n",
      "Iteration: 14 \t--- Loss: 0.055\n",
      "Iteration: 15 \t--- Loss: 0.060\n",
      "Iteration: 16 \t--- Loss: 0.051\n",
      "Iteration: 17 \t--- Loss: 0.055\n",
      "Iteration: 18 \t--- Loss: 0.051\n",
      "Iteration: 19 \t--- Loss: 0.050\n",
      "Iteration: 20 \t--- Loss: 0.055\n",
      "Iteration: 21 \t--- Loss: 0.050\n",
      "Iteration: 22 \t--- Loss: 0.050\n",
      "Iteration: 23 \t--- Loss: 0.050\n",
      "Iteration: 24 \t--- Loss: 0.054\n",
      "Iteration: 25 \t--- Loss: 0.054\n",
      "Iteration: 26 \t--- Loss: 0.055\n",
      "Iteration: 27 \t--- Loss: 0.050\n",
      "Iteration: 28 \t--- Loss: 0.053\n",
      "Iteration: 29 \t--- Loss: 0.048\n",
      "Iteration: 30 \t--- Loss: 0.051\n",
      "Iteration: 31 \t--- Loss: 0.051\n",
      "Iteration: 32 \t--- Loss: 0.049\n",
      "Iteration: 33 \t--- Loss: 0.050\n",
      "Iteration: 34 \t--- Loss: 0.052\n",
      "Iteration: 35 \t--- Loss: 0.048\n",
      "Iteration: 36 \t--- Loss: 0.052\n",
      "Iteration: 37 \t--- Loss: 0.050\n",
      "Iteration: 38 \t--- Loss: 0.049\n",
      "Iteration: 39 \t--- Loss: 0.046\n",
      "Iteration: 40 \t--- Loss: 0.047\n",
      "Iteration: 41 \t--- Loss: 0.051\n",
      "Iteration: 42 \t--- Loss: 0.053\n",
      "Iteration: 43 \t--- Loss: 0.049\n",
      "Iteration: 44 \t--- Loss: 0.049\n",
      "Iteration: 45 \t--- Loss: 0.048\n",
      "Iteration: 46 \t--- Loss: 0.051\n",
      "Iteration: 47 \t--- Loss: 0.052\n",
      "Iteration: 48 \t--- Loss: 0.051\n",
      "Iteration: 49 \t--- Loss: 0.046\n",
      "Iteration: 50 \t--- Loss: 0.046\n",
      "Iteration: 51 \t--- Loss: 0.047\n",
      "Iteration: 52 \t--- Loss: 0.049\n",
      "Iteration: 53 \t--- Loss: 0.051\n",
      "Iteration: 54 \t--- Loss: 0.053\n",
      "Iteration: 55 \t--- Loss: 0.046\n",
      "Iteration: 56 \t--- Loss: 0.048\n",
      "Iteration: 57 \t--- Loss: 0.053\n",
      "Iteration: 58 \t--- Loss: 0.051\n",
      "Iteration: 59 \t--- Loss: 0.048\n",
      "Iteration: 60 \t--- Loss: 0.050\n",
      "Iteration: 61 \t--- Loss: 0.051\n",
      "Iteration: 62 \t--- Loss: 0.047\n",
      "Iteration: 63 \t--- Loss: 0.051\n",
      "Iteration: 64 \t--- Loss: 0.049\n",
      "Iteration: 65 \t--- Loss: 0.047\n",
      "Iteration: 66 \t--- Loss: 0.049\n",
      "Iteration: 67 \t--- Loss: 0.048\n",
      "Iteration: 68 \t--- Loss: 0.050\n",
      "Iteration: 69 \t--- Loss: 0.052\n",
      "Iteration: 70 \t--- Loss: 0.048\n",
      "Iteration: 71 \t--- Loss: 0.047\n",
      "Iteration: 72 \t--- Loss: 0.053\n",
      "Iteration: 73 \t--- Loss: 0.051\n",
      "Iteration: 74 \t--- Loss: 0.050\n",
      "Iteration: 75 \t--- Loss: 0.056\n",
      "Iteration: 76 \t--- Loss: 0.047\n",
      "Iteration: 77 \t--- Loss: 0.047\n",
      "Iteration: 78 \t--- Loss: 0.050\n",
      "Iteration: 79 \t--- Loss: 0.049\n",
      "Iteration: 80 \t--- Loss: 0.048\n",
      "Iteration: 81 \t--- Loss: 0.048\n",
      "Iteration: 82 \t--- Loss: 0.047\n",
      "Iteration: 83 \t--- Loss: 0.045\n",
      "Iteration: 84 \t--- Loss: 0.053\n",
      "Iteration: 85 \t--- Loss: 0.048\n",
      "Iteration: 86 \t--- Loss: 0.044\n",
      "Iteration: 87 \t--- Loss: 0.049\n",
      "Iteration: 88 \t--- Loss: 0.049\n",
      "Iteration: 89 \t--- Loss: 0.049\n",
      "Iteration: 90 \t--- Loss: 0.045\n",
      "Iteration: 91 \t--- Loss: 0.050\n",
      "Iteration: 92 \t--- Loss: 0.047\n",
      "Iteration: 93 \t--- Loss: 0.053\n",
      "Iteration: 94 \t--- Loss: 0.049\n",
      "Iteration: 95 \t--- Loss: 0.048\n",
      "Iteration: 96 \t--- Loss: 0.049\n",
      "Iteration: 97 \t--- Loss: 0.048\n",
      "Iteration: 98 \t--- Loss: 0.048\n",
      "Iteration: 99 \t--- Loss: 0.052\n",
      "Iteration: 100 \t--- Loss: 0.048\n",
      "Iteration: 101 \t--- Loss: 0.051\n",
      "Iteration: 102 \t--- Loss: 0.049\n",
      "Iteration: 103 \t--- Loss: 0.047\n",
      "Iteration: 104 \t--- Loss: 0.047\n",
      "Iteration: 105 \t--- Loss: 0.047\n",
      "Iteration: 106 \t--- Loss: 0.048\n",
      "Iteration: 107 \t--- Loss: 0.049\n",
      "Iteration: 108 \t--- Loss: 0.054\n",
      "Iteration: 109 \t--- Loss: 0.047\n",
      "Iteration: 110 \t--- Loss: 0.048\n",
      "Iteration: 111 \t--- Loss: 0.045\n",
      "Iteration: 112 \t--- Loss: 0.051\n",
      "Iteration: 113 \t--- Loss: 0.048\n",
      "Iteration: 114 \t--- Loss: 0.047\n",
      "Iteration: 115 \t--- Loss: 0.046\n",
      "Iteration: 116 \t--- Loss: 0.048\n",
      "Iteration: 117 \t--- Loss: 0.044\n",
      "Iteration: 118 \t--- Loss: 0.049\n",
      "Iteration: 119 \t--- Loss: 0.048\n",
      "Iteration: 120 \t--- Loss: 0.047\n",
      "Iteration: 121 \t--- Loss: 0.048\n",
      "Iteration: 122 \t--- Loss: 0.046\n",
      "Iteration: 123 \t--- Loss: 0.049\n",
      "Iteration: 124 \t--- Loss: 0.042\n",
      "Iteration: 125 \t--- Loss: 0.049\n",
      "Iteration: 126 \t--- Loss: 0.049\n",
      "Iteration: 127 \t--- Loss: 0.044\n",
      "Iteration: 128 \t--- Loss: 0.048\n",
      "Iteration: 129 \t--- Loss: 0.052\n",
      "Iteration: 130 \t--- Loss: 0.049\n",
      "Iteration: 131 \t--- Loss: 0.043\n",
      "Iteration: 132 \t--- Loss: 0.047\n",
      "Iteration: 133 \t--- Loss: 0.048\n",
      "Iteration: 134 \t--- Loss: 0.047\n",
      "Iteration: 135 \t--- Loss: 0.049\n",
      "Iteration: 136 \t--- Loss: 0.050\n",
      "Iteration: 137 \t--- Loss: 0.050\n",
      "Iteration: 138 \t--- Loss: 0.047\n",
      "Iteration: 139 \t--- Loss: 0.049\n",
      "Iteration: 140 \t--- Loss: 0.049\n",
      "Iteration: 141 \t--- Loss: 0.050\n",
      "Iteration: 142 \t--- Loss: 0.050\n",
      "Iteration: 143 \t--- Loss: 0.049\n",
      "Iteration: 144 \t--- Loss: 0.047\n",
      "Iteration: 145 \t--- Loss: 0.048\n",
      "Iteration: 146 \t--- Loss: 0.049\n",
      "Iteration: 147 \t--- Loss: 0.045\n",
      "Iteration: 148 \t--- Loss: 0.050\n",
      "Iteration: 149 \t--- Loss: 0.042\n",
      "Iteration: 150 \t--- Loss: 0.046\n",
      "Iteration: 151 \t--- Loss: 0.049\n",
      "Iteration: 152 \t--- Loss: 0.049\n",
      "Iteration: 153 \t--- Loss: 0.047\n",
      "Iteration: 154 \t--- Loss: 0.047\n",
      "Iteration: 155 \t--- Loss: 0.044\n",
      "Iteration: 156 \t--- Loss: 0.047\n",
      "Iteration: 157 \t--- Loss: 0.048\n",
      "Iteration: 158 \t--- Loss: 0.046\n",
      "Iteration: 159 \t--- Loss: 0.046\n",
      "Iteration: 160 \t--- Loss: 0.044\n",
      "Iteration: 161 \t--- Loss: 0.048\n",
      "Iteration: 162 \t--- Loss: 0.043\n",
      "Iteration: 163 \t--- Loss: 0.051\n",
      "Iteration: 164 \t--- Loss: 0.048\n",
      "Iteration: 165 \t--- Loss: 0.045\n",
      "Iteration: 166 \t--- Loss: 0.046\n",
      "Iteration: 167 \t--- Loss: 0.046\n",
      "Iteration: 168 \t--- Loss: 0.046\n",
      "Iteration: 169 \t--- Loss: 0.046\n",
      "Iteration: 170 \t--- Loss: 0.045\n",
      "Iteration: 171 \t--- Loss: 0.046\n",
      "Iteration: 172 \t--- Loss: 0.042\n",
      "Iteration: 173 \t--- Loss: 0.046\n",
      "Iteration: 174 \t--- Loss: 0.048\n",
      "Iteration: 175 \t--- Loss: 0.050\n",
      "Iteration: 176 \t--- Loss: 0.043\n",
      "Iteration: 177 \t--- Loss: 0.046\n",
      "Iteration: 178 \t--- Loss: 0.045\n",
      "Iteration: 179 \t--- Loss: 0.043\n",
      "Iteration: 180 \t--- Loss: 0.046\n",
      "Iteration: 181 \t--- Loss: 0.047\n",
      "Iteration: 182 \t--- Loss: 0.045\n",
      "Iteration: 183 \t--- Loss: 0.048\n",
      "Iteration: 184 \t--- Loss: 0.045\n",
      "Iteration: 185 \t--- Loss: 0.048\n",
      "Iteration: 186 \t--- Loss: 0.048\n",
      "Iteration: 187 \t--- Loss: 0.046\n",
      "Iteration: 188 \t--- Loss: 0.041\n",
      "Iteration: 189 \t--- Loss: 0.050\n",
      "Iteration: 190 \t--- Loss: 0.050\n",
      "Iteration: 191 \t--- Loss: 0.047\n",
      "Iteration: 192 \t--- Loss: 0.046\n",
      "Iteration: 193 \t--- Loss: 0.046\n",
      "Iteration: 194 \t--- Loss: 0.047\n",
      "Iteration: 195 \t--- Loss: 0.043\n",
      "Iteration: 196 \t--- Loss: 0.046\n",
      "Iteration: 197 \t--- Loss: 0.045\n",
      "Iteration: 198 \t--- Loss: 0.047\n",
      "Iteration: 199 \t--- Loss: 0.045\n",
      "Iteration: 200 \t--- Loss: 0.045\n",
      "Iteration: 201 \t--- Loss: 0.044\n",
      "Iteration: 202 \t--- Loss: 0.045\n",
      "Iteration: 203 \t--- Loss: 0.051\n",
      "Iteration: 204 \t--- Loss: 0.045\n",
      "Iteration: 205 \t--- Loss: 0.048\n",
      "Iteration: 206 \t--- Loss: 0.045\n",
      "Iteration: 207 \t--- Loss: 0.050\n",
      "Iteration: 208 \t--- Loss: 0.043\n",
      "Iteration: 209 \t--- Loss: 0.044\n",
      "Iteration: 210 \t--- Loss: 0.047\n",
      "Iteration: 211 \t--- Loss: 0.044\n",
      "Iteration: 212 \t--- Loss: 0.046\n",
      "Iteration: 213 \t--- Loss: 0.046\n",
      "Iteration: 214 \t--- Loss: 0.045\n",
      "Iteration: 215 \t--- Loss: 0.050\n",
      "Iteration: 216 \t--- Loss: 0.048\n",
      "Iteration: 217 \t--- Loss: 0.048\n",
      "Iteration: 218 \t--- Loss: 0.050\n",
      "Iteration: 219 \t--- Loss: 0.044\n",
      "Iteration: 220 \t--- Loss: 0.043\n",
      "Iteration: 221 \t--- Loss: 0.050\n",
      "Iteration: 222 \t--- Loss: 0.048\n",
      "Iteration: 223 \t--- Loss: 0.046\n",
      "Iteration: 224 \t--- Loss: 0.053\n",
      "Iteration: 225 \t--- Loss: 0.044\n",
      "Iteration: 226 \t--- Loss: 0.046\n",
      "Iteration: 227 \t--- Loss: 0.044\n",
      "Iteration: 228 \t--- Loss: 0.046\n",
      "Iteration: 229 \t--- Loss: 0.049\n",
      "Iteration: 230 \t--- Loss: 0.044\n",
      "Iteration: 231 \t--- Loss: 0.047\n",
      "Iteration: 232 \t--- Loss: 0.044\n",
      "Iteration: 233 \t--- Loss: 0.046\n",
      "Iteration: 234 \t--- Loss: 0.043\n",
      "Iteration: 235 \t--- Loss: 0.044\n",
      "Iteration: 236 \t--- Loss: 0.047\n",
      "Iteration: 237 \t--- Loss: 0.046\n",
      "Iteration: 238 \t--- Loss: 0.044\n",
      "Iteration: 239 \t--- Loss: 0.043\n",
      "Iteration: 240 \t--- Loss: 0.046\n",
      "Iteration: 241 \t--- Loss: 0.049\n",
      "Iteration: 242 \t--- Loss: 0.045\n",
      "Iteration: 243 \t--- Loss: 0.052\n",
      "Iteration: 244 \t--- Loss: 0.049\n",
      "Iteration: 245 \t--- Loss: 0.045\n",
      "Iteration: 246 \t--- Loss: 0.046\n",
      "Iteration: 247 \t--- Loss: 0.042\n",
      "Iteration: 248 \t--- Loss: 0.046\n",
      "Iteration: 249 \t--- Loss: 0.048\n",
      "Iteration: 250 \t--- Loss: 0.043\n",
      "Iteration: 251 \t--- Loss: 0.043\n",
      "Iteration: 252 \t--- Loss: 0.048\n",
      "Iteration: 253 \t--- Loss: 0.044\n",
      "Iteration: 254 \t--- Loss: 0.045\n",
      "Iteration: 255 \t--- Loss: 0.044\n",
      "Iteration: 256 \t--- Loss: 0.049\n",
      "Iteration: 257 \t--- Loss: 0.047\n",
      "Iteration: 258 \t--- Loss: 0.040\n",
      "Iteration: 259 \t--- Loss: 0.044Iteration: 0 \t--- Loss: 0.079\n",
      "Iteration: 1 \t--- Loss: 0.077\n",
      "Iteration: 2 \t--- Loss: 0.071\n",
      "Iteration: 3 \t--- Loss: 0.077\n",
      "Iteration: 4 \t--- Loss: 0.072\n",
      "Iteration: 5 \t--- Loss: 0.079\n",
      "Iteration: 6 \t--- Loss: 0.069\n",
      "Iteration: 7 \t--- Loss: 0.072\n",
      "Iteration: 8 \t--- Loss: 0.071\n",
      "Iteration: 9 \t--- Loss: 0.072\n",
      "Iteration: 10 \t--- Loss: 0.070\n",
      "Iteration: 11 \t--- Loss: 0.067\n",
      "Iteration: 12 \t--- Loss: 0.072\n",
      "Iteration: 13 \t--- Loss: 0.071\n",
      "Iteration: 14 \t--- Loss: 0.076\n",
      "Iteration: 15 \t--- Loss: 0.076\n",
      "Iteration: 16 \t--- Loss: 0.070\n",
      "Iteration: 17 \t--- Loss: 0.072\n",
      "Iteration: 18 \t--- Loss: 0.075\n",
      "Iteration: 19 \t--- Loss: 0.069\n",
      "Iteration: 20 \t--- Loss: 0.070\n",
      "Iteration: 21 \t--- Loss: 0.070\n",
      "Iteration: 22 \t--- Loss: 0.070\n",
      "Iteration: 23 \t--- Loss: 0.069\n",
      "Iteration: 24 \t--- Loss: 0.068\n",
      "Iteration: 25 \t--- Loss: 0.071\n",
      "Iteration: 26 \t--- Loss: 0.069\n",
      "Iteration: 27 \t--- Loss: 0.070\n",
      "Iteration: 28 \t--- Loss: 0.069\n",
      "Iteration: 29 \t--- Loss: 0.068\n",
      "Iteration: 30 \t--- Loss: 0.072\n",
      "Iteration: 31 \t--- Loss: 0.071\n",
      "Iteration: 32 \t--- Loss: 0.069\n",
      "Iteration: 33 \t--- Loss: 0.070\n",
      "Iteration: 34 \t--- Loss: 0.075\n",
      "Iteration: 35 \t--- Loss: 0.069\n",
      "Iteration: 36 \t--- Loss: 0.068\n",
      "Iteration: 37 \t--- Loss: 0.071\n",
      "Iteration: 38 \t--- Loss: 0.066\n",
      "Iteration: 39 \t--- Loss: 0.071\n",
      "Iteration: 40 \t--- Loss: 0.065\n",
      "Iteration: 41 \t--- Loss: 0.069\n",
      "Iteration: 42 \t--- Loss: 0.067\n",
      "Iteration: 43 \t--- Loss: 0.064\n",
      "Iteration: 44 \t--- Loss: 0.071\n",
      "Iteration: 45 \t--- Loss: 0.069\n",
      "Iteration: 46 \t--- Loss: 0.068\n",
      "Iteration: 47 \t--- Loss: 0.072\n",
      "Iteration: 48 \t--- Loss: 0.070\n",
      "Iteration: 49 \t--- Loss: 0.069\n",
      "Iteration: 50 \t--- Loss: 0.068\n",
      "Iteration: 51 \t--- Loss: 0.066\n",
      "Iteration: 52 \t--- Loss: 0.066\n",
      "Iteration: 53 \t--- Loss: 0.072\n",
      "Iteration: 54 \t--- Loss: 0.068\n",
      "Iteration: 55 \t--- Loss: 0.068\n",
      "Iteration: 56 \t--- Loss: 0.068\n",
      "Iteration: 57 \t--- Loss: 0.071\n",
      "Iteration: 58 \t--- Loss: 0.071\n",
      "Iteration: 59 \t--- Loss: 0.069\n",
      "Iteration: 60 \t--- Loss: 0.066\n",
      "Iteration: 61 \t--- Loss: 0.069\n",
      "Iteration: 62 \t--- Loss: 0.067\n",
      "Iteration: 63 \t--- Loss: 0.067\n",
      "Iteration: 64 \t--- Loss: 0.069\n",
      "Iteration: 65 \t--- Loss: 0.069\n",
      "Iteration: 66 \t--- Loss: 0.066\n",
      "Iteration: 67 \t--- Loss: 0.069\n",
      "Iteration: 68 \t--- Loss: 0.065\n",
      "Iteration: 69 \t--- Loss: 0.066\n",
      "Iteration: 70 \t--- Loss: 0.067\n",
      "Iteration: 71 \t--- Loss: 0.066\n",
      "Iteration: 72 \t--- Loss: 0.065\n",
      "Iteration: 73 \t--- Loss: 0.067\n",
      "Iteration: 74 \t--- Loss: 0.067\n",
      "Iteration: 75 \t--- Loss: 0.068\n",
      "Iteration: 76 \t--- Loss: 0.068\n",
      "Iteration: 77 \t--- Loss: 0.067\n",
      "Iteration: 78 \t--- Loss: 0.064\n",
      "Iteration: 79 \t--- Loss: 0.067\n",
      "Iteration: 80 \t--- Loss: 0.069\n",
      "Iteration: 81 \t--- Loss: 0.065\n",
      "Iteration: 82 \t--- Loss: 0.069\n",
      "Iteration: 83 \t--- Loss: 0.066\n",
      "Iteration: 84 \t--- Loss: 0.065\n",
      "Iteration: 85 \t--- Loss: 0.068\n",
      "Iteration: 86 \t--- Loss: 0.067\n",
      "Iteration: 87 \t--- Loss: 0.069\n",
      "Iteration: 88 \t--- Loss: 0.070\n",
      "Iteration: 89 \t--- Loss: 0.068\n",
      "Iteration: 90 \t--- Loss: 0.068\n",
      "Iteration: 91 \t--- Loss: 0.070\n",
      "Iteration: 92 \t--- Loss: 0.066\n",
      "Iteration: 93 \t--- Loss: 0.069\n",
      "Iteration: 94 \t--- Loss: 0.064\n",
      "Iteration: 95 \t--- Loss: 0.065\n",
      "Iteration: 96 \t--- Loss: 0.066\n",
      "Iteration: 97 \t--- Loss: 0.064\n",
      "Iteration: 98 \t--- Loss: 0.069\n",
      "Iteration: 99 \t--- Loss: 0.065\n",
      "Iteration: 100 \t--- Loss: 0.064\n",
      "Iteration: 101 \t--- Loss: 0.065\n",
      "Iteration: 102 \t--- Loss: 0.070\n",
      "Iteration: 103 \t--- Loss: 0.064\n",
      "Iteration: 104 \t--- Loss: 0.065\n",
      "Iteration: 105 \t--- Loss: 0.067\n",
      "Iteration: 106 \t--- Loss: 0.066\n",
      "Iteration: 107 \t--- Loss: 0.068\n",
      "Iteration: 108 \t--- Loss: 0.062\n",
      "Iteration: 109 \t--- Loss: 0.066\n",
      "Iteration: 110 \t--- Loss: 0.065\n",
      "Iteration: 111 \t--- Loss: 0.065\n",
      "Iteration: 112 \t--- Loss: 0.064\n",
      "Iteration: 113 \t--- Loss: 0.071\n",
      "Iteration: 114 \t--- Loss: 0.066\n",
      "Iteration: 115 \t--- Loss: 0.063\n",
      "Iteration: 116 \t--- Loss: 0.067\n",
      "Iteration: 117 \t--- Loss: 0.065\n",
      "Iteration: 118 \t--- Loss: 0.062\n",
      "Iteration: 119 \t--- Loss: 0.069\n",
      "Iteration: 120 \t--- Loss: 0.067\n",
      "Iteration: 121 \t--- Loss: 0.062\n",
      "Iteration: 122 \t--- Loss: 0.066\n",
      "Iteration: 123 \t--- Loss: 0.067\n",
      "Iteration: 124 \t--- Loss: 0.064\n",
      "Iteration: 125 \t--- Loss: 0.064\n",
      "Iteration: 126 \t--- Loss: 0.064\n",
      "Iteration: 127 \t--- Loss: 0.066\n",
      "Iteration: 128 \t--- Loss: 0.065\n",
      "Iteration: 129 \t--- Loss: 0.068\n",
      "Iteration: 130 \t--- Loss: 0.061\n",
      "Iteration: 131 \t--- Loss: 0.064\n",
      "Iteration: 132 \t--- Loss: 0.062\n",
      "Iteration: 133 \t--- Loss: 0.066\n",
      "Iteration: 134 \t--- Loss: 0.068\n",
      "Iteration: 135 \t--- Loss: 0.064\n",
      "Iteration: 136 \t--- Loss: 0.065\n",
      "Iteration: 137 \t--- Loss: 0.065\n",
      "Iteration: 138 \t--- Loss: 0.063\n",
      "Iteration: 139 \t--- Loss: 0.067\n",
      "Iteration: 140 \t--- Loss: 0.061\n",
      "Iteration: 141 \t--- Loss: 0.067\n",
      "Iteration: 142 \t--- Loss: 0.058\n",
      "Iteration: 143 \t--- Loss: 0.062\n",
      "Iteration: 144 \t--- Loss: 0.061\n",
      "Iteration: 145 \t--- Loss: 0.064\n",
      "Iteration: 146 \t--- Loss: 0.063\n",
      "Iteration: 147 \t--- Loss: 0.061\n",
      "Iteration: 148 \t--- Loss: 0.063\n",
      "Iteration: 149 \t--- Loss: 0.063\n",
      "Iteration: 150 \t--- Loss: 0.062\n",
      "Iteration: 151 \t--- Loss: 0.067\n",
      "Iteration: 152 \t--- Loss: 0.063\n",
      "Iteration: 153 \t--- Loss: 0.064\n",
      "Iteration: 154 \t--- Loss: 0.065\n",
      "Iteration: 155 \t--- Loss: 0.058\n",
      "Iteration: 156 \t--- Loss: 0.065\n",
      "Iteration: 157 \t--- Loss: 0.069\n",
      "Iteration: 158 \t--- Loss: 0.064\n",
      "Iteration: 159 \t--- Loss: 0.064\n",
      "Iteration: 160 \t--- Loss: 0.063\n",
      "Iteration: 161 \t--- Loss: 0.065\n",
      "Iteration: 162 \t--- Loss: 0.064\n",
      "Iteration: 163 \t--- Loss: 0.063\n",
      "Iteration: 164 \t--- Loss: 0.060\n",
      "Iteration: 165 \t--- Loss: 0.066\n",
      "Iteration: 166 \t--- Loss: 0.063\n",
      "Iteration: 167 \t--- Loss: 0.058\n",
      "Iteration: 168 \t--- Loss: 0.066\n",
      "Iteration: 169 \t--- Loss: 0.064\n",
      "Iteration: 170 \t--- Loss: 0.062\n",
      "Iteration: 171 \t--- Loss: 0.062\n",
      "Iteration: 172 \t--- Loss: 0.062\n",
      "Iteration: 173 \t--- Loss: 0.065\n",
      "Iteration: 174 \t--- Loss: 0.061\n",
      "Iteration: 175 \t--- Loss: 0.064\n",
      "Iteration: 176 \t--- Loss: 0.063\n",
      "Iteration: 177 \t--- Loss: 0.065\n",
      "Iteration: 178 \t--- Loss: 0.064\n",
      "Iteration: 179 \t--- Loss: 0.060\n",
      "Iteration: 180 \t--- Loss: 0.061\n",
      "Iteration: 181 \t--- Loss: 0.058\n",
      "Iteration: 182 \t--- Loss: 0.064\n",
      "Iteration: 183 \t--- Loss: 0.063\n",
      "Iteration: 184 \t--- Loss: 0.068\n",
      "Iteration: 185 \t--- Loss: 0.059\n",
      "Iteration: 186 \t--- Loss: 0.064\n",
      "Iteration: 187 \t--- Loss: 0.058\n",
      "Iteration: 188 \t--- Loss: 0.059\n",
      "Iteration: 189 \t--- Loss: 0.064\n",
      "Iteration: 190 \t--- Loss: 0.063\n",
      "Iteration: 191 \t--- Loss: 0.064\n",
      "Iteration: 192 \t--- Loss: 0.064\n",
      "Iteration: 193 \t--- Loss: 0.061\n",
      "Iteration: 194 \t--- Loss: 0.057\n",
      "Iteration: 195 \t--- Loss: 0.063\n",
      "Iteration: 196 \t--- Loss: 0.060\n",
      "Iteration: 197 \t--- Loss: 0.063\n",
      "Iteration: 198 \t--- Loss: 0.059\n",
      "Iteration: 199 \t--- Loss: 0.061\n",
      "Iteration: 200 \t--- Loss: 0.063\n",
      "Iteration: 201 \t--- Loss: 0.060\n",
      "Iteration: 202 \t--- Loss: 0.063\n",
      "Iteration: 203 \t--- Loss: 0.058\n",
      "Iteration: 204 \t--- Loss: 0.060\n",
      "Iteration: 205 \t--- Loss: 0.060\n",
      "Iteration: 206 \t--- Loss: 0.058\n",
      "Iteration: 207 \t--- Loss: 0.060\n",
      "Iteration: 208 \t--- Loss: 0.061\n",
      "Iteration: 209 \t--- Loss: 0.061\n",
      "Iteration: 210 \t--- Loss: 0.058\n",
      "Iteration: 211 \t--- Loss: 0.062\n",
      "Iteration: 212 \t--- Loss: 0.061\n",
      "Iteration: 213 \t--- Loss: 0.059\n",
      "Iteration: 214 \t--- Loss: 0.057\n",
      "Iteration: 215 \t--- Loss: 0.064\n",
      "Iteration: 216 \t--- Loss: 0.062\n",
      "Iteration: 217 \t--- Loss: 0.060\n",
      "Iteration: 218 \t--- Loss: 0.062\n",
      "Iteration: 219 \t--- Loss: 0.056\n",
      "Iteration: 220 \t--- Loss: 0.058\n",
      "Iteration: 221 \t--- Loss: 0.058\n",
      "Iteration: 222 \t--- Loss: 0.056\n",
      "Iteration: 223 \t--- Loss: 0.059\n",
      "Iteration: 224 \t--- Loss: 0.061\n",
      "Iteration: 225 \t--- Loss: 0.057\n",
      "Iteration: 226 \t--- Loss: 0.055\n",
      "Iteration: 227 \t--- Loss: 0.058\n",
      "Iteration: 228 \t--- Loss: 0.055\n",
      "Iteration: 229 \t--- Loss: 0.048\n",
      "Iteration: 230 \t--- Loss: 0.053\n",
      "Iteration: 231 \t--- Loss: 0.053\n",
      "Iteration: 232 \t--- Loss: 0.053\n",
      "Iteration: 233 \t--- Loss: 0.052\n",
      "Iteration: 234 \t--- Loss: 0.052\n",
      "Iteration: 235 \t--- Loss: 0.045\n",
      "Iteration: 236 \t--- Loss: 0.045\n",
      "Iteration: 237 \t--- Loss: 0.045\n",
      "Iteration: 238 \t--- Loss: 0.046\n",
      "Iteration: 239 \t--- Loss: 0.048\n",
      "Iteration: 240 \t--- Loss: 0.045\n",
      "Iteration: 241 \t--- Loss: 0.048\n",
      "Iteration: 242 \t--- Loss: 0.042\n",
      "Iteration: 243 \t--- Loss: 0.045\n",
      "Iteration: 244 \t--- Loss: 0.045\n",
      "Iteration: 245 \t--- Loss: 0.043\n",
      "Iteration: 246 \t--- Loss: 0.045\n",
      "Iteration: 247 \t--- Loss: 0.044\n",
      "Iteration: 248 \t--- Loss: 0.042\n",
      "Iteration: 249 \t--- Loss: 0.047\n",
      "Iteration: 250 \t--- Loss: 0.040\n",
      "Iteration: 251 \t--- Loss: 0.047\n",
      "Iteration: 252 \t--- Loss: 0.042\n",
      "Iteration: 253 \t--- Loss: 0.040\n",
      "Iteration: 254 \t--- Loss: 0.042\n",
      "Iteration: 255 \t--- Loss: 0.041\n",
      "Iteration: 256 \t--- Loss: 0.041\n",
      "Iteration: 257 \t--- Loss: 0.043\n",
      "Iteration: 258 \t--- Loss: 0.043\n",
      "Iteration: 259 \t--- Loss: 0.040"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:41<00:00, 101.16s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.045\n",
      "Iteration: 261 \t--- Loss: 0.046\n",
      "Iteration: 262 \t--- Loss: 0.047\n",
      "Iteration: 263 \t--- Loss: 0.047\n",
      "Iteration: 264 \t--- Loss: 0.047\n",
      "Iteration: 265 \t--- Loss: 0.046\n",
      "Iteration: 266 \t--- Loss: 0.049\n",
      "Iteration: 267 \t--- Loss: 0.044\n",
      "Iteration: 268 \t--- Loss: 0.047\n",
      "Iteration: 269 \t--- Loss: 0.046\n",
      "Iteration: 270 \t--- Loss: 0.042\n",
      "Iteration: 271 \t--- Loss: 0.044\n",
      "Iteration: 272 \t--- Loss: 0.039\n",
      "Iteration: 273 \t--- Loss: 0.044\n",
      "Iteration: 274 \t--- Loss: 0.045\n",
      "Iteration: 275 \t--- Loss: 0.044\n",
      "Iteration: 276 \t--- Loss: 0.045\n",
      "Iteration: 277 \t--- Loss: 0.043\n",
      "Iteration: 278 \t--- Loss: 0.042\n",
      "Iteration: 279 \t--- Loss: 0.046\n",
      "Iteration: 280 \t--- Loss: 0.044\n",
      "Iteration: 281 \t--- Loss: 0.044\n",
      "Iteration: 282 \t--- Loss: 0.043\n",
      "Iteration: 283 \t--- Loss: 0.040\n",
      "Iteration: 284 \t--- Loss: 0.040\n",
      "Iteration: 285 \t--- Loss: 0.040\n",
      "Iteration: 286 \t--- Loss: 0.043\n",
      "Iteration: 287 \t--- Loss: 0.043\n",
      "Iteration: 288 \t--- Loss: 0.043\n",
      "Iteration: 289 \t--- Loss: 0.041\n",
      "Iteration: 290 \t--- Loss: 0.043\n",
      "Iteration: 291 \t--- Loss: 0.044\n",
      "Iteration: 292 \t--- Loss: 0.045\n",
      "Iteration: 293 \t--- Loss: 0.042\n",
      "Iteration: 294 \t--- Loss: 0.049\n",
      "Iteration: 295 \t--- Loss: 0.046\n",
      "Iteration: 296 \t--- Loss: 0.042\n",
      "Iteration: 297 \t--- Loss: 0.045\n",
      "Iteration: 298 \t--- Loss: 0.042\n",
      "Iteration: 299 \t--- Loss: 0.042\n",
      "Iteration: 300 \t--- Loss: 0.043\n",
      "Iteration: 301 \t--- Loss: 0.042\n",
      "Iteration: 302 \t--- Loss: 0.042\n",
      "Iteration: 303 \t--- Loss: 0.044\n",
      "Iteration: 304 \t--- Loss: 0.044\n",
      "Iteration: 305 \t--- Loss: 0.041\n",
      "Iteration: 306 \t--- Loss: 0.043\n",
      "Iteration: 307 \t--- Loss: 0.041\n",
      "Iteration: 308 \t--- Loss: 0.044\n",
      "Iteration: 309 \t--- Loss: 0.046\n",
      "Iteration: 310 \t--- Loss: 0.044\n",
      "Iteration: 311 \t--- Loss: 0.041\n",
      "Iteration: 312 \t--- Loss: 0.042\n",
      "Iteration: 313 \t--- Loss: 0.036\n",
      "Iteration: 314 \t--- Loss: 0.042\n",
      "Iteration: 315 \t--- Loss: 0.044\n",
      "Iteration: 316 \t--- Loss: 0.042\n",
      "Iteration: 317 \t--- Loss: 0.041\n",
      "Iteration: 318 \t--- Loss: 0.051\n",
      "Iteration: 319 \t--- Loss: 0.047\n",
      "Iteration: 320 \t--- Loss: 0.043\n",
      "Iteration: 321 \t--- Loss: 0.041\n",
      "Iteration: 322 \t--- Loss: 0.040\n",
      "Iteration: 323 \t--- Loss: 0.041\n",
      "Iteration: 324 \t--- Loss: 0.042\n",
      "Iteration: 325 \t--- Loss: 0.046\n",
      "Iteration: 326 \t--- Loss: 0.040\n",
      "Iteration: 327 \t--- Loss: 0.041\n",
      "Iteration: 328 \t--- Loss: 0.043\n",
      "Iteration: 329 \t--- Loss: 0.045\n",
      "Iteration: 330 \t--- Loss: 0.043\n",
      "Iteration: 331 \t--- Loss: 0.044\n",
      "Iteration: 332 \t--- Loss: 0.044\n",
      "Iteration: 333 \t--- Loss: 0.041\n",
      "Iteration: 334 \t--- Loss: 0.040\n",
      "Iteration: 335 \t--- Loss: 0.039\n",
      "Iteration: 336 \t--- Loss: 0.041\n",
      "Iteration: 337 \t--- Loss: 0.040\n",
      "Iteration: 338 \t--- Loss: 0.044\n",
      "Iteration: 339 \t--- Loss: 0.042\n",
      "Iteration: 340 \t--- Loss: 0.047\n",
      "Iteration: 341 \t--- Loss: 0.041\n",
      "Iteration: 342 \t--- Loss: 0.043\n",
      "Iteration: 343 \t--- Loss: 0.045\n",
      "Iteration: 344 \t--- Loss: 0.039\n",
      "Iteration: 345 \t--- Loss: 0.039\n",
      "Iteration: 346 \t--- Loss: 0.039\n",
      "Iteration: 347 \t--- Loss: 0.041\n",
      "Iteration: 348 \t--- Loss: 0.040\n",
      "Iteration: 349 \t--- Loss: 0.043\n",
      "Iteration: 350 \t--- Loss: 0.043\n",
      "Iteration: 351 \t--- Loss: 0.042\n",
      "Iteration: 352 \t--- Loss: 0.042\n",
      "Iteration: 353 \t--- Loss: 0.041\n",
      "Iteration: 354 \t--- Loss: 0.043\n",
      "Iteration: 355 \t--- Loss: 0.043\n",
      "Iteration: 356 \t--- Loss: 0.039\n",
      "Iteration: 357 \t--- Loss: 0.039\n",
      "Iteration: 358 \t--- Loss: 0.041\n",
      "Iteration: 359 \t--- Loss: 0.041\n",
      "Iteration: 360 \t--- Loss: 0.041\n",
      "Iteration: 361 \t--- Loss: 0.040\n",
      "Iteration: 362 \t--- Loss: 0.038\n",
      "Iteration: 363 \t--- Loss: 0.036\n",
      "Iteration: 364 \t--- Loss: 0.039\n",
      "Iteration: 365 \t--- Loss: 0.043\n",
      "Iteration: 366 \t--- Loss: 0.038\n",
      "Iteration: 367 \t--- Loss: 0.038\n",
      "Iteration: 368 \t--- Loss: 0.039\n",
      "Iteration: 369 \t--- Loss: 0.041\n",
      "Iteration: 370 \t--- Loss: 0.041\n",
      "Iteration: 371 \t--- Loss: 0.038\n",
      "Iteration: 372 \t--- Loss: 0.041\n",
      "Iteration: 373 \t--- Loss: 0.041\n",
      "Iteration: 374 \t--- Loss: 0.041\n",
      "Iteration: 375 \t--- Loss: 0.040\n",
      "Iteration: 376 \t--- Loss: 0.041\n",
      "Iteration: 377 \t--- Loss: 0.043\n",
      "Iteration: 378 \t--- Loss: 0.037\n",
      "Iteration: 379 \t--- Loss: 0.040\n",
      "Iteration: 380 \t--- Loss: 0.037\n",
      "Iteration: 381 \t--- Loss: 0.036\n",
      "Iteration: 382 \t--- Loss: 0.040\n",
      "Iteration: 383 \t--- Loss: 0.039\n",
      "Iteration: 384 \t--- Loss: 0.039\n",
      "Iteration: 385 \t--- Loss: 0.039\n",
      "Iteration: 386 \t--- Loss: 0.035\n",
      "Iteration: 387 \t--- Loss: 0.039\n",
      "Iteration: 388 \t--- Loss: 0.035\n",
      "Iteration: 389 \t--- Loss: 0.035\n",
      "Iteration: 390 \t--- Loss: 0.037\n",
      "Iteration: 391 \t--- Loss: 0.037\n",
      "Iteration: 392 \t--- Loss: 0.038\n",
      "Iteration: 393 \t--- Loss: 0.036\n",
      "Iteration: 394 \t--- Loss: 0.036\n",
      "Iteration: 395 \t--- Loss: 0.036\n",
      "Iteration: 396 \t--- Loss: 0.037\n",
      "Iteration: 397 \t--- Loss: 0.036\n",
      "Iteration: 398 \t--- Loss: 0.035\n",
      "Iteration: 399 \t--- Loss: 0.033\n",
      "Iteration: 400 \t--- Loss: 0.037\n",
      "Iteration: 401 \t--- Loss: 0.035\n",
      "Iteration: 402 \t--- Loss: 0.031\n",
      "Iteration: 403 \t--- Loss: 0.036\n",
      "Iteration: 404 \t--- Loss: 0.036\n",
      "Iteration: 405 \t--- Loss: 0.035\n",
      "Iteration: 406 \t--- Loss: 0.034\n",
      "Iteration: 407 \t--- Loss: 0.034\n",
      "Iteration: 408 \t--- Loss: 0.034\n",
      "Iteration: 409 \t--- Loss: 0.032\n",
      "Iteration: 410 \t--- Loss: 0.032\n",
      "Iteration: 411 \t--- Loss: 0.034\n",
      "Iteration: 412 \t--- Loss: 0.033\n",
      "Iteration: 413 \t--- Loss: 0.031\n",
      "Iteration: 414 \t--- Loss: 0.032\n",
      "Iteration: 415 \t--- Loss: 0.032\n",
      "Iteration: 416 \t--- Loss: 0.033\n",
      "Iteration: 417 \t--- Loss: 0.032\n",
      "Iteration: 418 \t--- Loss: 0.028\n",
      "Iteration: 419 \t--- Loss: 0.029\n",
      "Iteration: 420 \t--- Loss: 0.030\n",
      "Iteration: 421 \t--- Loss: 0.031\n",
      "Iteration: 422 \t--- Loss: 0.028\n",
      "Iteration: 423 \t--- Loss: 0.029\n",
      "Iteration: 424 \t--- Loss: 0.028\n",
      "Iteration: 425 \t--- Loss: 0.027\n",
      "Iteration: 426 \t--- Loss: 0.032\n",
      "Iteration: 427 \t--- Loss: 0.028\n",
      "Iteration: 428 \t--- Loss: 0.030\n",
      "Iteration: 429 \t--- Loss: 0.030\n",
      "Iteration: 430 \t--- Loss: 0.027\n",
      "Iteration: 431 \t--- Loss: 0.028\n",
      "Iteration: 432 \t--- Loss: 0.026\n",
      "Iteration: 433 \t--- Loss: 0.031\n",
      "Iteration: 434 \t--- Loss: 0.025\n",
      "Iteration: 435 \t--- Loss: 0.026\n",
      "Iteration: 436 \t--- Loss: 0.034\n",
      "Iteration: 437 \t--- Loss: 0.027\n",
      "Iteration: 438 \t--- Loss: 0.029\n",
      "Iteration: 439 \t--- Loss: 0.027\n",
      "Iteration: 440 \t--- Loss: 0.028\n",
      "Iteration: 441 \t--- Loss: 0.029\n",
      "Iteration: 442 \t--- Loss: 0.028\n",
      "Iteration: 443 \t--- Loss: 0.026\n",
      "Iteration: 444 \t--- Loss: 0.026\n",
      "Iteration: 445 \t--- Loss: 0.027\n",
      "Iteration: 446 \t--- Loss: 0.025\n",
      "Iteration: 447 \t--- Loss: 0.026\n",
      "Iteration: 448 \t--- Loss: 0.030\n",
      "Iteration: 449 \t--- Loss: 0.025\n",
      "Iteration: 450 \t--- Loss: 0.025\n",
      "Iteration: 451 \t--- Loss: 0.024\n",
      "Iteration: 452 \t--- Loss: 0.026\n",
      "Iteration: 453 \t--- Loss: 0.027\n",
      "Iteration: 454 \t--- Loss: 0.026\n",
      "Iteration: 455 \t--- Loss: 0.026\n",
      "Iteration: 456 \t--- Loss: 0.029\n",
      "Iteration: 457 \t--- Loss: 0.024\n",
      "Iteration: 458 \t--- Loss: 0.028\n",
      "Iteration: 459 \t--- Loss: 0.028\n",
      "Iteration: 460 \t--- Loss: 0.026\n",
      "Iteration: 461 \t--- Loss: 0.028\n",
      "Iteration: 462 \t--- Loss: 0.025\n",
      "Iteration: 463 \t--- Loss: 0.027\n",
      "Iteration: 464 \t--- Loss: 0.027\n",
      "Iteration: 465 \t--- Loss: 0.028\n",
      "Iteration: 466 \t--- Loss: 0.025\n",
      "Iteration: 467 \t--- Loss: 0.028\n",
      "Iteration: 468 \t--- Loss: 0.023\n",
      "Iteration: 469 \t--- Loss: 0.025\n",
      "Iteration: 470 \t--- Loss: 0.026\n",
      "Iteration: 471 \t--- Loss: 0.024\n",
      "Iteration: 472 \t--- Loss: 0.023\n",
      "Iteration: 473 \t--- Loss: 0.023\n",
      "Iteration: 474 \t--- Loss: 0.024\n",
      "Iteration: 475 \t--- Loss: 0.023\n",
      "Iteration: 476 \t--- Loss: 0.025\n",
      "Iteration: 477 \t--- Loss: 0.025\n",
      "Iteration: 478 \t--- Loss: 0.024\n",
      "Iteration: 479 \t--- Loss: 0.023\n",
      "Iteration: 480 \t--- Loss: 0.025\n",
      "Iteration: 481 \t--- Loss: 0.025\n",
      "Iteration: 482 \t--- Loss: 0.026\n",
      "Iteration: 483 \t--- Loss: 0.024\n",
      "Iteration: 484 \t--- Loss: 0.022\n",
      "Iteration: 485 \t--- Loss: 0.025\n",
      "Iteration: 486 \t--- Loss: 0.024\n",
      "Iteration: 487 \t--- Loss: 0.022\n",
      "Iteration: 488 \t--- Loss: 0.024\n",
      "Iteration: 489 \t--- Loss: 0.025\n",
      "Iteration: 490 \t--- Loss: 0.023\n",
      "Iteration: 491 \t--- Loss: 0.023\n",
      "Iteration: 492 \t--- Loss: 0.022\n",
      "Iteration: 493 \t--- Loss: 0.023\n",
      "Iteration: 494 \t--- Loss: 0.023\n",
      "Iteration: 495 \t--- Loss: 0.024\n",
      "Iteration: 496 \t--- Loss: 0.025\n",
      "Iteration: 497 \t--- Loss: 0.019\n",
      "Iteration: 498 \t--- Loss: 0.023\n",
      "Iteration: 499 \t--- Loss: 0.022\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:33<00:00, 93.16s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.042\n",
      "Iteration: 261 \t--- Loss: 0.041\n",
      "Iteration: 262 \t--- Loss: 0.040\n",
      "Iteration: 263 \t--- Loss: 0.041\n",
      "Iteration: 264 \t--- Loss: 0.039\n",
      "Iteration: 265 \t--- Loss: 0.039\n",
      "Iteration: 266 \t--- Loss: 0.037\n",
      "Iteration: 267 \t--- Loss: 0.043\n",
      "Iteration: 268 \t--- Loss: 0.040\n",
      "Iteration: 269 \t--- Loss: 0.039\n",
      "Iteration: 270 \t--- Loss: 0.041\n",
      "Iteration: 271 \t--- Loss: 0.040\n",
      "Iteration: 272 \t--- Loss: 0.041\n",
      "Iteration: 273 \t--- Loss: 0.043\n",
      "Iteration: 274 \t--- Loss: 0.038\n",
      "Iteration: 275 \t--- Loss: 0.040\n",
      "Iteration: 276 \t--- Loss: 0.040\n",
      "Iteration: 277 \t--- Loss: 0.041\n",
      "Iteration: 278 \t--- Loss: 0.042\n",
      "Iteration: 279 \t--- Loss: 0.037\n",
      "Iteration: 280 \t--- Loss: 0.038\n",
      "Iteration: 281 \t--- Loss: 0.036\n",
      "Iteration: 282 \t--- Loss: 0.038\n",
      "Iteration: 283 \t--- Loss: 0.039\n",
      "Iteration: 284 \t--- Loss: 0.041\n",
      "Iteration: 285 \t--- Loss: 0.038\n",
      "Iteration: 286 \t--- Loss: 0.038\n",
      "Iteration: 287 \t--- Loss: 0.040\n",
      "Iteration: 288 \t--- Loss: 0.035\n",
      "Iteration: 289 \t--- Loss: 0.038\n",
      "Iteration: 290 \t--- Loss: 0.039\n",
      "Iteration: 291 \t--- Loss: 0.035\n",
      "Iteration: 292 \t--- Loss: 0.040\n",
      "Iteration: 293 \t--- Loss: 0.038\n",
      "Iteration: 294 \t--- Loss: 0.037\n",
      "Iteration: 295 \t--- Loss: 0.041\n",
      "Iteration: 296 \t--- Loss: 0.036\n",
      "Iteration: 297 \t--- Loss: 0.039\n",
      "Iteration: 298 \t--- Loss: 0.036\n",
      "Iteration: 299 \t--- Loss: 0.038\n",
      "Iteration: 300 \t--- Loss: 0.038\n",
      "Iteration: 301 \t--- Loss: 0.037\n",
      "Iteration: 302 \t--- Loss: 0.037\n",
      "Iteration: 303 \t--- Loss: 0.039\n",
      "Iteration: 304 \t--- Loss: 0.036\n",
      "Iteration: 305 \t--- Loss: 0.038\n",
      "Iteration: 306 \t--- Loss: 0.036\n",
      "Iteration: 307 \t--- Loss: 0.037\n",
      "Iteration: 308 \t--- Loss: 0.036\n",
      "Iteration: 309 \t--- Loss: 0.035\n",
      "Iteration: 310 \t--- Loss: 0.034\n",
      "Iteration: 311 \t--- Loss: 0.035\n",
      "Iteration: 312 \t--- Loss: 0.034\n",
      "Iteration: 313 \t--- Loss: 0.035\n",
      "Iteration: 314 \t--- Loss: 0.034\n",
      "Iteration: 315 \t--- Loss: 0.037\n",
      "Iteration: 316 \t--- Loss: 0.035\n",
      "Iteration: 317 \t--- Loss: 0.033\n",
      "Iteration: 318 \t--- Loss: 0.037\n",
      "Iteration: 319 \t--- Loss: 0.035\n",
      "Iteration: 320 \t--- Loss: 0.032\n",
      "Iteration: 321 \t--- Loss: 0.036\n",
      "Iteration: 322 \t--- Loss: 0.039\n",
      "Iteration: 323 \t--- Loss: 0.033\n",
      "Iteration: 324 \t--- Loss: 0.034\n",
      "Iteration: 325 \t--- Loss: 0.033\n",
      "Iteration: 326 \t--- Loss: 0.036\n",
      "Iteration: 327 \t--- Loss: 0.036\n",
      "Iteration: 328 \t--- Loss: 0.036\n",
      "Iteration: 329 \t--- Loss: 0.039\n",
      "Iteration: 330 \t--- Loss: 0.036\n",
      "Iteration: 331 \t--- Loss: 0.044\n",
      "Iteration: 332 \t--- Loss: 0.051\n",
      "Iteration: 333 \t--- Loss: 0.047\n",
      "Iteration: 334 \t--- Loss: 0.067\n",
      "Iteration: 335 \t--- Loss: 0.039\n",
      "Iteration: 336 \t--- Loss: 0.044\n",
      "Iteration: 337 \t--- Loss: 0.049\n",
      "Iteration: 338 \t--- Loss: 0.068\n",
      "Iteration: 339 \t--- Loss: 0.039\n",
      "Iteration: 340 \t--- Loss: 0.040\n",
      "Iteration: 341 \t--- Loss: 0.040\n",
      "Iteration: 342 \t--- Loss: 0.044\n",
      "Iteration: 343 \t--- Loss: 0.050\n",
      "Iteration: 344 \t--- Loss: 0.071\n",
      "Iteration: 345 \t--- Loss: 0.031\n",
      "Iteration: 346 \t--- Loss: 0.034\n",
      "Iteration: 347 \t--- Loss: 0.039\n",
      "Iteration: 348 \t--- Loss: 0.035\n",
      "Iteration: 349 \t--- Loss: 0.040\n",
      "Iteration: 350 \t--- Loss: 0.043\n",
      "Iteration: 351 \t--- Loss: 0.045\n",
      "Iteration: 352 \t--- Loss: 0.063\n",
      "Iteration: 353 \t--- Loss: 0.040\n",
      "Iteration: 354 \t--- Loss: 0.044\n",
      "Iteration: 355 \t--- Loss: 0.042\n",
      "Iteration: 356 \t--- Loss: 0.053\n",
      "Iteration: 357 \t--- Loss: 0.039\n",
      "Iteration: 358 \t--- Loss: 0.042\n",
      "Iteration: 359 \t--- Loss: 0.042\n",
      "Iteration: 360 \t--- Loss: 0.049\n",
      "Iteration: 361 \t--- Loss: 0.040\n",
      "Iteration: 362 \t--- Loss: 0.043\n",
      "Iteration: 363 \t--- Loss: 0.040\n",
      "Iteration: 364 \t--- Loss: 0.042\n",
      "Iteration: 365 \t--- Loss: 0.040\n",
      "Iteration: 366 \t--- Loss: 0.043\n",
      "Iteration: 367 \t--- Loss: 0.036\n",
      "Iteration: 368 \t--- Loss: 0.037\n",
      "Iteration: 369 \t--- Loss: 0.036\n",
      "Iteration: 370 \t--- Loss: 0.036\n",
      "Iteration: 371 \t--- Loss: 0.037\n",
      "Iteration: 372 \t--- Loss: 0.048\n",
      "Iteration: 373 \t--- Loss: 0.040\n",
      "Iteration: 374 \t--- Loss: 0.049\n",
      "Iteration: 375 \t--- Loss: 0.038\n",
      "Iteration: 376 \t--- Loss: 0.044\n",
      "Iteration: 377 \t--- Loss: 0.039\n",
      "Iteration: 378 \t--- Loss: 0.044\n",
      "Iteration: 379 \t--- Loss: 0.034\n",
      "Iteration: 380 \t--- Loss: 0.037\n",
      "Iteration: 381 \t--- Loss: 0.035\n",
      "Iteration: 382 \t--- Loss: 0.033\n",
      "Iteration: 383 \t--- Loss: 0.031\n",
      "Iteration: 384 \t--- Loss: 0.033\n",
      "Iteration: 385 \t--- Loss: 0.028\n",
      "Iteration: 386 \t--- Loss: 0.029\n",
      "Iteration: 387 \t--- Loss: 0.033\n",
      "Iteration: 388 \t--- Loss: 0.033\n",
      "Iteration: 389 \t--- Loss: 0.036\n",
      "Iteration: 390 \t--- Loss: 0.039\n",
      "Iteration: 391 \t--- Loss: 0.038\n",
      "Iteration: 392 \t--- Loss: 0.043\n",
      "Iteration: 393 \t--- Loss: 0.039\n",
      "Iteration: 394 \t--- Loss: 0.046\n",
      "Iteration: 395 \t--- Loss: 0.031\n",
      "Iteration: 396 \t--- Loss: 0.031\n",
      "Iteration: 397 \t--- Loss: 0.029\n",
      "Iteration: 398 \t--- Loss: 0.027\n",
      "Iteration: 399 \t--- Loss: 0.028\n",
      "Iteration: 400 \t--- Loss: 0.033\n",
      "Iteration: 401 \t--- Loss: 0.030\n",
      "Iteration: 402 \t--- Loss: 0.031\n",
      "Iteration: 403 \t--- Loss: 0.032\n",
      "Iteration: 404 \t--- Loss: 0.033\n",
      "Iteration: 405 \t--- Loss: 0.030\n",
      "Iteration: 406 \t--- Loss: 0.029\n",
      "Iteration: 407 \t--- Loss: 0.030\n",
      "Iteration: 408 \t--- Loss: 0.031\n",
      "Iteration: 409 \t--- Loss: 0.028\n",
      "Iteration: 410 \t--- Loss: 0.030\n",
      "Iteration: 411 \t--- Loss: 0.031\n",
      "Iteration: 412 \t--- Loss: 0.033\n",
      "Iteration: 413 \t--- Loss: 0.033\n",
      "Iteration: 414 \t--- Loss: 0.038\n",
      "Iteration: 415 \t--- Loss: 0.034\n",
      "Iteration: 416 \t--- Loss: 0.041\n",
      "Iteration: 417 \t--- Loss: 0.028\n",
      "Iteration: 418 \t--- Loss: 0.029\n",
      "Iteration: 419 \t--- Loss: 0.030\n",
      "Iteration: 420 \t--- Loss: 0.030\n",
      "Iteration: 421 \t--- Loss: 0.027\n",
      "Iteration: 422 \t--- Loss: 0.029\n",
      "Iteration: 423 \t--- Loss: 0.035\n",
      "Iteration: 424 \t--- Loss: 0.044\n",
      "Iteration: 425 \t--- Loss: 0.032\n",
      "Iteration: 426 \t--- Loss: 0.032\n",
      "Iteration: 427 \t--- Loss: 0.028\n",
      "Iteration: 428 \t--- Loss: 0.030\n",
      "Iteration: 429 \t--- Loss: 0.031\n",
      "Iteration: 430 \t--- Loss: 0.036\n",
      "Iteration: 431 \t--- Loss: 0.031\n",
      "Iteration: 432 \t--- Loss: 0.033\n",
      "Iteration: 433 \t--- Loss: 0.031\n",
      "Iteration: 434 \t--- Loss: 0.030\n",
      "Iteration: 435 \t--- Loss: 0.025\n",
      "Iteration: 436 \t--- Loss: 0.026\n",
      "Iteration: 437 \t--- Loss: 0.026\n",
      "Iteration: 438 \t--- Loss: 0.027\n",
      "Iteration: 439 \t--- Loss: 0.024\n",
      "Iteration: 440 \t--- Loss: 0.025\n",
      "Iteration: 441 \t--- Loss: 0.026\n",
      "Iteration: 442 \t--- Loss: 0.026\n",
      "Iteration: 443 \t--- Loss: 0.023\n",
      "Iteration: 444 \t--- Loss: 0.026\n",
      "Iteration: 445 \t--- Loss: 0.026\n",
      "Iteration: 446 \t--- Loss: 0.025\n",
      "Iteration: 447 \t--- Loss: 0.026\n",
      "Iteration: 448 \t--- Loss: 0.026\n",
      "Iteration: 449 \t--- Loss: 0.031\n",
      "Iteration: 450 \t--- Loss: 0.032\n",
      "Iteration: 451 \t--- Loss: 0.031\n",
      "Iteration: 452 \t--- Loss: 0.034\n",
      "Iteration: 453 \t--- Loss: 0.032\n",
      "Iteration: 454 \t--- Loss: 0.038\n",
      "Iteration: 455 \t--- Loss: 0.028\n",
      "Iteration: 456 \t--- Loss: 0.027\n",
      "Iteration: 457 \t--- Loss: 0.025\n",
      "Iteration: 458 \t--- Loss: 0.027\n",
      "Iteration: 459 \t--- Loss: 0.028\n",
      "Iteration: 460 \t--- Loss: 0.029\n",
      "Iteration: 461 \t--- Loss: 0.029\n",
      "Iteration: 462 \t--- Loss: 0.031\n",
      "Iteration: 463 \t--- Loss: 0.028\n",
      "Iteration: 464 \t--- Loss: 0.031\n",
      "Iteration: 465 \t--- Loss: 0.028\n",
      "Iteration: 466 \t--- Loss: 0.031\n",
      "Iteration: 467 \t--- Loss: 0.027\n",
      "Iteration: 468 \t--- Loss: 0.027\n",
      "Iteration: 469 \t--- Loss: 0.028\n",
      "Iteration: 470 \t--- Loss: 0.028\n",
      "Iteration: 471 \t--- Loss: 0.028\n",
      "Iteration: 472 \t--- Loss: 0.027\n",
      "Iteration: 473 \t--- Loss: 0.027\n",
      "Iteration: 474 \t--- Loss: 0.026\n",
      "Iteration: 475 \t--- Loss: 0.026\n",
      "Iteration: 476 \t--- Loss: 0.026\n",
      "Iteration: 477 \t--- Loss: 0.026\n",
      "Iteration: 478 \t--- Loss: 0.023\n",
      "Iteration: 479 \t--- Loss: 0.026\n",
      "Iteration: 480 \t--- Loss: 0.025\n",
      "Iteration: 481 \t--- Loss: 0.026\n",
      "Iteration: 482 \t--- Loss: 0.025\n",
      "Iteration: 483 \t--- Loss: 0.024\n",
      "Iteration: 484 \t--- Loss: 0.023\n",
      "Iteration: 485 \t--- Loss: 0.023\n",
      "Iteration: 486 \t--- Loss: 0.025\n",
      "Iteration: 487 \t--- Loss: 0.023\n",
      "Iteration: 488 \t--- Loss: 0.023\n",
      "Iteration: 489 \t--- Loss: 0.025\n",
      "Iteration: 490 \t--- Loss: 0.023\n",
      "Iteration: 491 \t--- Loss: 0.024\n",
      "Iteration: 492 \t--- Loss: 0.024\n",
      "Iteration: 493 \t--- Loss: 0.023\n",
      "Iteration: 494 \t--- Loss: 0.024\n",
      "Iteration: 495 \t--- Loss: 0.023\n",
      "Iteration: 496 \t--- Loss: 0.023\n",
      "Iteration: 497 \t--- Loss: 0.023\n",
      "Iteration: 498 \t--- Loss: 0.025\n",
      "Iteration: 499 \t--- Loss: 0.025\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:10<00:04,  1.56s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.014\n",
      "Iteration: 261 \t--- Loss: 0.016\n",
      "Iteration: 262 \t--- Loss: 0.020\n",
      "Iteration: 263 \t--- Loss: 0.013\n",
      "Iteration: 264 \t--- Loss: 0.011\n",
      "Iteration: 265 \t--- Loss: 0.012\n",
      "Iteration: 266 \t--- Loss: 0.010\n",
      "Iteration: 267 \t--- Loss: 0.011\n",
      "Iteration: 268 \t--- Loss: 0.010\n",
      "Iteration: 269 \t--- Loss: 0.011\n",
      "Iteration: 270 \t--- Loss: 0.009\n",
      "Iteration: 271 \t--- Loss: 0.010\n",
      "Iteration: 272 \t--- Loss: 0.010\n",
      "Iteration: 273 \t--- Loss: 0.010\n",
      "Iteration: 274 \t--- Loss: 0.009\n",
      "Iteration: 275 \t--- Loss: 0.010\n",
      "Iteration: 276 \t--- Loss: 0.010\n",
      "Iteration: 277 \t--- Loss: 0.010\n",
      "Iteration: 278 \t--- Loss: 0.010\n",
      "Iteration: 279 \t--- Loss: 0.010\n",
      "Iteration: 280 \t--- Loss: 0.009\n",
      "Iteration: 281 \t--- Loss: 0.009\n",
      "Iteration: 282 \t--- Loss: 0.010\n",
      "Iteration: 283 \t--- Loss: 0.011\n",
      "Iteration: 284 \t--- Loss: 0.009\n",
      "Iteration: 285 \t--- Loss: 0.010\n",
      "Iteration: 286 \t--- Loss: 0.010\n",
      "Iteration: 287 \t--- Loss: 0.009\n",
      "Iteration: 288 \t--- Loss: 0.010\n",
      "Iteration: 289 \t--- Loss: 0.010\n",
      "Iteration: 290 \t--- Loss: 0.010\n",
      "Iteration: 291 \t--- Loss: 0.010\n",
      "Iteration: 292 \t--- Loss: 0.010\n",
      "Iteration: 293 \t--- Loss: 0.011\n",
      "Iteration: 294 \t--- Loss: 0.008\n",
      "Iteration: 295 \t--- Loss: 0.010\n",
      "Iteration: 296 \t--- Loss: 0.009\n",
      "Iteration: 297 \t--- Loss: 0.009\n",
      "Iteration: 298 \t--- Loss: 0.009\n",
      "Iteration: 299 \t--- Loss: 0.010\n",
      "Iteration: 300 \t--- Loss: 0.010\n",
      "Iteration: 301 \t--- Loss: 0.010\n",
      "Iteration: 302 \t--- Loss: 0.011\n",
      "Iteration: 303 \t--- Loss: 0.011\n",
      "Iteration: 304 \t--- Loss: 0.011\n",
      "Iteration: 305 \t--- Loss: 0.013\n",
      "Iteration: 306 \t--- Loss: 0.010\n",
      "Iteration: 307 \t--- Loss: 0.013\n",
      "Iteration: 308 \t--- Loss: 0.012\n",
      "Iteration: 309 \t--- Loss: 0.012\n",
      "Iteration: 310 \t--- Loss: 0.014\n",
      "Iteration: 311 \t--- Loss: 0.013\n",
      "Iteration: 312 \t--- Loss: 0.017\n",
      "Iteration: 313 \t--- Loss: 0.015\n",
      "Iteration: 314 \t--- Loss: 0.015\n",
      "Iteration: 315 \t--- Loss: 0.013\n",
      "Iteration: 316 \t--- Loss: 0.016\n",
      "Iteration: 317 \t--- Loss: 0.012\n",
      "Iteration: 318 \t--- Loss: 0.011\n",
      "Iteration: 319 \t--- Loss: 0.011\n",
      "Iteration: 320 \t--- Loss: 0.009\n",
      "Iteration: 321 \t--- Loss: 0.009\n",
      "Iteration: 322 \t--- Loss: 0.009\n",
      "Iteration: 323 \t--- Loss: 0.009\n",
      "Iteration: 324 \t--- Loss: 0.009\n",
      "Iteration: 325 \t--- Loss: 0.009\n",
      "Iteration: 326 \t--- Loss: 0.009\n",
      "Iteration: 327 \t--- Loss: 0.009\n",
      "Iteration: 328 \t--- Loss: 0.008\n",
      "Iteration: 329 \t--- Loss: 0.008\n",
      "Iteration: 330 \t--- Loss: 0.008\n",
      "Iteration: 331 \t--- Loss: 0.008\n",
      "Iteration: 332 \t--- Loss: 0.007\n",
      "Iteration: 333 \t--- Loss: 0.008\n",
      "Iteration: 334 \t--- Loss: 0.007\n",
      "Iteration: 335 \t--- Loss: 0.008\n",
      "Iteration: 336 \t--- Loss: 0.008\n",
      "Iteration: 337 \t--- Loss: 0.007\n",
      "Iteration: 338 \t--- Loss: 0.008\n",
      "Iteration: 339 \t--- Loss: 0.008\n",
      "Iteration: 340 \t--- Loss: 0.008\n",
      "Iteration: 341 \t--- Loss: 0.008\n",
      "Iteration: 342 \t--- Loss: 0.008\n",
      "Iteration: 343 \t--- Loss: 0.008\n",
      "Iteration: 344 \t--- Loss: 0.007\n",
      "Iteration: 345 \t--- Loss: 0.008\n",
      "Iteration: 346 \t--- Loss: 0.007\n",
      "Iteration: 347 \t--- Loss: 0.007\n",
      "Iteration: 348 \t--- Loss: 0.007\n",
      "Iteration: 349 \t--- Loss: 0.007\n",
      "Iteration: 350 \t--- Loss: 0.007\n",
      "Iteration: 351 \t--- Loss: 0.007\n",
      "Iteration: 352 \t--- Loss: 0.008\n",
      "Iteration: 353 \t--- Loss: 0.007\n",
      "Iteration: 354 \t--- Loss: 0.008\n",
      "Iteration: 355 \t--- Loss: 0.008\n",
      "Iteration: 356 \t--- Loss: 0.007\n",
      "Iteration: 357 \t--- Loss: 0.008\n",
      "Iteration: 358 \t--- Loss: 0.007\n",
      "Iteration: 359 \t--- Loss: 0.007\n",
      "Iteration: 360 \t--- Loss: 0.006\n",
      "Iteration: 361 \t--- Loss: 0.007\n",
      "Iteration: 362 \t--- Loss: 0.007\n",
      "Iteration: 363 \t--- Loss: 0.008\n",
      "Iteration: 364 \t--- Loss: 0.008\n",
      "Iteration: 365 \t--- Loss: 0.006\n",
      "Iteration: 366 \t--- Loss: 0.008\n",
      "Iteration: 367 \t--- Loss: 0.007\n",
      "Iteration: 368 \t--- Loss: 0.007\n",
      "Iteration: 369 \t--- Loss: 0.007\n",
      "Iteration: 370 \t--- Loss: 0.007\n",
      "Iteration: 371 \t--- Loss: 0.007\n",
      "Iteration: 372 \t--- Loss: 0.006\n",
      "Iteration: 373 \t--- Loss: 0.007\n",
      "Iteration: 374 \t--- Loss: 0.007\n",
      "Iteration: 375 \t--- Loss: 0.006\n",
      "Iteration: 376 \t--- Loss: 0.007\n",
      "Iteration: 377 \t--- Loss: 0.007\n",
      "Iteration: 378 \t--- Loss: 0.008\n",
      "Iteration: 379 \t--- Loss: 0.007\n",
      "Iteration: 380 \t--- Loss: 0.006\n",
      "Iteration: 381 \t--- Loss: 0.007\n",
      "Iteration: 382 \t--- Loss: 0.007\n",
      "Iteration: 383 \t--- Loss: 0.007\n",
      "Iteration: 384 \t--- Loss: 0.007\n",
      "Iteration: 385 \t--- Loss: 0.007\n",
      "Iteration: 386 \t--- Loss: 0.007\n",
      "Iteration: 387 \t--- Loss: 0.006\n",
      "Iteration: 388 \t--- Loss: 0.007\n",
      "Iteration: 389 \t--- Loss: 0.007\n",
      "Iteration: 390 \t--- Loss: 0.007\n",
      "Iteration: 391 \t--- Loss: 0.006\n",
      "Iteration: 392 \t--- Loss: 0.007\n",
      "Iteration: 393 \t--- Loss: 0.008\n",
      "Iteration: 394 \t--- Loss: 0.006\n",
      "Iteration: 395 \t--- Loss: 0.007\n",
      "Iteration: 396 \t--- Loss: 0.007\n",
      "Iteration: 397 \t--- Loss: 0.007\n",
      "Iteration: 398 \t--- Loss: 0.007\n",
      "Iteration: 399 \t--- Loss: 0.006\n",
      "Iteration: 400 \t--- Loss: 0.007\n",
      "Iteration: 401 \t--- Loss: 0.006\n",
      "Iteration: 402 \t--- Loss: 0.006\n",
      "Iteration: 403 \t--- Loss: 0.006\n",
      "Iteration: 404 \t--- Loss: 0.007\n",
      "Iteration: 405 \t--- Loss: 0.007\n",
      "Iteration: 406 \t--- Loss: 0.006\n",
      "Iteration: 407 \t--- Loss: 0.006\n",
      "Iteration: 408 \t--- Loss: 0.006\n",
      "Iteration: 409 \t--- Loss: 0.007\n",
      "Iteration: 410 \t--- Loss: 0.007\n",
      "Iteration: 411 \t--- Loss: 0.006\n",
      "Iteration: 412 \t--- Loss: 0.007\n",
      "Iteration: 413 \t--- Loss: 0.007\n",
      "Iteration: 414 \t--- Loss: 0.006\n",
      "Iteration: 415 \t--- Loss: 0.007\n",
      "Iteration: 416 \t--- Loss: 0.006\n",
      "Iteration: 417 \t--- Loss: 0.007\n",
      "Iteration: 418 \t--- Loss: 0.006\n",
      "Iteration: 419 \t--- Loss: 0.007\n",
      "Iteration: 420 \t--- Loss: 0.006\n",
      "Iteration: 421 \t--- Loss: 0.006\n",
      "Iteration: 422 \t--- Loss: 0.007\n",
      "Iteration: 423 \t--- Loss: 0.006\n",
      "Iteration: 424 \t--- Loss: 0.006\n",
      "Iteration: 425 \t--- Loss: 0.006\n",
      "Iteration: 426 \t--- Loss: 0.006\n",
      "Iteration: 427 \t--- Loss: 0.006\n",
      "Iteration: 428 \t--- Loss: 0.006\n",
      "Iteration: 429 \t--- Loss: 0.006\n",
      "Iteration: 430 \t--- Loss: 0.006\n",
      "Iteration: 431 \t--- Loss: 0.007\n",
      "Iteration: 432 \t--- Loss: 0.006\n",
      "Iteration: 433 \t--- Loss: 0.006\n",
      "Iteration: 434 \t--- Loss: 0.007\n",
      "Iteration: 435 \t--- Loss: 0.006\n",
      "Iteration: 436 \t--- Loss: 0.006\n",
      "Iteration: 437 \t--- Loss: 0.005\n",
      "Iteration: 438 \t--- Loss: 0.005\n",
      "Iteration: 439 \t--- Loss: 0.006\n",
      "Iteration: 440 \t--- Loss: 0.006\n",
      "Iteration: 441 \t--- Loss: 0.007\n",
      "Iteration: 442 \t--- Loss: 0.006\n",
      "Iteration: 443 \t--- Loss: 0.005\n",
      "Iteration: 444 \t--- Loss: 0.006\n",
      "Iteration: 445 \t--- Loss: 0.006\n",
      "Iteration: 446 \t--- Loss: 0.005\n",
      "Iteration: 447 \t--- Loss: 0.005\n",
      "Iteration: 448 \t--- Loss: 0.005\n",
      "Iteration: 449 \t--- Loss: 0.006\n",
      "Iteration: 450 \t--- Loss: 0.006\n",
      "Iteration: 451 \t--- Loss: 0.006\n",
      "Iteration: 452 \t--- Loss: 0.006\n",
      "Iteration: 453 \t--- Loss: 0.005\n",
      "Iteration: 454 \t--- Loss: 0.006\n",
      "Iteration: 455 \t--- Loss: 0.005\n",
      "Iteration: 456 \t--- Loss: 0.006\n",
      "Iteration: 457 \t--- Loss: 0.005\n",
      "Iteration: 458 \t--- Loss: 0.006\n",
      "Iteration: 459 \t--- Loss: 0.006\n",
      "Iteration: 460 \t--- Loss: 0.006\n",
      "Iteration: 461 \t--- Loss: 0.006\n",
      "Iteration: 462 \t--- Loss: 0.006\n",
      "Iteration: 463 \t--- Loss: 0.006\n",
      "Iteration: 464 \t--- Loss: 0.006\n",
      "Iteration: 465 \t--- Loss: 0.006\n",
      "Iteration: 466 \t--- Loss: 0.005\n",
      "Iteration: 467 \t--- Loss: 0.006\n",
      "Iteration: 468 \t--- Loss: 0.006\n",
      "Iteration: 469 \t--- Loss: 0.007\n",
      "Iteration: 470 \t--- Loss: 0.005\n",
      "Iteration: 471 \t--- Loss: 0.006\n",
      "Iteration: 472 \t--- Loss: 0.006\n",
      "Iteration: 473 \t--- Loss: 0.006\n",
      "Iteration: 474 \t--- Loss: 0.006\n",
      "Iteration: 475 \t--- Loss: 0.006\n",
      "Iteration: 476 \t--- Loss: 0.006\n",
      "Iteration: 477 \t--- Loss: 0.006\n",
      "Iteration: 478 \t--- Loss: 0.006\n",
      "Iteration: 479 \t--- Loss: 0.006\n",
      "Iteration: 480 \t--- Loss: 0.006\n",
      "Iteration: 481 \t--- Loss: 0.006\n",
      "Iteration: 482 \t--- Loss: 0.006\n",
      "Iteration: 483 \t--- Loss: 0.005\n",
      "Iteration: 484 \t--- Loss: 0.005\n",
      "Iteration: 485 \t--- Loss: 0.007\n",
      "Iteration: 486 \t--- Loss: 0.006\n",
      "Iteration: 487 \t--- Loss: 0.006\n",
      "Iteration: 488 \t--- Loss: 0.007\n",
      "Iteration: 489 \t--- Loss: 0.006\n",
      "Iteration: 490 \t--- Loss: 0.006\n",
      "Iteration: 491 \t--- Loss: 0.005\n",
      "Iteration: 492 \t--- Loss: 0.006\n",
      "Iteration: 493 \t--- Loss: 0.007\n",
      "Iteration: 494 \t--- Loss: 0.006\n",
      "Iteration: 495 \t--- Loss: 0.007\n",
      "Iteration: 496 \t--- Loss: 0.005\n",
      "Iteration: 497 \t--- Loss: 0.005\n",
      "Iteration: 498 \t--- Loss: 0.006\n",
      "Iteration: 499 \t--- Loss: 0.006\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.53s/it][Parallel(n_jobs=5)]: Done  37 tasks      | elapsed: 23.9min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it][Parallel(n_jobs=5)]: Done  38 tasks      | elapsed: 24.0min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:12<00:28,  4.08s/it][Parallel(n_jobs=5)]: Done  39 tasks      | elapsed: 24.1min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.266\n",
      "Iteration: 1 \t--- Loss: 0.293\n",
      "Iteration: 2 \t--- Loss: 0.270\n",
      "Iteration: 3 \t--- Loss: 0.196\n",
      "Iteration: 4 \t--- Loss: 0.223\n",
      "Iteration: 5 \t--- Loss: 0.223\n",
      "Iteration: 6 \t--- Loss: 0.197\n",
      "Iteration: 7 \t--- Loss: 0.189\n",
      "Iteration: 8 \t--- Loss: 0.212\n",
      "Iteration: 9 \t--- Loss: 0.206\n",
      "Iteration: 10 \t--- Loss: 0.179\n",
      "Iteration: 11 \t--- Loss: 0.217\n",
      "Iteration: 12 \t--- Loss: 0.185\n",
      "Iteration: 13 \t--- Loss: 0.190\n",
      "Iteration: 14 \t--- Loss: 0.182\n",
      "Iteration: 15 \t--- Loss: 0.173\n",
      "Iteration: 16 \t--- Loss: 0.160\n",
      "Iteration: 17 \t--- Loss: 0.172\n",
      "Iteration: 18 \t--- Loss: 0.172\n",
      "Iteration: 19 \t--- Loss: 0.180\n",
      "Iteration: 20 \t--- Loss: 0.157\n",
      "Iteration: 21 \t--- Loss: 0.158\n",
      "Iteration: 22 \t--- Loss: 0.157\n",
      "Iteration: 23 \t--- Loss: 0.156\n",
      "Iteration: 24 \t--- Loss: 0.162\n",
      "Iteration: 25 \t--- Loss: 0.150\n",
      "Iteration: 26 \t--- Loss: 0.149\n",
      "Iteration: 27 \t--- Loss: 0.155\n",
      "Iteration: 28 \t--- Loss: 0.151\n",
      "Iteration: 29 \t--- Loss: 0.142\n",
      "Iteration: 30 \t--- Loss: 0.160\n",
      "Iteration: 31 \t--- Loss: 0.165\n",
      "Iteration: 32 \t--- Loss: 0.145\n",
      "Iteration: 33 \t--- Loss: 0.163\n",
      "Iteration: 34 \t--- Loss: 0.152\n",
      "Iteration: 35 \t--- Loss: 0.153\n",
      "Iteration: 36 \t--- Loss: 0.144\n",
      "Iteration: 37 \t--- Loss: 0.172\n",
      "Iteration: 38 \t--- Loss: 0.137\n",
      "Iteration: 39 \t--- Loss: 0.147\n",
      "Iteration: 40 \t--- Loss: 0.145\n",
      "Iteration: 41 \t--- Loss: 0.148\n",
      "Iteration: 42 \t--- Loss: 0.160\n",
      "Iteration: 43 \t--- Loss: 0.164\n",
      "Iteration: 44 \t--- Loss: 0.167\n",
      "Iteration: 45 \t--- Loss: 0.159\n",
      "Iteration: 46 \t--- Loss: 0.145\n",
      "Iteration: 47 \t--- Loss: 0.153\n",
      "Iteration: 48 \t--- Loss: 0.158\n",
      "Iteration: 49 \t--- Loss: 0.155\n",
      "Iteration: 50 \t--- Loss: 0.145\n",
      "Iteration: 51 \t--- Loss: 0.159\n",
      "Iteration: 52 \t--- Loss: 0.146\n",
      "Iteration: 53 \t--- Loss: 0.157\n",
      "Iteration: 54 \t--- Loss: 0.162\n",
      "Iteration: 55 \t--- Loss: 0.138\n",
      "Iteration: 56 \t--- Loss: 0.142\n",
      "Iteration: 57 \t--- Loss: 0.142\n",
      "Iteration: 58 \t--- Loss: 0.134\n",
      "Iteration: 59 \t--- Loss: 0.154\n",
      "Iteration: 60 \t--- Loss: 0.143\n",
      "Iteration: 61 \t--- Loss: 0.139\n",
      "Iteration: 62 \t--- Loss: 0.135\n",
      "Iteration: 63 \t--- Loss: 0.151\n",
      "Iteration: 64 \t--- Loss: 0.141\n",
      "Iteration: 65 \t--- Loss: 0.166\n",
      "Iteration: 66 \t--- Loss: 0.143\n",
      "Iteration: 67 \t--- Loss: 0.155\n",
      "Iteration: 68 \t--- Loss: 0.166\n",
      "Iteration: 69 \t--- Loss: 0.127\n",
      "Iteration: 70 \t--- Loss: 0.154\n",
      "Iteration: 71 \t--- Loss: 0.148\n",
      "Iteration: 72 \t--- Loss: 0.144\n",
      "Iteration: 73 \t--- Loss: 0.146\n",
      "Iteration: 74 \t--- Loss: 0.191\n",
      "Iteration: 75 \t--- Loss: 0.139\n",
      "Iteration: 76 \t--- Loss: 0.161\n",
      "Iteration: 77 \t--- Loss: 0.147\n",
      "Iteration: 78 \t--- Loss: 0.173\n",
      "Iteration: 79 \t--- Loss: 0.147\n",
      "Iteration: 80 \t--- Loss: 0.144\n",
      "Iteration: 81 \t--- Loss: 0.144\n",
      "Iteration: 82 \t--- Loss: 0.150\n",
      "Iteration: 83 \t--- Loss: 0.144\n",
      "Iteration: 84 \t--- Loss: 0.153\n",
      "Iteration: 85 \t--- Loss: 0.167\n",
      "Iteration: 86 \t--- Loss: 0.146\n",
      "Iteration: 87 \t--- Loss: 0.155\n",
      "Iteration: 88 \t--- Loss: 0.146\n",
      "Iteration: 89 \t--- Loss: 0.142\n",
      "Iteration: 90 \t--- Loss: 0.145\n",
      "Iteration: 91 \t--- Loss: 0.121\n",
      "Iteration: 92 \t--- Loss: 0.137\n",
      "Iteration: 93 \t--- Loss: 0.151\n",
      "Iteration: 94 \t--- Loss: 0.154\n",
      "Iteration: 95 \t--- Loss: 0.145\n",
      "Iteration: 96 \t--- Loss: 0.134\n",
      "Iteration: 97 \t--- Loss: 0.134\n",
      "Iteration: 98 \t--- Loss: 0.148\n",
      "Iteration: 99 \t--- Loss: 0.165\n",
      "Iteration: 100 \t--- Loss: 0.132\n",
      "Iteration: 101 \t--- Loss: 0.154\n",
      "Iteration: 102 \t--- Loss: 0.160\n",
      "Iteration: 103 \t--- Loss: 0.158\n",
      "Iteration: 104 \t--- Loss: 0.135\n",
      "Iteration: 105 \t--- Loss: 0.129\n",
      "Iteration: 106 \t--- Loss: 0.183\n",
      "Iteration: 107 \t--- Loss: 0.147\n",
      "Iteration: 108 \t--- Loss: 0.148\n",
      "Iteration: 109 \t--- Loss: 0.145\n",
      "Iteration: 110 \t--- Loss: 0.156\n",
      "Iteration: 111 \t--- Loss: 0.152\n",
      "Iteration: 112 \t--- Loss: 0.169\n",
      "Iteration: 113 \t--- Loss: 0.166\n",
      "Iteration: 114 \t--- Loss: 0.143\n",
      "Iteration: 115 \t--- Loss: 0.131\n",
      "Iteration: 116 \t--- Loss: 0.153\n",
      "Iteration: 117 \t--- Loss: 0.142\n",
      "Iteration: 118 \t--- Loss: 0.132\n",
      "Iteration: 119 \t--- Loss: 0.155\n",
      "Iteration: 120 \t--- Loss: 0.136\n",
      "Iteration: 121 \t--- Loss: 0.171\n",
      "Iteration: 122 \t--- Loss: 0.133\n",
      "Iteration: 123 \t--- Loss: 0.134\n",
      "Iteration: 124 \t--- Loss: 0.139\n",
      "Iteration: 125 \t--- Loss: 0.153\n",
      "Iteration: 126 \t--- Loss: 0.160\n",
      "Iteration: 127 \t--- Loss: 0.165\n",
      "Iteration: 128 \t--- Loss: 0.167\n",
      "Iteration: 129 \t--- Loss: 0.154\n",
      "Iteration: 130 \t--- Loss: 0.164\n",
      "Iteration: 131 \t--- Loss: 0.152\n",
      "Iteration: 132 \t--- Loss: 0.165\n",
      "Iteration: 133 \t--- Loss: 0.135\n",
      "Iteration: 134 \t--- Loss: 0.138\n",
      "Iteration: 135 \t--- Loss: 0.158\n",
      "Iteration: 136 \t--- Loss: 0.140\n",
      "Iteration: 137 \t--- Loss: 0.139\n",
      "Iteration: 138 \t--- Loss: 0.130\n",
      "Iteration: 139 \t--- Loss: 0.166\n",
      "Iteration: 140 \t--- Loss: 0.143\n",
      "Iteration: 141 \t--- Loss: 0.141\n",
      "Iteration: 142 \t--- Loss: 0.152\n",
      "Iteration: 143 \t--- Loss: 0.164\n",
      "Iteration: 144 \t--- Loss: 0.158\n",
      "Iteration: 145 \t--- Loss: 0.149\n",
      "Iteration: 146 \t--- Loss: 0.150\n",
      "Iteration: 147 \t--- Loss: 0.123\n",
      "Iteration: 148 \t--- Loss: 0.143\n",
      "Iteration: 149 \t--- Loss: 0.154\n",
      "Iteration: 150 \t--- Loss: 0.136\n",
      "Iteration: 151 \t--- Loss: 0.135\n",
      "Iteration: 152 \t--- Loss: 0.134\n",
      "Iteration: 153 \t--- Loss: 0.168\n",
      "Iteration: 154 \t--- Loss: 0.155\n",
      "Iteration: 155 \t--- Loss: 0.153\n",
      "Iteration: 156 \t--- Loss: 0.176\n",
      "Iteration: 157 \t--- Loss: 0.164\n",
      "Iteration: 158 \t--- Loss: 0.152\n",
      "Iteration: 159 \t--- Loss: 0.145\n",
      "Iteration: 160 \t--- Loss: 0.142\n",
      "Iteration: 161 \t--- Loss: 0.134\n",
      "Iteration: 162 \t--- Loss: 0.162\n",
      "Iteration: 163 \t--- Loss: 0.154\n",
      "Iteration: 164 \t--- Loss: 0.164\n",
      "Iteration: 165 \t--- Loss: 0.172\n",
      "Iteration: 166 \t--- Loss: 0.171\n",
      "Iteration: 167 \t--- Loss: 0.152\n",
      "Iteration: 168 \t--- Loss: 0.147\n",
      "Iteration: 169 \t--- Loss: 0.166\n",
      "Iteration: 170 \t--- Loss: 0.133\n",
      "Iteration: 171 \t--- Loss: 0.148\n",
      "Iteration: 172 \t--- Loss: 0.131\n",
      "Iteration: 173 \t--- Loss: 0.124\n",
      "Iteration: 174 \t--- Loss: 0.110\n",
      "Iteration: 175 \t--- Loss: 0.160\n",
      "Iteration: 176 \t--- Loss: 0.152\n",
      "Iteration: 177 \t--- Loss: 0.147\n",
      "Iteration: 178 \t--- Loss: 0.155\n",
      "Iteration: 179 \t--- Loss: 0.137\n",
      "Iteration: 180 \t--- Loss: 0.152\n",
      "Iteration: 181 \t--- Loss: 0.157\n",
      "Iteration: 182 \t--- Loss: 0.171\n",
      "Iteration: 183 \t--- Loss: 0.137\n",
      "Iteration: 184 \t--- Loss: 0.149\n",
      "Iteration: 185 \t--- Loss: 0.135\n",
      "Iteration: 186 \t--- Loss: 0.144\n",
      "Iteration: 187 \t--- Loss: 0.127\n",
      "Iteration: 188 \t--- Loss: 0.135\n",
      "Iteration: 189 \t--- Loss: 0.144\n",
      "Iteration: 190 \t--- Loss: 0.168\n",
      "Iteration: 191 \t--- Loss: 0.135\n",
      "Iteration: 192 \t--- Loss: 0.147\n",
      "Iteration: 193 \t--- Loss: 0.150\n",
      "Iteration: 194 \t--- Loss: 0.137\n",
      "Iteration: 195 \t--- Loss: 0.149\n",
      "Iteration: 196 \t--- Loss: 0.151\n",
      "Iteration: 197 \t--- Loss: 0.152\n",
      "Iteration: 198 \t--- Loss: 0.158\n",
      "Iteration: 199 \t--- Loss: 0.156\n",
      "Iteration: 200 \t--- Loss: 0.137\n",
      "Iteration: 201 \t--- Loss: 0.136\n",
      "Iteration: 202 \t--- Loss: 0.157\n",
      "Iteration: 203 \t--- Loss: 0.145\n",
      "Iteration: 204 \t--- Loss: 0.148\n",
      "Iteration: 205 \t--- Loss: 0.141\n",
      "Iteration: 206 \t--- Loss: 0.144\n",
      "Iteration: 207 \t--- Loss: 0.163\n",
      "Iteration: 208 \t--- Loss: 0.140\n",
      "Iteration: 209 \t--- Loss: 0.124\n",
      "Iteration: 210 \t--- Loss: 0.163\n",
      "Iteration: 211 \t--- Loss: 0.137\n",
      "Iteration: 212 \t--- Loss: 0.144\n",
      "Iteration: 213 \t--- Loss: 0.155\n",
      "Iteration: 214 \t--- Loss: 0.146\n",
      "Iteration: 215 \t--- Loss: 0.138\n",
      "Iteration: 216 \t--- Loss: 0.140\n",
      "Iteration: 217 \t--- Loss: 0.142\n",
      "Iteration: 218 \t--- Loss: 0.141\n",
      "Iteration: 219 \t--- Loss: 0.121\n",
      "Iteration: 220 \t--- Loss: 0.150\n",
      "Iteration: 221 \t--- Loss: 0.141\n",
      "Iteration: 222 \t--- Loss: 0.127\n",
      "Iteration: 223 \t--- Loss: 0.147\n",
      "Iteration: 224 \t--- Loss: 0.150\n",
      "Iteration: 225 \t--- Loss: 0.160\n",
      "Iteration: 226 \t--- Loss: 0.124\n",
      "Iteration: 227 \t--- Loss: 0.140\n",
      "Iteration: 228 \t--- Loss: 0.139\n",
      "Iteration: 229 \t--- Loss: 0.153\n",
      "Iteration: 230 \t--- Loss: 0.156\n",
      "Iteration: 231 \t--- Loss: 0.155\n",
      "Iteration: 232 \t--- Loss: 0.132\n",
      "Iteration: 233 \t--- Loss: 0.149\n",
      "Iteration: 234 \t--- Loss: 0.145\n",
      "Iteration: 235 \t--- Loss: 0.145\n",
      "Iteration: 236 \t--- Loss: 0.144\n",
      "Iteration: 237 \t--- Loss: 0.142\n",
      "Iteration: 238 \t--- Loss: 0.137\n",
      "Iteration: 239 \t--- Loss: 0.155\n",
      "Iteration: 240 \t--- Loss: 0.159\n",
      "Iteration: 241 \t--- Loss: 0.139\n",
      "Iteration: 242 \t--- Loss: 0.152\n",
      "Iteration: 243 \t--- Loss: 0.143\n",
      "Iteration: 244 \t--- Loss: 0.138\n",
      "Iteration: 245 \t--- Loss: 0.154\n",
      "Iteration: 246 \t--- Loss: 0.137\n",
      "Iteration: 247 \t--- Loss: 0.156\n",
      "Iteration: 248 \t--- Loss: 0.160\n",
      "Iteration: 249 \t--- Loss: 0.146\n",
      "Iteration: 250 \t--- Loss: 0.158\n",
      "Iteration: 251 \t--- Loss: 0.139\n",
      "Iteration: 252 \t--- Loss: 0.142\n",
      "Iteration: 253 \t--- Loss: 0.147\n",
      "Iteration: 254 \t--- Loss: 0.146\n",
      "Iteration: 255 \t--- Loss: 0.145\n",
      "Iteration: 256 \t--- Loss: 0.141\n",
      "Iteration: 257 \t--- Loss: 0.151\n",
      "Iteration: 258 \t--- Loss: 0.158\n",
      "Iteration: 259 \t--- Loss: 0.131Iteration: 0 \t--- Loss: 0.534\n",
      "Iteration: 1 \t--- Loss: 0.445\n",
      "Iteration: 2 \t--- Loss: 0.440\n",
      "Iteration: 3 \t--- Loss: 0.407\n",
      "Iteration: 4 \t--- Loss: 0.348\n",
      "Iteration: 5 \t--- Loss: 0.345\n",
      "Iteration: 6 \t--- Loss: 0.314\n",
      "Iteration: 7 \t--- Loss: 0.302\n",
      "Iteration: 8 \t--- Loss: 0.302\n",
      "Iteration: 9 \t--- Loss: 0.278\n",
      "Iteration: 10 \t--- Loss: 0.266\n",
      "Iteration: 11 \t--- Loss: 0.253\n",
      "Iteration: 12 \t--- Loss: 0.249\n",
      "Iteration: 13 \t--- Loss: 0.236\n",
      "Iteration: 14 \t--- Loss: 0.232\n",
      "Iteration: 15 \t--- Loss: 0.241\n",
      "Iteration: 16 \t--- Loss: 0.237\n",
      "Iteration: 17 \t--- Loss: 0.223\n",
      "Iteration: 18 \t--- Loss: 0.214\n",
      "Iteration: 19 \t--- Loss: 0.227\n",
      "Iteration: 20 \t--- Loss: 0.211\n",
      "Iteration: 21 \t--- Loss: 0.229\n",
      "Iteration: 22 \t--- Loss: 0.220\n",
      "Iteration: 23 \t--- Loss: 0.213\n",
      "Iteration: 24 \t--- Loss: 0.212\n",
      "Iteration: 25 \t--- Loss: 0.208\n",
      "Iteration: 26 \t--- Loss: 0.205\n",
      "Iteration: 27 \t--- Loss: 0.205\n",
      "Iteration: 28 \t--- Loss: 0.208\n",
      "Iteration: 29 \t--- Loss: 0.215\n",
      "Iteration: 30 \t--- Loss: 0.204\n",
      "Iteration: 31 \t--- Loss: 0.202\n",
      "Iteration: 32 \t--- Loss: 0.202\n",
      "Iteration: 33 \t--- Loss: 0.205\n",
      "Iteration: 34 \t--- Loss: 0.202\n",
      "Iteration: 35 \t--- Loss: 0.212\n",
      "Iteration: 36 \t--- Loss: 0.206\n",
      "Iteration: 37 \t--- Loss: 0.188\n",
      "Iteration: 38 \t--- Loss: 0.203\n",
      "Iteration: 39 \t--- Loss: 0.200\n",
      "Iteration: 40 \t--- Loss: 0.204\n",
      "Iteration: 41 \t--- Loss: 0.187\n",
      "Iteration: 42 \t--- Loss: 0.206\n",
      "Iteration: 43 \t--- Loss: 0.197\n",
      "Iteration: 44 \t--- Loss: 0.206\n",
      "Iteration: 45 \t--- Loss: 0.197\n",
      "Iteration: 46 \t--- Loss: 0.196\n",
      "Iteration: 47 \t--- Loss: 0.209\n",
      "Iteration: 48 \t--- Loss: 0.197\n",
      "Iteration: 49 \t--- Loss: 0.198\n",
      "Iteration: 50 \t--- Loss: 0.199\n",
      "Iteration: 51 \t--- Loss: 0.206\n",
      "Iteration: 52 \t--- Loss: 0.196\n",
      "Iteration: 53 \t--- Loss: 0.199\n",
      "Iteration: 54 \t--- Loss: 0.201\n",
      "Iteration: 55 \t--- Loss: 0.198\n",
      "Iteration: 56 \t--- Loss: 0.197\n",
      "Iteration: 57 \t--- Loss: 0.205\n",
      "Iteration: 58 \t--- Loss: 0.195\n",
      "Iteration: 59 \t--- Loss: 0.210\n",
      "Iteration: 60 \t--- Loss: 0.201\n",
      "Iteration: 61 \t--- Loss: 0.208\n",
      "Iteration: 62 \t--- Loss: 0.195\n",
      "Iteration: 63 \t--- Loss: 0.200\n",
      "Iteration: 64 \t--- Loss: 0.204\n",
      "Iteration: 65 \t--- Loss: 0.199\n",
      "Iteration: 66 \t--- Loss: 0.193\n",
      "Iteration: 67 \t--- Loss: 0.205\n",
      "Iteration: 68 \t--- Loss: 0.202\n",
      "Iteration: 69 \t--- Loss: 0.192\n",
      "Iteration: 70 \t--- Loss: 0.206\n",
      "Iteration: 71 \t--- Loss: 0.195\n",
      "Iteration: 72 \t--- Loss: 0.196\n",
      "Iteration: 73 \t--- Loss: 0.190\n",
      "Iteration: 74 \t--- Loss: 0.197\n",
      "Iteration: 75 \t--- Loss: 0.206\n",
      "Iteration: 76 \t--- Loss: 0.192\n",
      "Iteration: 77 \t--- Loss: 0.193\n",
      "Iteration: 78 \t--- Loss: 0.191\n",
      "Iteration: 79 \t--- Loss: 0.199\n",
      "Iteration: 80 \t--- Loss: 0.198\n",
      "Iteration: 81 \t--- Loss: 0.191\n",
      "Iteration: 82 \t--- Loss: 0.192\n",
      "Iteration: 83 \t--- Loss: 0.192\n",
      "Iteration: 84 \t--- Loss: 0.199\n",
      "Iteration: 85 \t--- Loss: 0.197\n",
      "Iteration: 86 \t--- Loss: 0.197\n",
      "Iteration: 87 \t--- Loss: 0.198\n",
      "Iteration: 88 \t--- Loss: 0.194\n",
      "Iteration: 89 \t--- Loss: 0.195\n",
      "Iteration: 90 \t--- Loss: 0.204\n",
      "Iteration: 91 \t--- Loss: 0.207\n",
      "Iteration: 92 \t--- Loss: 0.203\n",
      "Iteration: 93 \t--- Loss: 0.199\n",
      "Iteration: 94 \t--- Loss: 0.202\n",
      "Iteration: 95 \t--- Loss: 0.197\n",
      "Iteration: 96 \t--- Loss: 0.201\n",
      "Iteration: 97 \t--- Loss: 0.205\n",
      "Iteration: 98 \t--- Loss: 0.196\n",
      "Iteration: 99 \t--- Loss: 0.197\n",
      "Iteration: 100 \t--- Loss: 0.199\n",
      "Iteration: 101 \t--- Loss: 0.206\n",
      "Iteration: 102 \t--- Loss: 0.201\n",
      "Iteration: 103 \t--- Loss: 0.203\n",
      "Iteration: 104 \t--- Loss: 0.196\n",
      "Iteration: 105 \t--- Loss: 0.202\n",
      "Iteration: 106 \t--- Loss: 0.192\n",
      "Iteration: 107 \t--- Loss: 0.196\n",
      "Iteration: 108 \t--- Loss: 0.199\n",
      "Iteration: 109 \t--- Loss: 0.202\n",
      "Iteration: 110 \t--- Loss: 0.201\n",
      "Iteration: 111 \t--- Loss: 0.189\n",
      "Iteration: 112 \t--- Loss: 0.189\n",
      "Iteration: 113 \t--- Loss: 0.201\n",
      "Iteration: 114 \t--- Loss: 0.192\n",
      "Iteration: 115 \t--- Loss: 0.190\n",
      "Iteration: 116 \t--- Loss: 0.199\n",
      "Iteration: 117 \t--- Loss: 0.202\n",
      "Iteration: 118 \t--- Loss: 0.195\n",
      "Iteration: 119 \t--- Loss: 0.191\n",
      "Iteration: 120 \t--- Loss: 0.200\n",
      "Iteration: 121 \t--- Loss: 0.203\n",
      "Iteration: 122 \t--- Loss: 0.198\n",
      "Iteration: 123 \t--- Loss: 0.198\n",
      "Iteration: 124 \t--- Loss: 0.197\n",
      "Iteration: 125 \t--- Loss: 0.195\n",
      "Iteration: 126 \t--- Loss: 0.207\n",
      "Iteration: 127 \t--- Loss: 0.195\n",
      "Iteration: 128 \t--- Loss: 0.195\n",
      "Iteration: 129 \t--- Loss: 0.202\n",
      "Iteration: 130 \t--- Loss: 0.198\n",
      "Iteration: 131 \t--- Loss: 0.203\n",
      "Iteration: 132 \t--- Loss: 0.202\n",
      "Iteration: 133 \t--- Loss: 0.195\n",
      "Iteration: 134 \t--- Loss: 0.199\n",
      "Iteration: 135 \t--- Loss: 0.201\n",
      "Iteration: 136 \t--- Loss: 0.193\n",
      "Iteration: 137 \t--- Loss: 0.206\n",
      "Iteration: 138 \t--- Loss: 0.203\n",
      "Iteration: 139 \t--- Loss: 0.192\n",
      "Iteration: 140 \t--- Loss: 0.192\n",
      "Iteration: 141 \t--- Loss: 0.209\n",
      "Iteration: 142 \t--- Loss: 0.194\n",
      "Iteration: 143 \t--- Loss: 0.207\n",
      "Iteration: 144 \t--- Loss: 0.202\n",
      "Iteration: 145 \t--- Loss: 0.195\n",
      "Iteration: 146 \t--- Loss: 0.198\n",
      "Iteration: 147 \t--- Loss: 0.191\n",
      "Iteration: 148 \t--- Loss: 0.195\n",
      "Iteration: 149 \t--- Loss: 0.207\n",
      "Iteration: 150 \t--- Loss: 0.196\n",
      "Iteration: 151 \t--- Loss: 0.212\n",
      "Iteration: 152 \t--- Loss: 0.206\n",
      "Iteration: 153 \t--- Loss: 0.205\n",
      "Iteration: 154 \t--- Loss: 0.193\n",
      "Iteration: 155 \t--- Loss: 0.194\n",
      "Iteration: 156 \t--- Loss: 0.200\n",
      "Iteration: 157 \t--- Loss: 0.203\n",
      "Iteration: 158 \t--- Loss: 0.191\n",
      "Iteration: 159 \t--- Loss: 0.196\n",
      "Iteration: 160 \t--- Loss: 0.198\n",
      "Iteration: 161 \t--- Loss: 0.194\n",
      "Iteration: 162 \t--- Loss: 0.195\n",
      "Iteration: 163 \t--- Loss: 0.198\n",
      "Iteration: 164 \t--- Loss: 0.196\n",
      "Iteration: 165 \t--- Loss: 0.202\n",
      "Iteration: 166 \t--- Loss: 0.197\n",
      "Iteration: 167 \t--- Loss: 0.194\n",
      "Iteration: 168 \t--- Loss: 0.194\n",
      "Iteration: 169 \t--- Loss: 0.196\n",
      "Iteration: 170 \t--- Loss: 0.192\n",
      "Iteration: 171 \t--- Loss: 0.193\n",
      "Iteration: 172 \t--- Loss: 0.198\n",
      "Iteration: 173 \t--- Loss: 0.195\n",
      "Iteration: 174 \t--- Loss: 0.196\n",
      "Iteration: 175 \t--- Loss: 0.187\n",
      "Iteration: 176 \t--- Loss: 0.198\n",
      "Iteration: 177 \t--- Loss: 0.203\n",
      "Iteration: 178 \t--- Loss: 0.198\n",
      "Iteration: 179 \t--- Loss: 0.196\n",
      "Iteration: 180 \t--- Loss: 0.194\n",
      "Iteration: 181 \t--- Loss: 0.205\n",
      "Iteration: 182 \t--- Loss: 0.195\n",
      "Iteration: 183 \t--- Loss: 0.205\n",
      "Iteration: 184 \t--- Loss: 0.201\n",
      "Iteration: 185 \t--- Loss: 0.198\n",
      "Iteration: 186 \t--- Loss: 0.199\n",
      "Iteration: 187 \t--- Loss: 0.191\n",
      "Iteration: 188 \t--- Loss: 0.193\n",
      "Iteration: 189 \t--- Loss: 0.200\n",
      "Iteration: 190 \t--- Loss: 0.196\n",
      "Iteration: 191 \t--- Loss: 0.194\n",
      "Iteration: 192 \t--- Loss: 0.199\n",
      "Iteration: 193 \t--- Loss: 0.189\n",
      "Iteration: 194 \t--- Loss: 0.206\n",
      "Iteration: 195 \t--- Loss: 0.203\n",
      "Iteration: 196 \t--- Loss: 0.201\n",
      "Iteration: 197 \t--- Loss: 0.209\n",
      "Iteration: 198 \t--- Loss: 0.197\n",
      "Iteration: 199 \t--- Loss: 0.199\n",
      "Iteration: 200 \t--- Loss: 0.197\n",
      "Iteration: 201 \t--- Loss: 0.199\n",
      "Iteration: 202 \t--- Loss: 0.195\n",
      "Iteration: 203 \t--- Loss: 0.206\n",
      "Iteration: 204 \t--- Loss: 0.199\n",
      "Iteration: 205 \t--- Loss: 0.202\n",
      "Iteration: 206 \t--- Loss: 0.201\n",
      "Iteration: 207 \t--- Loss: 0.188\n",
      "Iteration: 208 \t--- Loss: 0.196\n",
      "Iteration: 209 \t--- Loss: 0.202\n",
      "Iteration: 210 \t--- Loss: 0.187\n",
      "Iteration: 211 \t--- Loss: 0.195\n",
      "Iteration: 212 \t--- Loss: 0.200\n",
      "Iteration: 213 \t--- Loss: 0.189\n",
      "Iteration: 214 \t--- Loss: 0.199\n",
      "Iteration: 215 \t--- Loss: 0.197\n",
      "Iteration: 216 \t--- Loss: 0.197\n",
      "Iteration: 217 \t--- Loss: 0.190\n",
      "Iteration: 218 \t--- Loss: 0.200\n",
      "Iteration: 219 \t--- Loss: 0.190\n",
      "Iteration: 220 \t--- Loss: 0.199\n",
      "Iteration: 221 \t--- Loss: 0.193\n",
      "Iteration: 222 \t--- Loss: 0.203\n",
      "Iteration: 223 \t--- Loss: 0.205\n",
      "Iteration: 224 \t--- Loss: 0.199\n",
      "Iteration: 225 \t--- Loss: 0.198\n",
      "Iteration: 226 \t--- Loss: 0.209\n",
      "Iteration: 227 \t--- Loss: 0.202\n",
      "Iteration: 228 \t--- Loss: 0.201\n",
      "Iteration: 229 \t--- Loss: 0.199\n",
      "Iteration: 230 \t--- Loss: 0.209\n",
      "Iteration: 231 \t--- Loss: 0.195\n",
      "Iteration: 232 \t--- Loss: 0.191\n",
      "Iteration: 233 \t--- Loss: 0.197\n",
      "Iteration: 234 \t--- Loss: 0.193\n",
      "Iteration: 235 \t--- Loss: 0.198\n",
      "Iteration: 236 \t--- Loss: 0.193\n",
      "Iteration: 237 \t--- Loss: 0.205\n",
      "Iteration: 238 \t--- Loss: 0.209\n",
      "Iteration: 239 \t--- Loss: 0.198\n",
      "Iteration: 240 \t--- Loss: 0.194\n",
      "Iteration: 241 \t--- Loss: 0.196\n",
      "Iteration: 242 \t--- Loss: 0.189\n",
      "Iteration: 243 \t--- Loss: 0.206\n",
      "Iteration: 244 \t--- Loss: 0.189\n",
      "Iteration: 245 \t--- Loss: 0.192\n",
      "Iteration: 246 \t--- Loss: 0.192\n",
      "Iteration: 247 \t--- Loss: 0.201\n",
      "Iteration: 248 \t--- Loss: 0.198\n",
      "Iteration: 249 \t--- Loss: 0.198\n",
      "Iteration: 250 \t--- Loss: 0.198\n",
      "Iteration: 251 \t--- Loss: 0.203\n",
      "Iteration: 252 \t--- Loss: 0.192\n",
      "Iteration: 253 \t--- Loss: 0.199\n",
      "Iteration: 254 \t--- Loss: 0.193\n",
      "Iteration: 255 \t--- Loss: 0.199\n",
      "Iteration: 256 \t--- Loss: 0.203\n",
      "Iteration: 257 \t--- Loss: 0.191\n",
      "Iteration: 258 \t--- Loss: 0.197\n",
      "Iteration: 259 \t--- Loss: 0.193"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [08:56<00:00, 536.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.055\n",
      "Iteration: 261 \t--- Loss: 0.055\n",
      "Iteration: 262 \t--- Loss: 0.051\n",
      "Iteration: 263 \t--- Loss: 0.060\n",
      "Iteration: 264 \t--- Loss: 0.050\n",
      "Iteration: 265 \t--- Loss: 0.053\n",
      "Iteration: 266 \t--- Loss: 0.054\n",
      "Iteration: 267 \t--- Loss: 0.050\n",
      "Iteration: 268 \t--- Loss: 0.057\n",
      "Iteration: 269 \t--- Loss: 0.054\n",
      "Iteration: 270 \t--- Loss: 0.060\n",
      "Iteration: 271 \t--- Loss: 0.053\n",
      "Iteration: 272 \t--- Loss: 0.055\n",
      "Iteration: 273 \t--- Loss: 0.060\n",
      "Iteration: 274 \t--- Loss: 0.056\n",
      "Iteration: 275 \t--- Loss: 0.051\n",
      "Iteration: 276 \t--- Loss: 0.054\n",
      "Iteration: 277 \t--- Loss: 0.052\n",
      "Iteration: 278 \t--- Loss: 0.059\n",
      "Iteration: 279 \t--- Loss: 0.055\n",
      "Iteration: 280 \t--- Loss: 0.048\n",
      "Iteration: 281 \t--- Loss: 0.055\n",
      "Iteration: 282 \t--- Loss: 0.054\n",
      "Iteration: 283 \t--- Loss: 0.054\n",
      "Iteration: 284 \t--- Loss: 0.057\n",
      "Iteration: 285 \t--- Loss: 0.056\n",
      "Iteration: 286 \t--- Loss: 0.054\n",
      "Iteration: 287 \t--- Loss: 0.052\n",
      "Iteration: 288 \t--- Loss: 0.056\n",
      "Iteration: 289 \t--- Loss: 0.056\n",
      "Iteration: 290 \t--- Loss: 0.052\n",
      "Iteration: 291 \t--- Loss: 0.053\n",
      "Iteration: 292 \t--- Loss: 0.052\n",
      "Iteration: 293 \t--- Loss: 0.054\n",
      "Iteration: 294 \t--- Loss: 0.053\n",
      "Iteration: 295 \t--- Loss: 0.052\n",
      "Iteration: 296 \t--- Loss: 0.051\n",
      "Iteration: 297 \t--- Loss: 0.052\n",
      "Iteration: 298 \t--- Loss: 0.056\n",
      "Iteration: 299 \t--- Loss: 0.051\n",
      "Iteration: 300 \t--- Loss: 0.050\n",
      "Iteration: 301 \t--- Loss: 0.052\n",
      "Iteration: 302 \t--- Loss: 0.054\n",
      "Iteration: 303 \t--- Loss: 0.055\n",
      "Iteration: 304 \t--- Loss: 0.057\n",
      "Iteration: 305 \t--- Loss: 0.053\n",
      "Iteration: 306 \t--- Loss: 0.049\n",
      "Iteration: 307 \t--- Loss: 0.055\n",
      "Iteration: 308 \t--- Loss: 0.050\n",
      "Iteration: 309 \t--- Loss: 0.052\n",
      "Iteration: 310 \t--- Loss: 0.056\n",
      "Iteration: 311 \t--- Loss: 0.052\n",
      "Iteration: 312 \t--- Loss: 0.054\n",
      "Iteration: 313 \t--- Loss: 0.054\n",
      "Iteration: 314 \t--- Loss: 0.052\n",
      "Iteration: 315 \t--- Loss: 0.053\n",
      "Iteration: 316 \t--- Loss: 0.054\n",
      "Iteration: 317 \t--- Loss: 0.054\n",
      "Iteration: 318 \t--- Loss: 0.056\n",
      "Iteration: 319 \t--- Loss: 0.050\n",
      "Iteration: 320 \t--- Loss: 0.056\n",
      "Iteration: 321 \t--- Loss: 0.055\n",
      "Iteration: 322 \t--- Loss: 0.054\n",
      "Iteration: 323 \t--- Loss: 0.050\n",
      "Iteration: 324 \t--- Loss: 0.052\n",
      "Iteration: 325 \t--- Loss: 0.054\n",
      "Iteration: 326 \t--- Loss: 0.054\n",
      "Iteration: 327 \t--- Loss: 0.055\n",
      "Iteration: 328 \t--- Loss: 0.054\n",
      "Iteration: 329 \t--- Loss: 0.049\n",
      "Iteration: 330 \t--- Loss: 0.050\n",
      "Iteration: 331 \t--- Loss: 0.052\n",
      "Iteration: 332 \t--- Loss: 0.054\n",
      "Iteration: 333 \t--- Loss: 0.052\n",
      "Iteration: 334 \t--- Loss: 0.053\n",
      "Iteration: 335 \t--- Loss: 0.051\n",
      "Iteration: 336 \t--- Loss: 0.053\n",
      "Iteration: 337 \t--- Loss: 0.058\n",
      "Iteration: 338 \t--- Loss: 0.056\n",
      "Iteration: 339 \t--- Loss: 0.051\n",
      "Iteration: 340 \t--- Loss: 0.054\n",
      "Iteration: 341 \t--- Loss: 0.056\n",
      "Iteration: 342 \t--- Loss: 0.054\n",
      "Iteration: 343 \t--- Loss: 0.052\n",
      "Iteration: 344 \t--- Loss: 0.053\n",
      "Iteration: 345 \t--- Loss: 0.052\n",
      "Iteration: 346 \t--- Loss: 0.052\n",
      "Iteration: 347 \t--- Loss: 0.054\n",
      "Iteration: 348 \t--- Loss: 0.050\n",
      "Iteration: 349 \t--- Loss: 0.052\n",
      "Iteration: 350 \t--- Loss: 0.054\n",
      "Iteration: 351 \t--- Loss: 0.052\n",
      "Iteration: 352 \t--- Loss: 0.050\n",
      "Iteration: 353 \t--- Loss: 0.050\n",
      "Iteration: 354 \t--- Loss: 0.057\n",
      "Iteration: 355 \t--- Loss: 0.050\n",
      "Iteration: 356 \t--- Loss: 0.054\n",
      "Iteration: 357 \t--- Loss: 0.052\n",
      "Iteration: 358 \t--- Loss: 0.051\n",
      "Iteration: 359 \t--- Loss: 0.054\n",
      "Iteration: 360 \t--- Loss: 0.056\n",
      "Iteration: 361 \t--- Loss: 0.052\n",
      "Iteration: 362 \t--- Loss: 0.051\n",
      "Iteration: 363 \t--- Loss: 0.054\n",
      "Iteration: 364 \t--- Loss: 0.050\n",
      "Iteration: 365 \t--- Loss: 0.048\n",
      "Iteration: 366 \t--- Loss: 0.055\n",
      "Iteration: 367 \t--- Loss: 0.057\n",
      "Iteration: 368 \t--- Loss: 0.050\n",
      "Iteration: 369 \t--- Loss: 0.053\n",
      "Iteration: 370 \t--- Loss: 0.052\n",
      "Iteration: 371 \t--- Loss: 0.055\n",
      "Iteration: 372 \t--- Loss: 0.054\n",
      "Iteration: 373 \t--- Loss: 0.053\n",
      "Iteration: 374 \t--- Loss: 0.050\n",
      "Iteration: 375 \t--- Loss: 0.052\n",
      "Iteration: 376 \t--- Loss: 0.051\n",
      "Iteration: 377 \t--- Loss: 0.053\n",
      "Iteration: 378 \t--- Loss: 0.053\n",
      "Iteration: 379 \t--- Loss: 0.052\n",
      "Iteration: 380 \t--- Loss: 0.049\n",
      "Iteration: 381 \t--- Loss: 0.055\n",
      "Iteration: 382 \t--- Loss: 0.050\n",
      "Iteration: 383 \t--- Loss: 0.052\n",
      "Iteration: 384 \t--- Loss: 0.057\n",
      "Iteration: 385 \t--- Loss: 0.057\n",
      "Iteration: 386 \t--- Loss: 0.053\n",
      "Iteration: 387 \t--- Loss: 0.052\n",
      "Iteration: 388 \t--- Loss: 0.054\n",
      "Iteration: 389 \t--- Loss: 0.052\n",
      "Iteration: 390 \t--- Loss: 0.053\n",
      "Iteration: 391 \t--- Loss: 0.050\n",
      "Iteration: 392 \t--- Loss: 0.055\n",
      "Iteration: 393 \t--- Loss: 0.051\n",
      "Iteration: 394 \t--- Loss: 0.051\n",
      "Iteration: 395 \t--- Loss: 0.056\n",
      "Iteration: 396 \t--- Loss: 0.050\n",
      "Iteration: 397 \t--- Loss: 0.055\n",
      "Iteration: 398 \t--- Loss: 0.054\n",
      "Iteration: 399 \t--- Loss: 0.054\n",
      "Iteration: 400 \t--- Loss: 0.054\n",
      "Iteration: 401 \t--- Loss: 0.051\n",
      "Iteration: 402 \t--- Loss: 0.050\n",
      "Iteration: 403 \t--- Loss: 0.052\n",
      "Iteration: 404 \t--- Loss: 0.050\n",
      "Iteration: 405 \t--- Loss: 0.051\n",
      "Iteration: 406 \t--- Loss: 0.054\n",
      "Iteration: 407 \t--- Loss: 0.052\n",
      "Iteration: 408 \t--- Loss: 0.049\n",
      "Iteration: 409 \t--- Loss: 0.052\n",
      "Iteration: 410 \t--- Loss: 0.050\n",
      "Iteration: 411 \t--- Loss: 0.050\n",
      "Iteration: 412 \t--- Loss: 0.049\n",
      "Iteration: 413 \t--- Loss: 0.050\n",
      "Iteration: 414 \t--- Loss: 0.049\n",
      "Iteration: 415 \t--- Loss: 0.051\n",
      "Iteration: 416 \t--- Loss: 0.053\n",
      "Iteration: 417 \t--- Loss: 0.055\n",
      "Iteration: 418 \t--- Loss: 0.048\n",
      "Iteration: 419 \t--- Loss: 0.050\n",
      "Iteration: 420 \t--- Loss: 0.054\n",
      "Iteration: 421 \t--- Loss: 0.054\n",
      "Iteration: 422 \t--- Loss: 0.049\n",
      "Iteration: 423 \t--- Loss: 0.048\n",
      "Iteration: 424 \t--- Loss: 0.051\n",
      "Iteration: 425 \t--- Loss: 0.052\n",
      "Iteration: 426 \t--- Loss: 0.051\n",
      "Iteration: 427 \t--- Loss: 0.054\n",
      "Iteration: 428 \t--- Loss: 0.052\n",
      "Iteration: 429 \t--- Loss: 0.052\n",
      "Iteration: 430 \t--- Loss: 0.055\n",
      "Iteration: 431 \t--- Loss: 0.050\n",
      "Iteration: 432 \t--- Loss: 0.052\n",
      "Iteration: 433 \t--- Loss: 0.054\n",
      "Iteration: 434 \t--- Loss: 0.053\n",
      "Iteration: 435 \t--- Loss: 0.052\n",
      "Iteration: 436 \t--- Loss: 0.049\n",
      "Iteration: 437 \t--- Loss: 0.056\n",
      "Iteration: 438 \t--- Loss: 0.049\n",
      "Iteration: 439 \t--- Loss: 0.050\n",
      "Iteration: 440 \t--- Loss: 0.054\n",
      "Iteration: 441 \t--- Loss: 0.052\n",
      "Iteration: 442 \t--- Loss: 0.056\n",
      "Iteration: 443 \t--- Loss: 0.054\n",
      "Iteration: 444 \t--- Loss: 0.053\n",
      "Iteration: 445 \t--- Loss: 0.050\n",
      "Iteration: 446 \t--- Loss: 0.052\n",
      "Iteration: 447 \t--- Loss: 0.053\n",
      "Iteration: 448 \t--- Loss: 0.055\n",
      "Iteration: 449 \t--- Loss: 0.054\n",
      "Iteration: 450 \t--- Loss: 0.051\n",
      "Iteration: 451 \t--- Loss: 0.053\n",
      "Iteration: 452 \t--- Loss: 0.052\n",
      "Iteration: 453 \t--- Loss: 0.052\n",
      "Iteration: 454 \t--- Loss: 0.051\n",
      "Iteration: 455 \t--- Loss: 0.050\n",
      "Iteration: 456 \t--- Loss: 0.050\n",
      "Iteration: 457 \t--- Loss: 0.052\n",
      "Iteration: 458 \t--- Loss: 0.049\n",
      "Iteration: 459 \t--- Loss: 0.052\n",
      "Iteration: 460 \t--- Loss: 0.052\n",
      "Iteration: 461 \t--- Loss: 0.048\n",
      "Iteration: 462 \t--- Loss: 0.051\n",
      "Iteration: 463 \t--- Loss: 0.056\n",
      "Iteration: 464 \t--- Loss: 0.053\n",
      "Iteration: 465 \t--- Loss: 0.053\n",
      "Iteration: 466 \t--- Loss: 0.057\n",
      "Iteration: 467 \t--- Loss: 0.053\n",
      "Iteration: 468 \t--- Loss: 0.051\n",
      "Iteration: 469 \t--- Loss: 0.053\n",
      "Iteration: 470 \t--- Loss: 0.055\n",
      "Iteration: 471 \t--- Loss: 0.053\n",
      "Iteration: 472 \t--- Loss: 0.053\n",
      "Iteration: 473 \t--- Loss: 0.052\n",
      "Iteration: 474 \t--- Loss: 0.054\n",
      "Iteration: 475 \t--- Loss: 0.055\n",
      "Iteration: 476 \t--- Loss: 0.052\n",
      "Iteration: 477 \t--- Loss: 0.054\n",
      "Iteration: 478 \t--- Loss: 0.053\n",
      "Iteration: 479 \t--- Loss: 0.052\n",
      "Iteration: 480 \t--- Loss: 0.054\n",
      "Iteration: 481 \t--- Loss: 0.055\n",
      "Iteration: 482 \t--- Loss: 0.055\n",
      "Iteration: 483 \t--- Loss: 0.049\n",
      "Iteration: 484 \t--- Loss: 0.051\n",
      "Iteration: 485 \t--- Loss: 0.052\n",
      "Iteration: 486 \t--- Loss: 0.057\n",
      "Iteration: 487 \t--- Loss: 0.050\n",
      "Iteration: 488 \t--- Loss: 0.053\n",
      "Iteration: 489 \t--- Loss: 0.050\n",
      "Iteration: 490 \t--- Loss: 0.055\n",
      "Iteration: 491 \t--- Loss: 0.052\n",
      "Iteration: 492 \t--- Loss: 0.052\n",
      "Iteration: 493 \t--- Loss: 0.047\n",
      "Iteration: 494 \t--- Loss: 0.053\n",
      "Iteration: 495 \t--- Loss: 0.053\n",
      "Iteration: 496 \t--- Loss: 0.049\n",
      "Iteration: 497 \t--- Loss: 0.052\n",
      "Iteration: 498 \t--- Loss: 0.054\n",
      "Iteration: 499 \t--- Loss: 0.051\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.17s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.200\n",
      "Iteration: 261 \t--- Loss: 0.197\n",
      "Iteration: 262 \t--- Loss: 0.194\n",
      "Iteration: 263 \t--- Loss: 0.193\n",
      "Iteration: 264 \t--- Loss: 0.193\n",
      "Iteration: 265 \t--- Loss: 0.207\n",
      "Iteration: 266 \t--- Loss: 0.194\n",
      "Iteration: 267 \t--- Loss: 0.196\n",
      "Iteration: 268 \t--- Loss: 0.200\n",
      "Iteration: 269 \t--- Loss: 0.194\n",
      "Iteration: 270 \t--- Loss: 0.206\n",
      "Iteration: 271 \t--- Loss: 0.198\n",
      "Iteration: 272 \t--- Loss: 0.205\n",
      "Iteration: 273 \t--- Loss: 0.203\n",
      "Iteration: 274 \t--- Loss: 0.207\n",
      "Iteration: 275 \t--- Loss: 0.194\n",
      "Iteration: 276 \t--- Loss: 0.188\n",
      "Iteration: 277 \t--- Loss: 0.199\n",
      "Iteration: 278 \t--- Loss: 0.199\n",
      "Iteration: 279 \t--- Loss: 0.192\n",
      "Iteration: 280 \t--- Loss: 0.191\n",
      "Iteration: 281 \t--- Loss: 0.190\n",
      "Iteration: 282 \t--- Loss: 0.199\n",
      "Iteration: 283 \t--- Loss: 0.193\n",
      "Iteration: 284 \t--- Loss: 0.194\n",
      "Iteration: 285 \t--- Loss: 0.194\n",
      "Iteration: 286 \t--- Loss: 0.204\n",
      "Iteration: 287 \t--- Loss: 0.200\n",
      "Iteration: 288 \t--- Loss: 0.199\n",
      "Iteration: 289 \t--- Loss: 0.196\n",
      "Iteration: 290 \t--- Loss: 0.190\n",
      "Iteration: 291 \t--- Loss: 0.201\n",
      "Iteration: 292 \t--- Loss: 0.194\n",
      "Iteration: 293 \t--- Loss: 0.194\n",
      "Iteration: 294 \t--- Loss: 0.205\n",
      "Iteration: 295 \t--- Loss: 0.196\n",
      "Iteration: 296 \t--- Loss: 0.199\n",
      "Iteration: 297 \t--- Loss: 0.192\n",
      "Iteration: 298 \t--- Loss: 0.189\n",
      "Iteration: 299 \t--- Loss: 0.189\n",
      "Iteration: 300 \t--- Loss: 0.184\n",
      "Iteration: 301 \t--- Loss: 0.198\n",
      "Iteration: 302 \t--- Loss: 0.207\n",
      "Iteration: 303 \t--- Loss: 0.207\n",
      "Iteration: 304 \t--- Loss: 0.198\n",
      "Iteration: 305 \t--- Loss: 0.190\n",
      "Iteration: 306 \t--- Loss: 0.200\n",
      "Iteration: 307 \t--- Loss: 0.193\n",
      "Iteration: 308 \t--- Loss: 0.198\n",
      "Iteration: 309 \t--- Loss: 0.193\n",
      "Iteration: 310 \t--- Loss: 0.199\n",
      "Iteration: 311 \t--- Loss: 0.191\n",
      "Iteration: 312 \t--- Loss: 0.195\n",
      "Iteration: 313 \t--- Loss: 0.194\n",
      "Iteration: 314 \t--- Loss: 0.200\n",
      "Iteration: 315 \t--- Loss: 0.199\n",
      "Iteration: 316 \t--- Loss: 0.191\n",
      "Iteration: 317 \t--- Loss: 0.192\n",
      "Iteration: 318 \t--- Loss: 0.202\n",
      "Iteration: 319 \t--- Loss: 0.195\n",
      "Iteration: 320 \t--- Loss: 0.193\n",
      "Iteration: 321 \t--- Loss: 0.198\n",
      "Iteration: 322 \t--- Loss: 0.195\n",
      "Iteration: 323 \t--- Loss: 0.195\n",
      "Iteration: 324 \t--- Loss: 0.204\n",
      "Iteration: 325 \t--- Loss: 0.196\n",
      "Iteration: 326 \t--- Loss: 0.199\n",
      "Iteration: 327 \t--- Loss: 0.206\n",
      "Iteration: 328 \t--- Loss: 0.195\n",
      "Iteration: 329 \t--- Loss: 0.202\n",
      "Iteration: 330 \t--- Loss: 0.196\n",
      "Iteration: 331 \t--- Loss: 0.202\n",
      "Iteration: 332 \t--- Loss: 0.191\n",
      "Iteration: 333 \t--- Loss: 0.207\n",
      "Iteration: 334 \t--- Loss: 0.205\n",
      "Iteration: 335 \t--- Loss: 0.197\n",
      "Iteration: 336 \t--- Loss: 0.200\n",
      "Iteration: 337 \t--- Loss: 0.203\n",
      "Iteration: 338 \t--- Loss: 0.192\n",
      "Iteration: 339 \t--- Loss: 0.200\n",
      "Iteration: 340 \t--- Loss: 0.195\n",
      "Iteration: 341 \t--- Loss: 0.192\n",
      "Iteration: 342 \t--- Loss: 0.185\n",
      "Iteration: 343 \t--- Loss: 0.200\n",
      "Iteration: 344 \t--- Loss: 0.201\n",
      "Iteration: 345 \t--- Loss: 0.191\n",
      "Iteration: 346 \t--- Loss: 0.198\n",
      "Iteration: 347 \t--- Loss: 0.192\n",
      "Iteration: 348 \t--- Loss: 0.188\n",
      "Iteration: 349 \t--- Loss: 0.192\n",
      "Iteration: 350 \t--- Loss: 0.200\n",
      "Iteration: 351 \t--- Loss: 0.192\n",
      "Iteration: 352 \t--- Loss: 0.192\n",
      "Iteration: 353 \t--- Loss: 0.192\n",
      "Iteration: 354 \t--- Loss: 0.201\n",
      "Iteration: 355 \t--- Loss: 0.199\n",
      "Iteration: 356 \t--- Loss: 0.195\n",
      "Iteration: 357 \t--- Loss: 0.197\n",
      "Iteration: 358 \t--- Loss: 0.190\n",
      "Iteration: 359 \t--- Loss: 0.202\n",
      "Iteration: 360 \t--- Loss: 0.202\n",
      "Iteration: 361 \t--- Loss: 0.201\n",
      "Iteration: 362 \t--- Loss: 0.195\n",
      "Iteration: 363 \t--- Loss: 0.199\n",
      "Iteration: 364 \t--- Loss: 0.190\n",
      "Iteration: 365 \t--- Loss: 0.192\n",
      "Iteration: 366 \t--- Loss: 0.206\n",
      "Iteration: 367 \t--- Loss: 0.192\n",
      "Iteration: 368 \t--- Loss: 0.192\n",
      "Iteration: 369 \t--- Loss: 0.195\n",
      "Iteration: 370 \t--- Loss: 0.194\n",
      "Iteration: 371 \t--- Loss: 0.193\n",
      "Iteration: 372 \t--- Loss: 0.194\n",
      "Iteration: 373 \t--- Loss: 0.206\n",
      "Iteration: 374 \t--- Loss: 0.195\n",
      "Iteration: 375 \t--- Loss: 0.192\n",
      "Iteration: 376 \t--- Loss: 0.194\n",
      "Iteration: 377 \t--- Loss: 0.199\n",
      "Iteration: 378 \t--- Loss: 0.194\n",
      "Iteration: 379 \t--- Loss: 0.197\n",
      "Iteration: 380 \t--- Loss: 0.198\n",
      "Iteration: 381 \t--- Loss: 0.199\n",
      "Iteration: 382 \t--- Loss: 0.205\n",
      "Iteration: 383 \t--- Loss: 0.200\n",
      "Iteration: 384 \t--- Loss: 0.199\n",
      "Iteration: 385 \t--- Loss: 0.209\n",
      "Iteration: 386 \t--- Loss: 0.199\n",
      "Iteration: 387 \t--- Loss: 0.196\n",
      "Iteration: 388 \t--- Loss: 0.195\n",
      "Iteration: 389 \t--- Loss: 0.202\n",
      "Iteration: 390 \t--- Loss: 0.198\n",
      "Iteration: 391 \t--- Loss: 0.199\n",
      "Iteration: 392 \t--- Loss: 0.189\n",
      "Iteration: 393 \t--- Loss: 0.202\n",
      "Iteration: 394 \t--- Loss: 0.199\n",
      "Iteration: 395 \t--- Loss: 0.209\n",
      "Iteration: 396 \t--- Loss: 0.203\n",
      "Iteration: 397 \t--- Loss: 0.190\n",
      "Iteration: 398 \t--- Loss: 0.194\n",
      "Iteration: 399 \t--- Loss: 0.201\n",
      "Iteration: 400 \t--- Loss: 0.203\n",
      "Iteration: 401 \t--- Loss: 0.203\n",
      "Iteration: 402 \t--- Loss: 0.206\n",
      "Iteration: 403 \t--- Loss: 0.200\n",
      "Iteration: 404 \t--- Loss: 0.197\n",
      "Iteration: 405 \t--- Loss: 0.194\n",
      "Iteration: 406 \t--- Loss: 0.189\n",
      "Iteration: 407 \t--- Loss: 0.197\n",
      "Iteration: 408 \t--- Loss: 0.192\n",
      "Iteration: 409 \t--- Loss: 0.190\n",
      "Iteration: 410 \t--- Loss: 0.196\n",
      "Iteration: 411 \t--- Loss: 0.198\n",
      "Iteration: 412 \t--- Loss: 0.198\n",
      "Iteration: 413 \t--- Loss: 0.196\n",
      "Iteration: 414 \t--- Loss: 0.197\n",
      "Iteration: 415 \t--- Loss: 0.204\n",
      "Iteration: 416 \t--- Loss: 0.200\n",
      "Iteration: 417 \t--- Loss: 0.192\n",
      "Iteration: 418 \t--- Loss: 0.199\n",
      "Iteration: 419 \t--- Loss: 0.195\n",
      "Iteration: 420 \t--- Loss: 0.201\n",
      "Iteration: 421 \t--- Loss: 0.196\n",
      "Iteration: 422 \t--- Loss: 0.201\n",
      "Iteration: 423 \t--- Loss: 0.191\n",
      "Iteration: 424 \t--- Loss: 0.197\n",
      "Iteration: 425 \t--- Loss: 0.201\n",
      "Iteration: 426 \t--- Loss: 0.191\n",
      "Iteration: 427 \t--- Loss: 0.196\n",
      "Iteration: 428 \t--- Loss: 0.203\n",
      "Iteration: 429 \t--- Loss: 0.206\n",
      "Iteration: 430 \t--- Loss: 0.200\n",
      "Iteration: 431 \t--- Loss: 0.200\n",
      "Iteration: 432 \t--- Loss: 0.196\n",
      "Iteration: 433 \t--- Loss: 0.198\n",
      "Iteration: 434 \t--- Loss: 0.200\n",
      "Iteration: 435 \t--- Loss: 0.206\n",
      "Iteration: 436 \t--- Loss: 0.196\n",
      "Iteration: 437 \t--- Loss: 0.189\n",
      "Iteration: 438 \t--- Loss: 0.201\n",
      "Iteration: 439 \t--- Loss: 0.194\n",
      "Iteration: 440 \t--- Loss: 0.190\n",
      "Iteration: 441 \t--- Loss: 0.196\n",
      "Iteration: 442 \t--- Loss: 0.195\n",
      "Iteration: 443 \t--- Loss: 0.199\n",
      "Iteration: 444 \t--- Loss: 0.202\n",
      "Iteration: 445 \t--- Loss: 0.202\n",
      "Iteration: 446 \t--- Loss: 0.196\n",
      "Iteration: 447 \t--- Loss: 0.196\n",
      "Iteration: 448 \t--- Loss: 0.202\n",
      "Iteration: 449 \t--- Loss: 0.198\n",
      "Iteration: 450 \t--- Loss: 0.194\n",
      "Iteration: 451 \t--- Loss: 0.206\n",
      "Iteration: 452 \t--- Loss: 0.192\n",
      "Iteration: 453 \t--- Loss: 0.200\n",
      "Iteration: 454 \t--- Loss: 0.198\n",
      "Iteration: 455 \t--- Loss: 0.195\n",
      "Iteration: 456 \t--- Loss: 0.203\n",
      "Iteration: 457 \t--- Loss: 0.192\n",
      "Iteration: 458 \t--- Loss: 0.198\n",
      "Iteration: 459 \t--- Loss: 0.200\n",
      "Iteration: 460 \t--- Loss: 0.199\n",
      "Iteration: 461 \t--- Loss: 0.200\n",
      "Iteration: 462 \t--- Loss: 0.192\n",
      "Iteration: 463 \t--- Loss: 0.199\n",
      "Iteration: 464 \t--- Loss: 0.195\n",
      "Iteration: 465 \t--- Loss: 0.196\n",
      "Iteration: 466 \t--- Loss: 0.200\n",
      "Iteration: 467 \t--- Loss: 0.194\n",
      "Iteration: 468 \t--- Loss: 0.202\n",
      "Iteration: 469 \t--- Loss: 0.203\n",
      "Iteration: 470 \t--- Loss: 0.199\n",
      "Iteration: 471 \t--- Loss: 0.193\n",
      "Iteration: 472 \t--- Loss: 0.194\n",
      "Iteration: 473 \t--- Loss: 0.203\n",
      "Iteration: 474 \t--- Loss: 0.199\n",
      "Iteration: 475 \t--- Loss: 0.200\n",
      "Iteration: 476 \t--- Loss: 0.198\n",
      "Iteration: 477 \t--- Loss: 0.201\n",
      "Iteration: 478 \t--- Loss: 0.196\n",
      "Iteration: 479 \t--- Loss: 0.201\n",
      "Iteration: 480 \t--- Loss: 0.193\n",
      "Iteration: 481 \t--- Loss: 0.189\n",
      "Iteration: 482 \t--- Loss: 0.205\n",
      "Iteration: 483 \t--- Loss: 0.201\n",
      "Iteration: 484 \t--- Loss: 0.206\n",
      "Iteration: 485 \t--- Loss: 0.199\n",
      "Iteration: 486 \t--- Loss: 0.202\n",
      "Iteration: 487 \t--- Loss: 0.194\n",
      "Iteration: 488 \t--- Loss: 0.183\n",
      "Iteration: 489 \t--- Loss: 0.200\n",
      "Iteration: 490 \t--- Loss: 0.190\n",
      "Iteration: 491 \t--- Loss: 0.194\n",
      "Iteration: 492 \t--- Loss: 0.194\n",
      "Iteration: 493 \t--- Loss: 0.188\n",
      "Iteration: 494 \t--- Loss: 0.202\n",
      "Iteration: 495 \t--- Loss: 0.205\n",
      "Iteration: 496 \t--- Loss: 0.207\n",
      "Iteration: 497 \t--- Loss: 0.197\n",
      "Iteration: 498 \t--- Loss: 0.184\n",
      "Iteration: 499 \t--- Loss: 0.198\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:10,  1.37s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.134\n",
      "Iteration: 261 \t--- Loss: 0.151\n",
      "Iteration: 262 \t--- Loss: 0.149\n",
      "Iteration: 263 \t--- Loss: 0.152\n",
      "Iteration: 264 \t--- Loss: 0.141\n",
      "Iteration: 265 \t--- Loss: 0.149\n",
      "Iteration: 266 \t--- Loss: 0.150\n",
      "Iteration: 267 \t--- Loss: 0.135\n",
      "Iteration: 268 \t--- Loss: 0.143\n",
      "Iteration: 269 \t--- Loss: 0.142\n",
      "Iteration: 270 \t--- Loss: 0.162\n",
      "Iteration: 271 \t--- Loss: 0.146\n",
      "Iteration: 272 \t--- Loss: 0.144\n",
      "Iteration: 273 \t--- Loss: 0.141\n",
      "Iteration: 274 \t--- Loss: 0.143\n",
      "Iteration: 275 \t--- Loss: 0.136\n",
      "Iteration: 276 \t--- Loss: 0.140\n",
      "Iteration: 277 \t--- Loss: 0.154\n",
      "Iteration: 278 \t--- Loss: 0.156\n",
      "Iteration: 279 \t--- Loss: 0.143\n",
      "Iteration: 280 \t--- Loss: 0.138\n",
      "Iteration: 281 \t--- Loss: 0.141\n",
      "Iteration: 282 \t--- Loss: 0.156\n",
      "Iteration: 283 \t--- Loss: 0.145\n",
      "Iteration: 284 \t--- Loss: 0.153\n",
      "Iteration: 285 \t--- Loss: 0.133\n",
      "Iteration: 286 \t--- Loss: 0.137\n",
      "Iteration: 287 \t--- Loss: 0.164\n",
      "Iteration: 288 \t--- Loss: 0.167\n",
      "Iteration: 289 \t--- Loss: 0.140\n",
      "Iteration: 290 \t--- Loss: 0.165\n",
      "Iteration: 291 \t--- Loss: 0.138\n",
      "Iteration: 292 \t--- Loss: 0.148\n",
      "Iteration: 293 \t--- Loss: 0.162\n",
      "Iteration: 294 \t--- Loss: 0.157\n",
      "Iteration: 295 \t--- Loss: 0.154\n",
      "Iteration: 296 \t--- Loss: 0.130\n",
      "Iteration: 297 \t--- Loss: 0.151\n",
      "Iteration: 298 \t--- Loss: 0.156\n",
      "Iteration: 299 \t--- Loss: 0.159\n",
      "Iteration: 300 \t--- Loss: 0.167\n",
      "Iteration: 301 \t--- Loss: 0.157\n",
      "Iteration: 302 \t--- Loss: 0.150\n",
      "Iteration: 303 \t--- Loss: 0.143\n",
      "Iteration: 304 \t--- Loss: 0.159\n",
      "Iteration: 305 \t--- Loss: 0.150\n",
      "Iteration: 306 \t--- Loss: 0.145\n",
      "Iteration: 307 \t--- Loss: 0.164\n",
      "Iteration: 308 \t--- Loss: 0.162\n",
      "Iteration: 309 \t--- Loss: 0.161\n",
      "Iteration: 310 \t--- Loss: 0.163\n",
      "Iteration: 311 \t--- Loss: 0.159\n",
      "Iteration: 312 \t--- Loss: 0.130\n",
      "Iteration: 313 \t--- Loss: 0.159\n",
      "Iteration: 314 \t--- Loss: 0.142\n",
      "Iteration: 315 \t--- Loss: 0.164\n",
      "Iteration: 316 \t--- Loss: 0.134\n",
      "Iteration: 317 \t--- Loss: 0.153\n",
      "Iteration: 318 \t--- Loss: 0.154\n",
      "Iteration: 319 \t--- Loss: 0.159\n",
      "Iteration: 320 \t--- Loss: 0.158\n",
      "Iteration: 321 \t--- Loss: 0.145\n",
      "Iteration: 322 \t--- Loss: 0.128\n",
      "Iteration: 323 \t--- Loss: 0.140\n",
      "Iteration: 324 \t--- Loss: 0.139\n",
      "Iteration: 325 \t--- Loss: 0.161\n",
      "Iteration: 326 \t--- Loss: 0.141\n",
      "Iteration: 327 \t--- Loss: 0.141\n",
      "Iteration: 328 \t--- Loss: 0.141\n",
      "Iteration: 329 \t--- Loss: 0.151\n",
      "Iteration: 330 \t--- Loss: 0.150\n",
      "Iteration: 331 \t--- Loss: 0.145\n",
      "Iteration: 332 \t--- Loss: 0.144\n",
      "Iteration: 333 \t--- Loss: 0.139\n",
      "Iteration: 334 \t--- Loss: 0.155\n",
      "Iteration: 335 \t--- Loss: 0.125\n",
      "Iteration: 336 \t--- Loss: 0.152\n",
      "Iteration: 337 \t--- Loss: 0.138\n",
      "Iteration: 338 \t--- Loss: 0.157\n",
      "Iteration: 339 \t--- Loss: 0.137\n",
      "Iteration: 340 \t--- Loss: 0.164\n",
      "Iteration: 341 \t--- Loss: 0.149\n",
      "Iteration: 342 \t--- Loss: 0.150\n",
      "Iteration: 343 \t--- Loss: 0.141\n",
      "Iteration: 344 \t--- Loss: 0.140\n",
      "Iteration: 345 \t--- Loss: 0.132\n",
      "Iteration: 346 \t--- Loss: 0.140\n",
      "Iteration: 347 \t--- Loss: 0.167\n",
      "Iteration: 348 \t--- Loss: 0.152\n",
      "Iteration: 349 \t--- Loss: 0.156\n",
      "Iteration: 350 \t--- Loss: 0.153\n",
      "Iteration: 351 \t--- Loss: 0.151\n",
      "Iteration: 352 \t--- Loss: 0.157\n",
      "Iteration: 353 \t--- Loss: 0.140\n",
      "Iteration: 354 \t--- Loss: 0.130\n",
      "Iteration: 355 \t--- Loss: 0.162\n",
      "Iteration: 356 \t--- Loss: 0.162\n",
      "Iteration: 357 \t--- Loss: 0.122\n",
      "Iteration: 358 \t--- Loss: 0.138\n",
      "Iteration: 359 \t--- Loss: 0.129\n",
      "Iteration: 360 \t--- Loss: 0.142\n",
      "Iteration: 361 \t--- Loss: 0.129\n",
      "Iteration: 362 \t--- Loss: 0.146\n",
      "Iteration: 363 \t--- Loss: 0.135\n",
      "Iteration: 364 \t--- Loss: 0.160\n",
      "Iteration: 365 \t--- Loss: 0.160\n",
      "Iteration: 366 \t--- Loss: 0.125\n",
      "Iteration: 367 \t--- Loss: 0.152\n",
      "Iteration: 368 \t--- Loss: 0.155\n",
      "Iteration: 369 \t--- Loss: 0.156\n",
      "Iteration: 370 \t--- Loss: 0.150\n",
      "Iteration: 371 \t--- Loss: 0.151\n",
      "Iteration: 372 \t--- Loss: 0.141\n",
      "Iteration: 373 \t--- Loss: 0.134\n",
      "Iteration: 374 \t--- Loss: 0.142\n",
      "Iteration: 375 \t--- Loss: 0.129\n",
      "Iteration: 376 \t--- Loss: 0.160\n",
      "Iteration: 377 \t--- Loss: 0.130\n",
      "Iteration: 378 \t--- Loss: 0.132\n",
      "Iteration: 379 \t--- Loss: 0.146\n",
      "Iteration: 380 \t--- Loss: 0.143\n",
      "Iteration: 381 \t--- Loss: 0.155\n",
      "Iteration: 382 \t--- Loss: 0.162\n",
      "Iteration: 383 \t--- Loss: 0.167\n",
      "Iteration: 384 \t--- Loss: 0.130\n",
      "Iteration: 385 \t--- Loss: 0.148\n",
      "Iteration: 386 \t--- Loss: 0.139\n",
      "Iteration: 387 \t--- Loss: 0.135\n",
      "Iteration: 388 \t--- Loss: 0.150\n",
      "Iteration: 389 \t--- Loss: 0.143\n",
      "Iteration: 390 \t--- Loss: 0.138\n",
      "Iteration: 391 \t--- Loss: 0.163\n",
      "Iteration: 392 \t--- Loss: 0.134\n",
      "Iteration: 393 \t--- Loss: 0.159\n",
      "Iteration: 394 \t--- Loss: 0.147\n",
      "Iteration: 395 \t--- Loss: 0.174\n",
      "Iteration: 396 \t--- Loss: 0.151\n",
      "Iteration: 397 \t--- Loss: 0.139\n",
      "Iteration: 398 \t--- Loss: 0.141\n",
      "Iteration: 399 \t--- Loss: 0.162\n",
      "Iteration: 400 \t--- Loss: 0.129\n",
      "Iteration: 401 \t--- Loss: 0.135\n",
      "Iteration: 402 \t--- Loss: 0.149\n",
      "Iteration: 403 \t--- Loss: 0.131\n",
      "Iteration: 404 \t--- Loss: 0.154\n",
      "Iteration: 405 \t--- Loss: 0.154\n",
      "Iteration: 406 \t--- Loss: 0.151\n",
      "Iteration: 407 \t--- Loss: 0.133\n",
      "Iteration: 408 \t--- Loss: 0.135\n",
      "Iteration: 409 \t--- Loss: 0.151\n",
      "Iteration: 410 \t--- Loss: 0.150\n",
      "Iteration: 411 \t--- Loss: 0.130\n",
      "Iteration: 412 \t--- Loss: 0.169\n",
      "Iteration: 413 \t--- Loss: 0.151\n",
      "Iteration: 414 \t--- Loss: 0.134\n",
      "Iteration: 415 \t--- Loss: 0.140\n",
      "Iteration: 416 \t--- Loss: 0.138\n",
      "Iteration: 417 \t--- Loss: 0.143\n",
      "Iteration: 418 \t--- Loss: 0.140\n",
      "Iteration: 419 \t--- Loss: 0.139\n",
      "Iteration: 420 \t--- Loss: 0.134\n",
      "Iteration: 421 \t--- Loss: 0.139\n",
      "Iteration: 422 \t--- Loss: 0.164\n",
      "Iteration: 423 \t--- Loss: 0.137\n",
      "Iteration: 424 \t--- Loss: 0.130\n",
      "Iteration: 425 \t--- Loss: 0.137\n",
      "Iteration: 426 \t--- Loss: 0.131\n",
      "Iteration: 427 \t--- Loss: 0.159\n",
      "Iteration: 428 \t--- Loss: 0.130\n",
      "Iteration: 429 \t--- Loss: 0.155\n",
      "Iteration: 430 \t--- Loss: 0.135\n",
      "Iteration: 431 \t--- Loss: 0.130\n",
      "Iteration: 432 \t--- Loss: 0.146\n",
      "Iteration: 433 \t--- Loss: 0.150\n",
      "Iteration: 434 \t--- Loss: 0.129\n",
      "Iteration: 435 \t--- Loss: 0.160\n",
      "Iteration: 436 \t--- Loss: 0.153\n",
      "Iteration: 437 \t--- Loss: 0.136\n",
      "Iteration: 438 \t--- Loss: 0.137\n",
      "Iteration: 439 \t--- Loss: 0.142\n",
      "Iteration: 440 \t--- Loss: 0.134\n",
      "Iteration: 441 \t--- Loss: 0.139\n",
      "Iteration: 442 \t--- Loss: 0.133\n",
      "Iteration: 443 \t--- Loss: 0.154\n",
      "Iteration: 444 \t--- Loss: 0.132\n",
      "Iteration: 445 \t--- Loss: 0.139\n",
      "Iteration: 446 \t--- Loss: 0.142\n",
      "Iteration: 447 \t--- Loss: 0.147\n",
      "Iteration: 448 \t--- Loss: 0.146\n",
      "Iteration: 449 \t--- Loss: 0.155\n",
      "Iteration: 450 \t--- Loss: 0.154\n",
      "Iteration: 451 \t--- Loss: 0.148\n",
      "Iteration: 452 \t--- Loss: 0.141\n",
      "Iteration: 453 \t--- Loss: 0.131\n",
      "Iteration: 454 \t--- Loss: 0.163\n",
      "Iteration: 455 \t--- Loss: 0.141\n",
      "Iteration: 456 \t--- Loss: 0.135\n",
      "Iteration: 457 \t--- Loss: 0.170\n",
      "Iteration: 458 \t--- Loss: 0.149\n",
      "Iteration: 459 \t--- Loss: 0.155\n",
      "Iteration: 460 \t--- Loss: 0.142\n",
      "Iteration: 461 \t--- Loss: 0.142\n",
      "Iteration: 462 \t--- Loss: 0.140\n",
      "Iteration: 463 \t--- Loss: 0.150\n",
      "Iteration: 464 \t--- Loss: 0.154\n",
      "Iteration: 465 \t--- Loss: 0.165\n",
      "Iteration: 466 \t--- Loss: 0.145\n",
      "Iteration: 467 \t--- Loss: 0.132\n",
      "Iteration: 468 \t--- Loss: 0.147\n",
      "Iteration: 469 \t--- Loss: 0.154\n",
      "Iteration: 470 \t--- Loss: 0.155\n",
      "Iteration: 471 \t--- Loss: 0.148\n",
      "Iteration: 472 \t--- Loss: 0.160\n",
      "Iteration: 473 \t--- Loss: 0.150\n",
      "Iteration: 474 \t--- Loss: 0.151\n",
      "Iteration: 475 \t--- Loss: 0.136\n",
      "Iteration: 476 \t--- Loss: 0.144\n",
      "Iteration: 477 \t--- Loss: 0.148\n",
      "Iteration: 478 \t--- Loss: 0.155\n",
      "Iteration: 479 \t--- Loss: 0.160\n",
      "Iteration: 480 \t--- Loss: 0.138\n",
      "Iteration: 481 \t--- Loss: 0.144\n",
      "Iteration: 482 \t--- Loss: 0.136\n",
      "Iteration: 483 \t--- Loss: 0.138\n",
      "Iteration: 484 \t--- Loss: 0.177\n",
      "Iteration: 485 \t--- Loss: 0.158\n",
      "Iteration: 486 \t--- Loss: 0.154\n",
      "Iteration: 487 \t--- Loss: 0.144\n",
      "Iteration: 488 \t--- Loss: 0.153\n",
      "Iteration: 489 \t--- Loss: 0.145\n",
      "Iteration: 490 \t--- Loss: 0.157\n",
      "Iteration: 491 \t--- Loss: 0.156\n",
      "Iteration: 492 \t--- Loss: 0.144\n",
      "Iteration: 493 \t--- Loss: 0.133\n",
      "Iteration: 494 \t--- Loss: 0.144\n",
      "Iteration: 495 \t--- Loss: 0.149\n",
      "Iteration: 496 \t--- Loss: 0.142\n",
      "Iteration: 497 \t--- Loss: 0.138\n",
      "Iteration: 498 \t--- Loss: 0.147\n",
      "Iteration: 499 \t--- Loss: 0.144\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it][Parallel(n_jobs=5)]: Done  40 tasks      | elapsed: 25.5min\n",
      " 30%|███       | 3/10 [00:04<00:10,  1.43s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it][Parallel(n_jobs=5)]: Done  41 tasks      | elapsed: 25.6min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.57s/it][Parallel(n_jobs=5)]: Done  42 tasks      | elapsed: 25.7min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [09:59<?, ?it/s][Parallel(n_jobs=5)]: Done  43 tasks      | elapsed: 25.7min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.059\n",
      "Iteration: 261 \t--- Loss: 0.057\n",
      "Iteration: 262 \t--- Loss: 0.059\n",
      "Iteration: 263 \t--- Loss: 0.058\n",
      "Iteration: 264 \t--- Loss: 0.058\n",
      "Iteration: 265 \t--- Loss: 0.053\n",
      "Iteration: 266 \t--- Loss: 0.057\n",
      "Iteration: 267 \t--- Loss: 0.054\n",
      "Iteration: 268 \t--- Loss: 0.053\n",
      "Iteration: 269 \t--- Loss: 0.059\n",
      "Iteration: 270 \t--- Loss: 0.057\n",
      "Iteration: 271 \t--- Loss: 0.059\n",
      "Iteration: 272 \t--- Loss: 0.056\n",
      "Iteration: 273 \t--- Loss: 0.054\n",
      "Iteration: 274 \t--- Loss: 0.057\n",
      "Iteration: 275 \t--- Loss: 0.057\n",
      "Iteration: 276 \t--- Loss: 0.055\n",
      "Iteration: 277 \t--- Loss: 0.062\n",
      "Iteration: 278 \t--- Loss: 0.058\n",
      "Iteration: 279 \t--- Loss: 0.059\n",
      "Iteration: 280 \t--- Loss: 0.058\n",
      "Iteration: 281 \t--- Loss: 0.059\n",
      "Iteration: 282 \t--- Loss: 0.056\n",
      "Iteration: 283 \t--- Loss: 0.061\n",
      "Iteration: 284 \t--- Loss: 0.056\n",
      "Iteration: 285 \t--- Loss: 0.056\n",
      "Iteration: 286 \t--- Loss: 0.058\n",
      "Iteration: 287 \t--- Loss: 0.052\n",
      "Iteration: 288 \t--- Loss: 0.050\n",
      "Iteration: 289 \t--- Loss: 0.056\n",
      "Iteration: 290 \t--- Loss: 0.057\n",
      "Iteration: 291 \t--- Loss: 0.055\n",
      "Iteration: 292 \t--- Loss: 0.058\n",
      "Iteration: 293 \t--- Loss: 0.053\n",
      "Iteration: 294 \t--- Loss: 0.059\n",
      "Iteration: 295 \t--- Loss: 0.056\n",
      "Iteration: 296 \t--- Loss: 0.063\n",
      "Iteration: 297 \t--- Loss: 0.058\n",
      "Iteration: 298 \t--- Loss: 0.060\n",
      "Iteration: 299 \t--- Loss: 0.059\n",
      "Iteration: 300 \t--- Loss: 0.058\n",
      "Iteration: 301 \t--- Loss: 0.059\n",
      "Iteration: 302 \t--- Loss: 0.055\n",
      "Iteration: 303 \t--- Loss: 0.058\n",
      "Iteration: 304 \t--- Loss: 0.056\n",
      "Iteration: 305 \t--- Loss: 0.055\n",
      "Iteration: 306 \t--- Loss: 0.057\n",
      "Iteration: 307 \t--- Loss: 0.058\n",
      "Iteration: 308 \t--- Loss: 0.057\n",
      "Iteration: 309 \t--- Loss: 0.059\n",
      "Iteration: 310 \t--- Loss: 0.054\n",
      "Iteration: 311 \t--- Loss: 0.056\n",
      "Iteration: 312 \t--- Loss: 0.056\n",
      "Iteration: 313 \t--- Loss: 0.056\n",
      "Iteration: 314 \t--- Loss: 0.051\n",
      "Iteration: 315 \t--- Loss: 0.057\n",
      "Iteration: 316 \t--- Loss: 0.056\n",
      "Iteration: 317 \t--- Loss: 0.051\n",
      "Iteration: 318 \t--- Loss: 0.057\n",
      "Iteration: 319 \t--- Loss: 0.057\n",
      "Iteration: 320 \t--- Loss: 0.056\n",
      "Iteration: 321 \t--- Loss: 0.054\n",
      "Iteration: 322 \t--- Loss: 0.055\n",
      "Iteration: 323 \t--- Loss: 0.055\n",
      "Iteration: 324 \t--- Loss: 0.054\n",
      "Iteration: 325 \t--- Loss: 0.056\n",
      "Iteration: 326 \t--- Loss: 0.058\n",
      "Iteration: 327 \t--- Loss: 0.056\n",
      "Iteration: 328 \t--- Loss: 0.059\n",
      "Iteration: 329 \t--- Loss: 0.060\n",
      "Iteration: 330 \t--- Loss: 0.056\n",
      "Iteration: 331 \t--- Loss: 0.060\n",
      "Iteration: 332 \t--- Loss: 0.057\n",
      "Iteration: 333 \t--- Loss: 0.055\n",
      "Iteration: 334 \t--- Loss: 0.054\n",
      "Iteration: 335 \t--- Loss: 0.054\n",
      "Iteration: 336 \t--- Loss: 0.056\n",
      "Iteration: 337 \t--- Loss: 0.056\n",
      "Iteration: 338 \t--- Loss: 0.052\n",
      "Iteration: 339 \t--- Loss: 0.055\n",
      "Iteration: 340 \t--- Loss: 0.055\n",
      "Iteration: 341 \t--- Loss: 0.054\n",
      "Iteration: 342 \t--- Loss: 0.057\n",
      "Iteration: 343 \t--- Loss: 0.055\n",
      "Iteration: 344 \t--- Loss: 0.057\n",
      "Iteration: 345 \t--- Loss: 0.057\n",
      "Iteration: 346 \t--- Loss: 0.051\n",
      "Iteration: 347 \t--- Loss: 0.055\n",
      "Iteration: 348 \t--- Loss: 0.054\n",
      "Iteration: 349 \t--- Loss: 0.055\n",
      "Iteration: 350 \t--- Loss: 0.052\n",
      "Iteration: 351 \t--- Loss: 0.058\n",
      "Iteration: 352 \t--- Loss: 0.060\n",
      "Iteration: 353 \t--- Loss: 0.053\n",
      "Iteration: 354 \t--- Loss: 0.054\n",
      "Iteration: 355 \t--- Loss: 0.056\n",
      "Iteration: 356 \t--- Loss: 0.052\n",
      "Iteration: 357 \t--- Loss: 0.059\n",
      "Iteration: 358 \t--- Loss: 0.055\n",
      "Iteration: 359 \t--- Loss: 0.055\n",
      "Iteration: 360 \t--- Loss: 0.054\n",
      "Iteration: 361 \t--- Loss: 0.058\n",
      "Iteration: 362 \t--- Loss: 0.056\n",
      "Iteration: 363 \t--- Loss: 0.055\n",
      "Iteration: 364 \t--- Loss: 0.053\n",
      "Iteration: 365 \t--- Loss: 0.054\n",
      "Iteration: 366 \t--- Loss: 0.055\n",
      "Iteration: 367 \t--- Loss: 0.054\n",
      "Iteration: 368 \t--- Loss: 0.049\n",
      "Iteration: 369 \t--- Loss: 0.053\n",
      "Iteration: 370 \t--- Loss: 0.054\n",
      "Iteration: 371 \t--- Loss: 0.056\n",
      "Iteration: 372 \t--- Loss: 0.057\n",
      "Iteration: 373 \t--- Loss: 0.058\n",
      "Iteration: 374 \t--- Loss: 0.056\n",
      "Iteration: 375 \t--- Loss: 0.054\n",
      "Iteration: 376 \t--- Loss: 0.054\n",
      "Iteration: 377 \t--- Loss: 0.054\n",
      "Iteration: 378 \t--- Loss: 0.054\n",
      "Iteration: 379 \t--- Loss: 0.054\n",
      "Iteration: 380 \t--- Loss: 0.053\n",
      "Iteration: 381 \t--- Loss: 0.055\n",
      "Iteration: 382 \t--- Loss: 0.058\n",
      "Iteration: 383 \t--- Loss: 0.058\n",
      "Iteration: 384 \t--- Loss: 0.056\n",
      "Iteration: 385 \t--- Loss: 0.057\n",
      "Iteration: 386 \t--- Loss: 0.055\n",
      "Iteration: 387 \t--- Loss: 0.057\n",
      "Iteration: 388 \t--- Loss: 0.051\n",
      "Iteration: 389 \t--- Loss: 0.058\n",
      "Iteration: 390 \t--- Loss: 0.052\n",
      "Iteration: 391 \t--- Loss: 0.058\n",
      "Iteration: 392 \t--- Loss: 0.054\n",
      "Iteration: 393 \t--- Loss: 0.055\n",
      "Iteration: 394 \t--- Loss: 0.053\n",
      "Iteration: 395 \t--- Loss: 0.055\n",
      "Iteration: 396 \t--- Loss: 0.055\n",
      "Iteration: 397 \t--- Loss: 0.056\n",
      "Iteration: 398 \t--- Loss: 0.054\n",
      "Iteration: 399 \t--- Loss: 0.056\n",
      "Iteration: 400 \t--- Loss: 0.053\n",
      "Iteration: 401 \t--- Loss: 0.061\n",
      "Iteration: 402 \t--- Loss: 0.055\n",
      "Iteration: 403 \t--- Loss: 0.051\n",
      "Iteration: 404 \t--- Loss: 0.057\n",
      "Iteration: 405 \t--- Loss: 0.053\n",
      "Iteration: 406 \t--- Loss: 0.055\n",
      "Iteration: 407 \t--- Loss: 0.056\n",
      "Iteration: 408 \t--- Loss: 0.054\n",
      "Iteration: 409 \t--- Loss: 0.052\n",
      "Iteration: 410 \t--- Loss: 0.060\n",
      "Iteration: 411 \t--- Loss: 0.058\n",
      "Iteration: 412 \t--- Loss: 0.053\n",
      "Iteration: 413 \t--- Loss: 0.055\n",
      "Iteration: 414 \t--- Loss: 0.052\n",
      "Iteration: 415 \t--- Loss: 0.057\n",
      "Iteration: 416 \t--- Loss: 0.053\n",
      "Iteration: 417 \t--- Loss: 0.056\n",
      "Iteration: 418 \t--- Loss: 0.058\n",
      "Iteration: 419 \t--- Loss: 0.054\n",
      "Iteration: 420 \t--- Loss: 0.059\n",
      "Iteration: 421 \t--- Loss: 0.053\n",
      "Iteration: 422 \t--- Loss: 0.055\n",
      "Iteration: 423 \t--- Loss: 0.052\n",
      "Iteration: 424 \t--- Loss: 0.054\n",
      "Iteration: 425 \t--- Loss: 0.060\n",
      "Iteration: 426 \t--- Loss: 0.058\n",
      "Iteration: 427 \t--- Loss: 0.058\n",
      "Iteration: 428 \t--- Loss: 0.054\n",
      "Iteration: 429 \t--- Loss: 0.054\n",
      "Iteration: 430 \t--- Loss: 0.057\n",
      "Iteration: 431 \t--- Loss: 0.054\n",
      "Iteration: 432 \t--- Loss: 0.055\n",
      "Iteration: 433 \t--- Loss: 0.054\n",
      "Iteration: 434 \t--- Loss: 0.058\n",
      "Iteration: 435 \t--- Loss: 0.051\n",
      "\n",
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 2.333\n",
      "Iteration: 1 \t--- Loss: 2.205\n",
      "Iteration: 2 \t--- Loss: 1.904\n",
      "Iteration: 3 \t--- Loss: 1.814\n",
      "Iteration: 4 \t--- Loss: 1.820\n",
      "Iteration: 5 \t--- Loss: 1.697\n",
      "Iteration: 6 \t--- Loss: 1.714\n",
      "Iteration: 7 \t--- Loss: 1.653\n",
      "Iteration: 8 \t--- Loss: 1.629\n",
      "Iteration: 9 \t--- Loss: 1.600\n",
      "Iteration: 10 \t--- Loss: 1.638\n",
      "Iteration: 11 \t--- Loss: 1.668\n",
      "Iteration: 12 \t--- Loss: 1.640\n",
      "Iteration: 13 \t--- Loss: 1.643\n",
      "Iteration: 14 \t--- Loss: 1.682\n",
      "Iteration: 15 \t--- Loss: 1.646\n",
      "Iteration: 16 \t--- Loss: 1.567\n",
      "Iteration: 17 \t--- Loss: 1.607\n",
      "Iteration: 18 \t--- Loss: 1.567\n",
      "Iteration: 19 \t--- Loss: 1.561\n",
      "Iteration: 20 \t--- Loss: 1.549\n",
      "Iteration: 21 \t--- Loss: 1.612\n",
      "Iteration: 22 \t--- Loss: 1.563\n",
      "Iteration: 23 \t--- Loss: 1.573\n",
      "Iteration: 24 \t--- Loss: 1.640\n",
      "Iteration: 25 \t--- Loss: 1.608\n",
      "Iteration: 26 \t--- Loss: 1.588\n",
      "Iteration: 27 \t--- Loss: 1.600\n",
      "Iteration: 28 \t--- Loss: 1.606\n",
      "Iteration: 29 \t--- Loss: 1.641\n",
      "Iteration: 30 \t--- Loss: 1.649\n",
      "Iteration: 31 \t--- Loss: 1.678\n",
      "Iteration: 32 \t--- Loss: 1.597\n",
      "Iteration: 33 \t--- Loss: 1.574\n",
      "Iteration: 34 \t--- Loss: 1.597\n",
      "Iteration: 35 \t--- Loss: 1.554\n",
      "Iteration: 36 \t--- Loss: 1.571\n",
      "Iteration: 37 \t--- Loss: 1.585\n",
      "Iteration: 38 \t--- Loss: 1.658\n",
      "Iteration: 39 \t--- Loss: 1.578\n",
      "Iteration: 40 \t--- Loss: 1.611\n",
      "Iteration: 41 \t--- Loss: 1.560\n",
      "Iteration: 42 \t--- Loss: 1.485\n",
      "Iteration: 43 \t--- Loss: 1.581\n",
      "Iteration: 44 \t--- Loss: 1.637\n",
      "Iteration: 45 \t--- Loss: 1.542\n",
      "Iteration: 46 \t--- Loss: 1.625\n",
      "Iteration: 47 \t--- Loss: 1.581\n",
      "Iteration: 48 \t--- Loss: 1.608\n",
      "Iteration: 49 \t--- Loss: 1.647\n",
      "Iteration: 50 \t--- Loss: 1.585\n",
      "Iteration: 51 \t--- Loss: 1.562\n",
      "Iteration: 52 \t--- Loss: 1.588\n",
      "Iteration: 53 \t--- Loss: 1.509\n",
      "Iteration: 54 \t--- Loss: 1.567\n",
      "Iteration: 55 \t--- Loss: 1.612\n",
      "Iteration: 56 \t--- Loss: 1.572\n",
      "Iteration: 57 \t--- Loss: 1.626\n",
      "Iteration: 58 \t--- Loss: 1.561\n",
      "Iteration: 59 \t--- Loss: 1.557\n",
      "Iteration: 60 \t--- Loss: 1.626\n",
      "Iteration: 61 \t--- Loss: 1.565\n",
      "Iteration: 62 \t--- Loss: 1.531\n",
      "Iteration: 63 \t--- Loss: 1.535\n",
      "Iteration: 64 \t--- Loss: 1.613\n",
      "Iteration: 65 \t--- Loss: 1.594\n",
      "Iteration: 66 \t--- Loss: 1.545\n",
      "Iteration: 67 \t--- Loss: 1.642\n",
      "Iteration: 68 \t--- Loss: 1.606\n",
      "Iteration: 69 \t--- Loss: 1.568\n",
      "Iteration: 70 \t--- Loss: 1.583\n",
      "Iteration: 71 \t--- Loss: 1.589\n",
      "Iteration: 72 \t--- Loss: 1.644\n",
      "Iteration: 73 \t--- Loss: 1.666\n",
      "Iteration: 74 \t--- Loss: 1.576\n",
      "Iteration: 75 \t--- Loss: 1.553\n",
      "Iteration: 76 \t--- Loss: 1.641\n",
      "Iteration: 77 \t--- Loss: 1.539\n",
      "Iteration: 78 \t--- Loss: 1.615\n",
      "Iteration: 79 \t--- Loss: 1.600\n",
      "Iteration: 80 \t--- Loss: 1.552\n",
      "Iteration: 81 \t--- Loss: 1.587\n",
      "Iteration: 82 \t--- Loss: 1.616\n",
      "Iteration: 83 \t--- Loss: 1.585\n",
      "Iteration: 84 \t--- Loss: 1.587\n",
      "Iteration: 85 \t--- Loss: 1.662\n",
      "Iteration: 86 \t--- Loss: 1.551\n",
      "Iteration: 87 \t--- Loss: 1.602\n",
      "Iteration: 88 \t--- Loss: 1.540\n",
      "Iteration: 89 \t--- Loss: 1.669\n",
      "Iteration: 90 \t--- Loss: 1.573\n",
      "Iteration: 91 \t--- Loss: 1.570\n",
      "Iteration: 92 \t--- Loss: 1.642\n",
      "Iteration: 93 \t--- Loss: 1.585\n",
      "Iteration: 94 \t--- Loss: 1.610\n",
      "Iteration: 95 \t--- Loss: 1.610\n",
      "Iteration: 96 \t--- Loss: 1.618\n",
      "Iteration: 97 \t--- Loss: 1.541\n",
      "Iteration: 98 \t--- Loss: 1.571\n",
      "Iteration: 99 \t--- Loss: 1.624\n",
      "Iteration: 100 \t--- Loss: 1.634\n",
      "Iteration: 101 \t--- Loss: 1.573\n",
      "Iteration: 102 \t--- Loss: 1.588\n",
      "Iteration: 103 \t--- Loss: 1.620\n",
      "Iteration: 104 \t--- Loss: 1.583\n",
      "Iteration: 105 \t--- Loss: 1.604\n",
      "Iteration: 106 \t--- Loss: 1.609\n",
      "Iteration: 107 \t--- Loss: 1.594\n",
      "Iteration: 108 \t--- Loss: 1.556\n",
      "Iteration: 109 \t--- Loss: 1.593\n",
      "Iteration: 110 \t--- Loss: 1.562\n",
      "Iteration: 111 \t--- Loss: 1.602\n",
      "Iteration: 112 \t--- Loss: 1.596\n",
      "Iteration: 113 \t--- Loss: 1.628\n",
      "Iteration: 114 \t--- Loss: 1.570\n",
      "Iteration: 115 \t--- Loss: 1.618\n",
      "Iteration: 116 \t--- Loss: 1.579\n",
      "Iteration: 117 \t--- Loss: 1.606\n",
      "Iteration: 118 \t--- Loss: 1.636\n",
      "Iteration: 119 \t--- Loss: 1.580\n",
      "Iteration: 120 \t--- Loss: 1.604\n",
      "Iteration: 121 \t--- Loss: 1.616\n",
      "Iteration: 122 \t--- Loss: 1.597\n",
      "Iteration: 123 \t--- Loss: 1.617\n",
      "Iteration: 124 \t--- Loss: 1.639\n",
      "Iteration: 125 \t--- Loss: 1.632\n",
      "Iteration: 126 \t--- Loss: 1.568\n",
      "Iteration: 127 \t--- Loss: 1.625\n",
      "Iteration: 128 \t--- Loss: 1.586\n",
      "Iteration: 129 \t--- Loss: 1.549\n",
      "Iteration: 130 \t--- Loss: 1.595\n",
      "Iteration: 131 \t--- Loss: 1.597\n",
      "Iteration: 132 \t--- Loss: 1.562\n",
      "Iteration: 133 \t--- Loss: 1.600\n",
      "Iteration: 134 \t--- Loss: 1.630\n",
      "Iteration: 135 \t--- Loss: 1.646\n",
      "Iteration: 136 \t--- Loss: 1.582\n",
      "Iteration: 137 \t--- Loss: 1.635\n",
      "Iteration: 138 \t--- Loss: 1.600\n",
      "Iteration: 139 \t--- Loss: 1.596\n",
      "Iteration: 140 \t--- Loss: 1.618\n",
      "Iteration: 141 \t--- Loss: 1.614\n",
      "Iteration: 142 \t--- Loss: 1.628\n",
      "Iteration: 143 \t--- Loss: 1.599\n",
      "Iteration: 144 \t--- Loss: 1.647\n",
      "Iteration: 145 \t--- Loss: 1.592\n",
      "Iteration: 146 \t--- Loss: 1.577\n",
      "Iteration: 147 \t--- Loss: 1.595\n",
      "Iteration: 148 \t--- Loss: 1.549\n",
      "Iteration: 149 \t--- Loss: 1.573\n",
      "Iteration: 150 \t--- Loss: 1.611\n",
      "Iteration: 151 \t--- Loss: 1.610\n",
      "Iteration: 152 \t--- Loss: 1.598\n",
      "Iteration: 153 \t--- Loss: 1.600\n",
      "Iteration: 154 \t--- Loss: 1.580\n",
      "Iteration: 155 \t--- Loss: 1.635\n",
      "Iteration: 156 \t--- Loss: 1.648\n",
      "Iteration: 157 \t--- Loss: 1.645\n",
      "Iteration: 158 \t--- Loss: 1.584\n",
      "Iteration: 159 \t--- Loss: 1.653\n",
      "Iteration: 160 \t--- Loss: 1.556\n",
      "Iteration: 161 \t--- Loss: 1.620\n",
      "Iteration: 162 \t--- Loss: 1.657\n",
      "Iteration: 163 \t--- Loss: 1.574\n",
      "Iteration: 164 \t--- Loss: 1.545\n",
      "Iteration: 165 \t--- Loss: 1.615\n",
      "Iteration: 166 \t--- Loss: 1.560\n",
      "Iteration: 167 \t--- Loss: 1.622\n",
      "Iteration: 168 \t--- Loss: 1.582\n",
      "Iteration: 169 \t--- Loss: 1.603\n",
      "Iteration: 170 \t--- Loss: 1.537\n",
      "Iteration: 171 \t--- Loss: 1.564\n",
      "Iteration: 172 \t--- Loss: 1.601\n",
      "Iteration: 173 \t--- Loss: 1.623\n",
      "Iteration: 174 \t--- Loss: 1.575\n",
      "Iteration: 175 \t--- Loss: 1.579\n",
      "Iteration: 176 \t--- Loss: 1.511\n",
      "Iteration: 177 \t--- Loss: 1.593\n",
      "Iteration: 178 \t--- Loss: 1.557\n",
      "Iteration: 179 \t--- Loss: 1.564\n",
      "Iteration: 180 \t--- Loss: 1.578\n",
      "Iteration: 181 \t--- Loss: 1.548\n",
      "Iteration: 182 \t--- Loss: 1.577\n",
      "Iteration: 183 \t--- Loss: 1.638\n",
      "Iteration: 184 \t--- Loss: 1.583\n",
      "Iteration: 185 \t--- Loss: 1.601\n",
      "Iteration: 186 \t--- Loss: 1.575\n",
      "Iteration: 187 \t--- Loss: 1.599\n",
      "Iteration: 188 \t--- Loss: 1.579\n",
      "Iteration: 189 \t--- Loss: 1.602\n",
      "Iteration: 190 \t--- Loss: 1.569\n",
      "Iteration: 191 \t--- Loss: 1.526\n",
      "Iteration: 192 \t--- Loss: 1.547\n",
      "Iteration: 193 \t--- Loss: 1.609\n",
      "Iteration: 194 \t--- Loss: 1.611\n",
      "Iteration: 195 \t--- Loss: 1.595\n",
      "Iteration: 196 \t--- Loss: 1.551\n",
      "Iteration: 197 \t--- Loss: 1.617\n",
      "Iteration: 198 \t--- Loss: 1.573\n",
      "Iteration: 199 \t--- Loss: 1.632\n",
      "Iteration: 200 \t--- Loss: 1.583\n",
      "Iteration: 201 \t--- Loss: 1.579\n",
      "Iteration: 202 \t--- Loss: 1.591\n",
      "Iteration: 203 \t--- Loss: 1.575\n",
      "Iteration: 204 \t--- Loss: 1.547\n",
      "Iteration: 205 \t--- Loss: 1.611\n",
      "Iteration: 206 \t--- Loss: 1.576\n",
      "Iteration: 207 \t--- Loss: 1.609\n",
      "Iteration: 208 \t--- Loss: 1.615\n",
      "Iteration: 209 \t--- Loss: 1.553\n",
      "Iteration: 210 \t--- Loss: 1.557\n",
      "Iteration: 211 \t--- Loss: 1.570\n",
      "Iteration: 212 \t--- Loss: 1.653\n",
      "Iteration: 213 \t--- Loss: 1.639\n",
      "Iteration: 214 \t--- Loss: 1.607\n",
      "Iteration: 215 \t--- Loss: 1.545\n",
      "Iteration: 216 \t--- Loss: 1.646\n",
      "Iteration: 217 \t--- Loss: 1.629\n",
      "Iteration: 218 \t--- Loss: 1.595\n",
      "Iteration: 219 \t--- Loss: 1.608\n",
      "Iteration: 220 \t--- Loss: 1.513\n",
      "Iteration: 221 \t--- Loss: 1.593\n",
      "Iteration: 222 \t--- Loss: 1.620\n",
      "Iteration: 223 \t--- Loss: 1.609\n",
      "Iteration: 224 \t--- Loss: 1.656\n",
      "Iteration: 225 \t--- Loss: 1.534\n",
      "Iteration: 226 \t--- Loss: 1.573\n",
      "Iteration: 227 \t--- Loss: 1.565\n",
      "Iteration: 228 \t--- Loss: 1.572\n",
      "Iteration: 229 \t--- Loss: 1.589\n",
      "Iteration: 230 \t--- Loss: 1.582\n",
      "Iteration: 231 \t--- Loss: 1.595\n",
      "Iteration: 232 \t--- Loss: 1.525\n",
      "Iteration: 233 \t--- Loss: 1.584\n",
      "Iteration: 234 \t--- Loss: 1.617\n",
      "Iteration: 235 \t--- Loss: 1.588\n",
      "Iteration: 236 \t--- Loss: 1.599\n",
      "Iteration: 237 \t--- Loss: 1.601\n",
      "Iteration: 238 \t--- Loss: 1.610\n",
      "Iteration: 239 \t--- Loss: 1.598\n",
      "Iteration: 240 \t--- Loss: 1.621\n",
      "Iteration: 241 \t--- Loss: 1.595\n",
      "Iteration: 242 \t--- Loss: 1.557\n",
      "Iteration: 243 \t--- Loss: 1.495\n",
      "Iteration: 244 \t--- Loss: 1.554\n",
      "Iteration: 245 \t--- Loss: 1.584\n",
      "Iteration: 246 \t--- Loss: 1.615\n",
      "Iteration: 247 \t--- Loss: 1.566\n",
      "Iteration: 248 \t--- Loss: 1.594\n",
      "Iteration: 249 \t--- Loss: 1.591\n",
      "Iteration: 250 \t--- Loss: 1.529\n",
      "Iteration: 251 \t--- Loss: 1.604\n",
      "Iteration: 252 \t--- Loss: 1.653\n",
      "Iteration: 253 \t--- Loss: 1.639\n",
      "Iteration: 254 \t--- Loss: 1.585\n",
      "Iteration: 255 \t--- Loss: 1.688\n",
      "Iteration: 256 \t--- Loss: 1.609\n",
      "Iteration: 257 \t--- Loss: 1.543\n",
      "Iteration: 258 \t--- Loss: 1.597\n",
      "Iteration: 259 \t--- Loss: 1.582Iteration: 0 \t--- Loss: 0.562\n",
      "Iteration: 1 \t--- Loss: 0.511\n",
      "Iteration: 2 \t--- Loss: 0.460\n",
      "Iteration: 3 \t--- Loss: 0.429\n",
      "Iteration: 4 \t--- Loss: 0.401\n",
      "Iteration: 5 \t--- Loss: 0.369\n",
      "Iteration: 6 \t--- Loss: 0.356\n",
      "Iteration: 7 \t--- Loss: 0.334\n",
      "Iteration: 8 \t--- Loss: 0.325\n",
      "Iteration: 9 \t--- Loss: 0.313\n",
      "Iteration: 10 \t--- Loss: 0.298\n",
      "Iteration: 11 \t--- Loss: 0.285\n",
      "Iteration: 12 \t--- Loss: 0.285\n",
      "Iteration: 13 \t--- Loss: 0.277\n",
      "Iteration: 14 \t--- Loss: 0.269\n",
      "Iteration: 15 \t--- Loss: 0.260\n",
      "Iteration: 16 \t--- Loss: 0.269\n",
      "Iteration: 17 \t--- Loss: 0.269\n",
      "Iteration: 18 \t--- Loss: 0.260\n",
      "Iteration: 19 \t--- Loss: 0.254\n",
      "Iteration: 20 \t--- Loss: 0.247\n",
      "Iteration: 21 \t--- Loss: 0.263\n",
      "Iteration: 22 \t--- Loss: 0.248\n",
      "Iteration: 23 \t--- Loss: 0.235\n",
      "Iteration: 24 \t--- Loss: 0.243\n",
      "Iteration: 25 \t--- Loss: 0.248\n",
      "Iteration: 26 \t--- Loss: 0.247\n",
      "Iteration: 27 \t--- Loss: 0.235\n",
      "Iteration: 28 \t--- Loss: 0.238\n",
      "Iteration: 29 \t--- Loss: 0.241\n",
      "Iteration: 30 \t--- Loss: 0.222\n",
      "Iteration: 31 \t--- Loss: 0.241\n",
      "Iteration: 32 \t--- Loss: 0.241\n",
      "Iteration: 33 \t--- Loss: 0.233\n",
      "Iteration: 34 \t--- Loss: 0.241\n",
      "Iteration: 35 \t--- Loss: 0.243\n",
      "Iteration: 36 \t--- Loss: 0.237\n",
      "Iteration: 37 \t--- Loss: 0.230\n",
      "Iteration: 38 \t--- Loss: 0.236\n",
      "Iteration: 39 \t--- Loss: 0.244\n",
      "Iteration: 40 \t--- Loss: 0.245\n",
      "Iteration: 41 \t--- Loss: 0.244\n",
      "Iteration: 42 \t--- Loss: 0.237\n",
      "Iteration: 43 \t--- Loss: 0.232\n",
      "Iteration: 44 \t--- Loss: 0.231\n",
      "Iteration: 45 \t--- Loss: 0.235\n",
      "Iteration: 46 \t--- Loss: 0.233\n",
      "Iteration: 47 \t--- Loss: 0.233\n",
      "Iteration: 48 \t--- Loss: 0.232\n",
      "Iteration: 49 \t--- Loss: 0.230\n",
      "Iteration: 50 \t--- Loss: 0.221\n",
      "Iteration: 51 \t--- Loss: 0.249\n",
      "Iteration: 52 \t--- Loss: 0.229\n",
      "Iteration: 53 \t--- Loss: 0.230\n",
      "Iteration: 54 \t--- Loss: 0.235\n",
      "Iteration: 55 \t--- Loss: 0.236\n",
      "Iteration: 56 \t--- Loss: 0.223\n",
      "Iteration: 57 \t--- Loss: 0.222\n",
      "Iteration: 58 \t--- Loss: 0.240\n",
      "Iteration: 59 \t--- Loss: 0.227\n",
      "Iteration: 60 \t--- Loss: 0.244\n",
      "Iteration: 61 \t--- Loss: 0.230\n",
      "Iteration: 62 \t--- Loss: 0.222\n",
      "Iteration: 63 \t--- Loss: 0.229\n",
      "Iteration: 64 \t--- Loss: 0.222\n",
      "Iteration: 65 \t--- Loss: 0.227\n",
      "Iteration: 66 \t--- Loss: 0.235\n",
      "Iteration: 67 \t--- Loss: 0.224\n",
      "Iteration: 68 \t--- Loss: 0.236\n",
      "Iteration: 69 \t--- Loss: 0.232\n",
      "Iteration: 70 \t--- Loss: 0.237\n",
      "Iteration: 71 \t--- Loss: 0.225\n",
      "Iteration: 72 \t--- Loss: 0.232\n",
      "Iteration: 73 \t--- Loss: 0.232\n",
      "Iteration: 74 \t--- Loss: 0.229\n",
      "Iteration: 75 \t--- Loss: 0.237\n",
      "Iteration: 76 \t--- Loss: 0.236\n",
      "Iteration: 77 \t--- Loss: 0.222\n",
      "Iteration: 78 \t--- Loss: 0.227\n",
      "Iteration: 79 \t--- Loss: 0.220\n",
      "Iteration: 80 \t--- Loss: 0.240\n",
      "Iteration: 81 \t--- Loss: 0.226\n",
      "Iteration: 82 \t--- Loss: 0.226\n",
      "Iteration: 83 \t--- Loss: 0.228\n",
      "Iteration: 84 \t--- Loss: 0.219\n",
      "Iteration: 85 \t--- Loss: 0.224\n",
      "Iteration: 86 \t--- Loss: 0.237\n",
      "Iteration: 87 \t--- Loss: 0.231\n",
      "Iteration: 88 \t--- Loss: 0.239\n",
      "Iteration: 89 \t--- Loss: 0.227\n",
      "Iteration: 90 \t--- Loss: 0.222\n",
      "Iteration: 91 \t--- Loss: 0.226\n",
      "Iteration: 92 \t--- Loss: 0.226\n",
      "Iteration: 93 \t--- Loss: 0.218\n",
      "Iteration: 94 \t--- Loss: 0.241\n",
      "Iteration: 95 \t--- Loss: 0.223\n",
      "Iteration: 96 \t--- Loss: 0.239\n",
      "Iteration: 97 \t--- Loss: 0.224\n",
      "Iteration: 98 \t--- Loss: 0.239\n",
      "Iteration: 99 \t--- Loss: 0.231\n",
      "Iteration: 100 \t--- Loss: 0.226\n",
      "Iteration: 101 \t--- Loss: 0.224\n",
      "Iteration: 102 \t--- Loss: 0.236\n",
      "Iteration: 103 \t--- Loss: 0.229\n",
      "Iteration: 104 \t--- Loss: 0.238\n",
      "Iteration: 105 \t--- Loss: 0.234\n",
      "Iteration: 106 \t--- Loss: 0.236\n",
      "Iteration: 107 \t--- Loss: 0.233\n",
      "Iteration: 108 \t--- Loss: 0.235\n",
      "Iteration: 109 \t--- Loss: 0.238\n",
      "Iteration: 110 \t--- Loss: 0.221\n",
      "Iteration: 111 \t--- Loss: 0.221\n",
      "Iteration: 112 \t--- Loss: 0.225\n",
      "Iteration: 113 \t--- Loss: 0.236\n",
      "Iteration: 114 \t--- Loss: 0.232\n",
      "Iteration: 115 \t--- Loss: 0.229\n",
      "Iteration: 116 \t--- Loss: 0.230\n",
      "Iteration: 117 \t--- Loss: 0.222\n",
      "Iteration: 118 \t--- Loss: 0.225\n",
      "Iteration: 119 \t--- Loss: 0.235\n",
      "Iteration: 120 \t--- Loss: 0.235\n",
      "Iteration: 121 \t--- Loss: 0.228\n",
      "Iteration: 122 \t--- Loss: 0.225\n",
      "Iteration: 123 \t--- Loss: 0.229\n",
      "Iteration: 124 \t--- Loss: 0.239\n",
      "Iteration: 125 \t--- Loss: 0.233\n",
      "Iteration: 126 \t--- Loss: 0.248\n",
      "Iteration: 127 \t--- Loss: 0.224\n",
      "Iteration: 128 \t--- Loss: 0.233\n",
      "Iteration: 129 \t--- Loss: 0.234\n",
      "Iteration: 130 \t--- Loss: 0.225\n",
      "Iteration: 131 \t--- Loss: 0.231\n",
      "Iteration: 132 \t--- Loss: 0.218\n",
      "Iteration: 133 \t--- Loss: 0.232\n",
      "Iteration: 134 \t--- Loss: 0.239\n",
      "Iteration: 135 \t--- Loss: 0.231\n",
      "Iteration: 136 \t--- Loss: 0.223\n",
      "Iteration: 137 \t--- Loss: 0.237\n",
      "Iteration: 138 \t--- Loss: 0.236\n",
      "Iteration: 139 \t--- Loss: 0.237\n",
      "Iteration: 140 \t--- Loss: 0.218\n",
      "Iteration: 141 \t--- Loss: 0.232\n",
      "Iteration: 142 \t--- Loss: 0.212\n",
      "Iteration: 143 \t--- Loss: 0.246\n",
      "Iteration: 144 \t--- Loss: 0.229\n",
      "Iteration: 145 \t--- Loss: 0.209\n",
      "Iteration: 146 \t--- Loss: 0.230\n",
      "Iteration: 147 \t--- Loss: 0.229\n",
      "Iteration: 148 \t--- Loss: 0.232\n",
      "Iteration: 149 \t--- Loss: 0.243\n",
      "Iteration: 150 \t--- Loss: 0.228\n",
      "Iteration: 151 \t--- Loss: 0.232\n",
      "Iteration: 152 \t--- Loss: 0.226\n",
      "Iteration: 153 \t--- Loss: 0.224\n",
      "Iteration: 154 \t--- Loss: 0.235\n",
      "Iteration: 155 \t--- Loss: 0.224\n",
      "Iteration: 156 \t--- Loss: 0.222\n",
      "Iteration: 157 \t--- Loss: 0.237\n",
      "Iteration: 158 \t--- Loss: 0.228\n",
      "Iteration: 159 \t--- Loss: 0.232\n",
      "Iteration: 160 \t--- Loss: 0.227\n",
      "Iteration: 161 \t--- Loss: 0.227\n",
      "Iteration: 162 \t--- Loss: 0.229\n",
      "Iteration: 163 \t--- Loss: 0.237\n",
      "Iteration: 164 \t--- Loss: 0.240\n",
      "Iteration: 165 \t--- Loss: 0.232\n",
      "Iteration: 166 \t--- Loss: 0.233\n",
      "Iteration: 167 \t--- Loss: 0.229\n",
      "Iteration: 168 \t--- Loss: 0.235\n",
      "Iteration: 169 \t--- Loss: 0.228\n",
      "Iteration: 170 \t--- Loss: 0.225\n",
      "Iteration: 171 \t--- Loss: 0.234\n",
      "Iteration: 172 \t--- Loss: 0.217\n",
      "Iteration: 173 \t--- Loss: 0.241\n",
      "Iteration: 174 \t--- Loss: 0.233\n",
      "Iteration: 175 \t--- Loss: 0.228\n",
      "Iteration: 176 \t--- Loss: 0.230\n",
      "Iteration: 177 \t--- Loss: 0.226\n",
      "Iteration: 178 \t--- Loss: 0.236\n",
      "Iteration: 179 \t--- Loss: 0.231\n",
      "Iteration: 180 \t--- Loss: 0.224\n",
      "Iteration: 181 \t--- Loss: 0.252\n",
      "Iteration: 182 \t--- Loss: 0.237\n",
      "Iteration: 183 \t--- Loss: 0.236\n",
      "Iteration: 184 \t--- Loss: 0.223\n",
      "Iteration: 185 \t--- Loss: 0.231\n",
      "Iteration: 186 \t--- Loss: 0.242\n",
      "Iteration: 187 \t--- Loss: 0.232\n",
      "Iteration: 188 \t--- Loss: 0.242\n",
      "Iteration: 189 \t--- Loss: 0.230\n",
      "Iteration: 190 \t--- Loss: 0.229\n",
      "Iteration: 191 \t--- Loss: 0.219\n",
      "Iteration: 192 \t--- Loss: 0.237\n",
      "Iteration: 193 \t--- Loss: 0.229\n",
      "Iteration: 194 \t--- Loss: 0.226\n",
      "Iteration: 195 \t--- Loss: 0.221\n",
      "Iteration: 196 \t--- Loss: 0.230\n",
      "Iteration: 197 \t--- Loss: 0.228\n",
      "Iteration: 198 \t--- Loss: 0.236\n",
      "Iteration: 199 \t--- Loss: 0.229\n",
      "Iteration: 200 \t--- Loss: 0.219\n",
      "Iteration: 201 \t--- Loss: 0.225\n",
      "Iteration: 202 \t--- Loss: 0.229\n",
      "Iteration: 203 \t--- Loss: 0.227\n",
      "Iteration: 204 \t--- Loss: 0.233\n",
      "Iteration: 205 \t--- Loss: 0.235\n",
      "Iteration: 206 \t--- Loss: 0.244\n",
      "Iteration: 207 \t--- Loss: 0.234\n",
      "Iteration: 208 \t--- Loss: 0.215\n",
      "Iteration: 209 \t--- Loss: 0.226\n",
      "Iteration: 210 \t--- Loss: 0.227\n",
      "Iteration: 211 \t--- Loss: 0.227\n",
      "Iteration: 212 \t--- Loss: 0.239\n",
      "Iteration: 213 \t--- Loss: 0.227\n",
      "Iteration: 214 \t--- Loss: 0.231\n",
      "Iteration: 215 \t--- Loss: 0.233\n",
      "Iteration: 216 \t--- Loss: 0.229\n",
      "Iteration: 217 \t--- Loss: 0.232\n",
      "Iteration: 218 \t--- Loss: 0.218\n",
      "Iteration: 219 \t--- Loss: 0.218\n",
      "Iteration: 220 \t--- Loss: 0.235\n",
      "Iteration: 221 \t--- Loss: 0.221\n",
      "Iteration: 222 \t--- Loss: 0.220\n",
      "Iteration: 223 \t--- Loss: 0.222\n",
      "Iteration: 224 \t--- Loss: 0.225\n",
      "Iteration: 225 \t--- Loss: 0.226\n",
      "Iteration: 226 \t--- Loss: 0.233\n",
      "Iteration: 227 \t--- Loss: 0.227\n",
      "Iteration: 228 \t--- Loss: 0.235\n",
      "Iteration: 229 \t--- Loss: 0.215\n",
      "Iteration: 230 \t--- Loss: 0.227\n",
      "Iteration: 231 \t--- Loss: 0.222\n",
      "Iteration: 232 \t--- Loss: 0.236\n",
      "Iteration: 233 \t--- Loss: 0.234\n",
      "Iteration: 234 \t--- Loss: 0.235\n",
      "Iteration: 235 \t--- Loss: 0.224\n",
      "Iteration: 236 \t--- Loss: 0.227\n",
      "Iteration: 237 \t--- Loss: 0.236\n",
      "Iteration: 238 \t--- Loss: 0.222\n",
      "Iteration: 239 \t--- Loss: 0.234\n",
      "Iteration: 240 \t--- Loss: 0.223\n",
      "Iteration: 241 \t--- Loss: 0.238\n",
      "Iteration: 242 \t--- Loss: 0.230\n",
      "Iteration: 243 \t--- Loss: 0.242\n",
      "Iteration: 244 \t--- Loss: 0.231\n",
      "Iteration: 245 \t--- Loss: 0.235\n",
      "Iteration: 246 \t--- Loss: 0.240\n",
      "Iteration: 247 \t--- Loss: 0.235\n",
      "Iteration: 248 \t--- Loss: 0.244\n",
      "Iteration: 249 \t--- Loss: 0.233\n",
      "Iteration: 250 \t--- Loss: 0.234\n",
      "Iteration: 251 \t--- Loss: 0.228\n",
      "Iteration: 252 \t--- Loss: 0.230\n",
      "Iteration: 253 \t--- Loss: 0.225\n",
      "Iteration: 254 \t--- Loss: 0.221\n",
      "Iteration: 255 \t--- Loss: 0.219\n",
      "Iteration: 256 \t--- Loss: 0.241\n",
      "Iteration: 257 \t--- Loss: 0.234\n",
      "Iteration: 258 \t--- Loss: 0.229\n",
      "Iteration: 259 \t--- Loss: 0.218Iteration: 0 \t--- Loss: 0.088\n",
      "Iteration: 1 \t--- Loss: 0.082\n",
      "Iteration: 2 \t--- Loss: 0.077\n",
      "Iteration: 3 \t--- Loss: 0.081\n",
      "Iteration: 4 \t--- Loss: 0.076\n",
      "Iteration: 5 \t--- Loss: 0.081\n",
      "Iteration: 6 \t--- Loss: 0.077\n",
      "Iteration: 7 \t--- Loss: 0.073\n",
      "Iteration: 8 \t--- Loss: 0.084\n",
      "Iteration: 9 \t--- Loss: 0.082\n",
      "Iteration: 10 \t--- Loss: 0.086\n",
      "Iteration: 11 \t--- Loss: 0.079\n",
      "Iteration: 12 \t--- Loss: 0.084\n",
      "Iteration: 13 \t--- Loss: 0.079\n",
      "Iteration: 14 \t--- Loss: 0.082\n",
      "Iteration: 15 \t--- Loss: 0.077\n",
      "Iteration: 16 \t--- Loss: 0.078\n",
      "Iteration: 17 \t--- Loss: 0.078\n",
      "Iteration: 18 \t--- Loss: 0.086\n",
      "Iteration: 19 \t--- Loss: 0.079\n",
      "Iteration: 20 \t--- Loss: 0.079\n",
      "Iteration: 21 \t--- Loss: 0.074\n",
      "Iteration: 22 \t--- Loss: 0.085\n",
      "Iteration: 23 \t--- Loss: 0.078\n",
      "Iteration: 24 \t--- Loss: 0.075\n",
      "Iteration: 25 \t--- Loss: 0.083\n",
      "Iteration: 26 \t--- Loss: 0.076\n",
      "Iteration: 27 \t--- Loss: 0.076\n",
      "Iteration: 28 \t--- Loss: 0.076\n",
      "Iteration: 29 \t--- Loss: 0.080\n",
      "Iteration: 30 \t--- Loss: 0.083\n",
      "Iteration: 31 \t--- Loss: 0.077\n",
      "Iteration: 32 \t--- Loss: 0.084\n",
      "Iteration: 33 \t--- Loss: 0.082\n",
      "Iteration: 34 \t--- Loss: 0.081\n",
      "Iteration: 35 \t--- Loss: 0.078\n",
      "Iteration: 36 \t--- Loss: 0.077\n",
      "Iteration: 37 \t--- Loss: 0.081\n",
      "Iteration: 38 \t--- Loss: 0.085\n",
      "Iteration: 39 \t--- Loss: 0.075\n",
      "Iteration: 40 \t--- Loss: 0.077\n",
      "Iteration: 41 \t--- Loss: 0.077\n",
      "Iteration: 42 \t--- Loss: 0.080\n",
      "Iteration: 43 \t--- Loss: 0.082\n",
      "Iteration: 44 \t--- Loss: 0.076\n",
      "Iteration: 45 \t--- Loss: 0.078\n",
      "Iteration: 46 \t--- Loss: 0.079\n",
      "Iteration: 47 \t--- Loss: 0.076\n",
      "Iteration: 48 \t--- Loss: 0.082\n",
      "Iteration: 49 \t--- Loss: 0.079\n",
      "Iteration: 50 \t--- Loss: 0.078\n",
      "Iteration: 51 \t--- Loss: 0.077\n",
      "Iteration: 52 \t--- Loss: 0.081\n",
      "Iteration: 53 \t--- Loss: 0.082\n",
      "Iteration: 54 \t--- Loss: 0.077\n",
      "Iteration: 55 \t--- Loss: 0.082\n",
      "Iteration: 56 \t--- Loss: 0.079\n",
      "Iteration: 57 \t--- Loss: 0.075\n",
      "Iteration: 58 \t--- Loss: 0.077\n",
      "Iteration: 59 \t--- Loss: 0.075\n",
      "Iteration: 60 \t--- Loss: 0.077\n",
      "Iteration: 61 \t--- Loss: 0.078\n",
      "Iteration: 62 \t--- Loss: 0.080\n",
      "Iteration: 63 \t--- Loss: 0.080\n",
      "Iteration: 64 \t--- Loss: 0.077\n",
      "Iteration: 65 \t--- Loss: 0.076\n",
      "Iteration: 66 \t--- Loss: 0.085\n",
      "Iteration: 67 \t--- Loss: 0.073\n",
      "Iteration: 68 \t--- Loss: 0.076\n",
      "Iteration: 69 \t--- Loss: 0.075\n",
      "Iteration: 70 \t--- Loss: 0.064\n",
      "Iteration: 71 \t--- Loss: 0.059\n",
      "Iteration: 72 \t--- Loss: 0.044\n",
      "Iteration: 73 \t--- Loss: 0.040\n",
      "Iteration: 74 \t--- Loss: 0.042\n",
      "Iteration: 75 \t--- Loss: 0.043\n",
      "Iteration: 76 \t--- Loss: 0.039\n",
      "Iteration: 77 \t--- Loss: 0.039\n",
      "Iteration: 78 \t--- Loss: 0.052\n",
      "Iteration: 79 \t--- Loss: 0.042\n",
      "Iteration: 80 \t--- Loss: 0.042\n",
      "Iteration: 81 \t--- Loss: 0.042\n",
      "Iteration: 82 \t--- Loss: 0.041\n",
      "Iteration: 83 \t--- Loss: 0.036\n",
      "Iteration: 84 \t--- Loss: 0.042\n",
      "Iteration: 85 \t--- Loss: 0.037\n",
      "Iteration: 86 \t--- Loss: 0.044\n",
      "Iteration: 87 \t--- Loss: 0.040\n",
      "Iteration: 88 \t--- Loss: 0.044\n",
      "Iteration: 89 \t--- Loss: 0.039\n",
      "Iteration: 90 \t--- Loss: 0.045\n",
      "Iteration: 91 \t--- Loss: 0.048\n",
      "Iteration: 92 \t--- Loss: 0.048\n",
      "Iteration: 93 \t--- Loss: 0.059\n",
      "Iteration: 94 \t--- Loss: 0.046\n",
      "Iteration: 95 \t--- Loss: 0.057\n",
      "Iteration: 96 \t--- Loss: 0.063\n",
      "Iteration: 97 \t--- Loss: 0.100\n",
      "Iteration: 98 \t--- Loss: 0.037\n",
      "Iteration: 99 \t--- Loss: 0.064\n",
      "Iteration: 100 \t--- Loss: 0.114\n",
      "Iteration: 101 \t--- Loss: 0.067\n",
      "Iteration: 102 \t--- Loss: 0.033\n",
      "Iteration: 103 \t--- Loss: 0.032\n",
      "Iteration: 104 \t--- Loss: 0.036\n",
      "Iteration: 105 \t--- Loss: 0.034\n",
      "Iteration: 106 \t--- Loss: 0.037\n",
      "Iteration: 107 \t--- Loss: 0.037\n",
      "Iteration: 108 \t--- Loss: 0.070\n",
      "Iteration: 109 \t--- Loss: 0.113\n",
      "Iteration: 110 \t--- Loss: 0.048\n",
      "Iteration: 111 \t--- Loss: 0.052\n",
      "Iteration: 112 \t--- Loss: 0.072\n",
      "Iteration: 113 \t--- Loss: 0.039\n",
      "Iteration: 114 \t--- Loss: 0.044\n",
      "Iteration: 115 \t--- Loss: 0.061\n",
      "Iteration: 116 \t--- Loss: 0.088\n",
      "Iteration: 117 \t--- Loss: 0.029\n",
      "Iteration: 118 \t--- Loss: 0.037\n",
      "Iteration: 119 \t--- Loss: 0.049\n",
      "Iteration: 120 \t--- Loss: 0.039\n",
      "Iteration: 121 \t--- Loss: 0.058\n",
      "Iteration: 122 \t--- Loss: 0.031\n",
      "Iteration: 123 \t--- Loss: 0.033\n",
      "Iteration: 124 \t--- Loss: 0.037\n",
      "Iteration: 125 \t--- Loss: 0.043\n",
      "Iteration: 126 \t--- Loss: 0.064\n",
      "Iteration: 127 \t--- Loss: 0.114\n",
      "Iteration: 128 \t--- Loss: 0.051\n",
      "Iteration: 129 \t--- Loss: 0.027\n",
      "Iteration: 130 \t--- Loss: 0.030\n",
      "Iteration: 131 \t--- Loss: 0.040\n",
      "Iteration: 132 \t--- Loss: 0.053\n",
      "Iteration: 133 \t--- Loss: 0.031\n",
      "Iteration: 134 \t--- Loss: 0.030\n",
      "Iteration: 135 \t--- Loss: 0.030\n",
      "Iteration: 136 \t--- Loss: 0.033\n",
      "Iteration: 137 \t--- Loss: 0.030\n",
      "Iteration: 138 \t--- Loss: 0.032\n",
      "Iteration: 139 \t--- Loss: 0.051\n",
      "Iteration: 140 \t--- Loss: 0.085\n",
      "Iteration: 141 \t--- Loss: 0.030\n",
      "Iteration: 142 \t--- Loss: 0.039\n",
      "Iteration: 143 \t--- Loss: 0.052\n",
      "Iteration: 144 \t--- Loss: 0.030\n",
      "Iteration: 145 \t--- Loss: 0.029\n",
      "Iteration: 146 \t--- Loss: 0.028\n",
      "Iteration: 147 \t--- Loss: 0.034\n",
      "Iteration: 148 \t--- Loss: 0.024\n",
      "Iteration: 149 \t--- Loss: 0.022\n",
      "Iteration: 150 \t--- Loss: 0.026\n",
      "Iteration: 151 \t--- Loss: 0.029\n",
      "Iteration: 152 \t--- Loss: 0.026\n",
      "Iteration: 153 \t--- Loss: 0.035\n",
      "Iteration: 154 \t--- Loss: 0.039\n",
      "Iteration: 155 \t--- Loss: 0.066\n",
      "Iteration: 156 \t--- Loss: 0.016\n",
      "Iteration: 157 \t--- Loss: 0.017\n",
      "Iteration: 158 \t--- Loss: 0.017\n",
      "Iteration: 159 \t--- Loss: 0.017\n",
      "Iteration: 160 \t--- Loss: 0.022\n",
      "Iteration: 161 \t--- Loss: 0.023\n",
      "Iteration: 162 \t--- Loss: 0.024\n",
      "Iteration: 163 \t--- Loss: 0.032\n",
      "Iteration: 164 \t--- Loss: 0.030\n",
      "Iteration: 165 \t--- Loss: 0.048\n",
      "Iteration: 166 \t--- Loss: 0.020\n",
      "Iteration: 167 \t--- Loss: 0.025\n",
      "Iteration: 168 \t--- Loss: 0.026\n",
      "Iteration: 169 \t--- Loss: 0.032\n",
      "Iteration: 170 \t--- Loss: 0.022\n",
      "Iteration: 171 \t--- Loss: 0.028\n",
      "Iteration: 172 \t--- Loss: 0.028\n",
      "Iteration: 173 \t--- Loss: 0.034\n",
      "Iteration: 174 \t--- Loss: 0.028\n",
      "Iteration: 175 \t--- Loss: 0.048\n",
      "Iteration: 176 \t--- Loss: 0.016\n",
      "Iteration: 177 \t--- Loss: 0.014\n",
      "Iteration: 178 \t--- Loss: 0.015\n",
      "Iteration: 179 \t--- Loss: 0.014\n",
      "Iteration: 180 \t--- Loss: 0.015\n",
      "Iteration: 181 \t--- Loss: 0.014\n",
      "Iteration: 182 \t--- Loss: 0.017\n",
      "Iteration: 183 \t--- Loss: 0.016\n",
      "Iteration: 184 \t--- Loss: 0.017\n",
      "Iteration: 185 \t--- Loss: 0.019\n",
      "Iteration: 186 \t--- Loss: 0.023\n",
      "Iteration: 187 \t--- Loss: 0.036\n",
      "Iteration: 188 \t--- Loss: 0.029\n",
      "Iteration: 189 \t--- Loss: 0.045\n",
      "Iteration: 190 \t--- Loss: 0.011\n",
      "Iteration: 191 \t--- Loss: 0.011\n",
      "Iteration: 192 \t--- Loss: 0.011\n",
      "Iteration: 193 \t--- Loss: 0.011\n",
      "Iteration: 194 \t--- Loss: 0.011\n",
      "Iteration: 195 \t--- Loss: 0.010\n",
      "Iteration: 196 \t--- Loss: 0.011\n",
      "Iteration: 197 \t--- Loss: 0.010\n",
      "Iteration: 198 \t--- Loss: 0.013\n",
      "Iteration: 199 \t--- Loss: 0.013\n",
      "Iteration: 200 \t--- Loss: 0.018\n",
      "Iteration: 201 \t--- Loss: 0.022\n",
      "Iteration: 202 \t--- Loss: 0.025\n",
      "Iteration: 203 \t--- Loss: 0.038\n",
      "Iteration: 204 \t--- Loss: 0.013\n",
      "Iteration: 205 \t--- Loss: 0.020\n",
      "Iteration: 206 \t--- Loss: 0.024\n",
      "Iteration: 207 \t--- Loss: 0.031\n",
      "Iteration: 208 \t--- Loss: 0.010\n",
      "Iteration: 209 \t--- Loss: 0.010\n",
      "Iteration: 210 \t--- Loss: 0.014\n",
      "Iteration: 211 \t--- Loss: 0.012\n",
      "Iteration: 212 \t--- Loss: 0.012\n",
      "Iteration: 213 \t--- Loss: 0.013\n",
      "Iteration: 214 \t--- Loss: 0.012\n",
      "Iteration: 215 \t--- Loss: 0.014\n",
      "Iteration: 216 \t--- Loss: 0.017\n",
      "Iteration: 217 \t--- Loss: 0.020\n",
      "Iteration: 218 \t--- Loss: 0.016\n",
      "Iteration: 219 \t--- Loss: 0.016\n",
      "Iteration: 220 \t--- Loss: 0.012\n",
      "Iteration: 221 \t--- Loss: 0.010\n",
      "Iteration: 222 \t--- Loss: 0.010\n",
      "Iteration: 223 \t--- Loss: 0.009\n",
      "Iteration: 224 \t--- Loss: 0.009\n",
      "Iteration: 225 \t--- Loss: 0.009\n",
      "Iteration: 226 \t--- Loss: 0.011\n",
      "Iteration: 227 \t--- Loss: 0.011\n",
      "Iteration: 228 \t--- Loss: 0.012\n",
      "Iteration: 229 \t--- Loss: 0.015\n",
      "Iteration: 230 \t--- Loss: 0.014\n",
      "Iteration: 231 \t--- Loss: 0.016\n",
      "Iteration: 232 \t--- Loss: 0.013\n",
      "Iteration: 233 \t--- Loss: 0.015\n",
      "Iteration: 234 \t--- Loss: 0.012\n",
      "Iteration: 235 \t--- Loss: 0.014\n",
      "Iteration: 236 \t--- Loss: 0.011\n",
      "Iteration: 237 \t--- Loss: 0.011\n",
      "Iteration: 238 \t--- Loss: 0.008\n",
      "Iteration: 239 \t--- Loss: 0.008\n",
      "Iteration: 240 \t--- Loss: 0.011\n",
      "Iteration: 241 \t--- Loss: 0.011\n",
      "Iteration: 242 \t--- Loss: 0.009\n",
      "Iteration: 243 \t--- Loss: 0.010\n",
      "Iteration: 244 \t--- Loss: 0.009\n",
      "Iteration: 245 \t--- Loss: 0.008\n",
      "Iteration: 246 \t--- Loss: 0.010\n",
      "Iteration: 247 \t--- Loss: 0.008\n",
      "Iteration: 248 \t--- Loss: 0.008\n",
      "Iteration: 249 \t--- Loss: 0.007\n",
      "Iteration: 250 \t--- Loss: 0.006\n",
      "Iteration: 251 \t--- Loss: 0.006\n",
      "Iteration: 252 \t--- Loss: 0.007\n",
      "Iteration: 253 \t--- Loss: 0.007\n",
      "Iteration: 254 \t--- Loss: 0.007\n",
      "Iteration: 255 \t--- Loss: 0.007\n",
      "Iteration: 256 \t--- Loss: 0.007\n",
      "Iteration: 257 \t--- Loss: 0.007\n",
      "Iteration: 258 \t--- Loss: 0.007\n",
      "Iteration: 259 \t--- Loss: 0.007Iteration: 0 \t--- Loss: 0.087\n",
      "Iteration: 1 \t--- Loss: 0.085\n",
      "Iteration: 2 \t--- Loss: 0.087\n",
      "Iteration: 3 \t--- Loss: 0.089\n",
      "Iteration: 4 \t--- Loss: 0.090\n",
      "Iteration: 5 \t--- Loss: 0.089\n",
      "Iteration: 6 \t--- Loss: 0.090\n",
      "Iteration: 7 \t--- Loss: 0.084\n",
      "Iteration: 8 \t--- Loss: 0.088\n",
      "Iteration: 9 \t--- Loss: 0.087\n",
      "Iteration: 10 \t--- Loss: 0.092\n",
      "Iteration: 11 \t--- Loss: 0.088\n",
      "Iteration: 12 \t--- Loss: 0.088\n",
      "Iteration: 13 \t--- Loss: 0.087\n",
      "Iteration: 14 \t--- Loss: 0.084\n",
      "Iteration: 15 \t--- Loss: 0.089\n",
      "Iteration: 16 \t--- Loss: 0.089\n",
      "Iteration: 17 \t--- Loss: 0.084\n",
      "Iteration: 18 \t--- Loss: 0.087\n",
      "Iteration: 19 \t--- Loss: 0.087\n",
      "Iteration: 20 \t--- Loss: 0.091\n",
      "Iteration: 21 \t--- Loss: 0.088\n",
      "Iteration: 22 \t--- Loss: 0.090\n",
      "Iteration: 23 \t--- Loss: 0.085\n",
      "Iteration: 24 \t--- Loss: 0.094\n",
      "Iteration: 25 \t--- Loss: 0.082\n",
      "Iteration: 26 \t--- Loss: 0.087\n",
      "Iteration: 27 \t--- Loss: 0.089\n",
      "Iteration: 28 \t--- Loss: 0.086\n",
      "Iteration: 29 \t--- Loss: 0.090\n",
      "Iteration: 30 \t--- Loss: 0.087\n",
      "Iteration: 31 \t--- Loss: 0.088\n",
      "Iteration: 32 \t--- Loss: 0.086\n",
      "Iteration: 33 \t--- Loss: 0.082\n",
      "Iteration: 34 \t--- Loss: 0.086\n",
      "Iteration: 35 \t--- Loss: 0.088\n",
      "Iteration: 36 \t--- Loss: 0.088\n",
      "Iteration: 37 \t--- Loss: 0.089\n",
      "Iteration: 38 \t--- Loss: 0.089\n",
      "Iteration: 39 \t--- Loss: 0.089\n",
      "Iteration: 40 \t--- Loss: 0.089\n",
      "Iteration: 41 \t--- Loss: 0.090\n",
      "Iteration: 42 \t--- Loss: 0.082\n",
      "Iteration: 43 \t--- Loss: 0.087\n",
      "Iteration: 44 \t--- Loss: 0.086\n",
      "Iteration: 45 \t--- Loss: 0.084\n",
      "Iteration: 46 \t--- Loss: 0.092\n",
      "Iteration: 47 \t--- Loss: 0.087\n",
      "Iteration: 48 \t--- Loss: 0.091\n",
      "Iteration: 49 \t--- Loss: 0.091\n",
      "Iteration: 50 \t--- Loss: 0.084\n",
      "Iteration: 51 \t--- Loss: 0.087\n",
      "Iteration: 52 \t--- Loss: 0.090\n",
      "Iteration: 53 \t--- Loss: 0.088\n",
      "Iteration: 54 \t--- Loss: 0.085\n",
      "Iteration: 55 \t--- Loss: 0.090\n",
      "Iteration: 56 \t--- Loss: 0.086\n",
      "Iteration: 57 \t--- Loss: 0.090\n",
      "Iteration: 58 \t--- Loss: 0.087\n",
      "Iteration: 59 \t--- Loss: 0.084\n",
      "Iteration: 60 \t--- Loss: 0.078\n",
      "Iteration: 61 \t--- Loss: 0.075\n",
      "Iteration: 62 \t--- Loss: 0.063\n",
      "Iteration: 63 \t--- Loss: 0.054\n",
      "Iteration: 64 \t--- Loss: 0.050\n",
      "Iteration: 65 \t--- Loss: 0.045\n",
      "Iteration: 66 \t--- Loss: 0.050\n",
      "Iteration: 67 \t--- Loss: 0.047\n",
      "Iteration: 68 \t--- Loss: 0.047\n",
      "Iteration: 69 \t--- Loss: 0.050\n",
      "Iteration: 70 \t--- Loss: 0.052\n",
      "Iteration: 71 \t--- Loss: 0.049\n",
      "Iteration: 72 \t--- Loss: 0.049\n",
      "Iteration: 73 \t--- Loss: 0.050\n",
      "Iteration: 74 \t--- Loss: 0.046\n",
      "Iteration: 75 \t--- Loss: 0.046\n",
      "Iteration: 76 \t--- Loss: 0.048\n",
      "Iteration: 77 \t--- Loss: 0.048\n",
      "Iteration: 78 \t--- Loss: 0.050\n",
      "Iteration: 79 \t--- Loss: 0.046\n",
      "Iteration: 80 \t--- Loss: 0.049\n",
      "Iteration: 81 \t--- Loss: 0.057\n",
      "Iteration: 82 \t--- Loss: 0.061\n",
      "Iteration: 83 \t--- Loss: 0.075\n",
      "Iteration: 84 \t--- Loss: 0.143\n",
      "Iteration: 85 \t--- Loss: 0.095\n",
      "Iteration: 86 \t--- Loss: 0.044\n",
      "Iteration: 87 \t--- Loss: 0.041\n",
      "Iteration: 88 \t--- Loss: 0.050\n",
      "Iteration: 89 \t--- Loss: 0.052\n",
      "Iteration: 90 \t--- Loss: 0.080\n",
      "Iteration: 91 \t--- Loss: 0.135\n",
      "Iteration: 92 \t--- Loss: 0.093\n",
      "Iteration: 93 \t--- Loss: 0.046\n",
      "Iteration: 94 \t--- Loss: 0.051\n",
      "Iteration: 95 \t--- Loss: 0.058\n",
      "Iteration: 96 \t--- Loss: 0.048\n",
      "Iteration: 97 \t--- Loss: 0.057\n",
      "Iteration: 98 \t--- Loss: 0.057\n",
      "Iteration: 99 \t--- Loss: 0.082\n",
      "Iteration: 100 \t--- Loss: 0.038\n",
      "Iteration: 101 \t--- Loss: 0.039\n",
      "Iteration: 102 \t--- Loss: 0.040\n",
      "Iteration: 103 \t--- Loss: 0.047\n",
      "Iteration: 104 \t--- Loss: 0.068\n",
      "Iteration: 105 \t--- Loss: 0.057\n",
      "Iteration: 106 \t--- Loss: 0.094\n",
      "Iteration: 107 \t--- Loss: 0.032\n",
      "Iteration: 108 \t--- Loss: 0.040\n",
      "Iteration: 109 \t--- Loss: 0.037\n",
      "Iteration: 110 \t--- Loss: 0.049\n",
      "Iteration: 111 \t--- Loss: 0.073\n",
      "Iteration: 112 \t--- Loss: 0.039\n",
      "Iteration: 113 \t--- Loss: 0.047\n",
      "Iteration: 114 \t--- Loss: 0.065\n",
      "Iteration: 115 \t--- Loss: 0.113\n",
      "Iteration: 116 \t--- Loss: 0.057\n",
      "Iteration: 117 \t--- Loss: 0.037\n",
      "Iteration: 118 \t--- Loss: 0.040\n",
      "Iteration: 119 \t--- Loss: 0.037\n",
      "Iteration: 120 \t--- Loss: 0.041\n",
      "Iteration: 121 \t--- Loss: 0.048\n",
      "Iteration: 122 \t--- Loss: 0.065\n",
      "Iteration: 123 \t--- Loss: 0.027\n",
      "Iteration: 124 \t--- Loss: 0.030\n",
      "Iteration: 125 \t--- Loss: 0.027\n",
      "Iteration: 126 \t--- Loss: 0.029\n",
      "Iteration: 127 \t--- Loss: 0.032\n",
      "Iteration: 128 \t--- Loss: 0.040\n",
      "Iteration: 129 \t--- Loss: 0.043\n",
      "Iteration: 130 \t--- Loss: 0.062\n",
      "Iteration: 131 \t--- Loss: 0.029\n",
      "Iteration: 132 \t--- Loss: 0.031\n",
      "Iteration: 133 \t--- Loss: 0.040\n",
      "Iteration: 134 \t--- Loss: 0.065\n",
      "Iteration: 135 \t--- Loss: 0.028\n",
      "Iteration: 136 \t--- Loss: 0.034\n",
      "Iteration: 137 \t--- Loss: 0.036\n",
      "Iteration: 138 \t--- Loss: 0.052\n",
      "Iteration: 139 \t--- Loss: 0.032\n",
      "Iteration: 140 \t--- Loss: 0.039\n",
      "Iteration: 141 \t--- Loss: 0.033\n",
      "Iteration: 142 \t--- Loss: 0.047\n",
      "Iteration: 143 \t--- Loss: 0.035\n",
      "Iteration: 144 \t--- Loss: 0.043\n",
      "Iteration: 145 \t--- Loss: 0.026\n",
      "Iteration: 146 \t--- Loss: 0.028\n",
      "Iteration: 147 \t--- Loss: 0.026\n",
      "Iteration: 148 \t--- Loss: 0.028\n",
      "Iteration: 149 \t--- Loss: 0.033\n",
      "Iteration: 150 \t--- Loss: 0.045\n",
      "Iteration: 151 \t--- Loss: 0.031\n",
      "Iteration: 152 \t--- Loss: 0.040\n",
      "Iteration: 153 \t--- Loss: 0.025\n",
      "Iteration: 154 \t--- Loss: 0.026\n",
      "Iteration: 155 \t--- Loss: 0.030\n",
      "Iteration: 156 \t--- Loss: 0.039\n",
      "Iteration: 157 \t--- Loss: 0.029\n",
      "Iteration: 158 \t--- Loss: 0.040\n",
      "Iteration: 159 \t--- Loss: 0.020\n",
      "Iteration: 160 \t--- Loss: 0.017\n",
      "Iteration: 161 \t--- Loss: 0.020\n",
      "Iteration: 162 \t--- Loss: 0.020\n",
      "Iteration: 163 \t--- Loss: 0.022\n",
      "Iteration: 164 \t--- Loss: 0.024\n",
      "Iteration: 165 \t--- Loss: 0.024\n",
      "Iteration: 166 \t--- Loss: 0.025\n",
      "Iteration: 167 \t--- Loss: 0.025\n",
      "Iteration: 168 \t--- Loss: 0.029\n",
      "Iteration: 169 \t--- Loss: 0.026\n",
      "Iteration: 170 \t--- Loss: 0.032\n",
      "Iteration: 171 \t--- Loss: 0.024\n",
      "Iteration: 172 \t--- Loss: 0.029\n",
      "Iteration: 173 \t--- Loss: 0.023\n",
      "Iteration: 174 \t--- Loss: 0.027\n",
      "Iteration: 175 \t--- Loss: 0.019\n",
      "Iteration: 176 \t--- Loss: 0.023\n",
      "Iteration: 177 \t--- Loss: 0.021\n",
      "Iteration: 178 \t--- Loss: 0.019\n",
      "Iteration: 179 \t--- Loss: 0.020\n",
      "Iteration: 180 \t--- Loss: 0.023\n",
      "Iteration: 181 \t--- Loss: 0.025\n",
      "Iteration: 182 \t--- Loss: 0.029\n",
      "Iteration: 183 \t--- Loss: 0.023\n",
      "Iteration: 184 \t--- Loss: 0.028\n",
      "Iteration: 185 \t--- Loss: 0.020\n",
      "Iteration: 186 \t--- Loss: 0.025\n",
      "Iteration: 187 \t--- Loss: 0.019\n",
      "Iteration: 188 \t--- Loss: 0.021\n",
      "Iteration: 189 \t--- Loss: 0.016\n",
      "Iteration: 190 \t--- Loss: 0.015\n",
      "Iteration: 191 \t--- Loss: 0.016\n",
      "Iteration: 192 \t--- Loss: 0.015\n",
      "Iteration: 193 \t--- Loss: 0.015\n",
      "Iteration: 194 \t--- Loss: 0.016\n",
      "Iteration: 195 \t--- Loss: 0.017\n",
      "Iteration: 196 \t--- Loss: 0.018\n",
      "Iteration: 197 \t--- Loss: 0.014\n",
      "Iteration: 198 \t--- Loss: 0.014\n",
      "Iteration: 199 \t--- Loss: 0.015\n",
      "Iteration: 200 \t--- Loss: 0.015\n",
      "Iteration: 201 \t--- Loss: 0.014\n",
      "Iteration: 202 \t--- Loss: 0.014\n",
      "Iteration: 203 \t--- Loss: 0.014\n",
      "Iteration: 204 \t--- Loss: 0.013\n",
      "Iteration: 205 \t--- Loss: 0.013\n",
      "Iteration: 206 \t--- Loss: 0.013\n",
      "Iteration: 207 \t--- Loss: 0.012\n",
      "Iteration: 208 \t--- Loss: 0.011\n",
      "Iteration: 209 \t--- Loss: 0.013\n",
      "Iteration: 210 \t--- Loss: 0.014\n",
      "Iteration: 211 \t--- Loss: 0.014\n",
      "Iteration: 212 \t--- Loss: 0.015\n",
      "Iteration: 213 \t--- Loss: 0.013\n",
      "Iteration: 214 \t--- Loss: 0.011\n",
      "Iteration: 215 \t--- Loss: 0.012\n",
      "Iteration: 216 \t--- Loss: 0.011\n",
      "Iteration: 217 \t--- Loss: 0.012\n",
      "Iteration: 218 \t--- Loss: 0.013\n",
      "Iteration: 219 \t--- Loss: 0.013\n",
      "Iteration: 220 \t--- Loss: 0.015\n",
      "Iteration: 221 \t--- Loss: 0.012\n",
      "Iteration: 222 \t--- Loss: 0.012\n",
      "Iteration: 223 \t--- Loss: 0.012\n",
      "Iteration: 224 \t--- Loss: 0.011\n",
      "Iteration: 225 \t--- Loss: 0.013\n",
      "Iteration: 226 \t--- Loss: 0.014\n",
      "Iteration: 227 \t--- Loss: 0.013\n",
      "Iteration: 228 \t--- Loss: 0.015\n",
      "Iteration: 229 \t--- Loss: 0.012\n",
      "Iteration: 230 \t--- Loss: 0.011\n",
      "Iteration: 231 \t--- Loss: 0.010\n",
      "Iteration: 232 \t--- Loss: 0.011\n",
      "Iteration: 233 \t--- Loss: 0.011\n",
      "Iteration: 234 \t--- Loss: 0.012\n",
      "Iteration: 235 \t--- Loss: 0.011\n",
      "Iteration: 236 \t--- Loss: 0.011\n",
      "Iteration: 237 \t--- Loss: 0.009\n",
      "Iteration: 238 \t--- Loss: 0.010\n",
      "Iteration: 239 \t--- Loss: 0.009\n",
      "Iteration: 240 \t--- Loss: 0.008\n",
      "Iteration: 241 \t--- Loss: 0.009\n",
      "Iteration: 242 \t--- Loss: 0.008\n",
      "Iteration: 243 \t--- Loss: 0.009\n",
      "Iteration: 244 \t--- Loss: 0.008\n",
      "Iteration: 245 \t--- Loss: 0.008\n",
      "Iteration: 246 \t--- Loss: 0.007\n",
      "Iteration: 247 \t--- Loss: 0.008\n",
      "Iteration: 248 \t--- Loss: 0.007\n",
      "Iteration: 249 \t--- Loss: 0.007\n",
      "Iteration: 250 \t--- Loss: 0.007\n",
      "Iteration: 251 \t--- Loss: 0.008\n",
      "Iteration: 252 \t--- Loss: 0.007\n",
      "Iteration: 253 \t--- Loss: 0.008\n",
      "Iteration: 254 \t--- Loss: 0.007\n",
      "Iteration: 255 \t--- Loss: 0.008\n",
      "Iteration: 256 \t--- Loss: 0.007\n",
      "Iteration: 257 \t--- Loss: 0.008\n",
      "Iteration: 258 \t--- Loss: 0.008\n",
      "Iteration: 259 \t--- Loss: 0.008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:25<00:00, 85.80s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 1.561\n",
      "Iteration: 261 \t--- Loss: 1.537\n",
      "Iteration: 262 \t--- Loss: 1.648\n",
      "Iteration: 263 \t--- Loss: 1.657\n",
      "Iteration: 264 \t--- Loss: 1.588\n",
      "Iteration: 265 \t--- Loss: 1.654\n",
      "Iteration: 266 \t--- Loss: 1.637\n",
      "Iteration: 267 \t--- Loss: 1.671\n",
      "Iteration: 268 \t--- Loss: 1.614\n",
      "Iteration: 269 \t--- Loss: 1.593\n",
      "Iteration: 270 \t--- Loss: 1.529\n",
      "Iteration: 271 \t--- Loss: 1.628\n",
      "Iteration: 272 \t--- Loss: 1.586\n",
      "Iteration: 273 \t--- Loss: 1.589\n",
      "Iteration: 274 \t--- Loss: 1.607\n",
      "Iteration: 275 \t--- Loss: 1.530\n",
      "Iteration: 276 \t--- Loss: 1.615\n",
      "Iteration: 277 \t--- Loss: 1.585\n",
      "Iteration: 278 \t--- Loss: 1.573\n",
      "Iteration: 279 \t--- Loss: 1.525\n",
      "Iteration: 280 \t--- Loss: 1.595\n",
      "Iteration: 281 \t--- Loss: 1.664\n",
      "Iteration: 282 \t--- Loss: 1.668\n",
      "Iteration: 283 \t--- Loss: 1.557\n",
      "Iteration: 284 \t--- Loss: 1.659\n",
      "Iteration: 285 \t--- Loss: 1.597\n",
      "Iteration: 286 \t--- Loss: 1.594\n",
      "Iteration: 287 \t--- Loss: 1.574\n",
      "Iteration: 288 \t--- Loss: 1.531\n",
      "Iteration: 289 \t--- Loss: 1.531\n",
      "Iteration: 290 \t--- Loss: 1.572\n",
      "Iteration: 291 \t--- Loss: 1.597\n",
      "Iteration: 292 \t--- Loss: 1.630\n",
      "Iteration: 293 \t--- Loss: 1.605\n",
      "Iteration: 294 \t--- Loss: 1.610\n",
      "Iteration: 295 \t--- Loss: 1.513\n",
      "Iteration: 296 \t--- Loss: 1.629\n",
      "Iteration: 297 \t--- Loss: 1.621\n",
      "Iteration: 298 \t--- Loss: 1.585\n",
      "Iteration: 299 \t--- Loss: 1.616\n",
      "Iteration: 300 \t--- Loss: 1.617\n",
      "Iteration: 301 \t--- Loss: 1.541\n",
      "Iteration: 302 \t--- Loss: 1.643\n",
      "Iteration: 303 \t--- Loss: 1.589\n",
      "Iteration: 304 \t--- Loss: 1.587\n",
      "Iteration: 305 \t--- Loss: 1.595\n",
      "Iteration: 306 \t--- Loss: 1.595\n",
      "Iteration: 307 \t--- Loss: 1.575\n",
      "Iteration: 308 \t--- Loss: 1.647\n",
      "Iteration: 309 \t--- Loss: 1.610\n",
      "Iteration: 310 \t--- Loss: 1.496\n",
      "Iteration: 311 \t--- Loss: 1.624\n",
      "Iteration: 312 \t--- Loss: 1.606\n",
      "Iteration: 313 \t--- Loss: 1.635\n",
      "Iteration: 314 \t--- Loss: 1.625\n",
      "Iteration: 315 \t--- Loss: 1.601\n",
      "Iteration: 316 \t--- Loss: 1.590\n",
      "Iteration: 317 \t--- Loss: 1.648\n",
      "Iteration: 318 \t--- Loss: 1.631\n",
      "Iteration: 319 \t--- Loss: 1.564\n",
      "Iteration: 320 \t--- Loss: 1.579\n",
      "Iteration: 321 \t--- Loss: 1.566\n",
      "Iteration: 322 \t--- Loss: 1.630\n",
      "Iteration: 323 \t--- Loss: 1.620\n",
      "Iteration: 324 \t--- Loss: 1.652\n",
      "Iteration: 325 \t--- Loss: 1.579\n",
      "Iteration: 326 \t--- Loss: 1.601\n",
      "Iteration: 327 \t--- Loss: 1.564\n",
      "Iteration: 328 \t--- Loss: 1.602\n",
      "Iteration: 329 \t--- Loss: 1.605\n",
      "Iteration: 330 \t--- Loss: 1.607\n",
      "Iteration: 331 \t--- Loss: 1.586\n",
      "Iteration: 332 \t--- Loss: 1.653\n",
      "Iteration: 333 \t--- Loss: 1.622\n",
      "Iteration: 334 \t--- Loss: 1.556\n",
      "Iteration: 335 \t--- Loss: 1.552\n",
      "Iteration: 336 \t--- Loss: 1.646\n",
      "Iteration: 337 \t--- Loss: 1.615\n",
      "Iteration: 338 \t--- Loss: 1.546\n",
      "Iteration: 339 \t--- Loss: 1.582\n",
      "Iteration: 340 \t--- Loss: 1.635\n",
      "Iteration: 341 \t--- Loss: 1.558\n",
      "Iteration: 342 \t--- Loss: 1.628\n",
      "Iteration: 343 \t--- Loss: 1.612\n",
      "Iteration: 344 \t--- Loss: 1.581\n",
      "Iteration: 345 \t--- Loss: 1.553\n",
      "Iteration: 346 \t--- Loss: 1.536\n",
      "Iteration: 347 \t--- Loss: 1.624\n",
      "Iteration: 348 \t--- Loss: 1.599\n",
      "Iteration: 349 \t--- Loss: 1.557\n",
      "Iteration: 350 \t--- Loss: 1.603\n",
      "Iteration: 351 \t--- Loss: 1.590\n",
      "Iteration: 352 \t--- Loss: 1.679\n",
      "Iteration: 353 \t--- Loss: 1.618\n",
      "Iteration: 354 \t--- Loss: 1.591\n",
      "Iteration: 355 \t--- Loss: 1.518\n",
      "Iteration: 356 \t--- Loss: 1.613\n",
      "Iteration: 357 \t--- Loss: 1.591\n",
      "Iteration: 358 \t--- Loss: 1.574\n",
      "Iteration: 359 \t--- Loss: 1.661\n",
      "Iteration: 360 \t--- Loss: 1.547\n",
      "Iteration: 361 \t--- Loss: 1.618\n",
      "Iteration: 362 \t--- Loss: 1.590\n",
      "Iteration: 363 \t--- Loss: 1.577\n",
      "Iteration: 364 \t--- Loss: 1.595\n",
      "Iteration: 365 \t--- Loss: 1.532\n",
      "Iteration: 366 \t--- Loss: 1.585\n",
      "Iteration: 367 \t--- Loss: 1.609\n",
      "Iteration: 368 \t--- Loss: 1.544\n",
      "Iteration: 369 \t--- Loss: 1.662\n",
      "Iteration: 370 \t--- Loss: 1.612\n",
      "Iteration: 371 \t--- Loss: 1.620\n",
      "Iteration: 372 \t--- Loss: 1.590\n",
      "Iteration: 373 \t--- Loss: 1.513\n",
      "Iteration: 374 \t--- Loss: 1.578\n",
      "Iteration: 375 \t--- Loss: 1.580\n",
      "Iteration: 376 \t--- Loss: 1.534\n",
      "Iteration: 377 \t--- Loss: 1.624\n",
      "Iteration: 378 \t--- Loss: 1.583\n",
      "Iteration: 379 \t--- Loss: 1.577\n",
      "Iteration: 380 \t--- Loss: 1.601\n",
      "Iteration: 381 \t--- Loss: 1.593\n",
      "Iteration: 382 \t--- Loss: 1.563\n",
      "Iteration: 383 \t--- Loss: 1.601\n",
      "Iteration: 384 \t--- Loss: 1.590\n",
      "Iteration: 385 \t--- Loss: 1.588\n",
      "Iteration: 386 \t--- Loss: 1.632\n",
      "Iteration: 387 \t--- Loss: 1.568\n",
      "Iteration: 388 \t--- Loss: 1.556\n",
      "Iteration: 389 \t--- Loss: 1.578\n",
      "Iteration: 390 \t--- Loss: 1.671\n",
      "Iteration: 391 \t--- Loss: 1.642\n",
      "Iteration: 392 \t--- Loss: 1.628\n",
      "Iteration: 393 \t--- Loss: 1.609\n",
      "Iteration: 394 \t--- Loss: 1.652\n",
      "Iteration: 395 \t--- Loss: 1.545\n",
      "Iteration: 396 \t--- Loss: 1.650\n",
      "Iteration: 397 \t--- Loss: 1.629\n",
      "Iteration: 398 \t--- Loss: 1.566\n",
      "Iteration: 399 \t--- Loss: 1.625\n",
      "Iteration: 400 \t--- Loss: 1.620\n",
      "Iteration: 401 \t--- Loss: 1.558\n",
      "Iteration: 402 \t--- Loss: 1.554\n",
      "Iteration: 403 \t--- Loss: 1.552\n",
      "Iteration: 404 \t--- Loss: 1.583\n",
      "Iteration: 405 \t--- Loss: 1.590\n",
      "Iteration: 406 \t--- Loss: 1.544\n",
      "Iteration: 407 \t--- Loss: 1.561\n",
      "Iteration: 408 \t--- Loss: 1.628\n",
      "Iteration: 409 \t--- Loss: 1.562\n",
      "Iteration: 410 \t--- Loss: 1.592\n",
      "Iteration: 411 \t--- Loss: 1.582\n",
      "Iteration: 412 \t--- Loss: 1.650\n",
      "Iteration: 413 \t--- Loss: 1.577\n",
      "Iteration: 414 \t--- Loss: 1.552\n",
      "Iteration: 415 \t--- Loss: 1.624\n",
      "Iteration: 416 \t--- Loss: 1.650\n",
      "Iteration: 417 \t--- Loss: 1.600\n",
      "Iteration: 418 \t--- Loss: 1.512\n",
      "Iteration: 419 \t--- Loss: 1.583\n",
      "Iteration: 420 \t--- Loss: 1.639\n",
      "Iteration: 421 \t--- Loss: 1.641\n",
      "Iteration: 422 \t--- Loss: 1.593\n",
      "Iteration: 423 \t--- Loss: 1.633\n",
      "Iteration: 424 \t--- Loss: 1.622\n",
      "Iteration: 425 \t--- Loss: 1.577\n",
      "Iteration: 426 \t--- Loss: 1.563\n",
      "Iteration: 427 \t--- Loss: 1.589\n",
      "Iteration: 428 \t--- Loss: 1.578\n",
      "Iteration: 429 \t--- Loss: 1.597\n",
      "Iteration: 430 \t--- Loss: 1.614\n",
      "Iteration: 431 \t--- Loss: 1.624\n",
      "Iteration: 432 \t--- Loss: 1.585\n",
      "Iteration: 433 \t--- Loss: 1.662\n",
      "Iteration: 434 \t--- Loss: 1.588\n",
      "Iteration: 435 \t--- Loss: 1.595\n",
      "Iteration: 436 \t--- Loss: 1.662\n",
      "Iteration: 437 \t--- Loss: 1.619\n",
      "Iteration: 438 \t--- Loss: 1.686\n",
      "Iteration: 439 \t--- Loss: 1.554\n",
      "Iteration: 440 \t--- Loss: 1.679\n",
      "Iteration: 441 \t--- Loss: 1.608\n",
      "Iteration: 442 \t--- Loss: 1.593\n",
      "Iteration: 443 \t--- Loss: 1.583\n",
      "Iteration: 444 \t--- Loss: 1.588\n",
      "Iteration: 445 \t--- Loss: 1.550\n",
      "Iteration: 446 \t--- Loss: 1.610\n",
      "Iteration: 447 \t--- Loss: 1.532\n",
      "Iteration: 448 \t--- Loss: 1.608\n",
      "Iteration: 449 \t--- Loss: 1.541\n",
      "Iteration: 450 \t--- Loss: 1.584\n",
      "Iteration: 451 \t--- Loss: 1.664\n",
      "Iteration: 452 \t--- Loss: 1.620\n",
      "Iteration: 453 \t--- Loss: 1.616\n",
      "Iteration: 454 \t--- Loss: 1.644\n",
      "Iteration: 455 \t--- Loss: 1.595\n",
      "Iteration: 456 \t--- Loss: 1.638\n",
      "Iteration: 457 \t--- Loss: 1.637\n",
      "Iteration: 458 \t--- Loss: 1.599\n",
      "Iteration: 459 \t--- Loss: 1.604\n",
      "Iteration: 460 \t--- Loss: 1.620\n",
      "Iteration: 461 \t--- Loss: 1.557\n",
      "Iteration: 462 \t--- Loss: 1.567\n",
      "Iteration: 463 \t--- Loss: 1.620\n",
      "Iteration: 464 \t--- Loss: 1.659\n",
      "Iteration: 465 \t--- Loss: 1.546\n",
      "Iteration: 466 \t--- Loss: 1.547\n",
      "Iteration: 467 \t--- Loss: 1.628\n",
      "Iteration: 468 \t--- Loss: 1.573\n",
      "Iteration: 469 \t--- Loss: 1.588\n",
      "Iteration: 470 \t--- Loss: 1.579\n",
      "Iteration: 471 \t--- Loss: 1.586\n",
      "Iteration: 472 \t--- Loss: 1.598\n",
      "Iteration: 473 \t--- Loss: 1.569\n",
      "Iteration: 474 \t--- Loss: 1.596\n",
      "Iteration: 475 \t--- Loss: 1.610\n",
      "Iteration: 476 \t--- Loss: 1.600\n",
      "Iteration: 477 \t--- Loss: 1.573\n",
      "Iteration: 478 \t--- Loss: 1.611\n",
      "Iteration: 479 \t--- Loss: 1.587\n",
      "Iteration: 480 \t--- Loss: 1.557\n",
      "Iteration: 481 \t--- Loss: 1.578\n",
      "Iteration: 482 \t--- Loss: 1.628\n",
      "Iteration: 483 \t--- Loss: 1.583\n",
      "Iteration: 484 \t--- Loss: 1.626\n",
      "Iteration: 485 \t--- Loss: 1.556\n",
      "Iteration: 486 \t--- Loss: 1.576\n",
      "Iteration: 487 \t--- Loss: 1.603\n",
      "Iteration: 488 \t--- Loss: 1.532\n",
      "Iteration: 489 \t--- Loss: 1.628\n",
      "Iteration: 490 \t--- Loss: 1.547\n",
      "Iteration: 491 \t--- Loss: 1.566\n",
      "Iteration: 492 \t--- Loss: 1.547\n",
      "Iteration: 493 \t--- Loss: 1.603\n",
      "Iteration: 494 \t--- Loss: 1.600\n",
      "Iteration: 495 \t--- Loss: 1.578\n",
      "Iteration: 496 \t--- Loss: 1.636\n",
      "Iteration: 497 \t--- Loss: 1.559\n",
      "Iteration: 498 \t--- Loss: 1.579\n",
      "Iteration: 499 \t--- Loss: 1.626\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:09,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.232\n",
      "Iteration: 1 \t--- Loss: 0.224\n",
      "Iteration: 2 \t--- Loss: 0.220\n",
      "Iteration: 3 \t--- Loss: 0.214\n",
      "Iteration: 4 \t--- Loss: 0.209\n",
      "Iteration: 5 \t--- Loss: 0.200\n",
      "Iteration: 6 \t--- Loss: 0.204\n",
      "Iteration: 7 \t--- Loss: 0.201\n",
      "Iteration: 8 \t--- Loss: 0.194\n",
      "Iteration: 9 \t--- Loss: 0.191\n",
      "Iteration: 10 \t--- Loss: 0.189\n",
      "Iteration: 11 \t--- Loss: 0.185\n",
      "Iteration: 12 \t--- Loss: 0.185\n",
      "Iteration: 13 \t--- Loss: 0.183\n",
      "Iteration: 14 \t--- Loss: 0.188\n",
      "Iteration: 15 \t--- Loss: 0.181\n",
      "Iteration: 16 \t--- Loss: 0.171\n",
      "Iteration: 17 \t--- Loss: 0.171\n",
      "Iteration: 18 \t--- Loss: 0.167\n",
      "Iteration: 19 \t--- Loss: 0.171\n",
      "Iteration: 20 \t--- Loss: 0.169\n",
      "Iteration: 21 \t--- Loss: 0.166\n",
      "Iteration: 22 \t--- Loss: 0.170\n",
      "Iteration: 23 \t--- Loss: 0.163\n",
      "Iteration: 24 \t--- Loss: 0.171\n",
      "Iteration: 25 \t--- Loss: 0.152\n",
      "Iteration: 26 \t--- Loss: 0.158\n",
      "Iteration: 27 \t--- Loss: 0.162\n",
      "Iteration: 28 \t--- Loss: 0.149\n",
      "Iteration: 29 \t--- Loss: 0.155\n",
      "Iteration: 30 \t--- Loss: 0.140\n",
      "Iteration: 31 \t--- Loss: 0.128\n",
      "Iteration: 32 \t--- Loss: 0.116\n",
      "Iteration: 33 \t--- Loss: 0.115\n",
      "Iteration: 34 \t--- Loss: 0.128\n",
      "Iteration: 35 \t--- Loss: 0.109\n",
      "Iteration: 36 \t--- Loss: 0.113\n",
      "Iteration: 37 \t--- Loss: 0.112\n",
      "Iteration: 38 \t--- Loss: 0.112\n",
      "Iteration: 39 \t--- Loss: 0.147\n",
      "Iteration: 40 \t--- Loss: 0.191\n",
      "Iteration: 41 \t--- Loss: 0.189\n",
      "Iteration: 42 \t--- Loss: 0.132\n",
      "Iteration: 43 \t--- Loss: 0.152\n",
      "Iteration: 44 \t--- Loss: 0.191\n",
      "Iteration: 45 \t--- Loss: 0.177\n",
      "Iteration: 46 \t--- Loss: 0.145\n",
      "Iteration: 47 \t--- Loss: 0.104\n",
      "Iteration: 48 \t--- Loss: 0.116\n",
      "Iteration: 49 \t--- Loss: 0.130\n",
      "Iteration: 50 \t--- Loss: 0.114\n",
      "Iteration: 51 \t--- Loss: 0.138\n",
      "Iteration: 52 \t--- Loss: 0.094\n",
      "Iteration: 53 \t--- Loss: 0.103\n",
      "Iteration: 54 \t--- Loss: 0.104\n",
      "Iteration: 55 \t--- Loss: 0.209\n",
      "Iteration: 56 \t--- Loss: 0.253\n",
      "Iteration: 57 \t--- Loss: 0.246\n",
      "Iteration: 58 \t--- Loss: 0.233\n",
      "Iteration: 59 \t--- Loss: 0.229\n",
      "Iteration: 60 \t--- Loss: 0.219\n",
      "Iteration: 61 \t--- Loss: 0.212\n",
      "Iteration: 62 \t--- Loss: 0.204\n",
      "Iteration: 63 \t--- Loss: 0.204\n",
      "Iteration: 64 \t--- Loss: 0.193\n",
      "Iteration: 65 \t--- Loss: 0.192\n",
      "Iteration: 66 \t--- Loss: 0.179\n",
      "Iteration: 67 \t--- Loss: 0.179\n",
      "Iteration: 68 \t--- Loss: 0.172\n",
      "Iteration: 69 \t--- Loss: 0.175\n",
      "Iteration: 70 \t--- Loss: 0.173\n",
      "Iteration: 71 \t--- Loss: 0.161\n",
      "Iteration: 72 \t--- Loss: 0.165\n",
      "Iteration: 73 \t--- Loss: 0.150\n",
      "Iteration: 74 \t--- Loss: 0.157\n",
      "Iteration: 75 \t--- Loss: 0.151\n",
      "Iteration: 76 \t--- Loss: 0.152\n",
      "Iteration: 77 \t--- Loss: 0.150\n",
      "Iteration: 78 \t--- Loss: 0.152\n",
      "Iteration: 79 \t--- Loss: 0.145\n",
      "Iteration: 80 \t--- Loss: 0.136\n",
      "Iteration: 81 \t--- Loss: 0.138\n",
      "Iteration: 82 \t--- Loss: 0.138\n",
      "Iteration: 83 \t--- Loss: 0.139\n",
      "Iteration: 84 \t--- Loss: 0.130\n",
      "Iteration: 85 \t--- Loss: 0.128\n",
      "Iteration: 86 \t--- Loss: 0.131\n",
      "Iteration: 87 \t--- Loss: 0.118\n",
      "Iteration: 88 \t--- Loss: 0.123\n",
      "Iteration: 89 \t--- Loss: 0.120\n",
      "Iteration: 90 \t--- Loss: 0.116\n",
      "Iteration: 91 \t--- Loss: 0.117\n",
      "Iteration: 92 \t--- Loss: 0.112\n",
      "Iteration: 93 \t--- Loss: 0.111\n",
      "Iteration: 94 \t--- Loss: 0.116\n",
      "Iteration: 95 \t--- Loss: 0.106\n",
      "Iteration: 96 \t--- Loss: 0.100\n",
      "Iteration: 97 \t--- Loss: 0.097\n",
      "Iteration: 98 \t--- Loss: 0.088\n",
      "Iteration: 99 \t--- Loss: 0.089\n",
      "Iteration: 100 \t--- Loss: 0.081\n",
      "Iteration: 101 \t--- Loss: 0.080\n",
      "Iteration: 102 \t--- Loss: 0.073\n",
      "Iteration: 103 \t--- Loss: 0.064\n",
      "Iteration: 104 \t--- Loss: 0.056\n",
      "Iteration: 105 \t--- Loss: 0.051\n",
      "Iteration: 106 \t--- Loss: 0.046\n",
      "Iteration: 107 \t--- Loss: 0.049\n",
      "Iteration: 108 \t--- Loss: 0.052\n",
      "Iteration: 109 \t--- Loss: 0.047\n",
      "Iteration: 110 \t--- Loss: 0.051\n",
      "Iteration: 111 \t--- Loss: 0.044\n",
      "Iteration: 112 \t--- Loss: 0.047\n",
      "Iteration: 113 \t--- Loss: 0.048\n",
      "Iteration: 114 \t--- Loss: 0.047\n",
      "Iteration: 115 \t--- Loss: 0.046\n",
      "Iteration: 116 \t--- Loss: 0.044\n",
      "Iteration: 117 \t--- Loss: 0.045\n",
      "Iteration: 118 \t--- Loss: 0.044\n",
      "Iteration: 119 \t--- Loss: 0.044\n",
      "Iteration: 120 \t--- Loss: 0.045\n",
      "Iteration: 121 \t--- Loss: 0.043\n",
      "Iteration: 122 \t--- Loss: 0.043\n",
      "Iteration: 123 \t--- Loss: 0.042\n",
      "Iteration: 124 \t--- Loss: 0.041\n",
      "Iteration: 125 \t--- Loss: 0.043\n",
      "Iteration: 126 \t--- Loss: 0.042\n",
      "Iteration: 127 \t--- Loss: 0.040\n",
      "Iteration: 128 \t--- Loss: 0.039\n",
      "Iteration: 129 \t--- Loss: 0.039\n",
      "Iteration: 130 \t--- Loss: 0.041\n",
      "Iteration: 131 \t--- Loss: 0.040\n",
      "Iteration: 132 \t--- Loss: 0.037\n",
      "Iteration: 133 \t--- Loss: 0.039\n",
      "Iteration: 134 \t--- Loss: 0.035\n",
      "Iteration: 135 \t--- Loss: 0.037\n",
      "Iteration: 136 \t--- Loss: 0.037\n",
      "Iteration: 137 \t--- Loss: 0.037\n",
      "Iteration: 138 \t--- Loss: 0.039\n",
      "Iteration: 139 \t--- Loss: 0.039\n",
      "Iteration: 140 \t--- Loss: 0.036\n",
      "Iteration: 141 \t--- Loss: 0.040\n",
      "Iteration: 142 \t--- Loss: 0.038\n",
      "Iteration: 143 \t--- Loss: 0.038\n",
      "Iteration: 144 \t--- Loss: 0.036\n",
      "Iteration: 145 \t--- Loss: 0.036\n",
      "Iteration: 146 \t--- Loss: 0.037\n",
      "Iteration: 147 \t--- Loss: 0.036\n",
      "Iteration: 148 \t--- Loss: 0.037\n",
      "Iteration: 149 \t--- Loss: 0.036\n",
      "Iteration: 150 \t--- Loss: 0.037\n",
      "Iteration: 151 \t--- Loss: 0.035\n",
      "Iteration: 152 \t--- Loss: 0.034\n",
      "Iteration: 153 \t--- Loss: 0.033\n",
      "Iteration: 154 \t--- Loss: 0.034\n",
      "Iteration: 155 \t--- Loss: 0.035\n",
      "Iteration: 156 \t--- Loss: 0.035\n",
      "Iteration: 157 \t--- Loss: 0.038\n",
      "Iteration: 158 \t--- Loss: 0.036\n",
      "Iteration: 159 \t--- Loss: 0.034\n",
      "Iteration: 160 \t--- Loss: 0.033\n",
      "Iteration: 161 \t--- Loss: 0.035\n",
      "Iteration: 162 \t--- Loss: 0.035\n",
      "Iteration: 163 \t--- Loss: 0.034\n",
      "Iteration: 164 \t--- Loss: 0.034\n",
      "Iteration: 165 \t--- Loss: 0.037\n",
      "Iteration: 166 \t--- Loss: 0.036\n",
      "Iteration: 167 \t--- Loss: 0.035\n",
      "Iteration: 168 \t--- Loss: 0.038\n",
      "Iteration: 169 \t--- Loss: 0.036\n",
      "Iteration: 170 \t--- Loss: 0.035\n",
      "Iteration: 171 \t--- Loss: 0.036\n",
      "Iteration: 172 \t--- Loss: 0.034\n",
      "Iteration: 173 \t--- Loss: 0.035\n",
      "Iteration: 174 \t--- Loss: 0.034\n",
      "Iteration: 175 \t--- Loss: 0.032\n",
      "Iteration: 176 \t--- Loss: 0.035\n",
      "Iteration: 177 \t--- Loss: 0.034\n",
      "Iteration: 178 \t--- Loss: 0.036\n",
      "Iteration: 179 \t--- Loss: 0.032\n",
      "Iteration: 180 \t--- Loss: 0.034\n",
      "Iteration: 181 \t--- Loss: 0.037\n",
      "Iteration: 182 \t--- Loss: 0.035\n",
      "Iteration: 183 \t--- Loss: 0.034\n",
      "Iteration: 184 \t--- Loss: 0.034\n",
      "Iteration: 185 \t--- Loss: 0.038\n",
      "Iteration: 186 \t--- Loss: 0.041\n",
      "Iteration: 187 \t--- Loss: 0.057\n",
      "Iteration: 188 \t--- Loss: 0.070\n",
      "Iteration: 189 \t--- Loss: 0.036\n",
      "Iteration: 190 \t--- Loss: 0.032\n",
      "Iteration: 191 \t--- Loss: 0.037\n",
      "Iteration: 192 \t--- Loss: 0.038\n",
      "Iteration: 193 \t--- Loss: 0.060\n",
      "Iteration: 194 \t--- Loss: 0.095\n",
      "Iteration: 195 \t--- Loss: 0.035\n",
      "Iteration: 196 \t--- Loss: 0.039\n",
      "Iteration: 197 \t--- Loss: 0.036\n",
      "Iteration: 198 \t--- Loss: 0.045\n",
      "Iteration: 199 \t--- Loss: 0.055\n",
      "Iteration: 200 \t--- Loss: 0.039\n",
      "Iteration: 201 \t--- Loss: 0.047\n",
      "Iteration: 202 \t--- Loss: 0.036\n",
      "Iteration: 203 \t--- Loss: 0.039\n",
      "Iteration: 204 \t--- Loss: 0.030\n",
      "Iteration: 205 \t--- Loss: 0.028\n",
      "Iteration: 206 \t--- Loss: 0.034\n",
      "Iteration: 207 \t--- Loss: 0.034\n",
      "Iteration: 208 \t--- Loss: 0.033\n",
      "Iteration: 209 \t--- Loss: 0.029\n",
      "Iteration: 210 \t--- Loss: 0.033\n",
      "Iteration: 211 \t--- Loss: 0.038\n",
      "Iteration: 212 \t--- Loss: 0.032\n",
      "Iteration: 213 \t--- Loss: 0.029\n",
      "Iteration: 214 \t--- Loss: 0.031\n",
      "Iteration: 215 \t--- Loss: 0.031\n",
      "Iteration: 216 \t--- Loss: 0.030\n",
      "Iteration: 217 \t--- Loss: 0.028\n",
      "Iteration: 218 \t--- Loss: 0.030\n",
      "Iteration: 219 \t--- Loss: 0.028\n",
      "Iteration: 220 \t--- Loss: 0.028\n",
      "Iteration: 221 \t--- Loss: 0.032\n",
      "Iteration: 222 \t--- Loss: 0.026\n",
      "Iteration: 223 \t--- Loss: 0.028\n",
      "Iteration: 224 \t--- Loss: 0.029\n",
      "Iteration: 225 \t--- Loss: 0.034\n",
      "Iteration: 226 \t--- Loss: 0.027\n",
      "Iteration: 227 \t--- Loss: 0.026\n",
      "Iteration: 228 \t--- Loss: 0.028\n",
      "Iteration: 229 \t--- Loss: 0.026\n",
      "Iteration: 230 \t--- Loss: 0.024\n",
      "Iteration: 231 \t--- Loss: 0.025\n",
      "Iteration: 232 \t--- Loss: 0.025\n",
      "Iteration: 233 \t--- Loss: 0.024\n",
      "Iteration: 234 \t--- Loss: 0.025\n",
      "Iteration: 235 \t--- Loss: 0.025\n",
      "Iteration: 236 \t--- Loss: 0.023\n",
      "Iteration: 237 \t--- Loss: 0.020\n",
      "Iteration: 238 \t--- Loss: 0.024\n",
      "Iteration: 239 \t--- Loss: 0.021\n",
      "Iteration: 240 \t--- Loss: 0.022\n",
      "Iteration: 241 \t--- Loss: 0.021\n",
      "Iteration: 242 \t--- Loss: 0.023\n",
      "Iteration: 243 \t--- Loss: 0.021\n",
      "Iteration: 244 \t--- Loss: 0.023\n",
      "Iteration: 245 \t--- Loss: 0.022\n",
      "Iteration: 246 \t--- Loss: 0.022\n",
      "Iteration: 247 \t--- Loss: 0.018\n",
      "Iteration: 248 \t--- Loss: 0.021\n",
      "Iteration: 249 \t--- Loss: 0.017\n",
      "Iteration: 250 \t--- Loss: 0.019\n",
      "Iteration: 251 \t--- Loss: 0.018\n",
      "Iteration: 252 \t--- Loss: 0.021\n",
      "Iteration: 253 \t--- Loss: 0.018\n",
      "Iteration: 254 \t--- Loss: 0.020\n",
      "Iteration: 255 \t--- Loss: 0.019\n",
      "Iteration: 256 \t--- Loss: 0.021\n",
      "Iteration: 257 \t--- Loss: 0.019\n",
      "Iteration: 258 \t--- Loss: 0.021\n",
      "Iteration: 259 \t--- Loss: 0.018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:26<00:00, 86.49s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.222\n",
      "Iteration: 261 \t--- Loss: 0.220\n",
      "Iteration: 262 \t--- Loss: 0.219\n",
      "Iteration: 263 \t--- Loss: 0.221\n",
      "Iteration: 264 \t--- Loss: 0.231\n",
      "Iteration: 265 \t--- Loss: 0.232\n",
      "Iteration: 266 \t--- Loss: 0.226\n",
      "Iteration: 267 \t--- Loss: 0.235\n",
      "Iteration: 268 \t--- Loss: 0.238\n",
      "Iteration: 269 \t--- Loss: 0.234\n",
      "Iteration: 270 \t--- Loss: 0.223\n",
      "Iteration: 271 \t--- Loss: 0.239\n",
      "Iteration: 272 \t--- Loss: 0.230\n",
      "Iteration: 273 \t--- Loss: 0.226\n",
      "Iteration: 274 \t--- Loss: 0.224\n",
      "Iteration: 275 \t--- Loss: 0.218\n",
      "Iteration: 276 \t--- Loss: 0.234\n",
      "Iteration: 277 \t--- Loss: 0.235\n",
      "Iteration: 278 \t--- Loss: 0.232\n",
      "Iteration: 279 \t--- Loss: 0.236\n",
      "Iteration: 280 \t--- Loss: 0.219\n",
      "Iteration: 281 \t--- Loss: 0.233\n",
      "Iteration: 282 \t--- Loss: 0.232\n",
      "Iteration: 283 \t--- Loss: 0.228\n",
      "Iteration: 284 \t--- Loss: 0.248\n",
      "Iteration: 285 \t--- Loss: 0.240\n",
      "Iteration: 286 \t--- Loss: 0.225\n",
      "Iteration: 287 \t--- Loss: 0.227\n",
      "Iteration: 288 \t--- Loss: 0.232\n",
      "Iteration: 289 \t--- Loss: 0.227\n",
      "Iteration: 290 \t--- Loss: 0.231\n",
      "Iteration: 291 \t--- Loss: 0.229\n",
      "Iteration: 292 \t--- Loss: 0.246\n",
      "Iteration: 293 \t--- Loss: 0.233\n",
      "Iteration: 294 \t--- Loss: 0.223\n",
      "Iteration: 295 \t--- Loss: 0.214\n",
      "Iteration: 296 \t--- Loss: 0.231\n",
      "Iteration: 297 \t--- Loss: 0.241\n",
      "Iteration: 298 \t--- Loss: 0.214\n",
      "Iteration: 299 \t--- Loss: 0.231\n",
      "Iteration: 300 \t--- Loss: 0.220\n",
      "Iteration: 301 \t--- Loss: 0.230\n",
      "Iteration: 302 \t--- Loss: 0.229\n",
      "Iteration: 303 \t--- Loss: 0.232\n",
      "Iteration: 304 \t--- Loss: 0.239\n",
      "Iteration: 305 \t--- Loss: 0.237\n",
      "Iteration: 306 \t--- Loss: 0.238\n",
      "Iteration: 307 \t--- Loss: 0.231\n",
      "Iteration: 308 \t--- Loss: 0.235\n",
      "Iteration: 309 \t--- Loss: 0.235\n",
      "Iteration: 310 \t--- Loss: 0.229\n",
      "Iteration: 311 \t--- Loss: 0.217\n",
      "Iteration: 312 \t--- Loss: 0.229\n",
      "Iteration: 313 \t--- Loss: 0.233\n",
      "Iteration: 314 \t--- Loss: 0.225\n",
      "Iteration: 315 \t--- Loss: 0.234\n",
      "Iteration: 316 \t--- Loss: 0.230\n",
      "Iteration: 317 \t--- Loss: 0.228\n",
      "Iteration: 318 \t--- Loss: 0.249\n",
      "Iteration: 319 \t--- Loss: 0.217\n",
      "Iteration: 320 \t--- Loss: 0.224\n",
      "Iteration: 321 \t--- Loss: 0.242\n",
      "Iteration: 322 \t--- Loss: 0.228\n",
      "Iteration: 323 \t--- Loss: 0.234\n",
      "Iteration: 324 \t--- Loss: 0.236\n",
      "Iteration: 325 \t--- Loss: 0.233\n",
      "Iteration: 326 \t--- Loss: 0.232\n",
      "Iteration: 327 \t--- Loss: 0.224\n",
      "Iteration: 328 \t--- Loss: 0.237\n",
      "Iteration: 329 \t--- Loss: 0.227\n",
      "Iteration: 330 \t--- Loss: 0.224\n",
      "Iteration: 331 \t--- Loss: 0.232\n",
      "Iteration: 332 \t--- Loss: 0.232\n",
      "Iteration: 333 \t--- Loss: 0.239\n",
      "Iteration: 334 \t--- Loss: 0.223\n",
      "Iteration: 335 \t--- Loss: 0.239\n",
      "Iteration: 336 \t--- Loss: 0.224\n",
      "Iteration: 337 \t--- Loss: 0.232\n",
      "Iteration: 338 \t--- Loss: 0.211\n",
      "Iteration: 339 \t--- Loss: 0.223\n",
      "Iteration: 340 \t--- Loss: 0.232\n",
      "Iteration: 341 \t--- Loss: 0.238\n",
      "Iteration: 342 \t--- Loss: 0.226\n",
      "Iteration: 343 \t--- Loss: 0.232\n",
      "Iteration: 344 \t--- Loss: 0.230\n",
      "Iteration: 345 \t--- Loss: 0.227\n",
      "Iteration: 346 \t--- Loss: 0.235\n",
      "Iteration: 347 \t--- Loss: 0.225\n",
      "Iteration: 348 \t--- Loss: 0.238\n",
      "Iteration: 349 \t--- Loss: 0.225\n",
      "Iteration: 350 \t--- Loss: 0.223\n",
      "Iteration: 351 \t--- Loss: 0.229\n",
      "Iteration: 352 \t--- Loss: 0.233\n",
      "Iteration: 353 \t--- Loss: 0.229\n",
      "Iteration: 354 \t--- Loss: 0.236\n",
      "Iteration: 355 \t--- Loss: 0.237\n",
      "Iteration: 356 \t--- Loss: 0.230\n",
      "Iteration: 357 \t--- Loss: 0.216\n",
      "Iteration: 358 \t--- Loss: 0.225\n",
      "Iteration: 359 \t--- Loss: 0.241\n",
      "Iteration: 360 \t--- Loss: 0.234\n",
      "Iteration: 361 \t--- Loss: 0.238\n",
      "Iteration: 362 \t--- Loss: 0.222\n",
      "Iteration: 363 \t--- Loss: 0.226\n",
      "Iteration: 364 \t--- Loss: 0.223\n",
      "Iteration: 365 \t--- Loss: 0.238\n",
      "Iteration: 366 \t--- Loss: 0.248\n",
      "Iteration: 367 \t--- Loss: 0.242\n",
      "Iteration: 368 \t--- Loss: 0.223\n",
      "Iteration: 369 \t--- Loss: 0.259\n",
      "Iteration: 370 \t--- Loss: 0.228\n",
      "Iteration: 371 \t--- Loss: 0.234\n",
      "Iteration: 372 \t--- Loss: 0.230\n",
      "Iteration: 373 \t--- Loss: 0.235\n",
      "Iteration: 374 \t--- Loss: 0.242\n",
      "Iteration: 375 \t--- Loss: 0.231\n",
      "Iteration: 376 \t--- Loss: 0.230\n",
      "Iteration: 377 \t--- Loss: 0.236\n",
      "Iteration: 378 \t--- Loss: 0.217\n",
      "Iteration: 379 \t--- Loss: 0.236\n",
      "Iteration: 380 \t--- Loss: 0.229\n",
      "Iteration: 381 \t--- Loss: 0.237\n",
      "Iteration: 382 \t--- Loss: 0.234\n",
      "Iteration: 383 \t--- Loss: 0.224\n",
      "Iteration: 384 \t--- Loss: 0.235\n",
      "Iteration: 385 \t--- Loss: 0.221\n",
      "Iteration: 386 \t--- Loss: 0.240\n",
      "Iteration: 387 \t--- Loss: 0.233\n",
      "Iteration: 388 \t--- Loss: 0.222\n",
      "Iteration: 389 \t--- Loss: 0.225\n",
      "Iteration: 390 \t--- Loss: 0.246\n",
      "Iteration: 391 \t--- Loss: 0.223\n",
      "Iteration: 392 \t--- Loss: 0.241\n",
      "Iteration: 393 \t--- Loss: 0.238\n",
      "Iteration: 394 \t--- Loss: 0.222\n",
      "Iteration: 395 \t--- Loss: 0.244\n",
      "Iteration: 396 \t--- Loss: 0.230\n",
      "Iteration: 397 \t--- Loss: 0.223\n",
      "Iteration: 398 \t--- Loss: 0.230\n",
      "Iteration: 399 \t--- Loss: 0.221\n",
      "Iteration: 400 \t--- Loss: 0.229\n",
      "Iteration: 401 \t--- Loss: 0.247\n",
      "Iteration: 402 \t--- Loss: 0.233\n",
      "Iteration: 403 \t--- Loss: 0.238\n",
      "Iteration: 404 \t--- Loss: 0.232\n",
      "Iteration: 405 \t--- Loss: 0.223\n",
      "Iteration: 406 \t--- Loss: 0.228\n",
      "Iteration: 407 \t--- Loss: 0.223\n",
      "Iteration: 408 \t--- Loss: 0.235\n",
      "Iteration: 409 \t--- Loss: 0.230\n",
      "Iteration: 410 \t--- Loss: 0.224\n",
      "Iteration: 411 \t--- Loss: 0.242\n",
      "Iteration: 412 \t--- Loss: 0.217\n",
      "Iteration: 413 \t--- Loss: 0.237\n",
      "Iteration: 414 \t--- Loss: 0.243\n",
      "Iteration: 415 \t--- Loss: 0.230\n",
      "Iteration: 416 \t--- Loss: 0.239\n",
      "Iteration: 417 \t--- Loss: 0.233\n",
      "Iteration: 418 \t--- Loss: 0.230\n",
      "Iteration: 419 \t--- Loss: 0.241\n",
      "Iteration: 420 \t--- Loss: 0.229\n",
      "Iteration: 421 \t--- Loss: 0.228\n",
      "Iteration: 422 \t--- Loss: 0.231\n",
      "Iteration: 423 \t--- Loss: 0.227\n",
      "Iteration: 424 \t--- Loss: 0.234\n",
      "Iteration: 425 \t--- Loss: 0.230\n",
      "Iteration: 426 \t--- Loss: 0.231\n",
      "Iteration: 427 \t--- Loss: 0.236\n",
      "Iteration: 428 \t--- Loss: 0.230\n",
      "Iteration: 429 \t--- Loss: 0.237\n",
      "Iteration: 430 \t--- Loss: 0.236\n",
      "Iteration: 431 \t--- Loss: 0.233\n",
      "Iteration: 432 \t--- Loss: 0.232\n",
      "Iteration: 433 \t--- Loss: 0.229\n",
      "Iteration: 434 \t--- Loss: 0.249\n",
      "Iteration: 435 \t--- Loss: 0.227\n",
      "Iteration: 436 \t--- Loss: 0.243\n",
      "Iteration: 437 \t--- Loss: 0.225\n",
      "Iteration: 438 \t--- Loss: 0.223\n",
      "Iteration: 439 \t--- Loss: 0.237\n",
      "Iteration: 440 \t--- Loss: 0.243\n",
      "Iteration: 441 \t--- Loss: 0.225\n",
      "Iteration: 442 \t--- Loss: 0.220\n",
      "Iteration: 443 \t--- Loss: 0.237\n",
      "Iteration: 444 \t--- Loss: 0.227\n",
      "Iteration: 445 \t--- Loss: 0.232\n",
      "Iteration: 446 \t--- Loss: 0.223\n",
      "Iteration: 447 \t--- Loss: 0.238\n",
      "Iteration: 448 \t--- Loss: 0.231\n",
      "Iteration: 449 \t--- Loss: 0.220\n",
      "Iteration: 450 \t--- Loss: 0.242\n",
      "Iteration: 451 \t--- Loss: 0.240\n",
      "Iteration: 452 \t--- Loss: 0.223\n",
      "Iteration: 453 \t--- Loss: 0.229\n",
      "Iteration: 454 \t--- Loss: 0.236\n",
      "Iteration: 455 \t--- Loss: 0.219\n",
      "Iteration: 456 \t--- Loss: 0.229\n",
      "Iteration: 457 \t--- Loss: 0.227\n",
      "Iteration: 458 \t--- Loss: 0.231\n",
      "Iteration: 459 \t--- Loss: 0.221\n",
      "Iteration: 460 \t--- Loss: 0.227\n",
      "Iteration: 461 \t--- Loss: 0.239\n",
      "Iteration: 462 \t--- Loss: 0.228\n",
      "Iteration: 463 \t--- Loss: 0.234\n",
      "Iteration: 464 \t--- Loss: 0.238\n",
      "Iteration: 465 \t--- Loss: 0.218\n",
      "Iteration: 466 \t--- Loss: 0.229\n",
      "Iteration: 467 \t--- Loss: 0.233\n",
      "Iteration: 468 \t--- Loss: 0.218\n",
      "Iteration: 469 \t--- Loss: 0.230\n",
      "Iteration: 470 \t--- Loss: 0.241\n",
      "Iteration: 471 \t--- Loss: 0.228\n",
      "Iteration: 472 \t--- Loss: 0.235\n",
      "Iteration: 473 \t--- Loss: 0.235\n",
      "Iteration: 474 \t--- Loss: 0.232\n",
      "Iteration: 475 \t--- Loss: 0.222\n",
      "Iteration: 476 \t--- Loss: 0.227\n",
      "Iteration: 477 \t--- Loss: 0.238\n",
      "Iteration: 478 \t--- Loss: 0.230\n",
      "Iteration: 479 \t--- Loss: 0.240\n",
      "Iteration: 480 \t--- Loss: 0.233\n",
      "Iteration: 481 \t--- Loss: 0.234\n",
      "Iteration: 482 \t--- Loss: 0.238\n",
      "Iteration: 483 \t--- Loss: 0.227\n",
      "Iteration: 484 \t--- Loss: 0.238\n",
      "Iteration: 485 \t--- Loss: 0.214\n",
      "Iteration: 486 \t--- Loss: 0.227\n",
      "Iteration: 487 \t--- Loss: 0.225\n",
      "Iteration: 488 \t--- Loss: 0.242\n",
      "Iteration: 489 \t--- Loss: 0.238\n",
      "Iteration: 490 \t--- Loss: 0.230\n",
      "Iteration: 491 \t--- Loss: 0.238\n",
      "Iteration: 492 \t--- Loss: 0.235\n",
      "Iteration: 493 \t--- Loss: 0.238\n",
      "Iteration: 494 \t--- Loss: 0.225\n",
      "Iteration: 495 \t--- Loss: 0.228\n",
      "Iteration: 496 \t--- Loss: 0.230\n",
      "Iteration: 497 \t--- Loss: 0.238\n",
      "Iteration: 498 \t--- Loss: 0.232\n",
      "Iteration: 499 \t--- Loss: 0.245\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it][Parallel(n_jobs=5)]: Done  44 tasks      | elapsed: 27.2min\n",
      "100%|██████████| 1/1 [01:32<00:00, 92.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.006\n",
      "Iteration: 261 \t--- Loss: 0.006\n",
      "Iteration: 262 \t--- Loss: 0.008\n",
      "Iteration: 263 \t--- Loss: 0.008\n",
      "Iteration: 264 \t--- Loss: 0.009\n",
      "Iteration: 265 \t--- Loss: 0.009\n",
      "Iteration: 266 \t--- Loss: 0.006\n",
      "Iteration: 267 \t--- Loss: 0.006\n",
      "Iteration: 268 \t--- Loss: 0.005\n",
      "Iteration: 269 \t--- Loss: 0.005\n",
      "Iteration: 270 \t--- Loss: 0.005\n",
      "Iteration: 271 \t--- Loss: 0.005\n",
      "Iteration: 272 \t--- Loss: 0.005\n",
      "Iteration: 273 \t--- Loss: 0.005\n",
      "Iteration: 274 \t--- Loss: 0.006\n",
      "Iteration: 275 \t--- Loss: 0.005\n",
      "Iteration: 276 \t--- Loss: 0.005\n",
      "Iteration: 277 \t--- Loss: 0.005\n",
      "Iteration: 278 \t--- Loss: 0.005\n",
      "Iteration: 279 \t--- Loss: 0.005\n",
      "Iteration: 280 \t--- Loss: 0.005\n",
      "Iteration: 281 \t--- Loss: 0.005\n",
      "Iteration: 282 \t--- Loss: 0.005\n",
      "Iteration: 283 \t--- Loss: 0.005\n",
      "Iteration: 284 \t--- Loss: 0.005\n",
      "Iteration: 285 \t--- Loss: 0.004\n",
      "Iteration: 286 \t--- Loss: 0.005\n",
      "Iteration: 287 \t--- Loss: 0.005\n",
      "Iteration: 288 \t--- Loss: 0.005\n",
      "Iteration: 289 \t--- Loss: 0.005\n",
      "Iteration: 290 \t--- Loss: 0.004\n",
      "Iteration: 291 \t--- Loss: 0.005\n",
      "Iteration: 292 \t--- Loss: 0.004\n",
      "Iteration: 293 \t--- Loss: 0.004\n",
      "Iteration: 294 \t--- Loss: 0.005\n",
      "Iteration: 295 \t--- Loss: 0.005\n",
      "Iteration: 296 \t--- Loss: 0.005\n",
      "Iteration: 297 \t--- Loss: 0.005\n",
      "Iteration: 298 \t--- Loss: 0.005\n",
      "Iteration: 299 \t--- Loss: 0.005\n",
      "Iteration: 300 \t--- Loss: 0.005\n",
      "Iteration: 301 \t--- Loss: 0.005\n",
      "Iteration: 302 \t--- Loss: 0.005\n",
      "Iteration: 303 \t--- Loss: 0.005\n",
      "Iteration: 304 \t--- Loss: 0.004\n",
      "Iteration: 305 \t--- Loss: 0.005\n",
      "Iteration: 306 \t--- Loss: 0.004\n",
      "Iteration: 307 \t--- Loss: 0.005\n",
      "Iteration: 308 \t--- Loss: 0.005\n",
      "Iteration: 309 \t--- Loss: 0.004\n",
      "Iteration: 310 \t--- Loss: 0.005\n",
      "Iteration: 311 \t--- Loss: 0.004\n",
      "Iteration: 312 \t--- Loss: 0.004\n",
      "Iteration: 313 \t--- Loss: 0.005\n",
      "Iteration: 314 \t--- Loss: 0.005\n",
      "Iteration: 315 \t--- Loss: 0.005\n",
      "Iteration: 316 \t--- Loss: 0.004\n",
      "Iteration: 317 \t--- Loss: 0.005\n",
      "Iteration: 318 \t--- Loss: 0.004\n",
      "Iteration: 319 \t--- Loss: 0.005\n",
      "Iteration: 320 \t--- Loss: 0.004\n",
      "Iteration: 321 \t--- Loss: 0.004\n",
      "Iteration: 322 \t--- Loss: 0.004\n",
      "Iteration: 323 \t--- Loss: 0.004\n",
      "Iteration: 324 \t--- Loss: 0.004\n",
      "Iteration: 325 \t--- Loss: 0.004\n",
      "Iteration: 326 \t--- Loss: 0.004\n",
      "Iteration: 327 \t--- Loss: 0.004\n",
      "Iteration: 328 \t--- Loss: 0.004\n",
      "Iteration: 329 \t--- Loss: 0.005\n",
      "Iteration: 330 \t--- Loss: 0.005\n",
      "Iteration: 331 \t--- Loss: 0.005\n",
      "Iteration: 332 \t--- Loss: 0.004\n",
      "Iteration: 333 \t--- Loss: 0.004\n",
      "Iteration: 334 \t--- Loss: 0.005\n",
      "Iteration: 335 \t--- Loss: 0.004\n",
      "Iteration: 336 \t--- Loss: 0.004\n",
      "Iteration: 337 \t--- Loss: 0.004\n",
      "Iteration: 338 \t--- Loss: 0.004\n",
      "Iteration: 339 \t--- Loss: 0.004\n",
      "Iteration: 340 \t--- Loss: 0.004\n",
      "Iteration: 341 \t--- Loss: 0.004\n",
      "Iteration: 342 \t--- Loss: 0.005\n",
      "Iteration: 343 \t--- Loss: 0.004\n",
      "Iteration: 344 \t--- Loss: 0.004\n",
      "Iteration: 345 \t--- Loss: 0.004\n",
      "Iteration: 346 \t--- Loss: 0.004\n",
      "Iteration: 347 \t--- Loss: 0.004\n",
      "Iteration: 348 \t--- Loss: 0.004\n",
      "Iteration: 349 \t--- Loss: 0.004\n",
      "Iteration: 350 \t--- Loss: 0.004\n",
      "Iteration: 351 \t--- Loss: 0.004\n",
      "Iteration: 352 \t--- Loss: 0.004\n",
      "Iteration: 353 \t--- Loss: 0.004\n",
      "Iteration: 354 \t--- Loss: 0.004\n",
      "Iteration: 355 \t--- Loss: 0.004\n",
      "Iteration: 356 \t--- Loss: 0.004\n",
      "Iteration: 357 \t--- Loss: 0.004\n",
      "Iteration: 358 \t--- Loss: 0.004\n",
      "Iteration: 359 \t--- Loss: 0.004\n",
      "Iteration: 360 \t--- Loss: 0.004\n",
      "Iteration: 361 \t--- Loss: 0.004\n",
      "Iteration: 362 \t--- Loss: 0.004\n",
      "Iteration: 363 \t--- Loss: 0.004\n",
      "Iteration: 364 \t--- Loss: 0.004\n",
      "Iteration: 365 \t--- Loss: 0.004\n",
      "Iteration: 366 \t--- Loss: 0.004\n",
      "Iteration: 367 \t--- Loss: 0.004\n",
      "Iteration: 368 \t--- Loss: 0.004\n",
      "Iteration: 369 \t--- Loss: 0.004\n",
      "Iteration: 370 \t--- Loss: 0.004\n",
      "Iteration: 371 \t--- Loss: 0.004\n",
      "Iteration: 372 \t--- Loss: 0.004\n",
      "Iteration: 373 \t--- Loss: 0.004\n",
      "Iteration: 374 \t--- Loss: 0.004\n",
      "Iteration: 375 \t--- Loss: 0.004\n",
      "Iteration: 376 \t--- Loss: 0.004\n",
      "Iteration: 377 \t--- Loss: 0.004\n",
      "Iteration: 378 \t--- Loss: 0.004\n",
      "Iteration: 379 \t--- Loss: 0.004\n",
      "Iteration: 380 \t--- Loss: 0.004\n",
      "Iteration: 381 \t--- Loss: 0.004\n",
      "Iteration: 382 \t--- Loss: 0.004\n",
      "Iteration: 383 \t--- Loss: 0.004\n",
      "Iteration: 384 \t--- Loss: 0.004\n",
      "Iteration: 385 \t--- Loss: 0.004\n",
      "Iteration: 386 \t--- Loss: 0.004\n",
      "Iteration: 387 \t--- Loss: 0.004\n",
      "Iteration: 388 \t--- Loss: 0.004\n",
      "Iteration: 389 \t--- Loss: 0.004\n",
      "Iteration: 390 \t--- Loss: 0.004\n",
      "Iteration: 391 \t--- Loss: 0.004\n",
      "Iteration: 392 \t--- Loss: 0.004\n",
      "Iteration: 393 \t--- Loss: 0.004\n",
      "Iteration: 394 \t--- Loss: 0.004\n",
      "Iteration: 395 \t--- Loss: 0.004\n",
      "Iteration: 396 \t--- Loss: 0.004\n",
      "Iteration: 397 \t--- Loss: 0.004\n",
      "Iteration: 398 \t--- Loss: 0.004\n",
      "Iteration: 399 \t--- Loss: 0.004\n",
      "Iteration: 400 \t--- Loss: 0.004\n",
      "Iteration: 401 \t--- Loss: 0.004\n",
      "Iteration: 402 \t--- Loss: 0.004\n",
      "Iteration: 403 \t--- Loss: 0.004\n",
      "Iteration: 404 \t--- Loss: 0.004\n",
      "Iteration: 405 \t--- Loss: 0.004\n",
      "Iteration: 406 \t--- Loss: 0.004\n",
      "Iteration: 407 \t--- Loss: 0.004\n",
      "Iteration: 408 \t--- Loss: 0.004\n",
      "Iteration: 409 \t--- Loss: 0.004\n",
      "Iteration: 410 \t--- Loss: 0.004\n",
      "Iteration: 411 \t--- Loss: 0.004\n",
      "Iteration: 412 \t--- Loss: 0.004\n",
      "Iteration: 413 \t--- Loss: 0.004\n",
      "Iteration: 414 \t--- Loss: 0.004\n",
      "Iteration: 415 \t--- Loss: 0.004\n",
      "Iteration: 416 \t--- Loss: 0.004\n",
      "Iteration: 417 \t--- Loss: 0.003\n",
      "Iteration: 418 \t--- Loss: 0.004\n",
      "Iteration: 419 \t--- Loss: 0.004\n",
      "Iteration: 420 \t--- Loss: 0.004\n",
      "Iteration: 421 \t--- Loss: 0.004\n",
      "Iteration: 422 \t--- Loss: 0.004\n",
      "Iteration: 423 \t--- Loss: 0.003\n",
      "Iteration: 424 \t--- Loss: 0.004\n",
      "Iteration: 425 \t--- Loss: 0.004\n",
      "Iteration: 426 \t--- Loss: 0.004\n",
      "Iteration: 427 \t--- Loss: 0.003\n",
      "Iteration: 428 \t--- Loss: 0.004\n",
      "Iteration: 429 \t--- Loss: 0.004\n",
      "Iteration: 430 \t--- Loss: 0.004\n",
      "Iteration: 431 \t--- Loss: 0.004\n",
      "Iteration: 432 \t--- Loss: 0.003\n",
      "Iteration: 433 \t--- Loss: 0.004\n",
      "Iteration: 434 \t--- Loss: 0.004\n",
      "Iteration: 435 \t--- Loss: 0.004\n",
      "Iteration: 436 \t--- Loss: 0.004\n",
      "Iteration: 437 \t--- Loss: 0.004\n",
      "Iteration: 438 \t--- Loss: 0.003\n",
      "Iteration: 439 \t--- Loss: 0.004\n",
      "Iteration: 440 \t--- Loss: 0.003\n",
      "Iteration: 441 \t--- Loss: 0.004\n",
      "Iteration: 442 \t--- Loss: 0.004\n",
      "Iteration: 443 \t--- Loss: 0.004\n",
      "Iteration: 444 \t--- Loss: 0.004\n",
      "Iteration: 445 \t--- Loss: 0.004\n",
      "Iteration: 446 \t--- Loss: 0.004\n",
      "Iteration: 447 \t--- Loss: 0.003\n",
      "Iteration: 448 \t--- Loss: 0.004\n",
      "Iteration: 449 \t--- Loss: 0.004\n",
      "Iteration: 450 \t--- Loss: 0.004\n",
      "Iteration: 451 \t--- Loss: 0.003\n",
      "Iteration: 452 \t--- Loss: 0.003\n",
      "Iteration: 453 \t--- Loss: 0.004\n",
      "Iteration: 454 \t--- Loss: 0.003\n",
      "Iteration: 455 \t--- Loss: 0.004\n",
      "Iteration: 456 \t--- Loss: 0.004\n",
      "Iteration: 457 \t--- Loss: 0.004\n",
      "Iteration: 458 \t--- Loss: 0.003\n",
      "Iteration: 459 \t--- Loss: 0.004\n",
      "Iteration: 460 \t--- Loss: 0.004\n",
      "Iteration: 461 \t--- Loss: 0.003\n",
      "Iteration: 462 \t--- Loss: 0.004\n",
      "Iteration: 463 \t--- Loss: 0.004\n",
      "Iteration: 464 \t--- Loss: 0.004\n",
      "Iteration: 465 \t--- Loss: 0.004\n",
      "Iteration: 466 \t--- Loss: 0.004\n",
      "Iteration: 467 \t--- Loss: 0.004\n",
      "Iteration: 468 \t--- Loss: 0.004\n",
      "Iteration: 469 \t--- Loss: 0.004\n",
      "Iteration: 470 \t--- Loss: 0.004\n",
      "Iteration: 471 \t--- Loss: 0.003\n",
      "Iteration: 472 \t--- Loss: 0.004\n",
      "Iteration: 473 \t--- Loss: 0.003\n",
      "Iteration: 474 \t--- Loss: 0.004\n",
      "Iteration: 475 \t--- Loss: 0.004\n",
      "Iteration: 476 \t--- Loss: 0.004\n",
      "Iteration: 477 \t--- Loss: 0.004\n",
      "Iteration: 478 \t--- Loss: 0.004\n",
      "Iteration: 479 \t--- Loss: 0.004\n",
      "Iteration: 480 \t--- Loss: 0.004\n",
      "Iteration: 481 \t--- Loss: 0.004\n",
      "Iteration: 482 \t--- Loss: 0.004\n",
      "Iteration: 483 \t--- Loss: 0.004\n",
      "Iteration: 484 \t--- Loss: 0.004\n",
      "Iteration: 485 \t--- Loss: 0.004\n",
      "Iteration: 486 \t--- Loss: 0.004\n",
      "Iteration: 487 \t--- Loss: 0.004\n",
      "Iteration: 488 \t--- Loss: 0.004\n",
      "Iteration: 489 \t--- Loss: 0.004\n",
      "Iteration: 490 \t--- Loss: 0.004\n",
      "Iteration: 491 \t--- Loss: 0.004\n",
      "Iteration: 492 \t--- Loss: 0.003\n",
      "Iteration: 493 \t--- Loss: 0.004\n",
      "Iteration: 494 \t--- Loss: 0.004\n",
      "Iteration: 495 \t--- Loss: 0.004\n",
      "Iteration: 496 \t--- Loss: 0.004\n",
      "Iteration: 497 \t--- Loss: 0.004\n",
      "Iteration: 498 \t--- Loss: 0.004\n",
      "Iteration: 499 \t--- Loss: 0.004\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:11<00:03,  1.54s/it][Parallel(n_jobs=5)]: Done  45 tasks      | elapsed: 27.4min\n",
      "100%|██████████| 1/1 [01:38<00:00, 98.85s/it]t]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.008\n",
      "Iteration: 261 \t--- Loss: 0.008\n",
      "Iteration: 262 \t--- Loss: 0.008\n",
      "Iteration: 263 \t--- Loss: 0.008\n",
      "Iteration: 264 \t--- Loss: 0.008\n",
      "Iteration: 265 \t--- Loss: 0.008\n",
      "Iteration: 266 \t--- Loss: 0.008\n",
      "Iteration: 267 \t--- Loss: 0.007\n",
      "Iteration: 268 \t--- Loss: 0.008\n",
      "Iteration: 269 \t--- Loss: 0.008\n",
      "Iteration: 270 \t--- Loss: 0.008\n",
      "Iteration: 271 \t--- Loss: 0.008\n",
      "Iteration: 272 \t--- Loss: 0.008\n",
      "Iteration: 273 \t--- Loss: 0.008\n",
      "Iteration: 274 \t--- Loss: 0.008\n",
      "Iteration: 275 \t--- Loss: 0.008\n",
      "Iteration: 276 \t--- Loss: 0.008\n",
      "Iteration: 277 \t--- Loss: 0.008\n",
      "Iteration: 278 \t--- Loss: 0.007\n",
      "Iteration: 279 \t--- Loss: 0.007\n",
      "Iteration: 280 \t--- Loss: 0.007\n",
      "Iteration: 281 \t--- Loss: 0.007\n",
      "Iteration: 282 \t--- Loss: 0.007\n",
      "Iteration: 283 \t--- Loss: 0.007\n",
      "Iteration: 284 \t--- Loss: 0.007\n",
      "Iteration: 285 \t--- Loss: 0.006\n",
      "Iteration: 286 \t--- Loss: 0.007\n",
      "Iteration: 287 \t--- Loss: 0.006\n",
      "Iteration: 288 \t--- Loss: 0.007\n",
      "Iteration: 289 \t--- Loss: 0.007\n",
      "Iteration: 290 \t--- Loss: 0.006\n",
      "Iteration: 291 \t--- Loss: 0.006\n",
      "Iteration: 292 \t--- Loss: 0.006\n",
      "Iteration: 293 \t--- Loss: 0.007\n",
      "Iteration: 294 \t--- Loss: 0.007\n",
      "Iteration: 295 \t--- Loss: 0.007\n",
      "Iteration: 296 \t--- Loss: 0.008\n",
      "Iteration: 297 \t--- Loss: 0.007\n",
      "Iteration: 298 \t--- Loss: 0.007\n",
      "Iteration: 299 \t--- Loss: 0.008\n",
      "Iteration: 300 \t--- Loss: 0.008\n",
      "Iteration: 301 \t--- Loss: 0.008\n",
      "Iteration: 302 \t--- Loss: 0.007\n",
      "Iteration: 303 \t--- Loss: 0.007\n",
      "Iteration: 304 \t--- Loss: 0.007\n",
      "Iteration: 305 \t--- Loss: 0.006\n",
      "Iteration: 306 \t--- Loss: 0.007\n",
      "Iteration: 307 \t--- Loss: 0.007\n",
      "Iteration: 308 \t--- Loss: 0.007\n",
      "Iteration: 309 \t--- Loss: 0.006\n",
      "Iteration: 310 \t--- Loss: 0.006\n",
      "Iteration: 311 \t--- Loss: 0.007\n",
      "Iteration: 312 \t--- Loss: 0.006\n",
      "Iteration: 313 \t--- Loss: 0.006\n",
      "Iteration: 314 \t--- Loss: 0.006\n",
      "Iteration: 315 \t--- Loss: 0.006\n",
      "Iteration: 316 \t--- Loss: 0.006\n",
      "Iteration: 317 \t--- Loss: 0.006\n",
      "Iteration: 318 \t--- Loss: 0.006\n",
      "Iteration: 319 \t--- Loss: 0.006\n",
      "Iteration: 320 \t--- Loss: 0.006\n",
      "Iteration: 321 \t--- Loss: 0.007\n",
      "Iteration: 322 \t--- Loss: 0.006\n",
      "Iteration: 323 \t--- Loss: 0.006\n",
      "Iteration: 324 \t--- Loss: 0.006\n",
      "Iteration: 325 \t--- Loss: 0.006\n",
      "Iteration: 326 \t--- Loss: 0.006\n",
      "Iteration: 327 \t--- Loss: 0.005\n",
      "Iteration: 328 \t--- Loss: 0.006\n",
      "Iteration: 329 \t--- Loss: 0.006\n",
      "Iteration: 330 \t--- Loss: 0.006\n",
      "Iteration: 331 \t--- Loss: 0.006\n",
      "Iteration: 332 \t--- Loss: 0.006\n",
      "Iteration: 333 \t--- Loss: 0.006\n",
      "Iteration: 334 \t--- Loss: 0.006\n",
      "Iteration: 335 \t--- Loss: 0.006\n",
      "Iteration: 336 \t--- Loss: 0.006\n",
      "Iteration: 337 \t--- Loss: 0.006\n",
      "Iteration: 338 \t--- Loss: 0.006\n",
      "Iteration: 339 \t--- Loss: 0.006\n",
      "Iteration: 340 \t--- Loss: 0.006\n",
      "Iteration: 341 \t--- Loss: 0.006\n",
      "Iteration: 342 \t--- Loss: 0.006\n",
      "Iteration: 343 \t--- Loss: 0.006\n",
      "Iteration: 344 \t--- Loss: 0.006\n",
      "Iteration: 345 \t--- Loss: 0.006\n",
      "Iteration: 346 \t--- Loss: 0.006\n",
      "Iteration: 347 \t--- Loss: 0.005\n",
      "Iteration: 348 \t--- Loss: 0.006\n",
      "Iteration: 349 \t--- Loss: 0.005\n",
      "Iteration: 350 \t--- Loss: 0.006\n",
      "Iteration: 351 \t--- Loss: 0.006\n",
      "Iteration: 352 \t--- Loss: 0.006\n",
      "Iteration: 353 \t--- Loss: 0.006\n",
      "Iteration: 354 \t--- Loss: 0.006\n",
      "Iteration: 355 \t--- Loss: 0.006\n",
      "Iteration: 356 \t--- Loss: 0.006\n",
      "Iteration: 357 \t--- Loss: 0.006\n",
      "Iteration: 358 \t--- Loss: 0.005\n",
      "Iteration: 359 \t--- Loss: 0.005\n",
      "Iteration: 360 \t--- Loss: 0.006\n",
      "Iteration: 361 \t--- Loss: 0.005\n",
      "Iteration: 362 \t--- Loss: 0.006\n",
      "Iteration: 363 \t--- Loss: 0.006\n",
      "Iteration: 364 \t--- Loss: 0.006\n",
      "Iteration: 365 \t--- Loss: 0.006\n",
      "Iteration: 366 \t--- Loss: 0.006\n",
      "Iteration: 367 \t--- Loss: 0.006\n",
      "Iteration: 368 \t--- Loss: 0.005\n",
      "Iteration: 369 \t--- Loss: 0.005\n",
      "Iteration: 370 \t--- Loss: 0.006\n",
      "Iteration: 371 \t--- Loss: 0.006\n",
      "Iteration: 372 \t--- Loss: 0.006\n",
      "Iteration: 373 \t--- Loss: 0.006\n",
      "Iteration: 374 \t--- Loss: 0.005\n",
      "Iteration: 375 \t--- Loss: 0.005\n",
      "Iteration: 376 \t--- Loss: 0.005\n",
      "Iteration: 377 \t--- Loss: 0.005\n",
      "Iteration: 378 \t--- Loss: 0.005\n",
      "Iteration: 379 \t--- Loss: 0.006\n",
      "Iteration: 380 \t--- Loss: 0.005\n",
      "Iteration: 381 \t--- Loss: 0.005\n",
      "Iteration: 382 \t--- Loss: 0.006\n",
      "Iteration: 383 \t--- Loss: 0.005\n",
      "Iteration: 384 \t--- Loss: 0.006\n",
      "Iteration: 385 \t--- Loss: 0.006\n",
      "Iteration: 386 \t--- Loss: 0.005\n",
      "Iteration: 387 \t--- Loss: 0.005\n",
      "Iteration: 388 \t--- Loss: 0.005\n",
      "Iteration: 389 \t--- Loss: 0.006\n",
      "Iteration: 390 \t--- Loss: 0.006\n",
      "Iteration: 391 \t--- Loss: 0.006\n",
      "Iteration: 392 \t--- Loss: 0.005\n",
      "Iteration: 393 \t--- Loss: 0.005\n",
      "Iteration: 394 \t--- Loss: 0.005\n",
      "Iteration: 395 \t--- Loss: 0.006\n",
      "Iteration: 396 \t--- Loss: 0.005\n",
      "Iteration: 397 \t--- Loss: 0.005\n",
      "Iteration: 398 \t--- Loss: 0.005\n",
      "Iteration: 399 \t--- Loss: 0.006\n",
      "Iteration: 400 \t--- Loss: 0.006\n",
      "Iteration: 401 \t--- Loss: 0.006\n",
      "Iteration: 402 \t--- Loss: 0.005\n",
      "Iteration: 403 \t--- Loss: 0.005\n",
      "Iteration: 404 \t--- Loss: 0.005\n",
      "Iteration: 405 \t--- Loss: 0.005\n",
      "Iteration: 406 \t--- Loss: 0.006\n",
      "Iteration: 407 \t--- Loss: 0.005\n",
      "Iteration: 408 \t--- Loss: 0.005\n",
      "Iteration: 409 \t--- Loss: 0.005\n",
      "Iteration: 410 \t--- Loss: 0.005\n",
      "Iteration: 411 \t--- Loss: 0.005\n",
      "Iteration: 412 \t--- Loss: 0.005\n",
      "Iteration: 413 \t--- Loss: 0.005\n",
      "Iteration: 414 \t--- Loss: 0.005\n",
      "Iteration: 415 \t--- Loss: 0.005\n",
      "Iteration: 416 \t--- Loss: 0.005\n",
      "Iteration: 417 \t--- Loss: 0.005\n",
      "Iteration: 418 \t--- Loss: 0.006\n",
      "Iteration: 419 \t--- Loss: 0.006\n",
      "Iteration: 420 \t--- Loss: 0.006\n",
      "Iteration: 421 \t--- Loss: 0.005\n",
      "Iteration: 422 \t--- Loss: 0.005\n",
      "Iteration: 423 \t--- Loss: 0.005\n",
      "Iteration: 424 \t--- Loss: 0.006\n",
      "Iteration: 425 \t--- Loss: 0.005\n",
      "Iteration: 426 \t--- Loss: 0.006\n",
      "Iteration: 427 \t--- Loss: 0.005\n",
      "Iteration: 428 \t--- Loss: 0.005\n",
      "Iteration: 429 \t--- Loss: 0.005\n",
      "Iteration: 430 \t--- Loss: 0.006\n",
      "Iteration: 431 \t--- Loss: 0.005\n",
      "Iteration: 432 \t--- Loss: 0.005\n",
      "Iteration: 433 \t--- Loss: 0.005\n",
      "Iteration: 434 \t--- Loss: 0.005\n",
      "Iteration: 435 \t--- Loss: 0.005\n",
      "Iteration: 436 \t--- Loss: 0.005\n",
      "Iteration: 437 \t--- Loss: 0.005\n",
      "Iteration: 438 \t--- Loss: 0.005\n",
      "Iteration: 439 \t--- Loss: 0.005\n",
      "Iteration: 440 \t--- Loss: 0.005\n",
      "Iteration: 441 \t--- Loss: 0.005\n",
      "Iteration: 442 \t--- Loss: 0.006\n",
      "Iteration: 443 \t--- Loss: 0.005\n",
      "Iteration: 444 \t--- Loss: 0.005\n",
      "Iteration: 445 \t--- Loss: 0.005\n",
      "Iteration: 446 \t--- Loss: 0.005\n",
      "Iteration: 447 \t--- Loss: 0.005\n",
      "Iteration: 448 \t--- Loss: 0.005\n",
      "Iteration: 449 \t--- Loss: 0.005\n",
      "Iteration: 450 \t--- Loss: 0.006\n",
      "Iteration: 451 \t--- Loss: 0.005\n",
      "Iteration: 452 \t--- Loss: 0.005\n",
      "Iteration: 453 \t--- Loss: 0.005\n",
      "Iteration: 454 \t--- Loss: 0.005\n",
      "Iteration: 455 \t--- Loss: 0.005\n",
      "Iteration: 456 \t--- Loss: 0.005\n",
      "Iteration: 457 \t--- Loss: 0.005\n",
      "Iteration: 458 \t--- Loss: 0.005\n",
      "Iteration: 459 \t--- Loss: 0.006\n",
      "Iteration: 460 \t--- Loss: 0.005\n",
      "Iteration: 461 \t--- Loss: 0.006\n",
      "Iteration: 462 \t--- Loss: 0.005\n",
      "Iteration: 463 \t--- Loss: 0.006\n",
      "Iteration: 464 \t--- Loss: 0.006\n",
      "Iteration: 465 \t--- Loss: 0.005\n",
      "Iteration: 466 \t--- Loss: 0.005\n",
      "Iteration: 467 \t--- Loss: 0.006\n",
      "Iteration: 468 \t--- Loss: 0.006\n",
      "Iteration: 469 \t--- Loss: 0.005\n",
      "Iteration: 470 \t--- Loss: 0.005\n",
      "Iteration: 471 \t--- Loss: 0.005\n",
      "Iteration: 472 \t--- Loss: 0.005\n",
      "Iteration: 473 \t--- Loss: 0.005\n",
      "Iteration: 474 \t--- Loss: 0.005\n",
      "Iteration: 475 \t--- Loss: 0.005\n",
      "Iteration: 476 \t--- Loss: 0.005\n",
      "Iteration: 477 \t--- Loss: 0.005\n",
      "Iteration: 478 \t--- Loss: 0.006\n",
      "Iteration: 479 \t--- Loss: 0.005\n",
      "Iteration: 480 \t--- Loss: 0.006\n",
      "Iteration: 481 \t--- Loss: 0.005\n",
      "Iteration: 482 \t--- Loss: 0.005\n",
      "Iteration: 483 \t--- Loss: 0.006\n",
      "Iteration: 484 \t--- Loss: 0.006\n",
      "Iteration: 485 \t--- Loss: 0.005\n",
      "Iteration: 486 \t--- Loss: 0.005\n",
      "Iteration: 487 \t--- Loss: 0.005\n",
      "Iteration: 488 \t--- Loss: 0.005\n",
      "Iteration: 489 \t--- Loss: 0.006\n",
      "Iteration: 490 \t--- Loss: 0.005\n",
      "Iteration: 491 \t--- Loss: 0.005\n",
      "Iteration: 492 \t--- Loss: 0.005\n",
      "Iteration: 493 \t--- Loss: 0.005\n",
      "Iteration: 494 \t--- Loss: 0.005\n",
      "Iteration: 495 \t--- Loss: 0.006\n",
      "Iteration: 496 \t--- Loss: 0.006\n",
      "Iteration: 497 \t--- Loss: 0.006\n",
      "Iteration: 498 \t--- Loss: 0.005\n",
      "Iteration: 499 \t--- Loss: 0.005\n",
      "----  Optimizing the metamodel  ----\n",
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it][Parallel(n_jobs=5)]: Done  46 tasks      | elapsed: 27.4min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.58s/it][Parallel(n_jobs=5)]: Done  47 tasks      | elapsed: 27.6min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.53s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.173\n",
      "Iteration: 1 \t--- Loss: 0.175\n",
      "Iteration: 2 \t--- Loss: 0.163\n",
      "Iteration: 3 \t--- Loss: 0.155\n",
      "Iteration: 4 \t--- Loss: 0.164\n",
      "Iteration: 5 \t--- Loss: 0.157\n",
      "Iteration: 6 \t--- Loss: 0.151\n",
      "Iteration: 7 \t--- Loss: 0.154\n",
      "Iteration: 8 \t--- Loss: 0.155\n",
      "Iteration: 9 \t--- Loss: 0.146\n",
      "Iteration: 10 \t--- Loss: 0.144\n",
      "Iteration: 11 \t--- Loss: 0.143\n",
      "Iteration: 12 \t--- Loss: 0.146\n",
      "Iteration: 13 \t--- Loss: 0.128\n",
      "Iteration: 14 \t--- Loss: 0.137\n",
      "Iteration: 15 \t--- Loss: 0.144\n",
      "Iteration: 16 \t--- Loss: 0.131\n",
      "Iteration: 17 \t--- Loss: 0.133\n",
      "Iteration: 18 \t--- Loss: 0.127\n",
      "Iteration: 19 \t--- Loss: 0.132\n",
      "Iteration: 20 \t--- Loss: 0.124\n",
      "Iteration: 21 \t--- Loss: 0.132\n",
      "Iteration: 22 \t--- Loss: 0.127\n",
      "Iteration: 23 \t--- Loss: 0.124\n",
      "Iteration: 24 \t--- Loss: 0.130\n",
      "Iteration: 25 \t--- Loss: 0.122\n",
      "Iteration: 26 \t--- Loss: 0.123\n",
      "Iteration: 27 \t--- Loss: 0.130\n",
      "Iteration: 28 \t--- Loss: 0.130\n",
      "Iteration: 29 \t--- Loss: 0.112\n",
      "Iteration: 30 \t--- Loss: 0.126\n",
      "Iteration: 31 \t--- Loss: 0.126\n",
      "Iteration: 32 \t--- Loss: 0.127\n",
      "Iteration: 33 \t--- Loss: 0.123\n",
      "Iteration: 34 \t--- Loss: 0.117\n",
      "Iteration: 35 \t--- Loss: 0.127\n",
      "Iteration: 36 \t--- Loss: 0.118\n",
      "Iteration: 37 \t--- Loss: 0.137\n",
      "Iteration: 38 \t--- Loss: 0.121\n",
      "Iteration: 39 \t--- Loss: 0.132\n",
      "Iteration: 40 \t--- Loss: 0.130\n",
      "Iteration: 41 \t--- Loss: 0.125\n",
      "Iteration: 42 \t--- Loss: 0.122\n",
      "Iteration: 43 \t--- Loss: 0.116\n",
      "Iteration: 44 \t--- Loss: 0.126\n",
      "Iteration: 45 \t--- Loss: 0.124\n",
      "Iteration: 46 \t--- Loss: 0.125\n",
      "Iteration: 47 \t--- Loss: 0.118\n",
      "Iteration: 48 \t--- Loss: 0.121\n",
      "Iteration: 49 \t--- Loss: 0.130\n",
      "Iteration: 50 \t--- Loss: 0.109\n",
      "Iteration: 51 \t--- Loss: 0.122\n",
      "Iteration: 52 \t--- Loss: 0.129\n",
      "Iteration: 53 \t--- Loss: 0.122\n",
      "Iteration: 54 \t--- Loss: 0.123\n",
      "Iteration: 55 \t--- Loss: 0.119\n",
      "Iteration: 56 \t--- Loss: 0.122\n",
      "Iteration: 57 \t--- Loss: 0.117\n",
      "Iteration: 58 \t--- Loss: 0.124\n",
      "Iteration: 59 \t--- Loss: 0.124\n",
      "Iteration: 60 \t--- Loss: 0.124\n",
      "Iteration: 61 \t--- Loss: 0.119\n",
      "Iteration: 62 \t--- Loss: 0.117\n",
      "Iteration: 63 \t--- Loss: 0.121\n",
      "Iteration: 64 \t--- Loss: 0.132\n",
      "Iteration: 65 \t--- Loss: 0.120\n",
      "Iteration: 66 \t--- Loss: 0.129\n",
      "Iteration: 67 \t--- Loss: 0.128\n",
      "Iteration: 68 \t--- Loss: 0.120\n",
      "Iteration: 69 \t--- Loss: 0.121\n",
      "Iteration: 70 \t--- Loss: 0.129\n",
      "Iteration: 71 \t--- Loss: 0.116\n",
      "Iteration: 72 \t--- Loss: 0.130\n",
      "Iteration: 73 \t--- Loss: 0.130\n",
      "Iteration: 74 \t--- Loss: 0.114\n",
      "Iteration: 75 \t--- Loss: 0.120\n",
      "Iteration: 76 \t--- Loss: 0.124\n",
      "Iteration: 77 \t--- Loss: 0.112\n",
      "Iteration: 78 \t--- Loss: 0.132\n",
      "Iteration: 79 \t--- Loss: 0.114\n",
      "Iteration: 80 \t--- Loss: 0.115\n",
      "Iteration: 81 \t--- Loss: 0.126\n",
      "Iteration: 82 \t--- Loss: 0.123\n",
      "Iteration: 83 \t--- Loss: 0.105\n",
      "Iteration: 84 \t--- Loss: 0.118\n",
      "Iteration: 85 \t--- Loss: 0.114\n",
      "Iteration: 86 \t--- Loss: 0.121\n",
      "Iteration: 87 \t--- Loss: 0.117\n",
      "Iteration: 88 \t--- Loss: 0.127\n",
      "Iteration: 89 \t--- Loss: 0.111\n",
      "Iteration: 90 \t--- Loss: 0.119\n",
      "Iteration: 91 \t--- Loss: 0.123\n",
      "Iteration: 92 \t--- Loss: 0.119\n",
      "Iteration: 93 \t--- Loss: 0.109\n",
      "Iteration: 94 \t--- Loss: 0.111\n",
      "Iteration: 95 \t--- Loss: 0.125\n",
      "Iteration: 96 \t--- Loss: 0.125\n",
      "Iteration: 97 \t--- Loss: 0.125\n",
      "Iteration: 98 \t--- Loss: 0.125\n",
      "Iteration: 99 \t--- Loss: 0.135\n",
      "Iteration: 100 \t--- Loss: 0.130\n",
      "Iteration: 101 \t--- Loss: 0.130\n",
      "Iteration: 102 \t--- Loss: 0.123\n",
      "Iteration: 103 \t--- Loss: 0.124\n",
      "Iteration: 104 \t--- Loss: 0.129\n",
      "Iteration: 105 \t--- Loss: 0.123\n",
      "Iteration: 106 \t--- Loss: 0.119\n",
      "Iteration: 107 \t--- Loss: 0.108\n",
      "Iteration: 108 \t--- Loss: 0.119\n",
      "Iteration: 109 \t--- Loss: 0.115\n",
      "Iteration: 110 \t--- Loss: 0.120\n",
      "Iteration: 111 \t--- Loss: 0.128\n",
      "Iteration: 112 \t--- Loss: 0.119\n",
      "Iteration: 113 \t--- Loss: 0.124\n",
      "Iteration: 114 \t--- Loss: 0.125\n",
      "Iteration: 115 \t--- Loss: 0.129\n",
      "Iteration: 116 \t--- Loss: 0.121\n",
      "Iteration: 117 \t--- Loss: 0.123\n",
      "Iteration: 118 \t--- Loss: 0.111\n",
      "Iteration: 119 \t--- Loss: 0.127\n",
      "Iteration: 120 \t--- Loss: 0.119\n",
      "Iteration: 121 \t--- Loss: 0.128\n",
      "Iteration: 122 \t--- Loss: 0.121\n",
      "Iteration: 123 \t--- Loss: 0.125\n",
      "Iteration: 124 \t--- Loss: 0.117\n",
      "Iteration: 125 \t--- Loss: 0.122\n",
      "Iteration: 126 \t--- Loss: 0.120\n",
      "Iteration: 127 \t--- Loss: 0.124\n",
      "Iteration: 128 \t--- Loss: 0.130\n",
      "Iteration: 129 \t--- Loss: 0.118\n",
      "Iteration: 130 \t--- Loss: 0.122\n",
      "Iteration: 131 \t--- Loss: 0.123\n",
      "Iteration: 132 \t--- Loss: 0.133\n",
      "Iteration: 133 \t--- Loss: 0.111\n",
      "Iteration: 134 \t--- Loss: 0.121\n",
      "Iteration: 135 \t--- Loss: 0.123\n",
      "Iteration: 136 \t--- Loss: 0.126\n",
      "Iteration: 137 \t--- Loss: 0.113\n",
      "Iteration: 138 \t--- Loss: 0.115\n",
      "Iteration: 139 \t--- Loss: 0.122\n",
      "Iteration: 140 \t--- Loss: 0.110\n",
      "Iteration: 141 \t--- Loss: 0.122\n",
      "Iteration: 142 \t--- Loss: 0.111\n",
      "Iteration: 143 \t--- Loss: 0.123\n",
      "Iteration: 144 \t--- Loss: 0.133\n",
      "Iteration: 145 \t--- Loss: 0.119\n",
      "Iteration: 146 \t--- Loss: 0.131\n",
      "Iteration: 147 \t--- Loss: 0.125\n",
      "Iteration: 148 \t--- Loss: 0.114\n",
      "Iteration: 149 \t--- Loss: 0.115\n",
      "Iteration: 150 \t--- Loss: 0.123\n",
      "Iteration: 151 \t--- Loss: 0.115\n",
      "Iteration: 152 \t--- Loss: 0.124\n",
      "Iteration: 153 \t--- Loss: 0.121\n",
      "Iteration: 154 \t--- Loss: 0.122\n",
      "Iteration: 155 \t--- Loss: 0.124\n",
      "Iteration: 156 \t--- Loss: 0.114\n",
      "Iteration: 157 \t--- Loss: 0.130\n",
      "Iteration: 158 \t--- Loss: 0.115\n",
      "Iteration: 159 \t--- Loss: 0.120\n",
      "Iteration: 160 \t--- Loss: 0.121\n",
      "Iteration: 161 \t--- Loss: 0.121\n",
      "Iteration: 162 \t--- Loss: 0.120\n",
      "Iteration: 163 \t--- Loss: 0.121\n",
      "Iteration: 164 \t--- Loss: 0.127\n",
      "Iteration: 165 \t--- Loss: 0.128\n",
      "Iteration: 166 \t--- Loss: 0.126\n",
      "Iteration: 167 \t--- Loss: 0.122\n",
      "Iteration: 168 \t--- Loss: 0.127\n",
      "Iteration: 169 \t--- Loss: 0.129\n",
      "Iteration: 170 \t--- Loss: 0.117\n",
      "Iteration: 171 \t--- Loss: 0.118\n",
      "Iteration: 172 \t--- Loss: 0.124\n",
      "Iteration: 173 \t--- Loss: 0.119\n",
      "Iteration: 174 \t--- Loss: 0.116\n",
      "Iteration: 175 \t--- Loss: 0.130\n",
      "Iteration: 176 \t--- Loss: 0.127\n",
      "Iteration: 177 \t--- Loss: 0.133\n",
      "Iteration: 178 \t--- Loss: 0.119\n",
      "Iteration: 179 \t--- Loss: 0.129\n",
      "Iteration: 180 \t--- Loss: 0.117\n",
      "Iteration: 181 \t--- Loss: 0.118\n",
      "Iteration: 182 \t--- Loss: 0.124\n",
      "Iteration: 183 \t--- Loss: 0.120\n",
      "Iteration: 184 \t--- Loss: 0.116\n",
      "Iteration: 185 \t--- Loss: 0.122\n",
      "Iteration: 186 \t--- Loss: 0.112\n",
      "Iteration: 187 \t--- Loss: 0.111\n",
      "Iteration: 188 \t--- Loss: 0.111\n",
      "Iteration: 189 \t--- Loss: 0.119\n",
      "Iteration: 190 \t--- Loss: 0.122\n",
      "Iteration: 191 \t--- Loss: 0.120\n",
      "Iteration: 192 \t--- Loss: 0.113\n",
      "Iteration: 193 \t--- Loss: 0.113\n",
      "Iteration: 194 \t--- Loss: 0.124\n",
      "Iteration: 195 \t--- Loss: 0.121\n",
      "Iteration: 196 \t--- Loss: 0.113\n",
      "Iteration: 197 \t--- Loss: 0.124\n",
      "Iteration: 198 \t--- Loss: 0.129\n",
      "Iteration: 199 \t--- Loss: 0.125\n",
      "Iteration: 200 \t--- Loss: 0.111\n",
      "Iteration: 201 \t--- Loss: 0.112\n",
      "Iteration: 202 \t--- Loss: 0.122\n",
      "Iteration: 203 \t--- Loss: 0.126\n",
      "Iteration: 204 \t--- Loss: 0.124\n",
      "Iteration: 205 \t--- Loss: 0.113\n",
      "Iteration: 206 \t--- Loss: 0.123\n",
      "Iteration: 207 \t--- Loss: 0.124\n",
      "Iteration: 208 \t--- Loss: 0.129\n",
      "Iteration: 209 \t--- Loss: 0.118\n",
      "Iteration: 210 \t--- Loss: 0.114\n",
      "Iteration: 211 \t--- Loss: 0.139\n",
      "Iteration: 212 \t--- Loss: 0.113\n",
      "Iteration: 213 \t--- Loss: 0.115\n",
      "Iteration: 214 \t--- Loss: 0.120\n",
      "Iteration: 215 \t--- Loss: 0.123\n",
      "Iteration: 216 \t--- Loss: 0.113\n",
      "Iteration: 217 \t--- Loss: 0.129\n",
      "Iteration: 218 \t--- Loss: 0.127\n",
      "Iteration: 219 \t--- Loss: 0.118\n",
      "Iteration: 220 \t--- Loss: 0.119\n",
      "Iteration: 221 \t--- Loss: 0.119\n",
      "Iteration: 222 \t--- Loss: 0.123\n",
      "Iteration: 223 \t--- Loss: 0.117\n",
      "Iteration: 224 \t--- Loss: 0.117\n",
      "Iteration: 225 \t--- Loss: 0.121\n",
      "Iteration: 226 \t--- Loss: 0.129\n",
      "Iteration: 227 \t--- Loss: 0.119\n",
      "Iteration: 228 \t--- Loss: 0.116\n",
      "Iteration: 229 \t--- Loss: 0.111\n",
      "Iteration: 230 \t--- Loss: 0.121\n",
      "Iteration: 231 \t--- Loss: 0.120\n",
      "Iteration: 232 \t--- Loss: 0.123\n",
      "Iteration: 233 \t--- Loss: 0.119\n",
      "Iteration: 234 \t--- Loss: 0.116\n",
      "Iteration: 235 \t--- Loss: 0.132\n",
      "Iteration: 236 \t--- Loss: 0.126\n",
      "Iteration: 237 \t--- Loss: 0.123\n",
      "Iteration: 238 \t--- Loss: 0.130\n",
      "Iteration: 239 \t--- Loss: 0.122\n",
      "Iteration: 240 \t--- Loss: 0.126\n",
      "Iteration: 241 \t--- Loss: 0.123\n",
      "Iteration: 242 \t--- Loss: 0.128\n",
      "Iteration: 243 \t--- Loss: 0.119\n",
      "Iteration: 244 \t--- Loss: 0.133\n",
      "Iteration: 245 \t--- Loss: 0.121\n",
      "Iteration: 246 \t--- Loss: 0.108\n",
      "Iteration: 247 \t--- Loss: 0.129\n",
      "Iteration: 248 \t--- Loss: 0.126\n",
      "Iteration: 249 \t--- Loss: 0.127\n",
      "Iteration: 250 \t--- Loss: 0.120\n",
      "Iteration: 251 \t--- Loss: 0.123\n",
      "Iteration: 252 \t--- Loss: 0.116\n",
      "Iteration: 253 \t--- Loss: 0.122\n",
      "Iteration: 254 \t--- Loss: 0.125\n",
      "Iteration: 255 \t--- Loss: 0.120\n",
      "Iteration: 256 \t--- Loss: 0.111\n",
      "Iteration: 257 \t--- Loss: 0.119\n",
      "Iteration: 258 \t--- Loss: 0.116\n",
      "Iteration: 259 \t--- Loss: 0.123Iteration: 0 \t--- Loss: 0.562\n",
      "Iteration: 1 \t--- Loss: 0.523\n",
      "Iteration: 2 \t--- Loss: 0.469\n",
      "Iteration: 3 \t--- Loss: 0.450\n",
      "Iteration: 4 \t--- Loss: 0.427\n",
      "Iteration: 5 \t--- Loss: 0.401\n",
      "Iteration: 6 \t--- Loss: 0.377\n",
      "Iteration: 7 \t--- Loss: 0.349\n",
      "Iteration: 8 \t--- Loss: 0.356\n",
      "Iteration: 9 \t--- Loss: 0.344\n",
      "Iteration: 10 \t--- Loss: 0.345\n",
      "Iteration: 11 \t--- Loss: 0.336\n",
      "Iteration: 12 \t--- Loss: 0.300\n",
      "Iteration: 13 \t--- Loss: 0.316\n",
      "Iteration: 14 \t--- Loss: 0.318\n",
      "Iteration: 15 \t--- Loss: 0.289\n",
      "Iteration: 16 \t--- Loss: 0.306\n",
      "Iteration: 17 \t--- Loss: 0.309\n",
      "Iteration: 18 \t--- Loss: 0.310\n",
      "Iteration: 19 \t--- Loss: 0.286\n",
      "Iteration: 20 \t--- Loss: 0.297\n",
      "Iteration: 21 \t--- Loss: 0.271\n",
      "Iteration: 22 \t--- Loss: 0.280\n",
      "Iteration: 23 \t--- Loss: 0.288\n",
      "Iteration: 24 \t--- Loss: 0.273\n",
      "Iteration: 25 \t--- Loss: 0.269\n",
      "Iteration: 26 \t--- Loss: 0.266\n",
      "Iteration: 27 \t--- Loss: 0.273\n",
      "Iteration: 28 \t--- Loss: 0.259\n",
      "Iteration: 29 \t--- Loss: 0.256\n",
      "Iteration: 30 \t--- Loss: 0.291\n",
      "Iteration: 31 \t--- Loss: 0.262\n",
      "Iteration: 32 \t--- Loss: 0.262\n",
      "Iteration: 33 \t--- Loss: 0.258\n",
      "Iteration: 34 \t--- Loss: 0.266\n",
      "Iteration: 35 \t--- Loss: 0.268\n",
      "Iteration: 36 \t--- Loss: 0.257\n",
      "Iteration: 37 \t--- Loss: 0.277\n",
      "Iteration: 38 \t--- Loss: 0.259\n",
      "Iteration: 39 \t--- Loss: 0.255\n",
      "Iteration: 40 \t--- Loss: 0.261\n",
      "Iteration: 41 \t--- Loss: 0.255\n",
      "Iteration: 42 \t--- Loss: 0.281\n",
      "Iteration: 43 \t--- Loss: 0.271\n",
      "Iteration: 44 \t--- Loss: 0.261\n",
      "Iteration: 45 \t--- Loss: 0.281\n",
      "Iteration: 46 \t--- Loss: 0.266\n",
      "Iteration: 47 \t--- Loss: 0.265\n",
      "Iteration: 48 \t--- Loss: 0.253\n",
      "Iteration: 49 \t--- Loss: 0.266\n",
      "Iteration: 50 \t--- Loss: 0.264\n",
      "Iteration: 51 \t--- Loss: 0.252\n",
      "Iteration: 52 \t--- Loss: 0.279\n",
      "Iteration: 53 \t--- Loss: 0.260\n",
      "Iteration: 54 \t--- Loss: 0.253\n",
      "Iteration: 55 \t--- Loss: 0.262\n",
      "Iteration: 56 \t--- Loss: 0.266\n",
      "Iteration: 57 \t--- Loss: 0.265\n",
      "Iteration: 58 \t--- Loss: 0.251\n",
      "Iteration: 59 \t--- Loss: 0.251\n",
      "Iteration: 60 \t--- Loss: 0.261\n",
      "Iteration: 61 \t--- Loss: 0.263\n",
      "Iteration: 62 \t--- Loss: 0.260\n",
      "Iteration: 63 \t--- Loss: 0.275\n",
      "Iteration: 64 \t--- Loss: 0.262\n",
      "Iteration: 65 \t--- Loss: 0.240\n",
      "Iteration: 66 \t--- Loss: 0.277\n",
      "Iteration: 67 \t--- Loss: 0.257\n",
      "Iteration: 68 \t--- Loss: 0.262\n",
      "Iteration: 69 \t--- Loss: 0.254\n",
      "Iteration: 70 \t--- Loss: 0.265\n",
      "Iteration: 71 \t--- Loss: 0.244\n",
      "Iteration: 72 \t--- Loss: 0.256\n",
      "Iteration: 73 \t--- Loss: 0.270\n",
      "Iteration: 74 \t--- Loss: 0.264\n",
      "Iteration: 75 \t--- Loss: 0.270\n",
      "Iteration: 76 \t--- Loss: 0.256\n",
      "Iteration: 77 \t--- Loss: 0.260\n",
      "Iteration: 78 \t--- Loss: 0.279\n",
      "Iteration: 79 \t--- Loss: 0.241\n",
      "Iteration: 80 \t--- Loss: 0.250\n",
      "Iteration: 81 \t--- Loss: 0.245\n",
      "Iteration: 82 \t--- Loss: 0.262\n",
      "Iteration: 83 \t--- Loss: 0.266\n",
      "Iteration: 84 \t--- Loss: 0.252\n",
      "Iteration: 85 \t--- Loss: 0.249\n",
      "Iteration: 86 \t--- Loss: 0.252\n",
      "Iteration: 87 \t--- Loss: 0.251\n",
      "Iteration: 88 \t--- Loss: 0.261\n",
      "Iteration: 89 \t--- Loss: 0.264\n",
      "Iteration: 90 \t--- Loss: 0.245\n",
      "Iteration: 91 \t--- Loss: 0.266\n",
      "Iteration: 92 \t--- Loss: 0.258\n",
      "Iteration: 93 \t--- Loss: 0.278\n",
      "Iteration: 94 \t--- Loss: 0.268\n",
      "Iteration: 95 \t--- Loss: 0.245\n",
      "Iteration: 96 \t--- Loss: 0.248\n",
      "Iteration: 97 \t--- Loss: 0.276\n",
      "Iteration: 98 \t--- Loss: 0.258\n",
      "Iteration: 99 \t--- Loss: 0.260\n",
      "Iteration: 100 \t--- Loss: 0.244\n",
      "Iteration: 101 \t--- Loss: 0.264\n",
      "Iteration: 102 \t--- Loss: 0.267\n",
      "Iteration: 103 \t--- Loss: 0.253\n",
      "Iteration: 104 \t--- Loss: 0.255\n",
      "Iteration: 105 \t--- Loss: 0.261\n",
      "Iteration: 106 \t--- Loss: 0.261\n",
      "Iteration: 107 \t--- Loss: 0.254\n",
      "Iteration: 108 \t--- Loss: 0.262\n",
      "Iteration: 109 \t--- Loss: 0.256\n",
      "Iteration: 110 \t--- Loss: 0.272\n",
      "Iteration: 111 \t--- Loss: 0.258\n",
      "Iteration: 112 \t--- Loss: 0.241\n",
      "Iteration: 113 \t--- Loss: 0.262\n",
      "Iteration: 114 \t--- Loss: 0.253\n",
      "Iteration: 115 \t--- Loss: 0.259\n",
      "Iteration: 116 \t--- Loss: 0.270\n",
      "Iteration: 117 \t--- Loss: 0.279\n",
      "Iteration: 118 \t--- Loss: 0.268\n",
      "Iteration: 119 \t--- Loss: 0.260\n",
      "Iteration: 120 \t--- Loss: 0.260\n",
      "Iteration: 121 \t--- Loss: 0.250\n",
      "Iteration: 122 \t--- Loss: 0.274\n",
      "Iteration: 123 \t--- Loss: 0.259\n",
      "Iteration: 124 \t--- Loss: 0.270\n",
      "Iteration: 125 \t--- Loss: 0.258\n",
      "Iteration: 126 \t--- Loss: 0.270\n",
      "Iteration: 127 \t--- Loss: 0.268\n",
      "Iteration: 128 \t--- Loss: 0.257\n",
      "Iteration: 129 \t--- Loss: 0.269\n",
      "Iteration: 130 \t--- Loss: 0.259\n",
      "Iteration: 131 \t--- Loss: 0.258\n",
      "Iteration: 132 \t--- Loss: 0.249\n",
      "Iteration: 133 \t--- Loss: 0.243\n",
      "Iteration: 134 \t--- Loss: 0.260\n",
      "Iteration: 135 \t--- Loss: 0.270\n",
      "Iteration: 136 \t--- Loss: 0.255\n",
      "Iteration: 137 \t--- Loss: 0.272\n",
      "Iteration: 138 \t--- Loss: 0.249\n",
      "Iteration: 139 \t--- Loss: 0.268\n",
      "Iteration: 140 \t--- Loss: 0.241\n",
      "Iteration: 141 \t--- Loss: 0.265\n",
      "Iteration: 142 \t--- Loss: 0.255\n",
      "Iteration: 143 \t--- Loss: 0.248\n",
      "Iteration: 144 \t--- Loss: 0.258\n",
      "Iteration: 145 \t--- Loss: 0.248\n",
      "Iteration: 146 \t--- Loss: 0.245\n",
      "Iteration: 147 \t--- Loss: 0.262\n",
      "Iteration: 148 \t--- Loss: 0.250\n",
      "Iteration: 149 \t--- Loss: 0.256\n",
      "Iteration: 150 \t--- Loss: 0.242\n",
      "Iteration: 151 \t--- Loss: 0.267\n",
      "Iteration: 152 \t--- Loss: 0.262\n",
      "Iteration: 153 \t--- Loss: 0.257\n",
      "Iteration: 154 \t--- Loss: 0.245\n",
      "Iteration: 155 \t--- Loss: 0.251\n",
      "Iteration: 156 \t--- Loss: 0.232\n",
      "Iteration: 157 \t--- Loss: 0.277\n",
      "Iteration: 158 \t--- Loss: 0.245\n",
      "Iteration: 159 \t--- Loss: 0.253\n",
      "Iteration: 160 \t--- Loss: 0.267\n",
      "Iteration: 161 \t--- Loss: 0.250\n",
      "Iteration: 162 \t--- Loss: 0.262\n",
      "Iteration: 163 \t--- Loss: 0.245\n",
      "Iteration: 164 \t--- Loss: 0.286\n",
      "Iteration: 165 \t--- Loss: 0.265\n",
      "Iteration: 166 \t--- Loss: 0.260\n",
      "Iteration: 167 \t--- Loss: 0.258\n",
      "Iteration: 168 \t--- Loss: 0.240\n",
      "Iteration: 169 \t--- Loss: 0.267\n",
      "Iteration: 170 \t--- Loss: 0.272\n",
      "Iteration: 171 \t--- Loss: 0.261\n",
      "Iteration: 172 \t--- Loss: 0.276\n",
      "Iteration: 173 \t--- Loss: 0.266\n",
      "Iteration: 174 \t--- Loss: 0.257\n",
      "Iteration: 175 \t--- Loss: 0.253\n",
      "Iteration: 176 \t--- Loss: 0.253\n",
      "Iteration: 177 \t--- Loss: 0.240\n",
      "Iteration: 178 \t--- Loss: 0.281\n",
      "Iteration: 179 \t--- Loss: 0.263\n",
      "Iteration: 180 \t--- Loss: 0.260\n",
      "Iteration: 181 \t--- Loss: 0.246\n",
      "Iteration: 182 \t--- Loss: 0.261\n",
      "Iteration: 183 \t--- Loss: 0.241\n",
      "Iteration: 184 \t--- Loss: 0.266\n",
      "Iteration: 185 \t--- Loss: 0.249\n",
      "Iteration: 186 \t--- Loss: 0.250\n",
      "Iteration: 187 \t--- Loss: 0.244\n",
      "Iteration: 188 \t--- Loss: 0.243\n",
      "Iteration: 189 \t--- Loss: 0.269\n",
      "Iteration: 190 \t--- Loss: 0.276\n",
      "Iteration: 191 \t--- Loss: 0.284\n",
      "Iteration: 192 \t--- Loss: 0.248\n",
      "Iteration: 193 \t--- Loss: 0.260\n",
      "Iteration: 194 \t--- Loss: 0.265\n",
      "Iteration: 195 \t--- Loss: 0.247\n",
      "Iteration: 196 \t--- Loss: 0.268\n",
      "Iteration: 197 \t--- Loss: 0.252\n",
      "Iteration: 198 \t--- Loss: 0.242\n",
      "Iteration: 199 \t--- Loss: 0.265\n",
      "Iteration: 200 \t--- Loss: 0.260\n",
      "Iteration: 201 \t--- Loss: 0.280\n",
      "Iteration: 202 \t--- Loss: 0.248\n",
      "Iteration: 203 \t--- Loss: 0.277\n",
      "Iteration: 204 \t--- Loss: 0.260\n",
      "Iteration: 205 \t--- Loss: 0.253\n",
      "Iteration: 206 \t--- Loss: 0.234\n",
      "Iteration: 207 \t--- Loss: 0.257\n",
      "Iteration: 208 \t--- Loss: 0.262\n",
      "Iteration: 209 \t--- Loss: 0.245\n",
      "Iteration: 210 \t--- Loss: 0.254\n",
      "Iteration: 211 \t--- Loss: 0.256\n",
      "Iteration: 212 \t--- Loss: 0.245\n",
      "Iteration: 213 \t--- Loss: 0.263\n",
      "Iteration: 214 \t--- Loss: 0.259\n",
      "Iteration: 215 \t--- Loss: 0.255\n",
      "Iteration: 216 \t--- Loss: 0.255\n",
      "Iteration: 217 \t--- Loss: 0.266\n",
      "Iteration: 218 \t--- Loss: 0.267\n",
      "Iteration: 219 \t--- Loss: 0.261\n",
      "Iteration: 220 \t--- Loss: 0.259\n",
      "Iteration: 221 \t--- Loss: 0.263\n",
      "Iteration: 222 \t--- Loss: 0.276\n",
      "Iteration: 223 \t--- Loss: 0.267\n",
      "Iteration: 224 \t--- Loss: 0.238\n",
      "Iteration: 225 \t--- Loss: 0.253\n",
      "Iteration: 226 \t--- Loss: 0.254\n",
      "Iteration: 227 \t--- Loss: 0.270\n",
      "Iteration: 228 \t--- Loss: 0.239\n",
      "Iteration: 229 \t--- Loss: 0.261\n",
      "Iteration: 230 \t--- Loss: 0.256\n",
      "Iteration: 231 \t--- Loss: 0.263\n",
      "Iteration: 232 \t--- Loss: 0.266\n",
      "Iteration: 233 \t--- Loss: 0.266\n",
      "Iteration: 234 \t--- Loss: 0.263\n",
      "Iteration: 235 \t--- Loss: 0.223\n",
      "Iteration: 236 \t--- Loss: 0.253\n",
      "Iteration: 237 \t--- Loss: 0.241\n",
      "Iteration: 238 \t--- Loss: 0.242\n",
      "Iteration: 239 \t--- Loss: 0.250\n",
      "Iteration: 240 \t--- Loss: 0.268\n",
      "Iteration: 241 \t--- Loss: 0.256\n",
      "Iteration: 242 \t--- Loss: 0.256\n",
      "Iteration: 243 \t--- Loss: 0.249\n",
      "Iteration: 244 \t--- Loss: 0.249\n",
      "Iteration: 245 \t--- Loss: 0.270\n",
      "Iteration: 246 \t--- Loss: 0.262\n",
      "Iteration: 247 \t--- Loss: 0.252\n",
      "Iteration: 248 \t--- Loss: 0.264\n",
      "Iteration: 249 \t--- Loss: 0.259\n",
      "Iteration: 250 \t--- Loss: 0.251\n",
      "Iteration: 251 \t--- Loss: 0.258\n",
      "Iteration: 252 \t--- Loss: 0.262\n",
      "Iteration: 253 \t--- Loss: 0.257\n",
      "Iteration: 254 \t--- Loss: 0.252\n",
      "Iteration: 255 \t--- Loss: 0.258\n",
      "Iteration: 256 \t--- Loss: 0.244\n",
      "Iteration: 257 \t--- Loss: 0.271\n",
      "Iteration: 258 \t--- Loss: 0.248\n",
      "Iteration: 259 \t--- Loss: 0.246Iteration: 0 \t--- Loss: 0.129\n",
      "Iteration: 1 \t--- Loss: 0.111\n",
      "Iteration: 2 \t--- Loss: 0.107\n",
      "Iteration: 3 \t--- Loss: 0.103\n",
      "Iteration: 4 \t--- Loss: 0.087\n",
      "Iteration: 5 \t--- Loss: 0.085\n",
      "Iteration: 6 \t--- Loss: 0.071\n",
      "Iteration: 7 \t--- Loss: 0.070\n",
      "Iteration: 8 \t--- Loss: 0.063\n",
      "Iteration: 9 \t--- Loss: 0.062\n",
      "Iteration: 10 \t--- Loss: 0.055\n",
      "Iteration: 11 \t--- Loss: 0.051\n",
      "Iteration: 12 \t--- Loss: 0.049\n",
      "Iteration: 13 \t--- Loss: 0.044\n",
      "Iteration: 14 \t--- Loss: 0.043\n",
      "Iteration: 15 \t--- Loss: 0.041\n",
      "Iteration: 16 \t--- Loss: 0.038\n",
      "Iteration: 17 \t--- Loss: 0.036\n",
      "Iteration: 18 \t--- Loss: 0.033\n",
      "Iteration: 19 \t--- Loss: 0.032\n",
      "Iteration: 20 \t--- Loss: 0.033\n",
      "Iteration: 21 \t--- Loss: 0.031\n",
      "Iteration: 22 \t--- Loss: 0.027\n",
      "Iteration: 23 \t--- Loss: 0.028\n",
      "Iteration: 24 \t--- Loss: 0.029\n",
      "Iteration: 25 \t--- Loss: 0.027\n",
      "Iteration: 26 \t--- Loss: 0.028\n",
      "Iteration: 27 \t--- Loss: 0.026\n",
      "Iteration: 28 \t--- Loss: 0.024\n",
      "Iteration: 29 \t--- Loss: 0.024\n",
      "Iteration: 30 \t--- Loss: 0.024\n",
      "Iteration: 31 \t--- Loss: 0.024\n",
      "Iteration: 32 \t--- Loss: 0.024\n",
      "Iteration: 33 \t--- Loss: 0.022\n",
      "Iteration: 34 \t--- Loss: 0.022\n",
      "Iteration: 35 \t--- Loss: 0.022\n",
      "Iteration: 36 \t--- Loss: 0.023\n",
      "Iteration: 37 \t--- Loss: 0.022\n",
      "Iteration: 38 \t--- Loss: 0.021\n",
      "Iteration: 39 \t--- Loss: 0.021\n",
      "Iteration: 40 \t--- Loss: 0.020\n",
      "Iteration: 41 \t--- Loss: 0.020\n",
      "Iteration: 42 \t--- Loss: 0.020\n",
      "Iteration: 43 \t--- Loss: 0.020\n",
      "Iteration: 44 \t--- Loss: 0.019\n",
      "Iteration: 45 \t--- Loss: 0.019\n",
      "Iteration: 46 \t--- Loss: 0.020\n",
      "Iteration: 47 \t--- Loss: 0.019\n",
      "Iteration: 48 \t--- Loss: 0.019\n",
      "Iteration: 49 \t--- Loss: 0.018\n",
      "Iteration: 50 \t--- Loss: 0.019\n",
      "Iteration: 51 \t--- Loss: 0.019\n",
      "Iteration: 52 \t--- Loss: 0.017\n",
      "Iteration: 53 \t--- Loss: 0.016\n",
      "Iteration: 54 \t--- Loss: 0.018\n",
      "Iteration: 55 \t--- Loss: 0.018\n",
      "Iteration: 56 \t--- Loss: 0.017\n",
      "Iteration: 57 \t--- Loss: 0.018\n",
      "Iteration: 58 \t--- Loss: 0.019\n",
      "Iteration: 59 \t--- Loss: 0.018\n",
      "Iteration: 60 \t--- Loss: 0.017\n",
      "Iteration: 61 \t--- Loss: 0.018\n",
      "Iteration: 62 \t--- Loss: 0.017\n",
      "Iteration: 63 \t--- Loss: 0.016\n",
      "Iteration: 64 \t--- Loss: 0.017\n",
      "Iteration: 65 \t--- Loss: 0.017\n",
      "Iteration: 66 \t--- Loss: 0.016\n",
      "Iteration: 67 \t--- Loss: 0.016\n",
      "Iteration: 68 \t--- Loss: 0.016\n",
      "Iteration: 69 \t--- Loss: 0.018\n",
      "Iteration: 70 \t--- Loss: 0.018\n",
      "Iteration: 71 \t--- Loss: 0.018\n",
      "Iteration: 72 \t--- Loss: 0.017\n",
      "Iteration: 73 \t--- Loss: 0.017\n",
      "Iteration: 74 \t--- Loss: 0.016\n",
      "Iteration: 75 \t--- Loss: 0.016\n",
      "Iteration: 76 \t--- Loss: 0.015\n",
      "Iteration: 77 \t--- Loss: 0.016\n",
      "Iteration: 78 \t--- Loss: 0.015\n",
      "Iteration: 79 \t--- Loss: 0.017\n",
      "Iteration: 80 \t--- Loss: 0.017\n",
      "Iteration: 81 \t--- Loss: 0.016\n",
      "Iteration: 82 \t--- Loss: 0.015\n",
      "Iteration: 83 \t--- Loss: 0.017\n",
      "Iteration: 84 \t--- Loss: 0.017\n",
      "Iteration: 85 \t--- Loss: 0.015\n",
      "Iteration: 86 \t--- Loss: 0.016\n",
      "Iteration: 87 \t--- Loss: 0.016\n",
      "Iteration: 88 \t--- Loss: 0.016\n",
      "Iteration: 89 \t--- Loss: 0.015\n",
      "Iteration: 90 \t--- Loss: 0.014\n",
      "Iteration: 91 \t--- Loss: 0.015\n",
      "Iteration: 92 \t--- Loss: 0.016\n",
      "Iteration: 93 \t--- Loss: 0.017\n",
      "Iteration: 94 \t--- Loss: 0.015\n",
      "Iteration: 95 \t--- Loss: 0.016\n",
      "Iteration: 96 \t--- Loss: 0.014\n",
      "Iteration: 97 \t--- Loss: 0.015\n",
      "Iteration: 98 \t--- Loss: 0.015\n",
      "Iteration: 99 \t--- Loss: 0.016\n",
      "Iteration: 100 \t--- Loss: 0.015\n",
      "Iteration: 101 \t--- Loss: 0.015\n",
      "Iteration: 102 \t--- Loss: 0.015\n",
      "Iteration: 103 \t--- Loss: 0.015\n",
      "Iteration: 104 \t--- Loss: 0.015\n",
      "Iteration: 105 \t--- Loss: 0.017\n",
      "Iteration: 106 \t--- Loss: 0.016\n",
      "Iteration: 107 \t--- Loss: 0.013\n",
      "Iteration: 108 \t--- Loss: 0.015\n",
      "Iteration: 109 \t--- Loss: 0.015\n",
      "Iteration: 110 \t--- Loss: 0.015\n",
      "Iteration: 111 \t--- Loss: 0.015\n",
      "Iteration: 112 \t--- Loss: 0.015\n",
      "Iteration: 113 \t--- Loss: 0.014\n",
      "Iteration: 114 \t--- Loss: 0.015\n",
      "Iteration: 115 \t--- Loss: 0.014\n",
      "Iteration: 116 \t--- Loss: 0.015\n",
      "Iteration: 117 \t--- Loss: 0.014\n",
      "Iteration: 118 \t--- Loss: 0.015\n",
      "Iteration: 119 \t--- Loss: 0.015\n",
      "Iteration: 120 \t--- Loss: 0.014\n",
      "Iteration: 121 \t--- Loss: 0.015\n",
      "Iteration: 122 \t--- Loss: 0.015\n",
      "Iteration: 123 \t--- Loss: 0.014\n",
      "Iteration: 124 \t--- Loss: 0.015\n",
      "Iteration: 125 \t--- Loss: 0.015\n",
      "Iteration: 126 \t--- Loss: 0.014\n",
      "Iteration: 127 \t--- Loss: 0.015\n",
      "Iteration: 128 \t--- Loss: 0.015\n",
      "Iteration: 129 \t--- Loss: 0.014\n",
      "Iteration: 130 \t--- Loss: 0.015\n",
      "Iteration: 131 \t--- Loss: 0.015\n",
      "Iteration: 132 \t--- Loss: 0.014\n",
      "Iteration: 133 \t--- Loss: 0.015\n",
      "Iteration: 134 \t--- Loss: 0.016\n",
      "Iteration: 135 \t--- Loss: 0.014\n",
      "Iteration: 136 \t--- Loss: 0.013\n",
      "Iteration: 137 \t--- Loss: 0.015\n",
      "Iteration: 138 \t--- Loss: 0.014\n",
      "Iteration: 139 \t--- Loss: 0.013\n",
      "Iteration: 140 \t--- Loss: 0.014\n",
      "Iteration: 141 \t--- Loss: 0.017\n",
      "Iteration: 142 \t--- Loss: 0.015\n",
      "Iteration: 143 \t--- Loss: 0.014\n",
      "Iteration: 144 \t--- Loss: 0.015\n",
      "Iteration: 145 \t--- Loss: 0.015\n",
      "Iteration: 146 \t--- Loss: 0.014\n",
      "Iteration: 147 \t--- Loss: 0.016\n",
      "Iteration: 148 \t--- Loss: 0.015\n",
      "Iteration: 149 \t--- Loss: 0.015\n",
      "Iteration: 150 \t--- Loss: 0.015\n",
      "Iteration: 151 \t--- Loss: 0.013\n",
      "Iteration: 152 \t--- Loss: 0.014\n",
      "Iteration: 153 \t--- Loss: 0.016\n",
      "Iteration: 154 \t--- Loss: 0.014\n",
      "Iteration: 155 \t--- Loss: 0.016\n",
      "Iteration: 156 \t--- Loss: 0.016\n",
      "Iteration: 157 \t--- Loss: 0.014\n",
      "Iteration: 158 \t--- Loss: 0.016\n",
      "Iteration: 159 \t--- Loss: 0.016\n",
      "Iteration: 160 \t--- Loss: 0.014\n",
      "Iteration: 161 \t--- Loss: 0.014\n",
      "Iteration: 162 \t--- Loss: 0.015\n",
      "Iteration: 163 \t--- Loss: 0.014\n",
      "Iteration: 164 \t--- Loss: 0.014\n",
      "Iteration: 165 \t--- Loss: 0.013\n",
      "Iteration: 166 \t--- Loss: 0.015\n",
      "Iteration: 167 \t--- Loss: 0.015\n",
      "Iteration: 168 \t--- Loss: 0.014\n",
      "Iteration: 169 \t--- Loss: 0.014\n",
      "Iteration: 170 \t--- Loss: 0.014\n",
      "Iteration: 171 \t--- Loss: 0.014\n",
      "Iteration: 172 \t--- Loss: 0.014\n",
      "Iteration: 173 \t--- Loss: 0.016\n",
      "Iteration: 174 \t--- Loss: 0.014\n",
      "Iteration: 175 \t--- Loss: 0.014\n",
      "Iteration: 176 \t--- Loss: 0.016\n",
      "Iteration: 177 \t--- Loss: 0.016\n",
      "Iteration: 178 \t--- Loss: 0.013\n",
      "Iteration: 179 \t--- Loss: 0.014\n",
      "Iteration: 180 \t--- Loss: 0.015\n",
      "Iteration: 181 \t--- Loss: 0.016\n",
      "Iteration: 182 \t--- Loss: 0.014\n",
      "Iteration: 183 \t--- Loss: 0.013\n",
      "Iteration: 184 \t--- Loss: 0.014\n",
      "Iteration: 185 \t--- Loss: 0.013\n",
      "Iteration: 186 \t--- Loss: 0.015\n",
      "Iteration: 187 \t--- Loss: 0.013\n",
      "Iteration: 188 \t--- Loss: 0.013\n",
      "Iteration: 189 \t--- Loss: 0.015\n",
      "Iteration: 190 \t--- Loss: 0.014\n",
      "Iteration: 191 \t--- Loss: 0.014\n",
      "Iteration: 192 \t--- Loss: 0.014\n",
      "Iteration: 193 \t--- Loss: 0.014\n",
      "Iteration: 194 \t--- Loss: 0.015\n",
      "Iteration: 195 \t--- Loss: 0.015\n",
      "Iteration: 196 \t--- Loss: 0.014\n",
      "Iteration: 197 \t--- Loss: 0.014\n",
      "Iteration: 198 \t--- Loss: 0.014\n",
      "Iteration: 199 \t--- Loss: 0.015\n",
      "Iteration: 200 \t--- Loss: 0.015\n",
      "Iteration: 201 \t--- Loss: 0.015\n",
      "Iteration: 202 \t--- Loss: 0.014\n",
      "Iteration: 203 \t--- Loss: 0.014\n",
      "Iteration: 204 \t--- Loss: 0.015\n",
      "Iteration: 205 \t--- Loss: 0.014\n",
      "Iteration: 206 \t--- Loss: 0.015\n",
      "Iteration: 207 \t--- Loss: 0.015\n",
      "Iteration: 208 \t--- Loss: 0.014\n",
      "Iteration: 209 \t--- Loss: 0.015\n",
      "Iteration: 210 \t--- Loss: 0.014\n",
      "Iteration: 211 \t--- Loss: 0.015\n",
      "Iteration: 212 \t--- Loss: 0.015\n",
      "Iteration: 213 \t--- Loss: 0.014\n",
      "Iteration: 214 \t--- Loss: 0.014\n",
      "Iteration: 215 \t--- Loss: 0.013\n",
      "Iteration: 216 \t--- Loss: 0.014\n",
      "Iteration: 217 \t--- Loss: 0.014\n",
      "Iteration: 218 \t--- Loss: 0.014\n",
      "Iteration: 219 \t--- Loss: 0.015\n",
      "Iteration: 220 \t--- Loss: 0.014\n",
      "Iteration: 221 \t--- Loss: 0.015\n",
      "Iteration: 222 \t--- Loss: 0.014\n",
      "Iteration: 223 \t--- Loss: 0.014\n",
      "Iteration: 224 \t--- Loss: 0.014\n",
      "Iteration: 225 \t--- Loss: 0.014\n",
      "Iteration: 226 \t--- Loss: 0.016\n",
      "Iteration: 227 \t--- Loss: 0.014\n",
      "Iteration: 228 \t--- Loss: 0.014\n",
      "Iteration: 229 \t--- Loss: 0.015\n",
      "Iteration: 230 \t--- Loss: 0.014\n",
      "Iteration: 231 \t--- Loss: 0.013\n",
      "Iteration: 232 \t--- Loss: 0.013\n",
      "Iteration: 233 \t--- Loss: 0.013\n",
      "Iteration: 234 \t--- Loss: 0.013\n",
      "Iteration: 235 \t--- Loss: 0.015\n",
      "Iteration: 236 \t--- Loss: 0.015\n",
      "Iteration: 237 \t--- Loss: 0.014\n",
      "Iteration: 238 \t--- Loss: 0.014\n",
      "Iteration: 239 \t--- Loss: 0.015\n",
      "Iteration: 240 \t--- Loss: 0.014\n",
      "Iteration: 241 \t--- Loss: 0.015\n",
      "Iteration: 242 \t--- Loss: 0.016\n",
      "Iteration: 243 \t--- Loss: 0.014\n",
      "Iteration: 244 \t--- Loss: 0.013\n",
      "Iteration: 245 \t--- Loss: 0.014\n",
      "Iteration: 246 \t--- Loss: 0.014\n",
      "Iteration: 247 \t--- Loss: 0.015\n",
      "Iteration: 248 \t--- Loss: 0.014\n",
      "Iteration: 249 \t--- Loss: 0.014\n",
      "Iteration: 250 \t--- Loss: 0.014\n",
      "Iteration: 251 \t--- Loss: 0.014\n",
      "Iteration: 252 \t--- Loss: 0.013\n",
      "Iteration: 253 \t--- Loss: 0.014\n",
      "Iteration: 254 \t--- Loss: 0.016\n",
      "Iteration: 255 \t--- Loss: 0.014\n",
      "Iteration: 256 \t--- Loss: 0.015\n",
      "Iteration: 257 \t--- Loss: 0.013\n",
      "Iteration: 258 \t--- Loss: 0.015\n",
      "Iteration: 259 \t--- Loss: 0.014"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:36<00:00, 96.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.108\n",
      "Iteration: 261 \t--- Loss: 0.115\n",
      "Iteration: 262 \t--- Loss: 0.126\n",
      "Iteration: 263 \t--- Loss: 0.115\n",
      "Iteration: 264 \t--- Loss: 0.119\n",
      "Iteration: 265 \t--- Loss: 0.118\n",
      "Iteration: 266 \t--- Loss: 0.129\n",
      "Iteration: 267 \t--- Loss: 0.116\n",
      "Iteration: 268 \t--- Loss: 0.126\n",
      "Iteration: 269 \t--- Loss: 0.117\n",
      "Iteration: 270 \t--- Loss: 0.109\n",
      "Iteration: 271 \t--- Loss: 0.106\n",
      "Iteration: 272 \t--- Loss: 0.117\n",
      "Iteration: 273 \t--- Loss: 0.128\n",
      "Iteration: 274 \t--- Loss: 0.121\n",
      "Iteration: 275 \t--- Loss: 0.129\n",
      "Iteration: 276 \t--- Loss: 0.115\n",
      "Iteration: 277 \t--- Loss: 0.118\n",
      "Iteration: 278 \t--- Loss: 0.118\n",
      "Iteration: 279 \t--- Loss: 0.109\n",
      "Iteration: 280 \t--- Loss: 0.134\n",
      "Iteration: 281 \t--- Loss: 0.121\n",
      "Iteration: 282 \t--- Loss: 0.116\n",
      "Iteration: 283 \t--- Loss: 0.109\n",
      "Iteration: 284 \t--- Loss: 0.116\n",
      "Iteration: 285 \t--- Loss: 0.116\n",
      "Iteration: 286 \t--- Loss: 0.135\n",
      "Iteration: 287 \t--- Loss: 0.114\n",
      "Iteration: 288 \t--- Loss: 0.122\n",
      "Iteration: 289 \t--- Loss: 0.131\n",
      "Iteration: 290 \t--- Loss: 0.120\n",
      "Iteration: 291 \t--- Loss: 0.113\n",
      "Iteration: 292 \t--- Loss: 0.114\n",
      "Iteration: 293 \t--- Loss: 0.116\n",
      "Iteration: 294 \t--- Loss: 0.113\n",
      "Iteration: 295 \t--- Loss: 0.122\n",
      "Iteration: 296 \t--- Loss: 0.114\n",
      "Iteration: 297 \t--- Loss: 0.108\n",
      "Iteration: 298 \t--- Loss: 0.118\n",
      "Iteration: 299 \t--- Loss: 0.113\n",
      "Iteration: 300 \t--- Loss: 0.116\n",
      "Iteration: 301 \t--- Loss: 0.121\n",
      "Iteration: 302 \t--- Loss: 0.110\n",
      "Iteration: 303 \t--- Loss: 0.118\n",
      "Iteration: 304 \t--- Loss: 0.111\n",
      "Iteration: 305 \t--- Loss: 0.120\n",
      "Iteration: 306 \t--- Loss: 0.118\n",
      "Iteration: 307 \t--- Loss: 0.127\n",
      "Iteration: 308 \t--- Loss: 0.119\n",
      "Iteration: 309 \t--- Loss: 0.122\n",
      "Iteration: 310 \t--- Loss: 0.125\n",
      "Iteration: 311 \t--- Loss: 0.126\n",
      "Iteration: 312 \t--- Loss: 0.119\n",
      "Iteration: 313 \t--- Loss: 0.125\n",
      "Iteration: 314 \t--- Loss: 0.114\n",
      "Iteration: 315 \t--- Loss: 0.121\n",
      "Iteration: 316 \t--- Loss: 0.125\n",
      "Iteration: 317 \t--- Loss: 0.131\n",
      "Iteration: 318 \t--- Loss: 0.106\n",
      "Iteration: 319 \t--- Loss: 0.120\n",
      "Iteration: 320 \t--- Loss: 0.122\n",
      "Iteration: 321 \t--- Loss: 0.127\n",
      "Iteration: 322 \t--- Loss: 0.122\n",
      "Iteration: 323 \t--- Loss: 0.118\n",
      "Iteration: 324 \t--- Loss: 0.121\n",
      "Iteration: 325 \t--- Loss: 0.120\n",
      "Iteration: 326 \t--- Loss: 0.125\n",
      "Iteration: 327 \t--- Loss: 0.114\n",
      "Iteration: 328 \t--- Loss: 0.124\n",
      "Iteration: 329 \t--- Loss: 0.135\n",
      "Iteration: 330 \t--- Loss: 0.129\n",
      "Iteration: 331 \t--- Loss: 0.120\n",
      "Iteration: 332 \t--- Loss: 0.128\n",
      "Iteration: 333 \t--- Loss: 0.120\n",
      "Iteration: 334 \t--- Loss: 0.125\n",
      "Iteration: 335 \t--- Loss: 0.123\n",
      "Iteration: 336 \t--- Loss: 0.118\n",
      "Iteration: 337 \t--- Loss: 0.128\n",
      "Iteration: 338 \t--- Loss: 0.116\n",
      "Iteration: 339 \t--- Loss: 0.120\n",
      "Iteration: 340 \t--- Loss: 0.118\n",
      "Iteration: 341 \t--- Loss: 0.121\n",
      "Iteration: 342 \t--- Loss: 0.116\n",
      "Iteration: 343 \t--- Loss: 0.132\n",
      "Iteration: 344 \t--- Loss: 0.110\n",
      "Iteration: 345 \t--- Loss: 0.115\n",
      "Iteration: 346 \t--- Loss: 0.116\n",
      "Iteration: 347 \t--- Loss: 0.128\n",
      "Iteration: 348 \t--- Loss: 0.126\n",
      "Iteration: 349 \t--- Loss: 0.120\n",
      "Iteration: 350 \t--- Loss: 0.111\n",
      "Iteration: 351 \t--- Loss: 0.142\n",
      "Iteration: 352 \t--- Loss: 0.112\n",
      "Iteration: 353 \t--- Loss: 0.125\n",
      "Iteration: 354 \t--- Loss: 0.124\n",
      "Iteration: 355 \t--- Loss: 0.126\n",
      "Iteration: 356 \t--- Loss: 0.120\n",
      "Iteration: 357 \t--- Loss: 0.118\n",
      "Iteration: 358 \t--- Loss: 0.126\n",
      "Iteration: 359 \t--- Loss: 0.120\n",
      "Iteration: 360 \t--- Loss: 0.112\n",
      "Iteration: 361 \t--- Loss: 0.127\n",
      "Iteration: 362 \t--- Loss: 0.126\n",
      "Iteration: 363 \t--- Loss: 0.121\n",
      "Iteration: 364 \t--- Loss: 0.128\n",
      "Iteration: 365 \t--- Loss: 0.136\n",
      "Iteration: 366 \t--- Loss: 0.110\n",
      "Iteration: 367 \t--- Loss: 0.114\n",
      "Iteration: 368 \t--- Loss: 0.129\n",
      "Iteration: 369 \t--- Loss: 0.128\n",
      "Iteration: 370 \t--- Loss: 0.108\n",
      "Iteration: 371 \t--- Loss: 0.108\n",
      "Iteration: 372 \t--- Loss: 0.121\n",
      "Iteration: 373 \t--- Loss: 0.123\n",
      "Iteration: 374 \t--- Loss: 0.119\n",
      "Iteration: 375 \t--- Loss: 0.121\n",
      "Iteration: 376 \t--- Loss: 0.122\n",
      "Iteration: 377 \t--- Loss: 0.123\n",
      "Iteration: 378 \t--- Loss: 0.128\n",
      "Iteration: 379 \t--- Loss: 0.126\n",
      "Iteration: 380 \t--- Loss: 0.120\n",
      "Iteration: 381 \t--- Loss: 0.127\n",
      "Iteration: 382 \t--- Loss: 0.117\n",
      "Iteration: 383 \t--- Loss: 0.120\n",
      "Iteration: 384 \t--- Loss: 0.125\n",
      "Iteration: 385 \t--- Loss: 0.130\n",
      "Iteration: 386 \t--- Loss: 0.114\n",
      "Iteration: 387 \t--- Loss: 0.130\n",
      "Iteration: 388 \t--- Loss: 0.119\n",
      "Iteration: 389 \t--- Loss: 0.128\n",
      "Iteration: 390 \t--- Loss: 0.128\n",
      "Iteration: 391 \t--- Loss: 0.138\n",
      "Iteration: 392 \t--- Loss: 0.117\n",
      "Iteration: 393 \t--- Loss: 0.123\n",
      "Iteration: 394 \t--- Loss: 0.116\n",
      "Iteration: 395 \t--- Loss: 0.125\n",
      "Iteration: 396 \t--- Loss: 0.116\n",
      "Iteration: 397 \t--- Loss: 0.133\n",
      "Iteration: 398 \t--- Loss: 0.122\n",
      "Iteration: 399 \t--- Loss: 0.129\n",
      "Iteration: 400 \t--- Loss: 0.114\n",
      "Iteration: 401 \t--- Loss: 0.118\n",
      "Iteration: 402 \t--- Loss: 0.124\n",
      "Iteration: 403 \t--- Loss: 0.120\n",
      "Iteration: 404 \t--- Loss: 0.112\n",
      "Iteration: 405 \t--- Loss: 0.124\n",
      "Iteration: 406 \t--- Loss: 0.123\n",
      "Iteration: 407 \t--- Loss: 0.124\n",
      "Iteration: 408 \t--- Loss: 0.118\n",
      "Iteration: 409 \t--- Loss: 0.117\n",
      "Iteration: 410 \t--- Loss: 0.118\n",
      "Iteration: 411 \t--- Loss: 0.133\n",
      "Iteration: 412 \t--- Loss: 0.132\n",
      "Iteration: 413 \t--- Loss: 0.123\n",
      "Iteration: 414 \t--- Loss: 0.117\n",
      "Iteration: 415 \t--- Loss: 0.122\n",
      "Iteration: 416 \t--- Loss: 0.121\n",
      "Iteration: 417 \t--- Loss: 0.123\n",
      "Iteration: 418 \t--- Loss: 0.119\n",
      "Iteration: 419 \t--- Loss: 0.113\n",
      "Iteration: 420 \t--- Loss: 0.114\n",
      "Iteration: 421 \t--- Loss: 0.107\n",
      "Iteration: 422 \t--- Loss: 0.121\n",
      "Iteration: 423 \t--- Loss: 0.123\n",
      "Iteration: 424 \t--- Loss: 0.118\n",
      "Iteration: 425 \t--- Loss: 0.125\n",
      "Iteration: 426 \t--- Loss: 0.120\n",
      "Iteration: 427 \t--- Loss: 0.131\n",
      "Iteration: 428 \t--- Loss: 0.114\n",
      "Iteration: 429 \t--- Loss: 0.114\n",
      "Iteration: 430 \t--- Loss: 0.118\n",
      "Iteration: 431 \t--- Loss: 0.112\n",
      "Iteration: 432 \t--- Loss: 0.125\n",
      "Iteration: 433 \t--- Loss: 0.124\n",
      "Iteration: 434 \t--- Loss: 0.132\n",
      "Iteration: 435 \t--- Loss: 0.132\n",
      "Iteration: 436 \t--- Loss: 0.115\n",
      "Iteration: 437 \t--- Loss: 0.115\n",
      "Iteration: 438 \t--- Loss: 0.117\n",
      "Iteration: 439 \t--- Loss: 0.111\n",
      "Iteration: 440 \t--- Loss: 0.107\n",
      "Iteration: 441 \t--- Loss: 0.126\n",
      "Iteration: 442 \t--- Loss: 0.110\n",
      "Iteration: 443 \t--- Loss: 0.109\n",
      "Iteration: 444 \t--- Loss: 0.133\n",
      "Iteration: 445 \t--- Loss: 0.121\n",
      "Iteration: 446 \t--- Loss: 0.129\n",
      "Iteration: 447 \t--- Loss: 0.117\n",
      "Iteration: 448 \t--- Loss: 0.124\n",
      "Iteration: 449 \t--- Loss: 0.118\n",
      "Iteration: 450 \t--- Loss: 0.118\n",
      "Iteration: 451 \t--- Loss: 0.118\n",
      "Iteration: 452 \t--- Loss: 0.116\n",
      "Iteration: 453 \t--- Loss: 0.131\n",
      "Iteration: 454 \t--- Loss: 0.114\n",
      "Iteration: 455 \t--- Loss: 0.123\n",
      "Iteration: 456 \t--- Loss: 0.123\n",
      "Iteration: 457 \t--- Loss: 0.113\n",
      "Iteration: 458 \t--- Loss: 0.127\n",
      "Iteration: 459 \t--- Loss: 0.121\n",
      "Iteration: 460 \t--- Loss: 0.116\n",
      "Iteration: 461 \t--- Loss: 0.118\n",
      "Iteration: 462 \t--- Loss: 0.116\n",
      "Iteration: 463 \t--- Loss: 0.115\n",
      "Iteration: 464 \t--- Loss: 0.121\n",
      "Iteration: 465 \t--- Loss: 0.122\n",
      "Iteration: 466 \t--- Loss: 0.124\n",
      "Iteration: 467 \t--- Loss: 0.121\n",
      "Iteration: 468 \t--- Loss: 0.118\n",
      "Iteration: 469 \t--- Loss: 0.124\n",
      "Iteration: 470 \t--- Loss: 0.124\n",
      "Iteration: 471 \t--- Loss: 0.129\n",
      "Iteration: 472 \t--- Loss: 0.116\n",
      "Iteration: 473 \t--- Loss: 0.119\n",
      "Iteration: 474 \t--- Loss: 0.116\n",
      "Iteration: 475 \t--- Loss: 0.124\n",
      "Iteration: 476 \t--- Loss: 0.127\n",
      "Iteration: 477 \t--- Loss: 0.124\n",
      "Iteration: 478 \t--- Loss: 0.118\n",
      "Iteration: 479 \t--- Loss: 0.132\n",
      "Iteration: 480 \t--- Loss: 0.121\n",
      "Iteration: 481 \t--- Loss: 0.115\n",
      "Iteration: 482 \t--- Loss: 0.117\n",
      "Iteration: 483 \t--- Loss: 0.126\n",
      "Iteration: 484 \t--- Loss: 0.126\n",
      "Iteration: 485 \t--- Loss: 0.118\n",
      "Iteration: 486 \t--- Loss: 0.117\n",
      "Iteration: 487 \t--- Loss: 0.117\n",
      "Iteration: 488 \t--- Loss: 0.132\n",
      "Iteration: 489 \t--- Loss: 0.113\n",
      "Iteration: 490 \t--- Loss: 0.125\n",
      "Iteration: 491 \t--- Loss: 0.112\n",
      "Iteration: 492 \t--- Loss: 0.127\n",
      "Iteration: 493 \t--- Loss: 0.135\n",
      "Iteration: 494 \t--- Loss: 0.110\n",
      "Iteration: 495 \t--- Loss: 0.110\n",
      "Iteration: 496 \t--- Loss: 0.121\n",
      "Iteration: 497 \t--- Loss: 0.110\n",
      "Iteration: 498 \t--- Loss: 0.121\n",
      "Iteration: 499 \t--- Loss: 0.128\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:10,  1.45s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.267\n",
      "Iteration: 261 \t--- Loss: 0.268\n",
      "Iteration: 262 \t--- Loss: 0.262\n",
      "Iteration: 263 \t--- Loss: 0.253\n",
      "Iteration: 264 \t--- Loss: 0.258\n",
      "Iteration: 265 \t--- Loss: 0.269\n",
      "Iteration: 266 \t--- Loss: 0.244\n",
      "Iteration: 267 \t--- Loss: 0.267\n",
      "Iteration: 268 \t--- Loss: 0.236\n",
      "Iteration: 269 \t--- Loss: 0.285\n",
      "Iteration: 270 \t--- Loss: 0.266\n",
      "Iteration: 271 \t--- Loss: 0.247\n",
      "Iteration: 272 \t--- Loss: 0.275\n",
      "Iteration: 273 \t--- Loss: 0.255\n",
      "Iteration: 274 \t--- Loss: 0.241\n",
      "Iteration: 275 \t--- Loss: 0.262\n",
      "Iteration: 276 \t--- Loss: 0.264\n",
      "Iteration: 277 \t--- Loss: 0.277\n",
      "Iteration: 278 \t--- Loss: 0.268\n",
      "Iteration: 279 \t--- Loss: 0.244\n",
      "Iteration: 280 \t--- Loss: 0.281\n",
      "Iteration: 281 \t--- Loss: 0.266\n",
      "Iteration: 282 \t--- Loss: 0.250\n",
      "Iteration: 283 \t--- Loss: 0.253\n",
      "Iteration: 284 \t--- Loss: 0.264\n",
      "Iteration: 285 \t--- Loss: 0.254\n",
      "Iteration: 286 \t--- Loss: 0.271\n",
      "Iteration: 287 \t--- Loss: 0.270\n",
      "Iteration: 288 \t--- Loss: 0.256\n",
      "Iteration: 289 \t--- Loss: 0.275\n",
      "Iteration: 290 \t--- Loss: 0.250\n",
      "Iteration: 291 \t--- Loss: 0.264\n",
      "Iteration: 292 \t--- Loss: 0.262\n",
      "Iteration: 293 \t--- Loss: 0.255\n",
      "Iteration: 294 \t--- Loss: 0.260\n",
      "Iteration: 295 \t--- Loss: 0.265\n",
      "Iteration: 296 \t--- Loss: 0.260\n",
      "Iteration: 297 \t--- Loss: 0.253\n",
      "Iteration: 298 \t--- Loss: 0.252\n",
      "Iteration: 299 \t--- Loss: 0.264\n",
      "Iteration: 300 \t--- Loss: 0.266\n",
      "Iteration: 301 \t--- Loss: 0.268\n",
      "Iteration: 302 \t--- Loss: 0.255\n",
      "Iteration: 303 \t--- Loss: 0.256\n",
      "Iteration: 304 \t--- Loss: 0.259\n",
      "Iteration: 305 \t--- Loss: 0.266\n",
      "Iteration: 306 \t--- Loss: 0.264\n",
      "Iteration: 307 \t--- Loss: 0.262\n",
      "Iteration: 308 \t--- Loss: 0.244\n",
      "Iteration: 309 \t--- Loss: 0.242\n",
      "Iteration: 310 \t--- Loss: 0.256\n",
      "Iteration: 311 \t--- Loss: 0.278\n",
      "Iteration: 312 \t--- Loss: 0.255\n",
      "Iteration: 313 \t--- Loss: 0.270\n",
      "Iteration: 314 \t--- Loss: 0.262\n",
      "Iteration: 315 \t--- Loss: 0.254\n",
      "Iteration: 316 \t--- Loss: 0.251\n",
      "Iteration: 317 \t--- Loss: 0.246\n",
      "Iteration: 318 \t--- Loss: 0.250\n",
      "Iteration: 319 \t--- Loss: 0.259\n",
      "Iteration: 320 \t--- Loss: 0.252\n",
      "Iteration: 321 \t--- Loss: 0.256\n",
      "Iteration: 322 \t--- Loss: 0.262\n",
      "Iteration: 323 \t--- Loss: 0.258\n",
      "Iteration: 324 \t--- Loss: 0.274\n",
      "Iteration: 325 \t--- Loss: 0.252\n",
      "Iteration: 326 \t--- Loss: 0.272\n",
      "Iteration: 327 \t--- Loss: 0.237\n",
      "Iteration: 328 \t--- Loss: 0.270\n",
      "Iteration: 329 \t--- Loss: 0.246\n",
      "Iteration: 330 \t--- Loss: 0.290\n",
      "Iteration: 331 \t--- Loss: 0.259\n",
      "Iteration: 332 \t--- Loss: 0.282\n",
      "Iteration: 333 \t--- Loss: 0.273\n",
      "Iteration: 334 \t--- Loss: 0.244\n",
      "Iteration: 335 \t--- Loss: 0.253\n",
      "Iteration: 336 \t--- Loss: 0.254\n",
      "Iteration: 337 \t--- Loss: 0.265\n",
      "Iteration: 338 \t--- Loss: 0.278\n",
      "Iteration: 339 \t--- Loss: 0.237\n",
      "Iteration: 340 \t--- Loss: 0.268\n",
      "Iteration: 341 \t--- Loss: 0.265\n",
      "Iteration: 342 \t--- Loss: 0.247\n",
      "Iteration: 343 \t--- Loss: 0.265\n",
      "Iteration: 344 \t--- Loss: 0.249\n",
      "Iteration: 345 \t--- Loss: 0.253\n",
      "Iteration: 346 \t--- Loss: 0.254\n",
      "Iteration: 347 \t--- Loss: 0.251\n",
      "Iteration: 348 \t--- Loss: 0.261\n",
      "Iteration: 349 \t--- Loss: 0.263\n",
      "Iteration: 350 \t--- Loss: 0.259\n",
      "Iteration: 351 \t--- Loss: 0.248\n",
      "Iteration: 352 \t--- Loss: 0.239\n",
      "Iteration: 353 \t--- Loss: 0.264\n",
      "Iteration: 354 \t--- Loss: 0.254\n",
      "Iteration: 355 \t--- Loss: 0.273\n",
      "Iteration: 356 \t--- Loss: 0.253\n",
      "Iteration: 357 \t--- Loss: 0.257\n",
      "Iteration: 358 \t--- Loss: 0.239\n",
      "Iteration: 359 \t--- Loss: 0.232\n",
      "Iteration: 360 \t--- Loss: 0.237\n",
      "Iteration: 361 \t--- Loss: 0.264\n",
      "Iteration: 362 \t--- Loss: 0.255\n",
      "Iteration: 363 \t--- Loss: 0.261\n",
      "Iteration: 364 \t--- Loss: 0.264\n",
      "Iteration: 365 \t--- Loss: 0.265\n",
      "Iteration: 366 \t--- Loss: 0.247\n",
      "Iteration: 367 \t--- Loss: 0.272\n",
      "Iteration: 368 \t--- Loss: 0.264\n",
      "Iteration: 369 \t--- Loss: 0.255\n",
      "Iteration: 370 \t--- Loss: 0.259\n",
      "Iteration: 371 \t--- Loss: 0.243\n",
      "Iteration: 372 \t--- Loss: 0.263\n",
      "Iteration: 373 \t--- Loss: 0.265\n",
      "Iteration: 374 \t--- Loss: 0.260\n",
      "Iteration: 375 \t--- Loss: 0.262\n",
      "Iteration: 376 \t--- Loss: 0.251\n",
      "Iteration: 377 \t--- Loss: 0.230\n",
      "Iteration: 378 \t--- Loss: 0.261\n",
      "Iteration: 379 \t--- Loss: 0.263\n",
      "Iteration: 380 \t--- Loss: 0.247\n",
      "Iteration: 381 \t--- Loss: 0.268\n",
      "Iteration: 382 \t--- Loss: 0.264\n",
      "Iteration: 383 \t--- Loss: 0.265\n",
      "Iteration: 384 \t--- Loss: 0.260\n",
      "Iteration: 385 \t--- Loss: 0.254\n",
      "Iteration: 386 \t--- Loss: 0.262\n",
      "Iteration: 387 \t--- Loss: 0.284\n",
      "Iteration: 388 \t--- Loss: 0.259\n",
      "Iteration: 389 \t--- Loss: 0.255\n",
      "Iteration: 390 \t--- Loss: 0.263\n",
      "Iteration: 391 \t--- Loss: 0.275\n",
      "Iteration: 392 \t--- Loss: 0.269\n",
      "Iteration: 393 \t--- Loss: 0.252\n",
      "Iteration: 394 \t--- Loss: 0.274\n",
      "Iteration: 395 \t--- Loss: 0.266\n",
      "Iteration: 396 \t--- Loss: 0.267\n",
      "Iteration: 397 \t--- Loss: 0.264\n",
      "Iteration: 398 \t--- Loss: 0.242\n",
      "Iteration: 399 \t--- Loss: 0.246\n",
      "Iteration: 400 \t--- Loss: 0.262\n",
      "Iteration: 401 \t--- Loss: 0.276\n",
      "Iteration: 402 \t--- Loss: 0.248\n",
      "Iteration: 403 \t--- Loss: 0.259\n",
      "Iteration: 404 \t--- Loss: 0.262\n",
      "Iteration: 405 \t--- Loss: 0.273\n",
      "Iteration: 406 \t--- Loss: 0.262\n",
      "Iteration: 407 \t--- Loss: 0.254\n",
      "Iteration: 408 \t--- Loss: 0.243\n",
      "Iteration: 409 \t--- Loss: 0.269\n",
      "Iteration: 410 \t--- Loss: 0.264\n",
      "Iteration: 411 \t--- Loss: 0.250\n",
      "Iteration: 412 \t--- Loss: 0.254\n",
      "Iteration: 413 \t--- Loss: 0.254\n",
      "Iteration: 414 \t--- Loss: 0.255\n",
      "Iteration: 415 \t--- Loss: 0.256\n",
      "Iteration: 416 \t--- Loss: 0.242\n",
      "Iteration: 417 \t--- Loss: 0.260\n",
      "Iteration: 418 \t--- Loss: 0.250\n",
      "Iteration: 419 \t--- Loss: 0.244\n",
      "Iteration: 420 \t--- Loss: 0.243\n",
      "Iteration: 421 \t--- Loss: 0.255\n",
      "Iteration: 422 \t--- Loss: 0.272\n",
      "Iteration: 423 \t--- Loss: 0.248\n",
      "Iteration: 424 \t--- Loss: 0.249\n",
      "Iteration: 425 \t--- Loss: 0.248\n",
      "Iteration: 426 \t--- Loss: 0.265\n",
      "Iteration: 427 \t--- Loss: 0.245\n",
      "Iteration: 428 \t--- Loss: 0.255\n",
      "Iteration: 429 \t--- Loss: 0.253\n",
      "Iteration: 430 \t--- Loss: 0.267\n",
      "Iteration: 431 \t--- Loss: 0.268\n",
      "Iteration: 432 \t--- Loss: 0.248\n",
      "Iteration: 433 \t--- Loss: 0.252\n",
      "Iteration: 434 \t--- Loss: 0.259\n",
      "Iteration: 435 \t--- Loss: 0.246\n",
      "Iteration: 436 \t--- Loss: 0.292\n",
      "Iteration: 437 \t--- Loss: 0.254\n",
      "Iteration: 438 \t--- Loss: 0.259\n",
      "Iteration: 439 \t--- Loss: 0.257\n",
      "Iteration: 440 \t--- Loss: 0.257\n",
      "Iteration: 441 \t--- Loss: 0.261\n",
      "Iteration: 442 \t--- Loss: 0.260\n",
      "Iteration: 443 \t--- Loss: 0.254\n",
      "Iteration: 444 \t--- Loss: 0.248\n",
      "Iteration: 445 \t--- Loss: 0.261\n",
      "Iteration: 446 \t--- Loss: 0.238\n",
      "Iteration: 447 \t--- Loss: 0.265\n",
      "Iteration: 448 \t--- Loss: 0.252\n",
      "Iteration: 449 \t--- Loss: 0.261\n",
      "Iteration: 450 \t--- Loss: 0.262\n",
      "Iteration: 451 \t--- Loss: 0.270\n",
      "Iteration: 452 \t--- Loss: 0.249\n",
      "Iteration: 453 \t--- Loss: 0.262\n",
      "Iteration: 454 \t--- Loss: 0.256\n",
      "Iteration: 455 \t--- Loss: 0.255\n",
      "Iteration: 456 \t--- Loss: 0.282\n",
      "Iteration: 457 \t--- Loss: 0.266\n",
      "Iteration: 458 \t--- Loss: 0.241\n",
      "Iteration: 459 \t--- Loss: 0.257\n",
      "Iteration: 460 \t--- Loss: 0.248\n",
      "Iteration: 461 \t--- Loss: 0.263\n",
      "Iteration: 462 \t--- Loss: 0.268\n",
      "Iteration: 463 \t--- Loss: 0.262\n",
      "Iteration: 464 \t--- Loss: 0.259\n",
      "Iteration: 465 \t--- Loss: 0.283\n",
      "Iteration: 466 \t--- Loss: 0.255\n",
      "Iteration: 467 \t--- Loss: 0.254\n",
      "Iteration: 468 \t--- Loss: 0.258\n",
      "Iteration: 469 \t--- Loss: 0.251\n",
      "Iteration: 470 \t--- Loss: 0.243\n",
      "Iteration: 471 \t--- Loss: 0.255\n",
      "Iteration: 472 \t--- Loss: 0.257\n",
      "Iteration: 473 \t--- Loss: 0.264\n",
      "Iteration: 474 \t--- Loss: 0.245\n",
      "Iteration: 475 \t--- Loss: 0.263\n",
      "Iteration: 476 \t--- Loss: 0.282\n",
      "Iteration: 477 \t--- Loss: 0.261\n",
      "Iteration: 478 \t--- Loss: 0.248\n",
      "Iteration: 479 \t--- Loss: 0.240\n",
      "Iteration: 480 \t--- Loss: 0.268\n",
      "Iteration: 481 \t--- Loss: 0.240\n",
      "Iteration: 482 \t--- Loss: 0.255\n",
      "Iteration: 483 \t--- Loss: 0.265\n",
      "Iteration: 484 \t--- Loss: 0.248\n",
      "Iteration: 485 \t--- Loss: 0.269\n",
      "Iteration: 486 \t--- Loss: 0.251\n",
      "Iteration: 487 \t--- Loss: 0.261\n",
      "Iteration: 488 \t--- Loss: 0.257\n",
      "Iteration: 489 \t--- Loss: 0.254\n",
      "Iteration: 490 \t--- Loss: 0.268\n",
      "Iteration: 491 \t--- Loss: 0.257\n",
      "Iteration: 492 \t--- Loss: 0.251\n",
      "Iteration: 493 \t--- Loss: 0.243\n",
      "Iteration: 494 \t--- Loss: 0.264\n",
      "Iteration: 495 \t--- Loss: 0.253\n",
      "Iteration: 496 \t--- Loss: 0.251\n",
      "Iteration: 497 \t--- Loss: 0.267\n",
      "Iteration: 498 \t--- Loss: 0.270\n",
      "Iteration: 499 \t--- Loss: 0.252\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/it][Parallel(n_jobs=5)]: Done  48 tasks      | elapsed: 29.0min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.53s/it][Parallel(n_jobs=5)]: Done  49 tasks      | elapsed: 29.1min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:28<00:00, 88.96s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.014\n",
      "Iteration: 261 \t--- Loss: 0.015\n",
      "Iteration: 262 \t--- Loss: 0.013\n",
      "Iteration: 263 \t--- Loss: 0.014\n",
      "Iteration: 264 \t--- Loss: 0.014\n",
      "Iteration: 265 \t--- Loss: 0.015\n",
      "Iteration: 266 \t--- Loss: 0.014\n",
      "Iteration: 267 \t--- Loss: 0.015\n",
      "Iteration: 268 \t--- Loss: 0.014\n",
      "Iteration: 269 \t--- Loss: 0.015\n",
      "Iteration: 270 \t--- Loss: 0.013\n",
      "Iteration: 271 \t--- Loss: 0.013\n",
      "Iteration: 272 \t--- Loss: 0.013\n",
      "Iteration: 273 \t--- Loss: 0.015\n",
      "Iteration: 274 \t--- Loss: 0.013\n",
      "Iteration: 275 \t--- Loss: 0.013\n",
      "Iteration: 276 \t--- Loss: 0.014\n",
      "Iteration: 277 \t--- Loss: 0.013\n",
      "Iteration: 278 \t--- Loss: 0.015\n",
      "Iteration: 279 \t--- Loss: 0.014\n",
      "Iteration: 280 \t--- Loss: 0.014\n",
      "Iteration: 281 \t--- Loss: 0.014\n",
      "Iteration: 282 \t--- Loss: 0.014\n",
      "Iteration: 283 \t--- Loss: 0.015\n",
      "Iteration: 284 \t--- Loss: 0.013\n",
      "Iteration: 285 \t--- Loss: 0.013\n",
      "Iteration: 286 \t--- Loss: 0.014\n",
      "Iteration: 287 \t--- Loss: 0.015\n",
      "Iteration: 288 \t--- Loss: 0.014\n",
      "Iteration: 289 \t--- Loss: 0.013\n",
      "Iteration: 290 \t--- Loss: 0.014\n",
      "Iteration: 291 \t--- Loss: 0.014\n",
      "Iteration: 292 \t--- Loss: 0.014\n",
      "Iteration: 293 \t--- Loss: 0.013\n",
      "Iteration: 294 \t--- Loss: 0.014\n",
      "Iteration: 295 \t--- Loss: 0.014\n",
      "Iteration: 296 \t--- Loss: 0.014\n",
      "Iteration: 297 \t--- Loss: 0.014\n",
      "Iteration: 298 \t--- Loss: 0.013\n",
      "Iteration: 299 \t--- Loss: 0.014\n",
      "Iteration: 300 \t--- Loss: 0.014\n",
      "Iteration: 301 \t--- Loss: 0.013\n",
      "Iteration: 302 \t--- Loss: 0.014\n",
      "Iteration: 303 \t--- Loss: 0.013\n",
      "Iteration: 304 \t--- Loss: 0.013\n",
      "Iteration: 305 \t--- Loss: 0.014\n",
      "Iteration: 306 \t--- Loss: 0.015\n",
      "Iteration: 307 \t--- Loss: 0.014\n",
      "Iteration: 308 \t--- Loss: 0.016\n",
      "Iteration: 309 \t--- Loss: 0.014\n",
      "Iteration: 310 \t--- Loss: 0.014\n",
      "Iteration: 311 \t--- Loss: 0.012\n",
      "Iteration: 312 \t--- Loss: 0.014\n",
      "Iteration: 313 \t--- Loss: 0.014\n",
      "Iteration: 314 \t--- Loss: 0.014\n",
      "Iteration: 315 \t--- Loss: 0.014\n",
      "Iteration: 316 \t--- Loss: 0.013\n",
      "Iteration: 317 \t--- Loss: 0.015\n",
      "Iteration: 318 \t--- Loss: 0.014\n",
      "Iteration: 319 \t--- Loss: 0.013\n",
      "Iteration: 320 \t--- Loss: 0.014\n",
      "Iteration: 321 \t--- Loss: 0.014\n",
      "Iteration: 322 \t--- Loss: 0.014\n",
      "Iteration: 323 \t--- Loss: 0.014\n",
      "Iteration: 324 \t--- Loss: 0.016\n",
      "Iteration: 325 \t--- Loss: 0.014\n",
      "Iteration: 326 \t--- Loss: 0.014\n",
      "Iteration: 327 \t--- Loss: 0.014\n",
      "Iteration: 328 \t--- Loss: 0.016\n",
      "Iteration: 329 \t--- Loss: 0.014\n",
      "Iteration: 330 \t--- Loss: 0.013\n",
      "Iteration: 331 \t--- Loss: 0.013\n",
      "Iteration: 332 \t--- Loss: 0.014\n",
      "Iteration: 333 \t--- Loss: 0.014\n",
      "Iteration: 334 \t--- Loss: 0.013\n",
      "Iteration: 335 \t--- Loss: 0.014\n",
      "Iteration: 336 \t--- Loss: 0.013\n",
      "Iteration: 337 \t--- Loss: 0.014\n",
      "Iteration: 338 \t--- Loss: 0.014\n",
      "Iteration: 339 \t--- Loss: 0.013\n",
      "Iteration: 340 \t--- Loss: 0.014\n",
      "Iteration: 341 \t--- Loss: 0.015\n",
      "Iteration: 342 \t--- Loss: 0.012\n",
      "Iteration: 343 \t--- Loss: 0.013\n",
      "Iteration: 344 \t--- Loss: 0.016\n",
      "Iteration: 345 \t--- Loss: 0.013\n",
      "Iteration: 346 \t--- Loss: 0.014\n",
      "Iteration: 347 \t--- Loss: 0.013\n",
      "Iteration: 348 \t--- Loss: 0.013\n",
      "Iteration: 349 \t--- Loss: 0.015\n",
      "Iteration: 350 \t--- Loss: 0.014\n",
      "Iteration: 351 \t--- Loss: 0.013\n",
      "Iteration: 352 \t--- Loss: 0.015\n",
      "Iteration: 353 \t--- Loss: 0.015\n",
      "Iteration: 354 \t--- Loss: 0.013\n",
      "Iteration: 355 \t--- Loss: 0.014\n",
      "Iteration: 356 \t--- Loss: 0.013\n",
      "Iteration: 357 \t--- Loss: 0.015\n",
      "Iteration: 358 \t--- Loss: 0.013\n",
      "Iteration: 359 \t--- Loss: 0.013\n",
      "Iteration: 360 \t--- Loss: 0.013\n",
      "Iteration: 361 \t--- Loss: 0.014\n",
      "Iteration: 362 \t--- Loss: 0.016\n",
      "Iteration: 363 \t--- Loss: 0.014\n",
      "Iteration: 364 \t--- Loss: 0.014\n",
      "Iteration: 365 \t--- Loss: 0.013\n",
      "Iteration: 366 \t--- Loss: 0.014\n",
      "Iteration: 367 \t--- Loss: 0.015\n",
      "Iteration: 368 \t--- Loss: 0.013\n",
      "Iteration: 369 \t--- Loss: 0.012\n",
      "Iteration: 370 \t--- Loss: 0.013\n",
      "Iteration: 371 \t--- Loss: 0.014\n",
      "Iteration: 372 \t--- Loss: 0.014\n",
      "Iteration: 373 \t--- Loss: 0.015\n",
      "Iteration: 374 \t--- Loss: 0.014\n",
      "Iteration: 375 \t--- Loss: 0.015\n",
      "Iteration: 376 \t--- Loss: 0.013\n",
      "Iteration: 377 \t--- Loss: 0.014\n",
      "Iteration: 378 \t--- Loss: 0.013\n",
      "Iteration: 379 \t--- Loss: 0.014\n",
      "Iteration: 380 \t--- Loss: 0.014\n",
      "Iteration: 381 \t--- Loss: 0.014\n",
      "Iteration: 382 \t--- Loss: 0.013\n",
      "Iteration: 383 \t--- Loss: 0.013\n",
      "Iteration: 384 \t--- Loss: 0.013\n",
      "Iteration: 385 \t--- Loss: 0.015\n",
      "Iteration: 386 \t--- Loss: 0.014\n",
      "Iteration: 387 \t--- Loss: 0.014\n",
      "Iteration: 388 \t--- Loss: 0.015\n",
      "Iteration: 389 \t--- Loss: 0.015\n",
      "Iteration: 390 \t--- Loss: 0.013\n",
      "Iteration: 391 \t--- Loss: 0.013\n",
      "Iteration: 392 \t--- Loss: 0.014\n",
      "Iteration: 393 \t--- Loss: 0.015\n",
      "Iteration: 394 \t--- Loss: 0.014\n",
      "Iteration: 395 \t--- Loss: 0.013\n",
      "Iteration: 396 \t--- Loss: 0.013\n",
      "Iteration: 397 \t--- Loss: 0.015\n",
      "Iteration: 398 \t--- Loss: 0.014\n",
      "Iteration: 399 \t--- Loss: 0.015\n",
      "Iteration: 400 \t--- Loss: 0.014\n",
      "Iteration: 401 \t--- Loss: 0.013\n",
      "Iteration: 402 \t--- Loss: 0.014\n",
      "Iteration: 403 \t--- Loss: 0.014\n",
      "Iteration: 404 \t--- Loss: 0.014\n",
      "Iteration: 405 \t--- Loss: 0.013\n",
      "Iteration: 406 \t--- Loss: 0.013\n",
      "Iteration: 407 \t--- Loss: 0.013\n",
      "Iteration: 408 \t--- Loss: 0.014\n",
      "Iteration: 409 \t--- Loss: 0.014\n",
      "Iteration: 410 \t--- Loss: 0.014\n",
      "Iteration: 411 \t--- Loss: 0.014\n",
      "Iteration: 412 \t--- Loss: 0.015\n",
      "Iteration: 413 \t--- Loss: 0.015\n",
      "Iteration: 414 \t--- Loss: 0.014\n",
      "Iteration: 415 \t--- Loss: 0.013\n",
      "Iteration: 416 \t--- Loss: 0.014\n",
      "Iteration: 417 \t--- Loss: 0.015\n",
      "Iteration: 418 \t--- Loss: 0.014\n",
      "Iteration: 419 \t--- Loss: 0.014\n",
      "Iteration: 420 \t--- Loss: 0.014\n",
      "Iteration: 421 \t--- Loss: 0.013\n",
      "Iteration: 422 \t--- Loss: 0.015\n",
      "Iteration: 423 \t--- Loss: 0.013\n",
      "Iteration: 424 \t--- Loss: 0.015\n",
      "Iteration: 425 \t--- Loss: 0.014\n",
      "Iteration: 426 \t--- Loss: 0.014\n",
      "Iteration: 427 \t--- Loss: 0.013\n",
      "Iteration: 428 \t--- Loss: 0.014\n",
      "Iteration: 429 \t--- Loss: 0.015\n",
      "Iteration: 430 \t--- Loss: 0.014\n",
      "Iteration: 431 \t--- Loss: 0.015\n",
      "Iteration: 432 \t--- Loss: 0.014\n",
      "Iteration: 433 \t--- Loss: 0.013\n",
      "Iteration: 434 \t--- Loss: 0.014\n",
      "Iteration: 435 \t--- Loss: 0.013\n",
      "Iteration: 436 \t--- Loss: 0.015\n",
      "Iteration: 437 \t--- Loss: 0.013\n",
      "Iteration: 438 \t--- Loss: 0.013\n",
      "Iteration: 439 \t--- Loss: 0.012\n",
      "Iteration: 440 \t--- Loss: 0.015\n",
      "Iteration: 441 \t--- Loss: 0.015\n",
      "Iteration: 442 \t--- Loss: 0.014\n",
      "Iteration: 443 \t--- Loss: 0.015\n",
      "Iteration: 444 \t--- Loss: 0.013\n",
      "Iteration: 445 \t--- Loss: 0.015\n",
      "Iteration: 446 \t--- Loss: 0.013\n",
      "Iteration: 447 \t--- Loss: 0.014\n",
      "Iteration: 448 \t--- Loss: 0.013\n",
      "Iteration: 449 \t--- Loss: 0.012\n",
      "Iteration: 450 \t--- Loss: 0.016\n",
      "Iteration: 451 \t--- Loss: 0.012\n",
      "Iteration: 452 \t--- Loss: 0.014\n",
      "Iteration: 453 \t--- Loss: 0.013\n",
      "Iteration: 454 \t--- Loss: 0.014\n",
      "Iteration: 455 \t--- Loss: 0.014\n",
      "Iteration: 456 \t--- Loss: 0.014\n",
      "Iteration: 457 \t--- Loss: 0.014\n",
      "Iteration: 458 \t--- Loss: 0.013\n",
      "Iteration: 459 \t--- Loss: 0.013\n",
      "Iteration: 460 \t--- Loss: 0.014\n",
      "Iteration: 461 \t--- Loss: 0.013\n",
      "Iteration: 462 \t--- Loss: 0.014\n",
      "Iteration: 463 \t--- Loss: 0.014\n",
      "Iteration: 464 \t--- Loss: 0.013\n",
      "Iteration: 465 \t--- Loss: 0.013\n",
      "Iteration: 466 \t--- Loss: 0.014\n",
      "Iteration: 467 \t--- Loss: 0.013\n",
      "Iteration: 468 \t--- Loss: 0.014\n",
      "Iteration: 469 \t--- Loss: 0.015\n",
      "Iteration: 470 \t--- Loss: 0.014\n",
      "Iteration: 471 \t--- Loss: 0.013\n",
      "Iteration: 472 \t--- Loss: 0.015\n",
      "Iteration: 473 \t--- Loss: 0.015\n",
      "Iteration: 474 \t--- Loss: 0.013\n",
      "Iteration: 475 \t--- Loss: 0.013\n",
      "Iteration: 476 \t--- Loss: 0.014\n",
      "Iteration: 477 \t--- Loss: 0.014\n",
      "Iteration: 478 \t--- Loss: 0.016\n",
      "Iteration: 479 \t--- Loss: 0.015\n",
      "Iteration: 480 \t--- Loss: 0.013\n",
      "Iteration: 481 \t--- Loss: 0.014\n",
      "Iteration: 482 \t--- Loss: 0.014\n",
      "Iteration: 483 \t--- Loss: 0.014\n",
      "Iteration: 484 \t--- Loss: 0.015\n",
      "Iteration: 485 \t--- Loss: 0.013\n",
      "Iteration: 486 \t--- Loss: 0.014\n",
      "Iteration: 487 \t--- Loss: 0.013\n",
      "Iteration: 488 \t--- Loss: 0.014\n",
      "Iteration: 489 \t--- Loss: 0.014\n",
      "Iteration: 490 \t--- Loss: 0.014\n",
      "Iteration: 491 \t--- Loss: 0.014\n",
      "Iteration: 492 \t--- Loss: 0.013\n",
      "Iteration: 493 \t--- Loss: 0.015\n",
      "Iteration: 494 \t--- Loss: 0.015\n",
      "Iteration: 495 \t--- Loss: 0.013\n",
      "Iteration: 496 \t--- Loss: 0.014\n",
      "Iteration: 497 \t--- Loss: 0.013\n",
      "Iteration: 498 \t--- Loss: 0.014\n",
      "Iteration: 499 \t--- Loss: 0.014\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it][Parallel(n_jobs=5)]: Done  50 tasks      | elapsed: 29.4min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 1.275\n",
      "Iteration: 1 \t--- Loss: 1.138\n",
      "Iteration: 2 \t--- Loss: 1.071\n",
      "Iteration: 3 \t--- Loss: 0.987\n",
      "Iteration: 4 \t--- Loss: 0.927\n",
      "Iteration: 5 \t--- Loss: 0.887\n",
      "Iteration: 6 \t--- Loss: 0.858\n",
      "Iteration: 7 \t--- Loss: 0.823\n",
      "Iteration: 8 \t--- Loss: 0.802\n",
      "Iteration: 9 \t--- Loss: 0.794\n",
      "Iteration: 10 \t--- Loss: 0.784\n",
      "Iteration: 11 \t--- Loss: 0.774\n",
      "Iteration: 12 \t--- Loss: 0.763\n",
      "Iteration: 13 \t--- Loss: 0.770\n",
      "Iteration: 14 \t--- Loss: 0.765\n",
      "Iteration: 15 \t--- Loss: 0.745\n",
      "Iteration: 16 \t--- Loss: 0.734\n",
      "Iteration: 17 \t--- Loss: 0.727\n",
      "Iteration: 18 \t--- Loss: 0.754\n",
      "Iteration: 19 \t--- Loss: 0.750\n",
      "Iteration: 20 \t--- Loss: 0.740\n",
      "Iteration: 21 \t--- Loss: 0.725\n",
      "Iteration: 22 \t--- Loss: 0.728\n",
      "Iteration: 23 \t--- Loss: 0.740\n",
      "Iteration: 24 \t--- Loss: 0.733\n",
      "Iteration: 25 \t--- Loss: 0.737\n",
      "Iteration: 26 \t--- Loss: 0.732\n",
      "Iteration: 27 \t--- Loss: 0.736\n",
      "Iteration: 28 \t--- Loss: 0.742\n",
      "Iteration: 29 \t--- Loss: 0.712\n",
      "Iteration: 30 \t--- Loss: 0.729\n",
      "Iteration: 31 \t--- Loss: 0.734\n",
      "Iteration: 32 \t--- Loss: 0.731\n",
      "Iteration: 33 \t--- Loss: 0.725\n",
      "Iteration: 34 \t--- Loss: 0.714\n",
      "Iteration: 35 \t--- Loss: 0.727\n",
      "Iteration: 36 \t--- Loss: 0.727\n",
      "Iteration: 37 \t--- Loss: 0.723\n",
      "Iteration: 38 \t--- Loss: 0.728\n",
      "Iteration: 39 \t--- Loss: 0.720\n",
      "Iteration: 40 \t--- Loss: 0.735\n",
      "Iteration: 41 \t--- Loss: 0.736\n",
      "Iteration: 42 \t--- Loss: 0.736\n",
      "Iteration: 43 \t--- Loss: 0.724\n",
      "Iteration: 44 \t--- Loss: 0.709\n",
      "Iteration: 45 \t--- Loss: 0.741\n",
      "Iteration: 46 \t--- Loss: 0.712\n",
      "Iteration: 47 \t--- Loss: 0.716\n",
      "Iteration: 48 \t--- Loss: 0.734\n",
      "Iteration: 49 \t--- Loss: 0.733\n",
      "Iteration: 50 \t--- Loss: 0.739\n",
      "Iteration: 51 \t--- Loss: 0.716\n",
      "Iteration: 52 \t--- Loss: 0.735\n",
      "Iteration: 53 \t--- Loss: 0.721\n",
      "Iteration: 54 \t--- Loss: 0.730\n",
      "Iteration: 55 \t--- Loss: 0.728\n",
      "Iteration: 56 \t--- Loss: 0.737\n",
      "Iteration: 57 \t--- Loss: 0.746\n",
      "Iteration: 58 \t--- Loss: 0.732\n",
      "Iteration: 59 \t--- Loss: 0.720\n",
      "Iteration: 60 \t--- Loss: 0.715\n",
      "Iteration: 61 \t--- Loss: 0.722\n",
      "Iteration: 62 \t--- Loss: 0.722\n",
      "Iteration: 63 \t--- Loss: 0.726\n",
      "Iteration: 64 \t--- Loss: 0.720\n",
      "Iteration: 65 \t--- Loss: 0.721\n",
      "Iteration: 66 \t--- Loss: 0.726\n",
      "Iteration: 67 \t--- Loss: 0.716\n",
      "Iteration: 68 \t--- Loss: 0.720\n",
      "Iteration: 69 \t--- Loss: 0.718\n",
      "Iteration: 70 \t--- Loss: 0.734\n",
      "Iteration: 71 \t--- Loss: 0.717\n",
      "Iteration: 72 \t--- Loss: 0.725\n",
      "Iteration: 73 \t--- Loss: 0.743\n",
      "Iteration: 74 \t--- Loss: 0.729\n",
      "Iteration: 75 \t--- Loss: 0.734\n",
      "Iteration: 76 \t--- Loss: 0.730\n",
      "Iteration: 77 \t--- Loss: 0.719\n",
      "Iteration: 78 \t--- Loss: 0.717\n",
      "Iteration: 79 \t--- Loss: 0.735\n",
      "Iteration: 80 \t--- Loss: 0.740\n",
      "Iteration: 81 \t--- Loss: 0.722\n",
      "Iteration: 82 \t--- Loss: 0.711\n",
      "Iteration: 83 \t--- Loss: 0.729\n",
      "Iteration: 84 \t--- Loss: 0.741\n",
      "Iteration: 85 \t--- Loss: 0.736\n",
      "Iteration: 86 \t--- Loss: 0.733\n",
      "Iteration: 87 \t--- Loss: 0.720\n",
      "Iteration: 88 \t--- Loss: 0.724\n",
      "Iteration: 89 \t--- Loss: 0.714\n",
      "Iteration: 90 \t--- Loss: 0.723\n",
      "Iteration: 91 \t--- Loss: 0.705\n",
      "Iteration: 92 \t--- Loss: 0.723\n",
      "Iteration: 93 \t--- Loss: 0.721\n",
      "Iteration: 94 \t--- Loss: 0.727\n",
      "Iteration: 95 \t--- Loss: 0.734\n",
      "Iteration: 96 \t--- Loss: 0.728\n",
      "Iteration: 97 \t--- Loss: 0.734\n",
      "Iteration: 98 \t--- Loss: 0.720\n",
      "Iteration: 99 \t--- Loss: 0.731\n",
      "Iteration: 100 \t--- Loss: 0.706\n",
      "Iteration: 101 \t--- Loss: 0.709\n",
      "Iteration: 102 \t--- Loss: 0.718\n",
      "Iteration: 103 \t--- Loss: 0.718\n",
      "Iteration: 104 \t--- Loss: 0.732\n",
      "Iteration: 105 \t--- Loss: 0.739\n",
      "Iteration: 106 \t--- Loss: 0.732\n",
      "Iteration: 107 \t--- Loss: 0.730\n",
      "Iteration: 108 \t--- Loss: 0.741\n",
      "Iteration: 109 \t--- Loss: 0.733\n",
      "Iteration: 110 \t--- Loss: 0.723\n",
      "Iteration: 111 \t--- Loss: 0.734\n",
      "Iteration: 112 \t--- Loss: 0.721\n",
      "Iteration: 113 \t--- Loss: 0.716\n",
      "Iteration: 114 \t--- Loss: 0.728\n",
      "Iteration: 115 \t--- Loss: 0.718\n",
      "Iteration: 116 \t--- Loss: 0.740\n",
      "Iteration: 117 \t--- Loss: 0.722\n",
      "Iteration: 118 \t--- Loss: 0.727\n",
      "Iteration: 119 \t--- Loss: 0.733\n",
      "Iteration: 120 \t--- Loss: 0.744\n",
      "Iteration: 121 \t--- Loss: 0.729\n",
      "Iteration: 122 \t--- Loss: 0.729\n",
      "Iteration: 123 \t--- Loss: 0.731\n",
      "Iteration: 124 \t--- Loss: 0.745\n",
      "Iteration: 125 \t--- Loss: 0.732\n",
      "Iteration: 126 \t--- Loss: 0.735\n",
      "Iteration: 127 \t--- Loss: 0.727\n",
      "Iteration: 128 \t--- Loss: 0.738\n",
      "Iteration: 129 \t--- Loss: 0.732\n",
      "Iteration: 130 \t--- Loss: 0.729\n",
      "Iteration: 131 \t--- Loss: 0.738\n",
      "Iteration: 132 \t--- Loss: 0.735\n",
      "Iteration: 133 \t--- Loss: 0.710\n",
      "Iteration: 134 \t--- Loss: 0.730\n",
      "Iteration: 135 \t--- Loss: 0.726\n",
      "Iteration: 136 \t--- Loss: 0.740\n",
      "Iteration: 137 \t--- Loss: 0.717\n",
      "Iteration: 138 \t--- Loss: 0.735\n",
      "Iteration: 139 \t--- Loss: 0.708\n",
      "Iteration: 140 \t--- Loss: 0.720\n",
      "Iteration: 141 \t--- Loss: 0.728\n",
      "Iteration: 142 \t--- Loss: 0.716\n",
      "Iteration: 143 \t--- Loss: 0.730\n",
      "Iteration: 144 \t--- Loss: 0.726\n",
      "Iteration: 145 \t--- Loss: 0.725\n",
      "Iteration: 146 \t--- Loss: 0.736\n",
      "Iteration: 147 \t--- Loss: 0.724\n",
      "Iteration: 148 \t--- Loss: 0.714\n",
      "Iteration: 149 \t--- Loss: 0.739\n",
      "Iteration: 150 \t--- Loss: 0.729\n",
      "Iteration: 151 \t--- Loss: 0.730\n",
      "Iteration: 152 \t--- Loss: 0.724\n",
      "Iteration: 153 \t--- Loss: 0.720\n",
      "Iteration: 154 \t--- Loss: 0.723\n",
      "Iteration: 155 \t--- Loss: 0.725\n",
      "Iteration: 156 \t--- Loss: 0.736\n",
      "Iteration: 157 \t--- Loss: 0.730\n",
      "Iteration: 158 \t--- Loss: 0.711\n",
      "Iteration: 159 \t--- Loss: 0.727\n",
      "Iteration: 160 \t--- Loss: 0.717\n",
      "Iteration: 161 \t--- Loss: 0.736\n",
      "Iteration: 162 \t--- Loss: 0.723\n",
      "Iteration: 163 \t--- Loss: 0.737\n",
      "Iteration: 164 \t--- Loss: 0.711\n",
      "Iteration: 165 \t--- Loss: 0.711\n",
      "Iteration: 166 \t--- Loss: 0.743\n",
      "Iteration: 167 \t--- Loss: 0.706\n",
      "Iteration: 168 \t--- Loss: 0.720\n",
      "Iteration: 169 \t--- Loss: 0.732\n",
      "Iteration: 170 \t--- Loss: 0.735\n",
      "Iteration: 171 \t--- Loss: 0.742\n",
      "Iteration: 172 \t--- Loss: 0.729\n",
      "Iteration: 173 \t--- Loss: 0.736\n",
      "Iteration: 174 \t--- Loss: 0.730\n",
      "Iteration: 175 \t--- Loss: 0.727\n",
      "Iteration: 176 \t--- Loss: 0.720\n",
      "Iteration: 177 \t--- Loss: 0.736\n",
      "Iteration: 178 \t--- Loss: 0.729\n",
      "Iteration: 179 \t--- Loss: 0.726\n",
      "Iteration: 180 \t--- Loss: 0.728\n",
      "Iteration: 181 \t--- Loss: 0.731\n",
      "Iteration: 182 \t--- Loss: 0.733\n",
      "Iteration: 183 \t--- Loss: 0.739\n",
      "Iteration: 184 \t--- Loss: 0.720\n",
      "Iteration: 185 \t--- Loss: 0.726\n",
      "Iteration: 186 \t--- Loss: 0.733\n",
      "Iteration: 187 \t--- Loss: 0.744\n",
      "Iteration: 188 \t--- Loss: 0.739\n",
      "Iteration: 189 \t--- Loss: 0.710\n",
      "Iteration: 190 \t--- Loss: 0.723\n",
      "Iteration: 191 \t--- Loss: 0.722\n",
      "Iteration: 192 \t--- Loss: 0.715\n",
      "Iteration: 193 \t--- Loss: 0.732\n",
      "Iteration: 194 \t--- Loss: 0.727\n",
      "Iteration: 195 \t--- Loss: 0.742\n",
      "Iteration: 196 \t--- Loss: 0.722\n",
      "Iteration: 197 \t--- Loss: 0.724\n",
      "Iteration: 198 \t--- Loss: 0.721\n",
      "Iteration: 199 \t--- Loss: 0.735\n",
      "Iteration: 200 \t--- Loss: 0.736\n",
      "Iteration: 201 \t--- Loss: 0.713\n",
      "Iteration: 202 \t--- Loss: 0.728\n",
      "Iteration: 203 \t--- Loss: 0.728\n",
      "Iteration: 204 \t--- Loss: 0.713\n",
      "Iteration: 205 \t--- Loss: 0.736\n",
      "Iteration: 206 \t--- Loss: 0.712\n",
      "Iteration: 207 \t--- Loss: 0.716\n",
      "Iteration: 208 \t--- Loss: 0.740\n",
      "Iteration: 209 \t--- Loss: 0.733\n",
      "Iteration: 210 \t--- Loss: 0.725\n",
      "Iteration: 211 \t--- Loss: 0.738\n",
      "Iteration: 212 \t--- Loss: 0.718\n",
      "Iteration: 213 \t--- Loss: 0.744\n",
      "Iteration: 214 \t--- Loss: 0.731\n",
      "Iteration: 215 \t--- Loss: 0.732\n",
      "Iteration: 216 \t--- Loss: 0.712\n",
      "Iteration: 217 \t--- Loss: 0.718\n",
      "Iteration: 218 \t--- Loss: 0.742\n",
      "Iteration: 219 \t--- Loss: 0.729\n",
      "Iteration: 220 \t--- Loss: 0.728\n",
      "Iteration: 221 \t--- Loss: 0.736\n",
      "Iteration: 222 \t--- Loss: 0.740\n",
      "Iteration: 223 \t--- Loss: 0.725\n",
      "Iteration: 224 \t--- Loss: 0.725\n",
      "Iteration: 225 \t--- Loss: 0.735\n",
      "Iteration: 226 \t--- Loss: 0.726\n",
      "Iteration: 227 \t--- Loss: 0.733\n",
      "Iteration: 228 \t--- Loss: 0.723\n",
      "Iteration: 229 \t--- Loss: 0.727\n",
      "Iteration: 230 \t--- Loss: 0.736\n",
      "Iteration: 231 \t--- Loss: 0.735\n",
      "Iteration: 232 \t--- Loss: 0.717\n",
      "Iteration: 233 \t--- Loss: 0.711\n",
      "Iteration: 234 \t--- Loss: 0.726\n",
      "Iteration: 235 \t--- Loss: 0.719\n",
      "Iteration: 236 \t--- Loss: 0.722\n",
      "Iteration: 237 \t--- Loss: 0.721\n",
      "Iteration: 238 \t--- Loss: 0.726\n",
      "Iteration: 239 \t--- Loss: 0.726\n",
      "Iteration: 240 \t--- Loss: 0.706\n",
      "Iteration: 241 \t--- Loss: 0.724\n",
      "Iteration: 242 \t--- Loss: 0.726\n",
      "Iteration: 243 \t--- Loss: 0.737\n",
      "Iteration: 244 \t--- Loss: 0.735\n",
      "Iteration: 245 \t--- Loss: 0.730\n",
      "Iteration: 246 \t--- Loss: 0.709\n",
      "Iteration: 247 \t--- Loss: 0.725\n",
      "Iteration: 248 \t--- Loss: 0.728\n",
      "Iteration: 249 \t--- Loss: 0.716\n",
      "Iteration: 250 \t--- Loss: 0.737\n",
      "Iteration: 251 \t--- Loss: 0.730\n",
      "Iteration: 252 \t--- Loss: 0.728\n",
      "Iteration: 253 \t--- Loss: 0.725\n",
      "Iteration: 254 \t--- Loss: 0.726\n",
      "Iteration: 255 \t--- Loss: 0.726\n",
      "Iteration: 256 \t--- Loss: 0.732\n",
      "Iteration: 257 \t--- Loss: 0.734\n",
      "Iteration: 258 \t--- Loss: 0.724\n",
      "Iteration: 259 \t--- Loss: 0.731Iteration: 0 \t--- Loss: 0.367\n",
      "Iteration: 1 \t--- Loss: 0.280\n",
      "Iteration: 2 \t--- Loss: 0.291\n",
      "Iteration: 3 \t--- Loss: 0.261\n",
      "Iteration: 4 \t--- Loss: 0.308\n",
      "Iteration: 5 \t--- Loss: 0.213\n",
      "Iteration: 6 \t--- Loss: 0.261\n",
      "Iteration: 7 \t--- Loss: 0.203\n",
      "Iteration: 8 \t--- Loss: 0.213\n",
      "Iteration: 9 \t--- Loss: 0.199\n",
      "Iteration: 10 \t--- Loss: 0.197\n",
      "Iteration: 11 \t--- Loss: 0.210\n",
      "Iteration: 12 \t--- Loss: 0.188\n",
      "Iteration: 13 \t--- Loss: 0.176\n",
      "Iteration: 14 \t--- Loss: 0.193\n",
      "Iteration: 15 \t--- Loss: 0.181\n",
      "Iteration: 16 \t--- Loss: 0.189\n",
      "Iteration: 17 \t--- Loss: 0.181\n",
      "Iteration: 18 \t--- Loss: 0.178\n",
      "Iteration: 19 \t--- Loss: 0.172\n",
      "Iteration: 20 \t--- Loss: 0.148\n",
      "Iteration: 21 \t--- Loss: 0.160\n",
      "Iteration: 22 \t--- Loss: 0.164\n",
      "Iteration: 23 \t--- Loss: 0.168\n",
      "Iteration: 24 \t--- Loss: 0.186\n",
      "Iteration: 25 \t--- Loss: 0.125\n",
      "Iteration: 26 \t--- Loss: 0.177\n",
      "Iteration: 27 \t--- Loss: 0.138\n",
      "Iteration: 28 \t--- Loss: 0.154\n",
      "Iteration: 29 \t--- Loss: 0.130\n",
      "Iteration: 30 \t--- Loss: 0.135\n",
      "Iteration: 31 \t--- Loss: 0.173\n",
      "Iteration: 32 \t--- Loss: 0.156\n",
      "Iteration: 33 \t--- Loss: 0.166\n",
      "Iteration: 34 \t--- Loss: 0.148\n",
      "Iteration: 35 \t--- Loss: 0.170\n",
      "Iteration: 36 \t--- Loss: 0.160\n",
      "Iteration: 37 \t--- Loss: 0.151\n",
      "Iteration: 38 \t--- Loss: 0.159\n",
      "Iteration: 39 \t--- Loss: 0.160\n",
      "Iteration: 40 \t--- Loss: 0.152\n",
      "Iteration: 41 \t--- Loss: 0.162\n",
      "Iteration: 42 \t--- Loss: 0.157\n",
      "Iteration: 43 \t--- Loss: 0.135\n",
      "Iteration: 44 \t--- Loss: 0.122\n",
      "Iteration: 45 \t--- Loss: 0.143\n",
      "Iteration: 46 \t--- Loss: 0.146\n",
      "Iteration: 47 \t--- Loss: 0.161\n",
      "Iteration: 48 \t--- Loss: 0.154\n",
      "Iteration: 49 \t--- Loss: 0.159\n",
      "Iteration: 50 \t--- Loss: 0.145\n",
      "Iteration: 51 \t--- Loss: 0.154\n",
      "Iteration: 52 \t--- Loss: 0.139\n",
      "Iteration: 53 \t--- Loss: 0.165\n",
      "Iteration: 54 \t--- Loss: 0.144\n",
      "Iteration: 55 \t--- Loss: 0.159\n",
      "Iteration: 56 \t--- Loss: 0.157\n",
      "Iteration: 57 \t--- Loss: 0.154\n",
      "Iteration: 58 \t--- Loss: 0.156\n",
      "Iteration: 59 \t--- Loss: 0.143\n",
      "Iteration: 60 \t--- Loss: 0.172\n",
      "Iteration: 61 \t--- Loss: 0.154\n",
      "Iteration: 62 \t--- Loss: 0.154\n",
      "Iteration: 63 \t--- Loss: 0.132\n",
      "Iteration: 64 \t--- Loss: 0.168\n",
      "Iteration: 65 \t--- Loss: 0.168\n",
      "Iteration: 66 \t--- Loss: 0.144\n",
      "Iteration: 67 \t--- Loss: 0.139\n",
      "Iteration: 68 \t--- Loss: 0.125\n",
      "Iteration: 69 \t--- Loss: 0.165\n",
      "Iteration: 70 \t--- Loss: 0.148\n",
      "Iteration: 71 \t--- Loss: 0.143\n",
      "Iteration: 72 \t--- Loss: 0.148\n",
      "Iteration: 73 \t--- Loss: 0.153\n",
      "Iteration: 74 \t--- Loss: 0.142\n",
      "Iteration: 75 \t--- Loss: 0.146\n",
      "Iteration: 76 \t--- Loss: 0.142\n",
      "Iteration: 77 \t--- Loss: 0.159\n",
      "Iteration: 78 \t--- Loss: 0.139\n",
      "Iteration: 79 \t--- Loss: 0.145\n",
      "Iteration: 80 \t--- Loss: 0.147\n",
      "Iteration: 81 \t--- Loss: 0.155\n",
      "Iteration: 82 \t--- Loss: 0.153\n",
      "Iteration: 83 \t--- Loss: 0.145\n",
      "Iteration: 84 \t--- Loss: 0.153\n",
      "Iteration: 85 \t--- Loss: 0.143\n",
      "Iteration: 86 \t--- Loss: 0.142\n",
      "Iteration: 87 \t--- Loss: 0.150\n",
      "Iteration: 88 \t--- Loss: 0.136\n",
      "Iteration: 89 \t--- Loss: 0.173\n",
      "Iteration: 90 \t--- Loss: 0.157\n",
      "Iteration: 91 \t--- Loss: 0.176\n",
      "Iteration: 92 \t--- Loss: 0.155\n",
      "Iteration: 93 \t--- Loss: 0.154\n",
      "Iteration: 94 \t--- Loss: 0.148\n",
      "Iteration: 95 \t--- Loss: 0.137\n",
      "Iteration: 96 \t--- Loss: 0.153\n",
      "Iteration: 97 \t--- Loss: 0.129\n",
      "Iteration: 98 \t--- Loss: 0.144\n",
      "Iteration: 99 \t--- Loss: 0.140\n",
      "Iteration: 100 \t--- Loss: 0.150\n",
      "Iteration: 101 \t--- Loss: 0.141\n",
      "Iteration: 102 \t--- Loss: 0.149\n",
      "Iteration: 103 \t--- Loss: 0.160\n",
      "Iteration: 104 \t--- Loss: 0.142\n",
      "Iteration: 105 \t--- Loss: 0.163\n",
      "Iteration: 106 \t--- Loss: 0.159\n",
      "Iteration: 107 \t--- Loss: 0.148\n",
      "Iteration: 108 \t--- Loss: 0.126\n",
      "Iteration: 109 \t--- Loss: 0.152\n",
      "Iteration: 110 \t--- Loss: 0.145\n",
      "Iteration: 111 \t--- Loss: 0.142\n",
      "Iteration: 112 \t--- Loss: 0.146\n",
      "Iteration: 113 \t--- Loss: 0.133\n",
      "Iteration: 114 \t--- Loss: 0.145\n",
      "Iteration: 115 \t--- Loss: 0.148\n",
      "Iteration: 116 \t--- Loss: 0.147\n",
      "Iteration: 117 \t--- Loss: 0.142\n",
      "Iteration: 118 \t--- Loss: 0.136\n",
      "Iteration: 119 \t--- Loss: 0.156\n",
      "Iteration: 120 \t--- Loss: 0.143\n",
      "Iteration: 121 \t--- Loss: 0.150\n",
      "Iteration: 122 \t--- Loss: 0.148\n",
      "Iteration: 123 \t--- Loss: 0.150\n",
      "Iteration: 124 \t--- Loss: 0.142\n",
      "Iteration: 125 \t--- Loss: 0.168\n",
      "Iteration: 126 \t--- Loss: 0.150\n",
      "Iteration: 127 \t--- Loss: 0.147\n",
      "Iteration: 128 \t--- Loss: 0.126\n",
      "Iteration: 129 \t--- Loss: 0.152\n",
      "Iteration: 130 \t--- Loss: 0.146\n",
      "Iteration: 131 \t--- Loss: 0.168\n",
      "Iteration: 132 \t--- Loss: 0.150\n",
      "Iteration: 133 \t--- Loss: 0.147\n",
      "Iteration: 134 \t--- Loss: 0.162\n",
      "Iteration: 135 \t--- Loss: 0.131\n",
      "Iteration: 136 \t--- Loss: 0.130\n",
      "Iteration: 137 \t--- Loss: 0.136\n",
      "Iteration: 138 \t--- Loss: 0.143\n",
      "Iteration: 139 \t--- Loss: 0.113\n",
      "Iteration: 140 \t--- Loss: 0.139\n",
      "Iteration: 141 \t--- Loss: 0.146\n",
      "Iteration: 142 \t--- Loss: 0.154\n",
      "Iteration: 143 \t--- Loss: 0.143\n",
      "Iteration: 144 \t--- Loss: 0.146\n",
      "Iteration: 145 \t--- Loss: 0.143\n",
      "Iteration: 146 \t--- Loss: 0.140\n",
      "Iteration: 147 \t--- Loss: 0.135\n",
      "Iteration: 148 \t--- Loss: 0.140\n",
      "Iteration: 149 \t--- Loss: 0.158\n",
      "Iteration: 150 \t--- Loss: 0.141\n",
      "Iteration: 151 \t--- Loss: 0.151\n",
      "Iteration: 152 \t--- Loss: 0.155\n",
      "Iteration: 153 \t--- Loss: 0.150\n",
      "Iteration: 154 \t--- Loss: 0.141\n",
      "Iteration: 155 \t--- Loss: 0.147\n",
      "Iteration: 156 \t--- Loss: 0.141\n",
      "Iteration: 157 \t--- Loss: 0.141\n",
      "Iteration: 158 \t--- Loss: 0.136\n",
      "Iteration: 159 \t--- Loss: 0.135\n",
      "Iteration: 160 \t--- Loss: 0.144\n",
      "Iteration: 161 \t--- Loss: 0.129\n",
      "Iteration: 162 \t--- Loss: 0.149\n",
      "Iteration: 163 \t--- Loss: 0.125\n",
      "Iteration: 164 \t--- Loss: 0.155\n",
      "Iteration: 165 \t--- Loss: 0.154\n",
      "Iteration: 166 \t--- Loss: 0.131\n",
      "Iteration: 167 \t--- Loss: 0.139\n",
      "Iteration: 168 \t--- Loss: 0.141\n",
      "Iteration: 169 \t--- Loss: 0.135\n",
      "Iteration: 170 \t--- Loss: 0.124\n",
      "Iteration: 171 \t--- Loss: 0.140\n",
      "Iteration: 172 \t--- Loss: 0.154\n",
      "Iteration: 173 \t--- Loss: 0.170\n",
      "Iteration: 174 \t--- Loss: 0.133\n",
      "Iteration: 175 \t--- Loss: 0.166\n",
      "Iteration: 176 \t--- Loss: 0.137\n",
      "Iteration: 177 \t--- Loss: 0.146\n",
      "Iteration: 178 \t--- Loss: 0.146\n",
      "Iteration: 179 \t--- Loss: 0.149\n",
      "Iteration: 180 \t--- Loss: 0.138\n",
      "Iteration: 181 \t--- Loss: 0.148\n",
      "Iteration: 182 \t--- Loss: 0.150\n",
      "Iteration: 183 \t--- Loss: 0.163\n",
      "Iteration: 184 \t--- Loss: 0.132\n",
      "Iteration: 185 \t--- Loss: 0.148\n",
      "Iteration: 186 \t--- Loss: 0.139\n",
      "Iteration: 187 \t--- Loss: 0.138\n",
      "Iteration: 188 \t--- Loss: 0.144\n",
      "Iteration: 189 \t--- Loss: 0.150\n",
      "Iteration: 190 \t--- Loss: 0.144\n",
      "Iteration: 191 \t--- Loss: 0.134\n",
      "Iteration: 192 \t--- Loss: 0.162\n",
      "Iteration: 193 \t--- Loss: 0.139\n",
      "Iteration: 194 \t--- Loss: 0.142\n",
      "Iteration: 195 \t--- Loss: 0.146\n",
      "Iteration: 196 \t--- Loss: 0.159\n",
      "Iteration: 197 \t--- Loss: 0.133\n",
      "Iteration: 198 \t--- Loss: 0.155\n",
      "Iteration: 199 \t--- Loss: 0.140\n",
      "Iteration: 200 \t--- Loss: 0.146\n",
      "Iteration: 201 \t--- Loss: 0.150\n",
      "Iteration: 202 \t--- Loss: 0.140\n",
      "Iteration: 203 \t--- Loss: 0.137\n",
      "Iteration: 204 \t--- Loss: 0.168\n",
      "Iteration: 205 \t--- Loss: 0.156\n",
      "Iteration: 206 \t--- Loss: 0.141\n",
      "Iteration: 207 \t--- Loss: 0.128\n",
      "Iteration: 208 \t--- Loss: 0.120\n",
      "Iteration: 209 \t--- Loss: 0.134\n",
      "Iteration: 210 \t--- Loss: 0.131\n",
      "Iteration: 211 \t--- Loss: 0.137\n",
      "Iteration: 212 \t--- Loss: 0.120\n",
      "Iteration: 213 \t--- Loss: 0.138\n",
      "Iteration: 214 \t--- Loss: 0.168\n",
      "Iteration: 215 \t--- Loss: 0.157\n",
      "Iteration: 216 \t--- Loss: 0.122\n",
      "Iteration: 217 \t--- Loss: 0.148\n",
      "Iteration: 218 \t--- Loss: 0.154\n",
      "Iteration: 219 \t--- Loss: 0.128\n",
      "Iteration: 220 \t--- Loss: 0.140\n",
      "Iteration: 221 \t--- Loss: 0.154\n",
      "Iteration: 222 \t--- Loss: 0.153\n",
      "Iteration: 223 \t--- Loss: 0.148\n",
      "Iteration: 224 \t--- Loss: 0.157\n",
      "Iteration: 225 \t--- Loss: 0.158\n",
      "Iteration: 226 \t--- Loss: 0.148\n",
      "Iteration: 227 \t--- Loss: 0.165\n",
      "Iteration: 228 \t--- Loss: 0.143\n",
      "Iteration: 229 \t--- Loss: 0.123\n",
      "Iteration: 230 \t--- Loss: 0.136\n",
      "Iteration: 231 \t--- Loss: 0.165\n",
      "Iteration: 232 \t--- Loss: 0.157\n",
      "Iteration: 233 \t--- Loss: 0.142\n",
      "Iteration: 234 \t--- Loss: 0.133\n",
      "Iteration: 235 \t--- Loss: 0.156\n",
      "Iteration: 236 \t--- Loss: 0.151\n",
      "Iteration: 237 \t--- Loss: 0.151\n",
      "Iteration: 238 \t--- Loss: 0.147\n",
      "Iteration: 239 \t--- Loss: 0.176\n",
      "Iteration: 240 \t--- Loss: 0.161\n",
      "Iteration: 241 \t--- Loss: 0.156\n",
      "Iteration: 242 \t--- Loss: 0.156\n",
      "Iteration: 243 \t--- Loss: 0.134\n",
      "Iteration: 244 \t--- Loss: 0.156\n",
      "Iteration: 245 \t--- Loss: 0.150\n",
      "Iteration: 246 \t--- Loss: 0.147\n",
      "Iteration: 247 \t--- Loss: 0.146\n",
      "Iteration: 248 \t--- Loss: 0.143\n",
      "Iteration: 249 \t--- Loss: 0.144\n",
      "Iteration: 250 \t--- Loss: 0.154\n",
      "Iteration: 251 \t--- Loss: 0.139\n",
      "Iteration: 252 \t--- Loss: 0.141\n",
      "Iteration: 253 \t--- Loss: 0.150\n",
      "Iteration: 254 \t--- Loss: 0.148\n",
      "Iteration: 255 \t--- Loss: 0.156\n",
      "Iteration: 256 \t--- Loss: 0.179\n",
      "Iteration: 257 \t--- Loss: 0.145\n",
      "Iteration: 258 \t--- Loss: 0.129\n",
      "Iteration: 259 \t--- Loss: 0.149Iteration: 0 \t--- Loss: 0.112\n",
      "Iteration: 1 \t--- Loss: 0.095\n",
      "Iteration: 2 \t--- Loss: 0.098\n",
      "Iteration: 3 \t--- Loss: 0.093\n",
      "Iteration: 4 \t--- Loss: 0.088\n",
      "Iteration: 5 \t--- Loss: 0.085\n",
      "Iteration: 6 \t--- Loss: 0.078\n",
      "Iteration: 7 \t--- Loss: 0.082\n",
      "Iteration: 8 \t--- Loss: 0.072\n",
      "Iteration: 9 \t--- Loss: 0.070\n",
      "Iteration: 10 \t--- Loss: 0.069\n",
      "Iteration: 11 \t--- Loss: 0.071\n",
      "Iteration: 12 \t--- Loss: 0.061\n",
      "Iteration: 13 \t--- Loss: 0.058\n",
      "Iteration: 14 \t--- Loss: 0.061\n",
      "Iteration: 15 \t--- Loss: 0.058\n",
      "Iteration: 16 \t--- Loss: 0.055\n",
      "Iteration: 17 \t--- Loss: 0.055\n",
      "Iteration: 18 \t--- Loss: 0.054\n",
      "Iteration: 19 \t--- Loss: 0.055\n",
      "Iteration: 20 \t--- Loss: 0.047\n",
      "Iteration: 21 \t--- Loss: 0.055\n",
      "Iteration: 22 \t--- Loss: 0.049\n",
      "Iteration: 23 \t--- Loss: 0.051\n",
      "Iteration: 24 \t--- Loss: 0.049\n",
      "Iteration: 25 \t--- Loss: 0.049\n",
      "Iteration: 26 \t--- Loss: 0.049\n",
      "Iteration: 27 \t--- Loss: 0.049\n",
      "Iteration: 28 \t--- Loss: 0.047\n",
      "Iteration: 29 \t--- Loss: 0.045\n",
      "Iteration: 30 \t--- Loss: 0.046\n",
      "Iteration: 31 \t--- Loss: 0.047\n",
      "Iteration: 32 \t--- Loss: 0.045\n",
      "Iteration: 33 \t--- Loss: 0.046\n",
      "Iteration: 34 \t--- Loss: 0.041\n",
      "Iteration: 35 \t--- Loss: 0.044\n",
      "Iteration: 36 \t--- Loss: 0.048\n",
      "Iteration: 37 \t--- Loss: 0.046\n",
      "Iteration: 38 \t--- Loss: 0.046\n",
      "Iteration: 39 \t--- Loss: 0.043\n",
      "Iteration: 40 \t--- Loss: 0.043\n",
      "Iteration: 41 \t--- Loss: 0.045\n",
      "Iteration: 42 \t--- Loss: 0.045\n",
      "Iteration: 43 \t--- Loss: 0.043\n",
      "Iteration: 44 \t--- Loss: 0.043\n",
      "Iteration: 45 \t--- Loss: 0.045\n",
      "Iteration: 46 \t--- Loss: 0.045\n",
      "Iteration: 47 \t--- Loss: 0.045\n",
      "Iteration: 48 \t--- Loss: 0.041\n",
      "Iteration: 49 \t--- Loss: 0.045\n",
      "Iteration: 50 \t--- Loss: 0.043\n",
      "Iteration: 51 \t--- Loss: 0.047\n",
      "Iteration: 52 \t--- Loss: 0.041\n",
      "Iteration: 53 \t--- Loss: 0.045\n",
      "Iteration: 54 \t--- Loss: 0.045\n",
      "Iteration: 55 \t--- Loss: 0.042\n",
      "Iteration: 56 \t--- Loss: 0.042\n",
      "Iteration: 57 \t--- Loss: 0.043\n",
      "Iteration: 58 \t--- Loss: 0.044\n",
      "Iteration: 59 \t--- Loss: 0.043\n",
      "Iteration: 60 \t--- Loss: 0.040\n",
      "Iteration: 61 \t--- Loss: 0.045\n",
      "Iteration: 62 \t--- Loss: 0.041\n",
      "Iteration: 63 \t--- Loss: 0.042\n",
      "Iteration: 64 \t--- Loss: 0.042\n",
      "Iteration: 65 \t--- Loss: 0.042\n",
      "Iteration: 66 \t--- Loss: 0.043\n",
      "Iteration: 67 \t--- Loss: 0.042\n",
      "Iteration: 68 \t--- Loss: 0.048\n",
      "Iteration: 69 \t--- Loss: 0.043\n",
      "Iteration: 70 \t--- Loss: 0.041\n",
      "Iteration: 71 \t--- Loss: 0.045\n",
      "Iteration: 72 \t--- Loss: 0.043\n",
      "Iteration: 73 \t--- Loss: 0.041\n",
      "Iteration: 74 \t--- Loss: 0.046\n",
      "Iteration: 75 \t--- Loss: 0.042\n",
      "Iteration: 76 \t--- Loss: 0.043\n",
      "Iteration: 77 \t--- Loss: 0.044\n",
      "Iteration: 78 \t--- Loss: 0.042\n",
      "Iteration: 79 \t--- Loss: 0.043\n",
      "Iteration: 80 \t--- Loss: 0.044\n",
      "Iteration: 81 \t--- Loss: 0.039\n",
      "Iteration: 82 \t--- Loss: 0.044\n",
      "Iteration: 83 \t--- Loss: 0.042\n",
      "Iteration: 84 \t--- Loss: 0.044\n",
      "Iteration: 85 \t--- Loss: 0.046\n",
      "Iteration: 86 \t--- Loss: 0.043\n",
      "Iteration: 87 \t--- Loss: 0.040\n",
      "Iteration: 88 \t--- Loss: 0.039\n",
      "Iteration: 89 \t--- Loss: 0.040\n",
      "Iteration: 90 \t--- Loss: 0.042\n",
      "Iteration: 91 \t--- Loss: 0.041\n",
      "Iteration: 92 \t--- Loss: 0.039\n",
      "Iteration: 93 \t--- Loss: 0.043\n",
      "Iteration: 94 \t--- Loss: 0.042\n",
      "Iteration: 95 \t--- Loss: 0.040\n",
      "Iteration: 96 \t--- Loss: 0.044\n",
      "Iteration: 97 \t--- Loss: 0.041\n",
      "Iteration: 98 \t--- Loss: 0.044\n",
      "Iteration: 99 \t--- Loss: 0.039\n",
      "Iteration: 100 \t--- Loss: 0.041\n",
      "Iteration: 101 \t--- Loss: 0.044\n",
      "Iteration: 102 \t--- Loss: 0.045\n",
      "Iteration: 103 \t--- Loss: 0.042\n",
      "Iteration: 104 \t--- Loss: 0.042\n",
      "Iteration: 105 \t--- Loss: 0.043\n",
      "Iteration: 106 \t--- Loss: 0.043\n",
      "Iteration: 107 \t--- Loss: 0.043\n",
      "Iteration: 108 \t--- Loss: 0.044\n",
      "Iteration: 109 \t--- Loss: 0.043\n",
      "Iteration: 110 \t--- Loss: 0.040\n",
      "Iteration: 111 \t--- Loss: 0.041\n",
      "Iteration: 112 \t--- Loss: 0.039\n",
      "Iteration: 113 \t--- Loss: 0.042\n",
      "Iteration: 114 \t--- Loss: 0.041\n",
      "Iteration: 115 \t--- Loss: 0.041\n",
      "Iteration: 116 \t--- Loss: 0.040\n",
      "Iteration: 117 \t--- Loss: 0.044\n",
      "Iteration: 118 \t--- Loss: 0.043\n",
      "Iteration: 119 \t--- Loss: 0.042\n",
      "Iteration: 120 \t--- Loss: 0.040\n",
      "Iteration: 121 \t--- Loss: 0.041\n",
      "Iteration: 122 \t--- Loss: 0.041\n",
      "Iteration: 123 \t--- Loss: 0.043\n",
      "Iteration: 124 \t--- Loss: 0.043\n",
      "Iteration: 125 \t--- Loss: 0.039\n",
      "Iteration: 126 \t--- Loss: 0.043\n",
      "Iteration: 127 \t--- Loss: 0.044\n",
      "Iteration: 128 \t--- Loss: 0.040\n",
      "Iteration: 129 \t--- Loss: 0.039\n",
      "Iteration: 130 \t--- Loss: 0.042\n",
      "Iteration: 131 \t--- Loss: 0.040\n",
      "Iteration: 132 \t--- Loss: 0.040\n",
      "Iteration: 133 \t--- Loss: 0.042\n",
      "Iteration: 134 \t--- Loss: 0.042\n",
      "Iteration: 135 \t--- Loss: 0.041\n",
      "Iteration: 136 \t--- Loss: 0.040\n",
      "Iteration: 137 \t--- Loss: 0.042\n",
      "Iteration: 138 \t--- Loss: 0.043\n",
      "Iteration: 139 \t--- Loss: 0.043\n",
      "Iteration: 140 \t--- Loss: 0.043\n",
      "Iteration: 141 \t--- Loss: 0.040\n",
      "Iteration: 142 \t--- Loss: 0.039\n",
      "Iteration: 143 \t--- Loss: 0.042\n",
      "Iteration: 144 \t--- Loss: 0.043\n",
      "Iteration: 145 \t--- Loss: 0.038\n",
      "Iteration: 146 \t--- Loss: 0.041\n",
      "Iteration: 147 \t--- Loss: 0.041\n",
      "Iteration: 148 \t--- Loss: 0.041\n",
      "Iteration: 149 \t--- Loss: 0.043\n",
      "Iteration: 150 \t--- Loss: 0.045\n",
      "Iteration: 151 \t--- Loss: 0.039\n",
      "Iteration: 152 \t--- Loss: 0.042\n",
      "Iteration: 153 \t--- Loss: 0.043\n",
      "Iteration: 154 \t--- Loss: 0.040\n",
      "Iteration: 155 \t--- Loss: 0.042\n",
      "Iteration: 156 \t--- Loss: 0.042\n",
      "Iteration: 157 \t--- Loss: 0.038\n",
      "Iteration: 158 \t--- Loss: 0.039\n",
      "Iteration: 159 \t--- Loss: 0.041\n",
      "Iteration: 160 \t--- Loss: 0.044\n",
      "Iteration: 161 \t--- Loss: 0.042\n",
      "Iteration: 162 \t--- Loss: 0.039\n",
      "Iteration: 163 \t--- Loss: 0.044\n",
      "Iteration: 164 \t--- Loss: 0.042\n",
      "Iteration: 165 \t--- Loss: 0.040\n",
      "Iteration: 166 \t--- Loss: 0.041\n",
      "Iteration: 167 \t--- Loss: 0.041\n",
      "Iteration: 168 \t--- Loss: 0.043\n",
      "Iteration: 169 \t--- Loss: 0.042\n",
      "Iteration: 170 \t--- Loss: 0.041\n",
      "Iteration: 171 \t--- Loss: 0.043\n",
      "Iteration: 172 \t--- Loss: 0.042\n",
      "Iteration: 173 \t--- Loss: 0.044\n",
      "Iteration: 174 \t--- Loss: 0.044\n",
      "Iteration: 175 \t--- Loss: 0.043\n",
      "Iteration: 176 \t--- Loss: 0.039\n",
      "Iteration: 177 \t--- Loss: 0.040\n",
      "Iteration: 178 \t--- Loss: 0.040\n",
      "Iteration: 179 \t--- Loss: 0.037\n",
      "Iteration: 180 \t--- Loss: 0.041\n",
      "Iteration: 181 \t--- Loss: 0.040\n",
      "Iteration: 182 \t--- Loss: 0.041\n",
      "Iteration: 183 \t--- Loss: 0.041\n",
      "Iteration: 184 \t--- Loss: 0.045\n",
      "Iteration: 185 \t--- Loss: 0.042\n",
      "Iteration: 186 \t--- Loss: 0.044\n",
      "Iteration: 187 \t--- Loss: 0.042\n",
      "Iteration: 188 \t--- Loss: 0.037\n",
      "Iteration: 189 \t--- Loss: 0.040\n",
      "Iteration: 190 \t--- Loss: 0.043\n",
      "Iteration: 191 \t--- Loss: 0.041\n",
      "Iteration: 192 \t--- Loss: 0.043\n",
      "Iteration: 193 \t--- Loss: 0.043\n",
      "Iteration: 194 \t--- Loss: 0.040\n",
      "Iteration: 195 \t--- Loss: 0.042\n",
      "Iteration: 196 \t--- Loss: 0.045\n",
      "Iteration: 197 \t--- Loss: 0.041\n",
      "Iteration: 198 \t--- Loss: 0.043\n",
      "Iteration: 199 \t--- Loss: 0.041\n",
      "Iteration: 200 \t--- Loss: 0.040\n",
      "Iteration: 201 \t--- Loss: 0.043\n",
      "Iteration: 202 \t--- Loss: 0.042\n",
      "Iteration: 203 \t--- Loss: 0.043\n",
      "Iteration: 204 \t--- Loss: 0.038\n",
      "Iteration: 205 \t--- Loss: 0.042\n",
      "Iteration: 206 \t--- Loss: 0.043\n",
      "Iteration: 207 \t--- Loss: 0.040\n",
      "Iteration: 208 \t--- Loss: 0.042\n",
      "Iteration: 209 \t--- Loss: 0.041\n",
      "Iteration: 210 \t--- Loss: 0.041\n",
      "Iteration: 211 \t--- Loss: 0.043\n",
      "Iteration: 212 \t--- Loss: 0.043\n",
      "Iteration: 213 \t--- Loss: 0.042\n",
      "Iteration: 214 \t--- Loss: 0.039\n",
      "Iteration: 215 \t--- Loss: 0.041\n",
      "Iteration: 216 \t--- Loss: 0.044\n",
      "Iteration: 217 \t--- Loss: 0.041\n",
      "Iteration: 218 \t--- Loss: 0.045\n",
      "Iteration: 219 \t--- Loss: 0.043\n",
      "Iteration: 220 \t--- Loss: 0.042\n",
      "Iteration: 221 \t--- Loss: 0.044\n",
      "Iteration: 222 \t--- Loss: 0.039\n",
      "Iteration: 223 \t--- Loss: 0.044\n",
      "Iteration: 224 \t--- Loss: 0.042\n",
      "Iteration: 225 \t--- Loss: 0.042\n",
      "Iteration: 226 \t--- Loss: 0.041\n",
      "Iteration: 227 \t--- Loss: 0.042\n",
      "Iteration: 228 \t--- Loss: 0.043\n",
      "Iteration: 229 \t--- Loss: 0.041\n",
      "Iteration: 230 \t--- Loss: 0.045\n",
      "Iteration: 231 \t--- Loss: 0.041\n",
      "Iteration: 232 \t--- Loss: 0.038\n",
      "Iteration: 233 \t--- Loss: 0.042\n",
      "Iteration: 234 \t--- Loss: 0.041\n",
      "Iteration: 235 \t--- Loss: 0.041\n",
      "Iteration: 236 \t--- Loss: 0.043\n",
      "Iteration: 237 \t--- Loss: 0.042\n",
      "Iteration: 238 \t--- Loss: 0.043\n",
      "Iteration: 239 \t--- Loss: 0.041\n",
      "Iteration: 240 \t--- Loss: 0.046\n",
      "Iteration: 241 \t--- Loss: 0.040\n",
      "Iteration: 242 \t--- Loss: 0.039\n",
      "Iteration: 243 \t--- Loss: 0.042\n",
      "Iteration: 244 \t--- Loss: 0.040\n",
      "Iteration: 245 \t--- Loss: 0.042\n",
      "Iteration: 246 \t--- Loss: 0.040\n",
      "Iteration: 247 \t--- Loss: 0.040\n",
      "Iteration: 248 \t--- Loss: 0.045\n",
      "Iteration: 249 \t--- Loss: 0.038\n",
      "Iteration: 250 \t--- Loss: 0.042\n",
      "Iteration: 251 \t--- Loss: 0.041\n",
      "Iteration: 252 \t--- Loss: 0.041\n",
      "Iteration: 253 \t--- Loss: 0.042\n",
      "Iteration: 254 \t--- Loss: 0.043\n",
      "Iteration: 255 \t--- Loss: 0.044\n",
      "Iteration: 256 \t--- Loss: 0.039\n",
      "Iteration: 257 \t--- Loss: 0.040\n",
      "Iteration: 258 \t--- Loss: 0.040\n",
      "Iteration: 259 \t--- Loss: 0.041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.77s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.718\n",
      "Iteration: 261 \t--- Loss: 0.704\n",
      "Iteration: 262 \t--- Loss: 0.734\n",
      "Iteration: 263 \t--- Loss: 0.710\n",
      "Iteration: 264 \t--- Loss: 0.727\n",
      "Iteration: 265 \t--- Loss: 0.711\n",
      "Iteration: 266 \t--- Loss: 0.731\n",
      "Iteration: 267 \t--- Loss: 0.723\n",
      "Iteration: 268 \t--- Loss: 0.721\n",
      "Iteration: 269 \t--- Loss: 0.734\n",
      "Iteration: 270 \t--- Loss: 0.712\n",
      "Iteration: 271 \t--- Loss: 0.742\n",
      "Iteration: 272 \t--- Loss: 0.722\n",
      "Iteration: 273 \t--- Loss: 0.750\n",
      "Iteration: 274 \t--- Loss: 0.720\n",
      "Iteration: 275 \t--- Loss: 0.719\n",
      "Iteration: 276 \t--- Loss: 0.739\n",
      "Iteration: 277 \t--- Loss: 0.731\n",
      "Iteration: 278 \t--- Loss: 0.734\n",
      "Iteration: 279 \t--- Loss: 0.714\n",
      "Iteration: 280 \t--- Loss: 0.730\n",
      "Iteration: 281 \t--- Loss: 0.728\n",
      "Iteration: 282 \t--- Loss: 0.708\n",
      "Iteration: 283 \t--- Loss: 0.727\n",
      "Iteration: 284 \t--- Loss: 0.737\n",
      "Iteration: 285 \t--- Loss: 0.733\n",
      "Iteration: 286 \t--- Loss: 0.714\n",
      "Iteration: 287 \t--- Loss: 0.735\n",
      "Iteration: 288 \t--- Loss: 0.726\n",
      "Iteration: 289 \t--- Loss: 0.734\n",
      "Iteration: 290 \t--- Loss: 0.730\n",
      "Iteration: 291 \t--- Loss: 0.721\n",
      "Iteration: 292 \t--- Loss: 0.746\n",
      "Iteration: 293 \t--- Loss: 0.733\n",
      "Iteration: 294 \t--- Loss: 0.722\n",
      "Iteration: 295 \t--- Loss: 0.722\n",
      "Iteration: 296 \t--- Loss: 0.726\n",
      "Iteration: 297 \t--- Loss: 0.732\n",
      "Iteration: 298 \t--- Loss: 0.728\n",
      "Iteration: 299 \t--- Loss: 0.717\n",
      "Iteration: 300 \t--- Loss: 0.722\n",
      "Iteration: 301 \t--- Loss: 0.726\n",
      "Iteration: 302 \t--- Loss: 0.718\n",
      "Iteration: 303 \t--- Loss: 0.729\n",
      "Iteration: 304 \t--- Loss: 0.717\n",
      "Iteration: 305 \t--- Loss: 0.736\n",
      "Iteration: 306 \t--- Loss: 0.745\n",
      "Iteration: 307 \t--- Loss: 0.726\n",
      "Iteration: 308 \t--- Loss: 0.713\n",
      "Iteration: 309 \t--- Loss: 0.711\n",
      "Iteration: 310 \t--- Loss: 0.736\n",
      "Iteration: 311 \t--- Loss: 0.731\n",
      "Iteration: 312 \t--- Loss: 0.718\n",
      "Iteration: 313 \t--- Loss: 0.732\n",
      "Iteration: 314 \t--- Loss: 0.739\n",
      "Iteration: 315 \t--- Loss: 0.738\n",
      "Iteration: 316 \t--- Loss: 0.728\n",
      "Iteration: 317 \t--- Loss: 0.736\n",
      "Iteration: 318 \t--- Loss: 0.744\n",
      "Iteration: 319 \t--- Loss: 0.740\n",
      "Iteration: 320 \t--- Loss: 0.718\n",
      "Iteration: 321 \t--- Loss: 0.727\n",
      "Iteration: 322 \t--- Loss: 0.722\n",
      "Iteration: 323 \t--- Loss: 0.734\n",
      "Iteration: 324 \t--- Loss: 0.725\n",
      "Iteration: 325 \t--- Loss: 0.729\n",
      "Iteration: 326 \t--- Loss: 0.727\n",
      "Iteration: 327 \t--- Loss: 0.718\n",
      "Iteration: 328 \t--- Loss: 0.731\n",
      "Iteration: 329 \t--- Loss: 0.746\n",
      "Iteration: 330 \t--- Loss: 0.728\n",
      "Iteration: 331 \t--- Loss: 0.725\n",
      "Iteration: 332 \t--- Loss: 0.729\n",
      "Iteration: 333 \t--- Loss: 0.714\n",
      "Iteration: 334 \t--- Loss: 0.734\n",
      "Iteration: 335 \t--- Loss: 0.716\n",
      "Iteration: 336 \t--- Loss: 0.727\n",
      "Iteration: 337 \t--- Loss: 0.718\n",
      "Iteration: 338 \t--- Loss: 0.718\n",
      "Iteration: 339 \t--- Loss: 0.729\n",
      "Iteration: 340 \t--- Loss: 0.727\n",
      "Iteration: 341 \t--- Loss: 0.719\n",
      "Iteration: 342 \t--- Loss: 0.727\n",
      "Iteration: 343 \t--- Loss: 0.723\n",
      "Iteration: 344 \t--- Loss: 0.716\n",
      "Iteration: 345 \t--- Loss: 0.723\n",
      "Iteration: 346 \t--- Loss: 0.724\n",
      "Iteration: 347 \t--- Loss: 0.734\n",
      "Iteration: 348 \t--- Loss: 0.724\n",
      "Iteration: 349 \t--- Loss: 0.735\n",
      "Iteration: 350 \t--- Loss: 0.719\n",
      "Iteration: 351 \t--- Loss: 0.730\n",
      "Iteration: 352 \t--- Loss: 0.733\n",
      "Iteration: 353 \t--- Loss: 0.726\n",
      "Iteration: 354 \t--- Loss: 0.716\n",
      "Iteration: 355 \t--- Loss: 0.717\n",
      "Iteration: 356 \t--- Loss: 0.745\n",
      "Iteration: 357 \t--- Loss: 0.719\n",
      "Iteration: 358 \t--- Loss: 0.733\n",
      "Iteration: 359 \t--- Loss: 0.715\n",
      "Iteration: 360 \t--- Loss: 0.723\n",
      "Iteration: 361 \t--- Loss: 0.728\n",
      "Iteration: 362 \t--- Loss: 0.728\n",
      "Iteration: 363 \t--- Loss: 0.713\n",
      "Iteration: 364 \t--- Loss: 0.729\n",
      "Iteration: 365 \t--- Loss: 0.721\n",
      "Iteration: 366 \t--- Loss: 0.723\n",
      "Iteration: 367 \t--- Loss: 0.730\n",
      "Iteration: 368 \t--- Loss: 0.726\n",
      "Iteration: 369 \t--- Loss: 0.731\n",
      "Iteration: 370 \t--- Loss: 0.713\n",
      "Iteration: 371 \t--- Loss: 0.724\n",
      "Iteration: 372 \t--- Loss: 0.738\n",
      "Iteration: 373 \t--- Loss: 0.737\n",
      "Iteration: 374 \t--- Loss: 0.721\n",
      "Iteration: 375 \t--- Loss: 0.738\n",
      "Iteration: 376 \t--- Loss: 0.743\n",
      "Iteration: 377 \t--- Loss: 0.720\n",
      "Iteration: 378 \t--- Loss: 0.723\n",
      "Iteration: 379 \t--- Loss: 0.720\n",
      "Iteration: 380 \t--- Loss: 0.730\n",
      "Iteration: 381 \t--- Loss: 0.726\n",
      "Iteration: 382 \t--- Loss: 0.729\n",
      "Iteration: 383 \t--- Loss: 0.727\n",
      "Iteration: 384 \t--- Loss: 0.737\n",
      "Iteration: 385 \t--- Loss: 0.736\n",
      "Iteration: 386 \t--- Loss: 0.724\n",
      "Iteration: 387 \t--- Loss: 0.722\n",
      "Iteration: 388 \t--- Loss: 0.740\n",
      "Iteration: 389 \t--- Loss: 0.718\n",
      "Iteration: 390 \t--- Loss: 0.701\n",
      "Iteration: 391 \t--- Loss: 0.723\n",
      "Iteration: 392 \t--- Loss: 0.744\n",
      "Iteration: 393 \t--- Loss: 0.733\n",
      "Iteration: 394 \t--- Loss: 0.727\n",
      "Iteration: 395 \t--- Loss: 0.728\n",
      "Iteration: 396 \t--- Loss: 0.703\n",
      "Iteration: 397 \t--- Loss: 0.738\n",
      "Iteration: 398 \t--- Loss: 0.738\n",
      "Iteration: 399 \t--- Loss: 0.719\n",
      "Iteration: 400 \t--- Loss: 0.709\n",
      "Iteration: 401 \t--- Loss: 0.746\n",
      "Iteration: 402 \t--- Loss: 0.711\n",
      "Iteration: 403 \t--- Loss: 0.728\n",
      "Iteration: 404 \t--- Loss: 0.727\n",
      "Iteration: 405 \t--- Loss: 0.729\n",
      "Iteration: 406 \t--- Loss: 0.716\n",
      "Iteration: 407 \t--- Loss: 0.746\n",
      "Iteration: 408 \t--- Loss: 0.718\n",
      "Iteration: 409 \t--- Loss: 0.717\n",
      "Iteration: 410 \t--- Loss: 0.731\n",
      "Iteration: 411 \t--- Loss: 0.735\n",
      "Iteration: 412 \t--- Loss: 0.727\n",
      "Iteration: 413 \t--- Loss: 0.713\n",
      "Iteration: 414 \t--- Loss: 0.726\n",
      "Iteration: 415 \t--- Loss: 0.722\n",
      "Iteration: 416 \t--- Loss: 0.728\n",
      "Iteration: 417 \t--- Loss: 0.712\n",
      "Iteration: 418 \t--- Loss: 0.729\n",
      "Iteration: 419 \t--- Loss: 0.740\n",
      "Iteration: 420 \t--- Loss: 0.725\n",
      "Iteration: 421 \t--- Loss: 0.735\n",
      "Iteration: 422 \t--- Loss: 0.719\n",
      "Iteration: 423 \t--- Loss: 0.716\n",
      "Iteration: 424 \t--- Loss: 0.713\n",
      "Iteration: 425 \t--- Loss: 0.736\n",
      "Iteration: 426 \t--- Loss: 0.735\n",
      "Iteration: 427 \t--- Loss: 0.726\n",
      "Iteration: 428 \t--- Loss: 0.721\n",
      "Iteration: 429 \t--- Loss: 0.722\n",
      "Iteration: 430 \t--- Loss: 0.710\n",
      "Iteration: 431 \t--- Loss: 0.713\n",
      "Iteration: 432 \t--- Loss: 0.721\n",
      "Iteration: 433 \t--- Loss: 0.725\n",
      "Iteration: 434 \t--- Loss: 0.733\n",
      "Iteration: 435 \t--- Loss: 0.725\n",
      "Iteration: 436 \t--- Loss: 0.734\n",
      "Iteration: 437 \t--- Loss: 0.726\n",
      "Iteration: 438 \t--- Loss: 0.720\n",
      "Iteration: 439 \t--- Loss: 0.717\n",
      "Iteration: 440 \t--- Loss: 0.715\n",
      "Iteration: 441 \t--- Loss: 0.731\n",
      "Iteration: 442 \t--- Loss: 0.723\n",
      "Iteration: 443 \t--- Loss: 0.732\n",
      "Iteration: 444 \t--- Loss: 0.731\n",
      "Iteration: 445 \t--- Loss: 0.739\n",
      "Iteration: 446 \t--- Loss: 0.720\n",
      "Iteration: 447 \t--- Loss: 0.744\n",
      "Iteration: 448 \t--- Loss: 0.721\n",
      "Iteration: 449 \t--- Loss: 0.729\n",
      "Iteration: 450 \t--- Loss: 0.730\n",
      "Iteration: 451 \t--- Loss: 0.718\n",
      "Iteration: 452 \t--- Loss: 0.728\n",
      "Iteration: 453 \t--- Loss: 0.728\n",
      "Iteration: 454 \t--- Loss: 0.725\n",
      "Iteration: 455 \t--- Loss: 0.744\n",
      "Iteration: 456 \t--- Loss: 0.732\n",
      "Iteration: 457 \t--- Loss: 0.723\n",
      "Iteration: 458 \t--- Loss: 0.733\n",
      "Iteration: 459 \t--- Loss: 0.735\n",
      "Iteration: 460 \t--- Loss: 0.733\n",
      "Iteration: 461 \t--- Loss: 0.735\n",
      "Iteration: 462 \t--- Loss: 0.724\n",
      "Iteration: 463 \t--- Loss: 0.733\n",
      "Iteration: 464 \t--- Loss: 0.716\n",
      "Iteration: 465 \t--- Loss: 0.750\n",
      "Iteration: 466 \t--- Loss: 0.736\n",
      "Iteration: 467 \t--- Loss: 0.733\n",
      "Iteration: 468 \t--- Loss: 0.719\n",
      "Iteration: 469 \t--- Loss: 0.729\n",
      "Iteration: 470 \t--- Loss: 0.735\n",
      "Iteration: 471 \t--- Loss: 0.727\n",
      "Iteration: 472 \t--- Loss: 0.732\n",
      "Iteration: 473 \t--- Loss: 0.702\n",
      "Iteration: 474 \t--- Loss: 0.726\n",
      "Iteration: 475 \t--- Loss: 0.725\n",
      "Iteration: 476 \t--- Loss: 0.721\n",
      "Iteration: 477 \t--- Loss: 0.740\n",
      "Iteration: 478 \t--- Loss: 0.718\n",
      "Iteration: 479 \t--- Loss: 0.710\n",
      "Iteration: 480 \t--- Loss: 0.730\n",
      "Iteration: 481 \t--- Loss: 0.724\n",
      "Iteration: 482 \t--- Loss: 0.719\n",
      "Iteration: 483 \t--- Loss: 0.727\n",
      "Iteration: 484 \t--- Loss: 0.716\n",
      "Iteration: 485 \t--- Loss: 0.731\n",
      "Iteration: 486 \t--- Loss: 0.718\n",
      "Iteration: 487 \t--- Loss: 0.731\n",
      "Iteration: 488 \t--- Loss: 0.718\n",
      "Iteration: 489 \t--- Loss: 0.716\n",
      "Iteration: 490 \t--- Loss: 0.717\n",
      "Iteration: 491 \t--- Loss: 0.712\n",
      "Iteration: 492 \t--- Loss: 0.718\n",
      "Iteration: 493 \t--- Loss: 0.726\n",
      "Iteration: 494 \t--- Loss: 0.721\n",
      "Iteration: 495 \t--- Loss: 0.721\n",
      "Iteration: 496 \t--- Loss: 0.745\n",
      "Iteration: 497 \t--- Loss: 0.735\n",
      "Iteration: 498 \t--- Loss: 0.727\n",
      "Iteration: 499 \t--- Loss: 0.725\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:29<00:00, 89.66s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.163\n",
      "Iteration: 261 \t--- Loss: 0.126\n",
      "Iteration: 262 \t--- Loss: 0.148\n",
      "Iteration: 263 \t--- Loss: 0.147\n",
      "Iteration: 264 \t--- Loss: 0.147\n",
      "Iteration: 265 \t--- Loss: 0.149\n",
      "Iteration: 266 \t--- Loss: 0.127\n",
      "Iteration: 267 \t--- Loss: 0.135\n",
      "Iteration: 268 \t--- Loss: 0.148\n",
      "Iteration: 269 \t--- Loss: 0.148\n",
      "Iteration: 270 \t--- Loss: 0.148\n",
      "Iteration: 271 \t--- Loss: 0.157\n",
      "Iteration: 272 \t--- Loss: 0.161\n",
      "Iteration: 273 \t--- Loss: 0.142\n",
      "Iteration: 274 \t--- Loss: 0.134\n",
      "Iteration: 275 \t--- Loss: 0.146\n",
      "Iteration: 276 \t--- Loss: 0.156\n",
      "Iteration: 277 \t--- Loss: 0.149\n",
      "Iteration: 278 \t--- Loss: 0.125\n",
      "Iteration: 279 \t--- Loss: 0.144\n",
      "Iteration: 280 \t--- Loss: 0.132\n",
      "Iteration: 281 \t--- Loss: 0.136\n",
      "Iteration: 282 \t--- Loss: 0.147\n",
      "Iteration: 283 \t--- Loss: 0.133\n",
      "Iteration: 284 \t--- Loss: 0.169\n",
      "Iteration: 285 \t--- Loss: 0.162\n",
      "Iteration: 286 \t--- Loss: 0.165\n",
      "Iteration: 287 \t--- Loss: 0.142\n",
      "Iteration: 288 \t--- Loss: 0.137\n",
      "Iteration: 289 \t--- Loss: 0.160\n",
      "Iteration: 290 \t--- Loss: 0.133\n",
      "Iteration: 291 \t--- Loss: 0.134\n",
      "Iteration: 292 \t--- Loss: 0.159\n",
      "Iteration: 293 \t--- Loss: 0.142\n",
      "Iteration: 294 \t--- Loss: 0.133\n",
      "Iteration: 295 \t--- Loss: 0.155\n",
      "Iteration: 296 \t--- Loss: 0.140\n",
      "Iteration: 297 \t--- Loss: 0.135\n",
      "Iteration: 298 \t--- Loss: 0.134\n",
      "Iteration: 299 \t--- Loss: 0.147\n",
      "Iteration: 300 \t--- Loss: 0.132\n",
      "Iteration: 301 \t--- Loss: 0.153\n",
      "Iteration: 302 \t--- Loss: 0.154\n",
      "Iteration: 303 \t--- Loss: 0.161\n",
      "Iteration: 304 \t--- Loss: 0.134\n",
      "Iteration: 305 \t--- Loss: 0.138\n",
      "Iteration: 306 \t--- Loss: 0.140\n",
      "Iteration: 307 \t--- Loss: 0.146\n",
      "Iteration: 308 \t--- Loss: 0.156\n",
      "Iteration: 309 \t--- Loss: 0.150\n",
      "Iteration: 310 \t--- Loss: 0.150\n",
      "Iteration: 311 \t--- Loss: 0.123\n",
      "Iteration: 312 \t--- Loss: 0.150\n",
      "Iteration: 313 \t--- Loss: 0.148\n",
      "Iteration: 314 \t--- Loss: 0.142\n",
      "Iteration: 315 \t--- Loss: 0.143\n",
      "Iteration: 316 \t--- Loss: 0.136\n",
      "Iteration: 317 \t--- Loss: 0.156\n",
      "Iteration: 318 \t--- Loss: 0.126\n",
      "Iteration: 319 \t--- Loss: 0.145\n",
      "Iteration: 320 \t--- Loss: 0.131\n",
      "Iteration: 321 \t--- Loss: 0.148\n",
      "Iteration: 322 \t--- Loss: 0.138\n",
      "Iteration: 323 \t--- Loss: 0.175\n",
      "Iteration: 324 \t--- Loss: 0.143\n",
      "Iteration: 325 \t--- Loss: 0.144\n",
      "Iteration: 326 \t--- Loss: 0.146\n",
      "Iteration: 327 \t--- Loss: 0.134\n",
      "Iteration: 328 \t--- Loss: 0.141\n",
      "Iteration: 329 \t--- Loss: 0.132\n",
      "Iteration: 330 \t--- Loss: 0.140\n",
      "Iteration: 331 \t--- Loss: 0.131\n",
      "Iteration: 332 \t--- Loss: 0.136\n",
      "Iteration: 333 \t--- Loss: 0.137\n",
      "Iteration: 334 \t--- Loss: 0.146\n",
      "Iteration: 335 \t--- Loss: 0.154\n",
      "Iteration: 336 \t--- Loss: 0.179\n",
      "Iteration: 337 \t--- Loss: 0.139\n",
      "Iteration: 338 \t--- Loss: 0.147\n",
      "Iteration: 339 \t--- Loss: 0.151\n",
      "Iteration: 340 \t--- Loss: 0.130\n",
      "Iteration: 341 \t--- Loss: 0.158\n",
      "Iteration: 342 \t--- Loss: 0.140\n",
      "Iteration: 343 \t--- Loss: 0.157\n",
      "Iteration: 344 \t--- Loss: 0.133\n",
      "Iteration: 345 \t--- Loss: 0.163\n",
      "Iteration: 346 \t--- Loss: 0.143\n",
      "Iteration: 347 \t--- Loss: 0.139\n",
      "Iteration: 348 \t--- Loss: 0.128\n",
      "Iteration: 349 \t--- Loss: 0.148\n",
      "Iteration: 350 \t--- Loss: 0.151\n",
      "Iteration: 351 \t--- Loss: 0.161\n",
      "Iteration: 352 \t--- Loss: 0.149\n",
      "Iteration: 353 \t--- Loss: 0.141\n",
      "Iteration: 354 \t--- Loss: 0.148\n",
      "Iteration: 355 \t--- Loss: 0.157\n",
      "Iteration: 356 \t--- Loss: 0.147\n",
      "Iteration: 357 \t--- Loss: 0.136\n",
      "Iteration: 358 \t--- Loss: 0.156\n",
      "Iteration: 359 \t--- Loss: 0.141\n",
      "Iteration: 360 \t--- Loss: 0.145\n",
      "Iteration: 361 \t--- Loss: 0.132\n",
      "Iteration: 362 \t--- Loss: 0.128\n",
      "Iteration: 363 \t--- Loss: 0.135\n",
      "Iteration: 364 \t--- Loss: 0.162\n",
      "Iteration: 365 \t--- Loss: 0.141\n",
      "Iteration: 366 \t--- Loss: 0.156\n",
      "Iteration: 367 \t--- Loss: 0.135\n",
      "Iteration: 368 \t--- Loss: 0.144\n",
      "Iteration: 369 \t--- Loss: 0.135\n",
      "Iteration: 370 \t--- Loss: 0.159\n",
      "Iteration: 371 \t--- Loss: 0.137\n",
      "Iteration: 372 \t--- Loss: 0.132\n",
      "Iteration: 373 \t--- Loss: 0.148\n",
      "Iteration: 374 \t--- Loss: 0.139\n",
      "Iteration: 375 \t--- Loss: 0.144\n",
      "Iteration: 376 \t--- Loss: 0.137\n",
      "Iteration: 377 \t--- Loss: 0.144\n",
      "Iteration: 378 \t--- Loss: 0.148\n",
      "Iteration: 379 \t--- Loss: 0.140\n",
      "Iteration: 380 \t--- Loss: 0.148\n",
      "Iteration: 381 \t--- Loss: 0.163\n",
      "Iteration: 382 \t--- Loss: 0.141\n",
      "Iteration: 383 \t--- Loss: 0.152\n",
      "Iteration: 384 \t--- Loss: 0.125\n",
      "Iteration: 385 \t--- Loss: 0.143\n",
      "Iteration: 386 \t--- Loss: 0.137\n",
      "Iteration: 387 \t--- Loss: 0.131\n",
      "Iteration: 388 \t--- Loss: 0.131\n",
      "Iteration: 389 \t--- Loss: 0.143\n",
      "Iteration: 390 \t--- Loss: 0.145\n",
      "Iteration: 391 \t--- Loss: 0.154\n",
      "Iteration: 392 \t--- Loss: 0.158\n",
      "Iteration: 393 \t--- Loss: 0.145\n",
      "Iteration: 394 \t--- Loss: 0.144\n",
      "Iteration: 395 \t--- Loss: 0.149\n",
      "Iteration: 396 \t--- Loss: 0.150\n",
      "Iteration: 397 \t--- Loss: 0.139\n",
      "Iteration: 398 \t--- Loss: 0.157\n",
      "Iteration: 399 \t--- Loss: 0.132\n",
      "Iteration: 400 \t--- Loss: 0.143\n",
      "Iteration: 401 \t--- Loss: 0.148\n",
      "Iteration: 402 \t--- Loss: 0.152\n",
      "Iteration: 403 \t--- Loss: 0.164\n",
      "Iteration: 404 \t--- Loss: 0.134\n",
      "Iteration: 405 \t--- Loss: 0.150\n",
      "Iteration: 406 \t--- Loss: 0.141\n",
      "Iteration: 407 \t--- Loss: 0.136\n",
      "Iteration: 408 \t--- Loss: 0.169\n",
      "Iteration: 409 \t--- Loss: 0.156\n",
      "Iteration: 410 \t--- Loss: 0.138\n",
      "Iteration: 411 \t--- Loss: 0.154\n",
      "Iteration: 412 \t--- Loss: 0.103\n",
      "Iteration: 413 \t--- Loss: 0.147\n",
      "Iteration: 414 \t--- Loss: 0.148\n",
      "Iteration: 415 \t--- Loss: 0.136\n",
      "Iteration: 416 \t--- Loss: 0.144\n",
      "Iteration: 417 \t--- Loss: 0.147\n",
      "Iteration: 418 \t--- Loss: 0.144\n",
      "Iteration: 419 \t--- Loss: 0.164\n",
      "Iteration: 420 \t--- Loss: 0.143\n",
      "Iteration: 421 \t--- Loss: 0.159\n",
      "Iteration: 422 \t--- Loss: 0.147\n",
      "Iteration: 423 \t--- Loss: 0.138\n",
      "Iteration: 424 \t--- Loss: 0.142\n",
      "Iteration: 425 \t--- Loss: 0.152\n",
      "Iteration: 426 \t--- Loss: 0.160\n",
      "Iteration: 427 \t--- Loss: 0.144\n",
      "Iteration: 428 \t--- Loss: 0.133\n",
      "Iteration: 429 \t--- Loss: 0.140\n",
      "Iteration: 430 \t--- Loss: 0.149\n",
      "Iteration: 431 \t--- Loss: 0.136\n",
      "Iteration: 432 \t--- Loss: 0.136\n",
      "Iteration: 433 \t--- Loss: 0.144\n",
      "Iteration: 434 \t--- Loss: 0.135\n",
      "Iteration: 435 \t--- Loss: 0.160\n",
      "Iteration: 436 \t--- Loss: 0.137\n",
      "Iteration: 437 \t--- Loss: 0.138\n",
      "Iteration: 438 \t--- Loss: 0.141\n",
      "Iteration: 439 \t--- Loss: 0.133\n",
      "Iteration: 440 \t--- Loss: 0.132\n",
      "Iteration: 441 \t--- Loss: 0.143\n",
      "Iteration: 442 \t--- Loss: 0.166\n",
      "Iteration: 443 \t--- Loss: 0.149\n",
      "Iteration: 444 \t--- Loss: 0.147\n",
      "Iteration: 445 \t--- Loss: 0.156\n",
      "Iteration: 446 \t--- Loss: 0.135\n",
      "Iteration: 447 \t--- Loss: 0.154\n",
      "Iteration: 448 \t--- Loss: 0.136\n",
      "Iteration: 449 \t--- Loss: 0.140\n",
      "Iteration: 450 \t--- Loss: 0.166\n",
      "Iteration: 451 \t--- Loss: 0.144\n",
      "Iteration: 452 \t--- Loss: 0.152\n",
      "Iteration: 453 \t--- Loss: 0.167\n",
      "Iteration: 454 \t--- Loss: 0.142\n",
      "Iteration: 455 \t--- Loss: 0.146\n",
      "Iteration: 456 \t--- Loss: 0.144\n",
      "Iteration: 457 \t--- Loss: 0.152\n",
      "Iteration: 458 \t--- Loss: 0.142\n",
      "Iteration: 459 \t--- Loss: 0.134\n",
      "Iteration: 460 \t--- Loss: 0.151\n",
      "Iteration: 461 \t--- Loss: 0.129\n",
      "Iteration: 462 \t--- Loss: 0.132\n",
      "Iteration: 463 \t--- Loss: 0.148\n",
      "Iteration: 464 \t--- Loss: 0.146\n",
      "Iteration: 465 \t--- Loss: 0.139\n",
      "Iteration: 466 \t--- Loss: 0.132\n",
      "Iteration: 467 \t--- Loss: 0.152\n",
      "Iteration: 468 \t--- Loss: 0.149\n",
      "Iteration: 469 \t--- Loss: 0.133\n",
      "Iteration: 470 \t--- Loss: 0.149\n",
      "Iteration: 471 \t--- Loss: 0.139\n",
      "Iteration: 472 \t--- Loss: 0.144\n",
      "Iteration: 473 \t--- Loss: 0.130\n",
      "Iteration: 474 \t--- Loss: 0.154\n",
      "Iteration: 475 \t--- Loss: 0.141\n",
      "Iteration: 476 \t--- Loss: 0.145\n",
      "Iteration: 477 \t--- Loss: 0.147\n",
      "Iteration: 478 \t--- Loss: 0.167\n",
      "Iteration: 479 \t--- Loss: 0.147\n",
      "Iteration: 480 \t--- Loss: 0.149\n",
      "Iteration: 481 \t--- Loss: 0.134\n",
      "Iteration: 482 \t--- Loss: 0.130\n",
      "Iteration: 483 \t--- Loss: 0.146\n",
      "Iteration: 484 \t--- Loss: 0.137\n",
      "Iteration: 485 \t--- Loss: 0.144\n",
      "Iteration: 486 \t--- Loss: 0.153\n",
      "Iteration: 487 \t--- Loss: 0.161\n",
      "Iteration: 488 \t--- Loss: 0.151\n",
      "Iteration: 489 \t--- Loss: 0.147\n",
      "Iteration: 490 \t--- Loss: 0.139\n",
      "Iteration: 491 \t--- Loss: 0.131\n",
      "Iteration: 492 \t--- Loss: 0.143\n",
      "Iteration: 493 \t--- Loss: 0.134\n",
      "Iteration: 494 \t--- Loss: 0.136\n",
      "Iteration: 495 \t--- Loss: 0.156\n",
      "Iteration: 496 \t--- Loss: 0.151\n",
      "Iteration: 497 \t--- Loss: 0.162\n",
      "Iteration: 498 \t--- Loss: 0.143\n",
      "Iteration: 499 \t--- Loss: 0.135\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.49s/it][Parallel(n_jobs=5)]: Done  51 tasks      | elapsed: 30.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 9/10 [00:13<00:01,  1.57s/it][Parallel(n_jobs=5)]: Done  52 tasks      | elapsed: 30.9min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [01:42<00:00, 102.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.040\n",
      "Iteration: 261 \t--- Loss: 0.041\n",
      "Iteration: 262 \t--- Loss: 0.041\n",
      "Iteration: 263 \t--- Loss: 0.041\n",
      "Iteration: 264 \t--- Loss: 0.043\n",
      "Iteration: 265 \t--- Loss: 0.041\n",
      "Iteration: 266 \t--- Loss: 0.041\n",
      "Iteration: 267 \t--- Loss: 0.043\n",
      "Iteration: 268 \t--- Loss: 0.043\n",
      "Iteration: 269 \t--- Loss: 0.039\n",
      "Iteration: 270 \t--- Loss: 0.041\n",
      "Iteration: 271 \t--- Loss: 0.041\n",
      "Iteration: 272 \t--- Loss: 0.042\n",
      "Iteration: 273 \t--- Loss: 0.042\n",
      "Iteration: 274 \t--- Loss: 0.042\n",
      "Iteration: 275 \t--- Loss: 0.042\n",
      "Iteration: 276 \t--- Loss: 0.039\n",
      "Iteration: 277 \t--- Loss: 0.044\n",
      "Iteration: 278 \t--- Loss: 0.041\n",
      "Iteration: 279 \t--- Loss: 0.041\n",
      "Iteration: 280 \t--- Loss: 0.039\n",
      "Iteration: 281 \t--- Loss: 0.041\n",
      "Iteration: 282 \t--- Loss: 0.042\n",
      "Iteration: 283 \t--- Loss: 0.042\n",
      "Iteration: 284 \t--- Loss: 0.047\n",
      "Iteration: 285 \t--- Loss: 0.044\n",
      "Iteration: 286 \t--- Loss: 0.041\n",
      "Iteration: 287 \t--- Loss: 0.042\n",
      "Iteration: 288 \t--- Loss: 0.041\n",
      "Iteration: 289 \t--- Loss: 0.041\n",
      "Iteration: 290 \t--- Loss: 0.044\n",
      "Iteration: 291 \t--- Loss: 0.040\n",
      "Iteration: 292 \t--- Loss: 0.042\n",
      "Iteration: 293 \t--- Loss: 0.044\n",
      "Iteration: 294 \t--- Loss: 0.040\n",
      "Iteration: 295 \t--- Loss: 0.040\n",
      "Iteration: 296 \t--- Loss: 0.041\n",
      "Iteration: 297 \t--- Loss: 0.043\n",
      "Iteration: 298 \t--- Loss: 0.043\n",
      "Iteration: 299 \t--- Loss: 0.041\n",
      "Iteration: 300 \t--- Loss: 0.040\n",
      "Iteration: 301 \t--- Loss: 0.041\n",
      "Iteration: 302 \t--- Loss: 0.041\n",
      "Iteration: 303 \t--- Loss: 0.044\n",
      "Iteration: 304 \t--- Loss: 0.042\n",
      "Iteration: 305 \t--- Loss: 0.041\n",
      "Iteration: 306 \t--- Loss: 0.043\n",
      "Iteration: 307 \t--- Loss: 0.041\n",
      "Iteration: 308 \t--- Loss: 0.038\n",
      "Iteration: 309 \t--- Loss: 0.041\n",
      "Iteration: 310 \t--- Loss: 0.041\n",
      "Iteration: 311 \t--- Loss: 0.041\n",
      "Iteration: 312 \t--- Loss: 0.043\n",
      "Iteration: 313 \t--- Loss: 0.041\n",
      "Iteration: 314 \t--- Loss: 0.040\n",
      "Iteration: 315 \t--- Loss: 0.045\n",
      "Iteration: 316 \t--- Loss: 0.038\n",
      "Iteration: 317 \t--- Loss: 0.041\n",
      "Iteration: 318 \t--- Loss: 0.043\n",
      "Iteration: 319 \t--- Loss: 0.041\n",
      "Iteration: 320 \t--- Loss: 0.042\n",
      "Iteration: 321 \t--- Loss: 0.041\n",
      "Iteration: 322 \t--- Loss: 0.042\n",
      "Iteration: 323 \t--- Loss: 0.041\n",
      "Iteration: 324 \t--- Loss: 0.043\n",
      "Iteration: 325 \t--- Loss: 0.042\n",
      "Iteration: 326 \t--- Loss: 0.042\n",
      "Iteration: 327 \t--- Loss: 0.044\n",
      "Iteration: 328 \t--- Loss: 0.042\n",
      "Iteration: 329 \t--- Loss: 0.041\n",
      "Iteration: 330 \t--- Loss: 0.042\n",
      "Iteration: 331 \t--- Loss: 0.041\n",
      "Iteration: 332 \t--- Loss: 0.041\n",
      "Iteration: 333 \t--- Loss: 0.042\n",
      "Iteration: 334 \t--- Loss: 0.040\n",
      "Iteration: 335 \t--- Loss: 0.041\n",
      "Iteration: 336 \t--- Loss: 0.042\n",
      "Iteration: 337 \t--- Loss: 0.041\n",
      "Iteration: 338 \t--- Loss: 0.043\n",
      "Iteration: 339 \t--- Loss: 0.041\n",
      "Iteration: 340 \t--- Loss: 0.041\n",
      "Iteration: 341 \t--- Loss: 0.041\n",
      "Iteration: 342 \t--- Loss: 0.044\n",
      "Iteration: 343 \t--- Loss: 0.042\n",
      "Iteration: 344 \t--- Loss: 0.044\n",
      "Iteration: 345 \t--- Loss: 0.041\n",
      "Iteration: 346 \t--- Loss: 0.038\n",
      "Iteration: 347 \t--- Loss: 0.040\n",
      "Iteration: 348 \t--- Loss: 0.042\n",
      "Iteration: 349 \t--- Loss: 0.043\n",
      "Iteration: 350 \t--- Loss: 0.043\n",
      "Iteration: 351 \t--- Loss: 0.043\n",
      "Iteration: 352 \t--- Loss: 0.043\n",
      "Iteration: 353 \t--- Loss: 0.040\n",
      "Iteration: 354 \t--- Loss: 0.042\n",
      "Iteration: 355 \t--- Loss: 0.044\n",
      "Iteration: 356 \t--- Loss: 0.040\n",
      "Iteration: 357 \t--- Loss: 0.043\n",
      "Iteration: 358 \t--- Loss: 0.041\n",
      "Iteration: 359 \t--- Loss: 0.044\n",
      "Iteration: 360 \t--- Loss: 0.043\n",
      "Iteration: 361 \t--- Loss: 0.042\n",
      "Iteration: 362 \t--- Loss: 0.041\n",
      "Iteration: 363 \t--- Loss: 0.043\n",
      "Iteration: 364 \t--- Loss: 0.043\n",
      "Iteration: 365 \t--- Loss: 0.042\n",
      "Iteration: 366 \t--- Loss: 0.042\n",
      "Iteration: 367 \t--- Loss: 0.041\n",
      "Iteration: 368 \t--- Loss: 0.042\n",
      "Iteration: 369 \t--- Loss: 0.041\n",
      "Iteration: 370 \t--- Loss: 0.041\n",
      "Iteration: 371 \t--- Loss: 0.044\n",
      "Iteration: 372 \t--- Loss: 0.043\n",
      "Iteration: 373 \t--- Loss: 0.043\n",
      "Iteration: 374 \t--- Loss: 0.042\n",
      "Iteration: 375 \t--- Loss: 0.042\n",
      "Iteration: 376 \t--- Loss: 0.042\n",
      "Iteration: 377 \t--- Loss: 0.043\n",
      "Iteration: 378 \t--- Loss: 0.043\n",
      "Iteration: 379 \t--- Loss: 0.039\n",
      "Iteration: 380 \t--- Loss: 0.041\n",
      "Iteration: 381 \t--- Loss: 0.041\n",
      "Iteration: 382 \t--- Loss: 0.041\n",
      "Iteration: 383 \t--- Loss: 0.044\n",
      "Iteration: 384 \t--- Loss: 0.042\n",
      "Iteration: 385 \t--- Loss: 0.040\n",
      "Iteration: 386 \t--- Loss: 0.044\n",
      "Iteration: 387 \t--- Loss: 0.041\n",
      "Iteration: 388 \t--- Loss: 0.042\n",
      "Iteration: 389 \t--- Loss: 0.041\n",
      "Iteration: 390 \t--- Loss: 0.040\n",
      "Iteration: 391 \t--- Loss: 0.043\n",
      "Iteration: 392 \t--- Loss: 0.045\n",
      "Iteration: 393 \t--- Loss: 0.044\n",
      "Iteration: 394 \t--- Loss: 0.042\n",
      "Iteration: 395 \t--- Loss: 0.041\n",
      "Iteration: 396 \t--- Loss: 0.043\n",
      "Iteration: 397 \t--- Loss: 0.041\n",
      "Iteration: 398 \t--- Loss: 0.045\n",
      "Iteration: 399 \t--- Loss: 0.042\n",
      "Iteration: 400 \t--- Loss: 0.045\n",
      "Iteration: 401 \t--- Loss: 0.040\n",
      "Iteration: 402 \t--- Loss: 0.043\n",
      "Iteration: 403 \t--- Loss: 0.043\n",
      "Iteration: 404 \t--- Loss: 0.043\n",
      "Iteration: 405 \t--- Loss: 0.044\n",
      "Iteration: 406 \t--- Loss: 0.039\n",
      "Iteration: 407 \t--- Loss: 0.044\n",
      "Iteration: 408 \t--- Loss: 0.043\n",
      "Iteration: 409 \t--- Loss: 0.042\n",
      "Iteration: 410 \t--- Loss: 0.043\n",
      "Iteration: 411 \t--- Loss: 0.041\n",
      "Iteration: 412 \t--- Loss: 0.045\n",
      "Iteration: 413 \t--- Loss: 0.043\n",
      "Iteration: 414 \t--- Loss: 0.041\n",
      "Iteration: 415 \t--- Loss: 0.039\n",
      "Iteration: 416 \t--- Loss: 0.045\n",
      "Iteration: 417 \t--- Loss: 0.041\n",
      "Iteration: 418 \t--- Loss: 0.041\n",
      "Iteration: 419 \t--- Loss: 0.044\n",
      "Iteration: 420 \t--- Loss: 0.040\n",
      "Iteration: 421 \t--- Loss: 0.046\n",
      "Iteration: 422 \t--- Loss: 0.043\n",
      "Iteration: 423 \t--- Loss: 0.042\n",
      "Iteration: 424 \t--- Loss: 0.042\n",
      "Iteration: 425 \t--- Loss: 0.038\n",
      "Iteration: 426 \t--- Loss: 0.040\n",
      "Iteration: 427 \t--- Loss: 0.042\n",
      "Iteration: 428 \t--- Loss: 0.041\n",
      "Iteration: 429 \t--- Loss: 0.041\n",
      "Iteration: 430 \t--- Loss: 0.041\n",
      "Iteration: 431 \t--- Loss: 0.040\n",
      "Iteration: 432 \t--- Loss: 0.041\n",
      "Iteration: 433 \t--- Loss: 0.045\n",
      "Iteration: 434 \t--- Loss: 0.043\n",
      "Iteration: 435 \t--- Loss: 0.043\n",
      "Iteration: 436 \t--- Loss: 0.043\n",
      "Iteration: 437 \t--- Loss: 0.042\n",
      "Iteration: 438 \t--- Loss: 0.040\n",
      "Iteration: 439 \t--- Loss: 0.042\n",
      "Iteration: 440 \t--- Loss: 0.040\n",
      "Iteration: 441 \t--- Loss: 0.044\n",
      "Iteration: 442 \t--- Loss: 0.042\n",
      "Iteration: 443 \t--- Loss: 0.039\n",
      "Iteration: 444 \t--- Loss: 0.043\n",
      "Iteration: 445 \t--- Loss: 0.044\n",
      "Iteration: 446 \t--- Loss: 0.042\n",
      "Iteration: 447 \t--- Loss: 0.043\n",
      "Iteration: 448 \t--- Loss: 0.044\n",
      "Iteration: 449 \t--- Loss: 0.042\n",
      "Iteration: 450 \t--- Loss: 0.042\n",
      "Iteration: 451 \t--- Loss: 0.042\n",
      "Iteration: 452 \t--- Loss: 0.043\n",
      "Iteration: 453 \t--- Loss: 0.041\n",
      "Iteration: 454 \t--- Loss: 0.040\n",
      "Iteration: 455 \t--- Loss: 0.042\n",
      "Iteration: 456 \t--- Loss: 0.043\n",
      "Iteration: 457 \t--- Loss: 0.040\n",
      "Iteration: 458 \t--- Loss: 0.045\n",
      "Iteration: 459 \t--- Loss: 0.041\n",
      "Iteration: 460 \t--- Loss: 0.043\n",
      "Iteration: 461 \t--- Loss: 0.042\n",
      "Iteration: 462 \t--- Loss: 0.039\n",
      "Iteration: 463 \t--- Loss: 0.042\n",
      "Iteration: 464 \t--- Loss: 0.043\n",
      "Iteration: 465 \t--- Loss: 0.042\n",
      "Iteration: 466 \t--- Loss: 0.045\n",
      "Iteration: 467 \t--- Loss: 0.041\n",
      "Iteration: 468 \t--- Loss: 0.043\n",
      "Iteration: 469 \t--- Loss: 0.041\n",
      "Iteration: 470 \t--- Loss: 0.042\n",
      "Iteration: 471 \t--- Loss: 0.044\n",
      "Iteration: 472 \t--- Loss: 0.044\n",
      "Iteration: 473 \t--- Loss: 0.044\n",
      "Iteration: 474 \t--- Loss: 0.043\n",
      "Iteration: 475 \t--- Loss: 0.038\n",
      "Iteration: 476 \t--- Loss: 0.040\n",
      "Iteration: 477 \t--- Loss: 0.040\n",
      "Iteration: 478 \t--- Loss: 0.042\n",
      "Iteration: 479 \t--- Loss: 0.042\n",
      "Iteration: 480 \t--- Loss: 0.042\n",
      "Iteration: 481 \t--- Loss: 0.042\n",
      "Iteration: 482 \t--- Loss: 0.038\n",
      "Iteration: 483 \t--- Loss: 0.042\n",
      "Iteration: 484 \t--- Loss: 0.040\n",
      "Iteration: 485 \t--- Loss: 0.044\n",
      "Iteration: 486 \t--- Loss: 0.043\n",
      "Iteration: 487 \t--- Loss: 0.045\n",
      "Iteration: 488 \t--- Loss: 0.043\n",
      "Iteration: 489 \t--- Loss: 0.043\n",
      "Iteration: 490 \t--- Loss: 0.043\n",
      "Iteration: 491 \t--- Loss: 0.041\n",
      "Iteration: 492 \t--- Loss: 0.043\n",
      "Iteration: 493 \t--- Loss: 0.041\n",
      "Iteration: 494 \t--- Loss: 0.044\n",
      "Iteration: 495 \t--- Loss: 0.040\n",
      "Iteration: 496 \t--- Loss: 0.041\n",
      "Iteration: 497 \t--- Loss: 0.041\n",
      "Iteration: 498 \t--- Loss: 0.041\n",
      "Iteration: 499 \t--- Loss: 0.044\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.58s/it][Parallel(n_jobs=5)]: Done  53 tasks      | elapsed: 31.3min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.086\n",
      "Iteration: 1 \t--- Loss: 0.092\n",
      "Iteration: 2 \t--- Loss: 0.087\n",
      "Iteration: 3 \t--- Loss: 0.087\n",
      "Iteration: 4 \t--- Loss: 0.085\n",
      "Iteration: 5 \t--- Loss: 0.087\n",
      "Iteration: 6 \t--- Loss: 0.089\n",
      "Iteration: 7 \t--- Loss: 0.091\n",
      "Iteration: 8 \t--- Loss: 0.084\n",
      "Iteration: 9 \t--- Loss: 0.087\n",
      "Iteration: 10 \t--- Loss: 0.080\n",
      "Iteration: 11 \t--- Loss: 0.087\n",
      "Iteration: 12 \t--- Loss: 0.084\n",
      "Iteration: 13 \t--- Loss: 0.089\n",
      "Iteration: 14 \t--- Loss: 0.088\n",
      "Iteration: 15 \t--- Loss: 0.082\n",
      "Iteration: 16 \t--- Loss: 0.087\n",
      "Iteration: 17 \t--- Loss: 0.087\n",
      "Iteration: 18 \t--- Loss: 0.081\n",
      "Iteration: 19 \t--- Loss: 0.083\n",
      "Iteration: 20 \t--- Loss: 0.082\n",
      "Iteration: 21 \t--- Loss: 0.087\n",
      "Iteration: 22 \t--- Loss: 0.086\n",
      "Iteration: 23 \t--- Loss: 0.084\n",
      "Iteration: 24 \t--- Loss: 0.079\n",
      "Iteration: 25 \t--- Loss: 0.088\n",
      "Iteration: 26 \t--- Loss: 0.086\n",
      "Iteration: 27 \t--- Loss: 0.086\n",
      "Iteration: 28 \t--- Loss: 0.080\n",
      "Iteration: 29 \t--- Loss: 0.089\n",
      "Iteration: 30 \t--- Loss: 0.081\n",
      "Iteration: 31 \t--- Loss: 0.082\n",
      "Iteration: 32 \t--- Loss: 0.090\n",
      "Iteration: 33 \t--- Loss: 0.083\n",
      "Iteration: 34 \t--- Loss: 0.075\n",
      "Iteration: 35 \t--- Loss: 0.087\n",
      "Iteration: 36 \t--- Loss: 0.084\n",
      "Iteration: 37 \t--- Loss: 0.078\n",
      "Iteration: 38 \t--- Loss: 0.082\n",
      "Iteration: 39 \t--- Loss: 0.084\n",
      "Iteration: 40 \t--- Loss: 0.083\n",
      "Iteration: 41 \t--- Loss: 0.083\n",
      "Iteration: 42 \t--- Loss: 0.082\n",
      "Iteration: 43 \t--- Loss: 0.090\n",
      "Iteration: 44 \t--- Loss: 0.079\n",
      "Iteration: 45 \t--- Loss: 0.085\n",
      "Iteration: 46 \t--- Loss: 0.082\n",
      "Iteration: 47 \t--- Loss: 0.086\n",
      "Iteration: 48 \t--- Loss: 0.083\n",
      "Iteration: 49 \t--- Loss: 0.087\n",
      "Iteration: 50 \t--- Loss: 0.079\n",
      "Iteration: 51 \t--- Loss: 0.085\n",
      "Iteration: 52 \t--- Loss: 0.085\n",
      "Iteration: 53 \t--- Loss: 0.083\n",
      "Iteration: 54 \t--- Loss: 0.084\n",
      "Iteration: 55 \t--- Loss: 0.075\n",
      "Iteration: 56 \t--- Loss: 0.076\n",
      "Iteration: 57 \t--- Loss: 0.055\n",
      "Iteration: 58 \t--- Loss: 0.048\n",
      "Iteration: 59 \t--- Loss: 0.047\n",
      "Iteration: 60 \t--- Loss: 0.042\n",
      "Iteration: 61 \t--- Loss: 0.044\n",
      "Iteration: 62 \t--- Loss: 0.044\n",
      "Iteration: 63 \t--- Loss: 0.046\n",
      "Iteration: 64 \t--- Loss: 0.043\n",
      "Iteration: 65 \t--- Loss: 0.044\n",
      "Iteration: 66 \t--- Loss: 0.043\n",
      "Iteration: 67 \t--- Loss: 0.042\n",
      "Iteration: 68 \t--- Loss: 0.043\n",
      "Iteration: 69 \t--- Loss: 0.042\n",
      "Iteration: 70 \t--- Loss: 0.040\n",
      "Iteration: 71 \t--- Loss: 0.039\n",
      "Iteration: 72 \t--- Loss: 0.041\n",
      "Iteration: 73 \t--- Loss: 0.040\n",
      "Iteration: 74 \t--- Loss: 0.037\n",
      "Iteration: 75 \t--- Loss: 0.048\n",
      "Iteration: 76 \t--- Loss: 0.050\n",
      "Iteration: 77 \t--- Loss: 0.119\n",
      "Iteration: 78 \t--- Loss: 0.083\n",
      "Iteration: 79 \t--- Loss: 0.083\n",
      "Iteration: 80 \t--- Loss: 0.086\n",
      "Iteration: 81 \t--- Loss: 0.087\n",
      "Iteration: 82 \t--- Loss: 0.085\n",
      "Iteration: 83 \t--- Loss: 0.087\n",
      "Iteration: 84 \t--- Loss: 0.085\n",
      "Iteration: 85 \t--- Loss: 0.088\n",
      "Iteration: 86 \t--- Loss: 0.087\n",
      "Iteration: 87 \t--- Loss: 0.082\n",
      "Iteration: 88 \t--- Loss: 0.086\n",
      "Iteration: 89 \t--- Loss: 0.082\n",
      "Iteration: 90 \t--- Loss: 0.082\n",
      "Iteration: 91 \t--- Loss: 0.089\n",
      "Iteration: 92 \t--- Loss: 0.086\n",
      "Iteration: 93 \t--- Loss: 0.088\n",
      "Iteration: 94 \t--- Loss: 0.092\n",
      "Iteration: 95 \t--- Loss: 0.089\n",
      "Iteration: 96 \t--- Loss: 0.084\n",
      "Iteration: 97 \t--- Loss: 0.085\n",
      "Iteration: 98 \t--- Loss: 0.063\n",
      "Iteration: 99 \t--- Loss: 0.048\n",
      "Iteration: 100 \t--- Loss: 0.038\n",
      "Iteration: 101 \t--- Loss: 0.034\n",
      "Iteration: 102 \t--- Loss: 0.033\n",
      "Iteration: 103 \t--- Loss: 0.032\n",
      "Iteration: 104 \t--- Loss: 0.031\n",
      "Iteration: 105 \t--- Loss: 0.035\n",
      "Iteration: 106 \t--- Loss: 0.030\n",
      "Iteration: 107 \t--- Loss: 0.028\n",
      "Iteration: 108 \t--- Loss: 0.031\n",
      "Iteration: 109 \t--- Loss: 0.033\n",
      "Iteration: 110 \t--- Loss: 0.031\n",
      "Iteration: 111 \t--- Loss: 0.032\n",
      "Iteration: 112 \t--- Loss: 0.030\n",
      "Iteration: 113 \t--- Loss: 0.034\n",
      "Iteration: 114 \t--- Loss: 0.033\n",
      "Iteration: 115 \t--- Loss: 0.031\n",
      "Iteration: 116 \t--- Loss: 0.030\n",
      "Iteration: 117 \t--- Loss: 0.035\n",
      "Iteration: 118 \t--- Loss: 0.035\n",
      "Iteration: 119 \t--- Loss: 0.049\n",
      "Iteration: 120 \t--- Loss: 0.099\n",
      "Iteration: 121 \t--- Loss: 0.028\n",
      "Iteration: 122 \t--- Loss: 0.032\n",
      "Iteration: 123 \t--- Loss: 0.034\n",
      "Iteration: 124 \t--- Loss: 0.029\n",
      "Iteration: 125 \t--- Loss: 0.053\n",
      "Iteration: 126 \t--- Loss: 0.098\n",
      "Iteration: 127 \t--- Loss: 0.027\n",
      "Iteration: 128 \t--- Loss: 0.034\n",
      "Iteration: 129 \t--- Loss: 0.043\n",
      "Iteration: 130 \t--- Loss: 0.083\n",
      "Iteration: 131 \t--- Loss: 0.118\n",
      "Iteration: 132 \t--- Loss: 0.121\n",
      "Iteration: 133 \t--- Loss: 0.105\n",
      "Iteration: 134 \t--- Loss: 0.042\n",
      "Iteration: 135 \t--- Loss: 0.025\n",
      "Iteration: 136 \t--- Loss: 0.023\n",
      "Iteration: 137 \t--- Loss: 0.028\n",
      "Iteration: 138 \t--- Loss: 0.030\n",
      "Iteration: 139 \t--- Loss: 0.028\n",
      "Iteration: 140 \t--- Loss: 0.036\n",
      "Iteration: 141 \t--- Loss: 0.036\n",
      "Iteration: 142 \t--- Loss: 0.043\n",
      "Iteration: 143 \t--- Loss: 0.032\n",
      "Iteration: 144 \t--- Loss: 0.041\n",
      "Iteration: 145 \t--- Loss: 0.043\n",
      "Iteration: 146 \t--- Loss: 0.072\n",
      "Iteration: 147 \t--- Loss: 0.018\n",
      "Iteration: 148 \t--- Loss: 0.018\n",
      "Iteration: 149 \t--- Loss: 0.018\n",
      "Iteration: 150 \t--- Loss: 0.024\n",
      "Iteration: 151 \t--- Loss: 0.024\n",
      "Iteration: 152 \t--- Loss: 0.031\n",
      "Iteration: 153 \t--- Loss: 0.041\n",
      "Iteration: 154 \t--- Loss: 0.028\n",
      "Iteration: 155 \t--- Loss: 0.035\n",
      "Iteration: 156 \t--- Loss: 0.028\n",
      "Iteration: 157 \t--- Loss: 0.035\n",
      "Iteration: 158 \t--- Loss: 0.022\n",
      "Iteration: 159 \t--- Loss: 0.023\n",
      "Iteration: 160 \t--- Loss: 0.030\n",
      "Iteration: 161 \t--- Loss: 0.037\n",
      "Iteration: 162 \t--- Loss: 0.022\n",
      "Iteration: 163 \t--- Loss: 0.021\n",
      "Iteration: 164 \t--- Loss: 0.025\n",
      "Iteration: 165 \t--- Loss: 0.031\n",
      "Iteration: 166 \t--- Loss: 0.025\n",
      "Iteration: 167 \t--- Loss: 0.027\n",
      "Iteration: 168 \t--- Loss: 0.036\n",
      "Iteration: 169 \t--- Loss: 0.077\n",
      "Iteration: 170 \t--- Loss: 0.011\n",
      "Iteration: 171 \t--- Loss: 0.014\n",
      "Iteration: 172 \t--- Loss: 0.012\n",
      "Iteration: 173 \t--- Loss: 0.016\n",
      "Iteration: 174 \t--- Loss: 0.012\n",
      "Iteration: 175 \t--- Loss: 0.018\n",
      "Iteration: 176 \t--- Loss: 0.021\n",
      "Iteration: 177 \t--- Loss: 0.026\n",
      "Iteration: 178 \t--- Loss: 0.047\n",
      "Iteration: 179 \t--- Loss: 0.011\n",
      "Iteration: 180 \t--- Loss: 0.010\n",
      "Iteration: 181 \t--- Loss: 0.014\n",
      "Iteration: 182 \t--- Loss: 0.013\n",
      "Iteration: 183 \t--- Loss: 0.016\n",
      "Iteration: 184 \t--- Loss: 0.019\n",
      "Iteration: 185 \t--- Loss: 0.020\n",
      "Iteration: 186 \t--- Loss: 0.021\n",
      "Iteration: 187 \t--- Loss: 0.023\n",
      "Iteration: 188 \t--- Loss: 0.028\n",
      "Iteration: 189 \t--- Loss: 0.028\n",
      "Iteration: 190 \t--- Loss: 0.040\n",
      "Iteration: 191 \t--- Loss: 0.013\n",
      "Iteration: 192 \t--- Loss: 0.016\n",
      "Iteration: 193 \t--- Loss: 0.023\n",
      "Iteration: 194 \t--- Loss: 0.039\n",
      "Iteration: 195 \t--- Loss: 0.015\n",
      "Iteration: 196 \t--- Loss: 0.014\n",
      "Iteration: 197 \t--- Loss: 0.015\n",
      "Iteration: 198 \t--- Loss: 0.020\n",
      "Iteration: 199 \t--- Loss: 0.017\n",
      "Iteration: 200 \t--- Loss: 0.020\n",
      "Iteration: 201 \t--- Loss: 0.013\n",
      "Iteration: 202 \t--- Loss: 0.010\n",
      "Iteration: 203 \t--- Loss: 0.012\n",
      "Iteration: 204 \t--- Loss: 0.014\n",
      "Iteration: 205 \t--- Loss: 0.013\n",
      "Iteration: 206 \t--- Loss: 0.016\n",
      "Iteration: 207 \t--- Loss: 0.012\n",
      "Iteration: 208 \t--- Loss: 0.009\n",
      "Iteration: 209 \t--- Loss: 0.010\n",
      "Iteration: 210 \t--- Loss: 0.009\n",
      "Iteration: 211 \t--- Loss: 0.010\n",
      "Iteration: 212 \t--- Loss: 0.010\n",
      "Iteration: 213 \t--- Loss: 0.011\n",
      "Iteration: 214 \t--- Loss: 0.012\n",
      "Iteration: 215 \t--- Loss: 0.010\n",
      "Iteration: 216 \t--- Loss: 0.010\n",
      "Iteration: 217 \t--- Loss: 0.010\n",
      "Iteration: 218 \t--- Loss: 0.012\n",
      "Iteration: 219 \t--- Loss: 0.011\n",
      "Iteration: 220 \t--- Loss: 0.013\n",
      "Iteration: 221 \t--- Loss: 0.010\n",
      "Iteration: 222 \t--- Loss: 0.010\n",
      "Iteration: 223 \t--- Loss: 0.008\n",
      "Iteration: 224 \t--- Loss: 0.007\n",
      "Iteration: 225 \t--- Loss: 0.007\n",
      "Iteration: 226 \t--- Loss: 0.007\n",
      "Iteration: 227 \t--- Loss: 0.007\n",
      "Iteration: 228 \t--- Loss: 0.006\n",
      "Iteration: 229 \t--- Loss: 0.006\n",
      "Iteration: 230 \t--- Loss: 0.005\n",
      "Iteration: 231 \t--- Loss: 0.005\n",
      "Iteration: 232 \t--- Loss: 0.005\n",
      "Iteration: 233 \t--- Loss: 0.006\n",
      "Iteration: 234 \t--- Loss: 0.006\n",
      "Iteration: 235 \t--- Loss: 0.006\n",
      "Iteration: 236 \t--- Loss: 0.006\n",
      "Iteration: 237 \t--- Loss: 0.007\n",
      "Iteration: 238 \t--- Loss: 0.007\n",
      "Iteration: 239 \t--- Loss: 0.007\n",
      "Iteration: 240 \t--- Loss: 0.007\n",
      "Iteration: 241 \t--- Loss: 0.008\n",
      "Iteration: 242 \t--- Loss: 0.008\n",
      "Iteration: 243 \t--- Loss: 0.008\n",
      "Iteration: 244 \t--- Loss: 0.008\n",
      "Iteration: 245 \t--- Loss: 0.009\n",
      "Iteration: 246 \t--- Loss: 0.009\n",
      "Iteration: 247 \t--- Loss: 0.007\n",
      "Iteration: 248 \t--- Loss: 0.008\n",
      "Iteration: 249 \t--- Loss: 0.007\n",
      "Iteration: 250 \t--- Loss: 0.007\n",
      "Iteration: 251 \t--- Loss: 0.005\n",
      "Iteration: 252 \t--- Loss: 0.005\n",
      "Iteration: 253 \t--- Loss: 0.005\n",
      "Iteration: 254 \t--- Loss: 0.004\n",
      "Iteration: 255 \t--- Loss: 0.005\n",
      "Iteration: 256 \t--- Loss: 0.004\n",
      "Iteration: 257 \t--- Loss: 0.005\n",
      "Iteration: 258 \t--- Loss: 0.004\n",
      "Iteration: 259 \t--- Loss: 0.005Iteration: 0 \t--- Loss: 0.399\n",
      "Iteration: 1 \t--- Loss: 0.397\n",
      "Iteration: 2 \t--- Loss: 0.369\n",
      "Iteration: 3 \t--- Loss: 0.365\n",
      "Iteration: 4 \t--- Loss: 0.379\n",
      "Iteration: 5 \t--- Loss: 0.361\n",
      "Iteration: 6 \t--- Loss: 0.353\n",
      "Iteration: 7 \t--- Loss: 0.356\n",
      "Iteration: 8 \t--- Loss: 0.354\n",
      "Iteration: 9 \t--- Loss: 0.341\n",
      "Iteration: 10 \t--- Loss: 0.354\n",
      "Iteration: 11 \t--- Loss: 0.351\n",
      "Iteration: 12 \t--- Loss: 0.343\n",
      "Iteration: 13 \t--- Loss: 0.337\n",
      "Iteration: 14 \t--- Loss: 0.330\n",
      "Iteration: 15 \t--- Loss: 0.331\n",
      "Iteration: 16 \t--- Loss: 0.324\n",
      "Iteration: 17 \t--- Loss: 0.324\n",
      "Iteration: 18 \t--- Loss: 0.315\n",
      "Iteration: 19 \t--- Loss: 0.335\n",
      "Iteration: 20 \t--- Loss: 0.300\n",
      "Iteration: 21 \t--- Loss: 0.318\n",
      "Iteration: 22 \t--- Loss: 0.316\n",
      "Iteration: 23 \t--- Loss: 0.309\n",
      "Iteration: 24 \t--- Loss: 0.311\n",
      "Iteration: 25 \t--- Loss: 0.304\n",
      "Iteration: 26 \t--- Loss: 0.322\n",
      "Iteration: 27 \t--- Loss: 0.295\n",
      "Iteration: 28 \t--- Loss: 0.275\n",
      "Iteration: 29 \t--- Loss: 0.267\n",
      "Iteration: 30 \t--- Loss: 0.274\n",
      "Iteration: 31 \t--- Loss: 0.276\n",
      "Iteration: 32 \t--- Loss: 0.258\n",
      "Iteration: 33 \t--- Loss: 0.256\n",
      "Iteration: 34 \t--- Loss: 0.264\n",
      "Iteration: 35 \t--- Loss: 0.269\n",
      "Iteration: 36 \t--- Loss: 0.293\n",
      "Iteration: 37 \t--- Loss: 0.320\n",
      "Iteration: 38 \t--- Loss: 0.284\n",
      "Iteration: 39 \t--- Loss: 0.255\n",
      "Iteration: 40 \t--- Loss: 0.271\n",
      "Iteration: 41 \t--- Loss: 0.298\n",
      "Iteration: 42 \t--- Loss: 0.310\n",
      "Iteration: 43 \t--- Loss: 0.308\n",
      "Iteration: 44 \t--- Loss: 0.335\n",
      "Iteration: 45 \t--- Loss: 0.322\n",
      "Iteration: 46 \t--- Loss: 0.260\n",
      "Iteration: 47 \t--- Loss: 0.255\n",
      "Iteration: 48 \t--- Loss: 0.259\n",
      "Iteration: 49 \t--- Loss: 0.231\n",
      "Iteration: 50 \t--- Loss: 0.225\n",
      "Iteration: 51 \t--- Loss: 0.229\n",
      "Iteration: 52 \t--- Loss: 0.237\n",
      "Iteration: 53 \t--- Loss: 0.317\n",
      "Iteration: 54 \t--- Loss: 0.341\n",
      "Iteration: 55 \t--- Loss: 0.348\n",
      "Iteration: 56 \t--- Loss: 0.322\n",
      "Iteration: 57 \t--- Loss: 0.342\n",
      "Iteration: 58 \t--- Loss: 0.328\n",
      "Iteration: 59 \t--- Loss: 0.338\n",
      "Iteration: 60 \t--- Loss: 0.321\n",
      "Iteration: 61 \t--- Loss: 0.327\n",
      "Iteration: 62 \t--- Loss: 0.320\n",
      "Iteration: 63 \t--- Loss: 0.320\n",
      "Iteration: 64 \t--- Loss: 0.300\n",
      "Iteration: 65 \t--- Loss: 0.314\n",
      "Iteration: 66 \t--- Loss: 0.308\n",
      "Iteration: 67 \t--- Loss: 0.291\n",
      "Iteration: 68 \t--- Loss: 0.297\n",
      "Iteration: 69 \t--- Loss: 0.290\n",
      "Iteration: 70 \t--- Loss: 0.292\n",
      "Iteration: 71 \t--- Loss: 0.296\n",
      "Iteration: 72 \t--- Loss: 0.289\n",
      "Iteration: 73 \t--- Loss: 0.286\n",
      "Iteration: 74 \t--- Loss: 0.272\n",
      "Iteration: 75 \t--- Loss: 0.285\n",
      "Iteration: 76 \t--- Loss: 0.266\n",
      "Iteration: 77 \t--- Loss: 0.265\n",
      "Iteration: 78 \t--- Loss: 0.268\n",
      "Iteration: 79 \t--- Loss: 0.239\n",
      "Iteration: 80 \t--- Loss: 0.212\n",
      "Iteration: 81 \t--- Loss: 0.191\n",
      "Iteration: 82 \t--- Loss: 0.192\n",
      "Iteration: 83 \t--- Loss: 0.202\n",
      "Iteration: 84 \t--- Loss: 0.190\n",
      "Iteration: 85 \t--- Loss: 0.188\n",
      "Iteration: 86 \t--- Loss: 0.198\n",
      "Iteration: 87 \t--- Loss: 0.189\n",
      "Iteration: 88 \t--- Loss: 0.186\n",
      "Iteration: 89 \t--- Loss: 0.206\n",
      "Iteration: 90 \t--- Loss: 0.177\n",
      "Iteration: 91 \t--- Loss: 0.190\n",
      "Iteration: 92 \t--- Loss: 0.191\n",
      "Iteration: 93 \t--- Loss: 0.200\n",
      "Iteration: 94 \t--- Loss: 0.191\n",
      "Iteration: 95 \t--- Loss: 0.204\n",
      "Iteration: 96 \t--- Loss: 0.208\n",
      "Iteration: 97 \t--- Loss: 0.224\n",
      "Iteration: 98 \t--- Loss: 0.247\n",
      "Iteration: 99 \t--- Loss: 0.199\n",
      "Iteration: 100 \t--- Loss: 0.181\n",
      "Iteration: 101 \t--- Loss: 0.198\n",
      "Iteration: 102 \t--- Loss: 0.224\n",
      "Iteration: 103 \t--- Loss: 0.266\n",
      "Iteration: 104 \t--- Loss: 0.210\n",
      "Iteration: 105 \t--- Loss: 0.178\n",
      "Iteration: 106 \t--- Loss: 0.177\n",
      "Iteration: 107 \t--- Loss: 0.173\n",
      "Iteration: 108 \t--- Loss: 0.178\n",
      "Iteration: 109 \t--- Loss: 0.189\n",
      "Iteration: 110 \t--- Loss: 0.196\n",
      "Iteration: 111 \t--- Loss: 0.222\n",
      "Iteration: 112 \t--- Loss: 0.274\n",
      "Iteration: 113 \t--- Loss: 0.230\n",
      "Iteration: 114 \t--- Loss: 0.177\n",
      "Iteration: 115 \t--- Loss: 0.205\n",
      "Iteration: 116 \t--- Loss: 0.243\n",
      "Iteration: 117 \t--- Loss: 0.176\n",
      "Iteration: 118 \t--- Loss: 0.177\n",
      "Iteration: 119 \t--- Loss: 0.181\n",
      "Iteration: 120 \t--- Loss: 0.155\n",
      "Iteration: 121 \t--- Loss: 0.154\n",
      "Iteration: 122 \t--- Loss: 0.163\n",
      "Iteration: 123 \t--- Loss: 0.154\n",
      "Iteration: 124 \t--- Loss: 0.150\n",
      "Iteration: 125 \t--- Loss: 0.156\n",
      "Iteration: 126 \t--- Loss: 0.168\n",
      "Iteration: 127 \t--- Loss: 0.182\n",
      "Iteration: 128 \t--- Loss: 0.162\n",
      "Iteration: 129 \t--- Loss: 0.183\n",
      "Iteration: 130 \t--- Loss: 0.160\n",
      "Iteration: 131 \t--- Loss: 0.169\n",
      "Iteration: 132 \t--- Loss: 0.183\n",
      "Iteration: 133 \t--- Loss: 0.191\n",
      "Iteration: 134 \t--- Loss: 0.158\n",
      "Iteration: 135 \t--- Loss: 0.150\n",
      "Iteration: 136 \t--- Loss: 0.153\n",
      "Iteration: 137 \t--- Loss: 0.177\n",
      "Iteration: 138 \t--- Loss: 0.185\n",
      "Iteration: 139 \t--- Loss: 0.187\n",
      "Iteration: 140 \t--- Loss: 0.154\n",
      "Iteration: 141 \t--- Loss: 0.150\n",
      "Iteration: 142 \t--- Loss: 0.137\n",
      "Iteration: 143 \t--- Loss: 0.153\n",
      "Iteration: 144 \t--- Loss: 0.160\n",
      "Iteration: 145 \t--- Loss: 0.165\n",
      "Iteration: 146 \t--- Loss: 0.162\n",
      "Iteration: 147 \t--- Loss: 0.149\n",
      "Iteration: 148 \t--- Loss: 0.158\n",
      "Iteration: 149 \t--- Loss: 0.149\n",
      "Iteration: 150 \t--- Loss: 0.148\n",
      "Iteration: 151 \t--- Loss: 0.140\n",
      "Iteration: 152 \t--- Loss: 0.152\n",
      "Iteration: 153 \t--- Loss: 0.143\n",
      "Iteration: 154 \t--- Loss: 0.157\n",
      "Iteration: 155 \t--- Loss: 0.156\n",
      "Iteration: 156 \t--- Loss: 0.159\n",
      "Iteration: 157 \t--- Loss: 0.146\n",
      "Iteration: 158 \t--- Loss: 0.134\n",
      "Iteration: 159 \t--- Loss: 0.147\n",
      "Iteration: 160 \t--- Loss: 0.140\n",
      "Iteration: 161 \t--- Loss: 0.159\n",
      "Iteration: 162 \t--- Loss: 0.137\n",
      "Iteration: 163 \t--- Loss: 0.138\n",
      "Iteration: 164 \t--- Loss: 0.142\n",
      "Iteration: 165 \t--- Loss: 0.149\n",
      "Iteration: 166 \t--- Loss: 0.145\n",
      "Iteration: 167 \t--- Loss: 0.134\n",
      "Iteration: 168 \t--- Loss: 0.153\n",
      "Iteration: 169 \t--- Loss: 0.145\n",
      "Iteration: 170 \t--- Loss: 0.144\n",
      "Iteration: 171 \t--- Loss: 0.152\n",
      "Iteration: 172 \t--- Loss: 0.143\n",
      "Iteration: 173 \t--- Loss: 0.151\n",
      "Iteration: 174 \t--- Loss: 0.157\n",
      "Iteration: 175 \t--- Loss: 0.152\n",
      "Iteration: 176 \t--- Loss: 0.156\n",
      "Iteration: 177 \t--- Loss: 0.144\n",
      "Iteration: 178 \t--- Loss: 0.141\n",
      "Iteration: 179 \t--- Loss: 0.130\n",
      "Iteration: 180 \t--- Loss: 0.135\n",
      "Iteration: 181 \t--- Loss: 0.132\n",
      "Iteration: 182 \t--- Loss: 0.142\n",
      "Iteration: 183 \t--- Loss: 0.146\n",
      "Iteration: 184 \t--- Loss: 0.149\n",
      "Iteration: 185 \t--- Loss: 0.137\n",
      "Iteration: 186 \t--- Loss: 0.139\n",
      "Iteration: 187 \t--- Loss: 0.160\n",
      "Iteration: 188 \t--- Loss: 0.146\n",
      "Iteration: 189 \t--- Loss: 0.131\n",
      "Iteration: 190 \t--- Loss: 0.132\n",
      "Iteration: 191 \t--- Loss: 0.134\n",
      "Iteration: 192 \t--- Loss: 0.140\n",
      "Iteration: 193 \t--- Loss: 0.129\n",
      "Iteration: 194 \t--- Loss: 0.137\n",
      "Iteration: 195 \t--- Loss: 0.129\n",
      "Iteration: 196 \t--- Loss: 0.142\n",
      "Iteration: 197 \t--- Loss: 0.147\n",
      "Iteration: 198 \t--- Loss: 0.129\n",
      "Iteration: 199 \t--- Loss: 0.138\n",
      "Iteration: 200 \t--- Loss: 0.135\n",
      "Iteration: 201 \t--- Loss: 0.137\n",
      "Iteration: 202 \t--- Loss: 0.140\n",
      "Iteration: 203 \t--- Loss: 0.144\n",
      "Iteration: 204 \t--- Loss: 0.159\n",
      "Iteration: 205 \t--- Loss: 0.135\n",
      "Iteration: 206 \t--- Loss: 0.130\n",
      "Iteration: 207 \t--- Loss: 0.136\n",
      "Iteration: 208 \t--- Loss: 0.135\n",
      "Iteration: 209 \t--- Loss: 0.135\n",
      "Iteration: 210 \t--- Loss: 0.137\n",
      "Iteration: 211 \t--- Loss: 0.139\n",
      "Iteration: 212 \t--- Loss: 0.135\n",
      "Iteration: 213 \t--- Loss: 0.131\n",
      "Iteration: 214 \t--- Loss: 0.129\n",
      "Iteration: 215 \t--- Loss: 0.132\n",
      "Iteration: 216 \t--- Loss: 0.136\n",
      "Iteration: 217 \t--- Loss: 0.138\n",
      "Iteration: 218 \t--- Loss: 0.134\n",
      "Iteration: 219 \t--- Loss: 0.128\n",
      "Iteration: 220 \t--- Loss: 0.133\n",
      "Iteration: 221 \t--- Loss: 0.146\n",
      "Iteration: 222 \t--- Loss: 0.120\n",
      "Iteration: 223 \t--- Loss: 0.137\n",
      "Iteration: 224 \t--- Loss: 0.139\n",
      "Iteration: 225 \t--- Loss: 0.141\n",
      "Iteration: 226 \t--- Loss: 0.150\n",
      "Iteration: 227 \t--- Loss: 0.133\n",
      "Iteration: 228 \t--- Loss: 0.121\n",
      "Iteration: 229 \t--- Loss: 0.129\n",
      "Iteration: 230 \t--- Loss: 0.135\n",
      "Iteration: 231 \t--- Loss: 0.138\n",
      "Iteration: 232 \t--- Loss: 0.138\n",
      "Iteration: 233 \t--- Loss: 0.130\n",
      "Iteration: 234 \t--- Loss: 0.133\n",
      "Iteration: 235 \t--- Loss: 0.131\n",
      "Iteration: 236 \t--- Loss: 0.132\n",
      "Iteration: 237 \t--- Loss: 0.139\n",
      "Iteration: 238 \t--- Loss: 0.135\n",
      "Iteration: 239 \t--- Loss: 0.139\n",
      "Iteration: 240 \t--- Loss: 0.135\n",
      "Iteration: 241 \t--- Loss: 0.125\n",
      "Iteration: 242 \t--- Loss: 0.128\n",
      "Iteration: 243 \t--- Loss: 0.130\n",
      "Iteration: 244 \t--- Loss: 0.137\n",
      "Iteration: 245 \t--- Loss: 0.133\n",
      "Iteration: 246 \t--- Loss: 0.143\n",
      "Iteration: 247 \t--- Loss: 0.120\n",
      "Iteration: 248 \t--- Loss: 0.131\n",
      "Iteration: 249 \t--- Loss: 0.128\n",
      "Iteration: 250 \t--- Loss: 0.123\n",
      "Iteration: 251 \t--- Loss: 0.131\n",
      "Iteration: 252 \t--- Loss: 0.145\n",
      "Iteration: 253 \t--- Loss: 0.131\n",
      "Iteration: 254 \t--- Loss: 0.127\n",
      "Iteration: 255 \t--- Loss: 0.130\n",
      "Iteration: 256 \t--- Loss: 0.130\n",
      "Iteration: 257 \t--- Loss: 0.124\n",
      "Iteration: 258 \t--- Loss: 0.131\n",
      "Iteration: 259 \t--- Loss: 0.134Iteration: 0 \t--- Loss: 0.959\n",
      "Iteration: 1 \t--- Loss: 0.880\n",
      "Iteration: 2 \t--- Loss: 0.800\n",
      "Iteration: 3 \t--- Loss: 0.741\n",
      "Iteration: 4 \t--- Loss: 0.695\n",
      "Iteration: 5 \t--- Loss: 0.658\n",
      "Iteration: 6 \t--- Loss: 0.628\n",
      "Iteration: 7 \t--- Loss: 0.606\n",
      "Iteration: 8 \t--- Loss: 0.581\n",
      "Iteration: 9 \t--- Loss: 0.579\n",
      "Iteration: 10 \t--- Loss: 0.565\n",
      "Iteration: 11 \t--- Loss: 0.549\n",
      "Iteration: 12 \t--- Loss: 0.538\n",
      "Iteration: 13 \t--- Loss: 0.536\n",
      "Iteration: 14 \t--- Loss: 0.525\n",
      "Iteration: 15 \t--- Loss: 0.518\n",
      "Iteration: 16 \t--- Loss: 0.516\n",
      "Iteration: 17 \t--- Loss: 0.517\n",
      "Iteration: 18 \t--- Loss: 0.531\n",
      "Iteration: 19 \t--- Loss: 0.513\n",
      "Iteration: 20 \t--- Loss: 0.510\n",
      "Iteration: 21 \t--- Loss: 0.490\n",
      "Iteration: 22 \t--- Loss: 0.502\n",
      "Iteration: 23 \t--- Loss: 0.516\n",
      "Iteration: 24 \t--- Loss: 0.498\n",
      "Iteration: 25 \t--- Loss: 0.511\n",
      "Iteration: 26 \t--- Loss: 0.502\n",
      "Iteration: 27 \t--- Loss: 0.512\n",
      "Iteration: 28 \t--- Loss: 0.503\n",
      "Iteration: 29 \t--- Loss: 0.508\n",
      "Iteration: 30 \t--- Loss: 0.492\n",
      "Iteration: 31 \t--- Loss: 0.511\n",
      "Iteration: 32 \t--- Loss: 0.501\n",
      "Iteration: 33 \t--- Loss: 0.507\n",
      "Iteration: 34 \t--- Loss: 0.494\n",
      "Iteration: 35 \t--- Loss: 0.498\n",
      "Iteration: 36 \t--- Loss: 0.501\n",
      "Iteration: 37 \t--- Loss: 0.489\n",
      "Iteration: 38 \t--- Loss: 0.494\n",
      "Iteration: 39 \t--- Loss: 0.502\n",
      "Iteration: 40 \t--- Loss: 0.501\n",
      "Iteration: 41 \t--- Loss: 0.520\n",
      "Iteration: 42 \t--- Loss: 0.506\n",
      "Iteration: 43 \t--- Loss: 0.481\n",
      "Iteration: 44 \t--- Loss: 0.515\n",
      "Iteration: 45 \t--- Loss: 0.498\n",
      "Iteration: 46 \t--- Loss: 0.496\n",
      "Iteration: 47 \t--- Loss: 0.500\n",
      "Iteration: 48 \t--- Loss: 0.492\n",
      "Iteration: 49 \t--- Loss: 0.498\n",
      "Iteration: 50 \t--- Loss: 0.485\n",
      "Iteration: 51 \t--- Loss: 0.494\n",
      "Iteration: 52 \t--- Loss: 0.498\n",
      "Iteration: 53 \t--- Loss: 0.502\n",
      "Iteration: 54 \t--- Loss: 0.495\n",
      "Iteration: 55 \t--- Loss: 0.497\n",
      "Iteration: 56 \t--- Loss: 0.507\n",
      "Iteration: 57 \t--- Loss: 0.491\n",
      "Iteration: 58 \t--- Loss: 0.507\n",
      "Iteration: 59 \t--- Loss: 0.499\n",
      "Iteration: 60 \t--- Loss: 0.497\n",
      "Iteration: 61 \t--- Loss: 0.486\n",
      "Iteration: 62 \t--- Loss: 0.511\n",
      "Iteration: 63 \t--- Loss: 0.515\n",
      "Iteration: 64 \t--- Loss: 0.507\n",
      "Iteration: 65 \t--- Loss: 0.507\n",
      "Iteration: 66 \t--- Loss: 0.503\n",
      "Iteration: 67 \t--- Loss: 0.499\n",
      "Iteration: 68 \t--- Loss: 0.501\n",
      "Iteration: 69 \t--- Loss: 0.495\n",
      "Iteration: 70 \t--- Loss: 0.511\n",
      "Iteration: 71 \t--- Loss: 0.495\n",
      "Iteration: 72 \t--- Loss: 0.503\n",
      "Iteration: 73 \t--- Loss: 0.493\n",
      "Iteration: 74 \t--- Loss: 0.499\n",
      "Iteration: 75 \t--- Loss: 0.511\n",
      "Iteration: 76 \t--- Loss: 0.501\n",
      "Iteration: 77 \t--- Loss: 0.500\n",
      "Iteration: 78 \t--- Loss: 0.486\n",
      "Iteration: 79 \t--- Loss: 0.497\n",
      "Iteration: 80 \t--- Loss: 0.508\n",
      "Iteration: 81 \t--- Loss: 0.489\n",
      "Iteration: 82 \t--- Loss: 0.504\n",
      "Iteration: 83 \t--- Loss: 0.505\n",
      "Iteration: 84 \t--- Loss: 0.498\n",
      "Iteration: 85 \t--- Loss: 0.505\n",
      "Iteration: 86 \t--- Loss: 0.489\n",
      "Iteration: 87 \t--- Loss: 0.499\n",
      "Iteration: 88 \t--- Loss: 0.481\n",
      "Iteration: 89 \t--- Loss: 0.493\n",
      "Iteration: 90 \t--- Loss: 0.497\n",
      "Iteration: 91 \t--- Loss: 0.511\n",
      "Iteration: 92 \t--- Loss: 0.488\n",
      "Iteration: 93 \t--- Loss: 0.500\n",
      "Iteration: 94 \t--- Loss: 0.518\n",
      "Iteration: 95 \t--- Loss: 0.490\n",
      "Iteration: 96 \t--- Loss: 0.493\n",
      "Iteration: 97 \t--- Loss: 0.502\n",
      "Iteration: 98 \t--- Loss: 0.519\n",
      "Iteration: 99 \t--- Loss: 0.499\n",
      "Iteration: 100 \t--- Loss: 0.499\n",
      "Iteration: 101 \t--- Loss: 0.501\n",
      "Iteration: 102 \t--- Loss: 0.494\n",
      "Iteration: 103 \t--- Loss: 0.502\n",
      "Iteration: 104 \t--- Loss: 0.507\n",
      "Iteration: 105 \t--- Loss: 0.498\n",
      "Iteration: 106 \t--- Loss: 0.498\n",
      "Iteration: 107 \t--- Loss: 0.491\n",
      "Iteration: 108 \t--- Loss: 0.502\n",
      "Iteration: 109 \t--- Loss: 0.505\n",
      "Iteration: 110 \t--- Loss: 0.495\n",
      "Iteration: 111 \t--- Loss: 0.491\n",
      "Iteration: 112 \t--- Loss: 0.490\n",
      "Iteration: 113 \t--- Loss: 0.497\n",
      "Iteration: 114 \t--- Loss: 0.493\n",
      "Iteration: 115 \t--- Loss: 0.510\n",
      "Iteration: 116 \t--- Loss: 0.505\n",
      "Iteration: 117 \t--- Loss: 0.506\n",
      "Iteration: 118 \t--- Loss: 0.495\n",
      "Iteration: 119 \t--- Loss: 0.495\n",
      "Iteration: 120 \t--- Loss: 0.489\n",
      "Iteration: 121 \t--- Loss: 0.505\n",
      "Iteration: 122 \t--- Loss: 0.483\n",
      "Iteration: 123 \t--- Loss: 0.494\n",
      "Iteration: 124 \t--- Loss: 0.505\n",
      "Iteration: 125 \t--- Loss: 0.504\n",
      "Iteration: 126 \t--- Loss: 0.496\n",
      "Iteration: 127 \t--- Loss: 0.508\n",
      "Iteration: 128 \t--- Loss: 0.504\n",
      "Iteration: 129 \t--- Loss: 0.504\n",
      "Iteration: 130 \t--- Loss: 0.507\n",
      "Iteration: 131 \t--- Loss: 0.506\n",
      "Iteration: 132 \t--- Loss: 0.505\n",
      "Iteration: 133 \t--- Loss: 0.493\n",
      "Iteration: 134 \t--- Loss: 0.480\n",
      "Iteration: 135 \t--- Loss: 0.502\n",
      "Iteration: 136 \t--- Loss: 0.497\n",
      "Iteration: 137 \t--- Loss: 0.500\n",
      "Iteration: 138 \t--- Loss: 0.495\n",
      "Iteration: 139 \t--- Loss: 0.495\n",
      "Iteration: 140 \t--- Loss: 0.490\n",
      "Iteration: 141 \t--- Loss: 0.493\n",
      "Iteration: 142 \t--- Loss: 0.503\n",
      "Iteration: 143 \t--- Loss: 0.493\n",
      "Iteration: 144 \t--- Loss: 0.509\n",
      "Iteration: 145 \t--- Loss: 0.495\n",
      "Iteration: 146 \t--- Loss: 0.490\n",
      "Iteration: 147 \t--- Loss: 0.492\n",
      "Iteration: 148 \t--- Loss: 0.498\n",
      "Iteration: 149 \t--- Loss: 0.508\n",
      "Iteration: 150 \t--- Loss: 0.509\n",
      "Iteration: 151 \t--- Loss: 0.504\n",
      "Iteration: 152 \t--- Loss: 0.497\n",
      "Iteration: 153 \t--- Loss: 0.493\n",
      "Iteration: 154 \t--- Loss: 0.492\n",
      "Iteration: 155 \t--- Loss: 0.479\n",
      "Iteration: 156 \t--- Loss: 0.494\n",
      "Iteration: 157 \t--- Loss: 0.489\n",
      "Iteration: 158 \t--- Loss: 0.494\n",
      "Iteration: 159 \t--- Loss: 0.502\n",
      "Iteration: 160 \t--- Loss: 0.501\n",
      "Iteration: 161 \t--- Loss: 0.481\n",
      "Iteration: 162 \t--- Loss: 0.503\n",
      "Iteration: 163 \t--- Loss: 0.495\n",
      "Iteration: 164 \t--- Loss: 0.495\n",
      "Iteration: 165 \t--- Loss: 0.508\n",
      "Iteration: 166 \t--- Loss: 0.498\n",
      "Iteration: 167 \t--- Loss: 0.491\n",
      "Iteration: 168 \t--- Loss: 0.492\n",
      "Iteration: 169 \t--- Loss: 0.492\n",
      "Iteration: 170 \t--- Loss: 0.509\n",
      "Iteration: 171 \t--- Loss: 0.494\n",
      "Iteration: 172 \t--- Loss: 0.482\n",
      "Iteration: 173 \t--- Loss: 0.493\n",
      "Iteration: 174 \t--- Loss: 0.494\n",
      "Iteration: 175 \t--- Loss: 0.501\n",
      "Iteration: 176 \t--- Loss: 0.506\n",
      "Iteration: 177 \t--- Loss: 0.501\n",
      "Iteration: 178 \t--- Loss: 0.486\n",
      "Iteration: 179 \t--- Loss: 0.495\n",
      "Iteration: 180 \t--- Loss: 0.505\n",
      "Iteration: 181 \t--- Loss: 0.518\n",
      "Iteration: 182 \t--- Loss: 0.511\n",
      "Iteration: 183 \t--- Loss: 0.497\n",
      "Iteration: 184 \t--- Loss: 0.481\n",
      "Iteration: 185 \t--- Loss: 0.503\n",
      "Iteration: 186 \t--- Loss: 0.495\n",
      "Iteration: 187 \t--- Loss: 0.517\n",
      "Iteration: 188 \t--- Loss: 0.497\n",
      "Iteration: 189 \t--- Loss: 0.490\n",
      "Iteration: 190 \t--- Loss: 0.493\n",
      "Iteration: 191 \t--- Loss: 0.504\n",
      "Iteration: 192 \t--- Loss: 0.503\n",
      "Iteration: 193 \t--- Loss: 0.501\n",
      "Iteration: 194 \t--- Loss: 0.499\n",
      "Iteration: 195 \t--- Loss: 0.495\n",
      "Iteration: 196 \t--- Loss: 0.480\n",
      "Iteration: 197 \t--- Loss: 0.500\n",
      "Iteration: 198 \t--- Loss: 0.493\n",
      "Iteration: 199 \t--- Loss: 0.499\n",
      "Iteration: 200 \t--- Loss: 0.484\n",
      "Iteration: 201 \t--- Loss: 0.495\n",
      "Iteration: 202 \t--- Loss: 0.494\n",
      "Iteration: 203 \t--- Loss: 0.504\n",
      "Iteration: 204 \t--- Loss: 0.499\n",
      "Iteration: 205 \t--- Loss: 0.504\n",
      "Iteration: 206 \t--- Loss: 0.494\n",
      "Iteration: 207 \t--- Loss: 0.494\n",
      "Iteration: 208 \t--- Loss: 0.497\n",
      "Iteration: 209 \t--- Loss: 0.496\n",
      "Iteration: 210 \t--- Loss: 0.502\n",
      "Iteration: 211 \t--- Loss: 0.496\n",
      "Iteration: 212 \t--- Loss: 0.495\n",
      "Iteration: 213 \t--- Loss: 0.504\n",
      "Iteration: 214 \t--- Loss: 0.499\n",
      "Iteration: 215 \t--- Loss: 0.496\n",
      "Iteration: 216 \t--- Loss: 0.490\n",
      "Iteration: 217 \t--- Loss: 0.502\n",
      "Iteration: 218 \t--- Loss: 0.500\n",
      "Iteration: 219 \t--- Loss: 0.497\n",
      "Iteration: 220 \t--- Loss: 0.483\n",
      "Iteration: 221 \t--- Loss: 0.487\n",
      "Iteration: 222 \t--- Loss: 0.487\n",
      "Iteration: 223 \t--- Loss: 0.498\n",
      "Iteration: 224 \t--- Loss: 0.497\n",
      "Iteration: 225 \t--- Loss: 0.488\n",
      "Iteration: 226 \t--- Loss: 0.496\n",
      "Iteration: 227 \t--- Loss: 0.503\n",
      "Iteration: 228 \t--- Loss: 0.503\n",
      "Iteration: 229 \t--- Loss: 0.502\n",
      "Iteration: 230 \t--- Loss: 0.507\n",
      "Iteration: 231 \t--- Loss: 0.501\n",
      "Iteration: 232 \t--- Loss: 0.499\n",
      "Iteration: 233 \t--- Loss: 0.492\n",
      "Iteration: 234 \t--- Loss: 0.510\n",
      "Iteration: 235 \t--- Loss: 0.501\n",
      "Iteration: 236 \t--- Loss: 0.496\n",
      "Iteration: 237 \t--- Loss: 0.499\n",
      "Iteration: 238 \t--- Loss: 0.496\n",
      "Iteration: 239 \t--- Loss: 0.509\n",
      "Iteration: 240 \t--- Loss: 0.494\n",
      "Iteration: 241 \t--- Loss: 0.510\n",
      "Iteration: 242 \t--- Loss: 0.509\n",
      "Iteration: 243 \t--- Loss: 0.501\n",
      "Iteration: 244 \t--- Loss: 0.502\n",
      "Iteration: 245 \t--- Loss: 0.505\n",
      "Iteration: 246 \t--- Loss: 0.496\n",
      "Iteration: 247 \t--- Loss: 0.498\n",
      "Iteration: 248 \t--- Loss: 0.509\n",
      "Iteration: 249 \t--- Loss: 0.492\n",
      "Iteration: 250 \t--- Loss: 0.485\n",
      "Iteration: 251 \t--- Loss: 0.485\n",
      "Iteration: 252 \t--- Loss: 0.497\n",
      "Iteration: 253 \t--- Loss: 0.483\n",
      "Iteration: 254 \t--- Loss: 0.505\n",
      "Iteration: 255 \t--- Loss: 0.494\n",
      "Iteration: 256 \t--- Loss: 0.506\n",
      "Iteration: 257 \t--- Loss: 0.501\n",
      "Iteration: 258 \t--- Loss: 0.499\n",
      "Iteration: 259 \t--- Loss: 0.513"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:34<00:00, 94.29s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.005\n",
      "Iteration: 261 \t--- Loss: 0.006\n",
      "Iteration: 262 \t--- Loss: 0.005\n",
      "Iteration: 263 \t--- Loss: 0.004\n",
      "Iteration: 264 \t--- Loss: 0.004\n",
      "Iteration: 265 \t--- Loss: 0.004\n",
      "Iteration: 266 \t--- Loss: 0.004\n",
      "Iteration: 267 \t--- Loss: 0.004\n",
      "Iteration: 268 \t--- Loss: 0.003\n",
      "Iteration: 269 \t--- Loss: 0.003\n",
      "Iteration: 270 \t--- Loss: 0.004\n",
      "Iteration: 271 \t--- Loss: 0.004\n",
      "Iteration: 272 \t--- Loss: 0.005\n",
      "Iteration: 273 \t--- Loss: 0.005\n",
      "Iteration: 274 \t--- Loss: 0.005\n",
      "Iteration: 275 \t--- Loss: 0.005\n",
      "Iteration: 276 \t--- Loss: 0.004\n",
      "Iteration: 277 \t--- Loss: 0.004\n",
      "Iteration: 278 \t--- Loss: 0.005\n",
      "Iteration: 279 \t--- Loss: 0.005\n",
      "Iteration: 280 \t--- Loss: 0.004\n",
      "Iteration: 281 \t--- Loss: 0.004\n",
      "Iteration: 282 \t--- Loss: 0.004\n",
      "Iteration: 283 \t--- Loss: 0.003\n",
      "Iteration: 284 \t--- Loss: 0.003\n",
      "Iteration: 285 \t--- Loss: 0.002\n",
      "Iteration: 286 \t--- Loss: 0.002\n",
      "Iteration: 287 \t--- Loss: 0.003\n",
      "Iteration: 288 \t--- Loss: 0.002\n",
      "Iteration: 289 \t--- Loss: 0.002\n",
      "Iteration: 290 \t--- Loss: 0.002\n",
      "Iteration: 291 \t--- Loss: 0.002\n",
      "Iteration: 292 \t--- Loss: 0.002\n",
      "Iteration: 293 \t--- Loss: 0.002\n",
      "Iteration: 294 \t--- Loss: 0.002\n",
      "Iteration: 295 \t--- Loss: 0.002\n",
      "Iteration: 296 \t--- Loss: 0.002\n",
      "Iteration: 297 \t--- Loss: 0.002\n",
      "Iteration: 298 \t--- Loss: 0.002\n",
      "Iteration: 299 \t--- Loss: 0.002\n",
      "Iteration: 300 \t--- Loss: 0.002\n",
      "Iteration: 301 \t--- Loss: 0.002\n",
      "Iteration: 302 \t--- Loss: 0.002\n",
      "Iteration: 303 \t--- Loss: 0.002\n",
      "Iteration: 304 \t--- Loss: 0.002\n",
      "Iteration: 305 \t--- Loss: 0.002\n",
      "Iteration: 306 \t--- Loss: 0.002\n",
      "Iteration: 307 \t--- Loss: 0.002\n",
      "Iteration: 308 \t--- Loss: 0.002\n",
      "Iteration: 309 \t--- Loss: 0.002\n",
      "Iteration: 310 \t--- Loss: 0.002\n",
      "Iteration: 311 \t--- Loss: 0.002\n",
      "Iteration: 312 \t--- Loss: 0.002\n",
      "Iteration: 313 \t--- Loss: 0.002\n",
      "Iteration: 314 \t--- Loss: 0.002\n",
      "Iteration: 315 \t--- Loss: 0.002\n",
      "Iteration: 316 \t--- Loss: 0.002\n",
      "Iteration: 317 \t--- Loss: 0.002\n",
      "Iteration: 318 \t--- Loss: 0.002\n",
      "Iteration: 319 \t--- Loss: 0.002\n",
      "Iteration: 320 \t--- Loss: 0.002\n",
      "Iteration: 321 \t--- Loss: 0.002\n",
      "Iteration: 322 \t--- Loss: 0.002\n",
      "Iteration: 323 \t--- Loss: 0.002\n",
      "Iteration: 324 \t--- Loss: 0.002\n",
      "Iteration: 325 \t--- Loss: 0.002\n",
      "Iteration: 326 \t--- Loss: 0.002\n",
      "Iteration: 327 \t--- Loss: 0.002\n",
      "Iteration: 328 \t--- Loss: 0.001\n",
      "Iteration: 329 \t--- Loss: 0.001\n",
      "Iteration: 330 \t--- Loss: 0.002\n",
      "Iteration: 331 \t--- Loss: 0.002\n",
      "Iteration: 332 \t--- Loss: 0.001\n",
      "Iteration: 333 \t--- Loss: 0.001\n",
      "Iteration: 334 \t--- Loss: 0.002\n",
      "Iteration: 335 \t--- Loss: 0.001\n",
      "Iteration: 336 \t--- Loss: 0.001\n",
      "Iteration: 337 \t--- Loss: 0.002\n",
      "Iteration: 338 \t--- Loss: 0.001\n",
      "Iteration: 339 \t--- Loss: 0.002\n",
      "Iteration: 340 \t--- Loss: 0.001\n",
      "Iteration: 341 \t--- Loss: 0.001\n",
      "Iteration: 342 \t--- Loss: 0.001\n",
      "Iteration: 343 \t--- Loss: 0.002\n",
      "Iteration: 344 \t--- Loss: 0.001\n",
      "Iteration: 345 \t--- Loss: 0.001\n",
      "Iteration: 346 \t--- Loss: 0.001\n",
      "Iteration: 347 \t--- Loss: 0.001\n",
      "Iteration: 348 \t--- Loss: 0.001\n",
      "Iteration: 349 \t--- Loss: 0.001\n",
      "Iteration: 350 \t--- Loss: 0.001\n",
      "Iteration: 351 \t--- Loss: 0.001\n",
      "Iteration: 352 \t--- Loss: 0.001\n",
      "Iteration: 353 \t--- Loss: 0.001\n",
      "Iteration: 354 \t--- Loss: 0.001\n",
      "Iteration: 355 \t--- Loss: 0.001\n",
      "Iteration: 356 \t--- Loss: 0.001\n",
      "Iteration: 357 \t--- Loss: 0.001\n",
      "Iteration: 358 \t--- Loss: 0.001\n",
      "Iteration: 359 \t--- Loss: 0.001\n",
      "Iteration: 360 \t--- Loss: 0.001\n",
      "Iteration: 361 \t--- Loss: 0.001\n",
      "Iteration: 362 \t--- Loss: 0.001\n",
      "Iteration: 363 \t--- Loss: 0.001\n",
      "Iteration: 364 \t--- Loss: 0.001\n",
      "Iteration: 365 \t--- Loss: 0.001\n",
      "Iteration: 366 \t--- Loss: 0.001\n",
      "Iteration: 367 \t--- Loss: 0.001\n",
      "Iteration: 368 \t--- Loss: 0.001\n",
      "Iteration: 369 \t--- Loss: 0.001\n",
      "Iteration: 370 \t--- Loss: 0.001\n",
      "Iteration: 371 \t--- Loss: 0.001\n",
      "Iteration: 372 \t--- Loss: 0.001\n",
      "Iteration: 373 \t--- Loss: 0.001\n",
      "Iteration: 374 \t--- Loss: 0.001\n",
      "Iteration: 375 \t--- Loss: 0.001\n",
      "Iteration: 376 \t--- Loss: 0.001\n",
      "Iteration: 377 \t--- Loss: 0.001\n",
      "Iteration: 378 \t--- Loss: 0.001\n",
      "Iteration: 379 \t--- Loss: 0.001\n",
      "Iteration: 380 \t--- Loss: 0.001\n",
      "Iteration: 381 \t--- Loss: 0.001\n",
      "Iteration: 382 \t--- Loss: 0.001\n",
      "Iteration: 383 \t--- Loss: 0.001\n",
      "Iteration: 384 \t--- Loss: 0.001\n",
      "Iteration: 385 \t--- Loss: 0.001\n",
      "Iteration: 386 \t--- Loss: 0.001\n",
      "Iteration: 387 \t--- Loss: 0.001\n",
      "Iteration: 388 \t--- Loss: 0.001\n",
      "Iteration: 389 \t--- Loss: 0.001\n",
      "Iteration: 390 \t--- Loss: 0.001\n",
      "Iteration: 391 \t--- Loss: 0.001\n",
      "Iteration: 392 \t--- Loss: 0.001\n",
      "Iteration: 393 \t--- Loss: 0.001\n",
      "Iteration: 394 \t--- Loss: 0.001\n",
      "Iteration: 395 \t--- Loss: 0.001\n",
      "Iteration: 396 \t--- Loss: 0.001\n",
      "Iteration: 397 \t--- Loss: 0.001\n",
      "Iteration: 398 \t--- Loss: 0.001\n",
      "Iteration: 399 \t--- Loss: 0.001\n",
      "Iteration: 400 \t--- Loss: 0.001\n",
      "Iteration: 401 \t--- Loss: 0.001\n",
      "Iteration: 402 \t--- Loss: 0.001\n",
      "Iteration: 403 \t--- Loss: 0.001\n",
      "Iteration: 404 \t--- Loss: 0.001\n",
      "Iteration: 405 \t--- Loss: 0.001\n",
      "Iteration: 406 \t--- Loss: 0.001\n",
      "Iteration: 407 \t--- Loss: 0.001\n",
      "Iteration: 408 \t--- Loss: 0.001\n",
      "Iteration: 409 \t--- Loss: 0.001\n",
      "Iteration: 410 \t--- Loss: 0.001\n",
      "Iteration: 411 \t--- Loss: 0.001\n",
      "Iteration: 412 \t--- Loss: 0.001\n",
      "Iteration: 413 \t--- Loss: 0.001\n",
      "Iteration: 414 \t--- Loss: 0.001\n",
      "Iteration: 415 \t--- Loss: 0.001\n",
      "Iteration: 416 \t--- Loss: 0.001\n",
      "Iteration: 417 \t--- Loss: 0.001\n",
      "Iteration: 418 \t--- Loss: 0.001\n",
      "Iteration: 419 \t--- Loss: 0.001\n",
      "Iteration: 420 \t--- Loss: 0.001\n",
      "Iteration: 421 \t--- Loss: 0.001\n",
      "Iteration: 422 \t--- Loss: 0.001\n",
      "Iteration: 423 \t--- Loss: 0.001\n",
      "Iteration: 424 \t--- Loss: 0.001\n",
      "Iteration: 425 \t--- Loss: 0.001\n",
      "Iteration: 426 \t--- Loss: 0.001\n",
      "Iteration: 427 \t--- Loss: 0.001\n",
      "Iteration: 428 \t--- Loss: 0.001\n",
      "Iteration: 429 \t--- Loss: 0.001\n",
      "Iteration: 430 \t--- Loss: 0.001\n",
      "Iteration: 431 \t--- Loss: 0.001\n",
      "Iteration: 432 \t--- Loss: 0.001\n",
      "Iteration: 433 \t--- Loss: 0.001\n",
      "Iteration: 434 \t--- Loss: 0.001\n",
      "Iteration: 435 \t--- Loss: 0.001\n",
      "Iteration: 436 \t--- Loss: 0.001\n",
      "Iteration: 437 \t--- Loss: 0.001\n",
      "Iteration: 438 \t--- Loss: 0.001\n",
      "Iteration: 439 \t--- Loss: 0.001\n",
      "Iteration: 440 \t--- Loss: 0.001\n",
      "Iteration: 441 \t--- Loss: 0.001\n",
      "Iteration: 442 \t--- Loss: 0.001\n",
      "Iteration: 443 \t--- Loss: 0.001\n",
      "Iteration: 444 \t--- Loss: 0.001\n",
      "Iteration: 445 \t--- Loss: 0.001\n",
      "Iteration: 446 \t--- Loss: 0.001\n",
      "Iteration: 447 \t--- Loss: 0.001\n",
      "Iteration: 448 \t--- Loss: 0.001\n",
      "Iteration: 449 \t--- Loss: 0.001\n",
      "Iteration: 450 \t--- Loss: 0.001\n",
      "Iteration: 451 \t--- Loss: 0.001\n",
      "Iteration: 452 \t--- Loss: 0.001\n",
      "Iteration: 453 \t--- Loss: 0.001\n",
      "Iteration: 454 \t--- Loss: 0.001\n",
      "Iteration: 455 \t--- Loss: 0.001\n",
      "Iteration: 456 \t--- Loss: 0.001\n",
      "Iteration: 457 \t--- Loss: 0.001\n",
      "Iteration: 458 \t--- Loss: 0.001\n",
      "Iteration: 459 \t--- Loss: 0.001\n",
      "Iteration: 460 \t--- Loss: 0.001\n",
      "Iteration: 461 \t--- Loss: 0.001\n",
      "Iteration: 462 \t--- Loss: 0.001\n",
      "Iteration: 463 \t--- Loss: 0.001\n",
      "Iteration: 464 \t--- Loss: 0.001\n",
      "Iteration: 465 \t--- Loss: 0.001\n",
      "Iteration: 466 \t--- Loss: 0.001\n",
      "Iteration: 467 \t--- Loss: 0.001\n",
      "Iteration: 468 \t--- Loss: 0.001\n",
      "Iteration: 469 \t--- Loss: 0.001\n",
      "Iteration: 470 \t--- Loss: 0.001\n",
      "Iteration: 471 \t--- Loss: 0.001\n",
      "Iteration: 472 \t--- Loss: 0.001\n",
      "Iteration: 473 \t--- Loss: 0.001\n",
      "Iteration: 474 \t--- Loss: 0.001\n",
      "Iteration: 475 \t--- Loss: 0.001\n",
      "Iteration: 476 \t--- Loss: 0.001\n",
      "Iteration: 477 \t--- Loss: 0.001\n",
      "Iteration: 478 \t--- Loss: 0.001\n",
      "Iteration: 479 \t--- Loss: 0.001\n",
      "Iteration: 480 \t--- Loss: 0.001\n",
      "Iteration: 481 \t--- Loss: 0.001\n",
      "Iteration: 482 \t--- Loss: 0.001\n",
      "Iteration: 483 \t--- Loss: 0.001\n",
      "Iteration: 484 \t--- Loss: 0.001\n",
      "Iteration: 485 \t--- Loss: 0.001\n",
      "Iteration: 486 \t--- Loss: 0.001\n",
      "Iteration: 487 \t--- Loss: 0.001\n",
      "Iteration: 488 \t--- Loss: 0.001\n",
      "Iteration: 489 \t--- Loss: 0.001\n",
      "Iteration: 490 \t--- Loss: 0.001\n",
      "Iteration: 491 \t--- Loss: 0.001\n",
      "Iteration: 492 \t--- Loss: 0.001\n",
      "Iteration: 493 \t--- Loss: 0.001\n",
      "Iteration: 494 \t--- Loss: 0.001\n",
      "Iteration: 495 \t--- Loss: 0.001\n",
      "Iteration: 496 \t--- Loss: 0.001\n",
      "Iteration: 497 \t--- Loss: 0.001\n",
      "Iteration: 498 \t--- Loss: 0.001\n",
      "Iteration: 499 \t--- Loss: 0.001\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it][Parallel(n_jobs=5)]: Done  54 tasks      | elapsed: 32.5min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [08:38<00:00, 518.83s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.023\n",
      "Iteration: 261 \t--- Loss: 0.025\n",
      "Iteration: 262 \t--- Loss: 0.023\n",
      "Iteration: 263 \t--- Loss: 0.024\n",
      "Iteration: 264 \t--- Loss: 0.025\n",
      "Iteration: 265 \t--- Loss: 0.028\n",
      "Iteration: 266 \t--- Loss: 0.021\n",
      "Iteration: 267 \t--- Loss: 0.023\n",
      "Iteration: 268 \t--- Loss: 0.019\n",
      "Iteration: 269 \t--- Loss: 0.018\n",
      "Iteration: 270 \t--- Loss: 0.018\n",
      "Iteration: 271 \t--- Loss: 0.017\n",
      "Iteration: 272 \t--- Loss: 0.019\n",
      "Iteration: 273 \t--- Loss: 0.017\n",
      "Iteration: 274 \t--- Loss: 0.018\n",
      "Iteration: 275 \t--- Loss: 0.017\n",
      "Iteration: 276 \t--- Loss: 0.018\n",
      "Iteration: 277 \t--- Loss: 0.017\n",
      "Iteration: 278 \t--- Loss: 0.018\n",
      "Iteration: 279 \t--- Loss: 0.016\n",
      "Iteration: 280 \t--- Loss: 0.017\n",
      "Iteration: 281 \t--- Loss: 0.014\n",
      "Iteration: 282 \t--- Loss: 0.017\n",
      "Iteration: 283 \t--- Loss: 0.016\n",
      "Iteration: 284 \t--- Loss: 0.017\n",
      "Iteration: 285 \t--- Loss: 0.016\n",
      "Iteration: 286 \t--- Loss: 0.016\n",
      "Iteration: 287 \t--- Loss: 0.015\n",
      "Iteration: 288 \t--- Loss: 0.017\n",
      "Iteration: 289 \t--- Loss: 0.017\n",
      "Iteration: 290 \t--- Loss: 0.017\n",
      "Iteration: 291 \t--- Loss: 0.016\n",
      "Iteration: 292 \t--- Loss: 0.017\n",
      "Iteration: 293 \t--- Loss: 0.015\n",
      "Iteration: 294 \t--- Loss: 0.018\n",
      "Iteration: 295 \t--- Loss: 0.019\n",
      "Iteration: 296 \t--- Loss: 0.019\n",
      "Iteration: 297 \t--- Loss: 0.020\n",
      "Iteration: 298 \t--- Loss: 0.018\n",
      "Iteration: 299 \t--- Loss: 0.016\n",
      "Iteration: 300 \t--- Loss: 0.016\n",
      "Iteration: 301 \t--- Loss: 0.014\n",
      "Iteration: 302 \t--- Loss: 0.015\n",
      "Iteration: 303 \t--- Loss: 0.014\n",
      "Iteration: 304 \t--- Loss: 0.014\n",
      "Iteration: 305 \t--- Loss: 0.016\n",
      "Iteration: 306 \t--- Loss: 0.015\n",
      "Iteration: 307 \t--- Loss: 0.014\n",
      "Iteration: 308 \t--- Loss: 0.015\n",
      "Iteration: 309 \t--- Loss: 0.014\n",
      "Iteration: 310 \t--- Loss: 0.013\n",
      "Iteration: 311 \t--- Loss: 0.013\n",
      "Iteration: 312 \t--- Loss: 0.014\n",
      "Iteration: 313 \t--- Loss: 0.013\n",
      "Iteration: 314 \t--- Loss: 0.014\n",
      "Iteration: 315 \t--- Loss: 0.015\n",
      "Iteration: 316 \t--- Loss: 0.015\n",
      "Iteration: 317 \t--- Loss: 0.014\n",
      "Iteration: 318 \t--- Loss: 0.015\n",
      "Iteration: 319 \t--- Loss: 0.015\n",
      "Iteration: 320 \t--- Loss: 0.014\n",
      "Iteration: 321 \t--- Loss: 0.014\n",
      "Iteration: 322 \t--- Loss: 0.016\n",
      "Iteration: 323 \t--- Loss: 0.015\n",
      "Iteration: 324 \t--- Loss: 0.014\n",
      "Iteration: 325 \t--- Loss: 0.014\n",
      "Iteration: 326 \t--- Loss: 0.015\n",
      "Iteration: 327 \t--- Loss: 0.016\n",
      "Iteration: 328 \t--- Loss: 0.015\n",
      "Iteration: 329 \t--- Loss: 0.014\n",
      "Iteration: 330 \t--- Loss: 0.015\n",
      "Iteration: 331 \t--- Loss: 0.014\n",
      "Iteration: 332 \t--- Loss: 0.016\n",
      "Iteration: 333 \t--- Loss: 0.015\n",
      "Iteration: 334 \t--- Loss: 0.015\n",
      "Iteration: 335 \t--- Loss: 0.017\n",
      "Iteration: 336 \t--- Loss: 0.015\n",
      "Iteration: 337 \t--- Loss: 0.015\n",
      "Iteration: 338 \t--- Loss: 0.015\n",
      "Iteration: 339 \t--- Loss: 0.015\n",
      "Iteration: 340 \t--- Loss: 0.015\n",
      "Iteration: 341 \t--- Loss: 0.014\n",
      "Iteration: 342 \t--- Loss: 0.014\n",
      "Iteration: 343 \t--- Loss: 0.013\n",
      "Iteration: 344 \t--- Loss: 0.014\n",
      "Iteration: 345 \t--- Loss: 0.013\n",
      "Iteration: 346 \t--- Loss: 0.013\n",
      "Iteration: 347 \t--- Loss: 0.014\n",
      "Iteration: 348 \t--- Loss: 0.015\n",
      "Iteration: 349 \t--- Loss: 0.015\n",
      "Iteration: 350 \t--- Loss: 0.014\n",
      "Iteration: 351 \t--- Loss: 0.014\n",
      "Iteration: 352 \t--- Loss: 0.013\n",
      "Iteration: 353 \t--- Loss: 0.013\n",
      "Iteration: 354 \t--- Loss: 0.014\n",
      "Iteration: 355 \t--- Loss: 0.014\n",
      "Iteration: 356 \t--- Loss: 0.013\n",
      "Iteration: 357 \t--- Loss: 0.013\n",
      "Iteration: 358 \t--- Loss: 0.013\n",
      "Iteration: 359 \t--- Loss: 0.013\n",
      "Iteration: 360 \t--- Loss: 0.013\n",
      "Iteration: 361 \t--- Loss: 0.013\n",
      "Iteration: 362 \t--- Loss: 0.012\n",
      "Iteration: 363 \t--- Loss: 0.013\n",
      "Iteration: 364 \t--- Loss: 0.013\n",
      "Iteration: 365 \t--- Loss: 0.012\n",
      "Iteration: 366 \t--- Loss: 0.012\n",
      "Iteration: 367 \t--- Loss: 0.012\n",
      "Iteration: 368 \t--- Loss: 0.012\n",
      "Iteration: 369 \t--- Loss: 0.011\n",
      "Iteration: 370 \t--- Loss: 0.012\n",
      "Iteration: 371 \t--- Loss: 0.012\n",
      "Iteration: 372 \t--- Loss: 0.012\n",
      "Iteration: 373 \t--- Loss: 0.011\n",
      "Iteration: 374 \t--- Loss: 0.011\n",
      "Iteration: 375 \t--- Loss: 0.011\n",
      "Iteration: 376 \t--- Loss: 0.011\n",
      "Iteration: 377 \t--- Loss: 0.012\n",
      "Iteration: 378 \t--- Loss: 0.012\n",
      "Iteration: 379 \t--- Loss: 0.012\n",
      "Iteration: 380 \t--- Loss: 0.011\n",
      "Iteration: 381 \t--- Loss: 0.011\n",
      "Iteration: 382 \t--- Loss: 0.011\n",
      "Iteration: 383 \t--- Loss: 0.012\n",
      "Iteration: 384 \t--- Loss: 0.011\n",
      "Iteration: 385 \t--- Loss: 0.012\n",
      "Iteration: 386 \t--- Loss: 0.011\n",
      "Iteration: 387 \t--- Loss: 0.012\n",
      "Iteration: 388 \t--- Loss: 0.010\n",
      "Iteration: 389 \t--- Loss: 0.011\n",
      "Iteration: 390 \t--- Loss: 0.011\n",
      "Iteration: 391 \t--- Loss: 0.011\n",
      "Iteration: 392 \t--- Loss: 0.011\n",
      "Iteration: 393 \t--- Loss: 0.012\n",
      "Iteration: 394 \t--- Loss: 0.011\n",
      "Iteration: 395 \t--- Loss: 0.012\n",
      "Iteration: 396 \t--- Loss: 0.011\n",
      "Iteration: 397 \t--- Loss: 0.011\n",
      "Iteration: 398 \t--- Loss: 0.011\n",
      "Iteration: 399 \t--- Loss: 0.011\n",
      "Iteration: 400 \t--- Loss: 0.011\n",
      "Iteration: 401 \t--- Loss: 0.011\n",
      "Iteration: 402 \t--- Loss: 0.011\n",
      "Iteration: 403 \t--- Loss: 0.011\n",
      "Iteration: 404 \t--- Loss: 0.011\n",
      "Iteration: 405 \t--- Loss: 0.011\n",
      "Iteration: 406 \t--- Loss: 0.011\n",
      "Iteration: 407 \t--- Loss: 0.011\n",
      "Iteration: 408 \t--- Loss: 0.011\n",
      "Iteration: 409 \t--- Loss: 0.010\n",
      "Iteration: 410 \t--- Loss: 0.010\n",
      "Iteration: 411 \t--- Loss: 0.012\n",
      "Iteration: 412 \t--- Loss: 0.012\n",
      "Iteration: 413 \t--- Loss: 0.011\n",
      "Iteration: 414 \t--- Loss: 0.010\n",
      "Iteration: 415 \t--- Loss: 0.010\n",
      "Iteration: 416 \t--- Loss: 0.010\n",
      "Iteration: 417 \t--- Loss: 0.011\n",
      "Iteration: 418 \t--- Loss: 0.011\n",
      "Iteration: 419 \t--- Loss: 0.011\n",
      "Iteration: 420 \t--- Loss: 0.012\n",
      "Iteration: 421 \t--- Loss: 0.011\n",
      "Iteration: 422 \t--- Loss: 0.011\n",
      "Iteration: 423 \t--- Loss: 0.010\n",
      "Iteration: 424 \t--- Loss: 0.011\n",
      "Iteration: 425 \t--- Loss: 0.011\n",
      "Iteration: 426 \t--- Loss: 0.010\n",
      "Iteration: 427 \t--- Loss: 0.010\n",
      "Iteration: 428 \t--- Loss: 0.011\n",
      "Iteration: 429 \t--- Loss: 0.010\n",
      "Iteration: 430 \t--- Loss: 0.011\n",
      "Iteration: 431 \t--- Loss: 0.010\n",
      "Iteration: 432 \t--- Loss: 0.010\n",
      "Iteration: 433 \t--- Loss: 0.011\n",
      "Iteration: 434 \t--- Loss: 0.011\n",
      "Iteration: 435 \t--- Loss: 0.010\n",
      "Iteration: 436 \t--- Loss: 0.010\n",
      "Iteration: 437 \t--- Loss: 0.010\n",
      "Iteration: 438 \t--- Loss: 0.011\n",
      "Iteration: 439 \t--- Loss: 0.009\n",
      "Iteration: 440 \t--- Loss: 0.010\n",
      "Iteration: 441 \t--- Loss: 0.010\n",
      "Iteration: 442 \t--- Loss: 0.010\n",
      "Iteration: 443 \t--- Loss: 0.011\n",
      "Iteration: 444 \t--- Loss: 0.011\n",
      "Iteration: 445 \t--- Loss: 0.010\n",
      "Iteration: 446 \t--- Loss: 0.010\n",
      "Iteration: 447 \t--- Loss: 0.011\n",
      "Iteration: 448 \t--- Loss: 0.011\n",
      "Iteration: 449 \t--- Loss: 0.011\n",
      "Iteration: 450 \t--- Loss: 0.010\n",
      "Iteration: 451 \t--- Loss: 0.010\n",
      "Iteration: 452 \t--- Loss: 0.010\n",
      "Iteration: 453 \t--- Loss: 0.010\n",
      "Iteration: 454 \t--- Loss: 0.010\n",
      "Iteration: 455 \t--- Loss: 0.010\n",
      "Iteration: 456 \t--- Loss: 0.010\n",
      "Iteration: 457 \t--- Loss: 0.010\n",
      "Iteration: 458 \t--- Loss: 0.010\n",
      "Iteration: 459 \t--- Loss: 0.010\n",
      "Iteration: 460 \t--- Loss: 0.010\n",
      "Iteration: 461 \t--- Loss: 0.009\n",
      "Iteration: 462 \t--- Loss: 0.011\n",
      "Iteration: 463 \t--- Loss: 0.010\n",
      "Iteration: 464 \t--- Loss: 0.010\n",
      "Iteration: 465 \t--- Loss: 0.010\n",
      "Iteration: 466 \t--- Loss: 0.009\n",
      "Iteration: 467 \t--- Loss: 0.009\n",
      "Iteration: 468 \t--- Loss: 0.010\n",
      "Iteration: 469 \t--- Loss: 0.009\n",
      "Iteration: 470 \t--- Loss: 0.011\n",
      "Iteration: 471 \t--- Loss: 0.010\n",
      "Iteration: 472 \t--- Loss: 0.010\n",
      "Iteration: 473 \t--- Loss: 0.009\n",
      "Iteration: 474 \t--- Loss: 0.010\n",
      "Iteration: 475 \t--- Loss: 0.010\n",
      "Iteration: 476 \t--- Loss: 0.010\n",
      "Iteration: 477 \t--- Loss: 0.010\n",
      "Iteration: 478 \t--- Loss: 0.010\n",
      "Iteration: 479 \t--- Loss: 0.010\n",
      "Iteration: 480 \t--- Loss: 0.009\n",
      "Iteration: 481 \t--- Loss: 0.010\n",
      "Iteration: 482 \t--- Loss: 0.009\n",
      "Iteration: 483 \t--- Loss: 0.009\n",
      "Iteration: 484 \t--- Loss: 0.009\n",
      "Iteration: 485 \t--- Loss: 0.009\n",
      "Iteration: 486 \t--- Loss: 0.010\n",
      "Iteration: 487 \t--- Loss: 0.010\n",
      "Iteration: 488 \t--- Loss: 0.009\n",
      "Iteration: 489 \t--- Loss: 0.010\n",
      "Iteration: 490 \t--- Loss: 0.009\n",
      "Iteration: 491 \t--- Loss: 0.009\n",
      "Iteration: 492 \t--- Loss: 0.009\n",
      "Iteration: 493 \t--- Loss: 0.010\n",
      "Iteration: 494 \t--- Loss: 0.009\n",
      "Iteration: 495 \t--- Loss: 0.009\n",
      "Iteration: 496 \t--- Loss: 0.010\n",
      "Iteration: 497 \t--- Loss: 0.010\n",
      "Iteration: 498 \t--- Loss: 0.010\n",
      "Iteration: 499 \t--- Loss: 0.010\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:24<00:00, 84.60s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.498\n",
      "Iteration: 261 \t--- Loss: 0.495\n",
      "Iteration: 262 \t--- Loss: 0.497\n",
      "Iteration: 263 \t--- Loss: 0.500\n",
      "Iteration: 264 \t--- Loss: 0.506\n",
      "Iteration: 265 \t--- Loss: 0.485\n",
      "Iteration: 266 \t--- Loss: 0.497\n",
      "Iteration: 267 \t--- Loss: 0.517\n",
      "Iteration: 268 \t--- Loss: 0.489\n",
      "Iteration: 269 \t--- Loss: 0.490\n",
      "Iteration: 270 \t--- Loss: 0.499\n",
      "Iteration: 271 \t--- Loss: 0.503\n",
      "Iteration: 272 \t--- Loss: 0.496\n",
      "Iteration: 273 \t--- Loss: 0.490\n",
      "Iteration: 274 \t--- Loss: 0.502\n",
      "Iteration: 275 \t--- Loss: 0.499\n",
      "Iteration: 276 \t--- Loss: 0.491\n",
      "Iteration: 277 \t--- Loss: 0.510\n",
      "Iteration: 278 \t--- Loss: 0.508\n",
      "Iteration: 279 \t--- Loss: 0.499\n",
      "Iteration: 280 \t--- Loss: 0.501\n",
      "Iteration: 281 \t--- Loss: 0.503\n",
      "Iteration: 282 \t--- Loss: 0.498\n",
      "Iteration: 283 \t--- Loss: 0.499\n",
      "Iteration: 284 \t--- Loss: 0.500\n",
      "Iteration: 285 \t--- Loss: 0.506\n",
      "Iteration: 286 \t--- Loss: 0.498\n",
      "Iteration: 287 \t--- Loss: 0.508\n",
      "Iteration: 288 \t--- Loss: 0.497\n",
      "Iteration: 289 \t--- Loss: 0.499\n",
      "Iteration: 290 \t--- Loss: 0.489\n",
      "Iteration: 291 \t--- Loss: 0.491\n",
      "Iteration: 292 \t--- Loss: 0.498\n",
      "Iteration: 293 \t--- Loss: 0.511\n",
      "Iteration: 294 \t--- Loss: 0.500\n",
      "Iteration: 295 \t--- Loss: 0.518\n",
      "Iteration: 296 \t--- Loss: 0.500\n",
      "Iteration: 297 \t--- Loss: 0.498\n",
      "Iteration: 298 \t--- Loss: 0.514\n",
      "Iteration: 299 \t--- Loss: 0.496\n",
      "Iteration: 300 \t--- Loss: 0.504\n",
      "Iteration: 301 \t--- Loss: 0.505\n",
      "Iteration: 302 \t--- Loss: 0.502\n",
      "Iteration: 303 \t--- Loss: 0.502\n",
      "Iteration: 304 \t--- Loss: 0.500\n",
      "Iteration: 305 \t--- Loss: 0.486\n",
      "Iteration: 306 \t--- Loss: 0.483\n",
      "Iteration: 307 \t--- Loss: 0.495\n",
      "Iteration: 308 \t--- Loss: 0.500\n",
      "Iteration: 309 \t--- Loss: 0.502\n",
      "Iteration: 310 \t--- Loss: 0.501\n",
      "Iteration: 311 \t--- Loss: 0.499\n",
      "Iteration: 312 \t--- Loss: 0.497\n",
      "Iteration: 313 \t--- Loss: 0.500\n",
      "Iteration: 314 \t--- Loss: 0.502\n",
      "Iteration: 315 \t--- Loss: 0.499\n",
      "Iteration: 316 \t--- Loss: 0.507\n",
      "Iteration: 317 \t--- Loss: 0.497\n",
      "Iteration: 318 \t--- Loss: 0.508\n",
      "Iteration: 319 \t--- Loss: 0.493\n",
      "Iteration: 320 \t--- Loss: 0.496\n",
      "Iteration: 321 \t--- Loss: 0.477\n",
      "Iteration: 322 \t--- Loss: 0.499\n",
      "Iteration: 323 \t--- Loss: 0.501\n",
      "Iteration: 324 \t--- Loss: 0.497\n",
      "Iteration: 325 \t--- Loss: 0.485\n",
      "Iteration: 326 \t--- Loss: 0.501\n",
      "Iteration: 327 \t--- Loss: 0.498\n",
      "Iteration: 328 \t--- Loss: 0.502\n",
      "Iteration: 329 \t--- Loss: 0.490\n",
      "Iteration: 330 \t--- Loss: 0.517\n",
      "Iteration: 331 \t--- Loss: 0.519\n",
      "Iteration: 332 \t--- Loss: 0.503\n",
      "Iteration: 333 \t--- Loss: 0.490\n",
      "Iteration: 334 \t--- Loss: 0.505\n",
      "Iteration: 335 \t--- Loss: 0.475\n",
      "Iteration: 336 \t--- Loss: 0.486\n",
      "Iteration: 337 \t--- Loss: 0.499\n",
      "Iteration: 338 \t--- Loss: 0.495\n",
      "Iteration: 339 \t--- Loss: 0.501\n",
      "Iteration: 340 \t--- Loss: 0.507\n",
      "Iteration: 341 \t--- Loss: 0.498\n",
      "Iteration: 342 \t--- Loss: 0.498\n",
      "Iteration: 343 \t--- Loss: 0.508\n",
      "Iteration: 344 \t--- Loss: 0.492\n",
      "Iteration: 345 \t--- Loss: 0.486\n",
      "Iteration: 346 \t--- Loss: 0.512\n",
      "Iteration: 347 \t--- Loss: 0.503\n",
      "Iteration: 348 \t--- Loss: 0.501\n",
      "Iteration: 349 \t--- Loss: 0.502\n",
      "Iteration: 350 \t--- Loss: 0.503\n",
      "Iteration: 351 \t--- Loss: 0.491\n",
      "Iteration: 352 \t--- Loss: 0.498\n",
      "Iteration: 353 \t--- Loss: 0.508\n",
      "Iteration: 354 \t--- Loss: 0.513\n",
      "Iteration: 355 \t--- Loss: 0.497\n",
      "Iteration: 356 \t--- Loss: 0.506\n",
      "Iteration: 357 \t--- Loss: 0.510\n",
      "Iteration: 358 \t--- Loss: 0.498\n",
      "Iteration: 359 \t--- Loss: 0.506\n",
      "Iteration: 360 \t--- Loss: 0.480\n",
      "Iteration: 361 \t--- Loss: 0.501\n",
      "Iteration: 362 \t--- Loss: 0.491\n",
      "Iteration: 363 \t--- Loss: 0.494\n",
      "Iteration: 364 \t--- Loss: 0.490\n",
      "Iteration: 365 \t--- Loss: 0.492\n",
      "Iteration: 366 \t--- Loss: 0.501\n",
      "Iteration: 367 \t--- Loss: 0.505\n",
      "Iteration: 368 \t--- Loss: 0.502\n",
      "Iteration: 369 \t--- Loss: 0.475\n",
      "Iteration: 370 \t--- Loss: 0.502\n",
      "Iteration: 371 \t--- Loss: 0.497\n",
      "Iteration: 372 \t--- Loss: 0.516\n",
      "Iteration: 373 \t--- Loss: 0.490\n",
      "Iteration: 374 \t--- Loss: 0.497\n",
      "Iteration: 375 \t--- Loss: 0.498\n",
      "Iteration: 376 \t--- Loss: 0.499\n",
      "Iteration: 377 \t--- Loss: 0.501\n",
      "Iteration: 378 \t--- Loss: 0.505\n",
      "Iteration: 379 \t--- Loss: 0.495\n",
      "Iteration: 380 \t--- Loss: 0.494\n",
      "Iteration: 381 \t--- Loss: 0.501\n",
      "Iteration: 382 \t--- Loss: 0.494\n",
      "Iteration: 383 \t--- Loss: 0.485\n",
      "Iteration: 384 \t--- Loss: 0.506\n",
      "Iteration: 385 \t--- Loss: 0.511\n",
      "Iteration: 386 \t--- Loss: 0.500\n",
      "Iteration: 387 \t--- Loss: 0.498\n",
      "Iteration: 388 \t--- Loss: 0.491\n",
      "Iteration: 389 \t--- Loss: 0.492\n",
      "Iteration: 390 \t--- Loss: 0.481\n",
      "Iteration: 391 \t--- Loss: 0.502\n",
      "Iteration: 392 \t--- Loss: 0.496\n",
      "Iteration: 393 \t--- Loss: 0.511\n",
      "Iteration: 394 \t--- Loss: 0.498\n",
      "Iteration: 395 \t--- Loss: 0.503\n",
      "Iteration: 396 \t--- Loss: 0.507\n",
      "Iteration: 397 \t--- Loss: 0.502\n",
      "Iteration: 398 \t--- Loss: 0.500\n",
      "Iteration: 399 \t--- Loss: 0.506\n",
      "Iteration: 400 \t--- Loss: 0.501\n",
      "Iteration: 401 \t--- Loss: 0.495\n",
      "Iteration: 402 \t--- Loss: 0.481\n",
      "Iteration: 403 \t--- Loss: 0.502\n",
      "Iteration: 404 \t--- Loss: 0.499\n",
      "Iteration: 405 \t--- Loss: 0.497\n",
      "Iteration: 406 \t--- Loss: 0.512\n",
      "Iteration: 407 \t--- Loss: 0.499\n",
      "Iteration: 408 \t--- Loss: 0.486\n",
      "Iteration: 409 \t--- Loss: 0.499\n",
      "Iteration: 410 \t--- Loss: 0.490\n",
      "Iteration: 411 \t--- Loss: 0.505\n",
      "Iteration: 412 \t--- Loss: 0.488\n",
      "Iteration: 413 \t--- Loss: 0.495\n",
      "Iteration: 414 \t--- Loss: 0.492\n",
      "Iteration: 415 \t--- Loss: 0.510\n",
      "Iteration: 416 \t--- Loss: 0.509\n",
      "Iteration: 417 \t--- Loss: 0.499\n",
      "Iteration: 418 \t--- Loss: 0.504\n",
      "Iteration: 419 \t--- Loss: 0.493\n",
      "Iteration: 420 \t--- Loss: 0.494\n",
      "Iteration: 421 \t--- Loss: 0.498\n",
      "Iteration: 422 \t--- Loss: 0.502\n",
      "Iteration: 423 \t--- Loss: 0.506\n",
      "Iteration: 424 \t--- Loss: 0.508\n",
      "Iteration: 425 \t--- Loss: 0.499\n",
      "Iteration: 426 \t--- Loss: 0.505\n",
      "Iteration: 427 \t--- Loss: 0.492\n",
      "Iteration: 428 \t--- Loss: 0.474\n",
      "Iteration: 429 \t--- Loss: 0.484\n",
      "Iteration: 430 \t--- Loss: 0.503\n",
      "Iteration: 431 \t--- Loss: 0.512\n",
      "Iteration: 432 \t--- Loss: 0.505\n",
      "Iteration: 433 \t--- Loss: 0.510\n",
      "Iteration: 434 \t--- Loss: 0.511\n",
      "Iteration: 435 \t--- Loss: 0.494\n",
      "Iteration: 436 \t--- Loss: 0.490\n",
      "Iteration: 437 \t--- Loss: 0.504\n",
      "Iteration: 438 \t--- Loss: 0.497\n",
      "Iteration: 439 \t--- Loss: 0.492\n",
      "Iteration: 440 \t--- Loss: 0.508\n",
      "Iteration: 441 \t--- Loss: 0.511\n",
      "Iteration: 442 \t--- Loss: 0.511\n",
      "Iteration: 443 \t--- Loss: 0.491\n",
      "Iteration: 444 \t--- Loss: 0.506\n",
      "Iteration: 445 \t--- Loss: 0.510\n",
      "Iteration: 446 \t--- Loss: 0.508\n",
      "Iteration: 447 \t--- Loss: 0.506\n",
      "Iteration: 448 \t--- Loss: 0.491\n",
      "Iteration: 449 \t--- Loss: 0.497\n",
      "Iteration: 450 \t--- Loss: 0.493\n",
      "Iteration: 451 \t--- Loss: 0.512\n",
      "Iteration: 452 \t--- Loss: 0.497\n",
      "Iteration: 453 \t--- Loss: 0.503\n",
      "Iteration: 454 \t--- Loss: 0.497\n",
      "Iteration: 455 \t--- Loss: 0.505\n",
      "Iteration: 456 \t--- Loss: 0.498\n",
      "Iteration: 457 \t--- Loss: 0.486\n",
      "Iteration: 458 \t--- Loss: 0.502\n",
      "Iteration: 459 \t--- Loss: 0.509\n",
      "Iteration: 460 \t--- Loss: 0.493\n",
      "Iteration: 461 \t--- Loss: 0.502\n",
      "Iteration: 462 \t--- Loss: 0.491\n",
      "Iteration: 463 \t--- Loss: 0.514\n",
      "Iteration: 464 \t--- Loss: 0.489\n",
      "Iteration: 465 \t--- Loss: 0.507\n",
      "Iteration: 466 \t--- Loss: 0.487\n",
      "Iteration: 467 \t--- Loss: 0.501\n",
      "Iteration: 468 \t--- Loss: 0.502\n",
      "Iteration: 469 \t--- Loss: 0.502\n",
      "Iteration: 470 \t--- Loss: 0.500\n",
      "Iteration: 471 \t--- Loss: 0.498\n",
      "Iteration: 472 \t--- Loss: 0.495\n",
      "Iteration: 473 \t--- Loss: 0.500\n",
      "Iteration: 474 \t--- Loss: 0.507\n",
      "Iteration: 475 \t--- Loss: 0.494\n",
      "Iteration: 476 \t--- Loss: 0.499\n",
      "Iteration: 477 \t--- Loss: 0.484\n",
      "Iteration: 478 \t--- Loss: 0.486\n",
      "Iteration: 479 \t--- Loss: 0.503\n",
      "Iteration: 480 \t--- Loss: 0.495\n",
      "Iteration: 481 \t--- Loss: 0.502\n",
      "Iteration: 482 \t--- Loss: 0.499\n",
      "Iteration: 483 \t--- Loss: 0.495\n",
      "Iteration: 484 \t--- Loss: 0.514\n",
      "Iteration: 485 \t--- Loss: 0.504\n",
      "Iteration: 486 \t--- Loss: 0.499\n",
      "Iteration: 487 \t--- Loss: 0.504\n",
      "Iteration: 488 \t--- Loss: 0.512\n",
      "Iteration: 489 \t--- Loss: 0.502\n",
      "Iteration: 490 \t--- Loss: 0.490\n",
      "Iteration: 491 \t--- Loss: 0.492\n",
      "Iteration: 492 \t--- Loss: 0.488\n",
      "Iteration: 493 \t--- Loss: 0.501\n",
      "Iteration: 494 \t--- Loss: 0.503\n",
      "Iteration: 495 \t--- Loss: 0.497\n",
      "Iteration: 496 \t--- Loss: 0.498\n",
      "Iteration: 497 \t--- Loss: 0.491\n",
      "Iteration: 498 \t--- Loss: 0.498\n",
      "Iteration: 499 \t--- Loss: 0.509\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it][Parallel(n_jobs=5)]: Done  55 tasks      | elapsed: 33.0min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.22s/it][Parallel(n_jobs=5)]: Done  56 tasks      | elapsed: 33.3min\n",
      "100%|██████████| 10/10 [00:31<00:00,  3.17s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 1.983\n",
      "Iteration: 1 \t--- Loss: 1.750\n",
      "Iteration: 2 \t--- Loss: 1.709\n",
      "Iteration: 3 \t--- Loss: 1.701\n",
      "Iteration: 4 \t--- Loss: 1.505\n",
      "Iteration: 5 \t--- Loss: 1.383\n",
      "Iteration: 6 \t--- Loss: 1.400\n",
      "Iteration: 7 \t--- Loss: 1.367\n",
      "Iteration: 8 \t--- Loss: 1.403\n",
      "Iteration: 9 \t--- Loss: 1.356\n",
      "Iteration: 10 \t--- Loss: 1.349\n",
      "Iteration: 11 \t--- Loss: 1.289\n",
      "Iteration: 12 \t--- Loss: 1.325\n",
      "Iteration: 13 \t--- Loss: 1.355\n",
      "Iteration: 14 \t--- Loss: 1.308\n",
      "Iteration: 15 \t--- Loss: 1.305\n",
      "Iteration: 16 \t--- Loss: 1.243\n",
      "Iteration: 17 \t--- Loss: 1.327\n",
      "Iteration: 18 \t--- Loss: 1.284\n",
      "Iteration: 19 \t--- Loss: 1.299\n",
      "Iteration: 20 \t--- Loss: 1.294\n",
      "Iteration: 21 \t--- Loss: 1.380\n",
      "Iteration: 22 \t--- Loss: 1.331\n",
      "Iteration: 23 \t--- Loss: 1.318\n",
      "Iteration: 24 \t--- Loss: 1.281\n",
      "Iteration: 25 \t--- Loss: 1.273\n",
      "Iteration: 26 \t--- Loss: 1.302\n",
      "Iteration: 27 \t--- Loss: 1.332\n",
      "Iteration: 28 \t--- Loss: 1.231\n",
      "Iteration: 29 \t--- Loss: 1.296\n",
      "Iteration: 30 \t--- Loss: 1.295\n",
      "Iteration: 31 \t--- Loss: 1.321\n",
      "Iteration: 32 \t--- Loss: 1.257\n",
      "Iteration: 33 \t--- Loss: 1.338\n",
      "Iteration: 34 \t--- Loss: 1.292\n",
      "Iteration: 35 \t--- Loss: 1.298\n",
      "Iteration: 36 \t--- Loss: 1.266\n",
      "Iteration: 37 \t--- Loss: 1.273\n",
      "Iteration: 38 \t--- Loss: 1.282\n",
      "Iteration: 39 \t--- Loss: 1.316\n",
      "Iteration: 40 \t--- Loss: 1.306\n",
      "Iteration: 41 \t--- Loss: 1.293\n",
      "Iteration: 42 \t--- Loss: 1.276\n",
      "Iteration: 43 \t--- Loss: 1.349\n",
      "Iteration: 44 \t--- Loss: 1.249\n",
      "Iteration: 45 \t--- Loss: 1.335\n",
      "Iteration: 46 \t--- Loss: 1.260\n",
      "Iteration: 47 \t--- Loss: 1.317\n",
      "Iteration: 48 \t--- Loss: 1.306\n",
      "Iteration: 49 \t--- Loss: 1.295\n",
      "Iteration: 50 \t--- Loss: 1.293\n",
      "Iteration: 51 \t--- Loss: 1.290\n",
      "Iteration: 52 \t--- Loss: 1.280\n",
      "Iteration: 53 \t--- Loss: 1.335\n",
      "Iteration: 54 \t--- Loss: 1.317\n",
      "Iteration: 55 \t--- Loss: 1.319\n",
      "Iteration: 56 \t--- Loss: 1.271\n",
      "Iteration: 57 \t--- Loss: 1.269\n",
      "Iteration: 58 \t--- Loss: 1.288\n",
      "Iteration: 59 \t--- Loss: 1.258\n",
      "Iteration: 60 \t--- Loss: 1.271\n",
      "Iteration: 61 \t--- Loss: 1.269\n",
      "Iteration: 62 \t--- Loss: 1.264\n",
      "Iteration: 63 \t--- Loss: 1.299\n",
      "Iteration: 64 \t--- Loss: 1.348\n",
      "Iteration: 65 \t--- Loss: 1.265\n",
      "Iteration: 66 \t--- Loss: 1.271\n",
      "Iteration: 67 \t--- Loss: 1.302\n",
      "Iteration: 68 \t--- Loss: 1.321\n",
      "Iteration: 69 \t--- Loss: 1.342\n",
      "Iteration: 70 \t--- Loss: 1.321\n",
      "Iteration: 71 \t--- Loss: 1.305\n",
      "Iteration: 72 \t--- Loss: 1.322\n",
      "Iteration: 73 \t--- Loss: 1.236\n",
      "Iteration: 74 \t--- Loss: 1.303\n",
      "Iteration: 75 \t--- Loss: 1.267\n",
      "Iteration: 76 \t--- Loss: 1.304\n",
      "Iteration: 77 \t--- Loss: 1.282\n",
      "Iteration: 78 \t--- Loss: 1.297\n",
      "Iteration: 79 \t--- Loss: 1.280\n",
      "Iteration: 80 \t--- Loss: 1.278\n",
      "Iteration: 81 \t--- Loss: 1.311\n",
      "Iteration: 82 \t--- Loss: 1.279\n",
      "Iteration: 83 \t--- Loss: 1.242\n",
      "Iteration: 84 \t--- Loss: 1.299\n",
      "Iteration: 85 \t--- Loss: 1.256\n",
      "Iteration: 86 \t--- Loss: 1.286\n",
      "Iteration: 87 \t--- Loss: 1.289\n",
      "Iteration: 88 \t--- Loss: 1.301\n",
      "Iteration: 89 \t--- Loss: 1.273\n",
      "Iteration: 90 \t--- Loss: 1.350\n",
      "Iteration: 91 \t--- Loss: 1.299\n",
      "Iteration: 92 \t--- Loss: 1.284\n",
      "Iteration: 93 \t--- Loss: 1.271\n",
      "Iteration: 94 \t--- Loss: 1.299\n",
      "Iteration: 95 \t--- Loss: 1.308\n",
      "Iteration: 96 \t--- Loss: 1.278\n",
      "Iteration: 97 \t--- Loss: 1.283\n",
      "Iteration: 98 \t--- Loss: 1.252\n",
      "Iteration: 99 \t--- Loss: 1.320\n",
      "Iteration: 100 \t--- Loss: 1.262\n",
      "Iteration: 101 \t--- Loss: 1.272\n",
      "Iteration: 102 \t--- Loss: 1.263\n",
      "Iteration: 103 \t--- Loss: 1.332\n",
      "Iteration: 104 \t--- Loss: 1.345\n",
      "Iteration: 105 \t--- Loss: 1.315\n",
      "Iteration: 106 \t--- Loss: 1.307\n",
      "Iteration: 107 \t--- Loss: 1.271\n",
      "Iteration: 108 \t--- Loss: 1.278\n",
      "Iteration: 109 \t--- Loss: 1.296\n",
      "Iteration: 110 \t--- Loss: 1.305\n",
      "Iteration: 111 \t--- Loss: 1.290\n",
      "Iteration: 112 \t--- Loss: 1.312\n",
      "Iteration: 113 \t--- Loss: 1.320\n",
      "Iteration: 114 \t--- Loss: 1.282\n",
      "Iteration: 115 \t--- Loss: 1.278\n",
      "Iteration: 116 \t--- Loss: 1.285\n",
      "Iteration: 117 \t--- Loss: 1.316\n",
      "Iteration: 118 \t--- Loss: 1.348\n",
      "Iteration: 119 \t--- Loss: 1.278\n",
      "Iteration: 120 \t--- Loss: 1.332\n",
      "Iteration: 121 \t--- Loss: 1.292\n",
      "Iteration: 122 \t--- Loss: 1.263\n",
      "Iteration: 123 \t--- Loss: 1.282\n",
      "Iteration: 124 \t--- Loss: 1.298\n",
      "Iteration: 125 \t--- Loss: 1.322\n",
      "Iteration: 126 \t--- Loss: 1.288\n",
      "Iteration: 127 \t--- Loss: 1.335\n",
      "Iteration: 128 \t--- Loss: 1.293\n",
      "Iteration: 129 \t--- Loss: 1.320\n",
      "Iteration: 130 \t--- Loss: 1.303\n",
      "Iteration: 131 \t--- Loss: 1.272\n",
      "Iteration: 132 \t--- Loss: 1.307\n",
      "Iteration: 133 \t--- Loss: 1.272\n",
      "Iteration: 134 \t--- Loss: 1.292\n",
      "Iteration: 135 \t--- Loss: 1.299\n",
      "Iteration: 136 \t--- Loss: 1.312\n",
      "Iteration: 137 \t--- Loss: 1.308\n",
      "Iteration: 138 \t--- Loss: 1.253\n",
      "Iteration: 139 \t--- Loss: 1.302\n",
      "Iteration: 140 \t--- Loss: 1.269\n",
      "Iteration: 141 \t--- Loss: 1.278\n",
      "Iteration: 142 \t--- Loss: 1.270\n",
      "Iteration: 143 \t--- Loss: 1.290\n",
      "Iteration: 144 \t--- Loss: 1.283\n",
      "Iteration: 145 \t--- Loss: 1.298\n",
      "Iteration: 146 \t--- Loss: 1.305\n",
      "Iteration: 147 \t--- Loss: 1.284\n",
      "Iteration: 148 \t--- Loss: 1.293\n",
      "Iteration: 149 \t--- Loss: 1.303\n",
      "Iteration: 150 \t--- Loss: 1.296\n",
      "Iteration: 151 \t--- Loss: 1.212\n",
      "Iteration: 152 \t--- Loss: 1.247\n",
      "Iteration: 153 \t--- Loss: 1.318\n",
      "Iteration: 154 \t--- Loss: 1.277\n",
      "Iteration: 155 \t--- Loss: 1.291\n",
      "Iteration: 156 \t--- Loss: 1.323\n",
      "Iteration: 157 \t--- Loss: 1.338\n",
      "Iteration: 158 \t--- Loss: 1.317\n",
      "Iteration: 159 \t--- Loss: 1.256\n",
      "Iteration: 160 \t--- Loss: 1.268\n",
      "Iteration: 161 \t--- Loss: 1.248\n",
      "Iteration: 162 \t--- Loss: 1.264\n",
      "Iteration: 163 \t--- Loss: 1.345\n",
      "Iteration: 164 \t--- Loss: 1.240\n",
      "Iteration: 165 \t--- Loss: 1.345\n",
      "Iteration: 166 \t--- Loss: 1.364\n",
      "Iteration: 167 \t--- Loss: 1.277\n",
      "Iteration: 168 \t--- Loss: 1.282\n",
      "Iteration: 169 \t--- Loss: 1.261\n",
      "Iteration: 170 \t--- Loss: 1.268\n",
      "Iteration: 171 \t--- Loss: 1.294\n",
      "Iteration: 172 \t--- Loss: 1.296\n",
      "Iteration: 173 \t--- Loss: 1.316\n",
      "Iteration: 174 \t--- Loss: 1.287\n",
      "Iteration: 175 \t--- Loss: 1.307\n",
      "Iteration: 176 \t--- Loss: 1.286\n",
      "Iteration: 177 \t--- Loss: 1.305\n",
      "Iteration: 178 \t--- Loss: 1.307\n",
      "Iteration: 179 \t--- Loss: 1.251\n",
      "Iteration: 180 \t--- Loss: 1.336\n",
      "Iteration: 181 \t--- Loss: 1.272\n",
      "Iteration: 182 \t--- Loss: 1.326\n",
      "Iteration: 183 \t--- Loss: 1.335\n",
      "Iteration: 184 \t--- Loss: 1.310\n",
      "Iteration: 185 \t--- Loss: 1.247\n",
      "Iteration: 186 \t--- Loss: 1.274\n",
      "Iteration: 187 \t--- Loss: 1.291\n",
      "Iteration: 188 \t--- Loss: 1.269\n",
      "Iteration: 189 \t--- Loss: 1.313\n",
      "Iteration: 190 \t--- Loss: 1.345\n",
      "Iteration: 191 \t--- Loss: 1.321\n",
      "Iteration: 192 \t--- Loss: 1.305\n",
      "Iteration: 193 \t--- Loss: 1.290\n",
      "Iteration: 194 \t--- Loss: 1.292\n",
      "Iteration: 195 \t--- Loss: 1.313\n",
      "Iteration: 196 \t--- Loss: 1.291\n",
      "Iteration: 197 \t--- Loss: 1.332\n",
      "Iteration: 198 \t--- Loss: 1.266\n",
      "Iteration: 199 \t--- Loss: 1.310\n",
      "Iteration: 200 \t--- Loss: 1.282\n",
      "Iteration: 201 \t--- Loss: 1.304\n",
      "Iteration: 202 \t--- Loss: 1.271\n",
      "Iteration: 203 \t--- Loss: 1.296\n",
      "Iteration: 204 \t--- Loss: 1.319\n",
      "Iteration: 205 \t--- Loss: 1.230\n",
      "Iteration: 206 \t--- Loss: 1.331\n",
      "Iteration: 207 \t--- Loss: 1.283\n",
      "Iteration: 208 \t--- Loss: 1.280\n",
      "Iteration: 209 \t--- Loss: 1.364\n",
      "Iteration: 210 \t--- Loss: 1.305\n",
      "Iteration: 211 \t--- Loss: 1.333\n",
      "Iteration: 212 \t--- Loss: 1.255\n",
      "Iteration: 213 \t--- Loss: 1.341\n",
      "Iteration: 214 \t--- Loss: 1.312\n",
      "Iteration: 215 \t--- Loss: 1.306\n",
      "Iteration: 216 \t--- Loss: 1.270\n",
      "Iteration: 217 \t--- Loss: 1.310\n",
      "Iteration: 218 \t--- Loss: 1.304\n",
      "Iteration: 219 \t--- Loss: 1.301\n",
      "Iteration: 220 \t--- Loss: 1.292\n",
      "Iteration: 221 \t--- Loss: 1.248\n",
      "Iteration: 222 \t--- Loss: 1.357\n",
      "Iteration: 223 \t--- Loss: 1.351\n",
      "Iteration: 224 \t--- Loss: 1.304\n",
      "Iteration: 225 \t--- Loss: 1.329\n",
      "Iteration: 226 \t--- Loss: 1.324\n",
      "Iteration: 227 \t--- Loss: 1.285\n",
      "Iteration: 228 \t--- Loss: 1.270\n",
      "Iteration: 229 \t--- Loss: 1.331\n",
      "Iteration: 230 \t--- Loss: 1.246\n",
      "Iteration: 231 \t--- Loss: 1.284\n",
      "Iteration: 232 \t--- Loss: 1.349\n",
      "Iteration: 233 \t--- Loss: 1.261\n",
      "Iteration: 234 \t--- Loss: 1.311\n",
      "Iteration: 235 \t--- Loss: 1.266\n",
      "Iteration: 236 \t--- Loss: 1.286\n",
      "Iteration: 237 \t--- Loss: 1.301\n",
      "Iteration: 238 \t--- Loss: 1.309\n",
      "Iteration: 239 \t--- Loss: 1.335\n",
      "Iteration: 240 \t--- Loss: 1.313\n",
      "Iteration: 241 \t--- Loss: 1.275\n",
      "Iteration: 242 \t--- Loss: 1.303\n",
      "Iteration: 243 \t--- Loss: 1.230\n",
      "Iteration: 244 \t--- Loss: 1.267\n",
      "Iteration: 245 \t--- Loss: 1.284\n",
      "Iteration: 246 \t--- Loss: 1.343\n",
      "Iteration: 247 \t--- Loss: 1.304\n",
      "Iteration: 248 \t--- Loss: 1.240\n",
      "Iteration: 249 \t--- Loss: 1.280\n",
      "Iteration: 250 \t--- Loss: 1.273\n",
      "Iteration: 251 \t--- Loss: 1.330\n",
      "Iteration: 252 \t--- Loss: 1.215\n",
      "Iteration: 253 \t--- Loss: 1.256\n",
      "Iteration: 254 \t--- Loss: 1.313\n",
      "Iteration: 255 \t--- Loss: 1.331\n",
      "Iteration: 256 \t--- Loss: 1.317\n",
      "Iteration: 257 \t--- Loss: 1.385\n",
      "Iteration: 258 \t--- Loss: 1.257\n",
      "Iteration: 259 \t--- Loss: 1.259Iteration: 0 \t--- Loss: 0.372\n",
      "Iteration: 1 \t--- Loss: 0.328\n",
      "Iteration: 2 \t--- Loss: 0.313\n",
      "Iteration: 3 \t--- Loss: 0.295\n",
      "Iteration: 4 \t--- Loss: 0.272\n",
      "Iteration: 5 \t--- Loss: 0.235\n",
      "Iteration: 6 \t--- Loss: 0.233\n",
      "Iteration: 7 \t--- Loss: 0.220\n",
      "Iteration: 8 \t--- Loss: 0.210\n",
      "Iteration: 9 \t--- Loss: 0.192\n",
      "Iteration: 10 \t--- Loss: 0.198\n",
      "Iteration: 11 \t--- Loss: 0.172\n",
      "Iteration: 12 \t--- Loss: 0.166\n",
      "Iteration: 13 \t--- Loss: 0.164\n",
      "Iteration: 14 \t--- Loss: 0.165\n",
      "Iteration: 15 \t--- Loss: 0.164\n",
      "Iteration: 16 \t--- Loss: 0.149\n",
      "Iteration: 17 \t--- Loss: 0.148\n",
      "Iteration: 18 \t--- Loss: 0.154\n",
      "Iteration: 19 \t--- Loss: 0.143\n",
      "Iteration: 20 \t--- Loss: 0.145\n",
      "Iteration: 21 \t--- Loss: 0.141\n",
      "Iteration: 22 \t--- Loss: 0.138\n",
      "Iteration: 23 \t--- Loss: 0.135\n",
      "Iteration: 24 \t--- Loss: 0.130\n",
      "Iteration: 25 \t--- Loss: 0.135\n",
      "Iteration: 26 \t--- Loss: 0.136\n",
      "Iteration: 27 \t--- Loss: 0.134\n",
      "Iteration: 28 \t--- Loss: 0.128\n",
      "Iteration: 29 \t--- Loss: 0.135\n",
      "Iteration: 30 \t--- Loss: 0.128\n",
      "Iteration: 31 \t--- Loss: 0.126\n",
      "Iteration: 32 \t--- Loss: 0.123\n",
      "Iteration: 33 \t--- Loss: 0.122\n",
      "Iteration: 34 \t--- Loss: 0.127\n",
      "Iteration: 35 \t--- Loss: 0.123\n",
      "Iteration: 36 \t--- Loss: 0.128\n",
      "Iteration: 37 \t--- Loss: 0.122\n",
      "Iteration: 38 \t--- Loss: 0.125\n",
      "Iteration: 39 \t--- Loss: 0.127\n",
      "Iteration: 40 \t--- Loss: 0.124\n",
      "Iteration: 41 \t--- Loss: 0.124\n",
      "Iteration: 42 \t--- Loss: 0.122\n",
      "Iteration: 43 \t--- Loss: 0.122\n",
      "Iteration: 44 \t--- Loss: 0.120\n",
      "Iteration: 45 \t--- Loss: 0.122\n",
      "Iteration: 46 \t--- Loss: 0.122\n",
      "Iteration: 47 \t--- Loss: 0.124\n",
      "Iteration: 48 \t--- Loss: 0.127\n",
      "Iteration: 49 \t--- Loss: 0.122\n",
      "Iteration: 50 \t--- Loss: 0.115\n",
      "Iteration: 51 \t--- Loss: 0.118\n",
      "Iteration: 52 \t--- Loss: 0.121\n",
      "Iteration: 53 \t--- Loss: 0.114\n",
      "Iteration: 54 \t--- Loss: 0.120\n",
      "Iteration: 55 \t--- Loss: 0.120\n",
      "Iteration: 56 \t--- Loss: 0.123\n",
      "Iteration: 57 \t--- Loss: 0.121\n",
      "Iteration: 58 \t--- Loss: 0.117\n",
      "Iteration: 59 \t--- Loss: 0.119\n",
      "Iteration: 60 \t--- Loss: 0.115\n",
      "Iteration: 61 \t--- Loss: 0.118\n",
      "Iteration: 62 \t--- Loss: 0.119\n",
      "Iteration: 63 \t--- Loss: 0.119\n",
      "Iteration: 64 \t--- Loss: 0.119\n",
      "Iteration: 65 \t--- Loss: 0.119\n",
      "Iteration: 66 \t--- Loss: 0.121\n",
      "Iteration: 67 \t--- Loss: 0.118\n",
      "Iteration: 68 \t--- Loss: 0.118\n",
      "Iteration: 69 \t--- Loss: 0.121\n",
      "Iteration: 70 \t--- Loss: 0.116\n",
      "Iteration: 71 \t--- Loss: 0.117\n",
      "Iteration: 72 \t--- Loss: 0.118\n",
      "Iteration: 73 \t--- Loss: 0.120\n",
      "Iteration: 74 \t--- Loss: 0.113\n",
      "Iteration: 75 \t--- Loss: 0.118\n",
      "Iteration: 76 \t--- Loss: 0.120\n",
      "Iteration: 77 \t--- Loss: 0.115\n",
      "Iteration: 78 \t--- Loss: 0.119\n",
      "Iteration: 79 \t--- Loss: 0.120\n",
      "Iteration: 80 \t--- Loss: 0.116\n",
      "Iteration: 81 \t--- Loss: 0.116\n",
      "Iteration: 82 \t--- Loss: 0.117\n",
      "Iteration: 83 \t--- Loss: 0.121\n",
      "Iteration: 84 \t--- Loss: 0.121\n",
      "Iteration: 85 \t--- Loss: 0.116\n",
      "Iteration: 86 \t--- Loss: 0.116\n",
      "Iteration: 87 \t--- Loss: 0.115\n",
      "Iteration: 88 \t--- Loss: 0.116\n",
      "Iteration: 89 \t--- Loss: 0.119\n",
      "Iteration: 90 \t--- Loss: 0.114\n",
      "Iteration: 91 \t--- Loss: 0.117\n",
      "Iteration: 92 \t--- Loss: 0.116\n",
      "Iteration: 93 \t--- Loss: 0.116\n",
      "Iteration: 94 \t--- Loss: 0.118\n",
      "Iteration: 95 \t--- Loss: 0.119\n",
      "Iteration: 96 \t--- Loss: 0.118\n",
      "Iteration: 97 \t--- Loss: 0.118\n",
      "Iteration: 98 \t--- Loss: 0.119\n",
      "Iteration: 99 \t--- Loss: 0.121\n",
      "Iteration: 100 \t--- Loss: 0.113\n",
      "Iteration: 101 \t--- Loss: 0.118\n",
      "Iteration: 102 \t--- Loss: 0.117\n",
      "Iteration: 103 \t--- Loss: 0.116\n",
      "Iteration: 104 \t--- Loss: 0.117\n",
      "Iteration: 105 \t--- Loss: 0.120\n",
      "Iteration: 106 \t--- Loss: 0.116\n",
      "Iteration: 107 \t--- Loss: 0.116\n",
      "Iteration: 108 \t--- Loss: 0.115\n",
      "Iteration: 109 \t--- Loss: 0.118\n",
      "Iteration: 110 \t--- Loss: 0.120\n",
      "Iteration: 111 \t--- Loss: 0.117\n",
      "Iteration: 112 \t--- Loss: 0.120\n",
      "Iteration: 113 \t--- Loss: 0.117\n",
      "Iteration: 114 \t--- Loss: 0.118\n",
      "Iteration: 115 \t--- Loss: 0.118\n",
      "Iteration: 116 \t--- Loss: 0.116\n",
      "Iteration: 117 \t--- Loss: 0.119\n",
      "Iteration: 118 \t--- Loss: 0.117\n",
      "Iteration: 119 \t--- Loss: 0.116\n",
      "Iteration: 120 \t--- Loss: 0.119\n",
      "Iteration: 121 \t--- Loss: 0.115\n",
      "Iteration: 122 \t--- Loss: 0.119\n",
      "Iteration: 123 \t--- Loss: 0.117\n",
      "Iteration: 124 \t--- Loss: 0.118\n",
      "Iteration: 125 \t--- Loss: 0.117\n",
      "Iteration: 126 \t--- Loss: 0.119\n",
      "Iteration: 127 \t--- Loss: 0.117\n",
      "Iteration: 128 \t--- Loss: 0.119\n",
      "Iteration: 129 \t--- Loss: 0.116\n",
      "Iteration: 130 \t--- Loss: 0.120\n",
      "Iteration: 131 \t--- Loss: 0.116\n",
      "Iteration: 132 \t--- Loss: 0.113\n",
      "Iteration: 133 \t--- Loss: 0.117\n",
      "Iteration: 134 \t--- Loss: 0.118\n",
      "Iteration: 135 \t--- Loss: 0.115\n",
      "Iteration: 136 \t--- Loss: 0.118\n",
      "Iteration: 137 \t--- Loss: 0.116\n",
      "Iteration: 138 \t--- Loss: 0.118\n",
      "Iteration: 139 \t--- Loss: 0.120\n",
      "Iteration: 140 \t--- Loss: 0.116\n",
      "Iteration: 141 \t--- Loss: 0.115\n",
      "Iteration: 142 \t--- Loss: 0.116\n",
      "Iteration: 143 \t--- Loss: 0.117\n",
      "Iteration: 144 \t--- Loss: 0.114\n",
      "Iteration: 145 \t--- Loss: 0.115\n",
      "Iteration: 146 \t--- Loss: 0.119\n",
      "Iteration: 147 \t--- Loss: 0.117\n",
      "Iteration: 148 \t--- Loss: 0.117\n",
      "Iteration: 149 \t--- Loss: 0.118\n",
      "Iteration: 150 \t--- Loss: 0.116\n",
      "Iteration: 151 \t--- Loss: 0.119\n",
      "Iteration: 152 \t--- Loss: 0.120\n",
      "Iteration: 153 \t--- Loss: 0.120\n",
      "Iteration: 154 \t--- Loss: 0.117\n",
      "Iteration: 155 \t--- Loss: 0.116\n",
      "Iteration: 156 \t--- Loss: 0.118\n",
      "Iteration: 157 \t--- Loss: 0.117\n",
      "Iteration: 158 \t--- Loss: 0.112\n",
      "Iteration: 159 \t--- Loss: 0.120\n",
      "Iteration: 160 \t--- Loss: 0.118\n",
      "Iteration: 161 \t--- Loss: 0.118\n",
      "Iteration: 162 \t--- Loss: 0.116\n",
      "Iteration: 163 \t--- Loss: 0.115\n",
      "Iteration: 164 \t--- Loss: 0.115\n",
      "Iteration: 165 \t--- Loss: 0.117\n",
      "Iteration: 166 \t--- Loss: 0.118\n",
      "Iteration: 167 \t--- Loss: 0.119\n",
      "Iteration: 168 \t--- Loss: 0.117\n",
      "Iteration: 169 \t--- Loss: 0.122\n",
      "Iteration: 170 \t--- Loss: 0.119\n",
      "Iteration: 171 \t--- Loss: 0.114\n",
      "Iteration: 172 \t--- Loss: 0.117\n",
      "Iteration: 173 \t--- Loss: 0.115\n",
      "Iteration: 174 \t--- Loss: 0.117\n",
      "Iteration: 175 \t--- Loss: 0.119\n",
      "Iteration: 176 \t--- Loss: 0.116\n",
      "Iteration: 177 \t--- Loss: 0.116\n",
      "Iteration: 178 \t--- Loss: 0.114\n",
      "Iteration: 179 \t--- Loss: 0.115\n",
      "Iteration: 180 \t--- Loss: 0.117\n",
      "Iteration: 181 \t--- Loss: 0.116\n",
      "Iteration: 182 \t--- Loss: 0.119\n",
      "Iteration: 183 \t--- Loss: 0.117\n",
      "Iteration: 184 \t--- Loss: 0.116\n",
      "Iteration: 185 \t--- Loss: 0.116\n",
      "Iteration: 186 \t--- Loss: 0.116\n",
      "Iteration: 187 \t--- Loss: 0.120\n",
      "Iteration: 188 \t--- Loss: 0.113\n",
      "Iteration: 189 \t--- Loss: 0.116\n",
      "Iteration: 190 \t--- Loss: 0.118\n",
      "Iteration: 191 \t--- Loss: 0.120\n",
      "Iteration: 192 \t--- Loss: 0.119\n",
      "Iteration: 193 \t--- Loss: 0.114\n",
      "Iteration: 194 \t--- Loss: 0.120\n",
      "Iteration: 195 \t--- Loss: 0.115\n",
      "Iteration: 196 \t--- Loss: 0.119\n",
      "Iteration: 197 \t--- Loss: 0.117\n",
      "Iteration: 198 \t--- Loss: 0.112\n",
      "Iteration: 199 \t--- Loss: 0.119\n",
      "Iteration: 200 \t--- Loss: 0.115\n",
      "Iteration: 201 \t--- Loss: 0.118\n",
      "Iteration: 202 \t--- Loss: 0.114\n",
      "Iteration: 203 \t--- Loss: 0.119\n",
      "Iteration: 204 \t--- Loss: 0.118\n",
      "Iteration: 205 \t--- Loss: 0.120\n",
      "Iteration: 206 \t--- Loss: 0.116\n",
      "Iteration: 207 \t--- Loss: 0.116\n",
      "Iteration: 208 \t--- Loss: 0.114\n",
      "Iteration: 209 \t--- Loss: 0.117\n",
      "Iteration: 210 \t--- Loss: 0.118\n",
      "Iteration: 211 \t--- Loss: 0.122\n",
      "Iteration: 212 \t--- Loss: 0.120\n",
      "Iteration: 213 \t--- Loss: 0.116\n",
      "Iteration: 214 \t--- Loss: 0.118\n",
      "Iteration: 215 \t--- Loss: 0.115\n",
      "Iteration: 216 \t--- Loss: 0.115\n",
      "Iteration: 217 \t--- Loss: 0.116\n",
      "Iteration: 218 \t--- Loss: 0.113\n",
      "Iteration: 219 \t--- Loss: 0.121\n",
      "Iteration: 220 \t--- Loss: 0.120\n",
      "Iteration: 221 \t--- Loss: 0.120\n",
      "Iteration: 222 \t--- Loss: 0.116\n",
      "Iteration: 223 \t--- Loss: 0.116\n",
      "Iteration: 224 \t--- Loss: 0.116\n",
      "Iteration: 225 \t--- Loss: 0.117\n",
      "Iteration: 226 \t--- Loss: 0.118\n",
      "Iteration: 227 \t--- Loss: 0.121\n",
      "Iteration: 228 \t--- Loss: 0.118\n",
      "Iteration: 229 \t--- Loss: 0.118\n",
      "Iteration: 230 \t--- Loss: 0.120\n",
      "Iteration: 231 \t--- Loss: 0.116\n",
      "Iteration: 232 \t--- Loss: 0.115\n",
      "Iteration: 233 \t--- Loss: 0.117\n",
      "Iteration: 234 \t--- Loss: 0.116\n",
      "Iteration: 235 \t--- Loss: 0.119\n",
      "Iteration: 236 \t--- Loss: 0.119\n",
      "Iteration: 237 \t--- Loss: 0.117\n",
      "Iteration: 238 \t--- Loss: 0.116\n",
      "Iteration: 239 \t--- Loss: 0.119\n",
      "Iteration: 240 \t--- Loss: 0.116\n",
      "Iteration: 241 \t--- Loss: 0.116\n",
      "Iteration: 242 \t--- Loss: 0.117\n",
      "Iteration: 243 \t--- Loss: 0.119\n",
      "Iteration: 244 \t--- Loss: 0.116\n",
      "Iteration: 245 \t--- Loss: 0.118\n",
      "Iteration: 246 \t--- Loss: 0.116\n",
      "Iteration: 247 \t--- Loss: 0.118\n",
      "Iteration: 248 \t--- Loss: 0.119\n",
      "Iteration: 249 \t--- Loss: 0.118\n",
      "Iteration: 250 \t--- Loss: 0.118\n",
      "Iteration: 251 \t--- Loss: 0.117\n",
      "Iteration: 252 \t--- Loss: 0.117\n",
      "Iteration: 253 \t--- Loss: 0.118\n",
      "Iteration: 254 \t--- Loss: 0.117\n",
      "Iteration: 255 \t--- Loss: 0.116\n",
      "Iteration: 256 \t--- Loss: 0.115\n",
      "Iteration: 257 \t--- Loss: 0.119\n",
      "Iteration: 258 \t--- Loss: 0.115\n",
      "Iteration: 259 \t--- Loss: 0.117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:27<00:00, 87.74s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 1.343\n",
      "Iteration: 261 \t--- Loss: 1.316\n",
      "Iteration: 262 \t--- Loss: 1.289\n",
      "Iteration: 263 \t--- Loss: 1.336\n",
      "Iteration: 264 \t--- Loss: 1.341\n",
      "Iteration: 265 \t--- Loss: 1.261\n",
      "Iteration: 266 \t--- Loss: 1.303\n",
      "Iteration: 267 \t--- Loss: 1.245\n",
      "Iteration: 268 \t--- Loss: 1.353\n",
      "Iteration: 269 \t--- Loss: 1.268\n",
      "Iteration: 270 \t--- Loss: 1.354\n",
      "Iteration: 271 \t--- Loss: 1.254\n",
      "Iteration: 272 \t--- Loss: 1.310\n",
      "Iteration: 273 \t--- Loss: 1.340\n",
      "Iteration: 274 \t--- Loss: 1.281\n",
      "Iteration: 275 \t--- Loss: 1.339\n",
      "Iteration: 276 \t--- Loss: 1.327\n",
      "Iteration: 277 \t--- Loss: 1.283\n",
      "Iteration: 278 \t--- Loss: 1.309\n",
      "Iteration: 279 \t--- Loss: 1.311\n",
      "Iteration: 280 \t--- Loss: 1.276\n",
      "Iteration: 281 \t--- Loss: 1.309\n",
      "Iteration: 282 \t--- Loss: 1.321\n",
      "Iteration: 283 \t--- Loss: 1.366\n",
      "Iteration: 284 \t--- Loss: 1.319\n",
      "Iteration: 285 \t--- Loss: 1.250\n",
      "Iteration: 286 \t--- Loss: 1.281\n",
      "Iteration: 287 \t--- Loss: 1.264\n",
      "Iteration: 288 \t--- Loss: 1.260\n",
      "Iteration: 289 \t--- Loss: 1.335\n",
      "Iteration: 290 \t--- Loss: 1.294\n",
      "Iteration: 291 \t--- Loss: 1.294\n",
      "Iteration: 292 \t--- Loss: 1.321\n",
      "Iteration: 293 \t--- Loss: 1.311\n",
      "Iteration: 294 \t--- Loss: 1.282\n",
      "Iteration: 295 \t--- Loss: 1.310\n",
      "Iteration: 296 \t--- Loss: 1.307\n",
      "Iteration: 297 \t--- Loss: 1.342\n",
      "Iteration: 298 \t--- Loss: 1.337\n",
      "Iteration: 299 \t--- Loss: 1.257\n",
      "Iteration: 300 \t--- Loss: 1.312\n",
      "Iteration: 301 \t--- Loss: 1.292\n",
      "Iteration: 302 \t--- Loss: 1.363\n",
      "Iteration: 303 \t--- Loss: 1.276\n",
      "Iteration: 304 \t--- Loss: 1.307\n",
      "Iteration: 305 \t--- Loss: 1.308\n",
      "Iteration: 306 \t--- Loss: 1.343\n",
      "Iteration: 307 \t--- Loss: 1.275\n",
      "Iteration: 308 \t--- Loss: 1.293\n",
      "Iteration: 309 \t--- Loss: 1.358\n",
      "Iteration: 310 \t--- Loss: 1.319\n",
      "Iteration: 311 \t--- Loss: 1.282\n",
      "Iteration: 312 \t--- Loss: 1.261\n",
      "Iteration: 313 \t--- Loss: 1.250\n",
      "Iteration: 314 \t--- Loss: 1.296\n",
      "Iteration: 315 \t--- Loss: 1.302\n",
      "Iteration: 316 \t--- Loss: 1.281\n",
      "Iteration: 317 \t--- Loss: 1.291\n",
      "Iteration: 318 \t--- Loss: 1.263\n",
      "Iteration: 319 \t--- Loss: 1.261\n",
      "Iteration: 320 \t--- Loss: 1.327\n",
      "Iteration: 321 \t--- Loss: 1.275\n",
      "Iteration: 322 \t--- Loss: 1.377\n",
      "Iteration: 323 \t--- Loss: 1.271\n",
      "Iteration: 324 \t--- Loss: 1.314\n",
      "Iteration: 325 \t--- Loss: 1.299\n",
      "Iteration: 326 \t--- Loss: 1.320\n",
      "Iteration: 327 \t--- Loss: 1.355\n",
      "Iteration: 328 \t--- Loss: 1.320\n",
      "Iteration: 329 \t--- Loss: 1.318\n",
      "Iteration: 330 \t--- Loss: 1.348\n",
      "Iteration: 331 \t--- Loss: 1.311\n",
      "Iteration: 332 \t--- Loss: 1.287\n",
      "Iteration: 333 \t--- Loss: 1.328\n",
      "Iteration: 334 \t--- Loss: 1.235\n",
      "Iteration: 335 \t--- Loss: 1.376\n",
      "Iteration: 336 \t--- Loss: 1.348\n",
      "Iteration: 337 \t--- Loss: 1.309\n",
      "Iteration: 338 \t--- Loss: 1.261\n",
      "Iteration: 339 \t--- Loss: 1.301\n",
      "Iteration: 340 \t--- Loss: 1.292\n",
      "Iteration: 341 \t--- Loss: 1.293\n",
      "Iteration: 342 \t--- Loss: 1.280\n",
      "Iteration: 343 \t--- Loss: 1.306\n",
      "Iteration: 344 \t--- Loss: 1.356\n",
      "Iteration: 345 \t--- Loss: 1.317\n",
      "Iteration: 346 \t--- Loss: 1.301\n",
      "Iteration: 347 \t--- Loss: 1.309\n",
      "Iteration: 348 \t--- Loss: 1.337\n",
      "Iteration: 349 \t--- Loss: 1.269\n",
      "Iteration: 350 \t--- Loss: 1.289\n",
      "Iteration: 351 \t--- Loss: 1.262\n",
      "Iteration: 352 \t--- Loss: 1.307\n",
      "Iteration: 353 \t--- Loss: 1.273\n",
      "Iteration: 354 \t--- Loss: 1.262\n",
      "Iteration: 355 \t--- Loss: 1.258\n",
      "Iteration: 356 \t--- Loss: 1.291\n",
      "Iteration: 357 \t--- Loss: 1.311\n",
      "Iteration: 358 \t--- Loss: 1.305\n",
      "Iteration: 359 \t--- Loss: 1.250\n",
      "Iteration: 360 \t--- Loss: 1.311\n",
      "Iteration: 361 \t--- Loss: 1.331\n",
      "Iteration: 362 \t--- Loss: 1.283\n",
      "Iteration: 363 \t--- Loss: 1.279\n",
      "Iteration: 364 \t--- Loss: 1.294\n",
      "Iteration: 365 \t--- Loss: 1.346\n",
      "Iteration: 366 \t--- Loss: 1.289\n",
      "Iteration: 367 \t--- Loss: 1.262\n",
      "Iteration: 368 \t--- Loss: 1.327\n",
      "Iteration: 369 \t--- Loss: 1.286\n",
      "Iteration: 370 \t--- Loss: 1.295\n",
      "Iteration: 371 \t--- Loss: 1.261\n",
      "Iteration: 372 \t--- Loss: 1.328\n",
      "Iteration: 373 \t--- Loss: 1.248\n",
      "Iteration: 374 \t--- Loss: 1.293\n",
      "Iteration: 375 \t--- Loss: 1.243\n",
      "Iteration: 376 \t--- Loss: 1.317\n",
      "Iteration: 377 \t--- Loss: 1.325\n",
      "Iteration: 378 \t--- Loss: 1.321\n",
      "Iteration: 379 \t--- Loss: 1.309\n",
      "Iteration: 380 \t--- Loss: 1.279\n",
      "Iteration: 381 \t--- Loss: 1.369\n",
      "Iteration: 382 \t--- Loss: 1.277\n",
      "Iteration: 383 \t--- Loss: 1.331\n",
      "Iteration: 384 \t--- Loss: 1.356\n",
      "Iteration: 385 \t--- Loss: 1.312\n",
      "Iteration: 386 \t--- Loss: 1.314\n",
      "Iteration: 387 \t--- Loss: 1.373\n",
      "Iteration: 388 \t--- Loss: 1.260\n",
      "Iteration: 389 \t--- Loss: 1.292\n",
      "Iteration: 390 \t--- Loss: 1.311\n",
      "Iteration: 391 \t--- Loss: 1.289\n",
      "Iteration: 392 \t--- Loss: 1.264\n",
      "Iteration: 393 \t--- Loss: 1.307\n",
      "Iteration: 394 \t--- Loss: 1.308\n",
      "Iteration: 395 \t--- Loss: 1.291\n",
      "Iteration: 396 \t--- Loss: 1.313\n",
      "Iteration: 397 \t--- Loss: 1.283\n",
      "Iteration: 398 \t--- Loss: 1.275\n",
      "Iteration: 399 \t--- Loss: 1.347\n",
      "Iteration: 400 \t--- Loss: 1.302\n",
      "Iteration: 401 \t--- Loss: 1.285\n",
      "Iteration: 402 \t--- Loss: 1.330\n",
      "Iteration: 403 \t--- Loss: 1.256\n",
      "Iteration: 404 \t--- Loss: 1.273\n",
      "Iteration: 405 \t--- Loss: 1.270\n",
      "Iteration: 406 \t--- Loss: 1.279\n",
      "Iteration: 407 \t--- Loss: 1.321\n",
      "Iteration: 408 \t--- Loss: 1.298\n",
      "Iteration: 409 \t--- Loss: 1.289\n",
      "Iteration: 410 \t--- Loss: 1.271\n",
      "Iteration: 411 \t--- Loss: 1.315\n",
      "Iteration: 412 \t--- Loss: 1.292\n",
      "Iteration: 413 \t--- Loss: 1.269\n",
      "Iteration: 414 \t--- Loss: 1.277\n",
      "Iteration: 415 \t--- Loss: 1.252\n",
      "Iteration: 416 \t--- Loss: 1.266\n",
      "Iteration: 417 \t--- Loss: 1.278\n",
      "Iteration: 418 \t--- Loss: 1.301\n",
      "Iteration: 419 \t--- Loss: 1.343\n",
      "Iteration: 420 \t--- Loss: 1.281\n",
      "Iteration: 421 \t--- Loss: 1.294\n",
      "Iteration: 422 \t--- Loss: 1.336\n",
      "Iteration: 423 \t--- Loss: 1.260\n",
      "Iteration: 424 \t--- Loss: 1.278\n",
      "Iteration: 425 \t--- Loss: 1.326\n",
      "Iteration: 426 \t--- Loss: 1.314\n",
      "Iteration: 427 \t--- Loss: 1.237\n",
      "Iteration: 428 \t--- Loss: 1.305\n",
      "Iteration: 429 \t--- Loss: 1.330\n",
      "Iteration: 430 \t--- Loss: 1.356\n",
      "Iteration: 431 \t--- Loss: 1.294\n",
      "Iteration: 432 \t--- Loss: 1.296\n",
      "Iteration: 433 \t--- Loss: 1.279\n",
      "Iteration: 434 \t--- Loss: 1.281\n",
      "Iteration: 435 \t--- Loss: 1.289\n",
      "Iteration: 436 \t--- Loss: 1.253\n",
      "Iteration: 437 \t--- Loss: 1.251\n",
      "Iteration: 438 \t--- Loss: 1.281\n",
      "Iteration: 439 \t--- Loss: 1.235\n",
      "Iteration: 440 \t--- Loss: 1.305\n",
      "Iteration: 441 \t--- Loss: 1.279\n",
      "Iteration: 442 \t--- Loss: 1.300\n",
      "Iteration: 443 \t--- Loss: 1.275\n",
      "Iteration: 444 \t--- Loss: 1.309\n",
      "Iteration: 445 \t--- Loss: 1.296\n",
      "Iteration: 446 \t--- Loss: 1.239\n",
      "Iteration: 447 \t--- Loss: 1.351\n",
      "Iteration: 448 \t--- Loss: 1.301\n",
      "Iteration: 449 \t--- Loss: 1.261\n",
      "Iteration: 450 \t--- Loss: 1.280\n",
      "Iteration: 451 \t--- Loss: 1.292\n",
      "Iteration: 452 \t--- Loss: 1.273\n",
      "Iteration: 453 \t--- Loss: 1.253\n",
      "Iteration: 454 \t--- Loss: 1.319\n",
      "Iteration: 455 \t--- Loss: 1.300\n",
      "Iteration: 456 \t--- Loss: 1.364\n",
      "Iteration: 457 \t--- Loss: 1.340\n",
      "Iteration: 458 \t--- Loss: 1.317\n",
      "Iteration: 459 \t--- Loss: 1.312\n",
      "Iteration: 460 \t--- Loss: 1.277\n",
      "Iteration: 461 \t--- Loss: 1.335\n",
      "Iteration: 462 \t--- Loss: 1.330\n",
      "Iteration: 463 \t--- Loss: 1.316\n",
      "Iteration: 464 \t--- Loss: 1.230\n",
      "Iteration: 465 \t--- Loss: 1.268\n",
      "Iteration: 466 \t--- Loss: 1.303\n",
      "Iteration: 467 \t--- Loss: 1.240\n",
      "Iteration: 468 \t--- Loss: 1.260\n",
      "Iteration: 469 \t--- Loss: 1.303\n",
      "Iteration: 470 \t--- Loss: 1.306\n",
      "Iteration: 471 \t--- Loss: 1.319\n",
      "Iteration: 472 \t--- Loss: 1.344\n",
      "Iteration: 473 \t--- Loss: 1.295\n",
      "Iteration: 474 \t--- Loss: 1.360\n",
      "Iteration: 475 \t--- Loss: 1.296\n",
      "Iteration: 476 \t--- Loss: 1.319\n",
      "Iteration: 477 \t--- Loss: 1.330\n",
      "Iteration: 478 \t--- Loss: 1.287\n",
      "Iteration: 479 \t--- Loss: 1.296\n",
      "Iteration: 480 \t--- Loss: 1.318\n",
      "Iteration: 481 \t--- Loss: 1.260\n",
      "Iteration: 482 \t--- Loss: 1.303\n",
      "Iteration: 483 \t--- Loss: 1.293\n",
      "Iteration: 484 \t--- Loss: 1.292\n",
      "Iteration: 485 \t--- Loss: 1.305\n",
      "Iteration: 486 \t--- Loss: 1.313\n",
      "Iteration: 487 \t--- Loss: 1.318\n",
      "Iteration: 488 \t--- Loss: 1.320\n",
      "Iteration: 489 \t--- Loss: 1.258\n",
      "Iteration: 490 \t--- Loss: 1.267\n",
      "Iteration: 491 \t--- Loss: 1.306\n",
      "Iteration: 492 \t--- Loss: 1.288\n",
      "Iteration: 493 \t--- Loss: 1.360\n",
      "Iteration: 494 \t--- Loss: 1.261\n",
      "Iteration: 495 \t--- Loss: 1.305\n",
      "Iteration: 496 \t--- Loss: 1.305\n",
      "Iteration: 497 \t--- Loss: 1.317\n",
      "Iteration: 498 \t--- Loss: 1.253\n",
      "Iteration: 499 \t--- Loss: 1.262\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.77s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.117\n",
      "Iteration: 261 \t--- Loss: 0.119\n",
      "Iteration: 262 \t--- Loss: 0.117\n",
      "Iteration: 263 \t--- Loss: 0.117\n",
      "Iteration: 264 \t--- Loss: 0.117\n",
      "Iteration: 265 \t--- Loss: 0.118\n",
      "Iteration: 266 \t--- Loss: 0.116\n",
      "Iteration: 267 \t--- Loss: 0.117\n",
      "Iteration: 268 \t--- Loss: 0.118\n",
      "Iteration: 269 \t--- Loss: 0.118\n",
      "Iteration: 270 \t--- Loss: 0.117\n",
      "Iteration: 271 \t--- Loss: 0.116\n",
      "Iteration: 272 \t--- Loss: 0.116\n",
      "Iteration: 273 \t--- Loss: 0.115\n",
      "Iteration: 274 \t--- Loss: 0.115\n",
      "Iteration: 275 \t--- Loss: 0.119\n",
      "Iteration: 276 \t--- Loss: 0.114\n",
      "Iteration: 277 \t--- Loss: 0.118\n",
      "Iteration: 278 \t--- Loss: 0.118\n",
      "Iteration: 279 \t--- Loss: 0.116\n",
      "Iteration: 280 \t--- Loss: 0.119\n",
      "Iteration: 281 \t--- Loss: 0.118\n",
      "Iteration: 282 \t--- Loss: 0.118\n",
      "Iteration: 283 \t--- Loss: 0.116\n",
      "Iteration: 284 \t--- Loss: 0.118\n",
      "Iteration: 285 \t--- Loss: 0.117\n",
      "Iteration: 286 \t--- Loss: 0.118\n",
      "Iteration: 287 \t--- Loss: 0.117\n",
      "Iteration: 288 \t--- Loss: 0.118\n",
      "Iteration: 289 \t--- Loss: 0.115\n",
      "Iteration: 290 \t--- Loss: 0.116\n",
      "Iteration: 291 \t--- Loss: 0.119\n",
      "Iteration: 292 \t--- Loss: 0.118\n",
      "Iteration: 293 \t--- Loss: 0.119\n",
      "Iteration: 294 \t--- Loss: 0.118\n",
      "Iteration: 295 \t--- Loss: 0.118\n",
      "Iteration: 296 \t--- Loss: 0.116\n",
      "Iteration: 297 \t--- Loss: 0.118\n",
      "Iteration: 298 \t--- Loss: 0.118\n",
      "Iteration: 299 \t--- Loss: 0.118\n",
      "Iteration: 300 \t--- Loss: 0.115\n",
      "Iteration: 301 \t--- Loss: 0.114\n",
      "Iteration: 302 \t--- Loss: 0.116\n",
      "Iteration: 303 \t--- Loss: 0.119\n",
      "Iteration: 304 \t--- Loss: 0.116\n",
      "Iteration: 305 \t--- Loss: 0.117\n",
      "Iteration: 306 \t--- Loss: 0.117\n",
      "Iteration: 307 \t--- Loss: 0.117\n",
      "Iteration: 308 \t--- Loss: 0.115\n",
      "Iteration: 309 \t--- Loss: 0.117\n",
      "Iteration: 310 \t--- Loss: 0.117\n",
      "Iteration: 311 \t--- Loss: 0.118\n",
      "Iteration: 312 \t--- Loss: 0.116\n",
      "Iteration: 313 \t--- Loss: 0.113\n",
      "Iteration: 314 \t--- Loss: 0.121\n",
      "Iteration: 315 \t--- Loss: 0.120\n",
      "Iteration: 316 \t--- Loss: 0.118\n",
      "Iteration: 317 \t--- Loss: 0.119\n",
      "Iteration: 318 \t--- Loss: 0.121\n",
      "Iteration: 319 \t--- Loss: 0.118\n",
      "Iteration: 320 \t--- Loss: 0.117\n",
      "Iteration: 321 \t--- Loss: 0.117\n",
      "Iteration: 322 \t--- Loss: 0.119\n",
      "Iteration: 323 \t--- Loss: 0.118\n",
      "Iteration: 324 \t--- Loss: 0.118\n",
      "Iteration: 325 \t--- Loss: 0.116\n",
      "Iteration: 326 \t--- Loss: 0.115\n",
      "Iteration: 327 \t--- Loss: 0.117\n",
      "Iteration: 328 \t--- Loss: 0.119\n",
      "Iteration: 329 \t--- Loss: 0.116\n",
      "Iteration: 330 \t--- Loss: 0.118\n",
      "Iteration: 331 \t--- Loss: 0.117\n",
      "Iteration: 332 \t--- Loss: 0.114\n",
      "Iteration: 333 \t--- Loss: 0.114\n",
      "Iteration: 334 \t--- Loss: 0.117\n",
      "Iteration: 335 \t--- Loss: 0.119\n",
      "Iteration: 336 \t--- Loss: 0.117\n",
      "Iteration: 337 \t--- Loss: 0.117\n",
      "Iteration: 338 \t--- Loss: 0.120\n",
      "Iteration: 339 \t--- Loss: 0.114\n",
      "Iteration: 340 \t--- Loss: 0.116\n",
      "Iteration: 341 \t--- Loss: 0.118\n",
      "Iteration: 342 \t--- Loss: 0.117\n",
      "Iteration: 343 \t--- Loss: 0.117\n",
      "Iteration: 344 \t--- Loss: 0.118\n",
      "Iteration: 345 \t--- Loss: 0.117\n",
      "Iteration: 346 \t--- Loss: 0.119\n",
      "Iteration: 347 \t--- Loss: 0.117\n",
      "Iteration: 348 \t--- Loss: 0.119\n",
      "Iteration: 349 \t--- Loss: 0.118\n",
      "Iteration: 350 \t--- Loss: 0.118\n",
      "Iteration: 351 \t--- Loss: 0.116\n",
      "Iteration: 352 \t--- Loss: 0.120\n",
      "Iteration: 353 \t--- Loss: 0.118\n",
      "Iteration: 354 \t--- Loss: 0.117\n",
      "Iteration: 355 \t--- Loss: 0.118\n",
      "Iteration: 356 \t--- Loss: 0.117\n",
      "Iteration: 357 \t--- Loss: 0.117\n",
      "Iteration: 358 \t--- Loss: 0.117\n",
      "Iteration: 359 \t--- Loss: 0.117\n",
      "Iteration: 360 \t--- Loss: 0.115\n",
      "Iteration: 361 \t--- Loss: 0.119\n",
      "Iteration: 362 \t--- Loss: 0.119\n",
      "Iteration: 363 \t--- Loss: 0.118\n",
      "Iteration: 364 \t--- Loss: 0.116\n",
      "Iteration: 365 \t--- Loss: 0.114\n",
      "Iteration: 366 \t--- Loss: 0.114\n",
      "Iteration: 367 \t--- Loss: 0.117\n",
      "Iteration: 368 \t--- Loss: 0.114\n",
      "Iteration: 369 \t--- Loss: 0.119\n",
      "Iteration: 370 \t--- Loss: 0.117\n",
      "Iteration: 371 \t--- Loss: 0.116\n",
      "Iteration: 372 \t--- Loss: 0.114\n",
      "Iteration: 373 \t--- Loss: 0.117\n",
      "Iteration: 374 \t--- Loss: 0.118\n",
      "Iteration: 375 \t--- Loss: 0.119\n",
      "Iteration: 376 \t--- Loss: 0.116\n",
      "Iteration: 377 \t--- Loss: 0.115\n",
      "Iteration: 378 \t--- Loss: 0.116\n",
      "Iteration: 379 \t--- Loss: 0.117\n",
      "Iteration: 380 \t--- Loss: 0.117\n",
      "Iteration: 381 \t--- Loss: 0.118\n",
      "Iteration: 382 \t--- Loss: 0.116\n",
      "Iteration: 383 \t--- Loss: 0.118\n",
      "Iteration: 384 \t--- Loss: 0.112\n",
      "Iteration: 385 \t--- Loss: 0.116\n",
      "Iteration: 386 \t--- Loss: 0.116\n",
      "Iteration: 387 \t--- Loss: 0.117\n",
      "Iteration: 388 \t--- Loss: 0.119\n",
      "Iteration: 389 \t--- Loss: 0.117\n",
      "Iteration: 390 \t--- Loss: 0.112\n",
      "Iteration: 391 \t--- Loss: 0.116\n",
      "Iteration: 392 \t--- Loss: 0.115\n",
      "Iteration: 393 \t--- Loss: 0.120\n",
      "Iteration: 394 \t--- Loss: 0.115\n",
      "Iteration: 395 \t--- Loss: 0.121\n",
      "Iteration: 396 \t--- Loss: 0.118\n",
      "Iteration: 397 \t--- Loss: 0.113\n",
      "Iteration: 398 \t--- Loss: 0.115\n",
      "Iteration: 399 \t--- Loss: 0.114\n",
      "Iteration: 400 \t--- Loss: 0.116\n",
      "Iteration: 401 \t--- Loss: 0.119\n",
      "Iteration: 402 \t--- Loss: 0.118\n",
      "Iteration: 403 \t--- Loss: 0.120\n",
      "Iteration: 404 \t--- Loss: 0.117\n",
      "Iteration: 405 \t--- Loss: 0.115\n",
      "Iteration: 406 \t--- Loss: 0.116\n",
      "Iteration: 407 \t--- Loss: 0.115\n",
      "Iteration: 408 \t--- Loss: 0.117\n",
      "Iteration: 409 \t--- Loss: 0.114\n",
      "Iteration: 410 \t--- Loss: 0.116\n",
      "Iteration: 411 \t--- Loss: 0.115\n",
      "Iteration: 412 \t--- Loss: 0.117\n",
      "Iteration: 413 \t--- Loss: 0.119\n",
      "Iteration: 414 \t--- Loss: 0.116\n",
      "Iteration: 415 \t--- Loss: 0.116\n",
      "Iteration: 416 \t--- Loss: 0.116\n",
      "Iteration: 417 \t--- Loss: 0.120\n",
      "Iteration: 418 \t--- Loss: 0.120\n",
      "Iteration: 419 \t--- Loss: 0.118\n",
      "Iteration: 420 \t--- Loss: 0.118\n",
      "Iteration: 421 \t--- Loss: 0.116\n",
      "Iteration: 422 \t--- Loss: 0.114\n",
      "Iteration: 423 \t--- Loss: 0.117\n",
      "Iteration: 424 \t--- Loss: 0.115\n",
      "Iteration: 425 \t--- Loss: 0.119\n",
      "Iteration: 426 \t--- Loss: 0.118\n",
      "Iteration: 427 \t--- Loss: 0.120\n",
      "Iteration: 428 \t--- Loss: 0.119\n",
      "Iteration: 429 \t--- Loss: 0.118\n",
      "Iteration: 430 \t--- Loss: 0.115\n",
      "Iteration: 431 \t--- Loss: 0.115\n",
      "Iteration: 432 \t--- Loss: 0.116\n",
      "Iteration: 433 \t--- Loss: 0.115\n",
      "Iteration: 434 \t--- Loss: 0.113\n",
      "Iteration: 435 \t--- Loss: 0.118\n",
      "Iteration: 436 \t--- Loss: 0.117\n",
      "Iteration: 437 \t--- Loss: 0.117\n",
      "Iteration: 438 \t--- Loss: 0.117\n",
      "Iteration: 439 \t--- Loss: 0.115\n",
      "Iteration: 440 \t--- Loss: 0.117\n",
      "Iteration: 441 \t--- Loss: 0.116\n",
      "Iteration: 442 \t--- Loss: 0.118\n",
      "Iteration: 443 \t--- Loss: 0.119\n",
      "Iteration: 444 \t--- Loss: 0.117\n",
      "Iteration: 445 \t--- Loss: 0.112\n",
      "Iteration: 446 \t--- Loss: 0.112\n",
      "Iteration: 447 \t--- Loss: 0.119\n",
      "Iteration: 448 \t--- Loss: 0.117\n",
      "Iteration: 449 \t--- Loss: 0.120\n",
      "Iteration: 450 \t--- Loss: 0.112\n",
      "Iteration: 451 \t--- Loss: 0.121\n",
      "Iteration: 452 \t--- Loss: 0.118\n",
      "Iteration: 453 \t--- Loss: 0.119\n",
      "Iteration: 454 \t--- Loss: 0.118\n",
      "Iteration: 455 \t--- Loss: 0.116\n",
      "Iteration: 456 \t--- Loss: 0.116\n",
      "Iteration: 457 \t--- Loss: 0.119\n",
      "Iteration: 458 \t--- Loss: 0.116\n",
      "Iteration: 459 \t--- Loss: 0.116\n",
      "Iteration: 460 \t--- Loss: 0.117\n",
      "Iteration: 461 \t--- Loss: 0.113\n",
      "Iteration: 462 \t--- Loss: 0.117\n",
      "Iteration: 463 \t--- Loss: 0.117\n",
      "Iteration: 464 \t--- Loss: 0.116\n",
      "Iteration: 465 \t--- Loss: 0.120\n",
      "Iteration: 466 \t--- Loss: 0.117\n",
      "Iteration: 467 \t--- Loss: 0.113\n",
      "Iteration: 468 \t--- Loss: 0.118\n",
      "Iteration: 469 \t--- Loss: 0.119\n",
      "Iteration: 470 \t--- Loss: 0.120\n",
      "Iteration: 471 \t--- Loss: 0.116\n",
      "Iteration: 472 \t--- Loss: 0.114\n",
      "Iteration: 473 \t--- Loss: 0.117\n",
      "Iteration: 474 \t--- Loss: 0.118\n",
      "Iteration: 475 \t--- Loss: 0.117\n",
      "Iteration: 476 \t--- Loss: 0.116\n",
      "Iteration: 477 \t--- Loss: 0.116\n",
      "Iteration: 478 \t--- Loss: 0.118\n",
      "Iteration: 479 \t--- Loss: 0.116\n",
      "Iteration: 480 \t--- Loss: 0.115\n",
      "Iteration: 481 \t--- Loss: 0.119\n",
      "Iteration: 482 \t--- Loss: 0.120\n",
      "Iteration: 483 \t--- Loss: 0.117\n",
      "Iteration: 484 \t--- Loss: 0.117\n",
      "Iteration: 485 \t--- Loss: 0.116\n",
      "Iteration: 486 \t--- Loss: 0.116\n",
      "Iteration: 487 \t--- Loss: 0.117\n",
      "Iteration: 488 \t--- Loss: 0.118\n",
      "Iteration: 489 \t--- Loss: 0.119\n",
      "Iteration: 490 \t--- Loss: 0.116\n",
      "Iteration: 491 \t--- Loss: 0.117\n",
      "Iteration: 492 \t--- Loss: 0.121\n",
      "Iteration: 493 \t--- Loss: 0.114\n",
      "Iteration: 494 \t--- Loss: 0.115\n",
      "Iteration: 495 \t--- Loss: 0.114\n",
      "Iteration: 496 \t--- Loss: 0.117\n",
      "Iteration: 497 \t--- Loss: 0.118\n",
      "Iteration: 498 \t--- Loss: 0.115\n",
      "Iteration: 499 \t--- Loss: 0.118\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.50s/it][Parallel(n_jobs=5)]: Done  57 tasks      | elapsed: 34.7min\n",
      "\n",
      " 30%|███       | 3/10 [00:04<00:09,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:05<00:09,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.438\n",
      "Iteration: 1 \t--- Loss: 0.407\n",
      "Iteration: 2 \t--- Loss: 0.391\n",
      "Iteration: 3 \t--- Loss: 0.408\n",
      "Iteration: 4 \t--- Loss: 0.387\n",
      "Iteration: 5 \t--- Loss: 0.369\n",
      "Iteration: 6 \t--- Loss: 0.396\n",
      "Iteration: 7 \t--- Loss: 0.334\n",
      "Iteration: 8 \t--- Loss: 0.355\n",
      "Iteration: 9 \t--- Loss: 0.339\n",
      "Iteration: 10 \t--- Loss: 0.327\n",
      "Iteration: 11 \t--- Loss: 0.326\n",
      "Iteration: 12 \t--- Loss: 0.325\n",
      "Iteration: 13 \t--- Loss: 0.305\n",
      "Iteration: 14 \t--- Loss: 0.311\n",
      "Iteration: 15 \t--- Loss: 0.309\n",
      "Iteration: 16 \t--- Loss: 0.316\n",
      "Iteration: 17 \t--- Loss: 0.285\n",
      "Iteration: 18 \t--- Loss: 0.303\n",
      "Iteration: 19 \t--- Loss: 0.293\n",
      "Iteration: 20 \t--- Loss: 0.255\n",
      "Iteration: 21 \t--- Loss: 0.290\n",
      "Iteration: 22 \t--- Loss: 0.287\n",
      "Iteration: 23 \t--- Loss: 0.256\n",
      "Iteration: 24 \t--- Loss: 0.248\n",
      "Iteration: 25 \t--- Loss: 0.230\n",
      "Iteration: 26 \t--- Loss: 0.200\n",
      "Iteration: 27 \t--- Loss: 0.206\n",
      "Iteration: 28 \t--- Loss: 0.224\n",
      "Iteration: 29 \t--- Loss: 0.205\n",
      "Iteration: 30 \t--- Loss: 0.218\n",
      "Iteration: 31 \t--- Loss: 0.190\n",
      "Iteration: 32 \t--- Loss: 0.263\n",
      "Iteration: 33 \t--- Loss: 0.283\n",
      "Iteration: 34 \t--- Loss: 0.288\n",
      "Iteration: 35 \t--- Loss: 0.263\n",
      "Iteration: 36 \t--- Loss: 0.220\n",
      "Iteration: 37 \t--- Loss: 0.269\n",
      "Iteration: 38 \t--- Loss: 0.293\n",
      "Iteration: 39 \t--- Loss: 0.266\n",
      "Iteration: 40 \t--- Loss: 0.277\n",
      "Iteration: 41 \t--- Loss: 0.237\n",
      "Iteration: 42 \t--- Loss: 0.234\n",
      "Iteration: 43 \t--- Loss: 0.209\n",
      "Iteration: 44 \t--- Loss: 0.216\n",
      "Iteration: 45 \t--- Loss: 0.245\n",
      "Iteration: 46 \t--- Loss: 0.189\n",
      "Iteration: 47 \t--- Loss: 0.185\n",
      "Iteration: 48 \t--- Loss: 0.234\n",
      "Iteration: 49 \t--- Loss: 0.157\n",
      "Iteration: 50 \t--- Loss: 0.270\n",
      "Iteration: 51 \t--- Loss: 0.360\n",
      "Iteration: 52 \t--- Loss: 0.380\n",
      "Iteration: 53 \t--- Loss: 0.314\n",
      "Iteration: 54 \t--- Loss: 0.320\n",
      "Iteration: 55 \t--- Loss: 0.304\n",
      "Iteration: 56 \t--- Loss: 0.327\n",
      "Iteration: 57 \t--- Loss: 0.306\n",
      "Iteration: 58 \t--- Loss: 0.296\n",
      "Iteration: 59 \t--- Loss: 0.261\n",
      "Iteration: 60 \t--- Loss: 0.289\n",
      "Iteration: 61 \t--- Loss: 0.276\n",
      "Iteration: 62 \t--- Loss: 0.263\n",
      "Iteration: 63 \t--- Loss: 0.270\n",
      "Iteration: 64 \t--- Loss: 0.242\n",
      "Iteration: 65 \t--- Loss: 0.226\n",
      "Iteration: 66 \t--- Loss: 0.240\n",
      "Iteration: 67 \t--- Loss: 0.221\n",
      "Iteration: 68 \t--- Loss: 0.220\n",
      "Iteration: 69 \t--- Loss: 0.234\n",
      "Iteration: 70 \t--- Loss: 0.210\n",
      "Iteration: 71 \t--- Loss: 0.212\n",
      "Iteration: 72 \t--- Loss: 0.181\n",
      "Iteration: 73 \t--- Loss: 0.208\n",
      "Iteration: 74 \t--- Loss: 0.185\n",
      "Iteration: 75 \t--- Loss: 0.173\n",
      "Iteration: 76 \t--- Loss: 0.150\n",
      "Iteration: 77 \t--- Loss: 0.142\n",
      "Iteration: 78 \t--- Loss: 0.137\n",
      "Iteration: 79 \t--- Loss: 0.144\n",
      "Iteration: 80 \t--- Loss: 0.137\n",
      "Iteration: 81 \t--- Loss: 0.112\n",
      "Iteration: 82 \t--- Loss: 0.108\n",
      "Iteration: 83 \t--- Loss: 0.112\n",
      "Iteration: 84 \t--- Loss: 0.122\n",
      "Iteration: 85 \t--- Loss: 0.126\n",
      "Iteration: 86 \t--- Loss: 0.128\n",
      "Iteration: 87 \t--- Loss: 0.121\n",
      "Iteration: 88 \t--- Loss: 0.118\n",
      "Iteration: 89 \t--- Loss: 0.119\n",
      "Iteration: 90 \t--- Loss: 0.124\n",
      "Iteration: 91 \t--- Loss: 0.116\n",
      "Iteration: 92 \t--- Loss: 0.117\n",
      "Iteration: 93 \t--- Loss: 0.117\n",
      "Iteration: 94 \t--- Loss: 0.115\n",
      "Iteration: 95 \t--- Loss: 0.103\n",
      "Iteration: 96 \t--- Loss: 0.112\n",
      "Iteration: 97 \t--- Loss: 0.110\n",
      "Iteration: 98 \t--- Loss: 0.120\n",
      "Iteration: 99 \t--- Loss: 0.111\n",
      "Iteration: 100 \t--- Loss: 0.124\n",
      "Iteration: 101 \t--- Loss: 0.110\n",
      "Iteration: 102 \t--- Loss: 0.122\n",
      "Iteration: 103 \t--- Loss: 0.112\n",
      "Iteration: 104 \t--- Loss: 0.149\n",
      "Iteration: 105 \t--- Loss: 0.173\n",
      "Iteration: 106 \t--- Loss: 0.106\n",
      "Iteration: 107 \t--- Loss: 0.126\n",
      "Iteration: 108 \t--- Loss: 0.133\n",
      "Iteration: 109 \t--- Loss: 0.196\n",
      "Iteration: 110 \t--- Loss: 0.219\n",
      "Iteration: 111 \t--- Loss: 0.206\n",
      "Iteration: 112 \t--- Loss: 0.216\n",
      "Iteration: 113 \t--- Loss: 0.182\n",
      "Iteration: 114 \t--- Loss: 0.170\n",
      "Iteration: 115 \t--- Loss: 0.167\n",
      "Iteration: 116 \t--- Loss: 0.155\n",
      "Iteration: 117 \t--- Loss: 0.160\n",
      "Iteration: 118 \t--- Loss: 0.131\n",
      "Iteration: 119 \t--- Loss: 0.101\n",
      "Iteration: 120 \t--- Loss: 0.092\n",
      "Iteration: 121 \t--- Loss: 0.082\n",
      "Iteration: 122 \t--- Loss: 0.088\n",
      "Iteration: 123 \t--- Loss: 0.081\n",
      "Iteration: 124 \t--- Loss: 0.080\n",
      "Iteration: 125 \t--- Loss: 0.082\n",
      "Iteration: 126 \t--- Loss: 0.080\n",
      "Iteration: 127 \t--- Loss: 0.092\n",
      "Iteration: 128 \t--- Loss: 0.085\n",
      "Iteration: 129 \t--- Loss: 0.074\n",
      "Iteration: 130 \t--- Loss: 0.075\n",
      "Iteration: 131 \t--- Loss: 0.084\n",
      "Iteration: 132 \t--- Loss: 0.077\n",
      "Iteration: 133 \t--- Loss: 0.080\n",
      "Iteration: 134 \t--- Loss: 0.089\n",
      "Iteration: 135 \t--- Loss: 0.080\n",
      "Iteration: 136 \t--- Loss: 0.079\n",
      "Iteration: 137 \t--- Loss: 0.097\n",
      "Iteration: 138 \t--- Loss: 0.104\n",
      "Iteration: 139 \t--- Loss: 0.097\n",
      "Iteration: 140 \t--- Loss: 0.117\n",
      "Iteration: 141 \t--- Loss: 0.094\n",
      "Iteration: 142 \t--- Loss: 0.106\n",
      "Iteration: 143 \t--- Loss: 0.089\n",
      "Iteration: 144 \t--- Loss: 0.090\n",
      "Iteration: 145 \t--- Loss: 0.103\n",
      "Iteration: 146 \t--- Loss: 0.126\n",
      "Iteration: 147 \t--- Loss: 0.081\n",
      "Iteration: 148 \t--- Loss: 0.064\n",
      "Iteration: 149 \t--- Loss: 0.067\n",
      "Iteration: 150 \t--- Loss: 0.070\n",
      "Iteration: 151 \t--- Loss: 0.067\n",
      "Iteration: 152 \t--- Loss: 0.097\n",
      "Iteration: 153 \t--- Loss: 0.125\n",
      "Iteration: 154 \t--- Loss: 0.079\n",
      "Iteration: 155 \t--- Loss: 0.069\n",
      "Iteration: 156 \t--- Loss: 0.076\n",
      "Iteration: 157 \t--- Loss: 0.094\n",
      "Iteration: 158 \t--- Loss: 0.117\n",
      "Iteration: 159 \t--- Loss: 0.069\n",
      "Iteration: 160 \t--- Loss: 0.059\n",
      "Iteration: 161 \t--- Loss: 0.084\n",
      "Iteration: 162 \t--- Loss: 0.098\n",
      "Iteration: 163 \t--- Loss: 0.085\n",
      "Iteration: 164 \t--- Loss: 0.116\n",
      "Iteration: 165 \t--- Loss: 0.081\n",
      "Iteration: 166 \t--- Loss: 0.075\n",
      "Iteration: 167 \t--- Loss: 0.081\n",
      "Iteration: 168 \t--- Loss: 0.078\n",
      "Iteration: 169 \t--- Loss: 0.091\n",
      "Iteration: 170 \t--- Loss: 0.064\n",
      "Iteration: 171 \t--- Loss: 0.066\n",
      "Iteration: 172 \t--- Loss: 0.053\n",
      "Iteration: 173 \t--- Loss: 0.061\n",
      "Iteration: 174 \t--- Loss: 0.063\n",
      "Iteration: 175 \t--- Loss: 0.060\n",
      "Iteration: 176 \t--- Loss: 0.070\n",
      "Iteration: 177 \t--- Loss: 0.097\n",
      "Iteration: 178 \t--- Loss: 0.052\n",
      "Iteration: 179 \t--- Loss: 0.070\n",
      "Iteration: 180 \t--- Loss: 0.086\n",
      "Iteration: 181 \t--- Loss: 0.059\n",
      "Iteration: 182 \t--- Loss: 0.061\n",
      "Iteration: 183 \t--- Loss: 0.061\n",
      "Iteration: 184 \t--- Loss: 0.055\n",
      "Iteration: 185 \t--- Loss: 0.074\n",
      "Iteration: 186 \t--- Loss: 0.088\n",
      "Iteration: 187 \t--- Loss: 0.053\n",
      "Iteration: 188 \t--- Loss: 0.060\n",
      "Iteration: 189 \t--- Loss: 0.054\n",
      "Iteration: 190 \t--- Loss: 0.064\n",
      "Iteration: 191 \t--- Loss: 0.066\n",
      "Iteration: 192 \t--- Loss: 0.082\n",
      "Iteration: 193 \t--- Loss: 0.116\n",
      "Iteration: 194 \t--- Loss: 0.095\n",
      "Iteration: 195 \t--- Loss: 0.068\n",
      "Iteration: 196 \t--- Loss: 0.048\n",
      "Iteration: 197 \t--- Loss: 0.049\n",
      "Iteration: 198 \t--- Loss: 0.049\n",
      "Iteration: 199 \t--- Loss: 0.048\n",
      "Iteration: 200 \t--- Loss: 0.043\n",
      "Iteration: 201 \t--- Loss: 0.047\n",
      "Iteration: 202 \t--- Loss: 0.047\n",
      "Iteration: 203 \t--- Loss: 0.048\n",
      "Iteration: 204 \t--- Loss: 0.049\n",
      "Iteration: 205 \t--- Loss: 0.052\n",
      "Iteration: 206 \t--- Loss: 0.052\n",
      "Iteration: 207 \t--- Loss: 0.052\n",
      "Iteration: 208 \t--- Loss: 0.058\n",
      "Iteration: 209 \t--- Loss: 0.083\n",
      "Iteration: 210 \t--- Loss: 0.105\n",
      "Iteration: 211 \t--- Loss: 0.087\n",
      "Iteration: 212 \t--- Loss: 0.056\n",
      "Iteration: 213 \t--- Loss: 0.042\n",
      "Iteration: 214 \t--- Loss: 0.044\n",
      "Iteration: 215 \t--- Loss: 0.042\n",
      "Iteration: 216 \t--- Loss: 0.043\n",
      "Iteration: 217 \t--- Loss: 0.048\n",
      "Iteration: 218 \t--- Loss: 0.047\n",
      "Iteration: 219 \t--- Loss: 0.044\n",
      "Iteration: 220 \t--- Loss: 0.049\n",
      "Iteration: 221 \t--- Loss: 0.046\n",
      "Iteration: 222 \t--- Loss: 0.051\n",
      "Iteration: 223 \t--- Loss: 0.076\n",
      "Iteration: 224 \t--- Loss: 0.103\n",
      "Iteration: 225 \t--- Loss: 0.075\n",
      "Iteration: 226 \t--- Loss: 0.048\n",
      "Iteration: 227 \t--- Loss: 0.050\n",
      "Iteration: 228 \t--- Loss: 0.047\n",
      "Iteration: 229 \t--- Loss: 0.048\n",
      "Iteration: 230 \t--- Loss: 0.044\n",
      "Iteration: 231 \t--- Loss: 0.051\n",
      "Iteration: 232 \t--- Loss: 0.063\n",
      "Iteration: 233 \t--- Loss: 0.058\n",
      "Iteration: 234 \t--- Loss: 0.069\n",
      "Iteration: 235 \t--- Loss: 0.043\n",
      "Iteration: 236 \t--- Loss: 0.046\n",
      "Iteration: 237 \t--- Loss: 0.048\n",
      "Iteration: 238 \t--- Loss: 0.048\n",
      "Iteration: 239 \t--- Loss: 0.064\n",
      "Iteration: 240 \t--- Loss: 0.091\n",
      "Iteration: 241 \t--- Loss: 0.049\n",
      "Iteration: 242 \t--- Loss: 0.054\n",
      "Iteration: 243 \t--- Loss: 0.067\n",
      "Iteration: 244 \t--- Loss: 0.045\n",
      "Iteration: 245 \t--- Loss: 0.046\n",
      "Iteration: 246 \t--- Loss: 0.050\n",
      "Iteration: 247 \t--- Loss: 0.054\n",
      "Iteration: 248 \t--- Loss: 0.044\n",
      "Iteration: 249 \t--- Loss: 0.041\n",
      "Iteration: 250 \t--- Loss: 0.046\n",
      "Iteration: 251 \t--- Loss: 0.045\n",
      "Iteration: 252 \t--- Loss: 0.048\n",
      "Iteration: 253 \t--- Loss: 0.046\n",
      "Iteration: 254 \t--- Loss: 0.055\n",
      "Iteration: 255 \t--- Loss: 0.066\n",
      "Iteration: 256 \t--- Loss: 0.065\n",
      "Iteration: 257 \t--- Loss: 0.090\n",
      "Iteration: 258 \t--- Loss: 0.058\n",
      "Iteration: 259 \t--- Loss: 0.041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.49s/it][Parallel(n_jobs=5)]: Done  58 tasks      | elapsed: 34.9min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.049\n",
      "Iteration: 1 \t--- Loss: 0.050\n",
      "Iteration: 2 \t--- Loss: 0.050\n",
      "Iteration: 3 \t--- Loss: 0.047\n",
      "Iteration: 4 \t--- Loss: 0.045\n",
      "Iteration: 5 \t--- Loss: 0.045\n",
      "Iteration: 6 \t--- Loss: 0.042\n",
      "Iteration: 7 \t--- Loss: 0.043\n",
      "Iteration: 8 \t--- Loss: 0.039\n",
      "Iteration: 9 \t--- Loss: 0.040\n",
      "Iteration: 10 \t--- Loss: 0.041\n",
      "Iteration: 11 \t--- Loss: 0.037\n",
      "Iteration: 12 \t--- Loss: 0.038\n",
      "Iteration: 13 \t--- Loss: 0.036\n",
      "Iteration: 14 \t--- Loss: 0.037\n",
      "Iteration: 15 \t--- Loss: 0.035\n",
      "Iteration: 16 \t--- Loss: 0.039\n",
      "Iteration: 17 \t--- Loss: 0.034\n",
      "Iteration: 18 \t--- Loss: 0.035\n",
      "Iteration: 19 \t--- Loss: 0.035\n",
      "Iteration: 20 \t--- Loss: 0.034\n",
      "Iteration: 21 \t--- Loss: 0.033\n",
      "Iteration: 22 \t--- Loss: 0.033\n",
      "Iteration: 23 \t--- Loss: 0.033\n",
      "Iteration: 24 \t--- Loss: 0.033\n",
      "Iteration: 25 \t--- Loss: 0.032\n",
      "Iteration: 26 \t--- Loss: 0.033\n",
      "Iteration: 27 \t--- Loss: 0.034\n",
      "Iteration: 28 \t--- Loss: 0.032\n",
      "Iteration: 29 \t--- Loss: 0.033\n",
      "Iteration: 30 \t--- Loss: 0.031\n",
      "Iteration: 31 \t--- Loss: 0.033\n",
      "Iteration: 32 \t--- Loss: 0.031\n",
      "Iteration: 33 \t--- Loss: 0.032\n",
      "Iteration: 34 \t--- Loss: 0.032\n",
      "Iteration: 35 \t--- Loss: 0.031\n",
      "Iteration: 36 \t--- Loss: 0.032\n",
      "Iteration: 37 \t--- Loss: 0.030\n",
      "Iteration: 38 \t--- Loss: 0.031\n",
      "Iteration: 39 \t--- Loss: 0.032\n",
      "Iteration: 40 \t--- Loss: 0.031\n",
      "Iteration: 41 \t--- Loss: 0.031\n",
      "Iteration: 42 \t--- Loss: 0.031\n",
      "Iteration: 43 \t--- Loss: 0.031\n",
      "Iteration: 44 \t--- Loss: 0.031\n",
      "Iteration: 45 \t--- Loss: 0.030\n",
      "Iteration: 46 \t--- Loss: 0.030\n",
      "Iteration: 47 \t--- Loss: 0.030\n",
      "Iteration: 48 \t--- Loss: 0.030\n",
      "Iteration: 49 \t--- Loss: 0.031\n",
      "Iteration: 50 \t--- Loss: 0.032\n",
      "Iteration: 51 \t--- Loss: 0.031\n",
      "Iteration: 52 \t--- Loss: 0.031\n",
      "Iteration: 53 \t--- Loss: 0.031\n",
      "Iteration: 54 \t--- Loss: 0.031\n",
      "Iteration: 55 \t--- Loss: 0.031\n",
      "Iteration: 56 \t--- Loss: 0.031\n",
      "Iteration: 57 \t--- Loss: 0.030\n",
      "Iteration: 58 \t--- Loss: 0.032\n",
      "Iteration: 59 \t--- Loss: 0.030\n",
      "Iteration: 60 \t--- Loss: 0.029\n",
      "Iteration: 61 \t--- Loss: 0.030\n",
      "Iteration: 62 \t--- Loss: 0.031\n",
      "Iteration: 63 \t--- Loss: 0.031\n",
      "Iteration: 64 \t--- Loss: 0.031\n",
      "Iteration: 65 \t--- Loss: 0.030\n",
      "Iteration: 66 \t--- Loss: 0.029\n",
      "Iteration: 67 \t--- Loss: 0.030\n",
      "Iteration: 68 \t--- Loss: 0.031\n",
      "Iteration: 69 \t--- Loss: 0.029\n",
      "Iteration: 70 \t--- Loss: 0.031\n",
      "Iteration: 71 \t--- Loss: 0.031\n",
      "Iteration: 72 \t--- Loss: 0.031\n",
      "Iteration: 73 \t--- Loss: 0.030\n",
      "Iteration: 74 \t--- Loss: 0.030\n",
      "Iteration: 75 \t--- Loss: 0.029\n",
      "Iteration: 76 \t--- Loss: 0.030\n",
      "Iteration: 77 \t--- Loss: 0.030\n",
      "Iteration: 78 \t--- Loss: 0.029\n",
      "Iteration: 79 \t--- Loss: 0.031\n",
      "Iteration: 80 \t--- Loss: 0.030\n",
      "Iteration: 81 \t--- Loss: 0.031\n",
      "Iteration: 82 \t--- Loss: 0.028\n",
      "Iteration: 83 \t--- Loss: 0.031\n",
      "Iteration: 84 \t--- Loss: 0.030\n",
      "Iteration: 85 \t--- Loss: 0.030\n",
      "Iteration: 86 \t--- Loss: 0.031\n",
      "Iteration: 87 \t--- Loss: 0.030\n",
      "Iteration: 88 \t--- Loss: 0.029\n",
      "Iteration: 89 \t--- Loss: 0.029\n",
      "Iteration: 90 \t--- Loss: 0.031\n",
      "Iteration: 91 \t--- Loss: 0.028\n",
      "Iteration: 92 \t--- Loss: 0.030\n",
      "Iteration: 93 \t--- Loss: 0.030\n",
      "Iteration: 94 \t--- Loss: 0.030\n",
      "Iteration: 95 \t--- Loss: 0.030\n",
      "Iteration: 96 \t--- Loss: 0.029\n",
      "Iteration: 97 \t--- Loss: 0.030\n",
      "Iteration: 98 \t--- Loss: 0.029\n",
      "Iteration: 99 \t--- Loss: 0.031\n",
      "Iteration: 100 \t--- Loss: 0.031\n",
      "Iteration: 101 \t--- Loss: 0.031\n",
      "Iteration: 102 \t--- Loss: 0.029\n",
      "Iteration: 103 \t--- Loss: 0.030\n",
      "Iteration: 104 \t--- Loss: 0.030\n",
      "Iteration: 105 \t--- Loss: 0.031\n",
      "Iteration: 106 \t--- Loss: 0.030\n",
      "Iteration: 107 \t--- Loss: 0.030\n",
      "Iteration: 108 \t--- Loss: 0.029\n",
      "Iteration: 109 \t--- Loss: 0.031\n",
      "Iteration: 110 \t--- Loss: 0.030\n",
      "Iteration: 111 \t--- Loss: 0.030\n",
      "Iteration: 112 \t--- Loss: 0.031\n",
      "Iteration: 113 \t--- Loss: 0.030\n",
      "Iteration: 114 \t--- Loss: 0.030\n",
      "Iteration: 115 \t--- Loss: 0.029\n",
      "Iteration: 116 \t--- Loss: 0.031\n",
      "Iteration: 117 \t--- Loss: 0.029\n",
      "Iteration: 118 \t--- Loss: 0.029\n",
      "Iteration: 119 \t--- Loss: 0.030\n",
      "Iteration: 120 \t--- Loss: 0.030\n",
      "Iteration: 121 \t--- Loss: 0.028\n",
      "Iteration: 122 \t--- Loss: 0.030\n",
      "Iteration: 123 \t--- Loss: 0.030\n",
      "Iteration: 124 \t--- Loss: 0.031\n",
      "Iteration: 125 \t--- Loss: 0.029\n",
      "Iteration: 126 \t--- Loss: 0.030\n",
      "Iteration: 127 \t--- Loss: 0.030\n",
      "Iteration: 128 \t--- Loss: 0.030\n",
      "Iteration: 129 \t--- Loss: 0.030\n",
      "Iteration: 130 \t--- Loss: 0.029\n",
      "Iteration: 131 \t--- Loss: 0.031\n",
      "Iteration: 132 \t--- Loss: 0.031\n",
      "Iteration: 133 \t--- Loss: 0.030\n",
      "Iteration: 134 \t--- Loss: 0.029\n",
      "Iteration: 135 \t--- Loss: 0.030\n",
      "Iteration: 136 \t--- Loss: 0.031\n",
      "Iteration: 137 \t--- Loss: 0.030\n",
      "Iteration: 138 \t--- Loss: 0.030\n",
      "Iteration: 139 \t--- Loss: 0.031\n",
      "Iteration: 140 \t--- Loss: 0.028\n",
      "Iteration: 141 \t--- Loss: 0.031\n",
      "Iteration: 142 \t--- Loss: 0.031\n",
      "Iteration: 143 \t--- Loss: 0.031\n",
      "Iteration: 144 \t--- Loss: 0.031\n",
      "Iteration: 145 \t--- Loss: 0.029\n",
      "Iteration: 146 \t--- Loss: 0.031\n",
      "Iteration: 147 \t--- Loss: 0.031\n",
      "Iteration: 148 \t--- Loss: 0.030\n",
      "Iteration: 149 \t--- Loss: 0.030\n",
      "Iteration: 150 \t--- Loss: 0.029\n",
      "Iteration: 151 \t--- Loss: 0.029\n",
      "Iteration: 152 \t--- Loss: 0.030\n",
      "Iteration: 153 \t--- Loss: 0.030\n",
      "Iteration: 154 \t--- Loss: 0.030\n",
      "Iteration: 155 \t--- Loss: 0.030\n",
      "Iteration: 156 \t--- Loss: 0.030\n",
      "Iteration: 157 \t--- Loss: 0.029\n",
      "Iteration: 158 \t--- Loss: 0.029\n",
      "Iteration: 159 \t--- Loss: 0.031\n",
      "Iteration: 160 \t--- Loss: 0.029\n",
      "Iteration: 161 \t--- Loss: 0.029\n",
      "Iteration: 162 \t--- Loss: 0.029\n",
      "Iteration: 163 \t--- Loss: 0.029\n",
      "Iteration: 164 \t--- Loss: 0.031\n",
      "Iteration: 165 \t--- Loss: 0.029\n",
      "Iteration: 166 \t--- Loss: 0.031\n",
      "Iteration: 167 \t--- Loss: 0.029\n",
      "Iteration: 168 \t--- Loss: 0.029\n",
      "Iteration: 169 \t--- Loss: 0.030\n",
      "Iteration: 170 \t--- Loss: 0.028\n",
      "Iteration: 171 \t--- Loss: 0.029\n",
      "Iteration: 172 \t--- Loss: 0.028\n",
      "Iteration: 173 \t--- Loss: 0.028\n",
      "Iteration: 174 \t--- Loss: 0.031\n",
      "Iteration: 175 \t--- Loss: 0.029\n",
      "Iteration: 176 \t--- Loss: 0.031\n",
      "Iteration: 177 \t--- Loss: 0.031\n",
      "Iteration: 178 \t--- Loss: 0.029\n",
      "Iteration: 179 \t--- Loss: 0.030\n",
      "Iteration: 180 \t--- Loss: 0.028\n",
      "Iteration: 181 \t--- Loss: 0.029\n",
      "Iteration: 182 \t--- Loss: 0.030\n",
      "Iteration: 183 \t--- Loss: 0.031\n",
      "Iteration: 184 \t--- Loss: 0.030\n",
      "Iteration: 185 \t--- Loss: 0.029\n",
      "Iteration: 186 \t--- Loss: 0.029\n",
      "Iteration: 187 \t--- Loss: 0.029\n",
      "Iteration: 188 \t--- Loss: 0.029\n",
      "Iteration: 189 \t--- Loss: 0.029\n",
      "Iteration: 190 \t--- Loss: 0.030\n",
      "Iteration: 191 \t--- Loss: 0.030\n",
      "Iteration: 192 \t--- Loss: 0.029\n",
      "Iteration: 193 \t--- Loss: 0.030\n",
      "Iteration: 194 \t--- Loss: 0.029\n",
      "Iteration: 195 \t--- Loss: 0.030\n",
      "Iteration: 196 \t--- Loss: 0.029\n",
      "Iteration: 197 \t--- Loss: 0.030\n",
      "Iteration: 198 \t--- Loss: 0.030\n",
      "Iteration: 199 \t--- Loss: 0.029\n",
      "Iteration: 200 \t--- Loss: 0.031\n",
      "Iteration: 201 \t--- Loss: 0.029\n",
      "Iteration: 202 \t--- Loss: 0.029\n",
      "Iteration: 203 \t--- Loss: 0.030\n",
      "Iteration: 204 \t--- Loss: 0.030\n",
      "Iteration: 205 \t--- Loss: 0.030\n",
      "Iteration: 206 \t--- Loss: 0.030\n",
      "Iteration: 207 \t--- Loss: 0.030\n",
      "Iteration: 208 \t--- Loss: 0.028\n",
      "Iteration: 209 \t--- Loss: 0.029\n",
      "Iteration: 210 \t--- Loss: 0.030\n",
      "Iteration: 211 \t--- Loss: 0.030\n",
      "Iteration: 212 \t--- Loss: 0.030\n",
      "Iteration: 213 \t--- Loss: 0.028\n",
      "Iteration: 214 \t--- Loss: 0.030\n",
      "Iteration: 215 \t--- Loss: 0.030\n",
      "Iteration: 216 \t--- Loss: 0.029\n",
      "Iteration: 217 \t--- Loss: 0.029\n",
      "Iteration: 218 \t--- Loss: 0.029\n",
      "Iteration: 219 \t--- Loss: 0.028\n",
      "Iteration: 220 \t--- Loss: 0.030\n",
      "Iteration: 221 \t--- Loss: 0.028\n",
      "Iteration: 222 \t--- Loss: 0.030\n",
      "Iteration: 223 \t--- Loss: 0.030\n",
      "Iteration: 224 \t--- Loss: 0.029\n",
      "Iteration: 225 \t--- Loss: 0.028\n",
      "Iteration: 226 \t--- Loss: 0.031\n",
      "Iteration: 227 \t--- Loss: 0.031\n",
      "Iteration: 228 \t--- Loss: 0.030\n",
      "Iteration: 229 \t--- Loss: 0.031\n",
      "Iteration: 230 \t--- Loss: 0.030\n",
      "Iteration: 231 \t--- Loss: 0.030\n",
      "Iteration: 232 \t--- Loss: 0.027\n",
      "Iteration: 233 \t--- Loss: 0.029\n",
      "Iteration: 234 \t--- Loss: 0.030\n",
      "Iteration: 235 \t--- Loss: 0.030\n",
      "Iteration: 236 \t--- Loss: 0.031\n",
      "Iteration: 237 \t--- Loss: 0.029\n",
      "Iteration: 238 \t--- Loss: 0.029\n",
      "Iteration: 239 \t--- Loss: 0.028\n",
      "Iteration: 240 \t--- Loss: 0.029\n",
      "Iteration: 241 \t--- Loss: 0.029\n",
      "Iteration: 242 \t--- Loss: 0.029\n",
      "Iteration: 243 \t--- Loss: 0.030\n",
      "Iteration: 244 \t--- Loss: 0.029\n",
      "Iteration: 245 \t--- Loss: 0.029\n",
      "Iteration: 246 \t--- Loss: 0.029\n",
      "Iteration: 247 \t--- Loss: 0.029\n",
      "Iteration: 248 \t--- Loss: 0.029\n",
      "Iteration: 249 \t--- Loss: 0.028\n",
      "Iteration: 250 \t--- Loss: 0.029\n",
      "Iteration: 251 \t--- Loss: 0.030\n",
      "Iteration: 252 \t--- Loss: 0.028\n",
      "Iteration: 253 \t--- Loss: 0.029\n",
      "Iteration: 254 \t--- Loss: 0.029\n",
      "Iteration: 255 \t--- Loss: 0.032\n",
      "Iteration: 256 \t--- Loss: 0.030\n",
      "Iteration: 257 \t--- Loss: 0.029\n",
      "Iteration: 258 \t--- Loss: 0.029\n",
      "Iteration: 259 \t--- Loss: 0.029"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:34<00:00, 94.57s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.027\n",
      "Iteration: 261 \t--- Loss: 0.030\n",
      "Iteration: 262 \t--- Loss: 0.030\n",
      "Iteration: 263 \t--- Loss: 0.028\n",
      "Iteration: 264 \t--- Loss: 0.030\n",
      "Iteration: 265 \t--- Loss: 0.029\n",
      "Iteration: 266 \t--- Loss: 0.028\n",
      "Iteration: 267 \t--- Loss: 0.030\n",
      "Iteration: 268 \t--- Loss: 0.028\n",
      "Iteration: 269 \t--- Loss: 0.029\n",
      "Iteration: 270 \t--- Loss: 0.030\n",
      "Iteration: 271 \t--- Loss: 0.030\n",
      "Iteration: 272 \t--- Loss: 0.030\n",
      "Iteration: 273 \t--- Loss: 0.030\n",
      "Iteration: 274 \t--- Loss: 0.028\n",
      "Iteration: 275 \t--- Loss: 0.029\n",
      "Iteration: 276 \t--- Loss: 0.030\n",
      "Iteration: 277 \t--- Loss: 0.028\n",
      "Iteration: 278 \t--- Loss: 0.029\n",
      "Iteration: 279 \t--- Loss: 0.029\n",
      "Iteration: 280 \t--- Loss: 0.029\n",
      "Iteration: 281 \t--- Loss: 0.028\n",
      "Iteration: 282 \t--- Loss: 0.029\n",
      "Iteration: 283 \t--- Loss: 0.029\n",
      "Iteration: 284 \t--- Loss: 0.029\n",
      "Iteration: 285 \t--- Loss: 0.028\n",
      "Iteration: 286 \t--- Loss: 0.029\n",
      "Iteration: 287 \t--- Loss: 0.029\n",
      "Iteration: 288 \t--- Loss: 0.030\n",
      "Iteration: 289 \t--- Loss: 0.028\n",
      "Iteration: 290 \t--- Loss: 0.028\n",
      "Iteration: 291 \t--- Loss: 0.029\n",
      "Iteration: 292 \t--- Loss: 0.029\n",
      "Iteration: 293 \t--- Loss: 0.029\n",
      "Iteration: 294 \t--- Loss: 0.029\n",
      "Iteration: 295 \t--- Loss: 0.030\n",
      "Iteration: 296 \t--- Loss: 0.029\n",
      "Iteration: 297 \t--- Loss: 0.029\n",
      "Iteration: 298 \t--- Loss: 0.029\n",
      "Iteration: 299 \t--- Loss: 0.028\n",
      "Iteration: 300 \t--- Loss: 0.029\n",
      "Iteration: 301 \t--- Loss: 0.031\n",
      "Iteration: 302 \t--- Loss: 0.029\n",
      "Iteration: 303 \t--- Loss: 0.029\n",
      "Iteration: 304 \t--- Loss: 0.029\n",
      "Iteration: 305 \t--- Loss: 0.029\n",
      "Iteration: 306 \t--- Loss: 0.028\n",
      "Iteration: 307 \t--- Loss: 0.029\n",
      "Iteration: 308 \t--- Loss: 0.030\n",
      "Iteration: 309 \t--- Loss: 0.031\n",
      "Iteration: 310 \t--- Loss: 0.027\n",
      "Iteration: 311 \t--- Loss: 0.031\n",
      "Iteration: 312 \t--- Loss: 0.029\n",
      "Iteration: 313 \t--- Loss: 0.029\n",
      "Iteration: 314 \t--- Loss: 0.029\n",
      "Iteration: 315 \t--- Loss: 0.027\n",
      "Iteration: 316 \t--- Loss: 0.029\n",
      "Iteration: 317 \t--- Loss: 0.029\n",
      "Iteration: 318 \t--- Loss: 0.029\n",
      "Iteration: 319 \t--- Loss: 0.029\n",
      "Iteration: 320 \t--- Loss: 0.029\n",
      "Iteration: 321 \t--- Loss: 0.029\n",
      "Iteration: 322 \t--- Loss: 0.029\n",
      "Iteration: 323 \t--- Loss: 0.028\n",
      "Iteration: 324 \t--- Loss: 0.028\n",
      "Iteration: 325 \t--- Loss: 0.029\n",
      "Iteration: 326 \t--- Loss: 0.029\n",
      "Iteration: 327 \t--- Loss: 0.030\n",
      "Iteration: 328 \t--- Loss: 0.029\n",
      "Iteration: 329 \t--- Loss: 0.029\n",
      "Iteration: 330 \t--- Loss: 0.028\n",
      "Iteration: 331 \t--- Loss: 0.028\n",
      "Iteration: 332 \t--- Loss: 0.027\n",
      "Iteration: 333 \t--- Loss: 0.030\n",
      "Iteration: 334 \t--- Loss: 0.029\n",
      "Iteration: 335 \t--- Loss: 0.030\n",
      "Iteration: 336 \t--- Loss: 0.028\n",
      "Iteration: 337 \t--- Loss: 0.028\n",
      "Iteration: 338 \t--- Loss: 0.030\n",
      "Iteration: 339 \t--- Loss: 0.028\n",
      "Iteration: 340 \t--- Loss: 0.029\n",
      "Iteration: 341 \t--- Loss: 0.028\n",
      "Iteration: 342 \t--- Loss: 0.028\n",
      "Iteration: 343 \t--- Loss: 0.028\n",
      "Iteration: 344 \t--- Loss: 0.028\n",
      "Iteration: 345 \t--- Loss: 0.030\n",
      "Iteration: 346 \t--- Loss: 0.029\n",
      "Iteration: 347 \t--- Loss: 0.030\n",
      "Iteration: 348 \t--- Loss: 0.029\n",
      "Iteration: 349 \t--- Loss: 0.028\n",
      "Iteration: 350 \t--- Loss: 0.027\n",
      "Iteration: 351 \t--- Loss: 0.027\n",
      "Iteration: 352 \t--- Loss: 0.030\n",
      "Iteration: 353 \t--- Loss: 0.027\n",
      "Iteration: 354 \t--- Loss: 0.029\n",
      "Iteration: 355 \t--- Loss: 0.028\n",
      "Iteration: 356 \t--- Loss: 0.029\n",
      "Iteration: 357 \t--- Loss: 0.029\n",
      "Iteration: 358 \t--- Loss: 0.030\n",
      "Iteration: 359 \t--- Loss: 0.028\n",
      "Iteration: 360 \t--- Loss: 0.028\n",
      "Iteration: 361 \t--- Loss: 0.031\n",
      "Iteration: 362 \t--- Loss: 0.029\n",
      "Iteration: 363 \t--- Loss: 0.028\n",
      "Iteration: 364 \t--- Loss: 0.027\n",
      "Iteration: 365 \t--- Loss: 0.029\n",
      "Iteration: 366 \t--- Loss: 0.028\n",
      "Iteration: 367 \t--- Loss: 0.029\n",
      "Iteration: 368 \t--- Loss: 0.027\n",
      "Iteration: 369 \t--- Loss: 0.030\n",
      "Iteration: 370 \t--- Loss: 0.030\n",
      "Iteration: 371 \t--- Loss: 0.028\n",
      "Iteration: 372 \t--- Loss: 0.028\n",
      "Iteration: 373 \t--- Loss: 0.029\n",
      "Iteration: 374 \t--- Loss: 0.028\n",
      "Iteration: 375 \t--- Loss: 0.029\n",
      "Iteration: 376 \t--- Loss: 0.028\n",
      "Iteration: 377 \t--- Loss: 0.029\n",
      "Iteration: 378 \t--- Loss: 0.029\n",
      "Iteration: 379 \t--- Loss: 0.028\n",
      "Iteration: 380 \t--- Loss: 0.028\n",
      "Iteration: 381 \t--- Loss: 0.027\n",
      "Iteration: 382 \t--- Loss: 0.029\n",
      "Iteration: 383 \t--- Loss: 0.028\n",
      "Iteration: 384 \t--- Loss: 0.027\n",
      "Iteration: 385 \t--- Loss: 0.026\n",
      "Iteration: 386 \t--- Loss: 0.028\n",
      "Iteration: 387 \t--- Loss: 0.028\n",
      "Iteration: 388 \t--- Loss: 0.027\n",
      "Iteration: 389 \t--- Loss: 0.027\n",
      "Iteration: 390 \t--- Loss: 0.028\n",
      "Iteration: 391 \t--- Loss: 0.028\n",
      "Iteration: 392 \t--- Loss: 0.028\n",
      "Iteration: 393 \t--- Loss: 0.027\n",
      "Iteration: 394 \t--- Loss: 0.028\n",
      "Iteration: 395 \t--- Loss: 0.029\n",
      "Iteration: 396 \t--- Loss: 0.028\n",
      "Iteration: 397 \t--- Loss: 0.029\n",
      "Iteration: 398 \t--- Loss: 0.028\n",
      "Iteration: 399 \t--- Loss: 0.029\n",
      "Iteration: 400 \t--- Loss: 0.027\n",
      "Iteration: 401 \t--- Loss: 0.028\n",
      "Iteration: 402 \t--- Loss: 0.027\n",
      "Iteration: 403 \t--- Loss: 0.028\n",
      "Iteration: 404 \t--- Loss: 0.027\n",
      "Iteration: 405 \t--- Loss: 0.028\n",
      "Iteration: 406 \t--- Loss: 0.028\n",
      "Iteration: 407 \t--- Loss: 0.028\n",
      "Iteration: 408 \t--- Loss: 0.027\n",
      "Iteration: 409 \t--- Loss: 0.028\n",
      "Iteration: 410 \t--- Loss: 0.027\n",
      "Iteration: 411 \t--- Loss: 0.029\n",
      "Iteration: 412 \t--- Loss: 0.028\n",
      "Iteration: 413 \t--- Loss: 0.027\n",
      "Iteration: 414 \t--- Loss: 0.027\n",
      "Iteration: 415 \t--- Loss: 0.029\n",
      "Iteration: 416 \t--- Loss: 0.029\n",
      "Iteration: 417 \t--- Loss: 0.028\n",
      "Iteration: 418 \t--- Loss: 0.028\n",
      "Iteration: 419 \t--- Loss: 0.028\n",
      "Iteration: 420 \t--- Loss: 0.029\n",
      "Iteration: 421 \t--- Loss: 0.029\n",
      "Iteration: 422 \t--- Loss: 0.029\n",
      "Iteration: 423 \t--- Loss: 0.027\n",
      "Iteration: 424 \t--- Loss: 0.029\n",
      "Iteration: 425 \t--- Loss: 0.029\n",
      "Iteration: 426 \t--- Loss: 0.028\n",
      "Iteration: 427 \t--- Loss: 0.027\n",
      "Iteration: 428 \t--- Loss: 0.026\n",
      "Iteration: 429 \t--- Loss: 0.027\n",
      "Iteration: 430 \t--- Loss: 0.028\n",
      "Iteration: 431 \t--- Loss: 0.027\n",
      "Iteration: 432 \t--- Loss: 0.027\n",
      "Iteration: 433 \t--- Loss: 0.028\n",
      "Iteration: 434 \t--- Loss: 0.028\n",
      "Iteration: 435 \t--- Loss: 0.027\n",
      "Iteration: 436 \t--- Loss: 0.028\n",
      "Iteration: 437 \t--- Loss: 0.027\n",
      "Iteration: 438 \t--- Loss: 0.028\n",
      "Iteration: 439 \t--- Loss: 0.029\n",
      "Iteration: 440 \t--- Loss: 0.026\n",
      "Iteration: 441 \t--- Loss: 0.028\n",
      "Iteration: 442 \t--- Loss: 0.028\n",
      "Iteration: 443 \t--- Loss: 0.026\n",
      "Iteration: 444 \t--- Loss: 0.028\n",
      "Iteration: 445 \t--- Loss: 0.028\n",
      "Iteration: 446 \t--- Loss: 0.027\n",
      "Iteration: 447 \t--- Loss: 0.027\n",
      "Iteration: 448 \t--- Loss: 0.027\n",
      "Iteration: 449 \t--- Loss: 0.028\n",
      "Iteration: 450 \t--- Loss: 0.027\n",
      "Iteration: 451 \t--- Loss: 0.029\n",
      "Iteration: 452 \t--- Loss: 0.028\n",
      "Iteration: 453 \t--- Loss: 0.027\n",
      "Iteration: 454 \t--- Loss: 0.028\n",
      "Iteration: 455 \t--- Loss: 0.027\n",
      "Iteration: 456 \t--- Loss: 0.027\n",
      "Iteration: 457 \t--- Loss: 0.027\n",
      "Iteration: 458 \t--- Loss: 0.028\n",
      "Iteration: 459 \t--- Loss: 0.027\n",
      "Iteration: 460 \t--- Loss: 0.029\n",
      "Iteration: 461 \t--- Loss: 0.028\n",
      "Iteration: 462 \t--- Loss: 0.027\n",
      "Iteration: 463 \t--- Loss: 0.027\n",
      "Iteration: 464 \t--- Loss: 0.027\n",
      "Iteration: 465 \t--- Loss: 0.028\n",
      "Iteration: 466 \t--- Loss: 0.028\n",
      "Iteration: 467 \t--- Loss: 0.027\n",
      "Iteration: 468 \t--- Loss: 0.028\n",
      "Iteration: 469 \t--- Loss: 0.027\n",
      "Iteration: 470 \t--- Loss: 0.027\n",
      "Iteration: 471 \t--- Loss: 0.026\n",
      "Iteration: 472 \t--- Loss: 0.028\n",
      "Iteration: 473 \t--- Loss: 0.027\n",
      "Iteration: 474 \t--- Loss: 0.026\n",
      "Iteration: 475 \t--- Loss: 0.027\n",
      "Iteration: 476 \t--- Loss: 0.027\n",
      "Iteration: 477 \t--- Loss: 0.028\n",
      "Iteration: 478 \t--- Loss: 0.027\n",
      "Iteration: 479 \t--- Loss: 0.028\n",
      "Iteration: 480 \t--- Loss: 0.028\n",
      "Iteration: 481 \t--- Loss: 0.026\n",
      "Iteration: 482 \t--- Loss: 0.029\n",
      "Iteration: 483 \t--- Loss: 0.029\n",
      "Iteration: 484 \t--- Loss: 0.027\n",
      "Iteration: 485 \t--- Loss: 0.027\n",
      "Iteration: 486 \t--- Loss: 0.027\n",
      "Iteration: 487 \t--- Loss: 0.028\n",
      "Iteration: 488 \t--- Loss: 0.027\n",
      "Iteration: 489 \t--- Loss: 0.026\n",
      "Iteration: 490 \t--- Loss: 0.029\n",
      "Iteration: 491 \t--- Loss: 0.026\n",
      "Iteration: 492 \t--- Loss: 0.028\n",
      "Iteration: 493 \t--- Loss: 0.028\n",
      "Iteration: 494 \t--- Loss: 0.029\n",
      "Iteration: 495 \t--- Loss: 0.027\n",
      "Iteration: 496 \t--- Loss: 0.027\n",
      "Iteration: 497 \t--- Loss: 0.026\n",
      "Iteration: 498 \t--- Loss: 0.027\n",
      "Iteration: 499 \t--- Loss: 0.027\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.57s/it][Parallel(n_jobs=5)]: Done  59 tasks      | elapsed: 36.8min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.385\n",
      "Iteration: 1 \t--- Loss: 0.386\n",
      "Iteration: 2 \t--- Loss: 0.382\n",
      "Iteration: 3 \t--- Loss: 0.374\n",
      "Iteration: 4 \t--- Loss: 0.372\n",
      "Iteration: 5 \t--- Loss: 0.359\n",
      "Iteration: 6 \t--- Loss: 0.360\n",
      "Iteration: 7 \t--- Loss: 0.371\n",
      "Iteration: 8 \t--- Loss: 0.366\n",
      "Iteration: 9 \t--- Loss: 0.383\n",
      "Iteration: 10 \t--- Loss: 0.354\n",
      "Iteration: 11 \t--- Loss: 0.327\n",
      "Iteration: 12 \t--- Loss: 0.374\n",
      "Iteration: 13 \t--- Loss: 0.360\n",
      "Iteration: 14 \t--- Loss: 0.350\n",
      "Iteration: 15 \t--- Loss: 0.371\n",
      "Iteration: 16 \t--- Loss: 0.372\n",
      "Iteration: 17 \t--- Loss: 0.380\n",
      "Iteration: 18 \t--- Loss: 0.363\n",
      "Iteration: 19 \t--- Loss: 0.347\n",
      "Iteration: 20 \t--- Loss: 0.360\n",
      "Iteration: 21 \t--- Loss: 0.370\n",
      "Iteration: 22 \t--- Loss: 0.353\n",
      "Iteration: 23 \t--- Loss: 0.369\n",
      "Iteration: 24 \t--- Loss: 0.350\n",
      "Iteration: 25 \t--- Loss: 0.346\n",
      "Iteration: 26 \t--- Loss: 0.335\n",
      "Iteration: 27 \t--- Loss: 0.337\n",
      "Iteration: 28 \t--- Loss: 0.355\n",
      "Iteration: 29 \t--- Loss: 0.345\n",
      "Iteration: 30 \t--- Loss: 0.366\n",
      "Iteration: 31 \t--- Loss: 0.308\n",
      "Iteration: 32 \t--- Loss: 0.340\n",
      "Iteration: 33 \t--- Loss: 0.366\n",
      "Iteration: 34 \t--- Loss: 0.357\n",
      "Iteration: 35 \t--- Loss: 0.364\n",
      "Iteration: 36 \t--- Loss: 0.354\n",
      "Iteration: 37 \t--- Loss: 0.351\n",
      "Iteration: 38 \t--- Loss: 0.353\n",
      "Iteration: 39 \t--- Loss: 0.314\n",
      "Iteration: 40 \t--- Loss: 0.304\n",
      "Iteration: 41 \t--- Loss: 0.304\n",
      "Iteration: 42 \t--- Loss: 0.306\n",
      "Iteration: 43 \t--- Loss: 0.341\n",
      "Iteration: 44 \t--- Loss: 0.308\n",
      "Iteration: 45 \t--- Loss: 0.312\n",
      "Iteration: 46 \t--- Loss: 0.292\n",
      "Iteration: 47 \t--- Loss: 0.323\n",
      "Iteration: 48 \t--- Loss: 0.313\n",
      "Iteration: 49 \t--- Loss: 0.329\n",
      "Iteration: 50 \t--- Loss: 0.328\n",
      "Iteration: 51 \t--- Loss: 0.412\n",
      "Iteration: 52 \t--- Loss: 0.409\n",
      "Iteration: 53 \t--- Loss: 0.398\n",
      "Iteration: 54 \t--- Loss: 0.380\n",
      "Iteration: 55 \t--- Loss: 0.412\n",
      "Iteration: 56 \t--- Loss: 0.353\n",
      "Iteration: 57 \t--- Loss: 0.374\n",
      "Iteration: 58 \t--- Loss: 0.366\n",
      "Iteration: 59 \t--- Loss: 0.356\n",
      "Iteration: 60 \t--- Loss: 0.365\n",
      "Iteration: 61 \t--- Loss: 0.375\n",
      "Iteration: 62 \t--- Loss: 0.370\n",
      "Iteration: 63 \t--- Loss: 0.381\n",
      "Iteration: 64 \t--- Loss: 0.379\n",
      "Iteration: 65 \t--- Loss: 0.368\n",
      "Iteration: 66 \t--- Loss: 0.369\n",
      "Iteration: 67 \t--- Loss: 0.370\n",
      "Iteration: 68 \t--- Loss: 0.356\n",
      "Iteration: 69 \t--- Loss: 0.324\n",
      "Iteration: 70 \t--- Loss: 0.324\n",
      "Iteration: 71 \t--- Loss: 0.346\n",
      "Iteration: 72 \t--- Loss: 0.339\n",
      "Iteration: 73 \t--- Loss: 0.331\n",
      "Iteration: 74 \t--- Loss: 0.341\n",
      "Iteration: 75 \t--- Loss: 0.322\n",
      "Iteration: 76 \t--- Loss: 0.362\n",
      "Iteration: 77 \t--- Loss: 0.360\n",
      "Iteration: 78 \t--- Loss: 0.315\n",
      "Iteration: 79 \t--- Loss: 0.347\n",
      "Iteration: 80 \t--- Loss: 0.360\n",
      "Iteration: 81 \t--- Loss: 0.335\n",
      "Iteration: 82 \t--- Loss: 0.364\n",
      "Iteration: 83 \t--- Loss: 0.358\n",
      "Iteration: 84 \t--- Loss: 0.346\n",
      "Iteration: 85 \t--- Loss: 0.357\n",
      "Iteration: 86 \t--- Loss: 0.317\n",
      "Iteration: 87 \t--- Loss: 0.362\n",
      "Iteration: 88 \t--- Loss: 0.323\n",
      "Iteration: 89 \t--- Loss: 0.361\n",
      "Iteration: 90 \t--- Loss: 0.346\n",
      "Iteration: 91 \t--- Loss: 0.319\n",
      "Iteration: 92 \t--- Loss: 0.317\n",
      "Iteration: 93 \t--- Loss: 0.323\n",
      "Iteration: 94 \t--- Loss: 0.332\n",
      "Iteration: 95 \t--- Loss: 0.322\n",
      "Iteration: 96 \t--- Loss: 0.305\n",
      "Iteration: 97 \t--- Loss: 0.318\n",
      "Iteration: 98 \t--- Loss: 0.315\n",
      "Iteration: 99 \t--- Loss: 0.318\n",
      "Iteration: 100 \t--- Loss: 0.332\n",
      "Iteration: 101 \t--- Loss: 0.320\n",
      "Iteration: 102 \t--- Loss: 0.340\n",
      "Iteration: 103 \t--- Loss: 0.331\n",
      "Iteration: 104 \t--- Loss: 0.325\n",
      "Iteration: 105 \t--- Loss: 0.309\n",
      "Iteration: 106 \t--- Loss: 0.343\n",
      "Iteration: 107 \t--- Loss: 0.313\n",
      "Iteration: 108 \t--- Loss: 0.318\n",
      "Iteration: 109 \t--- Loss: 0.301\n",
      "Iteration: 110 \t--- Loss: 0.281\n",
      "Iteration: 111 \t--- Loss: 0.284\n",
      "Iteration: 112 \t--- Loss: 0.259\n",
      "Iteration: 113 \t--- Loss: 0.288\n",
      "Iteration: 114 \t--- Loss: 0.298\n",
      "Iteration: 115 \t--- Loss: 0.270\n",
      "Iteration: 116 \t--- Loss: 0.273\n",
      "Iteration: 117 \t--- Loss: 0.282\n",
      "Iteration: 118 \t--- Loss: 0.278\n",
      "Iteration: 119 \t--- Loss: 0.289\n",
      "Iteration: 120 \t--- Loss: 0.257\n",
      "Iteration: 121 \t--- Loss: 0.296\n",
      "Iteration: 122 \t--- Loss: 0.310\n",
      "Iteration: 123 \t--- Loss: 0.252\n",
      "Iteration: 124 \t--- Loss: 0.294\n",
      "Iteration: 125 \t--- Loss: 0.268\n",
      "Iteration: 126 \t--- Loss: 0.277\n",
      "Iteration: 127 \t--- Loss: 0.268\n",
      "Iteration: 128 \t--- Loss: 0.281\n",
      "Iteration: 129 \t--- Loss: 0.223\n",
      "Iteration: 130 \t--- Loss: 0.252\n",
      "Iteration: 131 \t--- Loss: 0.268\n",
      "Iteration: 132 \t--- Loss: 0.255\n",
      "Iteration: 133 \t--- Loss: 0.249\n",
      "Iteration: 134 \t--- Loss: 0.276\n",
      "Iteration: 135 \t--- Loss: 0.277\n",
      "Iteration: 136 \t--- Loss: 0.266\n",
      "Iteration: 137 \t--- Loss: 0.292\n",
      "Iteration: 138 \t--- Loss: 0.270\n",
      "Iteration: 139 \t--- Loss: 0.267\n",
      "Iteration: 140 \t--- Loss: 0.270\n",
      "Iteration: 141 \t--- Loss: 0.307\n",
      "Iteration: 142 \t--- Loss: 0.274\n",
      "Iteration: 143 \t--- Loss: 0.268\n",
      "Iteration: 144 \t--- Loss: 0.263\n",
      "Iteration: 145 \t--- Loss: 0.287\n",
      "Iteration: 146 \t--- Loss: 0.260\n",
      "Iteration: 147 \t--- Loss: 0.295\n",
      "Iteration: 148 \t--- Loss: 0.263\n",
      "Iteration: 149 \t--- Loss: 0.265\n",
      "Iteration: 150 \t--- Loss: 0.309\n",
      "Iteration: 151 \t--- Loss: 0.275\n",
      "Iteration: 152 \t--- Loss: 0.266\n",
      "Iteration: 153 \t--- Loss: 0.281\n",
      "Iteration: 154 \t--- Loss: 0.296\n",
      "Iteration: 155 \t--- Loss: 0.280\n",
      "Iteration: 156 \t--- Loss: 0.287\n",
      "Iteration: 157 \t--- Loss: 0.298\n",
      "Iteration: 158 \t--- Loss: 0.333\n",
      "Iteration: 159 \t--- Loss: 0.274\n",
      "Iteration: 160 \t--- Loss: 0.268\n",
      "Iteration: 161 \t--- Loss: 0.283\n",
      "Iteration: 162 \t--- Loss: 0.273\n",
      "Iteration: 163 \t--- Loss: 0.303\n",
      "Iteration: 164 \t--- Loss: 0.248\n",
      "Iteration: 165 \t--- Loss: 0.254\n",
      "Iteration: 166 \t--- Loss: 0.281\n",
      "Iteration: 167 \t--- Loss: 0.251\n",
      "Iteration: 168 \t--- Loss: 0.233\n",
      "Iteration: 169 \t--- Loss: 0.277\n",
      "Iteration: 170 \t--- Loss: 0.296\n",
      "Iteration: 171 \t--- Loss: 0.235\n",
      "Iteration: 172 \t--- Loss: 0.251\n",
      "Iteration: 173 \t--- Loss: 0.275\n",
      "Iteration: 174 \t--- Loss: 0.275\n",
      "Iteration: 175 \t--- Loss: 0.250\n",
      "Iteration: 176 \t--- Loss: 0.264\n",
      "Iteration: 177 \t--- Loss: 0.263\n",
      "Iteration: 178 \t--- Loss: 0.261\n",
      "Iteration: 179 \t--- Loss: 0.247\n",
      "Iteration: 180 \t--- Loss: 0.260\n",
      "Iteration: 181 \t--- Loss: 0.232\n",
      "Iteration: 182 \t--- Loss: 0.268\n",
      "Iteration: 183 \t--- Loss: 0.255\n",
      "Iteration: 184 \t--- Loss: 0.269\n",
      "Iteration: 185 \t--- Loss: 0.241\n",
      "Iteration: 186 \t--- Loss: 0.272\n",
      "Iteration: 187 \t--- Loss: 0.294\n",
      "Iteration: 188 \t--- Loss: 0.267\n",
      "Iteration: 189 \t--- Loss: 0.270\n",
      "Iteration: 190 \t--- Loss: 0.287\n",
      "Iteration: 191 \t--- Loss: 0.325\n",
      "Iteration: 192 \t--- Loss: 0.242\n",
      "Iteration: 193 \t--- Loss: 0.240\n",
      "Iteration: 194 \t--- Loss: 0.261\n",
      "Iteration: 195 \t--- Loss: 0.267\n",
      "Iteration: 196 \t--- Loss: 0.304\n",
      "Iteration: 197 \t--- Loss: 0.284\n",
      "Iteration: 198 \t--- Loss: 0.306\n",
      "Iteration: 199 \t--- Loss: 0.247\n",
      "Iteration: 200 \t--- Loss: 0.247\n",
      "Iteration: 201 \t--- Loss: 0.272\n",
      "Iteration: 202 \t--- Loss: 0.269\n",
      "Iteration: 203 \t--- Loss: 0.251\n",
      "Iteration: 204 \t--- Loss: 0.279\n",
      "Iteration: 205 \t--- Loss: 0.243\n",
      "Iteration: 206 \t--- Loss: 0.260\n",
      "Iteration: 207 \t--- Loss: 0.281\n",
      "Iteration: 208 \t--- Loss: 0.265\n",
      "Iteration: 209 \t--- Loss: 0.229\n",
      "Iteration: 210 \t--- Loss: 0.261\n",
      "Iteration: 211 \t--- Loss: 0.242\n",
      "Iteration: 212 \t--- Loss: 0.241\n",
      "Iteration: 213 \t--- Loss: 0.268\n",
      "Iteration: 214 \t--- Loss: 0.250\n",
      "Iteration: 215 \t--- Loss: 0.237\n",
      "Iteration: 216 \t--- Loss: 0.236\n",
      "Iteration: 217 \t--- Loss: 0.290\n",
      "Iteration: 218 \t--- Loss: 0.262\n",
      "Iteration: 219 \t--- Loss: 0.229\n",
      "Iteration: 220 \t--- Loss: 0.257\n",
      "Iteration: 221 \t--- Loss: 0.258\n",
      "Iteration: 222 \t--- Loss: 0.260\n",
      "Iteration: 223 \t--- Loss: 0.265\n",
      "Iteration: 224 \t--- Loss: 0.249\n",
      "Iteration: 225 \t--- Loss: 0.254\n",
      "Iteration: 226 \t--- Loss: 0.249\n",
      "Iteration: 227 \t--- Loss: 0.262\n",
      "Iteration: 228 \t--- Loss: 0.257\n",
      "Iteration: 229 \t--- Loss: 0.263\n",
      "Iteration: 230 \t--- Loss: 0.244\n",
      "Iteration: 231 \t--- Loss: 0.225\n",
      "Iteration: 232 \t--- Loss: 0.239\n",
      "Iteration: 233 \t--- Loss: 0.231\n",
      "Iteration: 234 \t--- Loss: 0.254\n",
      "Iteration: 235 \t--- Loss: 0.247\n",
      "Iteration: 236 \t--- Loss: 0.231\n",
      "Iteration: 237 \t--- Loss: 0.256\n",
      "Iteration: 238 \t--- Loss: 0.224\n",
      "Iteration: 239 \t--- Loss: 0.236\n",
      "Iteration: 240 \t--- Loss: 0.226\n",
      "Iteration: 241 \t--- Loss: 0.229\n",
      "Iteration: 242 \t--- Loss: 0.246\n",
      "Iteration: 243 \t--- Loss: 0.238\n",
      "Iteration: 244 \t--- Loss: 0.251\n",
      "Iteration: 245 \t--- Loss: 0.235\n",
      "Iteration: 246 \t--- Loss: 0.232\n",
      "Iteration: 247 \t--- Loss: 0.249\n",
      "Iteration: 248 \t--- Loss: 0.243\n",
      "Iteration: 249 \t--- Loss: 0.246\n",
      "Iteration: 250 \t--- Loss: 0.248\n",
      "Iteration: 251 \t--- Loss: 0.237\n",
      "Iteration: 252 \t--- Loss: 0.244\n",
      "Iteration: 253 \t--- Loss: 0.231\n",
      "Iteration: 254 \t--- Loss: 0.249\n",
      "Iteration: 255 \t--- Loss: 0.235\n",
      "Iteration: 256 \t--- Loss: 0.223\n",
      "Iteration: 257 \t--- Loss: 0.251\n",
      "Iteration: 258 \t--- Loss: 0.263\n",
      "Iteration: 259 \t--- Loss: 0.238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:36<00:00, 576.67s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.142\n",
      "Iteration: 261 \t--- Loss: 0.130\n",
      "Iteration: 262 \t--- Loss: 0.133\n",
      "Iteration: 263 \t--- Loss: 0.124\n",
      "Iteration: 264 \t--- Loss: 0.125\n",
      "Iteration: 265 \t--- Loss: 0.116\n",
      "Iteration: 266 \t--- Loss: 0.122\n",
      "Iteration: 267 \t--- Loss: 0.123\n",
      "Iteration: 268 \t--- Loss: 0.126\n",
      "Iteration: 269 \t--- Loss: 0.141\n",
      "Iteration: 270 \t--- Loss: 0.132\n",
      "Iteration: 271 \t--- Loss: 0.123\n",
      "Iteration: 272 \t--- Loss: 0.131\n",
      "Iteration: 273 \t--- Loss: 0.129\n",
      "Iteration: 274 \t--- Loss: 0.142\n",
      "Iteration: 275 \t--- Loss: 0.137\n",
      "Iteration: 276 \t--- Loss: 0.126\n",
      "Iteration: 277 \t--- Loss: 0.130\n",
      "Iteration: 278 \t--- Loss: 0.135\n",
      "Iteration: 279 \t--- Loss: 0.130\n",
      "Iteration: 280 \t--- Loss: 0.126\n",
      "Iteration: 281 \t--- Loss: 0.128\n",
      "Iteration: 282 \t--- Loss: 0.136\n",
      "Iteration: 283 \t--- Loss: 0.130\n",
      "Iteration: 284 \t--- Loss: 0.129\n",
      "Iteration: 285 \t--- Loss: 0.128\n",
      "Iteration: 286 \t--- Loss: 0.125\n",
      "Iteration: 287 \t--- Loss: 0.120\n",
      "Iteration: 288 \t--- Loss: 0.124\n",
      "Iteration: 289 \t--- Loss: 0.124\n",
      "Iteration: 290 \t--- Loss: 0.132\n",
      "Iteration: 291 \t--- Loss: 0.119\n",
      "Iteration: 292 \t--- Loss: 0.130\n",
      "Iteration: 293 \t--- Loss: 0.123\n",
      "Iteration: 294 \t--- Loss: 0.126\n",
      "Iteration: 295 \t--- Loss: 0.132\n",
      "Iteration: 296 \t--- Loss: 0.123\n",
      "Iteration: 297 \t--- Loss: 0.118\n",
      "Iteration: 298 \t--- Loss: 0.122\n",
      "Iteration: 299 \t--- Loss: 0.127\n",
      "Iteration: 300 \t--- Loss: 0.126\n",
      "Iteration: 301 \t--- Loss: 0.124\n",
      "Iteration: 302 \t--- Loss: 0.126\n",
      "Iteration: 303 \t--- Loss: 0.124\n",
      "Iteration: 304 \t--- Loss: 0.124\n",
      "Iteration: 305 \t--- Loss: 0.124\n",
      "Iteration: 306 \t--- Loss: 0.130\n",
      "Iteration: 307 \t--- Loss: 0.126\n",
      "Iteration: 308 \t--- Loss: 0.129\n",
      "Iteration: 309 \t--- Loss: 0.121\n",
      "Iteration: 310 \t--- Loss: 0.127\n",
      "Iteration: 311 \t--- Loss: 0.114\n",
      "Iteration: 312 \t--- Loss: 0.129\n",
      "Iteration: 313 \t--- Loss: 0.130\n",
      "Iteration: 314 \t--- Loss: 0.124\n",
      "Iteration: 315 \t--- Loss: 0.120\n",
      "Iteration: 316 \t--- Loss: 0.131\n",
      "Iteration: 317 \t--- Loss: 0.125\n",
      "Iteration: 318 \t--- Loss: 0.128\n",
      "Iteration: 319 \t--- Loss: 0.121\n",
      "Iteration: 320 \t--- Loss: 0.118\n",
      "Iteration: 321 \t--- Loss: 0.116\n",
      "Iteration: 322 \t--- Loss: 0.132\n",
      "Iteration: 323 \t--- Loss: 0.132\n",
      "Iteration: 324 \t--- Loss: 0.114\n",
      "Iteration: 325 \t--- Loss: 0.130\n",
      "Iteration: 326 \t--- Loss: 0.128\n",
      "Iteration: 327 \t--- Loss: 0.130\n",
      "Iteration: 328 \t--- Loss: 0.143\n",
      "Iteration: 329 \t--- Loss: 0.136\n",
      "Iteration: 330 \t--- Loss: 0.134\n",
      "Iteration: 331 \t--- Loss: 0.126\n",
      "Iteration: 332 \t--- Loss: 0.117\n",
      "Iteration: 333 \t--- Loss: 0.124\n",
      "Iteration: 334 \t--- Loss: 0.117\n",
      "Iteration: 335 \t--- Loss: 0.129\n",
      "Iteration: 336 \t--- Loss: 0.124\n",
      "Iteration: 337 \t--- Loss: 0.127\n",
      "Iteration: 338 \t--- Loss: 0.121\n",
      "Iteration: 339 \t--- Loss: 0.125\n",
      "Iteration: 340 \t--- Loss: 0.126\n",
      "Iteration: 341 \t--- Loss: 0.135\n",
      "Iteration: 342 \t--- Loss: 0.122\n",
      "Iteration: 343 \t--- Loss: 0.128\n",
      "Iteration: 344 \t--- Loss: 0.122\n",
      "Iteration: 345 \t--- Loss: 0.119\n",
      "Iteration: 346 \t--- Loss: 0.122\n",
      "Iteration: 347 \t--- Loss: 0.127\n",
      "Iteration: 348 \t--- Loss: 0.121\n",
      "Iteration: 349 \t--- Loss: 0.120\n",
      "Iteration: 350 \t--- Loss: 0.128\n",
      "Iteration: 351 \t--- Loss: 0.121\n",
      "Iteration: 352 \t--- Loss: 0.129\n",
      "Iteration: 353 \t--- Loss: 0.116\n",
      "Iteration: 354 \t--- Loss: 0.118\n",
      "Iteration: 355 \t--- Loss: 0.128\n",
      "Iteration: 356 \t--- Loss: 0.116\n",
      "Iteration: 357 \t--- Loss: 0.120\n",
      "Iteration: 358 \t--- Loss: 0.126\n",
      "Iteration: 359 \t--- Loss: 0.127\n",
      "Iteration: 360 \t--- Loss: 0.131\n",
      "Iteration: 361 \t--- Loss: 0.121\n",
      "Iteration: 362 \t--- Loss: 0.127\n",
      "Iteration: 363 \t--- Loss: 0.118\n",
      "Iteration: 364 \t--- Loss: 0.123\n",
      "Iteration: 365 \t--- Loss: 0.124\n",
      "Iteration: 366 \t--- Loss: 0.125\n",
      "Iteration: 367 \t--- Loss: 0.131\n",
      "Iteration: 368 \t--- Loss: 0.115\n",
      "Iteration: 369 \t--- Loss: 0.122\n",
      "Iteration: 370 \t--- Loss: 0.122\n",
      "Iteration: 371 \t--- Loss: 0.128\n",
      "Iteration: 372 \t--- Loss: 0.115\n",
      "Iteration: 373 \t--- Loss: 0.125\n",
      "Iteration: 374 \t--- Loss: 0.126\n",
      "Iteration: 375 \t--- Loss: 0.112\n",
      "Iteration: 376 \t--- Loss: 0.131\n",
      "Iteration: 377 \t--- Loss: 0.119\n",
      "Iteration: 378 \t--- Loss: 0.118\n",
      "Iteration: 379 \t--- Loss: 0.123\n",
      "Iteration: 380 \t--- Loss: 0.121\n",
      "Iteration: 381 \t--- Loss: 0.125\n",
      "Iteration: 382 \t--- Loss: 0.120\n",
      "Iteration: 383 \t--- Loss: 0.125\n",
      "Iteration: 384 \t--- Loss: 0.124\n",
      "Iteration: 385 \t--- Loss: 0.116\n",
      "Iteration: 386 \t--- Loss: 0.124\n",
      "Iteration: 387 \t--- Loss: 0.126\n",
      "Iteration: 388 \t--- Loss: 0.133\n",
      "Iteration: 389 \t--- Loss: 0.125\n",
      "Iteration: 390 \t--- Loss: 0.115\n",
      "Iteration: 391 \t--- Loss: 0.125\n",
      "Iteration: 392 \t--- Loss: 0.126\n",
      "Iteration: 393 \t--- Loss: 0.122\n",
      "Iteration: 394 \t--- Loss: 0.121\n",
      "Iteration: 395 \t--- Loss: 0.123\n",
      "Iteration: 396 \t--- Loss: 0.120\n",
      "Iteration: 397 \t--- Loss: 0.121\n",
      "Iteration: 398 \t--- Loss: 0.130\n",
      "Iteration: 399 \t--- Loss: 0.120\n",
      "Iteration: 400 \t--- Loss: 0.119\n",
      "Iteration: 401 \t--- Loss: 0.125\n",
      "Iteration: 402 \t--- Loss: 0.121\n",
      "Iteration: 403 \t--- Loss: 0.123\n",
      "Iteration: 404 \t--- Loss: 0.127\n",
      "Iteration: 405 \t--- Loss: 0.120\n",
      "Iteration: 406 \t--- Loss: 0.131\n",
      "Iteration: 407 \t--- Loss: 0.117\n",
      "Iteration: 408 \t--- Loss: 0.116\n",
      "Iteration: 409 \t--- Loss: 0.122\n",
      "Iteration: 410 \t--- Loss: 0.122\n",
      "Iteration: 411 \t--- Loss: 0.124\n",
      "Iteration: 412 \t--- Loss: 0.121\n",
      "Iteration: 413 \t--- Loss: 0.120\n",
      "Iteration: 414 \t--- Loss: 0.116\n",
      "Iteration: 415 \t--- Loss: 0.122\n",
      "Iteration: 416 \t--- Loss: 0.115\n",
      "Iteration: 417 \t--- Loss: 0.118\n",
      "Iteration: 418 \t--- Loss: 0.117\n",
      "Iteration: 419 \t--- Loss: 0.113\n",
      "Iteration: 420 \t--- Loss: 0.132\n",
      "Iteration: 421 \t--- Loss: 0.126\n",
      "Iteration: 422 \t--- Loss: 0.125\n",
      "Iteration: 423 \t--- Loss: 0.126\n",
      "Iteration: 424 \t--- Loss: 0.116\n",
      "Iteration: 425 \t--- Loss: 0.111\n",
      "Iteration: 426 \t--- Loss: 0.124\n",
      "Iteration: 427 \t--- Loss: 0.116\n",
      "Iteration: 428 \t--- Loss: 0.110\n",
      "Iteration: 429 \t--- Loss: 0.118\n",
      "Iteration: 430 \t--- Loss: 0.110\n",
      "Iteration: 431 \t--- Loss: 0.127\n",
      "Iteration: 432 \t--- Loss: 0.120\n",
      "Iteration: 433 \t--- Loss: 0.113\n",
      "Iteration: 434 \t--- Loss: 0.117\n",
      "Iteration: 435 \t--- Loss: 0.123\n",
      "Iteration: 436 \t--- Loss: 0.125\n",
      "Iteration: 437 \t--- Loss: 0.124\n",
      "Iteration: 438 \t--- Loss: 0.117\n",
      "Iteration: 439 \t--- Loss: 0.121\n",
      "Iteration: 440 \t--- Loss: 0.129\n",
      "Iteration: 441 \t--- Loss: 0.118\n",
      "Iteration: 442 \t--- Loss: 0.133\n",
      "Iteration: 443 \t--- Loss: 0.118\n",
      "Iteration: 444 \t--- Loss: 0.115\n",
      "Iteration: 445 \t--- Loss: 0.118\n",
      "Iteration: 446 \t--- Loss: 0.112\n",
      "Iteration: 447 \t--- Loss: 0.115\n",
      "Iteration: 448 \t--- Loss: 0.124\n",
      "Iteration: 449 \t--- Loss: 0.121\n",
      "Iteration: 450 \t--- Loss: 0.118\n",
      "Iteration: 451 \t--- Loss: 0.114\n",
      "Iteration: 452 \t--- Loss: 0.118\n",
      "Iteration: 453 \t--- Loss: 0.122\n",
      "Iteration: 454 \t--- Loss: 0.115\n",
      "Iteration: 455 \t--- Loss: 0.119\n",
      "Iteration: 456 \t--- Loss: 0.119\n",
      "Iteration: 457 \t--- Loss: 0.124\n",
      "Iteration: 458 \t--- Loss: 0.123\n",
      "Iteration: 459 \t--- Loss: 0.122\n",
      "Iteration: 460 \t--- Loss: 0.119\n",
      "Iteration: 461 \t--- Loss: 0.120\n",
      "Iteration: 462 \t--- Loss: 0.113\n",
      "Iteration: 463 \t--- Loss: 0.127\n",
      "Iteration: 464 \t--- Loss: 0.131\n",
      "Iteration: 465 \t--- Loss: 0.126\n",
      "Iteration: 466 \t--- Loss: 0.125\n",
      "Iteration: 467 \t--- Loss: 0.127\n",
      "Iteration: 468 \t--- Loss: 0.126\n",
      "Iteration: 469 \t--- Loss: 0.114\n",
      "Iteration: 470 \t--- Loss: 0.121\n",
      "Iteration: 471 \t--- Loss: 0.120\n",
      "Iteration: 472 \t--- Loss: 0.125\n",
      "Iteration: 473 \t--- Loss: 0.117\n",
      "Iteration: 474 \t--- Loss: 0.124\n",
      "Iteration: 475 \t--- Loss: 0.119\n",
      "Iteration: 476 \t--- Loss: 0.127\n",
      "Iteration: 477 \t--- Loss: 0.115\n",
      "Iteration: 478 \t--- Loss: 0.113\n",
      "Iteration: 479 \t--- Loss: 0.112\n",
      "Iteration: 480 \t--- Loss: 0.122\n",
      "Iteration: 481 \t--- Loss: 0.122\n",
      "Iteration: 482 \t--- Loss: 0.128\n",
      "Iteration: 483 \t--- Loss: 0.123\n",
      "Iteration: 484 \t--- Loss: 0.118\n",
      "Iteration: 485 \t--- Loss: 0.116\n",
      "Iteration: 486 \t--- Loss: 0.120\n",
      "Iteration: 487 \t--- Loss: 0.123\n",
      "Iteration: 488 \t--- Loss: 0.123\n",
      "Iteration: 489 \t--- Loss: 0.122\n",
      "Iteration: 490 \t--- Loss: 0.126\n",
      "Iteration: 491 \t--- Loss: 0.111\n",
      "Iteration: 492 \t--- Loss: 0.124\n",
      "Iteration: 493 \t--- Loss: 0.123\n",
      "Iteration: 494 \t--- Loss: 0.123\n",
      "Iteration: 495 \t--- Loss: 0.111\n",
      "Iteration: 496 \t--- Loss: 0.126\n",
      "Iteration: 497 \t--- Loss: 0.123\n",
      "Iteration: 498 \t--- Loss: 0.125\n",
      "Iteration: 499 \t--- Loss: 0.123\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [06:25<00:00, 385.82s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.042\n",
      "Iteration: 261 \t--- Loss: 0.061\n",
      "Iteration: 262 \t--- Loss: 0.076\n",
      "Iteration: 263 \t--- Loss: 0.043\n",
      "Iteration: 264 \t--- Loss: 0.040\n",
      "Iteration: 265 \t--- Loss: 0.047\n",
      "Iteration: 266 \t--- Loss: 0.039\n",
      "Iteration: 267 \t--- Loss: 0.046\n",
      "Iteration: 268 \t--- Loss: 0.042\n",
      "Iteration: 269 \t--- Loss: 0.046\n",
      "Iteration: 270 \t--- Loss: 0.039\n",
      "Iteration: 271 \t--- Loss: 0.040\n",
      "Iteration: 272 \t--- Loss: 0.043\n",
      "Iteration: 273 \t--- Loss: 0.049\n",
      "Iteration: 274 \t--- Loss: 0.045\n",
      "Iteration: 275 \t--- Loss: 0.044\n",
      "Iteration: 276 \t--- Loss: 0.051\n",
      "Iteration: 277 \t--- Loss: 0.065\n",
      "Iteration: 278 \t--- Loss: 0.038\n",
      "Iteration: 279 \t--- Loss: 0.042\n",
      "Iteration: 280 \t--- Loss: 0.044\n",
      "Iteration: 281 \t--- Loss: 0.060\n",
      "Iteration: 282 \t--- Loss: 0.077\n",
      "Iteration: 283 \t--- Loss: 0.046\n",
      "Iteration: 284 \t--- Loss: 0.036\n",
      "Iteration: 285 \t--- Loss: 0.037\n",
      "Iteration: 286 \t--- Loss: 0.034\n",
      "Iteration: 287 \t--- Loss: 0.041\n",
      "Iteration: 288 \t--- Loss: 0.044\n",
      "Iteration: 289 \t--- Loss: 0.046\n",
      "Iteration: 290 \t--- Loss: 0.060\n",
      "Iteration: 291 \t--- Loss: 0.038\n",
      "Iteration: 292 \t--- Loss: 0.042\n",
      "Iteration: 293 \t--- Loss: 0.038\n",
      "Iteration: 294 \t--- Loss: 0.044\n",
      "Iteration: 295 \t--- Loss: 0.058\n",
      "Iteration: 296 \t--- Loss: 0.084\n",
      "Iteration: 297 \t--- Loss: 0.066\n",
      "Iteration: 298 \t--- Loss: 0.045\n",
      "Iteration: 299 \t--- Loss: 0.033\n",
      "Iteration: 300 \t--- Loss: 0.033\n",
      "Iteration: 301 \t--- Loss: 0.039\n",
      "Iteration: 302 \t--- Loss: 0.036\n",
      "Iteration: 303 \t--- Loss: 0.043\n",
      "Iteration: 304 \t--- Loss: 0.039\n",
      "Iteration: 305 \t--- Loss: 0.038\n",
      "Iteration: 306 \t--- Loss: 0.036\n",
      "Iteration: 307 \t--- Loss: 0.038\n",
      "Iteration: 308 \t--- Loss: 0.035\n",
      "Iteration: 309 \t--- Loss: 0.049\n",
      "Iteration: 310 \t--- Loss: 0.063\n",
      "Iteration: 311 \t--- Loss: 0.040\n",
      "Iteration: 312 \t--- Loss: 0.044\n",
      "Iteration: 313 \t--- Loss: 0.080\n",
      "Iteration: 314 \t--- Loss: 0.135\n",
      "Iteration: 315 \t--- Loss: 0.135\n",
      "Iteration: 316 \t--- Loss: 0.140\n",
      "Iteration: 317 \t--- Loss: 0.126\n",
      "Iteration: 318 \t--- Loss: 0.127\n",
      "Iteration: 319 \t--- Loss: 0.129\n",
      "Iteration: 320 \t--- Loss: 0.150\n",
      "Iteration: 321 \t--- Loss: 0.130\n",
      "Iteration: 322 \t--- Loss: 0.143\n",
      "Iteration: 323 \t--- Loss: 0.133\n",
      "Iteration: 324 \t--- Loss: 0.131\n",
      "Iteration: 325 \t--- Loss: 0.143\n",
      "Iteration: 326 \t--- Loss: 0.140\n",
      "Iteration: 327 \t--- Loss: 0.142\n",
      "Iteration: 328 \t--- Loss: 0.146\n",
      "Iteration: 329 \t--- Loss: 0.140\n",
      "Iteration: 330 \t--- Loss: 0.157\n",
      "Iteration: 331 \t--- Loss: 0.173\n",
      "Iteration: 332 \t--- Loss: 0.163\n",
      "Iteration: 333 \t--- Loss: 0.166\n",
      "Iteration: 334 \t--- Loss: 0.164\n",
      "Iteration: 335 \t--- Loss: 0.173\n",
      "Iteration: 336 \t--- Loss: 0.163\n",
      "Iteration: 337 \t--- Loss: 0.170\n",
      "Iteration: 338 \t--- Loss: 0.184\n",
      "Iteration: 339 \t--- Loss: 0.184\n",
      "Iteration: 340 \t--- Loss: 0.181\n",
      "Iteration: 341 \t--- Loss: 0.175\n",
      "Iteration: 342 \t--- Loss: 0.177\n",
      "Iteration: 343 \t--- Loss: 0.182\n",
      "Iteration: 344 \t--- Loss: 0.188\n",
      "Iteration: 345 \t--- Loss: 0.187\n",
      "Iteration: 346 \t--- Loss: 0.173\n",
      "Iteration: 347 \t--- Loss: 0.187\n",
      "Iteration: 348 \t--- Loss: 0.189\n",
      "Iteration: 349 \t--- Loss: 0.180\n",
      "Iteration: 350 \t--- Loss: 0.189\n",
      "Iteration: 351 \t--- Loss: 0.204\n",
      "Iteration: 352 \t--- Loss: 0.187\n",
      "Iteration: 353 \t--- Loss: 0.198\n",
      "Iteration: 354 \t--- Loss: 0.185\n",
      "Iteration: 355 \t--- Loss: 0.169\n",
      "Iteration: 356 \t--- Loss: 0.197\n",
      "Iteration: 357 \t--- Loss: 0.179\n",
      "Iteration: 358 \t--- Loss: 0.182\n",
      "Iteration: 359 \t--- Loss: 0.188\n",
      "Iteration: 360 \t--- Loss: 0.176\n",
      "Iteration: 361 \t--- Loss: 0.174\n",
      "Iteration: 362 \t--- Loss: 0.176\n",
      "Iteration: 363 \t--- Loss: 0.172\n",
      "Iteration: 364 \t--- Loss: 0.165\n",
      "Iteration: 365 \t--- Loss: 0.180\n",
      "Iteration: 366 \t--- Loss: 0.173\n",
      "Iteration: 367 \t--- Loss: 0.177\n",
      "Iteration: 368 \t--- Loss: 0.159\n",
      "Iteration: 369 \t--- Loss: 0.168\n",
      "Iteration: 370 \t--- Loss: 0.145\n",
      "Iteration: 371 \t--- Loss: 0.164\n",
      "Iteration: 372 \t--- Loss: 0.153\n",
      "Iteration: 373 \t--- Loss: 0.160\n",
      "Iteration: 374 \t--- Loss: 0.159\n",
      "Iteration: 375 \t--- Loss: 0.141\n",
      "Iteration: 376 \t--- Loss: 0.137\n",
      "Iteration: 377 \t--- Loss: 0.124\n",
      "Iteration: 378 \t--- Loss: 0.127\n",
      "Iteration: 379 \t--- Loss: 0.139\n",
      "Iteration: 380 \t--- Loss: 0.129\n",
      "Iteration: 381 \t--- Loss: 0.120\n",
      "Iteration: 382 \t--- Loss: 0.118\n",
      "Iteration: 383 \t--- Loss: 0.124\n",
      "Iteration: 384 \t--- Loss: 0.110\n",
      "Iteration: 385 \t--- Loss: 0.114\n",
      "Iteration: 386 \t--- Loss: 0.121\n",
      "Iteration: 387 \t--- Loss: 0.110\n",
      "Iteration: 388 \t--- Loss: 0.108\n",
      "Iteration: 389 \t--- Loss: 0.104\n",
      "Iteration: 390 \t--- Loss: 0.100\n",
      "Iteration: 391 \t--- Loss: 0.091\n",
      "Iteration: 392 \t--- Loss: 0.094\n",
      "Iteration: 393 \t--- Loss: 0.085\n",
      "Iteration: 394 \t--- Loss: 0.086\n",
      "Iteration: 395 \t--- Loss: 0.083\n",
      "Iteration: 396 \t--- Loss: 0.077\n",
      "Iteration: 397 \t--- Loss: 0.076\n",
      "Iteration: 398 \t--- Loss: 0.072\n",
      "Iteration: 399 \t--- Loss: 0.070\n",
      "Iteration: 400 \t--- Loss: 0.068\n",
      "Iteration: 401 \t--- Loss: 0.065\n",
      "Iteration: 402 \t--- Loss: 0.063\n",
      "Iteration: 403 \t--- Loss: 0.063\n",
      "Iteration: 404 \t--- Loss: 0.061\n",
      "Iteration: 405 \t--- Loss: 0.059\n",
      "Iteration: 406 \t--- Loss: 0.057\n",
      "Iteration: 407 \t--- Loss: 0.055\n",
      "Iteration: 408 \t--- Loss: 0.052\n",
      "Iteration: 409 \t--- Loss: 0.052\n",
      "Iteration: 410 \t--- Loss: 0.052\n",
      "Iteration: 411 \t--- Loss: 0.051\n",
      "Iteration: 412 \t--- Loss: 0.049\n",
      "Iteration: 413 \t--- Loss: 0.049\n",
      "Iteration: 414 \t--- Loss: 0.047\n",
      "Iteration: 415 \t--- Loss: 0.047\n",
      "Iteration: 416 \t--- Loss: 0.046\n",
      "Iteration: 417 \t--- Loss: 0.045\n",
      "Iteration: 418 \t--- Loss: 0.045\n",
      "Iteration: 419 \t--- Loss: 0.044\n",
      "Iteration: 420 \t--- Loss: 0.043\n",
      "Iteration: 421 \t--- Loss: 0.044\n",
      "Iteration: 422 \t--- Loss: 0.044\n",
      "Iteration: 423 \t--- Loss: 0.041\n",
      "Iteration: 424 \t--- Loss: 0.042\n",
      "Iteration: 425 \t--- Loss: 0.042\n",
      "Iteration: 426 \t--- Loss: 0.041\n",
      "Iteration: 427 \t--- Loss: 0.039\n",
      "Iteration: 428 \t--- Loss: 0.041\n",
      "Iteration: 429 \t--- Loss: 0.039\n",
      "Iteration: 430 \t--- Loss: 0.039\n",
      "Iteration: 431 \t--- Loss: 0.040\n",
      "Iteration: 432 \t--- Loss: 0.038\n",
      "Iteration: 433 \t--- Loss: 0.037\n",
      "Iteration: 434 \t--- Loss: 0.037\n",
      "Iteration: 435 \t--- Loss: 0.037\n",
      "Iteration: 436 \t--- Loss: 0.035\n",
      "Iteration: 437 \t--- Loss: 0.035\n",
      "Iteration: 438 \t--- Loss: 0.036\n",
      "Iteration: 439 \t--- Loss: 0.035\n",
      "Iteration: 440 \t--- Loss: 0.035\n",
      "Iteration: 441 \t--- Loss: 0.034\n",
      "Iteration: 442 \t--- Loss: 0.034\n",
      "Iteration: 443 \t--- Loss: 0.033\n",
      "Iteration: 444 \t--- Loss: 0.034\n",
      "Iteration: 445 \t--- Loss: 0.032\n",
      "Iteration: 446 \t--- Loss: 0.032\n",
      "Iteration: 447 \t--- Loss: 0.032\n",
      "Iteration: 448 \t--- Loss: 0.032\n",
      "Iteration: 449 \t--- Loss: 0.031\n",
      "Iteration: 450 \t--- Loss: 0.031\n",
      "Iteration: 451 \t--- Loss: 0.031\n",
      "Iteration: 452 \t--- Loss: 0.031\n",
      "Iteration: 453 \t--- Loss: 0.031\n",
      "Iteration: 454 \t--- Loss: 0.030\n",
      "Iteration: 455 \t--- Loss: 0.030\n",
      "Iteration: 456 \t--- Loss: 0.031\n",
      "Iteration: 457 \t--- Loss: 0.030\n",
      "Iteration: 458 \t--- Loss: 0.031\n",
      "Iteration: 459 \t--- Loss: 0.029\n",
      "Iteration: 460 \t--- Loss: 0.028\n",
      "Iteration: 461 \t--- Loss: 0.029\n",
      "Iteration: 462 \t--- Loss: 0.028\n",
      "Iteration: 463 \t--- Loss: 0.027\n",
      "Iteration: 464 \t--- Loss: 0.028\n",
      "Iteration: 465 \t--- Loss: 0.028\n",
      "Iteration: 466 \t--- Loss: 0.026\n",
      "Iteration: 467 \t--- Loss: 0.027\n",
      "Iteration: 468 \t--- Loss: 0.027\n",
      "Iteration: 469 \t--- Loss: 0.027\n",
      "Iteration: 470 \t--- Loss: 0.027\n",
      "Iteration: 471 \t--- Loss: 0.027\n",
      "Iteration: 472 \t--- Loss: 0.026\n",
      "Iteration: 473 \t--- Loss: 0.027\n",
      "Iteration: 474 \t--- Loss: 0.027\n",
      "Iteration: 475 \t--- Loss: 0.026\n",
      "Iteration: 476 \t--- Loss: 0.025\n",
      "Iteration: 477 \t--- Loss: 0.025\n",
      "Iteration: 478 \t--- Loss: 0.025\n",
      "Iteration: 479 \t--- Loss: 0.025\n",
      "Iteration: 480 \t--- Loss: 0.024\n",
      "Iteration: 481 \t--- Loss: 0.025\n",
      "Iteration: 482 \t--- Loss: 0.024\n",
      "Iteration: 483 \t--- Loss: 0.025\n",
      "Iteration: 484 \t--- Loss: 0.025\n",
      "Iteration: 485 \t--- Loss: 0.024\n",
      "Iteration: 486 \t--- Loss: 0.023\n",
      "Iteration: 487 \t--- Loss: 0.025\n",
      "Iteration: 488 \t--- Loss: 0.025\n",
      "Iteration: 489 \t--- Loss: 0.023\n",
      "Iteration: 490 \t--- Loss: 0.024\n",
      "Iteration: 491 \t--- Loss: 0.024\n",
      "Iteration: 492 \t--- Loss: 0.023\n",
      "Iteration: 493 \t--- Loss: 0.024\n",
      "Iteration: 494 \t--- Loss: 0.023\n",
      "Iteration: 495 \t--- Loss: 0.023\n",
      "Iteration: 496 \t--- Loss: 0.023\n",
      "Iteration: 497 \t--- Loss: 0.023\n",
      "Iteration: 498 \t--- Loss: 0.023\n",
      "Iteration: 499 \t--- Loss: 0.022\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:21<00:14,  3.58s/it][Parallel(n_jobs=5)]: Done  60 tasks      | elapsed: 37.4min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.53s/it][Parallel(n_jobs=5)]: Done  61 tasks      | elapsed: 37.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [03:13<00:00, 193.88s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.232\n",
      "Iteration: 261 \t--- Loss: 0.237\n",
      "Iteration: 262 \t--- Loss: 0.242\n",
      "Iteration: 263 \t--- Loss: 0.233\n",
      "Iteration: 264 \t--- Loss: 0.248\n",
      "Iteration: 265 \t--- Loss: 0.241\n",
      "Iteration: 266 \t--- Loss: 0.253\n",
      "Iteration: 267 \t--- Loss: 0.247\n",
      "Iteration: 268 \t--- Loss: 0.233\n",
      "Iteration: 269 \t--- Loss: 0.238\n",
      "Iteration: 270 \t--- Loss: 0.227\n",
      "Iteration: 271 \t--- Loss: 0.270\n",
      "Iteration: 272 \t--- Loss: 0.272\n",
      "Iteration: 273 \t--- Loss: 0.264\n",
      "Iteration: 274 \t--- Loss: 0.229\n",
      "Iteration: 275 \t--- Loss: 0.251\n",
      "Iteration: 276 \t--- Loss: 0.235\n",
      "Iteration: 277 \t--- Loss: 0.242\n",
      "Iteration: 278 \t--- Loss: 0.234\n",
      "Iteration: 279 \t--- Loss: 0.236\n",
      "Iteration: 280 \t--- Loss: 0.270\n",
      "Iteration: 281 \t--- Loss: 0.253\n",
      "Iteration: 282 \t--- Loss: 0.232\n",
      "Iteration: 283 \t--- Loss: 0.251\n",
      "Iteration: 284 \t--- Loss: 0.226\n",
      "Iteration: 285 \t--- Loss: 0.245\n",
      "Iteration: 286 \t--- Loss: 0.228\n",
      "Iteration: 287 \t--- Loss: 0.252\n",
      "Iteration: 288 \t--- Loss: 0.241\n",
      "Iteration: 289 \t--- Loss: 0.236\n",
      "Iteration: 290 \t--- Loss: 0.224\n",
      "Iteration: 291 \t--- Loss: 0.245\n",
      "Iteration: 292 \t--- Loss: 0.242\n",
      "Iteration: 293 \t--- Loss: 0.224\n",
      "Iteration: 294 \t--- Loss: 0.256\n",
      "Iteration: 295 \t--- Loss: 0.257\n",
      "Iteration: 296 \t--- Loss: 0.220\n",
      "Iteration: 297 \t--- Loss: 0.222\n",
      "Iteration: 298 \t--- Loss: 0.252\n",
      "Iteration: 299 \t--- Loss: 0.252\n",
      "Iteration: 300 \t--- Loss: 0.231\n",
      "Iteration: 301 \t--- Loss: 0.244\n",
      "Iteration: 302 \t--- Loss: 0.245\n",
      "Iteration: 303 \t--- Loss: 0.228\n",
      "Iteration: 304 \t--- Loss: 0.253\n",
      "Iteration: 305 \t--- Loss: 0.237\n",
      "Iteration: 306 \t--- Loss: 0.257\n",
      "Iteration: 307 \t--- Loss: 0.230\n",
      "Iteration: 308 \t--- Loss: 0.239\n",
      "Iteration: 309 \t--- Loss: 0.240\n",
      "Iteration: 310 \t--- Loss: 0.236\n",
      "Iteration: 311 \t--- Loss: 0.248\n",
      "Iteration: 312 \t--- Loss: 0.232\n",
      "Iteration: 313 \t--- Loss: 0.242\n",
      "Iteration: 314 \t--- Loss: 0.244\n",
      "Iteration: 315 \t--- Loss: 0.240\n",
      "Iteration: 316 \t--- Loss: 0.227\n",
      "Iteration: 317 \t--- Loss: 0.241\n",
      "Iteration: 318 \t--- Loss: 0.256\n",
      "Iteration: 319 \t--- Loss: 0.256\n",
      "Iteration: 320 \t--- Loss: 0.248\n",
      "Iteration: 321 \t--- Loss: 0.228\n",
      "Iteration: 322 \t--- Loss: 0.252\n",
      "Iteration: 323 \t--- Loss: 0.234\n",
      "Iteration: 324 \t--- Loss: 0.211\n",
      "Iteration: 325 \t--- Loss: 0.246\n",
      "Iteration: 326 \t--- Loss: 0.237\n",
      "Iteration: 327 \t--- Loss: 0.230\n",
      "Iteration: 328 \t--- Loss: 0.224\n",
      "Iteration: 329 \t--- Loss: 0.241\n",
      "Iteration: 330 \t--- Loss: 0.243\n",
      "Iteration: 331 \t--- Loss: 0.215\n",
      "Iteration: 332 \t--- Loss: 0.268\n",
      "Iteration: 333 \t--- Loss: 0.240\n",
      "Iteration: 334 \t--- Loss: 0.225\n",
      "Iteration: 335 \t--- Loss: 0.238\n",
      "Iteration: 336 \t--- Loss: 0.239\n",
      "Iteration: 337 \t--- Loss: 0.227\n",
      "Iteration: 338 \t--- Loss: 0.223\n",
      "Iteration: 339 \t--- Loss: 0.246\n",
      "Iteration: 340 \t--- Loss: 0.245\n",
      "Iteration: 341 \t--- Loss: 0.231\n",
      "Iteration: 342 \t--- Loss: 0.251\n",
      "Iteration: 343 \t--- Loss: 0.237\n",
      "Iteration: 344 \t--- Loss: 0.233\n",
      "Iteration: 345 \t--- Loss: 0.236\n",
      "Iteration: 346 \t--- Loss: 0.251\n",
      "Iteration: 347 \t--- Loss: 0.250\n",
      "Iteration: 348 \t--- Loss: 0.237\n",
      "Iteration: 349 \t--- Loss: 0.238\n",
      "Iteration: 350 \t--- Loss: 0.254\n",
      "Iteration: 351 \t--- Loss: 0.245\n",
      "Iteration: 352 \t--- Loss: 0.242\n",
      "Iteration: 353 \t--- Loss: 0.228\n",
      "Iteration: 354 \t--- Loss: 0.253\n",
      "Iteration: 355 \t--- Loss: 0.258\n",
      "Iteration: 356 \t--- Loss: 0.237\n",
      "Iteration: 357 \t--- Loss: 0.216\n",
      "Iteration: 358 \t--- Loss: 0.244\n",
      "Iteration: 359 \t--- Loss: 0.242\n",
      "Iteration: 360 \t--- Loss: 0.237\n",
      "Iteration: 361 \t--- Loss: 0.244\n",
      "Iteration: 362 \t--- Loss: 0.243\n",
      "Iteration: 363 \t--- Loss: 0.247\n",
      "Iteration: 364 \t--- Loss: 0.239\n",
      "Iteration: 365 \t--- Loss: 0.240\n",
      "Iteration: 366 \t--- Loss: 0.243\n",
      "Iteration: 367 \t--- Loss: 0.225\n",
      "Iteration: 368 \t--- Loss: 0.249\n",
      "Iteration: 369 \t--- Loss: 0.238\n",
      "Iteration: 370 \t--- Loss: 0.240\n",
      "Iteration: 371 \t--- Loss: 0.233\n",
      "Iteration: 372 \t--- Loss: 0.224\n",
      "Iteration: 373 \t--- Loss: 0.250\n",
      "Iteration: 374 \t--- Loss: 0.241\n",
      "Iteration: 375 \t--- Loss: 0.228\n",
      "Iteration: 376 \t--- Loss: 0.250\n",
      "Iteration: 377 \t--- Loss: 0.247\n",
      "Iteration: 378 \t--- Loss: 0.225\n",
      "Iteration: 379 \t--- Loss: 0.254\n",
      "Iteration: 380 \t--- Loss: 0.253\n",
      "Iteration: 381 \t--- Loss: 0.225\n",
      "Iteration: 382 \t--- Loss: 0.234\n",
      "Iteration: 383 \t--- Loss: 0.266\n",
      "Iteration: 384 \t--- Loss: 0.239\n",
      "Iteration: 385 \t--- Loss: 0.238\n",
      "Iteration: 386 \t--- Loss: 0.228\n",
      "Iteration: 387 \t--- Loss: 0.244\n",
      "Iteration: 388 \t--- Loss: 0.239\n",
      "Iteration: 389 \t--- Loss: 0.220\n",
      "Iteration: 390 \t--- Loss: 0.253\n",
      "Iteration: 391 \t--- Loss: 0.228\n",
      "Iteration: 392 \t--- Loss: 0.259\n",
      "Iteration: 393 \t--- Loss: 0.242\n",
      "Iteration: 394 \t--- Loss: 0.239\n",
      "Iteration: 395 \t--- Loss: 0.238\n",
      "Iteration: 396 \t--- Loss: 0.242\n",
      "Iteration: 397 \t--- Loss: 0.256\n",
      "Iteration: 398 \t--- Loss: 0.243\n",
      "Iteration: 399 \t--- Loss: 0.219\n",
      "Iteration: 400 \t--- Loss: 0.260\n",
      "Iteration: 401 \t--- Loss: 0.237\n",
      "Iteration: 402 \t--- Loss: 0.249\n",
      "Iteration: 403 \t--- Loss: 0.228\n",
      "Iteration: 404 \t--- Loss: 0.252\n",
      "Iteration: 405 \t--- Loss: 0.254\n",
      "Iteration: 406 \t--- Loss: 0.248\n",
      "Iteration: 407 \t--- Loss: 0.213\n",
      "Iteration: 408 \t--- Loss: 0.240\n",
      "Iteration: 409 \t--- Loss: 0.229\n",
      "Iteration: 410 \t--- Loss: 0.252\n",
      "Iteration: 411 \t--- Loss: 0.241\n",
      "Iteration: 412 \t--- Loss: 0.241\n",
      "Iteration: 413 \t--- Loss: 0.245\n",
      "Iteration: 414 \t--- Loss: 0.248\n",
      "Iteration: 415 \t--- Loss: 0.242\n",
      "Iteration: 416 \t--- Loss: 0.228\n",
      "Iteration: 417 \t--- Loss: 0.238\n",
      "Iteration: 418 \t--- Loss: 0.228\n",
      "Iteration: 419 \t--- Loss: 0.231\n",
      "Iteration: 420 \t--- Loss: 0.248\n",
      "Iteration: 421 \t--- Loss: 0.230\n",
      "Iteration: 422 \t--- Loss: 0.239\n",
      "Iteration: 423 \t--- Loss: 0.237\n",
      "Iteration: 424 \t--- Loss: 0.239\n",
      "Iteration: 425 \t--- Loss: 0.233\n",
      "Iteration: 426 \t--- Loss: 0.227\n",
      "Iteration: 427 \t--- Loss: 0.244\n",
      "Iteration: 428 \t--- Loss: 0.227\n",
      "Iteration: 429 \t--- Loss: 0.239\n",
      "Iteration: 430 \t--- Loss: 0.227\n",
      "Iteration: 431 \t--- Loss: 0.226\n",
      "Iteration: 432 \t--- Loss: 0.227\n",
      "Iteration: 433 \t--- Loss: 0.257\n",
      "Iteration: 434 \t--- Loss: 0.236\n",
      "Iteration: 435 \t--- Loss: 0.257\n",
      "Iteration: 436 \t--- Loss: 0.235\n",
      "Iteration: 437 \t--- Loss: 0.254\n",
      "Iteration: 438 \t--- Loss: 0.241\n",
      "Iteration: 439 \t--- Loss: 0.238\n",
      "Iteration: 440 \t--- Loss: 0.238\n",
      "Iteration: 441 \t--- Loss: 0.244\n",
      "Iteration: 442 \t--- Loss: 0.240\n",
      "Iteration: 443 \t--- Loss: 0.250\n",
      "Iteration: 444 \t--- Loss: 0.233\n",
      "Iteration: 445 \t--- Loss: 0.247\n",
      "Iteration: 446 \t--- Loss: 0.259\n",
      "Iteration: 447 \t--- Loss: 0.238\n",
      "Iteration: 448 \t--- Loss: 0.233\n",
      "Iteration: 449 \t--- Loss: 0.232\n",
      "Iteration: 450 \t--- Loss: 0.236\n",
      "Iteration: 451 \t--- Loss: 0.238\n",
      "Iteration: 452 \t--- Loss: 0.243\n",
      "Iteration: 453 \t--- Loss: 0.255\n",
      "Iteration: 454 \t--- Loss: 0.249\n",
      "Iteration: 455 \t--- Loss: 0.239\n",
      "Iteration: 456 \t--- Loss: 0.231\n",
      "Iteration: 457 \t--- Loss: 0.245\n",
      "Iteration: 458 \t--- Loss: 0.251\n",
      "Iteration: 459 \t--- Loss: 0.240\n",
      "Iteration: 460 \t--- Loss: 0.224\n",
      "Iteration: 461 \t--- Loss: 0.242\n",
      "Iteration: 462 \t--- Loss: 0.250\n",
      "Iteration: 463 \t--- Loss: 0.231\n",
      "Iteration: 464 \t--- Loss: 0.211\n",
      "Iteration: 465 \t--- Loss: 0.243\n",
      "Iteration: 466 \t--- Loss: 0.248\n",
      "Iteration: 467 \t--- Loss: 0.229\n",
      "Iteration: 468 \t--- Loss: 0.229\n",
      "Iteration: 469 \t--- Loss: 0.230\n",
      "Iteration: 470 \t--- Loss: 0.249\n",
      "Iteration: 471 \t--- Loss: 0.235\n",
      "Iteration: 472 \t--- Loss: 0.264\n",
      "Iteration: 473 \t--- Loss: 0.240\n",
      "Iteration: 474 \t--- Loss: 0.239\n",
      "Iteration: 475 \t--- Loss: 0.252\n",
      "Iteration: 476 \t--- Loss: 0.247\n",
      "Iteration: 477 \t--- Loss: 0.231\n",
      "Iteration: 478 \t--- Loss: 0.260\n",
      "Iteration: 479 \t--- Loss: 0.241\n",
      "Iteration: 480 \t--- Loss: 0.244\n",
      "Iteration: 481 \t--- Loss: 0.249\n",
      "Iteration: 482 \t--- Loss: 0.223\n",
      "Iteration: 483 \t--- Loss: 0.244\n",
      "Iteration: 484 \t--- Loss: 0.259\n",
      "Iteration: 485 \t--- Loss: 0.236\n",
      "Iteration: 486 \t--- Loss: 0.248\n",
      "Iteration: 487 \t--- Loss: 0.221\n",
      "Iteration: 488 \t--- Loss: 0.250\n",
      "Iteration: 489 \t--- Loss: 0.229\n",
      "Iteration: 490 \t--- Loss: 0.227\n",
      "Iteration: 491 \t--- Loss: 0.237\n",
      "Iteration: 492 \t--- Loss: 0.235\n",
      "Iteration: 493 \t--- Loss: 0.239\n",
      "Iteration: 494 \t--- Loss: 0.225\n",
      "Iteration: 495 \t--- Loss: 0.244\n",
      "Iteration: 496 \t--- Loss: 0.260\n",
      "Iteration: 497 \t--- Loss: 0.253\n",
      "Iteration: 498 \t--- Loss: 0.229\n",
      "Iteration: 499 \t--- Loss: 0.246\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.56s/it][Parallel(n_jobs=5)]: Done  62 tasks      | elapsed: 38.2min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.309\n",
      "Iteration: 1 \t--- Loss: 0.271\n",
      "Iteration: 2 \t--- Loss: 0.280\n",
      "Iteration: 3 \t--- Loss: 0.248\n",
      "Iteration: 4 \t--- Loss: 0.277\n",
      "Iteration: 5 \t--- Loss: 0.256\n",
      "Iteration: 6 \t--- Loss: 0.253\n",
      "Iteration: 7 \t--- Loss: 0.246\n",
      "Iteration: 8 \t--- Loss: 0.262\n",
      "Iteration: 9 \t--- Loss: 0.237\n",
      "Iteration: 10 \t--- Loss: 0.225\n",
      "Iteration: 11 \t--- Loss: 0.249\n",
      "Iteration: 12 \t--- Loss: 0.229\n",
      "Iteration: 13 \t--- Loss: 0.223\n",
      "Iteration: 14 \t--- Loss: 0.223\n",
      "Iteration: 15 \t--- Loss: 0.225\n",
      "Iteration: 16 \t--- Loss: 0.209\n",
      "Iteration: 17 \t--- Loss: 0.210\n",
      "Iteration: 18 \t--- Loss: 0.208\n",
      "Iteration: 19 \t--- Loss: 0.195\n",
      "Iteration: 20 \t--- Loss: 0.206\n",
      "Iteration: 21 \t--- Loss: 0.214\n",
      "Iteration: 22 \t--- Loss: 0.185\n",
      "Iteration: 23 \t--- Loss: 0.204\n",
      "Iteration: 24 \t--- Loss: 0.183\n",
      "Iteration: 25 \t--- Loss: 0.195\n",
      "Iteration: 26 \t--- Loss: 0.195\n",
      "Iteration: 27 \t--- Loss: 0.166\n",
      "Iteration: 28 \t--- Loss: 0.146\n",
      "Iteration: 29 \t--- Loss: 0.150\n",
      "Iteration: 30 \t--- Loss: 0.158\n",
      "Iteration: 31 \t--- Loss: 0.153\n",
      "Iteration: 32 \t--- Loss: 0.170\n",
      "Iteration: 33 \t--- Loss: 0.143\n",
      "Iteration: 34 \t--- Loss: 0.157\n",
      "Iteration: 35 \t--- Loss: 0.161\n",
      "Iteration: 36 \t--- Loss: 0.187\n",
      "Iteration: 37 \t--- Loss: 0.213\n",
      "Iteration: 38 \t--- Loss: 0.215\n",
      "Iteration: 39 \t--- Loss: 0.219\n",
      "Iteration: 40 \t--- Loss: 0.195\n",
      "Iteration: 41 \t--- Loss: 0.143\n",
      "Iteration: 42 \t--- Loss: 0.152\n",
      "Iteration: 43 \t--- Loss: 0.166\n",
      "Iteration: 44 \t--- Loss: 0.131\n",
      "Iteration: 45 \t--- Loss: 0.130\n",
      "Iteration: 46 \t--- Loss: 0.272\n",
      "Iteration: 47 \t--- Loss: 0.405\n",
      "Iteration: 48 \t--- Loss: 0.386\n",
      "Iteration: 49 \t--- Loss: 0.373\n",
      "Iteration: 50 \t--- Loss: 0.380\n",
      "Iteration: 51 \t--- Loss: 0.361\n",
      "Iteration: 52 \t--- Loss: 0.362\n",
      "Iteration: 53 \t--- Loss: 0.339\n",
      "Iteration: 54 \t--- Loss: 0.328\n",
      "Iteration: 55 \t--- Loss: 0.320\n",
      "Iteration: 56 \t--- Loss: 0.308\n",
      "Iteration: 57 \t--- Loss: 0.315\n",
      "Iteration: 58 \t--- Loss: 0.311\n",
      "Iteration: 59 \t--- Loss: 0.262\n",
      "Iteration: 60 \t--- Loss: 0.267\n",
      "Iteration: 61 \t--- Loss: 0.255\n",
      "Iteration: 62 \t--- Loss: 0.238\n",
      "Iteration: 63 \t--- Loss: 0.229\n",
      "Iteration: 64 \t--- Loss: 0.240\n",
      "Iteration: 65 \t--- Loss: 0.225\n",
      "Iteration: 66 \t--- Loss: 0.205\n",
      "Iteration: 67 \t--- Loss: 0.213\n",
      "Iteration: 68 \t--- Loss: 0.217\n",
      "Iteration: 69 \t--- Loss: 0.212\n",
      "Iteration: 70 \t--- Loss: 0.212\n",
      "Iteration: 71 \t--- Loss: 0.185\n",
      "Iteration: 72 \t--- Loss: 0.182\n",
      "Iteration: 73 \t--- Loss: 0.197\n",
      "Iteration: 74 \t--- Loss: 0.173\n",
      "Iteration: 75 \t--- Loss: 0.193\n",
      "Iteration: 76 \t--- Loss: 0.160\n",
      "Iteration: 77 \t--- Loss: 0.157\n",
      "Iteration: 78 \t--- Loss: 0.173\n",
      "Iteration: 79 \t--- Loss: 0.161\n",
      "Iteration: 80 \t--- Loss: 0.164\n",
      "Iteration: 81 \t--- Loss: 0.165\n",
      "Iteration: 82 \t--- Loss: 0.146\n",
      "Iteration: 83 \t--- Loss: 0.154\n",
      "Iteration: 84 \t--- Loss: 0.151\n",
      "Iteration: 85 \t--- Loss: 0.135\n",
      "Iteration: 86 \t--- Loss: 0.124\n",
      "Iteration: 87 \t--- Loss: 0.134\n",
      "Iteration: 88 \t--- Loss: 0.137\n",
      "Iteration: 89 \t--- Loss: 0.125\n",
      "Iteration: 90 \t--- Loss: 0.121\n",
      "Iteration: 91 \t--- Loss: 0.106\n",
      "Iteration: 92 \t--- Loss: 0.102\n",
      "Iteration: 93 \t--- Loss: 0.105\n",
      "Iteration: 94 \t--- Loss: 0.107\n",
      "Iteration: 95 \t--- Loss: 0.099\n",
      "Iteration: 96 \t--- Loss: 0.089\n",
      "Iteration: 97 \t--- Loss: 0.080\n",
      "Iteration: 98 \t--- Loss: 0.079\n",
      "Iteration: 99 \t--- Loss: 0.065\n",
      "Iteration: 100 \t--- Loss: 0.077\n",
      "Iteration: 101 \t--- Loss: 0.066\n",
      "Iteration: 102 \t--- Loss: 0.068\n",
      "Iteration: 103 \t--- Loss: 0.066\n",
      "Iteration: 104 \t--- Loss: 0.067\n",
      "Iteration: 105 \t--- Loss: 0.068\n",
      "Iteration: 106 \t--- Loss: 0.060\n",
      "Iteration: 107 \t--- Loss: 0.056\n",
      "Iteration: 108 \t--- Loss: 0.055\n",
      "Iteration: 109 \t--- Loss: 0.064\n",
      "Iteration: 110 \t--- Loss: 0.058\n",
      "Iteration: 111 \t--- Loss: 0.061\n",
      "Iteration: 112 \t--- Loss: 0.066\n",
      "Iteration: 113 \t--- Loss: 0.064\n",
      "Iteration: 114 \t--- Loss: 0.056\n",
      "Iteration: 115 \t--- Loss: 0.049\n",
      "Iteration: 116 \t--- Loss: 0.059\n",
      "Iteration: 117 \t--- Loss: 0.055\n",
      "Iteration: 118 \t--- Loss: 0.061\n",
      "Iteration: 119 \t--- Loss: 0.059\n",
      "Iteration: 120 \t--- Loss: 0.048\n",
      "Iteration: 121 \t--- Loss: 0.049\n",
      "Iteration: 122 \t--- Loss: 0.054\n",
      "Iteration: 123 \t--- Loss: 0.046\n",
      "Iteration: 124 \t--- Loss: 0.055\n",
      "Iteration: 125 \t--- Loss: 0.055\n",
      "Iteration: 126 \t--- Loss: 0.052\n",
      "Iteration: 127 \t--- Loss: 0.049\n",
      "Iteration: 128 \t--- Loss: 0.047\n",
      "Iteration: 129 \t--- Loss: 0.054\n",
      "Iteration: 130 \t--- Loss: 0.050\n",
      "Iteration: 131 \t--- Loss: 0.046\n",
      "Iteration: 132 \t--- Loss: 0.049\n",
      "Iteration: 133 \t--- Loss: 0.044\n",
      "Iteration: 134 \t--- Loss: 0.048\n",
      "Iteration: 135 \t--- Loss: 0.046\n",
      "Iteration: 136 \t--- Loss: 0.043\n",
      "Iteration: 137 \t--- Loss: 0.048\n",
      "Iteration: 138 \t--- Loss: 0.039\n",
      "Iteration: 139 \t--- Loss: 0.049\n",
      "Iteration: 140 \t--- Loss: 0.047\n",
      "Iteration: 141 \t--- Loss: 0.047\n",
      "Iteration: 142 \t--- Loss: 0.041\n",
      "Iteration: 143 \t--- Loss: 0.044\n",
      "Iteration: 144 \t--- Loss: 0.044\n",
      "Iteration: 145 \t--- Loss: 0.042\n",
      "Iteration: 146 \t--- Loss: 0.041\n",
      "Iteration: 147 \t--- Loss: 0.034\n",
      "Iteration: 148 \t--- Loss: 0.040\n",
      "Iteration: 149 \t--- Loss: 0.047\n",
      "Iteration: 150 \t--- Loss: 0.031\n",
      "Iteration: 151 \t--- Loss: 0.042\n",
      "Iteration: 152 \t--- Loss: 0.038\n",
      "Iteration: 153 \t--- Loss: 0.037\n",
      "Iteration: 154 \t--- Loss: 0.035\n",
      "Iteration: 155 \t--- Loss: 0.034\n",
      "Iteration: 156 \t--- Loss: 0.037\n",
      "Iteration: 157 \t--- Loss: 0.037\n",
      "Iteration: 158 \t--- Loss: 0.030\n",
      "Iteration: 159 \t--- Loss: 0.032\n",
      "Iteration: 160 \t--- Loss: 0.035\n",
      "Iteration: 161 \t--- Loss: 0.032\n",
      "Iteration: 162 \t--- Loss: 0.034\n",
      "Iteration: 163 \t--- Loss: 0.034\n",
      "Iteration: 164 \t--- Loss: 0.032\n",
      "Iteration: 165 \t--- Loss: 0.033\n",
      "Iteration: 166 \t--- Loss: 0.032\n",
      "Iteration: 167 \t--- Loss: 0.033\n",
      "Iteration: 168 \t--- Loss: 0.028\n",
      "Iteration: 169 \t--- Loss: 0.027\n",
      "Iteration: 170 \t--- Loss: 0.030\n",
      "Iteration: 171 \t--- Loss: 0.032\n",
      "Iteration: 172 \t--- Loss: 0.031\n",
      "Iteration: 173 \t--- Loss: 0.029\n",
      "Iteration: 174 \t--- Loss: 0.028\n",
      "Iteration: 175 \t--- Loss: 0.029\n",
      "Iteration: 176 \t--- Loss: 0.025\n",
      "Iteration: 177 \t--- Loss: 0.027\n",
      "Iteration: 178 \t--- Loss: 0.025\n",
      "Iteration: 179 \t--- Loss: 0.026\n",
      "Iteration: 180 \t--- Loss: 0.028\n",
      "Iteration: 181 \t--- Loss: 0.027\n",
      "Iteration: 182 \t--- Loss: 0.029\n",
      "Iteration: 183 \t--- Loss: 0.025\n",
      "Iteration: 184 \t--- Loss: 0.026\n",
      "Iteration: 185 \t--- Loss: 0.025\n",
      "Iteration: 186 \t--- Loss: 0.026\n",
      "Iteration: 187 \t--- Loss: 0.027\n",
      "Iteration: 188 \t--- Loss: 0.024\n",
      "Iteration: 189 \t--- Loss: 0.020\n",
      "Iteration: 190 \t--- Loss: 0.020\n",
      "Iteration: 191 \t--- Loss: 0.024\n",
      "Iteration: 192 \t--- Loss: 0.025\n",
      "Iteration: 193 \t--- Loss: 0.020\n",
      "Iteration: 194 \t--- Loss: 0.020\n",
      "Iteration: 195 \t--- Loss: 0.019\n",
      "Iteration: 196 \t--- Loss: 0.021\n",
      "Iteration: 197 \t--- Loss: 0.020\n",
      "Iteration: 198 \t--- Loss: 0.021\n",
      "Iteration: 199 \t--- Loss: 0.023\n",
      "Iteration: 200 \t--- Loss: 0.022\n",
      "Iteration: 201 \t--- Loss: 0.020\n",
      "Iteration: 202 \t--- Loss: 0.020\n",
      "Iteration: 203 \t--- Loss: 0.021\n",
      "Iteration: 204 \t--- Loss: 0.022\n",
      "Iteration: 205 \t--- Loss: 0.016\n",
      "Iteration: 206 \t--- Loss: 0.018\n",
      "Iteration: 207 \t--- Loss: 0.017\n",
      "Iteration: 208 \t--- Loss: 0.019\n",
      "Iteration: 209 \t--- Loss: 0.015\n",
      "Iteration: 210 \t--- Loss: 0.018\n",
      "Iteration: 211 \t--- Loss: 0.017\n",
      "Iteration: 212 \t--- Loss: 0.018\n",
      "Iteration: 213 \t--- Loss: 0.015\n",
      "Iteration: 214 \t--- Loss: 0.016\n",
      "Iteration: 215 \t--- Loss: 0.018\n",
      "Iteration: 216 \t--- Loss: 0.016\n",
      "Iteration: 217 \t--- Loss: 0.018\n",
      "Iteration: 218 \t--- Loss: 0.017\n",
      "Iteration: 219 \t--- Loss: 0.016\n",
      "Iteration: 220 \t--- Loss: 0.015\n",
      "Iteration: 221 \t--- Loss: 0.014\n",
      "Iteration: 222 \t--- Loss: 0.018\n",
      "Iteration: 223 \t--- Loss: 0.015\n",
      "Iteration: 224 \t--- Loss: 0.015\n",
      "Iteration: 225 \t--- Loss: 0.015\n",
      "Iteration: 226 \t--- Loss: 0.014\n",
      "Iteration: 227 \t--- Loss: 0.012\n",
      "Iteration: 228 \t--- Loss: 0.015\n",
      "Iteration: 229 \t--- Loss: 0.013\n",
      "Iteration: 230 \t--- Loss: 0.015\n",
      "Iteration: 231 \t--- Loss: 0.013\n",
      "Iteration: 232 \t--- Loss: 0.012\n",
      "Iteration: 233 \t--- Loss: 0.013\n",
      "Iteration: 234 \t--- Loss: 0.012\n",
      "Iteration: 235 \t--- Loss: 0.012\n",
      "Iteration: 236 \t--- Loss: 0.012\n",
      "Iteration: 237 \t--- Loss: 0.011\n",
      "Iteration: 238 \t--- Loss: 0.010\n",
      "Iteration: 239 \t--- Loss: 0.010\n",
      "Iteration: 240 \t--- Loss: 0.011\n",
      "Iteration: 241 \t--- Loss: 0.011\n",
      "Iteration: 242 \t--- Loss: 0.011\n",
      "Iteration: 243 \t--- Loss: 0.013\n",
      "Iteration: 244 \t--- Loss: 0.011\n",
      "Iteration: 245 \t--- Loss: 0.009\n",
      "Iteration: 246 \t--- Loss: 0.010\n",
      "Iteration: 247 \t--- Loss: 0.012\n",
      "Iteration: 248 \t--- Loss: 0.011\n",
      "Iteration: 249 \t--- Loss: 0.010\n",
      "Iteration: 250 \t--- Loss: 0.011\n",
      "Iteration: 251 \t--- Loss: 0.010\n",
      "Iteration: 252 \t--- Loss: 0.011\n",
      "Iteration: 253 \t--- Loss: 0.009\n",
      "Iteration: 254 \t--- Loss: 0.009\n",
      "Iteration: 255 \t--- Loss: 0.010\n",
      "Iteration: 256 \t--- Loss: 0.009\n",
      "Iteration: 257 \t--- Loss: 0.008\n",
      "Iteration: 258 \t--- Loss: 0.009\n",
      "Iteration: 259 \t--- Loss: 0.008Iteration: 0 \t--- Loss: 0.738\n",
      "Iteration: 1 \t--- Loss: 0.681\n",
      "Iteration: 2 \t--- Loss: 0.630\n",
      "Iteration: 3 \t--- Loss: 0.586\n",
      "Iteration: 4 \t--- Loss: 0.535\n",
      "Iteration: 5 \t--- Loss: 0.512\n",
      "Iteration: 6 \t--- Loss: 0.482\n",
      "Iteration: 7 \t--- Loss: 0.468\n",
      "Iteration: 8 \t--- Loss: 0.448\n",
      "Iteration: 9 \t--- Loss: 0.422\n",
      "Iteration: 10 \t--- Loss: 0.411\n",
      "Iteration: 11 \t--- Loss: 0.402\n",
      "Iteration: 12 \t--- Loss: 0.386\n",
      "Iteration: 13 \t--- Loss: 0.380\n",
      "Iteration: 14 \t--- Loss: 0.405\n",
      "Iteration: 15 \t--- Loss: 0.376\n",
      "Iteration: 16 \t--- Loss: 0.370\n",
      "Iteration: 17 \t--- Loss: 0.377\n",
      "Iteration: 18 \t--- Loss: 0.370\n",
      "Iteration: 19 \t--- Loss: 0.391\n",
      "Iteration: 20 \t--- Loss: 0.373\n",
      "Iteration: 21 \t--- Loss: 0.370\n",
      "Iteration: 22 \t--- Loss: 0.375\n",
      "Iteration: 23 \t--- Loss: 0.371\n",
      "Iteration: 24 \t--- Loss: 0.359\n",
      "Iteration: 25 \t--- Loss: 0.376\n",
      "Iteration: 26 \t--- Loss: 0.365\n",
      "Iteration: 27 \t--- Loss: 0.364\n",
      "Iteration: 28 \t--- Loss: 0.355\n",
      "Iteration: 29 \t--- Loss: 0.360\n",
      "Iteration: 30 \t--- Loss: 0.356\n",
      "Iteration: 31 \t--- Loss: 0.348\n",
      "Iteration: 32 \t--- Loss: 0.352\n",
      "Iteration: 33 \t--- Loss: 0.362\n",
      "Iteration: 34 \t--- Loss: 0.344\n",
      "Iteration: 35 \t--- Loss: 0.353\n",
      "Iteration: 36 \t--- Loss: 0.355\n",
      "Iteration: 37 \t--- Loss: 0.365\n",
      "Iteration: 38 \t--- Loss: 0.358\n",
      "Iteration: 39 \t--- Loss: 0.358\n",
      "Iteration: 40 \t--- Loss: 0.354\n",
      "Iteration: 41 \t--- Loss: 0.334\n",
      "Iteration: 42 \t--- Loss: 0.371\n",
      "Iteration: 43 \t--- Loss: 0.341\n",
      "Iteration: 44 \t--- Loss: 0.342\n",
      "Iteration: 45 \t--- Loss: 0.343\n",
      "Iteration: 46 \t--- Loss: 0.353\n",
      "Iteration: 47 \t--- Loss: 0.349\n",
      "Iteration: 48 \t--- Loss: 0.342\n",
      "Iteration: 49 \t--- Loss: 0.347\n",
      "Iteration: 50 \t--- Loss: 0.357\n",
      "Iteration: 51 \t--- Loss: 0.353\n",
      "Iteration: 52 \t--- Loss: 0.350\n",
      "Iteration: 53 \t--- Loss: 0.362\n",
      "Iteration: 54 \t--- Loss: 0.358\n",
      "Iteration: 55 \t--- Loss: 0.337\n",
      "Iteration: 56 \t--- Loss: 0.356\n",
      "Iteration: 57 \t--- Loss: 0.344\n",
      "Iteration: 58 \t--- Loss: 0.356\n",
      "Iteration: 59 \t--- Loss: 0.342\n",
      "Iteration: 60 \t--- Loss: 0.353\n",
      "Iteration: 61 \t--- Loss: 0.356\n",
      "Iteration: 62 \t--- Loss: 0.346\n",
      "Iteration: 63 \t--- Loss: 0.333\n",
      "Iteration: 64 \t--- Loss: 0.343\n",
      "Iteration: 65 \t--- Loss: 0.355\n",
      "Iteration: 66 \t--- Loss: 0.346\n",
      "Iteration: 67 \t--- Loss: 0.358\n",
      "Iteration: 68 \t--- Loss: 0.347\n",
      "Iteration: 69 \t--- Loss: 0.345\n",
      "Iteration: 70 \t--- Loss: 0.360\n",
      "Iteration: 71 \t--- Loss: 0.351\n",
      "Iteration: 72 \t--- Loss: 0.348\n",
      "Iteration: 73 \t--- Loss: 0.353\n",
      "Iteration: 74 \t--- Loss: 0.367\n",
      "Iteration: 75 \t--- Loss: 0.350\n",
      "Iteration: 76 \t--- Loss: 0.360\n",
      "Iteration: 77 \t--- Loss: 0.348\n",
      "Iteration: 78 \t--- Loss: 0.351\n",
      "Iteration: 79 \t--- Loss: 0.340\n",
      "Iteration: 80 \t--- Loss: 0.352\n",
      "Iteration: 81 \t--- Loss: 0.355\n",
      "Iteration: 82 \t--- Loss: 0.340\n",
      "Iteration: 83 \t--- Loss: 0.356\n",
      "Iteration: 84 \t--- Loss: 0.337\n",
      "Iteration: 85 \t--- Loss: 0.351\n",
      "Iteration: 86 \t--- Loss: 0.344\n",
      "Iteration: 87 \t--- Loss: 0.356\n",
      "Iteration: 88 \t--- Loss: 0.349\n",
      "Iteration: 89 \t--- Loss: 0.348\n",
      "Iteration: 90 \t--- Loss: 0.347\n",
      "Iteration: 91 \t--- Loss: 0.345\n",
      "Iteration: 92 \t--- Loss: 0.358\n",
      "Iteration: 93 \t--- Loss: 0.351\n",
      "Iteration: 94 \t--- Loss: 0.359\n",
      "Iteration: 95 \t--- Loss: 0.346\n",
      "Iteration: 96 \t--- Loss: 0.335\n",
      "Iteration: 97 \t--- Loss: 0.349\n",
      "Iteration: 98 \t--- Loss: 0.353\n",
      "Iteration: 99 \t--- Loss: 0.341\n",
      "Iteration: 100 \t--- Loss: 0.355\n",
      "Iteration: 101 \t--- Loss: 0.335\n",
      "Iteration: 102 \t--- Loss: 0.347\n",
      "Iteration: 103 \t--- Loss: 0.341\n",
      "Iteration: 104 \t--- Loss: 0.337\n",
      "Iteration: 105 \t--- Loss: 0.345\n",
      "Iteration: 106 \t--- Loss: 0.354\n",
      "Iteration: 107 \t--- Loss: 0.355\n",
      "Iteration: 108 \t--- Loss: 0.353\n",
      "Iteration: 109 \t--- Loss: 0.345\n",
      "Iteration: 110 \t--- Loss: 0.351\n",
      "Iteration: 111 \t--- Loss: 0.333\n",
      "Iteration: 112 \t--- Loss: 0.351\n",
      "Iteration: 113 \t--- Loss: 0.339\n",
      "Iteration: 114 \t--- Loss: 0.350\n",
      "Iteration: 115 \t--- Loss: 0.344\n",
      "Iteration: 116 \t--- Loss: 0.341\n",
      "Iteration: 117 \t--- Loss: 0.354\n",
      "Iteration: 118 \t--- Loss: 0.353\n",
      "Iteration: 119 \t--- Loss: 0.348\n",
      "Iteration: 120 \t--- Loss: 0.352\n",
      "Iteration: 121 \t--- Loss: 0.352\n",
      "Iteration: 122 \t--- Loss: 0.345\n",
      "Iteration: 123 \t--- Loss: 0.350\n",
      "Iteration: 124 \t--- Loss: 0.338\n",
      "Iteration: 125 \t--- Loss: 0.348\n",
      "Iteration: 126 \t--- Loss: 0.356\n",
      "Iteration: 127 \t--- Loss: 0.346\n",
      "Iteration: 128 \t--- Loss: 0.363\n",
      "Iteration: 129 \t--- Loss: 0.360\n",
      "Iteration: 130 \t--- Loss: 0.344\n",
      "Iteration: 131 \t--- Loss: 0.360\n",
      "Iteration: 132 \t--- Loss: 0.340\n",
      "Iteration: 133 \t--- Loss: 0.355\n",
      "Iteration: 134 \t--- Loss: 0.346\n",
      "Iteration: 135 \t--- Loss: 0.339\n",
      "Iteration: 136 \t--- Loss: 0.348\n",
      "Iteration: 137 \t--- Loss: 0.344\n",
      "Iteration: 138 \t--- Loss: 0.360\n",
      "Iteration: 139 \t--- Loss: 0.356\n",
      "Iteration: 140 \t--- Loss: 0.342\n",
      "Iteration: 141 \t--- Loss: 0.356\n",
      "Iteration: 142 \t--- Loss: 0.365\n",
      "Iteration: 143 \t--- Loss: 0.343\n",
      "Iteration: 144 \t--- Loss: 0.348\n",
      "Iteration: 145 \t--- Loss: 0.367\n",
      "Iteration: 146 \t--- Loss: 0.336\n",
      "Iteration: 147 \t--- Loss: 0.367\n",
      "Iteration: 148 \t--- Loss: 0.346\n",
      "Iteration: 149 \t--- Loss: 0.342\n",
      "Iteration: 150 \t--- Loss: 0.350\n",
      "Iteration: 151 \t--- Loss: 0.356\n",
      "Iteration: 152 \t--- Loss: 0.339\n",
      "Iteration: 153 \t--- Loss: 0.363\n",
      "Iteration: 154 \t--- Loss: 0.356\n",
      "Iteration: 155 \t--- Loss: 0.354\n",
      "Iteration: 156 \t--- Loss: 0.331\n",
      "Iteration: 157 \t--- Loss: 0.346\n",
      "Iteration: 158 \t--- Loss: 0.350\n",
      "Iteration: 159 \t--- Loss: 0.342\n",
      "Iteration: 160 \t--- Loss: 0.343\n",
      "Iteration: 161 \t--- Loss: 0.345\n",
      "Iteration: 162 \t--- Loss: 0.346\n",
      "Iteration: 163 \t--- Loss: 0.355\n",
      "Iteration: 164 \t--- Loss: 0.342\n",
      "Iteration: 165 \t--- Loss: 0.342\n",
      "Iteration: 166 \t--- Loss: 0.340\n",
      "Iteration: 167 \t--- Loss: 0.343\n",
      "Iteration: 168 \t--- Loss: 0.351\n",
      "Iteration: 169 \t--- Loss: 0.355\n",
      "Iteration: 170 \t--- Loss: 0.358\n",
      "Iteration: 171 \t--- Loss: 0.358\n",
      "Iteration: 172 \t--- Loss: 0.355\n",
      "Iteration: 173 \t--- Loss: 0.346\n",
      "Iteration: 174 \t--- Loss: 0.345\n",
      "Iteration: 175 \t--- Loss: 0.355\n",
      "Iteration: 176 \t--- Loss: 0.349\n",
      "Iteration: 177 \t--- Loss: 0.335\n",
      "Iteration: 178 \t--- Loss: 0.360\n",
      "Iteration: 179 \t--- Loss: 0.338\n",
      "Iteration: 180 \t--- Loss: 0.344\n",
      "Iteration: 181 \t--- Loss: 0.344\n",
      "Iteration: 182 \t--- Loss: 0.357\n",
      "Iteration: 183 \t--- Loss: 0.343\n",
      "Iteration: 184 \t--- Loss: 0.344\n",
      "Iteration: 185 \t--- Loss: 0.348\n",
      "Iteration: 186 \t--- Loss: 0.342\n",
      "Iteration: 187 \t--- Loss: 0.338\n",
      "Iteration: 188 \t--- Loss: 0.365\n",
      "Iteration: 189 \t--- Loss: 0.354\n",
      "Iteration: 190 \t--- Loss: 0.351\n",
      "Iteration: 191 \t--- Loss: 0.369\n",
      "Iteration: 192 \t--- Loss: 0.360\n",
      "Iteration: 193 \t--- Loss: 0.343\n",
      "Iteration: 194 \t--- Loss: 0.352\n",
      "Iteration: 195 \t--- Loss: 0.338\n",
      "Iteration: 196 \t--- Loss: 0.345\n",
      "Iteration: 197 \t--- Loss: 0.348\n",
      "Iteration: 198 \t--- Loss: 0.340\n",
      "Iteration: 199 \t--- Loss: 0.345\n",
      "Iteration: 200 \t--- Loss: 0.355\n",
      "Iteration: 201 \t--- Loss: 0.358\n",
      "Iteration: 202 \t--- Loss: 0.338\n",
      "Iteration: 203 \t--- Loss: 0.358\n",
      "Iteration: 204 \t--- Loss: 0.346\n",
      "Iteration: 205 \t--- Loss: 0.355\n",
      "Iteration: 206 \t--- Loss: 0.348\n",
      "Iteration: 207 \t--- Loss: 0.333\n",
      "Iteration: 208 \t--- Loss: 0.352\n",
      "Iteration: 209 \t--- Loss: 0.352\n",
      "Iteration: 210 \t--- Loss: 0.351\n",
      "Iteration: 211 \t--- Loss: 0.352\n",
      "Iteration: 212 \t--- Loss: 0.358\n",
      "Iteration: 213 \t--- Loss: 0.331\n",
      "Iteration: 214 \t--- Loss: 0.356\n",
      "Iteration: 215 \t--- Loss: 0.357\n",
      "Iteration: 216 \t--- Loss: 0.351\n",
      "Iteration: 217 \t--- Loss: 0.345\n",
      "Iteration: 218 \t--- Loss: 0.360\n",
      "Iteration: 219 \t--- Loss: 0.346\n",
      "Iteration: 220 \t--- Loss: 0.347\n",
      "Iteration: 221 \t--- Loss: 0.338\n",
      "Iteration: 222 \t--- Loss: 0.345\n",
      "Iteration: 223 \t--- Loss: 0.341\n",
      "Iteration: 224 \t--- Loss: 0.355\n",
      "Iteration: 225 \t--- Loss: 0.357\n",
      "Iteration: 226 \t--- Loss: 0.347\n",
      "Iteration: 227 \t--- Loss: 0.345\n",
      "Iteration: 228 \t--- Loss: 0.346\n",
      "Iteration: 229 \t--- Loss: 0.349\n",
      "Iteration: 230 \t--- Loss: 0.340\n",
      "Iteration: 231 \t--- Loss: 0.342\n",
      "Iteration: 232 \t--- Loss: 0.340\n",
      "Iteration: 233 \t--- Loss: 0.333\n",
      "Iteration: 234 \t--- Loss: 0.346\n",
      "Iteration: 235 \t--- Loss: 0.348\n",
      "Iteration: 236 \t--- Loss: 0.353\n",
      "Iteration: 237 \t--- Loss: 0.348\n",
      "Iteration: 238 \t--- Loss: 0.353\n",
      "Iteration: 239 \t--- Loss: 0.338\n",
      "Iteration: 240 \t--- Loss: 0.342\n",
      "Iteration: 241 \t--- Loss: 0.356\n",
      "Iteration: 242 \t--- Loss: 0.350\n",
      "Iteration: 243 \t--- Loss: 0.346\n",
      "Iteration: 244 \t--- Loss: 0.340\n",
      "Iteration: 245 \t--- Loss: 0.360\n",
      "Iteration: 246 \t--- Loss: 0.345\n",
      "Iteration: 247 \t--- Loss: 0.369\n",
      "Iteration: 248 \t--- Loss: 0.340\n",
      "Iteration: 249 \t--- Loss: 0.350\n",
      "Iteration: 250 \t--- Loss: 0.368\n",
      "Iteration: 251 \t--- Loss: 0.357\n",
      "Iteration: 252 \t--- Loss: 0.344\n",
      "Iteration: 253 \t--- Loss: 0.348\n",
      "Iteration: 254 \t--- Loss: 0.340\n",
      "Iteration: 255 \t--- Loss: 0.350\n",
      "Iteration: 256 \t--- Loss: 0.351\n",
      "Iteration: 257 \t--- Loss: 0.341\n",
      "Iteration: 258 \t--- Loss: 0.346\n",
      "Iteration: 259 \t--- Loss: 0.345"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:37<00:00, 97.23s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.007\n",
      "Iteration: 261 \t--- Loss: 0.009\n",
      "Iteration: 262 \t--- Loss: 0.009\n",
      "Iteration: 263 \t--- Loss: 0.009\n",
      "Iteration: 264 \t--- Loss: 0.008\n",
      "Iteration: 265 \t--- Loss: 0.008\n",
      "Iteration: 266 \t--- Loss: 0.008\n",
      "Iteration: 267 \t--- Loss: 0.007\n",
      "Iteration: 268 \t--- Loss: 0.008\n",
      "Iteration: 269 \t--- Loss: 0.008\n",
      "Iteration: 270 \t--- Loss: 0.008\n",
      "Iteration: 271 \t--- Loss: 0.007\n",
      "Iteration: 272 \t--- Loss: 0.006\n",
      "Iteration: 273 \t--- Loss: 0.007\n",
      "Iteration: 274 \t--- Loss: 0.008\n",
      "Iteration: 275 \t--- Loss: 0.007\n",
      "Iteration: 276 \t--- Loss: 0.008\n",
      "Iteration: 277 \t--- Loss: 0.007\n",
      "Iteration: 278 \t--- Loss: 0.007\n",
      "Iteration: 279 \t--- Loss: 0.007\n",
      "Iteration: 280 \t--- Loss: 0.006\n",
      "Iteration: 281 \t--- Loss: 0.007\n",
      "Iteration: 282 \t--- Loss: 0.007\n",
      "Iteration: 283 \t--- Loss: 0.007\n",
      "Iteration: 284 \t--- Loss: 0.006\n",
      "Iteration: 285 \t--- Loss: 0.006\n",
      "Iteration: 286 \t--- Loss: 0.007\n",
      "Iteration: 287 \t--- Loss: 0.006\n",
      "Iteration: 288 \t--- Loss: 0.006\n",
      "Iteration: 289 \t--- Loss: 0.006\n",
      "Iteration: 290 \t--- Loss: 0.006\n",
      "Iteration: 291 \t--- Loss: 0.006\n",
      "Iteration: 292 \t--- Loss: 0.006\n",
      "Iteration: 293 \t--- Loss: 0.006\n",
      "Iteration: 294 \t--- Loss: 0.006\n",
      "Iteration: 295 \t--- Loss: 0.006\n",
      "Iteration: 296 \t--- Loss: 0.006\n",
      "Iteration: 297 \t--- Loss: 0.006\n",
      "Iteration: 298 \t--- Loss: 0.005\n",
      "Iteration: 299 \t--- Loss: 0.005\n",
      "Iteration: 300 \t--- Loss: 0.006\n",
      "Iteration: 301 \t--- Loss: 0.006\n",
      "Iteration: 302 \t--- Loss: 0.005\n",
      "Iteration: 303 \t--- Loss: 0.006\n",
      "Iteration: 304 \t--- Loss: 0.005\n",
      "Iteration: 305 \t--- Loss: 0.005\n",
      "Iteration: 306 \t--- Loss: 0.006\n",
      "Iteration: 307 \t--- Loss: 0.005\n",
      "Iteration: 308 \t--- Loss: 0.006\n",
      "Iteration: 309 \t--- Loss: 0.006\n",
      "Iteration: 310 \t--- Loss: 0.005\n",
      "Iteration: 311 \t--- Loss: 0.005\n",
      "Iteration: 312 \t--- Loss: 0.006\n",
      "Iteration: 313 \t--- Loss: 0.005\n",
      "Iteration: 314 \t--- Loss: 0.005\n",
      "Iteration: 315 \t--- Loss: 0.005\n",
      "Iteration: 316 \t--- Loss: 0.005\n",
      "Iteration: 317 \t--- Loss: 0.005\n",
      "Iteration: 318 \t--- Loss: 0.005\n",
      "Iteration: 319 \t--- Loss: 0.005\n",
      "Iteration: 320 \t--- Loss: 0.005\n",
      "Iteration: 321 \t--- Loss: 0.005\n",
      "Iteration: 322 \t--- Loss: 0.005\n",
      "Iteration: 323 \t--- Loss: 0.005\n",
      "Iteration: 324 \t--- Loss: 0.005\n",
      "Iteration: 325 \t--- Loss: 0.005\n",
      "Iteration: 326 \t--- Loss: 0.005\n",
      "Iteration: 327 \t--- Loss: 0.005\n",
      "Iteration: 328 \t--- Loss: 0.004\n",
      "Iteration: 329 \t--- Loss: 0.005\n",
      "Iteration: 330 \t--- Loss: 0.005\n",
      "Iteration: 331 \t--- Loss: 0.005\n",
      "Iteration: 332 \t--- Loss: 0.005\n",
      "Iteration: 333 \t--- Loss: 0.005\n",
      "Iteration: 334 \t--- Loss: 0.004\n",
      "Iteration: 335 \t--- Loss: 0.004\n",
      "Iteration: 336 \t--- Loss: 0.004\n",
      "Iteration: 337 \t--- Loss: 0.005\n",
      "Iteration: 338 \t--- Loss: 0.005\n",
      "Iteration: 339 \t--- Loss: 0.005\n",
      "Iteration: 340 \t--- Loss: 0.005\n",
      "Iteration: 341 \t--- Loss: 0.004\n",
      "Iteration: 342 \t--- Loss: 0.005\n",
      "Iteration: 343 \t--- Loss: 0.005\n",
      "Iteration: 344 \t--- Loss: 0.005\n",
      "Iteration: 345 \t--- Loss: 0.004\n",
      "Iteration: 346 \t--- Loss: 0.004\n",
      "Iteration: 347 \t--- Loss: 0.004\n",
      "Iteration: 348 \t--- Loss: 0.004\n",
      "Iteration: 349 \t--- Loss: 0.005\n",
      "Iteration: 350 \t--- Loss: 0.005\n",
      "Iteration: 351 \t--- Loss: 0.004\n",
      "Iteration: 352 \t--- Loss: 0.005\n",
      "Iteration: 353 \t--- Loss: 0.004\n",
      "Iteration: 354 \t--- Loss: 0.005\n",
      "Iteration: 355 \t--- Loss: 0.005\n",
      "Iteration: 356 \t--- Loss: 0.004\n",
      "Iteration: 357 \t--- Loss: 0.005\n",
      "Iteration: 358 \t--- Loss: 0.004\n",
      "Iteration: 359 \t--- Loss: 0.004\n",
      "Iteration: 360 \t--- Loss: 0.005\n",
      "Iteration: 361 \t--- Loss: 0.005\n",
      "Iteration: 362 \t--- Loss: 0.005\n",
      "Iteration: 363 \t--- Loss: 0.005\n",
      "Iteration: 364 \t--- Loss: 0.004\n",
      "Iteration: 365 \t--- Loss: 0.004\n",
      "Iteration: 366 \t--- Loss: 0.005\n",
      "Iteration: 367 \t--- Loss: 0.005\n",
      "Iteration: 368 \t--- Loss: 0.004\n",
      "Iteration: 369 \t--- Loss: 0.004\n",
      "Iteration: 370 \t--- Loss: 0.004\n",
      "Iteration: 371 \t--- Loss: 0.005\n",
      "Iteration: 372 \t--- Loss: 0.005\n",
      "Iteration: 373 \t--- Loss: 0.004\n",
      "Iteration: 374 \t--- Loss: 0.004\n",
      "Iteration: 375 \t--- Loss: 0.004\n",
      "Iteration: 376 \t--- Loss: 0.004\n",
      "Iteration: 377 \t--- Loss: 0.004\n",
      "Iteration: 378 \t--- Loss: 0.004\n",
      "Iteration: 379 \t--- Loss: 0.004\n",
      "Iteration: 380 \t--- Loss: 0.004\n",
      "Iteration: 381 \t--- Loss: 0.004\n",
      "Iteration: 382 \t--- Loss: 0.004\n",
      "Iteration: 383 \t--- Loss: 0.004\n",
      "Iteration: 384 \t--- Loss: 0.005\n",
      "Iteration: 385 \t--- Loss: 0.005\n",
      "Iteration: 386 \t--- Loss: 0.004\n",
      "Iteration: 387 \t--- Loss: 0.004\n",
      "Iteration: 388 \t--- Loss: 0.005\n",
      "Iteration: 389 \t--- Loss: 0.004\n",
      "Iteration: 390 \t--- Loss: 0.004\n",
      "Iteration: 391 \t--- Loss: 0.004\n",
      "Iteration: 392 \t--- Loss: 0.004\n",
      "Iteration: 393 \t--- Loss: 0.005\n",
      "Iteration: 394 \t--- Loss: 0.004\n",
      "Iteration: 395 \t--- Loss: 0.004\n",
      "Iteration: 396 \t--- Loss: 0.004\n",
      "Iteration: 397 \t--- Loss: 0.004\n",
      "Iteration: 398 \t--- Loss: 0.005\n",
      "Iteration: 399 \t--- Loss: 0.004\n",
      "Iteration: 400 \t--- Loss: 0.004\n",
      "Iteration: 401 \t--- Loss: 0.004\n",
      "Iteration: 402 \t--- Loss: 0.004\n",
      "Iteration: 403 \t--- Loss: 0.004\n",
      "Iteration: 404 \t--- Loss: 0.005\n",
      "Iteration: 405 \t--- Loss: 0.004\n",
      "Iteration: 406 \t--- Loss: 0.004\n",
      "Iteration: 407 \t--- Loss: 0.004\n",
      "Iteration: 408 \t--- Loss: 0.004\n",
      "Iteration: 409 \t--- Loss: 0.004\n",
      "Iteration: 410 \t--- Loss: 0.004\n",
      "Iteration: 411 \t--- Loss: 0.004\n",
      "Iteration: 412 \t--- Loss: 0.005\n",
      "Iteration: 413 \t--- Loss: 0.005\n",
      "Iteration: 414 \t--- Loss: 0.004\n",
      "Iteration: 415 \t--- Loss: 0.004\n",
      "Iteration: 416 \t--- Loss: 0.004\n",
      "Iteration: 417 \t--- Loss: 0.005\n",
      "Iteration: 418 \t--- Loss: 0.004\n",
      "Iteration: 419 \t--- Loss: 0.004\n",
      "Iteration: 420 \t--- Loss: 0.004\n",
      "Iteration: 421 \t--- Loss: 0.004\n",
      "Iteration: 422 \t--- Loss: 0.004\n",
      "Iteration: 423 \t--- Loss: 0.005\n",
      "Iteration: 424 \t--- Loss: 0.004\n",
      "Iteration: 425 \t--- Loss: 0.004\n",
      "Iteration: 426 \t--- Loss: 0.004\n",
      "Iteration: 427 \t--- Loss: 0.004\n",
      "Iteration: 428 \t--- Loss: 0.004\n",
      "Iteration: 429 \t--- Loss: 0.004\n",
      "Iteration: 430 \t--- Loss: 0.004\n",
      "Iteration: 431 \t--- Loss: 0.004\n",
      "Iteration: 432 \t--- Loss: 0.005\n",
      "Iteration: 433 \t--- Loss: 0.005\n",
      "Iteration: 434 \t--- Loss: 0.004\n",
      "Iteration: 435 \t--- Loss: 0.004\n",
      "Iteration: 436 \t--- Loss: 0.004\n",
      "Iteration: 437 \t--- Loss: 0.004\n",
      "Iteration: 438 \t--- Loss: 0.004\n",
      "Iteration: 439 \t--- Loss: 0.004\n",
      "Iteration: 440 \t--- Loss: 0.004\n",
      "Iteration: 441 \t--- Loss: 0.004\n",
      "Iteration: 442 \t--- Loss: 0.004\n",
      "Iteration: 443 \t--- Loss: 0.004\n",
      "Iteration: 444 \t--- Loss: 0.004\n",
      "Iteration: 445 \t--- Loss: 0.004\n",
      "Iteration: 446 \t--- Loss: 0.004\n",
      "Iteration: 447 \t--- Loss: 0.004\n",
      "Iteration: 448 \t--- Loss: 0.004\n",
      "Iteration: 449 \t--- Loss: 0.005\n",
      "Iteration: 450 \t--- Loss: 0.004\n",
      "Iteration: 451 \t--- Loss: 0.004\n",
      "Iteration: 452 \t--- Loss: 0.005\n",
      "Iteration: 453 \t--- Loss: 0.004\n",
      "Iteration: 454 \t--- Loss: 0.004\n",
      "Iteration: 455 \t--- Loss: 0.004\n",
      "Iteration: 456 \t--- Loss: 0.004\n",
      "Iteration: 457 \t--- Loss: 0.004\n",
      "Iteration: 458 \t--- Loss: 0.004\n",
      "Iteration: 459 \t--- Loss: 0.004\n",
      "Iteration: 460 \t--- Loss: 0.004\n",
      "Iteration: 461 \t--- Loss: 0.004\n",
      "Iteration: 462 \t--- Loss: 0.004\n",
      "Iteration: 463 \t--- Loss: 0.004\n",
      "Iteration: 464 \t--- Loss: 0.004\n",
      "Iteration: 465 \t--- Loss: 0.004\n",
      "Iteration: 466 \t--- Loss: 0.005\n",
      "Iteration: 467 \t--- Loss: 0.004\n",
      "Iteration: 468 \t--- Loss: 0.004\n",
      "Iteration: 469 \t--- Loss: 0.004\n",
      "Iteration: 470 \t--- Loss: 0.004\n",
      "Iteration: 471 \t--- Loss: 0.004\n",
      "Iteration: 472 \t--- Loss: 0.004\n",
      "Iteration: 473 \t--- Loss: 0.004\n",
      "Iteration: 474 \t--- Loss: 0.004\n",
      "Iteration: 475 \t--- Loss: 0.004\n",
      "Iteration: 476 \t--- Loss: 0.004\n",
      "Iteration: 477 \t--- Loss: 0.004\n",
      "Iteration: 478 \t--- Loss: 0.004\n",
      "Iteration: 479 \t--- Loss: 0.004\n",
      "Iteration: 480 \t--- Loss: 0.004\n",
      "Iteration: 481 \t--- Loss: 0.005\n",
      "Iteration: 482 \t--- Loss: 0.005\n",
      "Iteration: 483 \t--- Loss: 0.004\n",
      "Iteration: 484 \t--- Loss: 0.004\n",
      "Iteration: 485 \t--- Loss: 0.004\n",
      "Iteration: 486 \t--- Loss: 0.004\n",
      "Iteration: 487 \t--- Loss: 0.004\n",
      "Iteration: 488 \t--- Loss: 0.004\n",
      "Iteration: 489 \t--- Loss: 0.004\n",
      "Iteration: 490 \t--- Loss: 0.004\n",
      "Iteration: 491 \t--- Loss: 0.004\n",
      "Iteration: 492 \t--- Loss: 0.004\n",
      "Iteration: 493 \t--- Loss: 0.004\n",
      "Iteration: 494 \t--- Loss: 0.004\n",
      "Iteration: 495 \t--- Loss: 0.005\n",
      "Iteration: 496 \t--- Loss: 0.004\n",
      "Iteration: 497 \t--- Loss: 0.004\n",
      "Iteration: 498 \t--- Loss: 0.004\n",
      "Iteration: 499 \t--- Loss: 0.004\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.53s/it][Parallel(n_jobs=5)]: Done  63 tasks      | elapsed: 39.4min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.49s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:24<00:00, 84.49s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.346\n",
      "Iteration: 261 \t--- Loss: 0.341\n",
      "Iteration: 262 \t--- Loss: 0.353\n",
      "Iteration: 263 \t--- Loss: 0.341\n",
      "Iteration: 264 \t--- Loss: 0.350\n",
      "Iteration: 265 \t--- Loss: 0.350\n",
      "Iteration: 266 \t--- Loss: 0.339\n",
      "Iteration: 267 \t--- Loss: 0.337\n",
      "Iteration: 268 \t--- Loss: 0.358\n",
      "Iteration: 269 \t--- Loss: 0.345\n",
      "Iteration: 270 \t--- Loss: 0.349\n",
      "Iteration: 271 \t--- Loss: 0.360\n",
      "Iteration: 272 \t--- Loss: 0.355\n",
      "Iteration: 273 \t--- Loss: 0.331\n",
      "Iteration: 274 \t--- Loss: 0.349\n",
      "Iteration: 275 \t--- Loss: 0.348\n",
      "Iteration: 276 \t--- Loss: 0.335\n",
      "Iteration: 277 \t--- Loss: 0.343\n",
      "Iteration: 278 \t--- Loss: 0.352\n",
      "Iteration: 279 \t--- Loss: 0.343\n",
      "Iteration: 280 \t--- Loss: 0.350\n",
      "Iteration: 281 \t--- Loss: 0.339\n",
      "Iteration: 282 \t--- Loss: 0.351\n",
      "Iteration: 283 \t--- Loss: 0.333\n",
      "Iteration: 284 \t--- Loss: 0.349\n",
      "Iteration: 285 \t--- Loss: 0.351\n",
      "Iteration: 286 \t--- Loss: 0.342\n",
      "Iteration: 287 \t--- Loss: 0.337\n",
      "Iteration: 288 \t--- Loss: 0.360\n",
      "Iteration: 289 \t--- Loss: 0.329\n",
      "Iteration: 290 \t--- Loss: 0.331\n",
      "Iteration: 291 \t--- Loss: 0.340\n",
      "Iteration: 292 \t--- Loss: 0.340\n",
      "Iteration: 293 \t--- Loss: 0.359\n",
      "Iteration: 294 \t--- Loss: 0.343\n",
      "Iteration: 295 \t--- Loss: 0.352\n",
      "Iteration: 296 \t--- Loss: 0.349\n",
      "Iteration: 297 \t--- Loss: 0.356\n",
      "Iteration: 298 \t--- Loss: 0.345\n",
      "Iteration: 299 \t--- Loss: 0.353\n",
      "Iteration: 300 \t--- Loss: 0.346\n",
      "Iteration: 301 \t--- Loss: 0.349\n",
      "Iteration: 302 \t--- Loss: 0.363\n",
      "Iteration: 303 \t--- Loss: 0.345\n",
      "Iteration: 304 \t--- Loss: 0.349\n",
      "Iteration: 305 \t--- Loss: 0.348\n",
      "Iteration: 306 \t--- Loss: 0.358\n",
      "Iteration: 307 \t--- Loss: 0.344\n",
      "Iteration: 308 \t--- Loss: 0.348\n",
      "Iteration: 309 \t--- Loss: 0.354\n",
      "Iteration: 310 \t--- Loss: 0.359\n",
      "Iteration: 311 \t--- Loss: 0.350\n",
      "Iteration: 312 \t--- Loss: 0.347\n",
      "Iteration: 313 \t--- Loss: 0.355\n",
      "Iteration: 314 \t--- Loss: 0.355\n",
      "Iteration: 315 \t--- Loss: 0.340\n",
      "Iteration: 316 \t--- Loss: 0.342\n",
      "Iteration: 317 \t--- Loss: 0.349\n",
      "Iteration: 318 \t--- Loss: 0.367\n",
      "Iteration: 319 \t--- Loss: 0.365\n",
      "Iteration: 320 \t--- Loss: 0.357\n",
      "Iteration: 321 \t--- Loss: 0.337\n",
      "Iteration: 322 \t--- Loss: 0.356\n",
      "Iteration: 323 \t--- Loss: 0.352\n",
      "Iteration: 324 \t--- Loss: 0.336\n",
      "Iteration: 325 \t--- Loss: 0.339\n",
      "Iteration: 326 \t--- Loss: 0.332\n",
      "Iteration: 327 \t--- Loss: 0.352\n",
      "Iteration: 328 \t--- Loss: 0.342\n",
      "Iteration: 329 \t--- Loss: 0.348\n",
      "Iteration: 330 \t--- Loss: 0.351\n",
      "Iteration: 331 \t--- Loss: 0.331\n",
      "Iteration: 332 \t--- Loss: 0.350\n",
      "Iteration: 333 \t--- Loss: 0.350\n",
      "Iteration: 334 \t--- Loss: 0.369\n",
      "Iteration: 335 \t--- Loss: 0.349\n",
      "Iteration: 336 \t--- Loss: 0.353\n",
      "Iteration: 337 \t--- Loss: 0.361\n",
      "Iteration: 338 \t--- Loss: 0.343\n",
      "Iteration: 339 \t--- Loss: 0.358\n",
      "Iteration: 340 \t--- Loss: 0.353\n",
      "Iteration: 341 \t--- Loss: 0.350\n",
      "Iteration: 342 \t--- Loss: 0.332\n",
      "Iteration: 343 \t--- Loss: 0.365\n",
      "Iteration: 344 \t--- Loss: 0.335\n",
      "Iteration: 345 \t--- Loss: 0.349\n",
      "Iteration: 346 \t--- Loss: 0.339\n",
      "Iteration: 347 \t--- Loss: 0.363\n",
      "Iteration: 348 \t--- Loss: 0.360\n",
      "Iteration: 349 \t--- Loss: 0.353\n",
      "Iteration: 350 \t--- Loss: 0.353\n",
      "Iteration: 351 \t--- Loss: 0.346\n",
      "Iteration: 352 \t--- Loss: 0.344\n",
      "Iteration: 353 \t--- Loss: 0.360\n",
      "Iteration: 354 \t--- Loss: 0.340\n",
      "Iteration: 355 \t--- Loss: 0.334\n",
      "Iteration: 356 \t--- Loss: 0.353\n",
      "Iteration: 357 \t--- Loss: 0.340\n",
      "Iteration: 358 \t--- Loss: 0.345\n",
      "Iteration: 359 \t--- Loss: 0.348\n",
      "Iteration: 360 \t--- Loss: 0.344\n",
      "Iteration: 361 \t--- Loss: 0.368\n",
      "Iteration: 362 \t--- Loss: 0.346\n",
      "Iteration: 363 \t--- Loss: 0.341\n",
      "Iteration: 364 \t--- Loss: 0.342\n",
      "Iteration: 365 \t--- Loss: 0.348\n",
      "Iteration: 366 \t--- Loss: 0.342\n",
      "Iteration: 367 \t--- Loss: 0.333\n",
      "Iteration: 368 \t--- Loss: 0.345\n",
      "Iteration: 369 \t--- Loss: 0.332\n",
      "Iteration: 370 \t--- Loss: 0.351\n",
      "Iteration: 371 \t--- Loss: 0.341\n",
      "Iteration: 372 \t--- Loss: 0.330\n",
      "Iteration: 373 \t--- Loss: 0.342\n",
      "Iteration: 374 \t--- Loss: 0.354\n",
      "Iteration: 375 \t--- Loss: 0.361\n",
      "Iteration: 376 \t--- Loss: 0.351\n",
      "Iteration: 377 \t--- Loss: 0.341\n",
      "Iteration: 378 \t--- Loss: 0.362\n",
      "Iteration: 379 \t--- Loss: 0.349\n",
      "Iteration: 380 \t--- Loss: 0.358\n",
      "Iteration: 381 \t--- Loss: 0.348\n",
      "Iteration: 382 \t--- Loss: 0.351\n",
      "Iteration: 383 \t--- Loss: 0.344\n",
      "Iteration: 384 \t--- Loss: 0.340\n",
      "Iteration: 385 \t--- Loss: 0.369\n",
      "Iteration: 386 \t--- Loss: 0.348\n",
      "Iteration: 387 \t--- Loss: 0.355\n",
      "Iteration: 388 \t--- Loss: 0.360\n",
      "Iteration: 389 \t--- Loss: 0.355\n",
      "Iteration: 390 \t--- Loss: 0.367\n",
      "Iteration: 391 \t--- Loss: 0.348\n",
      "Iteration: 392 \t--- Loss: 0.349\n",
      "Iteration: 393 \t--- Loss: 0.344\n",
      "Iteration: 394 \t--- Loss: 0.359\n",
      "Iteration: 395 \t--- Loss: 0.343\n",
      "Iteration: 396 \t--- Loss: 0.351\n",
      "Iteration: 397 \t--- Loss: 0.363\n",
      "Iteration: 398 \t--- Loss: 0.349\n",
      "Iteration: 399 \t--- Loss: 0.341\n",
      "Iteration: 400 \t--- Loss: 0.356\n",
      "Iteration: 401 \t--- Loss: 0.355\n",
      "Iteration: 402 \t--- Loss: 0.352\n",
      "Iteration: 403 \t--- Loss: 0.349\n",
      "Iteration: 404 \t--- Loss: 0.346\n",
      "Iteration: 405 \t--- Loss: 0.349\n",
      "Iteration: 406 \t--- Loss: 0.358\n",
      "Iteration: 407 \t--- Loss: 0.342\n",
      "Iteration: 408 \t--- Loss: 0.342\n",
      "Iteration: 409 \t--- Loss: 0.352\n",
      "Iteration: 410 \t--- Loss: 0.347\n",
      "Iteration: 411 \t--- Loss: 0.349\n",
      "Iteration: 412 \t--- Loss: 0.354\n",
      "Iteration: 413 \t--- Loss: 0.361\n",
      "Iteration: 414 \t--- Loss: 0.337\n",
      "Iteration: 415 \t--- Loss: 0.350\n",
      "Iteration: 416 \t--- Loss: 0.341\n",
      "Iteration: 417 \t--- Loss: 0.348\n",
      "Iteration: 418 \t--- Loss: 0.355\n",
      "Iteration: 419 \t--- Loss: 0.341\n",
      "Iteration: 420 \t--- Loss: 0.339\n",
      "Iteration: 421 \t--- Loss: 0.351\n",
      "Iteration: 422 \t--- Loss: 0.367\n",
      "Iteration: 423 \t--- Loss: 0.359\n",
      "Iteration: 424 \t--- Loss: 0.362\n",
      "Iteration: 425 \t--- Loss: 0.349\n",
      "Iteration: 426 \t--- Loss: 0.349\n",
      "Iteration: 427 \t--- Loss: 0.342\n",
      "Iteration: 428 \t--- Loss: 0.349\n",
      "Iteration: 429 \t--- Loss: 0.350\n",
      "Iteration: 430 \t--- Loss: 0.333\n",
      "Iteration: 431 \t--- Loss: 0.358\n",
      "Iteration: 432 \t--- Loss: 0.341\n",
      "Iteration: 433 \t--- Loss: 0.348\n",
      "Iteration: 434 \t--- Loss: 0.362\n",
      "Iteration: 435 \t--- Loss: 0.363\n",
      "Iteration: 436 \t--- Loss: 0.345\n",
      "Iteration: 437 \t--- Loss: 0.346\n",
      "Iteration: 438 \t--- Loss: 0.349\n",
      "Iteration: 439 \t--- Loss: 0.360\n",
      "Iteration: 440 \t--- Loss: 0.350\n",
      "Iteration: 441 \t--- Loss: 0.334\n",
      "Iteration: 442 \t--- Loss: 0.342\n",
      "Iteration: 443 \t--- Loss: 0.358\n",
      "Iteration: 444 \t--- Loss: 0.354\n",
      "Iteration: 445 \t--- Loss: 0.363\n",
      "Iteration: 446 \t--- Loss: 0.344\n",
      "Iteration: 447 \t--- Loss: 0.355\n",
      "Iteration: 448 \t--- Loss: 0.363\n",
      "Iteration: 449 \t--- Loss: 0.340\n",
      "Iteration: 450 \t--- Loss: 0.350\n",
      "Iteration: 451 \t--- Loss: 0.330\n",
      "Iteration: 452 \t--- Loss: 0.333\n",
      "Iteration: 453 \t--- Loss: 0.339\n",
      "Iteration: 454 \t--- Loss: 0.356\n",
      "Iteration: 455 \t--- Loss: 0.337\n",
      "Iteration: 456 \t--- Loss: 0.343\n",
      "Iteration: 457 \t--- Loss: 0.354\n",
      "Iteration: 458 \t--- Loss: 0.352\n",
      "Iteration: 459 \t--- Loss: 0.344\n",
      "Iteration: 460 \t--- Loss: 0.351\n",
      "Iteration: 461 \t--- Loss: 0.360\n",
      "Iteration: 462 \t--- Loss: 0.340\n",
      "Iteration: 463 \t--- Loss: 0.352\n",
      "Iteration: 464 \t--- Loss: 0.340\n",
      "Iteration: 465 \t--- Loss: 0.352\n",
      "Iteration: 466 \t--- Loss: 0.344\n",
      "Iteration: 467 \t--- Loss: 0.354\n",
      "Iteration: 468 \t--- Loss: 0.335\n",
      "Iteration: 469 \t--- Loss: 0.356\n",
      "Iteration: 470 \t--- Loss: 0.343\n",
      "Iteration: 471 \t--- Loss: 0.358\n",
      "Iteration: 472 \t--- Loss: 0.352\n",
      "Iteration: 473 \t--- Loss: 0.348\n",
      "Iteration: 474 \t--- Loss: 0.355\n",
      "Iteration: 475 \t--- Loss: 0.357\n",
      "Iteration: 476 \t--- Loss: 0.349\n",
      "Iteration: 477 \t--- Loss: 0.344\n",
      "Iteration: 478 \t--- Loss: 0.351\n",
      "Iteration: 479 \t--- Loss: 0.360\n",
      "Iteration: 480 \t--- Loss: 0.343\n",
      "Iteration: 481 \t--- Loss: 0.362\n",
      "Iteration: 482 \t--- Loss: 0.358\n",
      "Iteration: 483 \t--- Loss: 0.337\n",
      "Iteration: 484 \t--- Loss: 0.355\n",
      "Iteration: 485 \t--- Loss: 0.353\n",
      "Iteration: 486 \t--- Loss: 0.352\n",
      "Iteration: 487 \t--- Loss: 0.348\n",
      "Iteration: 488 \t--- Loss: 0.342\n",
      "Iteration: 489 \t--- Loss: 0.344\n",
      "Iteration: 490 \t--- Loss: 0.348\n",
      "Iteration: 491 \t--- Loss: 0.347\n",
      "Iteration: 492 \t--- Loss: 0.363\n",
      "Iteration: 493 \t--- Loss: 0.339\n",
      "Iteration: 494 \t--- Loss: 0.351\n",
      "Iteration: 495 \t--- Loss: 0.368\n",
      "Iteration: 496 \t--- Loss: 0.348\n",
      "Iteration: 497 \t--- Loss: 0.335\n",
      "Iteration: 498 \t--- Loss: 0.359\n",
      "Iteration: 499 \t--- Loss: 0.352\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.54s/it][Parallel(n_jobs=5)]: Done  64 tasks      | elapsed: 39.9min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.49s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 4.746\n",
      "Iteration: 1 \t--- Loss: 4.560\n",
      "Iteration: 2 \t--- Loss: 4.040\n",
      "Iteration: 3 \t--- Loss: 3.971\n",
      "Iteration: 4 \t--- Loss: 4.133\n",
      "Iteration: 5 \t--- Loss: 4.030\n",
      "Iteration: 6 \t--- Loss: 3.866\n",
      "Iteration: 7 \t--- Loss: 3.858\n",
      "Iteration: 8 \t--- Loss: 3.622\n",
      "Iteration: 9 \t--- Loss: 3.683\n",
      "Iteration: 10 \t--- Loss: 3.615\n",
      "Iteration: 11 \t--- Loss: 4.083\n",
      "Iteration: 12 \t--- Loss: 4.099\n",
      "Iteration: 13 \t--- Loss: 3.785\n",
      "Iteration: 14 \t--- Loss: 3.778\n",
      "Iteration: 15 \t--- Loss: 3.646\n",
      "Iteration: 16 \t--- Loss: 3.832\n",
      "Iteration: 17 \t--- Loss: 3.943\n",
      "Iteration: 18 \t--- Loss: 3.483\n",
      "Iteration: 19 \t--- Loss: 3.678\n",
      "Iteration: 20 \t--- Loss: 3.673\n",
      "Iteration: 21 \t--- Loss: 3.740\n",
      "Iteration: 22 \t--- Loss: 3.752\n",
      "Iteration: 23 \t--- Loss: 3.866\n",
      "Iteration: 24 \t--- Loss: 3.624\n",
      "Iteration: 25 \t--- Loss: 3.607\n",
      "Iteration: 26 \t--- Loss: 3.470\n",
      "Iteration: 27 \t--- Loss: 3.693\n",
      "Iteration: 28 \t--- Loss: 3.628\n",
      "Iteration: 29 \t--- Loss: 3.624\n",
      "Iteration: 30 \t--- Loss: 3.836\n",
      "Iteration: 31 \t--- Loss: 3.758\n",
      "Iteration: 32 \t--- Loss: 3.765\n",
      "Iteration: 33 \t--- Loss: 3.412\n",
      "Iteration: 34 \t--- Loss: 3.779\n",
      "Iteration: 35 \t--- Loss: 3.655\n",
      "Iteration: 36 \t--- Loss: 3.801\n",
      "Iteration: 37 \t--- Loss: 3.705\n",
      "Iteration: 38 \t--- Loss: 3.559\n",
      "Iteration: 39 \t--- Loss: 3.675\n",
      "Iteration: 40 \t--- Loss: 3.908\n",
      "Iteration: 41 \t--- Loss: 3.694\n",
      "Iteration: 42 \t--- Loss: 4.063\n",
      "Iteration: 43 \t--- Loss: 3.968\n",
      "Iteration: 44 \t--- Loss: 3.570\n",
      "Iteration: 45 \t--- Loss: 3.720\n",
      "Iteration: 46 \t--- Loss: 3.793\n",
      "Iteration: 47 \t--- Loss: 3.645\n",
      "Iteration: 48 \t--- Loss: 3.656\n",
      "Iteration: 49 \t--- Loss: 3.934\n",
      "Iteration: 50 \t--- Loss: 3.831\n",
      "Iteration: 51 \t--- Loss: 3.657\n",
      "Iteration: 52 \t--- Loss: 3.643\n",
      "Iteration: 53 \t--- Loss: 3.560\n",
      "Iteration: 54 \t--- Loss: 3.941\n",
      "Iteration: 55 \t--- Loss: 3.530\n",
      "Iteration: 56 \t--- Loss: 3.614\n",
      "Iteration: 57 \t--- Loss: 3.811\n",
      "Iteration: 58 \t--- Loss: 3.667\n",
      "Iteration: 59 \t--- Loss: 3.546\n",
      "Iteration: 60 \t--- Loss: 3.375\n",
      "Iteration: 61 \t--- Loss: 3.713\n",
      "Iteration: 62 \t--- Loss: 3.727\n",
      "Iteration: 63 \t--- Loss: 3.737\n",
      "Iteration: 64 \t--- Loss: 3.615\n",
      "Iteration: 65 \t--- Loss: 3.776\n",
      "Iteration: 66 \t--- Loss: 3.785\n",
      "Iteration: 67 \t--- Loss: 3.773\n",
      "Iteration: 68 \t--- Loss: 3.652\n",
      "Iteration: 69 \t--- Loss: 3.768\n",
      "Iteration: 70 \t--- Loss: 3.803\n",
      "Iteration: 71 \t--- Loss: 3.544\n",
      "Iteration: 72 \t--- Loss: 3.789\n",
      "Iteration: 73 \t--- Loss: 3.666\n",
      "Iteration: 74 \t--- Loss: 3.701\n",
      "Iteration: 75 \t--- Loss: 3.836\n",
      "Iteration: 76 \t--- Loss: 3.694\n",
      "Iteration: 77 \t--- Loss: 3.834\n",
      "Iteration: 78 \t--- Loss: 3.704\n",
      "Iteration: 79 \t--- Loss: 3.509\n",
      "Iteration: 80 \t--- Loss: 3.828\n",
      "Iteration: 81 \t--- Loss: 3.526\n",
      "Iteration: 82 \t--- Loss: 3.394\n",
      "Iteration: 83 \t--- Loss: 3.750\n",
      "Iteration: 84 \t--- Loss: 3.638\n",
      "Iteration: 85 \t--- Loss: 3.653\n",
      "Iteration: 86 \t--- Loss: 3.701\n",
      "Iteration: 87 \t--- Loss: 3.542\n",
      "Iteration: 88 \t--- Loss: 3.578\n",
      "Iteration: 89 \t--- Loss: 3.674\n",
      "Iteration: 90 \t--- Loss: 3.714\n",
      "Iteration: 91 \t--- Loss: 3.811\n",
      "Iteration: 92 \t--- Loss: 3.699\n",
      "Iteration: 93 \t--- Loss: 3.754\n",
      "Iteration: 94 \t--- Loss: 3.725\n",
      "Iteration: 95 \t--- Loss: 3.852\n",
      "Iteration: 96 \t--- Loss: 3.792\n",
      "Iteration: 97 \t--- Loss: 3.594\n",
      "Iteration: 98 \t--- Loss: 3.667\n",
      "Iteration: 99 \t--- Loss: 3.908\n",
      "Iteration: 100 \t--- Loss: 3.740\n",
      "Iteration: 101 \t--- Loss: 3.646\n",
      "Iteration: 102 \t--- Loss: 3.684\n",
      "Iteration: 103 \t--- Loss: 3.758\n",
      "Iteration: 104 \t--- Loss: 3.591\n",
      "Iteration: 105 \t--- Loss: 3.644\n",
      "Iteration: 106 \t--- Loss: 3.577\n",
      "Iteration: 107 \t--- Loss: 3.843\n",
      "Iteration: 108 \t--- Loss: 3.671\n",
      "Iteration: 109 \t--- Loss: 3.831\n",
      "Iteration: 110 \t--- Loss: 3.862\n",
      "Iteration: 111 \t--- Loss: 3.903\n",
      "Iteration: 112 \t--- Loss: 4.051\n",
      "Iteration: 113 \t--- Loss: 3.723\n",
      "Iteration: 114 \t--- Loss: 3.754\n",
      "Iteration: 115 \t--- Loss: 3.864\n",
      "Iteration: 116 \t--- Loss: 3.669\n",
      "Iteration: 117 \t--- Loss: 3.851\n",
      "Iteration: 118 \t--- Loss: 3.586\n",
      "Iteration: 119 \t--- Loss: 3.693\n",
      "Iteration: 120 \t--- Loss: 3.864\n",
      "Iteration: 121 \t--- Loss: 3.393\n",
      "Iteration: 122 \t--- Loss: 3.749\n",
      "Iteration: 123 \t--- Loss: 4.019\n",
      "Iteration: 124 \t--- Loss: 3.933\n",
      "Iteration: 125 \t--- Loss: 3.760\n",
      "Iteration: 126 \t--- Loss: 3.639\n",
      "Iteration: 127 \t--- Loss: 3.531\n",
      "Iteration: 128 \t--- Loss: 3.803\n",
      "Iteration: 129 \t--- Loss: 3.570\n",
      "Iteration: 130 \t--- Loss: 3.813\n",
      "Iteration: 131 \t--- Loss: 3.714\n",
      "Iteration: 132 \t--- Loss: 3.652\n",
      "Iteration: 133 \t--- Loss: 3.521\n",
      "Iteration: 134 \t--- Loss: 3.890\n",
      "Iteration: 135 \t--- Loss: 3.374\n",
      "Iteration: 136 \t--- Loss: 3.935\n",
      "Iteration: 137 \t--- Loss: 3.861\n",
      "Iteration: 138 \t--- Loss: 3.504\n",
      "Iteration: 139 \t--- Loss: 3.625\n",
      "Iteration: 140 \t--- Loss: 3.684\n",
      "Iteration: 141 \t--- Loss: 3.637\n",
      "Iteration: 142 \t--- Loss: 3.765\n",
      "Iteration: 143 \t--- Loss: 3.676\n",
      "Iteration: 144 \t--- Loss: 3.732\n",
      "Iteration: 145 \t--- Loss: 3.758\n",
      "Iteration: 146 \t--- Loss: 3.653\n",
      "Iteration: 147 \t--- Loss: 3.700\n",
      "Iteration: 148 \t--- Loss: 3.650\n",
      "Iteration: 149 \t--- Loss: 3.598\n",
      "Iteration: 150 \t--- Loss: 3.753\n",
      "Iteration: 151 \t--- Loss: 3.497\n",
      "Iteration: 152 \t--- Loss: 3.631\n",
      "Iteration: 153 \t--- Loss: 3.731\n",
      "Iteration: 154 \t--- Loss: 3.559\n",
      "Iteration: 155 \t--- Loss: 3.487\n",
      "Iteration: 156 \t--- Loss: 3.387\n",
      "Iteration: 157 \t--- Loss: 3.913\n",
      "Iteration: 158 \t--- Loss: 3.729\n",
      "Iteration: 159 \t--- Loss: 3.611\n",
      "Iteration: 160 \t--- Loss: 3.736\n",
      "Iteration: 161 \t--- Loss: 3.733\n",
      "Iteration: 162 \t--- Loss: 3.604\n",
      "Iteration: 163 \t--- Loss: 3.691\n",
      "Iteration: 164 \t--- Loss: 3.532\n",
      "Iteration: 165 \t--- Loss: 3.944\n",
      "Iteration: 166 \t--- Loss: 3.866\n",
      "Iteration: 167 \t--- Loss: 3.894\n",
      "Iteration: 168 \t--- Loss: 3.643\n",
      "Iteration: 169 \t--- Loss: 3.838\n",
      "Iteration: 170 \t--- Loss: 3.507\n",
      "Iteration: 171 \t--- Loss: 3.696\n",
      "Iteration: 172 \t--- Loss: 3.617\n",
      "Iteration: 173 \t--- Loss: 3.938\n",
      "Iteration: 174 \t--- Loss: 3.678\n",
      "Iteration: 175 \t--- Loss: 3.891\n",
      "Iteration: 176 \t--- Loss: 3.780\n",
      "Iteration: 177 \t--- Loss: 3.646\n",
      "Iteration: 178 \t--- Loss: 3.540\n",
      "Iteration: 179 \t--- Loss: 3.671\n",
      "Iteration: 180 \t--- Loss: 3.689\n",
      "Iteration: 181 \t--- Loss: 3.773\n",
      "Iteration: 182 \t--- Loss: 3.839\n",
      "Iteration: 183 \t--- Loss: 3.628\n",
      "Iteration: 184 \t--- Loss: 3.610\n",
      "Iteration: 185 \t--- Loss: 3.789\n",
      "Iteration: 186 \t--- Loss: 3.716\n",
      "Iteration: 187 \t--- Loss: 3.974\n",
      "Iteration: 188 \t--- Loss: 3.595\n",
      "Iteration: 189 \t--- Loss: 3.761\n",
      "Iteration: 190 \t--- Loss: 3.732\n",
      "Iteration: 191 \t--- Loss: 3.605\n",
      "Iteration: 192 \t--- Loss: 3.843\n",
      "Iteration: 193 \t--- Loss: 3.602\n",
      "Iteration: 194 \t--- Loss: 3.878\n",
      "Iteration: 195 \t--- Loss: 3.732\n",
      "Iteration: 196 \t--- Loss: 3.912\n",
      "Iteration: 197 \t--- Loss: 3.515\n",
      "Iteration: 198 \t--- Loss: 3.527\n",
      "Iteration: 199 \t--- Loss: 3.574\n",
      "Iteration: 200 \t--- Loss: 3.535\n",
      "Iteration: 201 \t--- Loss: 3.578\n",
      "Iteration: 202 \t--- Loss: 3.606\n",
      "Iteration: 203 \t--- Loss: 3.540\n",
      "Iteration: 204 \t--- Loss: 3.793\n",
      "Iteration: 205 \t--- Loss: 3.620\n",
      "Iteration: 206 \t--- Loss: 3.779\n",
      "Iteration: 207 \t--- Loss: 3.884\n",
      "Iteration: 208 \t--- Loss: 3.695\n",
      "Iteration: 209 \t--- Loss: 3.665\n",
      "Iteration: 210 \t--- Loss: 3.903\n",
      "Iteration: 211 \t--- Loss: 3.671\n",
      "Iteration: 212 \t--- Loss: 3.574\n",
      "Iteration: 213 \t--- Loss: 3.594\n",
      "Iteration: 214 \t--- Loss: 3.852\n",
      "Iteration: 215 \t--- Loss: 3.658\n",
      "Iteration: 216 \t--- Loss: 3.676\n",
      "Iteration: 217 \t--- Loss: 3.784\n",
      "Iteration: 218 \t--- Loss: 3.814\n",
      "Iteration: 219 \t--- Loss: 3.763\n",
      "Iteration: 220 \t--- Loss: 3.972\n",
      "Iteration: 221 \t--- Loss: 3.701\n",
      "Iteration: 222 \t--- Loss: 3.783\n",
      "Iteration: 223 \t--- Loss: 3.641\n",
      "Iteration: 224 \t--- Loss: 3.742\n",
      "Iteration: 225 \t--- Loss: 3.713\n",
      "Iteration: 226 \t--- Loss: 3.726\n",
      "Iteration: 227 \t--- Loss: 3.797\n",
      "Iteration: 228 \t--- Loss: 3.681\n",
      "Iteration: 229 \t--- Loss: 3.528\n",
      "Iteration: 230 \t--- Loss: 3.812\n",
      "Iteration: 231 \t--- Loss: 3.577\n",
      "Iteration: 232 \t--- Loss: 3.665\n",
      "Iteration: 233 \t--- Loss: 3.756\n",
      "Iteration: 234 \t--- Loss: 3.546\n",
      "Iteration: 235 \t--- Loss: 3.647\n",
      "Iteration: 236 \t--- Loss: 3.785\n",
      "Iteration: 237 \t--- Loss: 3.659\n",
      "Iteration: 238 \t--- Loss: 3.633\n",
      "Iteration: 239 \t--- Loss: 3.837\n",
      "Iteration: 240 \t--- Loss: 3.759\n",
      "Iteration: 241 \t--- Loss: 3.503\n",
      "Iteration: 242 \t--- Loss: 3.509\n",
      "Iteration: 243 \t--- Loss: 3.501\n",
      "Iteration: 244 \t--- Loss: 3.769\n",
      "Iteration: 245 \t--- Loss: 3.872\n",
      "Iteration: 246 \t--- Loss: 3.749\n",
      "Iteration: 247 \t--- Loss: 3.952\n",
      "Iteration: 248 \t--- Loss: 3.982\n",
      "Iteration: 249 \t--- Loss: 3.753\n",
      "Iteration: 250 \t--- Loss: 3.819\n",
      "Iteration: 251 \t--- Loss: 3.659\n",
      "Iteration: 252 \t--- Loss: 3.619\n",
      "Iteration: 253 \t--- Loss: 3.624\n",
      "Iteration: 254 \t--- Loss: 3.632\n",
      "Iteration: 255 \t--- Loss: 3.778\n",
      "Iteration: 256 \t--- Loss: 3.725\n",
      "Iteration: 257 \t--- Loss: 3.650\n",
      "Iteration: 258 \t--- Loss: 3.572\n",
      "Iteration: 259 \t--- Loss: 3.994Iteration: 0 \t--- Loss: 0.656\n",
      "Iteration: 1 \t--- Loss: 0.611\n",
      "Iteration: 2 \t--- Loss: 0.512\n",
      "Iteration: 3 \t--- Loss: 0.492\n",
      "Iteration: 4 \t--- Loss: 0.502\n",
      "Iteration: 5 \t--- Loss: 0.437\n",
      "Iteration: 6 \t--- Loss: 0.432\n",
      "Iteration: 7 \t--- Loss: 0.403\n",
      "Iteration: 8 \t--- Loss: 0.390\n",
      "Iteration: 9 \t--- Loss: 0.372\n",
      "Iteration: 10 \t--- Loss: 0.362\n",
      "Iteration: 11 \t--- Loss: 0.355\n",
      "Iteration: 12 \t--- Loss: 0.343\n",
      "Iteration: 13 \t--- Loss: 0.315\n",
      "Iteration: 14 \t--- Loss: 0.335\n",
      "Iteration: 15 \t--- Loss: 0.326\n",
      "Iteration: 16 \t--- Loss: 0.339\n",
      "Iteration: 17 \t--- Loss: 0.303\n",
      "Iteration: 18 \t--- Loss: 0.314\n",
      "Iteration: 19 \t--- Loss: 0.295\n",
      "Iteration: 20 \t--- Loss: 0.303\n",
      "Iteration: 21 \t--- Loss: 0.332\n",
      "Iteration: 22 \t--- Loss: 0.318\n",
      "Iteration: 23 \t--- Loss: 0.321\n",
      "Iteration: 24 \t--- Loss: 0.295\n",
      "Iteration: 25 \t--- Loss: 0.296\n",
      "Iteration: 26 \t--- Loss: 0.306\n",
      "Iteration: 27 \t--- Loss: 0.300\n",
      "Iteration: 28 \t--- Loss: 0.314\n",
      "Iteration: 29 \t--- Loss: 0.317\n",
      "Iteration: 30 \t--- Loss: 0.298\n",
      "Iteration: 31 \t--- Loss: 0.305\n",
      "Iteration: 32 \t--- Loss: 0.312\n",
      "Iteration: 33 \t--- Loss: 0.291\n",
      "Iteration: 34 \t--- Loss: 0.314\n",
      "Iteration: 35 \t--- Loss: 0.278\n",
      "Iteration: 36 \t--- Loss: 0.311\n",
      "Iteration: 37 \t--- Loss: 0.308\n",
      "Iteration: 38 \t--- Loss: 0.285\n",
      "Iteration: 39 \t--- Loss: 0.297\n",
      "Iteration: 40 \t--- Loss: 0.289\n",
      "Iteration: 41 \t--- Loss: 0.281\n",
      "Iteration: 42 \t--- Loss: 0.294\n",
      "Iteration: 43 \t--- Loss: 0.304\n",
      "Iteration: 44 \t--- Loss: 0.294\n",
      "Iteration: 45 \t--- Loss: 0.297\n",
      "Iteration: 46 \t--- Loss: 0.285\n",
      "Iteration: 47 \t--- Loss: 0.306\n",
      "Iteration: 48 \t--- Loss: 0.288\n",
      "Iteration: 49 \t--- Loss: 0.291\n",
      "Iteration: 50 \t--- Loss: 0.286\n",
      "Iteration: 51 \t--- Loss: 0.289\n",
      "Iteration: 52 \t--- Loss: 0.296\n",
      "Iteration: 53 \t--- Loss: 0.291\n",
      "Iteration: 54 \t--- Loss: 0.291\n",
      "Iteration: 55 \t--- Loss: 0.288\n",
      "Iteration: 56 \t--- Loss: 0.297\n",
      "Iteration: 57 \t--- Loss: 0.301\n",
      "Iteration: 58 \t--- Loss: 0.281\n",
      "Iteration: 59 \t--- Loss: 0.297\n",
      "Iteration: 60 \t--- Loss: 0.299\n",
      "Iteration: 61 \t--- Loss: 0.293\n",
      "Iteration: 62 \t--- Loss: 0.296\n",
      "Iteration: 63 \t--- Loss: 0.305\n",
      "Iteration: 64 \t--- Loss: 0.281\n",
      "Iteration: 65 \t--- Loss: 0.302\n",
      "Iteration: 66 \t--- Loss: 0.312\n",
      "Iteration: 67 \t--- Loss: 0.281\n",
      "Iteration: 68 \t--- Loss: 0.291\n",
      "Iteration: 69 \t--- Loss: 0.287\n",
      "Iteration: 70 \t--- Loss: 0.296\n",
      "Iteration: 71 \t--- Loss: 0.297\n",
      "Iteration: 72 \t--- Loss: 0.291\n",
      "Iteration: 73 \t--- Loss: 0.299\n",
      "Iteration: 74 \t--- Loss: 0.310\n",
      "Iteration: 75 \t--- Loss: 0.285\n",
      "Iteration: 76 \t--- Loss: 0.291\n",
      "Iteration: 77 \t--- Loss: 0.298\n",
      "Iteration: 78 \t--- Loss: 0.301\n",
      "Iteration: 79 \t--- Loss: 0.286\n",
      "Iteration: 80 \t--- Loss: 0.284\n",
      "Iteration: 81 \t--- Loss: 0.308\n",
      "Iteration: 82 \t--- Loss: 0.271\n",
      "Iteration: 83 \t--- Loss: 0.300\n",
      "Iteration: 84 \t--- Loss: 0.302\n",
      "Iteration: 85 \t--- Loss: 0.301\n",
      "Iteration: 86 \t--- Loss: 0.285\n",
      "Iteration: 87 \t--- Loss: 0.285\n",
      "Iteration: 88 \t--- Loss: 0.285\n",
      "Iteration: 89 \t--- Loss: 0.318\n",
      "Iteration: 90 \t--- Loss: 0.288\n",
      "Iteration: 91 \t--- Loss: 0.284\n",
      "Iteration: 92 \t--- Loss: 0.299\n",
      "Iteration: 93 \t--- Loss: 0.296\n",
      "Iteration: 94 \t--- Loss: 0.285\n",
      "Iteration: 95 \t--- Loss: 0.286\n",
      "Iteration: 96 \t--- Loss: 0.288\n",
      "Iteration: 97 \t--- Loss: 0.292\n",
      "Iteration: 98 \t--- Loss: 0.287\n",
      "Iteration: 99 \t--- Loss: 0.286\n",
      "Iteration: 100 \t--- Loss: 0.291\n",
      "Iteration: 101 \t--- Loss: 0.300\n",
      "Iteration: 102 \t--- Loss: 0.290\n",
      "Iteration: 103 \t--- Loss: 0.288\n",
      "Iteration: 104 \t--- Loss: 0.320\n",
      "Iteration: 105 \t--- Loss: 0.286\n",
      "Iteration: 106 \t--- Loss: 0.305\n",
      "Iteration: 107 \t--- Loss: 0.278\n",
      "Iteration: 108 \t--- Loss: 0.299\n",
      "Iteration: 109 \t--- Loss: 0.288\n",
      "Iteration: 110 \t--- Loss: 0.304\n",
      "Iteration: 111 \t--- Loss: 0.289\n",
      "Iteration: 112 \t--- Loss: 0.302\n",
      "Iteration: 113 \t--- Loss: 0.289\n",
      "Iteration: 114 \t--- Loss: 0.291\n",
      "Iteration: 115 \t--- Loss: 0.274\n",
      "Iteration: 116 \t--- Loss: 0.304\n",
      "Iteration: 117 \t--- Loss: 0.294\n",
      "Iteration: 118 \t--- Loss: 0.296\n",
      "Iteration: 119 \t--- Loss: 0.297\n",
      "Iteration: 120 \t--- Loss: 0.301\n",
      "Iteration: 121 \t--- Loss: 0.306\n",
      "Iteration: 122 \t--- Loss: 0.300\n",
      "Iteration: 123 \t--- Loss: 0.288\n",
      "Iteration: 124 \t--- Loss: 0.300\n",
      "Iteration: 125 \t--- Loss: 0.298\n",
      "Iteration: 126 \t--- Loss: 0.289\n",
      "Iteration: 127 \t--- Loss: 0.290\n",
      "Iteration: 128 \t--- Loss: 0.299\n",
      "Iteration: 129 \t--- Loss: 0.303\n",
      "Iteration: 130 \t--- Loss: 0.291\n",
      "Iteration: 131 \t--- Loss: 0.297\n",
      "Iteration: 132 \t--- Loss: 0.297\n",
      "Iteration: 133 \t--- Loss: 0.300\n",
      "Iteration: 134 \t--- Loss: 0.287\n",
      "Iteration: 135 \t--- Loss: 0.292\n",
      "Iteration: 136 \t--- Loss: 0.282\n",
      "Iteration: 137 \t--- Loss: 0.280\n",
      "Iteration: 138 \t--- Loss: 0.303\n",
      "Iteration: 139 \t--- Loss: 0.296\n",
      "Iteration: 140 \t--- Loss: 0.306\n",
      "Iteration: 141 \t--- Loss: 0.304\n",
      "Iteration: 142 \t--- Loss: 0.277\n",
      "Iteration: 143 \t--- Loss: 0.286\n",
      "Iteration: 144 \t--- Loss: 0.303\n",
      "Iteration: 145 \t--- Loss: 0.296\n",
      "Iteration: 146 \t--- Loss: 0.292\n",
      "Iteration: 147 \t--- Loss: 0.267\n",
      "Iteration: 148 \t--- Loss: 0.300\n",
      "Iteration: 149 \t--- Loss: 0.307\n",
      "Iteration: 150 \t--- Loss: 0.284\n",
      "Iteration: 151 \t--- Loss: 0.292\n",
      "Iteration: 152 \t--- Loss: 0.304\n",
      "Iteration: 153 \t--- Loss: 0.282\n",
      "Iteration: 154 \t--- Loss: 0.279\n",
      "Iteration: 155 \t--- Loss: 0.285\n",
      "Iteration: 156 \t--- Loss: 0.287\n",
      "Iteration: 157 \t--- Loss: 0.290\n",
      "Iteration: 158 \t--- Loss: 0.297\n",
      "Iteration: 159 \t--- Loss: 0.290\n",
      "Iteration: 160 \t--- Loss: 0.281\n",
      "Iteration: 161 \t--- Loss: 0.297\n",
      "Iteration: 162 \t--- Loss: 0.284\n",
      "Iteration: 163 \t--- Loss: 0.297\n",
      "Iteration: 164 \t--- Loss: 0.282\n",
      "Iteration: 165 \t--- Loss: 0.276\n",
      "Iteration: 166 \t--- Loss: 0.284\n",
      "Iteration: 167 \t--- Loss: 0.309\n",
      "Iteration: 168 \t--- Loss: 0.291\n",
      "Iteration: 169 \t--- Loss: 0.282\n",
      "Iteration: 170 \t--- Loss: 0.300\n",
      "Iteration: 171 \t--- Loss: 0.290\n",
      "Iteration: 172 \t--- Loss: 0.307\n",
      "Iteration: 173 \t--- Loss: 0.293\n",
      "Iteration: 174 \t--- Loss: 0.297\n",
      "Iteration: 175 \t--- Loss: 0.284\n",
      "Iteration: 176 \t--- Loss: 0.287\n",
      "Iteration: 177 \t--- Loss: 0.280\n",
      "Iteration: 178 \t--- Loss: 0.278\n",
      "Iteration: 179 \t--- Loss: 0.294\n",
      "Iteration: 180 \t--- Loss: 0.296\n",
      "Iteration: 181 \t--- Loss: 0.273\n",
      "Iteration: 182 \t--- Loss: 0.276\n",
      "Iteration: 183 \t--- Loss: 0.286\n",
      "Iteration: 184 \t--- Loss: 0.282\n",
      "Iteration: 185 \t--- Loss: 0.285\n",
      "Iteration: 186 \t--- Loss: 0.300\n",
      "Iteration: 187 \t--- Loss: 0.297\n",
      "Iteration: 188 \t--- Loss: 0.289\n",
      "Iteration: 189 \t--- Loss: 0.310\n",
      "Iteration: 190 \t--- Loss: 0.280\n",
      "Iteration: 191 \t--- Loss: 0.292\n",
      "Iteration: 192 \t--- Loss: 0.297\n",
      "Iteration: 193 \t--- Loss: 0.288\n",
      "Iteration: 194 \t--- Loss: 0.298\n",
      "Iteration: 195 \t--- Loss: 0.298\n",
      "Iteration: 196 \t--- Loss: 0.288\n",
      "Iteration: 197 \t--- Loss: 0.296\n",
      "Iteration: 198 \t--- Loss: 0.290\n",
      "Iteration: 199 \t--- Loss: 0.308\n",
      "Iteration: 200 \t--- Loss: 0.285\n",
      "Iteration: 201 \t--- Loss: 0.287\n",
      "Iteration: 202 \t--- Loss: 0.301\n",
      "Iteration: 203 \t--- Loss: 0.296\n",
      "Iteration: 204 \t--- Loss: 0.293\n",
      "Iteration: 205 \t--- Loss: 0.292\n",
      "Iteration: 206 \t--- Loss: 0.283\n",
      "Iteration: 207 \t--- Loss: 0.283\n",
      "Iteration: 208 \t--- Loss: 0.292\n",
      "Iteration: 209 \t--- Loss: 0.288\n",
      "Iteration: 210 \t--- Loss: 0.295\n",
      "Iteration: 211 \t--- Loss: 0.288\n",
      "Iteration: 212 \t--- Loss: 0.312\n",
      "Iteration: 213 \t--- Loss: 0.285\n",
      "Iteration: 214 \t--- Loss: 0.272\n",
      "Iteration: 215 \t--- Loss: 0.291\n",
      "Iteration: 216 \t--- Loss: 0.288\n",
      "Iteration: 217 \t--- Loss: 0.294\n",
      "Iteration: 218 \t--- Loss: 0.289\n",
      "Iteration: 219 \t--- Loss: 0.303\n",
      "Iteration: 220 \t--- Loss: 0.288\n",
      "Iteration: 221 \t--- Loss: 0.289\n",
      "Iteration: 222 \t--- Loss: 0.299\n",
      "Iteration: 223 \t--- Loss: 0.282\n",
      "Iteration: 224 \t--- Loss: 0.295\n",
      "Iteration: 225 \t--- Loss: 0.295\n",
      "Iteration: 226 \t--- Loss: 0.291\n",
      "Iteration: 227 \t--- Loss: 0.292\n",
      "Iteration: 228 \t--- Loss: 0.278\n",
      "Iteration: 229 \t--- Loss: 0.312\n",
      "Iteration: 230 \t--- Loss: 0.292\n",
      "Iteration: 231 \t--- Loss: 0.297\n",
      "Iteration: 232 \t--- Loss: 0.284\n",
      "Iteration: 233 \t--- Loss: 0.322\n",
      "Iteration: 234 \t--- Loss: 0.289\n",
      "Iteration: 235 \t--- Loss: 0.283\n",
      "Iteration: 236 \t--- Loss: 0.297\n",
      "Iteration: 237 \t--- Loss: 0.291\n",
      "Iteration: 238 \t--- Loss: 0.293\n",
      "Iteration: 239 \t--- Loss: 0.274\n",
      "Iteration: 240 \t--- Loss: 0.275\n",
      "Iteration: 241 \t--- Loss: 0.276\n",
      "Iteration: 242 \t--- Loss: 0.299\n",
      "Iteration: 243 \t--- Loss: 0.280\n",
      "Iteration: 244 \t--- Loss: 0.287\n",
      "Iteration: 245 \t--- Loss: 0.306\n",
      "Iteration: 246 \t--- Loss: 0.287\n",
      "Iteration: 247 \t--- Loss: 0.305\n",
      "Iteration: 248 \t--- Loss: 0.299\n",
      "Iteration: 249 \t--- Loss: 0.293\n",
      "Iteration: 250 \t--- Loss: 0.292\n",
      "Iteration: 251 \t--- Loss: 0.306\n",
      "Iteration: 252 \t--- Loss: 0.300\n",
      "Iteration: 253 \t--- Loss: 0.297\n",
      "Iteration: 254 \t--- Loss: 0.304\n",
      "Iteration: 255 \t--- Loss: 0.296\n",
      "Iteration: 256 \t--- Loss: 0.291\n",
      "Iteration: 257 \t--- Loss: 0.302\n",
      "Iteration: 258 \t--- Loss: 0.281\n",
      "Iteration: 259 \t--- Loss: 0.289"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:26<00:00, 86.33s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 3.823\n",
      "Iteration: 261 \t--- Loss: 3.755\n",
      "Iteration: 262 \t--- Loss: 3.764\n",
      "Iteration: 263 \t--- Loss: 3.803\n",
      "Iteration: 264 \t--- Loss: 3.449\n",
      "Iteration: 265 \t--- Loss: 3.721\n",
      "Iteration: 266 \t--- Loss: 3.928\n",
      "Iteration: 267 \t--- Loss: 3.623\n",
      "Iteration: 268 \t--- Loss: 3.923\n",
      "Iteration: 269 \t--- Loss: 3.821\n",
      "Iteration: 270 \t--- Loss: 3.657\n",
      "Iteration: 271 \t--- Loss: 3.858\n",
      "Iteration: 272 \t--- Loss: 3.632\n",
      "Iteration: 273 \t--- Loss: 3.785\n",
      "Iteration: 274 \t--- Loss: 3.919\n",
      "Iteration: 275 \t--- Loss: 3.510\n",
      "Iteration: 276 \t--- Loss: 3.632\n",
      "Iteration: 277 \t--- Loss: 3.578\n",
      "Iteration: 278 \t--- Loss: 3.661\n",
      "Iteration: 279 \t--- Loss: 3.645\n",
      "Iteration: 280 \t--- Loss: 3.990\n",
      "Iteration: 281 \t--- Loss: 3.681\n",
      "Iteration: 282 \t--- Loss: 3.829\n",
      "Iteration: 283 \t--- Loss: 3.754\n",
      "Iteration: 284 \t--- Loss: 3.791\n",
      "Iteration: 285 \t--- Loss: 3.527\n",
      "Iteration: 286 \t--- Loss: 3.750\n",
      "Iteration: 287 \t--- Loss: 3.730\n",
      "Iteration: 288 \t--- Loss: 3.540\n",
      "Iteration: 289 \t--- Loss: 3.929\n",
      "Iteration: 290 \t--- Loss: 3.786\n",
      "Iteration: 291 \t--- Loss: 3.819\n",
      "Iteration: 292 \t--- Loss: 3.634\n",
      "Iteration: 293 \t--- Loss: 3.688\n",
      "Iteration: 294 \t--- Loss: 3.749\n",
      "Iteration: 295 \t--- Loss: 3.729\n",
      "Iteration: 296 \t--- Loss: 4.006\n",
      "Iteration: 297 \t--- Loss: 3.725\n",
      "Iteration: 298 \t--- Loss: 3.662\n",
      "Iteration: 299 \t--- Loss: 3.807\n",
      "Iteration: 300 \t--- Loss: 3.354\n",
      "Iteration: 301 \t--- Loss: 3.805\n",
      "Iteration: 302 \t--- Loss: 3.671\n",
      "Iteration: 303 \t--- Loss: 3.686\n",
      "Iteration: 304 \t--- Loss: 3.726\n",
      "Iteration: 305 \t--- Loss: 3.612\n",
      "Iteration: 306 \t--- Loss: 3.902\n",
      "Iteration: 307 \t--- Loss: 3.866\n",
      "Iteration: 308 \t--- Loss: 3.995\n",
      "Iteration: 309 \t--- Loss: 3.827\n",
      "Iteration: 310 \t--- Loss: 3.672\n",
      "Iteration: 311 \t--- Loss: 3.764\n",
      "Iteration: 312 \t--- Loss: 3.537\n",
      "Iteration: 313 \t--- Loss: 3.791\n",
      "Iteration: 314 \t--- Loss: 3.787\n",
      "Iteration: 315 \t--- Loss: 3.793\n",
      "Iteration: 316 \t--- Loss: 3.549\n",
      "Iteration: 317 \t--- Loss: 3.692\n",
      "Iteration: 318 \t--- Loss: 3.603\n",
      "Iteration: 319 \t--- Loss: 3.690\n",
      "Iteration: 320 \t--- Loss: 3.627\n",
      "Iteration: 321 \t--- Loss: 3.919\n",
      "Iteration: 322 \t--- Loss: 3.756\n",
      "Iteration: 323 \t--- Loss: 3.828\n",
      "Iteration: 324 \t--- Loss: 3.711\n",
      "Iteration: 325 \t--- Loss: 3.667\n",
      "Iteration: 326 \t--- Loss: 3.785\n",
      "Iteration: 327 \t--- Loss: 3.679\n",
      "Iteration: 328 \t--- Loss: 3.878\n",
      "Iteration: 329 \t--- Loss: 3.581\n",
      "Iteration: 330 \t--- Loss: 3.842\n",
      "Iteration: 331 \t--- Loss: 3.682\n",
      "Iteration: 332 \t--- Loss: 3.846\n",
      "Iteration: 333 \t--- Loss: 3.631\n",
      "Iteration: 334 \t--- Loss: 3.885\n",
      "Iteration: 335 \t--- Loss: 3.590\n",
      "Iteration: 336 \t--- Loss: 3.568\n",
      "Iteration: 337 \t--- Loss: 3.619\n",
      "Iteration: 338 \t--- Loss: 3.752\n",
      "Iteration: 339 \t--- Loss: 3.675\n",
      "Iteration: 340 \t--- Loss: 3.806\n",
      "Iteration: 341 \t--- Loss: 3.731\n",
      "Iteration: 342 \t--- Loss: 3.931\n",
      "Iteration: 343 \t--- Loss: 3.826\n",
      "Iteration: 344 \t--- Loss: 3.515\n",
      "Iteration: 345 \t--- Loss: 3.815\n",
      "Iteration: 346 \t--- Loss: 3.839\n",
      "Iteration: 347 \t--- Loss: 3.959\n",
      "Iteration: 348 \t--- Loss: 3.737\n",
      "Iteration: 349 \t--- Loss: 3.721\n",
      "Iteration: 350 \t--- Loss: 3.608\n",
      "Iteration: 351 \t--- Loss: 3.731\n",
      "Iteration: 352 \t--- Loss: 3.966\n",
      "Iteration: 353 \t--- Loss: 3.650\n",
      "Iteration: 354 \t--- Loss: 3.783\n",
      "Iteration: 355 \t--- Loss: 3.635\n",
      "Iteration: 356 \t--- Loss: 3.848\n",
      "Iteration: 357 \t--- Loss: 3.836\n",
      "Iteration: 358 \t--- Loss: 3.650\n",
      "Iteration: 359 \t--- Loss: 3.661\n",
      "Iteration: 360 \t--- Loss: 3.253\n",
      "Iteration: 361 \t--- Loss: 3.579\n",
      "Iteration: 362 \t--- Loss: 3.737\n",
      "Iteration: 363 \t--- Loss: 3.850\n",
      "Iteration: 364 \t--- Loss: 3.692\n",
      "Iteration: 365 \t--- Loss: 3.840\n",
      "Iteration: 366 \t--- Loss: 3.563\n",
      "Iteration: 367 \t--- Loss: 3.626\n",
      "Iteration: 368 \t--- Loss: 3.755\n",
      "Iteration: 369 \t--- Loss: 3.757\n",
      "Iteration: 370 \t--- Loss: 3.859\n",
      "Iteration: 371 \t--- Loss: 3.713\n",
      "Iteration: 372 \t--- Loss: 3.597\n",
      "Iteration: 373 \t--- Loss: 3.808\n",
      "Iteration: 374 \t--- Loss: 3.650\n",
      "Iteration: 375 \t--- Loss: 3.894\n",
      "Iteration: 376 \t--- Loss: 3.481\n",
      "Iteration: 377 \t--- Loss: 3.906\n",
      "Iteration: 378 \t--- Loss: 3.608\n",
      "Iteration: 379 \t--- Loss: 3.640\n",
      "Iteration: 380 \t--- Loss: 3.719\n",
      "Iteration: 381 \t--- Loss: 3.634\n",
      "Iteration: 382 \t--- Loss: 3.996\n",
      "Iteration: 383 \t--- Loss: 3.301\n",
      "Iteration: 384 \t--- Loss: 3.855\n",
      "Iteration: 385 \t--- Loss: 3.766\n",
      "Iteration: 386 \t--- Loss: 3.617\n",
      "Iteration: 387 \t--- Loss: 3.756\n",
      "Iteration: 388 \t--- Loss: 3.661\n",
      "Iteration: 389 \t--- Loss: 3.703\n",
      "Iteration: 390 \t--- Loss: 3.778\n",
      "Iteration: 391 \t--- Loss: 3.716\n",
      "Iteration: 392 \t--- Loss: 3.641\n",
      "Iteration: 393 \t--- Loss: 3.487\n",
      "Iteration: 394 \t--- Loss: 3.612\n",
      "Iteration: 395 \t--- Loss: 3.529\n",
      "Iteration: 396 \t--- Loss: 3.736\n",
      "Iteration: 397 \t--- Loss: 3.973\n",
      "Iteration: 398 \t--- Loss: 3.645\n",
      "Iteration: 399 \t--- Loss: 3.752\n",
      "Iteration: 400 \t--- Loss: 3.517\n",
      "Iteration: 401 \t--- Loss: 3.692\n",
      "Iteration: 402 \t--- Loss: 3.771\n",
      "Iteration: 403 \t--- Loss: 3.703\n",
      "Iteration: 404 \t--- Loss: 3.757\n",
      "Iteration: 405 \t--- Loss: 3.854\n",
      "Iteration: 406 \t--- Loss: 3.922\n",
      "Iteration: 407 \t--- Loss: 3.539\n",
      "Iteration: 408 \t--- Loss: 3.757\n",
      "Iteration: 409 \t--- Loss: 3.762\n",
      "Iteration: 410 \t--- Loss: 3.755\n",
      "Iteration: 411 \t--- Loss: 3.666\n",
      "Iteration: 412 \t--- Loss: 3.824\n",
      "Iteration: 413 \t--- Loss: 3.722\n",
      "Iteration: 414 \t--- Loss: 3.409\n",
      "Iteration: 415 \t--- Loss: 4.001\n",
      "Iteration: 416 \t--- Loss: 3.644\n",
      "Iteration: 417 \t--- Loss: 3.870\n",
      "Iteration: 418 \t--- Loss: 3.574\n",
      "Iteration: 419 \t--- Loss: 3.781\n",
      "Iteration: 420 \t--- Loss: 3.622\n",
      "Iteration: 421 \t--- Loss: 3.748\n",
      "Iteration: 422 \t--- Loss: 3.621\n",
      "Iteration: 423 \t--- Loss: 3.630\n",
      "Iteration: 424 \t--- Loss: 3.777\n",
      "Iteration: 425 \t--- Loss: 3.654\n",
      "Iteration: 426 \t--- Loss: 3.753\n",
      "Iteration: 427 \t--- Loss: 3.807\n",
      "Iteration: 428 \t--- Loss: 4.118\n",
      "Iteration: 429 \t--- Loss: 3.617\n",
      "Iteration: 430 \t--- Loss: 3.524\n",
      "Iteration: 431 \t--- Loss: 3.729\n",
      "Iteration: 432 \t--- Loss: 3.730\n",
      "Iteration: 433 \t--- Loss: 3.643\n",
      "Iteration: 434 \t--- Loss: 3.559\n",
      "Iteration: 435 \t--- Loss: 3.725\n",
      "Iteration: 436 \t--- Loss: 3.633\n",
      "Iteration: 437 \t--- Loss: 3.882\n",
      "Iteration: 438 \t--- Loss: 3.498\n",
      "Iteration: 439 \t--- Loss: 3.728\n",
      "Iteration: 440 \t--- Loss: 3.860\n",
      "Iteration: 441 \t--- Loss: 3.599\n",
      "Iteration: 442 \t--- Loss: 3.710\n",
      "Iteration: 443 \t--- Loss: 3.889\n",
      "Iteration: 444 \t--- Loss: 3.700\n",
      "Iteration: 445 \t--- Loss: 3.805\n",
      "Iteration: 446 \t--- Loss: 3.656\n",
      "Iteration: 447 \t--- Loss: 3.628\n",
      "Iteration: 448 \t--- Loss: 3.679\n",
      "Iteration: 449 \t--- Loss: 3.848\n",
      "Iteration: 450 \t--- Loss: 3.731\n",
      "Iteration: 451 \t--- Loss: 3.736\n",
      "Iteration: 452 \t--- Loss: 3.790\n",
      "Iteration: 453 \t--- Loss: 3.630\n",
      "Iteration: 454 \t--- Loss: 3.798\n",
      "Iteration: 455 \t--- Loss: 3.875\n",
      "Iteration: 456 \t--- Loss: 3.683\n",
      "Iteration: 457 \t--- Loss: 3.842\n",
      "Iteration: 458 \t--- Loss: 3.712\n",
      "Iteration: 459 \t--- Loss: 3.831\n",
      "Iteration: 460 \t--- Loss: 3.747\n",
      "Iteration: 461 \t--- Loss: 3.769\n",
      "Iteration: 462 \t--- Loss: 3.917\n",
      "Iteration: 463 \t--- Loss: 3.515\n",
      "Iteration: 464 \t--- Loss: 3.648\n",
      "Iteration: 465 \t--- Loss: 3.555\n",
      "Iteration: 466 \t--- Loss: 3.773\n",
      "Iteration: 467 \t--- Loss: 3.851\n",
      "Iteration: 468 \t--- Loss: 3.766\n",
      "Iteration: 469 \t--- Loss: 3.626\n",
      "Iteration: 470 \t--- Loss: 3.672\n",
      "Iteration: 471 \t--- Loss: 3.650\n",
      "Iteration: 472 \t--- Loss: 3.534\n",
      "Iteration: 473 \t--- Loss: 3.829\n",
      "Iteration: 474 \t--- Loss: 3.768\n",
      "Iteration: 475 \t--- Loss: 3.658\n",
      "Iteration: 476 \t--- Loss: 3.907\n",
      "Iteration: 477 \t--- Loss: 3.721\n",
      "Iteration: 478 \t--- Loss: 3.783\n",
      "Iteration: 479 \t--- Loss: 3.696\n",
      "Iteration: 480 \t--- Loss: 3.782\n",
      "Iteration: 481 \t--- Loss: 3.713\n",
      "Iteration: 482 \t--- Loss: 3.693\n",
      "Iteration: 483 \t--- Loss: 3.813\n",
      "Iteration: 484 \t--- Loss: 3.641\n",
      "Iteration: 485 \t--- Loss: 3.752\n",
      "Iteration: 486 \t--- Loss: 3.734\n",
      "Iteration: 487 \t--- Loss: 3.723\n",
      "Iteration: 488 \t--- Loss: 3.669\n",
      "Iteration: 489 \t--- Loss: 3.915\n",
      "Iteration: 490 \t--- Loss: 3.831\n",
      "Iteration: 491 \t--- Loss: 3.562\n",
      "Iteration: 492 \t--- Loss: 3.678\n",
      "Iteration: 493 \t--- Loss: 3.756\n",
      "Iteration: 494 \t--- Loss: 3.833\n",
      "Iteration: 495 \t--- Loss: 3.652\n",
      "Iteration: 496 \t--- Loss: 3.781\n",
      "Iteration: 497 \t--- Loss: 4.080\n",
      "Iteration: 498 \t--- Loss: 3.498\n",
      "Iteration: 499 \t--- Loss: 3.558\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.50s/it][Parallel(n_jobs=5)]: Done  65 tasks      | elapsed: 41.1min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:28<00:00, 88.47s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.304\n",
      "Iteration: 261 \t--- Loss: 0.291\n",
      "Iteration: 262 \t--- Loss: 0.283\n",
      "Iteration: 263 \t--- Loss: 0.278\n",
      "Iteration: 264 \t--- Loss: 0.282\n",
      "Iteration: 265 \t--- Loss: 0.294\n",
      "Iteration: 266 \t--- Loss: 0.297\n",
      "Iteration: 267 \t--- Loss: 0.298\n",
      "Iteration: 268 \t--- Loss: 0.301\n",
      "Iteration: 269 \t--- Loss: 0.286\n",
      "Iteration: 270 \t--- Loss: 0.295\n",
      "Iteration: 271 \t--- Loss: 0.287\n",
      "Iteration: 272 \t--- Loss: 0.302\n",
      "Iteration: 273 \t--- Loss: 0.297\n",
      "Iteration: 274 \t--- Loss: 0.286\n",
      "Iteration: 275 \t--- Loss: 0.289\n",
      "Iteration: 276 \t--- Loss: 0.306\n",
      "Iteration: 277 \t--- Loss: 0.286\n",
      "Iteration: 278 \t--- Loss: 0.293\n",
      "Iteration: 279 \t--- Loss: 0.305\n",
      "Iteration: 280 \t--- Loss: 0.284\n",
      "Iteration: 281 \t--- Loss: 0.285\n",
      "Iteration: 282 \t--- Loss: 0.301\n",
      "Iteration: 283 \t--- Loss: 0.301\n",
      "Iteration: 284 \t--- Loss: 0.283\n",
      "Iteration: 285 \t--- Loss: 0.304\n",
      "Iteration: 286 \t--- Loss: 0.305\n",
      "Iteration: 287 \t--- Loss: 0.305\n",
      "Iteration: 288 \t--- Loss: 0.289\n",
      "Iteration: 289 \t--- Loss: 0.284\n",
      "Iteration: 290 \t--- Loss: 0.287\n",
      "Iteration: 291 \t--- Loss: 0.292\n",
      "Iteration: 292 \t--- Loss: 0.278\n",
      "Iteration: 293 \t--- Loss: 0.301\n",
      "Iteration: 294 \t--- Loss: 0.294\n",
      "Iteration: 295 \t--- Loss: 0.282\n",
      "Iteration: 296 \t--- Loss: 0.299\n",
      "Iteration: 297 \t--- Loss: 0.280\n",
      "Iteration: 298 \t--- Loss: 0.298\n",
      "Iteration: 299 \t--- Loss: 0.304\n",
      "Iteration: 300 \t--- Loss: 0.297\n",
      "Iteration: 301 \t--- Loss: 0.281\n",
      "Iteration: 302 \t--- Loss: 0.300\n",
      "Iteration: 303 \t--- Loss: 0.278\n",
      "Iteration: 304 \t--- Loss: 0.300\n",
      "Iteration: 305 \t--- Loss: 0.309\n",
      "Iteration: 306 \t--- Loss: 0.278\n",
      "Iteration: 307 \t--- Loss: 0.291\n",
      "Iteration: 308 \t--- Loss: 0.299\n",
      "Iteration: 309 \t--- Loss: 0.299\n",
      "Iteration: 310 \t--- Loss: 0.299\n",
      "Iteration: 311 \t--- Loss: 0.296\n",
      "Iteration: 312 \t--- Loss: 0.287\n",
      "Iteration: 313 \t--- Loss: 0.294\n",
      "Iteration: 314 \t--- Loss: 0.291\n",
      "Iteration: 315 \t--- Loss: 0.291\n",
      "Iteration: 316 \t--- Loss: 0.312\n",
      "Iteration: 317 \t--- Loss: 0.271\n",
      "Iteration: 318 \t--- Loss: 0.294\n",
      "Iteration: 319 \t--- Loss: 0.308\n",
      "Iteration: 320 \t--- Loss: 0.292\n",
      "Iteration: 321 \t--- Loss: 0.280\n",
      "Iteration: 322 \t--- Loss: 0.273\n",
      "Iteration: 323 \t--- Loss: 0.304\n",
      "Iteration: 324 \t--- Loss: 0.303\n",
      "Iteration: 325 \t--- Loss: 0.277\n",
      "Iteration: 326 \t--- Loss: 0.270\n",
      "Iteration: 327 \t--- Loss: 0.303\n",
      "Iteration: 328 \t--- Loss: 0.275\n",
      "Iteration: 329 \t--- Loss: 0.277\n",
      "Iteration: 330 \t--- Loss: 0.296\n",
      "Iteration: 331 \t--- Loss: 0.297\n",
      "Iteration: 332 \t--- Loss: 0.293\n",
      "Iteration: 333 \t--- Loss: 0.284\n",
      "Iteration: 334 \t--- Loss: 0.287\n",
      "Iteration: 335 \t--- Loss: 0.312\n",
      "Iteration: 336 \t--- Loss: 0.291\n",
      "Iteration: 337 \t--- Loss: 0.306\n",
      "Iteration: 338 \t--- Loss: 0.301\n",
      "Iteration: 339 \t--- Loss: 0.290\n",
      "Iteration: 340 \t--- Loss: 0.301\n",
      "Iteration: 341 \t--- Loss: 0.306\n",
      "Iteration: 342 \t--- Loss: 0.289\n",
      "Iteration: 343 \t--- Loss: 0.300\n",
      "Iteration: 344 \t--- Loss: 0.291\n",
      "Iteration: 345 \t--- Loss: 0.294\n",
      "Iteration: 346 \t--- Loss: 0.297\n",
      "Iteration: 347 \t--- Loss: 0.293\n",
      "Iteration: 348 \t--- Loss: 0.294\n",
      "Iteration: 349 \t--- Loss: 0.312\n",
      "Iteration: 350 \t--- Loss: 0.291\n",
      "Iteration: 351 \t--- Loss: 0.281\n",
      "Iteration: 352 \t--- Loss: 0.279\n",
      "Iteration: 353 \t--- Loss: 0.283\n",
      "Iteration: 354 \t--- Loss: 0.288\n",
      "Iteration: 355 \t--- Loss: 0.292\n",
      "Iteration: 356 \t--- Loss: 0.294\n",
      "Iteration: 357 \t--- Loss: 0.293\n",
      "Iteration: 358 \t--- Loss: 0.305\n",
      "Iteration: 359 \t--- Loss: 0.282\n",
      "Iteration: 360 \t--- Loss: 0.307\n",
      "Iteration: 361 \t--- Loss: 0.295\n",
      "Iteration: 362 \t--- Loss: 0.295\n",
      "Iteration: 363 \t--- Loss: 0.298\n",
      "Iteration: 364 \t--- Loss: 0.288\n",
      "Iteration: 365 \t--- Loss: 0.299\n",
      "Iteration: 366 \t--- Loss: 0.300\n",
      "Iteration: 367 \t--- Loss: 0.296\n",
      "Iteration: 368 \t--- Loss: 0.301\n",
      "Iteration: 369 \t--- Loss: 0.295\n",
      "Iteration: 370 \t--- Loss: 0.290\n",
      "Iteration: 371 \t--- Loss: 0.288\n",
      "Iteration: 372 \t--- Loss: 0.290\n",
      "Iteration: 373 \t--- Loss: 0.292\n",
      "Iteration: 374 \t--- Loss: 0.294\n",
      "Iteration: 375 \t--- Loss: 0.287\n",
      "Iteration: 376 \t--- Loss: 0.283\n",
      "Iteration: 377 \t--- Loss: 0.296\n",
      "Iteration: 378 \t--- Loss: 0.277\n",
      "Iteration: 379 \t--- Loss: 0.311\n",
      "Iteration: 380 \t--- Loss: 0.298\n",
      "Iteration: 381 \t--- Loss: 0.290\n",
      "Iteration: 382 \t--- Loss: 0.297\n",
      "Iteration: 383 \t--- Loss: 0.291\n",
      "Iteration: 384 \t--- Loss: 0.319\n",
      "Iteration: 385 \t--- Loss: 0.287\n",
      "Iteration: 386 \t--- Loss: 0.305\n",
      "Iteration: 387 \t--- Loss: 0.292\n",
      "Iteration: 388 \t--- Loss: 0.296\n",
      "Iteration: 389 \t--- Loss: 0.274\n",
      "Iteration: 390 \t--- Loss: 0.301\n",
      "Iteration: 391 \t--- Loss: 0.310\n",
      "Iteration: 392 \t--- Loss: 0.294\n",
      "Iteration: 393 \t--- Loss: 0.297\n",
      "Iteration: 394 \t--- Loss: 0.285\n",
      "Iteration: 395 \t--- Loss: 0.303\n",
      "Iteration: 396 \t--- Loss: 0.305\n",
      "Iteration: 397 \t--- Loss: 0.297\n",
      "Iteration: 398 \t--- Loss: 0.293\n",
      "Iteration: 399 \t--- Loss: 0.293\n",
      "Iteration: 400 \t--- Loss: 0.288\n",
      "Iteration: 401 \t--- Loss: 0.286\n",
      "Iteration: 402 \t--- Loss: 0.303\n",
      "Iteration: 403 \t--- Loss: 0.291\n",
      "Iteration: 404 \t--- Loss: 0.310\n",
      "Iteration: 405 \t--- Loss: 0.301\n",
      "Iteration: 406 \t--- Loss: 0.297\n",
      "Iteration: 407 \t--- Loss: 0.294\n",
      "Iteration: 408 \t--- Loss: 0.302\n",
      "Iteration: 409 \t--- Loss: 0.290\n",
      "Iteration: 410 \t--- Loss: 0.302\n",
      "Iteration: 411 \t--- Loss: 0.293\n",
      "Iteration: 412 \t--- Loss: 0.282\n",
      "Iteration: 413 \t--- Loss: 0.285\n",
      "Iteration: 414 \t--- Loss: 0.294\n",
      "Iteration: 415 \t--- Loss: 0.291\n",
      "Iteration: 416 \t--- Loss: 0.279\n",
      "Iteration: 417 \t--- Loss: 0.303\n",
      "Iteration: 418 \t--- Loss: 0.299\n",
      "Iteration: 419 \t--- Loss: 0.295\n",
      "Iteration: 420 \t--- Loss: 0.290\n",
      "Iteration: 421 \t--- Loss: 0.298\n",
      "Iteration: 422 \t--- Loss: 0.266\n",
      "Iteration: 423 \t--- Loss: 0.283\n",
      "Iteration: 424 \t--- Loss: 0.296\n",
      "Iteration: 425 \t--- Loss: 0.294\n",
      "Iteration: 426 \t--- Loss: 0.281\n",
      "Iteration: 427 \t--- Loss: 0.288\n",
      "Iteration: 428 \t--- Loss: 0.317\n",
      "Iteration: 429 \t--- Loss: 0.301\n",
      "Iteration: 430 \t--- Loss: 0.303\n",
      "Iteration: 431 \t--- Loss: 0.309\n",
      "Iteration: 432 \t--- Loss: 0.263\n",
      "Iteration: 433 \t--- Loss: 0.293\n",
      "Iteration: 434 \t--- Loss: 0.291\n",
      "Iteration: 435 \t--- Loss: 0.302\n",
      "Iteration: 436 \t--- Loss: 0.319\n",
      "Iteration: 437 \t--- Loss: 0.295\n",
      "Iteration: 438 \t--- Loss: 0.285\n",
      "Iteration: 439 \t--- Loss: 0.291\n",
      "Iteration: 440 \t--- Loss: 0.300\n",
      "Iteration: 441 \t--- Loss: 0.295\n",
      "Iteration: 442 \t--- Loss: 0.288\n",
      "Iteration: 443 \t--- Loss: 0.289\n",
      "Iteration: 444 \t--- Loss: 0.303\n",
      "Iteration: 445 \t--- Loss: 0.290\n",
      "Iteration: 446 \t--- Loss: 0.287\n",
      "Iteration: 447 \t--- Loss: 0.309\n",
      "Iteration: 448 \t--- Loss: 0.288\n",
      "Iteration: 449 \t--- Loss: 0.298\n",
      "Iteration: 450 \t--- Loss: 0.282\n",
      "Iteration: 451 \t--- Loss: 0.295\n",
      "Iteration: 452 \t--- Loss: 0.291\n",
      "Iteration: 453 \t--- Loss: 0.284\n",
      "Iteration: 454 \t--- Loss: 0.293\n",
      "Iteration: 455 \t--- Loss: 0.295\n",
      "Iteration: 456 \t--- Loss: 0.289\n",
      "Iteration: 457 \t--- Loss: 0.290\n",
      "Iteration: 458 \t--- Loss: 0.307\n",
      "Iteration: 459 \t--- Loss: 0.283\n",
      "Iteration: 460 \t--- Loss: 0.294\n",
      "Iteration: 461 \t--- Loss: 0.315\n",
      "Iteration: 462 \t--- Loss: 0.289\n",
      "Iteration: 463 \t--- Loss: 0.306\n",
      "Iteration: 464 \t--- Loss: 0.293\n",
      "Iteration: 465 \t--- Loss: 0.295\n",
      "Iteration: 466 \t--- Loss: 0.274\n",
      "Iteration: 467 \t--- Loss: 0.289\n",
      "Iteration: 468 \t--- Loss: 0.285\n",
      "Iteration: 469 \t--- Loss: 0.295\n",
      "Iteration: 470 \t--- Loss: 0.289\n",
      "Iteration: 471 \t--- Loss: 0.279\n",
      "Iteration: 472 \t--- Loss: 0.305\n",
      "Iteration: 473 \t--- Loss: 0.293\n",
      "Iteration: 474 \t--- Loss: 0.291\n",
      "Iteration: 475 \t--- Loss: 0.303\n",
      "Iteration: 476 \t--- Loss: 0.284\n",
      "Iteration: 477 \t--- Loss: 0.274\n",
      "Iteration: 478 \t--- Loss: 0.283\n",
      "Iteration: 479 \t--- Loss: 0.296\n",
      "Iteration: 480 \t--- Loss: 0.289\n",
      "Iteration: 481 \t--- Loss: 0.291\n",
      "Iteration: 482 \t--- Loss: 0.294\n",
      "Iteration: 483 \t--- Loss: 0.285\n",
      "Iteration: 484 \t--- Loss: 0.289\n",
      "Iteration: 485 \t--- Loss: 0.301\n",
      "Iteration: 486 \t--- Loss: 0.285\n",
      "Iteration: 487 \t--- Loss: 0.298\n",
      "Iteration: 488 \t--- Loss: 0.302\n",
      "Iteration: 489 \t--- Loss: 0.280\n",
      "Iteration: 490 \t--- Loss: 0.313\n",
      "Iteration: 491 \t--- Loss: 0.305\n",
      "Iteration: 492 \t--- Loss: 0.295\n",
      "Iteration: 493 \t--- Loss: 0.302\n",
      "Iteration: 494 \t--- Loss: 0.300\n",
      "Iteration: 495 \t--- Loss: 0.281\n",
      "Iteration: 496 \t--- Loss: 0.301\n",
      "Iteration: 497 \t--- Loss: 0.308\n",
      "Iteration: 498 \t--- Loss: 0.318\n",
      "Iteration: 499 \t--- Loss: 0.283\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it][Parallel(n_jobs=5)]: Done  66 tasks      | elapsed: 41.6min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 1.625\n",
      "Iteration: 1 \t--- Loss: 1.502\n",
      "Iteration: 2 \t--- Loss: 1.392\n",
      "Iteration: 3 \t--- Loss: 1.285\n",
      "Iteration: 4 \t--- Loss: 1.235\n",
      "Iteration: 5 \t--- Loss: 1.173\n",
      "Iteration: 6 \t--- Loss: 1.127\n",
      "Iteration: 7 \t--- Loss: 1.120\n",
      "Iteration: 8 \t--- Loss: 1.114\n",
      "Iteration: 9 \t--- Loss: 1.084\n",
      "Iteration: 10 \t--- Loss: 1.081\n",
      "Iteration: 11 \t--- Loss: 1.054\n",
      "Iteration: 12 \t--- Loss: 1.056\n",
      "Iteration: 13 \t--- Loss: 1.049\n",
      "Iteration: 14 \t--- Loss: 1.051\n",
      "Iteration: 15 \t--- Loss: 1.026\n",
      "Iteration: 16 \t--- Loss: 1.052\n",
      "Iteration: 17 \t--- Loss: 1.042\n",
      "Iteration: 18 \t--- Loss: 1.025\n",
      "Iteration: 19 \t--- Loss: 1.029\n",
      "Iteration: 20 \t--- Loss: 1.032\n",
      "Iteration: 21 \t--- Loss: 1.030\n",
      "Iteration: 22 \t--- Loss: 1.011\n",
      "Iteration: 23 \t--- Loss: 1.009\n",
      "Iteration: 24 \t--- Loss: 1.026\n",
      "Iteration: 25 \t--- Loss: 1.024\n",
      "Iteration: 26 \t--- Loss: 1.026\n",
      "Iteration: 27 \t--- Loss: 1.026\n",
      "Iteration: 28 \t--- Loss: 1.022\n",
      "Iteration: 29 \t--- Loss: 1.048\n",
      "Iteration: 30 \t--- Loss: 1.012\n",
      "Iteration: 31 \t--- Loss: 1.031\n",
      "Iteration: 32 \t--- Loss: 1.014\n",
      "Iteration: 33 \t--- Loss: 1.027\n",
      "Iteration: 34 \t--- Loss: 1.023\n",
      "Iteration: 35 \t--- Loss: 1.018\n",
      "Iteration: 36 \t--- Loss: 1.028\n",
      "Iteration: 37 \t--- Loss: 1.026\n",
      "Iteration: 38 \t--- Loss: 1.037\n",
      "Iteration: 39 \t--- Loss: 1.021\n",
      "Iteration: 40 \t--- Loss: 1.030\n",
      "Iteration: 41 \t--- Loss: 1.021\n",
      "Iteration: 42 \t--- Loss: 1.034\n",
      "Iteration: 43 \t--- Loss: 1.016\n",
      "Iteration: 44 \t--- Loss: 1.016\n",
      "Iteration: 45 \t--- Loss: 1.021\n",
      "Iteration: 46 \t--- Loss: 1.011\n",
      "Iteration: 47 \t--- Loss: 1.009\n",
      "Iteration: 48 \t--- Loss: 0.997\n",
      "Iteration: 49 \t--- Loss: 1.006\n",
      "Iteration: 50 \t--- Loss: 1.028\n",
      "Iteration: 51 \t--- Loss: 1.018\n",
      "Iteration: 52 \t--- Loss: 1.008\n",
      "Iteration: 53 \t--- Loss: 1.031\n",
      "Iteration: 54 \t--- Loss: 1.022\n",
      "Iteration: 55 \t--- Loss: 1.007\n",
      "Iteration: 56 \t--- Loss: 1.009\n",
      "Iteration: 57 \t--- Loss: 1.033\n",
      "Iteration: 58 \t--- Loss: 1.023\n",
      "Iteration: 59 \t--- Loss: 1.013\n",
      "Iteration: 60 \t--- Loss: 1.016\n",
      "Iteration: 61 \t--- Loss: 1.020\n",
      "Iteration: 62 \t--- Loss: 1.030\n",
      "Iteration: 63 \t--- Loss: 1.036\n",
      "Iteration: 64 \t--- Loss: 1.028\n",
      "Iteration: 65 \t--- Loss: 1.032\n",
      "Iteration: 66 \t--- Loss: 1.030\n",
      "Iteration: 67 \t--- Loss: 1.022\n",
      "Iteration: 68 \t--- Loss: 1.018\n",
      "Iteration: 69 \t--- Loss: 1.021\n",
      "Iteration: 70 \t--- Loss: 1.012\n",
      "Iteration: 71 \t--- Loss: 1.008\n",
      "Iteration: 72 \t--- Loss: 1.017\n",
      "Iteration: 73 \t--- Loss: 1.033\n",
      "Iteration: 74 \t--- Loss: 1.033\n",
      "Iteration: 75 \t--- Loss: 1.024\n",
      "Iteration: 76 \t--- Loss: 1.027\n",
      "Iteration: 77 \t--- Loss: 1.018\n",
      "Iteration: 78 \t--- Loss: 1.032\n",
      "Iteration: 79 \t--- Loss: 1.018\n",
      "Iteration: 80 \t--- Loss: 1.028\n",
      "Iteration: 81 \t--- Loss: 1.028\n",
      "Iteration: 82 \t--- Loss: 1.031\n",
      "Iteration: 83 \t--- Loss: 0.999\n",
      "Iteration: 84 \t--- Loss: 1.028\n",
      "Iteration: 85 \t--- Loss: 1.034\n",
      "Iteration: 86 \t--- Loss: 1.025\n",
      "Iteration: 87 \t--- Loss: 1.034\n",
      "Iteration: 88 \t--- Loss: 1.024\n",
      "Iteration: 89 \t--- Loss: 1.017\n",
      "Iteration: 90 \t--- Loss: 1.031\n",
      "Iteration: 91 \t--- Loss: 1.032\n",
      "Iteration: 92 \t--- Loss: 1.009\n",
      "Iteration: 93 \t--- Loss: 1.017\n",
      "Iteration: 94 \t--- Loss: 1.019\n",
      "Iteration: 95 \t--- Loss: 1.014\n",
      "Iteration: 96 \t--- Loss: 1.020\n",
      "Iteration: 97 \t--- Loss: 1.004\n",
      "Iteration: 98 \t--- Loss: 1.010\n",
      "Iteration: 99 \t--- Loss: 1.024\n",
      "Iteration: 100 \t--- Loss: 1.005\n",
      "Iteration: 101 \t--- Loss: 1.017\n",
      "Iteration: 102 \t--- Loss: 1.026\n",
      "Iteration: 103 \t--- Loss: 1.025\n",
      "Iteration: 104 \t--- Loss: 1.016\n",
      "Iteration: 105 \t--- Loss: 1.013\n",
      "Iteration: 106 \t--- Loss: 1.014\n",
      "Iteration: 107 \t--- Loss: 1.013\n",
      "Iteration: 108 \t--- Loss: 1.019\n",
      "Iteration: 109 \t--- Loss: 1.009\n",
      "Iteration: 110 \t--- Loss: 1.011\n",
      "Iteration: 111 \t--- Loss: 1.019\n",
      "Iteration: 112 \t--- Loss: 1.021\n",
      "Iteration: 113 \t--- Loss: 1.032\n",
      "Iteration: 114 \t--- Loss: 1.023\n",
      "Iteration: 115 \t--- Loss: 1.027\n",
      "Iteration: 116 \t--- Loss: 1.009\n",
      "Iteration: 117 \t--- Loss: 1.032\n",
      "Iteration: 118 \t--- Loss: 1.007\n",
      "Iteration: 119 \t--- Loss: 1.019\n",
      "Iteration: 120 \t--- Loss: 1.017\n",
      "Iteration: 121 \t--- Loss: 1.027\n",
      "Iteration: 122 \t--- Loss: 1.010\n",
      "Iteration: 123 \t--- Loss: 1.020\n",
      "Iteration: 124 \t--- Loss: 1.012\n",
      "Iteration: 125 \t--- Loss: 1.020\n",
      "Iteration: 126 \t--- Loss: 1.027\n",
      "Iteration: 127 \t--- Loss: 1.027\n",
      "Iteration: 128 \t--- Loss: 1.020\n",
      "Iteration: 129 \t--- Loss: 1.010\n",
      "Iteration: 130 \t--- Loss: 1.010\n",
      "Iteration: 131 \t--- Loss: 1.042\n",
      "Iteration: 132 \t--- Loss: 1.020\n",
      "Iteration: 133 \t--- Loss: 1.009\n",
      "Iteration: 134 \t--- Loss: 1.035\n",
      "Iteration: 135 \t--- Loss: 1.027\n",
      "Iteration: 136 \t--- Loss: 1.026\n",
      "Iteration: 137 \t--- Loss: 1.019\n",
      "Iteration: 138 \t--- Loss: 1.022\n",
      "Iteration: 139 \t--- Loss: 1.033\n",
      "Iteration: 140 \t--- Loss: 1.009\n",
      "Iteration: 141 \t--- Loss: 1.013\n",
      "Iteration: 142 \t--- Loss: 1.015\n",
      "Iteration: 143 \t--- Loss: 1.019\n",
      "Iteration: 144 \t--- Loss: 1.035\n",
      "Iteration: 145 \t--- Loss: 1.023\n",
      "Iteration: 146 \t--- Loss: 1.011\n",
      "Iteration: 147 \t--- Loss: 1.012\n",
      "Iteration: 148 \t--- Loss: 1.014\n",
      "Iteration: 149 \t--- Loss: 1.009\n",
      "Iteration: 150 \t--- Loss: 1.012\n",
      "Iteration: 151 \t--- Loss: 1.019\n",
      "Iteration: 152 \t--- Loss: 1.033\n",
      "Iteration: 153 \t--- Loss: 1.013\n",
      "Iteration: 154 \t--- Loss: 1.029\n",
      "Iteration: 155 \t--- Loss: 1.021\n",
      "Iteration: 156 \t--- Loss: 1.031\n",
      "Iteration: 157 \t--- Loss: 1.016\n",
      "Iteration: 158 \t--- Loss: 1.015\n",
      "Iteration: 159 \t--- Loss: 1.024\n",
      "Iteration: 160 \t--- Loss: 1.023\n",
      "Iteration: 161 \t--- Loss: 1.017\n",
      "Iteration: 162 \t--- Loss: 1.032\n",
      "Iteration: 163 \t--- Loss: 1.017\n",
      "Iteration: 164 \t--- Loss: 1.037\n",
      "Iteration: 165 \t--- Loss: 1.034\n",
      "Iteration: 166 \t--- Loss: 1.037\n",
      "Iteration: 167 \t--- Loss: 1.041\n",
      "Iteration: 168 \t--- Loss: 1.011\n",
      "Iteration: 169 \t--- Loss: 1.016\n",
      "Iteration: 170 \t--- Loss: 1.025\n",
      "Iteration: 171 \t--- Loss: 1.018\n",
      "Iteration: 172 \t--- Loss: 1.022\n",
      "Iteration: 173 \t--- Loss: 1.019\n",
      "Iteration: 174 \t--- Loss: 1.033\n",
      "Iteration: 175 \t--- Loss: 1.023\n",
      "Iteration: 176 \t--- Loss: 1.019\n",
      "Iteration: 177 \t--- Loss: 1.011\n",
      "Iteration: 178 \t--- Loss: 1.027\n",
      "Iteration: 179 \t--- Loss: 1.035\n",
      "Iteration: 180 \t--- Loss: 1.008\n",
      "Iteration: 181 \t--- Loss: 1.008\n",
      "Iteration: 182 \t--- Loss: 1.027\n",
      "Iteration: 183 \t--- Loss: 1.017\n",
      "Iteration: 184 \t--- Loss: 1.025\n",
      "Iteration: 185 \t--- Loss: 1.028\n",
      "Iteration: 186 \t--- Loss: 1.017\n",
      "Iteration: 187 \t--- Loss: 1.037\n",
      "Iteration: 188 \t--- Loss: 1.032\n",
      "Iteration: 189 \t--- Loss: 1.007\n",
      "Iteration: 190 \t--- Loss: 1.016\n",
      "Iteration: 191 \t--- Loss: 1.021\n",
      "Iteration: 192 \t--- Loss: 1.027\n",
      "Iteration: 193 \t--- Loss: 1.016\n",
      "Iteration: 194 \t--- Loss: 1.023\n",
      "Iteration: 195 \t--- Loss: 1.020\n",
      "Iteration: 196 \t--- Loss: 1.034\n",
      "Iteration: 197 \t--- Loss: 1.027\n",
      "Iteration: 198 \t--- Loss: 1.020\n",
      "Iteration: 199 \t--- Loss: 1.035\n",
      "Iteration: 200 \t--- Loss: 1.024\n",
      "Iteration: 201 \t--- Loss: 1.032\n",
      "Iteration: 202 \t--- Loss: 1.016\n",
      "Iteration: 203 \t--- Loss: 1.026\n",
      "Iteration: 204 \t--- Loss: 1.027\n",
      "Iteration: 205 \t--- Loss: 1.014\n",
      "Iteration: 206 \t--- Loss: 1.020\n",
      "Iteration: 207 \t--- Loss: 1.000\n",
      "Iteration: 208 \t--- Loss: 1.006\n",
      "Iteration: 209 \t--- Loss: 1.028\n",
      "Iteration: 210 \t--- Loss: 1.017\n",
      "Iteration: 211 \t--- Loss: 1.032\n",
      "Iteration: 212 \t--- Loss: 1.020\n",
      "Iteration: 213 \t--- Loss: 1.024\n",
      "Iteration: 214 \t--- Loss: 1.032\n",
      "Iteration: 215 \t--- Loss: 1.026\n",
      "Iteration: 216 \t--- Loss: 1.004\n",
      "Iteration: 217 \t--- Loss: 1.024\n",
      "Iteration: 218 \t--- Loss: 1.020\n",
      "Iteration: 219 \t--- Loss: 1.018\n",
      "Iteration: 220 \t--- Loss: 1.011\n",
      "Iteration: 221 \t--- Loss: 1.003\n",
      "Iteration: 222 \t--- Loss: 1.021\n",
      "Iteration: 223 \t--- Loss: 1.021\n",
      "Iteration: 224 \t--- Loss: 1.013\n",
      "Iteration: 225 \t--- Loss: 1.016\n",
      "Iteration: 226 \t--- Loss: 1.011\n",
      "Iteration: 227 \t--- Loss: 1.018\n",
      "Iteration: 228 \t--- Loss: 1.017\n",
      "Iteration: 229 \t--- Loss: 1.015\n",
      "Iteration: 230 \t--- Loss: 1.026\n",
      "Iteration: 231 \t--- Loss: 1.026\n",
      "Iteration: 232 \t--- Loss: 1.006\n",
      "Iteration: 233 \t--- Loss: 1.020\n",
      "Iteration: 234 \t--- Loss: 1.001\n",
      "Iteration: 235 \t--- Loss: 1.006\n",
      "Iteration: 236 \t--- Loss: 1.018\n",
      "Iteration: 237 \t--- Loss: 1.032\n",
      "Iteration: 238 \t--- Loss: 1.015\n",
      "Iteration: 239 \t--- Loss: 1.030\n",
      "Iteration: 240 \t--- Loss: 1.019\n",
      "Iteration: 241 \t--- Loss: 1.016\n",
      "Iteration: 242 \t--- Loss: 1.011\n",
      "Iteration: 243 \t--- Loss: 1.022\n",
      "Iteration: 244 \t--- Loss: 1.016\n",
      "Iteration: 245 \t--- Loss: 1.024\n",
      "Iteration: 246 \t--- Loss: 1.018\n",
      "Iteration: 247 \t--- Loss: 1.018\n",
      "Iteration: 248 \t--- Loss: 1.028\n",
      "Iteration: 249 \t--- Loss: 1.027\n",
      "Iteration: 250 \t--- Loss: 1.026\n",
      "Iteration: 251 \t--- Loss: 1.027\n",
      "Iteration: 252 \t--- Loss: 1.023\n",
      "Iteration: 253 \t--- Loss: 1.023\n",
      "Iteration: 254 \t--- Loss: 1.014\n",
      "Iteration: 255 \t--- Loss: 1.015\n",
      "Iteration: 256 \t--- Loss: 1.029\n",
      "Iteration: 257 \t--- Loss: 1.015\n",
      "Iteration: 258 \t--- Loss: 1.034\n",
      "Iteration: 259 \t--- Loss: 1.015"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [09:59<?, ?it/s][Parallel(n_jobs=5)]: Done  67 tasks      | elapsed: 42.5min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.427\n",
      "Iteration: 1 \t--- Loss: 0.391\n",
      "Iteration: 2 \t--- Loss: 0.388\n",
      "Iteration: 3 \t--- Loss: 0.377\n",
      "Iteration: 4 \t--- Loss: 0.366\n",
      "Iteration: 5 \t--- Loss: 0.353\n",
      "Iteration: 6 \t--- Loss: 0.360\n",
      "Iteration: 7 \t--- Loss: 0.343\n",
      "Iteration: 8 \t--- Loss: 0.323\n",
      "Iteration: 9 \t--- Loss: 0.335\n",
      "Iteration: 10 \t--- Loss: 0.329\n",
      "Iteration: 11 \t--- Loss: 0.306\n",
      "Iteration: 12 \t--- Loss: 0.305\n",
      "Iteration: 13 \t--- Loss: 0.305\n",
      "Iteration: 14 \t--- Loss: 0.292\n",
      "Iteration: 15 \t--- Loss: 0.296\n",
      "Iteration: 16 \t--- Loss: 0.277\n",
      "Iteration: 17 \t--- Loss: 0.284\n",
      "Iteration: 18 \t--- Loss: 0.269\n",
      "Iteration: 19 \t--- Loss: 0.280\n",
      "Iteration: 20 \t--- Loss: 0.263\n",
      "Iteration: 21 \t--- Loss: 0.247\n",
      "Iteration: 22 \t--- Loss: 0.260\n",
      "Iteration: 23 \t--- Loss: 0.253\n",
      "Iteration: 24 \t--- Loss: 0.241\n",
      "Iteration: 25 \t--- Loss: 0.232\n",
      "Iteration: 26 \t--- Loss: 0.233\n",
      "Iteration: 27 \t--- Loss: 0.211\n",
      "Iteration: 28 \t--- Loss: 0.202\n",
      "Iteration: 29 \t--- Loss: 0.201\n",
      "Iteration: 30 \t--- Loss: 0.195\n",
      "Iteration: 31 \t--- Loss: 0.191\n",
      "Iteration: 32 \t--- Loss: 0.203\n",
      "Iteration: 33 \t--- Loss: 0.225\n",
      "Iteration: 34 \t--- Loss: 0.465\n",
      "Iteration: 35 \t--- Loss: 0.736\n",
      "\n",
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:26<00:00, 86.11s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 1.027\n",
      "Iteration: 261 \t--- Loss: 1.003\n",
      "Iteration: 262 \t--- Loss: 1.018\n",
      "Iteration: 263 \t--- Loss: 1.021\n",
      "Iteration: 264 \t--- Loss: 1.023\n",
      "Iteration: 265 \t--- Loss: 1.036\n",
      "Iteration: 266 \t--- Loss: 1.019\n",
      "Iteration: 267 \t--- Loss: 1.024\n",
      "Iteration: 268 \t--- Loss: 1.011\n",
      "Iteration: 269 \t--- Loss: 1.020\n",
      "Iteration: 270 \t--- Loss: 1.008\n",
      "Iteration: 271 \t--- Loss: 1.024\n",
      "Iteration: 272 \t--- Loss: 1.023\n",
      "Iteration: 273 \t--- Loss: 1.019\n",
      "Iteration: 274 \t--- Loss: 1.018\n",
      "Iteration: 275 \t--- Loss: 1.031\n",
      "Iteration: 276 \t--- Loss: 1.034\n",
      "Iteration: 277 \t--- Loss: 1.031\n",
      "Iteration: 278 \t--- Loss: 1.019\n",
      "Iteration: 279 \t--- Loss: 1.030\n",
      "Iteration: 280 \t--- Loss: 1.024\n",
      "Iteration: 281 \t--- Loss: 1.026\n",
      "Iteration: 282 \t--- Loss: 1.004\n",
      "Iteration: 283 \t--- Loss: 1.022\n",
      "Iteration: 284 \t--- Loss: 1.022\n",
      "Iteration: 285 \t--- Loss: 1.032\n",
      "Iteration: 286 \t--- Loss: 1.010\n",
      "Iteration: 287 \t--- Loss: 1.014\n",
      "Iteration: 288 \t--- Loss: 1.024\n",
      "Iteration: 289 \t--- Loss: 1.014\n",
      "Iteration: 290 \t--- Loss: 1.027\n",
      "Iteration: 291 \t--- Loss: 1.022\n",
      "Iteration: 292 \t--- Loss: 1.024\n",
      "Iteration: 293 \t--- Loss: 1.021\n",
      "Iteration: 294 \t--- Loss: 1.015\n",
      "Iteration: 295 \t--- Loss: 1.031\n",
      "Iteration: 296 \t--- Loss: 1.017\n",
      "Iteration: 297 \t--- Loss: 1.032\n",
      "Iteration: 298 \t--- Loss: 1.022\n",
      "Iteration: 299 \t--- Loss: 1.025\n",
      "Iteration: 300 \t--- Loss: 1.026\n",
      "Iteration: 301 \t--- Loss: 1.028\n",
      "Iteration: 302 \t--- Loss: 1.018\n",
      "Iteration: 303 \t--- Loss: 1.015\n",
      "Iteration: 304 \t--- Loss: 1.024\n",
      "Iteration: 305 \t--- Loss: 1.021\n",
      "Iteration: 306 \t--- Loss: 1.017\n",
      "Iteration: 307 \t--- Loss: 1.012\n",
      "Iteration: 308 \t--- Loss: 1.037\n",
      "Iteration: 309 \t--- Loss: 1.017\n",
      "Iteration: 310 \t--- Loss: 1.028\n",
      "Iteration: 311 \t--- Loss: 1.014\n",
      "Iteration: 312 \t--- Loss: 1.007\n",
      "Iteration: 313 \t--- Loss: 1.010\n",
      "Iteration: 314 \t--- Loss: 1.029\n",
      "Iteration: 315 \t--- Loss: 1.031\n",
      "Iteration: 316 \t--- Loss: 1.028\n",
      "Iteration: 317 \t--- Loss: 1.014\n",
      "Iteration: 318 \t--- Loss: 1.029\n",
      "Iteration: 319 \t--- Loss: 1.013\n",
      "Iteration: 320 \t--- Loss: 1.029\n",
      "Iteration: 321 \t--- Loss: 0.999\n",
      "Iteration: 322 \t--- Loss: 1.022\n",
      "Iteration: 323 \t--- Loss: 1.009\n",
      "Iteration: 324 \t--- Loss: 1.032\n",
      "Iteration: 325 \t--- Loss: 1.027\n",
      "Iteration: 326 \t--- Loss: 1.017\n",
      "Iteration: 327 \t--- Loss: 1.016\n",
      "Iteration: 328 \t--- Loss: 1.007\n",
      "Iteration: 329 \t--- Loss: 1.020\n",
      "Iteration: 330 \t--- Loss: 1.037\n",
      "Iteration: 331 \t--- Loss: 1.026\n",
      "Iteration: 332 \t--- Loss: 1.007\n",
      "Iteration: 333 \t--- Loss: 1.014\n",
      "Iteration: 334 \t--- Loss: 1.026\n",
      "Iteration: 335 \t--- Loss: 1.018\n",
      "Iteration: 336 \t--- Loss: 1.023\n",
      "Iteration: 337 \t--- Loss: 1.036\n",
      "Iteration: 338 \t--- Loss: 1.013\n",
      "Iteration: 339 \t--- Loss: 1.028\n",
      "Iteration: 340 \t--- Loss: 1.019\n",
      "Iteration: 341 \t--- Loss: 1.016\n",
      "Iteration: 342 \t--- Loss: 1.026\n",
      "Iteration: 343 \t--- Loss: 1.021\n",
      "Iteration: 344 \t--- Loss: 1.021\n",
      "Iteration: 345 \t--- Loss: 1.023\n",
      "Iteration: 346 \t--- Loss: 1.028\n",
      "Iteration: 347 \t--- Loss: 1.019\n",
      "Iteration: 348 \t--- Loss: 1.016\n",
      "Iteration: 349 \t--- Loss: 1.004\n",
      "Iteration: 350 \t--- Loss: 1.012\n",
      "Iteration: 351 \t--- Loss: 1.022\n",
      "Iteration: 352 \t--- Loss: 1.013\n",
      "Iteration: 353 \t--- Loss: 1.028\n",
      "Iteration: 354 \t--- Loss: 1.015\n",
      "Iteration: 355 \t--- Loss: 1.022\n",
      "Iteration: 356 \t--- Loss: 1.036\n",
      "Iteration: 357 \t--- Loss: 1.015\n",
      "Iteration: 358 \t--- Loss: 1.018\n",
      "Iteration: 359 \t--- Loss: 1.007\n",
      "Iteration: 360 \t--- Loss: 1.014\n",
      "Iteration: 361 \t--- Loss: 1.013\n",
      "Iteration: 362 \t--- Loss: 1.014\n",
      "Iteration: 363 \t--- Loss: 1.036\n",
      "Iteration: 364 \t--- Loss: 1.022\n",
      "Iteration: 365 \t--- Loss: 1.010\n",
      "Iteration: 366 \t--- Loss: 1.017\n",
      "Iteration: 367 \t--- Loss: 1.034\n",
      "Iteration: 368 \t--- Loss: 1.004\n",
      "Iteration: 369 \t--- Loss: 1.011\n",
      "Iteration: 370 \t--- Loss: 1.032\n",
      "Iteration: 371 \t--- Loss: 1.017\n",
      "Iteration: 372 \t--- Loss: 1.030\n",
      "Iteration: 373 \t--- Loss: 1.021\n",
      "Iteration: 374 \t--- Loss: 1.031\n",
      "Iteration: 375 \t--- Loss: 1.009\n",
      "Iteration: 376 \t--- Loss: 1.014\n",
      "Iteration: 377 \t--- Loss: 1.013\n",
      "Iteration: 378 \t--- Loss: 1.001\n",
      "Iteration: 379 \t--- Loss: 1.010\n",
      "Iteration: 380 \t--- Loss: 1.015\n",
      "Iteration: 381 \t--- Loss: 1.013\n",
      "Iteration: 382 \t--- Loss: 1.024\n",
      "Iteration: 383 \t--- Loss: 1.017\n",
      "Iteration: 384 \t--- Loss: 1.021\n",
      "Iteration: 385 \t--- Loss: 1.018\n",
      "Iteration: 386 \t--- Loss: 1.043\n",
      "Iteration: 387 \t--- Loss: 1.019\n",
      "Iteration: 388 \t--- Loss: 1.029\n",
      "Iteration: 389 \t--- Loss: 1.023\n",
      "Iteration: 390 \t--- Loss: 1.023\n",
      "Iteration: 391 \t--- Loss: 1.026\n",
      "Iteration: 392 \t--- Loss: 1.018\n",
      "Iteration: 393 \t--- Loss: 1.015\n",
      "Iteration: 394 \t--- Loss: 1.027\n",
      "Iteration: 395 \t--- Loss: 1.007\n",
      "Iteration: 396 \t--- Loss: 1.030\n",
      "Iteration: 397 \t--- Loss: 1.025\n",
      "Iteration: 398 \t--- Loss: 1.012\n",
      "Iteration: 399 \t--- Loss: 1.028\n",
      "Iteration: 400 \t--- Loss: 1.023\n",
      "Iteration: 401 \t--- Loss: 1.016\n",
      "Iteration: 402 \t--- Loss: 1.028\n",
      "Iteration: 403 \t--- Loss: 1.023\n",
      "Iteration: 404 \t--- Loss: 1.025\n",
      "Iteration: 405 \t--- Loss: 1.023\n",
      "Iteration: 406 \t--- Loss: 1.024\n",
      "Iteration: 407 \t--- Loss: 1.013\n",
      "Iteration: 408 \t--- Loss: 1.009\n",
      "Iteration: 409 \t--- Loss: 1.017\n",
      "Iteration: 410 \t--- Loss: 1.019\n",
      "Iteration: 411 \t--- Loss: 1.009\n",
      "Iteration: 412 \t--- Loss: 1.023\n",
      "Iteration: 413 \t--- Loss: 1.024\n",
      "Iteration: 414 \t--- Loss: 1.024\n",
      "Iteration: 415 \t--- Loss: 1.025\n",
      "Iteration: 416 \t--- Loss: 1.016\n",
      "Iteration: 417 \t--- Loss: 1.014\n",
      "Iteration: 418 \t--- Loss: 1.004\n",
      "Iteration: 419 \t--- Loss: 1.007\n",
      "Iteration: 420 \t--- Loss: 1.023\n",
      "Iteration: 421 \t--- Loss: 1.022\n",
      "Iteration: 422 \t--- Loss: 1.018\n",
      "Iteration: 423 \t--- Loss: 1.013\n",
      "Iteration: 424 \t--- Loss: 1.023\n",
      "Iteration: 425 \t--- Loss: 1.024\n",
      "Iteration: 426 \t--- Loss: 1.009\n",
      "Iteration: 427 \t--- Loss: 1.026\n",
      "Iteration: 428 \t--- Loss: 1.028\n",
      "Iteration: 429 \t--- Loss: 1.024\n",
      "Iteration: 430 \t--- Loss: 1.011\n",
      "Iteration: 431 \t--- Loss: 1.004\n",
      "Iteration: 432 \t--- Loss: 1.011\n",
      "Iteration: 433 \t--- Loss: 1.022\n",
      "Iteration: 434 \t--- Loss: 1.031\n",
      "Iteration: 435 \t--- Loss: 0.998\n",
      "Iteration: 436 \t--- Loss: 1.014\n",
      "Iteration: 437 \t--- Loss: 1.027\n",
      "Iteration: 438 \t--- Loss: 1.002\n",
      "Iteration: 439 \t--- Loss: 1.015\n",
      "Iteration: 440 \t--- Loss: 1.018\n",
      "Iteration: 441 \t--- Loss: 1.019\n",
      "Iteration: 442 \t--- Loss: 1.027\n",
      "Iteration: 443 \t--- Loss: 1.012\n",
      "Iteration: 444 \t--- Loss: 1.019\n",
      "Iteration: 445 \t--- Loss: 1.033\n",
      "Iteration: 446 \t--- Loss: 1.028\n",
      "Iteration: 447 \t--- Loss: 1.026\n",
      "Iteration: 448 \t--- Loss: 1.009\n",
      "Iteration: 449 \t--- Loss: 1.012\n",
      "Iteration: 450 \t--- Loss: 1.023\n",
      "Iteration: 451 \t--- Loss: 1.032\n",
      "Iteration: 452 \t--- Loss: 1.019\n",
      "Iteration: 453 \t--- Loss: 1.017\n",
      "Iteration: 454 \t--- Loss: 1.022\n",
      "Iteration: 455 \t--- Loss: 1.026\n",
      "Iteration: 456 \t--- Loss: 1.026\n",
      "Iteration: 457 \t--- Loss: 1.022\n",
      "Iteration: 458 \t--- Loss: 1.013\n",
      "Iteration: 459 \t--- Loss: 1.024\n",
      "Iteration: 460 \t--- Loss: 1.019\n",
      "Iteration: 461 \t--- Loss: 1.018\n",
      "Iteration: 462 \t--- Loss: 1.014\n",
      "Iteration: 463 \t--- Loss: 1.022\n",
      "Iteration: 464 \t--- Loss: 1.034\n",
      "Iteration: 465 \t--- Loss: 1.020\n",
      "Iteration: 466 \t--- Loss: 1.009\n",
      "Iteration: 467 \t--- Loss: 1.023\n",
      "Iteration: 468 \t--- Loss: 1.019\n",
      "Iteration: 469 \t--- Loss: 1.011\n",
      "Iteration: 470 \t--- Loss: 1.020\n",
      "Iteration: 471 \t--- Loss: 1.019\n",
      "Iteration: 472 \t--- Loss: 1.001\n",
      "Iteration: 473 \t--- Loss: 1.018\n",
      "Iteration: 474 \t--- Loss: 1.013\n",
      "Iteration: 475 \t--- Loss: 1.010\n",
      "Iteration: 476 \t--- Loss: 1.007\n",
      "Iteration: 477 \t--- Loss: 1.009\n",
      "Iteration: 478 \t--- Loss: 1.024\n",
      "Iteration: 479 \t--- Loss: 1.013\n",
      "Iteration: 480 \t--- Loss: 1.038\n",
      "Iteration: 481 \t--- Loss: 1.020\n",
      "Iteration: 482 \t--- Loss: 1.007\n",
      "Iteration: 483 \t--- Loss: 1.048\n",
      "Iteration: 484 \t--- Loss: 1.018\n",
      "Iteration: 485 \t--- Loss: 1.014\n",
      "Iteration: 486 \t--- Loss: 1.019\n",
      "Iteration: 487 \t--- Loss: 1.024\n",
      "Iteration: 488 \t--- Loss: 1.019\n",
      "Iteration: 489 \t--- Loss: 1.010\n",
      "Iteration: 490 \t--- Loss: 1.019\n",
      "Iteration: 491 \t--- Loss: 1.035\n",
      "Iteration: 492 \t--- Loss: 1.029\n",
      "Iteration: 493 \t--- Loss: 1.031\n",
      "Iteration: 494 \t--- Loss: 1.024\n",
      "Iteration: 495 \t--- Loss: 1.010\n",
      "Iteration: 496 \t--- Loss: 1.030\n",
      "Iteration: 497 \t--- Loss: 1.009\n",
      "Iteration: 498 \t--- Loss: 1.017\n",
      "Iteration: 499 \t--- Loss: 1.019\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:11<00:03,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.296\n",
      "Iteration: 1 \t--- Loss: 0.272\n",
      "Iteration: 2 \t--- Loss: 0.264\n",
      "Iteration: 3 \t--- Loss: 0.246\n",
      "Iteration: 4 \t--- Loss: 0.233\n",
      "Iteration: 5 \t--- Loss: 0.231\n",
      "Iteration: 6 \t--- Loss: 0.222\n",
      "Iteration: 7 \t--- Loss: 0.207\n",
      "Iteration: 8 \t--- Loss: 0.200\n",
      "Iteration: 9 \t--- Loss: 0.210\n",
      "Iteration: 10 \t--- Loss: 0.188\n",
      "Iteration: 11 \t--- Loss: 0.194\n",
      "Iteration: 12 \t--- Loss: 0.183\n",
      "Iteration: 13 \t--- Loss: 0.176\n",
      "Iteration: 14 \t--- Loss: 0.174\n",
      "Iteration: 15 \t--- Loss: 0.175\n",
      "Iteration: 16 \t--- Loss: 0.169\n",
      "Iteration: 17 \t--- Loss: 0.166\n",
      "Iteration: 18 \t--- Loss: 0.169\n",
      "Iteration: 19 \t--- Loss: 0.158\n",
      "Iteration: 20 \t--- Loss: 0.158\n",
      "Iteration: 21 \t--- Loss: 0.164\n",
      "Iteration: 22 \t--- Loss: 0.162\n",
      "Iteration: 23 \t--- Loss: 0.169\n",
      "Iteration: 24 \t--- Loss: 0.162\n",
      "Iteration: 25 \t--- Loss: 0.154\n",
      "Iteration: 26 \t--- Loss: 0.152\n",
      "Iteration: 27 \t--- Loss: 0.161\n",
      "Iteration: 28 \t--- Loss: 0.148\n",
      "Iteration: 29 \t--- Loss: 0.155\n",
      "Iteration: 30 \t--- Loss: 0.153\n",
      "Iteration: 31 \t--- Loss: 0.151\n",
      "Iteration: 32 \t--- Loss: 0.162\n",
      "Iteration: 33 \t--- Loss: 0.159\n",
      "Iteration: 34 \t--- Loss: 0.153\n",
      "Iteration: 35 \t--- Loss: 0.151\n",
      "Iteration: 36 \t--- Loss: 0.159\n",
      "Iteration: 37 \t--- Loss: 0.147\n",
      "Iteration: 38 \t--- Loss: 0.146\n",
      "Iteration: 39 \t--- Loss: 0.150\n",
      "Iteration: 40 \t--- Loss: 0.158\n",
      "Iteration: 41 \t--- Loss: 0.138\n",
      "Iteration: 42 \t--- Loss: 0.142\n",
      "Iteration: 43 \t--- Loss: 0.154\n",
      "Iteration: 44 \t--- Loss: 0.154\n",
      "Iteration: 45 \t--- Loss: 0.141\n",
      "Iteration: 46 \t--- Loss: 0.146\n",
      "Iteration: 47 \t--- Loss: 0.144\n",
      "Iteration: 48 \t--- Loss: 0.142\n",
      "Iteration: 49 \t--- Loss: 0.149\n",
      "Iteration: 50 \t--- Loss: 0.130\n",
      "Iteration: 51 \t--- Loss: 0.148\n",
      "Iteration: 52 \t--- Loss: 0.141\n",
      "Iteration: 53 \t--- Loss: 0.145\n",
      "Iteration: 54 \t--- Loss: 0.152\n",
      "Iteration: 55 \t--- Loss: 0.143\n",
      "Iteration: 56 \t--- Loss: 0.143\n",
      "Iteration: 57 \t--- Loss: 0.140\n",
      "Iteration: 58 \t--- Loss: 0.148\n",
      "Iteration: 59 \t--- Loss: 0.141\n",
      "Iteration: 60 \t--- Loss: 0.144\n",
      "Iteration: 61 \t--- Loss: 0.138\n",
      "Iteration: 62 \t--- Loss: 0.149\n",
      "Iteration: 63 \t--- Loss: 0.149\n",
      "Iteration: 64 \t--- Loss: 0.145\n",
      "Iteration: 65 \t--- Loss: 0.144\n",
      "Iteration: 66 \t--- Loss: 0.141\n",
      "Iteration: 67 \t--- Loss: 0.142\n",
      "Iteration: 68 \t--- Loss: 0.141\n",
      "Iteration: 69 \t--- Loss: 0.150\n",
      "Iteration: 70 \t--- Loss: 0.142\n",
      "Iteration: 71 \t--- Loss: 0.135\n",
      "Iteration: 72 \t--- Loss: 0.141\n",
      "Iteration: 73 \t--- Loss: 0.142\n",
      "Iteration: 74 \t--- Loss: 0.139\n",
      "Iteration: 75 \t--- Loss: 0.141\n",
      "Iteration: 76 \t--- Loss: 0.135\n",
      "Iteration: 77 \t--- Loss: 0.147\n",
      "Iteration: 78 \t--- Loss: 0.142\n",
      "Iteration: 79 \t--- Loss: 0.143\n",
      "Iteration: 80 \t--- Loss: 0.136\n",
      "Iteration: 81 \t--- Loss: 0.154\n",
      "Iteration: 82 \t--- Loss: 0.142\n",
      "Iteration: 83 \t--- Loss: 0.137\n",
      "Iteration: 84 \t--- Loss: 0.145\n",
      "Iteration: 85 \t--- Loss: 0.143\n",
      "Iteration: 86 \t--- Loss: 0.142\n",
      "Iteration: 87 \t--- Loss: 0.139\n",
      "Iteration: 88 \t--- Loss: 0.139\n",
      "Iteration: 89 \t--- Loss: 0.144\n",
      "Iteration: 90 \t--- Loss: 0.138\n",
      "Iteration: 91 \t--- Loss: 0.147\n",
      "Iteration: 92 \t--- Loss: 0.140\n",
      "Iteration: 93 \t--- Loss: 0.137\n",
      "Iteration: 94 \t--- Loss: 0.141\n",
      "Iteration: 95 \t--- Loss: 0.132\n",
      "Iteration: 96 \t--- Loss: 0.133\n",
      "Iteration: 97 \t--- Loss: 0.134\n",
      "Iteration: 98 \t--- Loss: 0.140\n",
      "Iteration: 99 \t--- Loss: 0.138\n",
      "Iteration: 100 \t--- Loss: 0.138\n",
      "Iteration: 101 \t--- Loss: 0.143\n",
      "Iteration: 102 \t--- Loss: 0.135\n",
      "Iteration: 103 \t--- Loss: 0.148\n",
      "Iteration: 104 \t--- Loss: 0.133\n",
      "Iteration: 105 \t--- Loss: 0.145\n",
      "Iteration: 106 \t--- Loss: 0.138\n",
      "Iteration: 107 \t--- Loss: 0.140\n",
      "Iteration: 108 \t--- Loss: 0.142\n",
      "Iteration: 109 \t--- Loss: 0.142\n",
      "Iteration: 110 \t--- Loss: 0.138\n",
      "Iteration: 111 \t--- Loss: 0.147\n",
      "Iteration: 112 \t--- Loss: 0.131\n",
      "Iteration: 113 \t--- Loss: 0.130\n",
      "Iteration: 114 \t--- Loss: 0.140\n",
      "Iteration: 115 \t--- Loss: 0.138\n",
      "Iteration: 116 \t--- Loss: 0.134\n",
      "Iteration: 117 \t--- Loss: 0.141\n",
      "Iteration: 118 \t--- Loss: 0.141\n",
      "Iteration: 119 \t--- Loss: 0.146\n",
      "Iteration: 120 \t--- Loss: 0.133\n",
      "Iteration: 121 \t--- Loss: 0.146\n",
      "Iteration: 122 \t--- Loss: 0.152\n",
      "Iteration: 123 \t--- Loss: 0.143\n",
      "Iteration: 124 \t--- Loss: 0.150\n",
      "Iteration: 125 \t--- Loss: 0.125\n",
      "Iteration: 126 \t--- Loss: 0.147\n",
      "Iteration: 127 \t--- Loss: 0.129\n",
      "Iteration: 128 \t--- Loss: 0.140\n",
      "Iteration: 129 \t--- Loss: 0.130\n",
      "Iteration: 130 \t--- Loss: 0.135\n",
      "Iteration: 131 \t--- Loss: 0.150\n",
      "Iteration: 132 \t--- Loss: 0.146\n",
      "Iteration: 133 \t--- Loss: 0.140\n",
      "Iteration: 134 \t--- Loss: 0.134\n",
      "Iteration: 135 \t--- Loss: 0.147\n",
      "Iteration: 136 \t--- Loss: 0.147\n",
      "Iteration: 137 \t--- Loss: 0.136\n",
      "Iteration: 138 \t--- Loss: 0.132\n",
      "Iteration: 139 \t--- Loss: 0.140\n",
      "Iteration: 140 \t--- Loss: 0.136\n",
      "Iteration: 141 \t--- Loss: 0.145\n",
      "Iteration: 142 \t--- Loss: 0.137\n",
      "Iteration: 143 \t--- Loss: 0.141\n",
      "Iteration: 144 \t--- Loss: 0.131\n",
      "Iteration: 145 \t--- Loss: 0.150\n",
      "Iteration: 146 \t--- Loss: 0.141\n",
      "Iteration: 147 \t--- Loss: 0.141\n",
      "Iteration: 148 \t--- Loss: 0.146\n",
      "Iteration: 149 \t--- Loss: 0.142\n",
      "Iteration: 150 \t--- Loss: 0.141\n",
      "Iteration: 151 \t--- Loss: 0.144\n",
      "Iteration: 152 \t--- Loss: 0.136\n",
      "Iteration: 153 \t--- Loss: 0.134\n",
      "Iteration: 154 \t--- Loss: 0.145\n",
      "Iteration: 155 \t--- Loss: 0.145\n",
      "Iteration: 156 \t--- Loss: 0.152\n",
      "Iteration: 157 \t--- Loss: 0.143\n",
      "Iteration: 158 \t--- Loss: 0.131\n",
      "Iteration: 159 \t--- Loss: 0.140\n",
      "Iteration: 160 \t--- Loss: 0.137\n",
      "Iteration: 161 \t--- Loss: 0.131\n",
      "Iteration: 162 \t--- Loss: 0.139\n",
      "Iteration: 163 \t--- Loss: 0.138\n",
      "Iteration: 164 \t--- Loss: 0.142\n",
      "Iteration: 165 \t--- Loss: 0.148\n",
      "Iteration: 166 \t--- Loss: 0.132\n",
      "Iteration: 167 \t--- Loss: 0.143\n",
      "Iteration: 168 \t--- Loss: 0.143\n",
      "Iteration: 169 \t--- Loss: 0.136\n",
      "Iteration: 170 \t--- Loss: 0.135\n",
      "Iteration: 171 \t--- Loss: 0.147\n",
      "Iteration: 172 \t--- Loss: 0.137\n",
      "Iteration: 173 \t--- Loss: 0.139\n",
      "Iteration: 174 \t--- Loss: 0.141\n",
      "Iteration: 175 \t--- Loss: 0.135\n",
      "Iteration: 176 \t--- Loss: 0.141\n",
      "Iteration: 177 \t--- Loss: 0.135\n",
      "Iteration: 178 \t--- Loss: 0.133\n",
      "Iteration: 179 \t--- Loss: 0.136\n",
      "Iteration: 180 \t--- Loss: 0.149\n",
      "Iteration: 181 \t--- Loss: 0.144\n",
      "Iteration: 182 \t--- Loss: 0.136\n",
      "Iteration: 183 \t--- Loss: 0.146\n",
      "Iteration: 184 \t--- Loss: 0.139\n",
      "Iteration: 185 \t--- Loss: 0.142\n",
      "Iteration: 186 \t--- Loss: 0.135\n",
      "Iteration: 187 \t--- Loss: 0.139\n",
      "Iteration: 188 \t--- Loss: 0.123\n",
      "Iteration: 189 \t--- Loss: 0.132\n",
      "Iteration: 190 \t--- Loss: 0.140\n",
      "Iteration: 191 \t--- Loss: 0.131\n",
      "Iteration: 192 \t--- Loss: 0.133\n",
      "Iteration: 193 \t--- Loss: 0.134\n",
      "Iteration: 194 \t--- Loss: 0.138\n",
      "Iteration: 195 \t--- Loss: 0.143\n",
      "Iteration: 196 \t--- Loss: 0.133\n",
      "Iteration: 197 \t--- Loss: 0.135\n",
      "Iteration: 198 \t--- Loss: 0.147\n",
      "Iteration: 199 \t--- Loss: 0.129\n",
      "Iteration: 200 \t--- Loss: 0.138\n",
      "Iteration: 201 \t--- Loss: 0.138\n",
      "Iteration: 202 \t--- Loss: 0.136\n",
      "Iteration: 203 \t--- Loss: 0.139\n",
      "Iteration: 204 \t--- Loss: 0.141\n",
      "Iteration: 205 \t--- Loss: 0.141\n",
      "Iteration: 206 \t--- Loss: 0.132\n",
      "Iteration: 207 \t--- Loss: 0.138\n",
      "Iteration: 208 \t--- Loss: 0.141\n",
      "Iteration: 209 \t--- Loss: 0.149\n",
      "Iteration: 210 \t--- Loss: 0.134\n",
      "Iteration: 211 \t--- Loss: 0.144\n",
      "Iteration: 212 \t--- Loss: 0.144\n",
      "Iteration: 213 \t--- Loss: 0.138\n",
      "Iteration: 214 \t--- Loss: 0.142\n",
      "Iteration: 215 \t--- Loss: 0.137\n",
      "Iteration: 216 \t--- Loss: 0.124\n",
      "Iteration: 217 \t--- Loss: 0.137\n",
      "Iteration: 218 \t--- Loss: 0.145\n",
      "Iteration: 219 \t--- Loss: 0.146\n",
      "Iteration: 220 \t--- Loss: 0.135\n",
      "Iteration: 221 \t--- Loss: 0.143\n",
      "Iteration: 222 \t--- Loss: 0.134\n",
      "Iteration: 223 \t--- Loss: 0.131\n",
      "Iteration: 224 \t--- Loss: 0.139\n",
      "Iteration: 225 \t--- Loss: 0.146\n",
      "Iteration: 226 \t--- Loss: 0.140\n",
      "Iteration: 227 \t--- Loss: 0.140\n",
      "Iteration: 228 \t--- Loss: 0.139\n",
      "Iteration: 229 \t--- Loss: 0.135\n",
      "Iteration: 230 \t--- Loss: 0.125\n",
      "Iteration: 231 \t--- Loss: 0.145\n",
      "Iteration: 232 \t--- Loss: 0.135\n",
      "Iteration: 233 \t--- Loss: 0.137\n",
      "Iteration: 234 \t--- Loss: 0.133\n",
      "Iteration: 235 \t--- Loss: 0.135\n",
      "Iteration: 236 \t--- Loss: 0.133\n",
      "Iteration: 237 \t--- Loss: 0.139\n",
      "Iteration: 238 \t--- Loss: 0.143\n",
      "Iteration: 239 \t--- Loss: 0.143\n",
      "Iteration: 240 \t--- Loss: 0.141\n",
      "Iteration: 241 \t--- Loss: 0.133\n",
      "Iteration: 242 \t--- Loss: 0.146\n",
      "Iteration: 243 \t--- Loss: 0.143\n",
      "Iteration: 244 \t--- Loss: 0.147\n",
      "Iteration: 245 \t--- Loss: 0.142\n",
      "Iteration: 246 \t--- Loss: 0.139\n",
      "Iteration: 247 \t--- Loss: 0.140\n",
      "Iteration: 248 \t--- Loss: 0.143\n",
      "Iteration: 249 \t--- Loss: 0.132\n",
      "Iteration: 250 \t--- Loss: 0.145\n",
      "Iteration: 251 \t--- Loss: 0.141\n",
      "Iteration: 252 \t--- Loss: 0.139\n",
      "Iteration: 253 \t--- Loss: 0.126\n",
      "Iteration: 254 \t--- Loss: 0.144\n",
      "Iteration: 255 \t--- Loss: 0.135\n",
      "Iteration: 256 \t--- Loss: 0.140\n",
      "Iteration: 257 \t--- Loss: 0.136\n",
      "Iteration: 258 \t--- Loss: 0.140\n",
      "Iteration: 259 \t--- Loss: 0.133"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it][Parallel(n_jobs=5)]: Done  68 tasks      | elapsed: 43.3min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:25<00:00, 85.16s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.138\n",
      "Iteration: 261 \t--- Loss: 0.144\n",
      "Iteration: 262 \t--- Loss: 0.138\n",
      "Iteration: 263 \t--- Loss: 0.139\n",
      "Iteration: 264 \t--- Loss: 0.138\n",
      "Iteration: 265 \t--- Loss: 0.139\n",
      "Iteration: 266 \t--- Loss: 0.148\n",
      "Iteration: 267 \t--- Loss: 0.128\n",
      "Iteration: 268 \t--- Loss: 0.138\n",
      "Iteration: 269 \t--- Loss: 0.141\n",
      "Iteration: 270 \t--- Loss: 0.136\n",
      "Iteration: 271 \t--- Loss: 0.144\n",
      "Iteration: 272 \t--- Loss: 0.139\n",
      "Iteration: 273 \t--- Loss: 0.137\n",
      "Iteration: 274 \t--- Loss: 0.126\n",
      "Iteration: 275 \t--- Loss: 0.137\n",
      "Iteration: 276 \t--- Loss: 0.140\n",
      "Iteration: 277 \t--- Loss: 0.129\n",
      "Iteration: 278 \t--- Loss: 0.135\n",
      "Iteration: 279 \t--- Loss: 0.136\n",
      "Iteration: 280 \t--- Loss: 0.136\n",
      "Iteration: 281 \t--- Loss: 0.144\n",
      "Iteration: 282 \t--- Loss: 0.142\n",
      "Iteration: 283 \t--- Loss: 0.137\n",
      "Iteration: 284 \t--- Loss: 0.139\n",
      "Iteration: 285 \t--- Loss: 0.138\n",
      "Iteration: 286 \t--- Loss: 0.132\n",
      "Iteration: 287 \t--- Loss: 0.143\n",
      "Iteration: 288 \t--- Loss: 0.146\n",
      "Iteration: 289 \t--- Loss: 0.144\n",
      "Iteration: 290 \t--- Loss: 0.140\n",
      "Iteration: 291 \t--- Loss: 0.128\n",
      "Iteration: 292 \t--- Loss: 0.134\n",
      "Iteration: 293 \t--- Loss: 0.141\n",
      "Iteration: 294 \t--- Loss: 0.146\n",
      "Iteration: 295 \t--- Loss: 0.142\n",
      "Iteration: 296 \t--- Loss: 0.147\n",
      "Iteration: 297 \t--- Loss: 0.140\n",
      "Iteration: 298 \t--- Loss: 0.139\n",
      "Iteration: 299 \t--- Loss: 0.147\n",
      "Iteration: 300 \t--- Loss: 0.134\n",
      "Iteration: 301 \t--- Loss: 0.140\n",
      "Iteration: 302 \t--- Loss: 0.139\n",
      "Iteration: 303 \t--- Loss: 0.142\n",
      "Iteration: 304 \t--- Loss: 0.146\n",
      "Iteration: 305 \t--- Loss: 0.131\n",
      "Iteration: 306 \t--- Loss: 0.142\n",
      "Iteration: 307 \t--- Loss: 0.138\n",
      "Iteration: 308 \t--- Loss: 0.143\n",
      "Iteration: 309 \t--- Loss: 0.148\n",
      "Iteration: 310 \t--- Loss: 0.139\n",
      "Iteration: 311 \t--- Loss: 0.135\n",
      "Iteration: 312 \t--- Loss: 0.140\n",
      "Iteration: 313 \t--- Loss: 0.148\n",
      "Iteration: 314 \t--- Loss: 0.137\n",
      "Iteration: 315 \t--- Loss: 0.132\n",
      "Iteration: 316 \t--- Loss: 0.138\n",
      "Iteration: 317 \t--- Loss: 0.140\n",
      "Iteration: 318 \t--- Loss: 0.136\n",
      "Iteration: 319 \t--- Loss: 0.138\n",
      "Iteration: 320 \t--- Loss: 0.144\n",
      "Iteration: 321 \t--- Loss: 0.123\n",
      "Iteration: 322 \t--- Loss: 0.146\n",
      "Iteration: 323 \t--- Loss: 0.136\n",
      "Iteration: 324 \t--- Loss: 0.131\n",
      "Iteration: 325 \t--- Loss: 0.143\n",
      "Iteration: 326 \t--- Loss: 0.138\n",
      "Iteration: 327 \t--- Loss: 0.136\n",
      "Iteration: 328 \t--- Loss: 0.142\n",
      "Iteration: 329 \t--- Loss: 0.131\n",
      "Iteration: 330 \t--- Loss: 0.145\n",
      "Iteration: 331 \t--- Loss: 0.139\n",
      "Iteration: 332 \t--- Loss: 0.142\n",
      "Iteration: 333 \t--- Loss: 0.136\n",
      "Iteration: 334 \t--- Loss: 0.144\n",
      "Iteration: 335 \t--- Loss: 0.138\n",
      "Iteration: 336 \t--- Loss: 0.138\n",
      "Iteration: 337 \t--- Loss: 0.147\n",
      "Iteration: 338 \t--- Loss: 0.138\n",
      "Iteration: 339 \t--- Loss: 0.136\n",
      "Iteration: 340 \t--- Loss: 0.140\n",
      "Iteration: 341 \t--- Loss: 0.140\n",
      "Iteration: 342 \t--- Loss: 0.145\n",
      "Iteration: 343 \t--- Loss: 0.129\n",
      "Iteration: 344 \t--- Loss: 0.141\n",
      "Iteration: 345 \t--- Loss: 0.138\n",
      "Iteration: 346 \t--- Loss: 0.133\n",
      "Iteration: 347 \t--- Loss: 0.134\n",
      "Iteration: 348 \t--- Loss: 0.135\n",
      "Iteration: 349 \t--- Loss: 0.143\n",
      "Iteration: 350 \t--- Loss: 0.131\n",
      "Iteration: 351 \t--- Loss: 0.139\n",
      "Iteration: 352 \t--- Loss: 0.137\n",
      "Iteration: 353 \t--- Loss: 0.136\n",
      "Iteration: 354 \t--- Loss: 0.138\n",
      "Iteration: 355 \t--- Loss: 0.140\n",
      "Iteration: 356 \t--- Loss: 0.142\n",
      "Iteration: 357 \t--- Loss: 0.146\n",
      "Iteration: 358 \t--- Loss: 0.135\n",
      "Iteration: 359 \t--- Loss: 0.147\n",
      "Iteration: 360 \t--- Loss: 0.135\n",
      "Iteration: 361 \t--- Loss: 0.133\n",
      "Iteration: 362 \t--- Loss: 0.133\n",
      "Iteration: 363 \t--- Loss: 0.149\n",
      "Iteration: 364 \t--- Loss: 0.133\n",
      "Iteration: 365 \t--- Loss: 0.133\n",
      "Iteration: 366 \t--- Loss: 0.148\n",
      "Iteration: 367 \t--- Loss: 0.133\n",
      "Iteration: 368 \t--- Loss: 0.139\n",
      "Iteration: 369 \t--- Loss: 0.137\n",
      "Iteration: 370 \t--- Loss: 0.145\n",
      "Iteration: 371 \t--- Loss: 0.126\n",
      "Iteration: 372 \t--- Loss: 0.147\n",
      "Iteration: 373 \t--- Loss: 0.138\n",
      "Iteration: 374 \t--- Loss: 0.137\n",
      "Iteration: 375 \t--- Loss: 0.143\n",
      "Iteration: 376 \t--- Loss: 0.138\n",
      "Iteration: 377 \t--- Loss: 0.134\n",
      "Iteration: 378 \t--- Loss: 0.143\n",
      "Iteration: 379 \t--- Loss: 0.140\n",
      "Iteration: 380 \t--- Loss: 0.139\n",
      "Iteration: 381 \t--- Loss: 0.136\n",
      "Iteration: 382 \t--- Loss: 0.146\n",
      "Iteration: 383 \t--- Loss: 0.131\n",
      "Iteration: 384 \t--- Loss: 0.141\n",
      "Iteration: 385 \t--- Loss: 0.139\n",
      "Iteration: 386 \t--- Loss: 0.145\n",
      "Iteration: 387 \t--- Loss: 0.138\n",
      "Iteration: 388 \t--- Loss: 0.136\n",
      "Iteration: 389 \t--- Loss: 0.144\n",
      "Iteration: 390 \t--- Loss: 0.137\n",
      "Iteration: 391 \t--- Loss: 0.136\n",
      "Iteration: 392 \t--- Loss: 0.140\n",
      "Iteration: 393 \t--- Loss: 0.136\n",
      "Iteration: 394 \t--- Loss: 0.136\n",
      "Iteration: 395 \t--- Loss: 0.144\n",
      "Iteration: 396 \t--- Loss: 0.136\n",
      "Iteration: 397 \t--- Loss: 0.135\n",
      "Iteration: 398 \t--- Loss: 0.139\n",
      "Iteration: 399 \t--- Loss: 0.135\n",
      "Iteration: 400 \t--- Loss: 0.139\n",
      "Iteration: 401 \t--- Loss: 0.146\n",
      "Iteration: 402 \t--- Loss: 0.137\n",
      "Iteration: 403 \t--- Loss: 0.147\n",
      "Iteration: 404 \t--- Loss: 0.143\n",
      "Iteration: 405 \t--- Loss: 0.133\n",
      "Iteration: 406 \t--- Loss: 0.136\n",
      "Iteration: 407 \t--- Loss: 0.136\n",
      "Iteration: 408 \t--- Loss: 0.142\n",
      "Iteration: 409 \t--- Loss: 0.143\n",
      "Iteration: 410 \t--- Loss: 0.146\n",
      "Iteration: 411 \t--- Loss: 0.132\n",
      "Iteration: 412 \t--- Loss: 0.137\n",
      "Iteration: 413 \t--- Loss: 0.140\n",
      "Iteration: 414 \t--- Loss: 0.131\n",
      "Iteration: 415 \t--- Loss: 0.145\n",
      "Iteration: 416 \t--- Loss: 0.134\n",
      "Iteration: 417 \t--- Loss: 0.143\n",
      "Iteration: 418 \t--- Loss: 0.145\n",
      "Iteration: 419 \t--- Loss: 0.129\n",
      "Iteration: 420 \t--- Loss: 0.147\n",
      "Iteration: 421 \t--- Loss: 0.132\n",
      "Iteration: 422 \t--- Loss: 0.137\n",
      "Iteration: 423 \t--- Loss: 0.140\n",
      "Iteration: 424 \t--- Loss: 0.135\n",
      "Iteration: 425 \t--- Loss: 0.144\n",
      "Iteration: 426 \t--- Loss: 0.138\n",
      "Iteration: 427 \t--- Loss: 0.144\n",
      "Iteration: 428 \t--- Loss: 0.140\n",
      "Iteration: 429 \t--- Loss: 0.135\n",
      "Iteration: 430 \t--- Loss: 0.127\n",
      "Iteration: 431 \t--- Loss: 0.140\n",
      "Iteration: 432 \t--- Loss: 0.137\n",
      "Iteration: 433 \t--- Loss: 0.136\n",
      "Iteration: 434 \t--- Loss: 0.144\n",
      "Iteration: 435 \t--- Loss: 0.140\n",
      "Iteration: 436 \t--- Loss: 0.133\n",
      "Iteration: 437 \t--- Loss: 0.137\n",
      "Iteration: 438 \t--- Loss: 0.137\n",
      "Iteration: 439 \t--- Loss: 0.136\n",
      "Iteration: 440 \t--- Loss: 0.137\n",
      "Iteration: 441 \t--- Loss: 0.135\n",
      "Iteration: 442 \t--- Loss: 0.134\n",
      "Iteration: 443 \t--- Loss: 0.143\n",
      "Iteration: 444 \t--- Loss: 0.133\n",
      "Iteration: 445 \t--- Loss: 0.141\n",
      "Iteration: 446 \t--- Loss: 0.138\n",
      "Iteration: 447 \t--- Loss: 0.141\n",
      "Iteration: 448 \t--- Loss: 0.143\n",
      "Iteration: 449 \t--- Loss: 0.137\n",
      "Iteration: 450 \t--- Loss: 0.147\n",
      "Iteration: 451 \t--- Loss: 0.135\n",
      "Iteration: 452 \t--- Loss: 0.148\n",
      "Iteration: 453 \t--- Loss: 0.147\n",
      "Iteration: 454 \t--- Loss: 0.138\n",
      "Iteration: 455 \t--- Loss: 0.141\n",
      "Iteration: 456 \t--- Loss: 0.142\n",
      "Iteration: 457 \t--- Loss: 0.146\n",
      "Iteration: 458 \t--- Loss: 0.135\n",
      "Iteration: 459 \t--- Loss: 0.142\n",
      "Iteration: 460 \t--- Loss: 0.138\n",
      "Iteration: 461 \t--- Loss: 0.141\n",
      "Iteration: 462 \t--- Loss: 0.140\n",
      "Iteration: 463 \t--- Loss: 0.146\n",
      "Iteration: 464 \t--- Loss: 0.138\n",
      "Iteration: 465 \t--- Loss: 0.138\n",
      "Iteration: 466 \t--- Loss: 0.136\n",
      "Iteration: 467 \t--- Loss: 0.130\n",
      "Iteration: 468 \t--- Loss: 0.146\n",
      "Iteration: 469 \t--- Loss: 0.139\n",
      "Iteration: 470 \t--- Loss: 0.138\n",
      "Iteration: 471 \t--- Loss: 0.146\n",
      "Iteration: 472 \t--- Loss: 0.140\n",
      "Iteration: 473 \t--- Loss: 0.143\n",
      "Iteration: 474 \t--- Loss: 0.142\n",
      "Iteration: 475 \t--- Loss: 0.134\n",
      "Iteration: 476 \t--- Loss: 0.143\n",
      "Iteration: 477 \t--- Loss: 0.147\n",
      "Iteration: 478 \t--- Loss: 0.139\n",
      "Iteration: 479 \t--- Loss: 0.132\n",
      "Iteration: 480 \t--- Loss: 0.138\n",
      "Iteration: 481 \t--- Loss: 0.129\n",
      "Iteration: 482 \t--- Loss: 0.143\n",
      "Iteration: 483 \t--- Loss: 0.147\n",
      "Iteration: 484 \t--- Loss: 0.131\n",
      "Iteration: 485 \t--- Loss: 0.134\n",
      "Iteration: 486 \t--- Loss: 0.138\n",
      "Iteration: 487 \t--- Loss: 0.137\n",
      "Iteration: 488 \t--- Loss: 0.139\n",
      "Iteration: 489 \t--- Loss: 0.144\n",
      "Iteration: 490 \t--- Loss: 0.140\n",
      "Iteration: 491 \t--- Loss: 0.133\n",
      "Iteration: 492 \t--- Loss: 0.143\n",
      "Iteration: 493 \t--- Loss: 0.134\n",
      "Iteration: 494 \t--- Loss: 0.144\n",
      "Iteration: 495 \t--- Loss: 0.146\n",
      "Iteration: 496 \t--- Loss: 0.134\n",
      "Iteration: 497 \t--- Loss: 0.143\n",
      "Iteration: 498 \t--- Loss: 0.138\n",
      "Iteration: 499 \t--- Loss: 0.141\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:09,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 2.072\n",
      "Iteration: 1 \t--- Loss: 1.821\n",
      "Iteration: 2 \t--- Loss: 1.836\n",
      "Iteration: 3 \t--- Loss: 1.649\n",
      "Iteration: 4 \t--- Loss: 1.551\n",
      "Iteration: 5 \t--- Loss: 1.529\n",
      "Iteration: 6 \t--- Loss: 1.494\n",
      "Iteration: 7 \t--- Loss: 1.442\n",
      "Iteration: 8 \t--- Loss: 1.411\n",
      "Iteration: 9 \t--- Loss: 1.440\n",
      "Iteration: 10 \t--- Loss: 1.390\n",
      "Iteration: 11 \t--- Loss: 1.408\n",
      "Iteration: 12 \t--- Loss: 1.388\n",
      "Iteration: 13 \t--- Loss: 1.365\n",
      "Iteration: 14 \t--- Loss: 1.377\n",
      "Iteration: 15 \t--- Loss: 1.345\n",
      "Iteration: 16 \t--- Loss: 1.375\n",
      "Iteration: 17 \t--- Loss: 1.381\n",
      "Iteration: 18 \t--- Loss: 1.409\n",
      "Iteration: 19 \t--- Loss: 1.337\n",
      "Iteration: 20 \t--- Loss: 1.336\n",
      "Iteration: 21 \t--- Loss: 1.312\n",
      "Iteration: 22 \t--- Loss: 1.382\n",
      "Iteration: 23 \t--- Loss: 1.375\n",
      "Iteration: 24 \t--- Loss: 1.379\n",
      "Iteration: 25 \t--- Loss: 1.347\n",
      "Iteration: 26 \t--- Loss: 1.393\n",
      "Iteration: 27 \t--- Loss: 1.360\n",
      "Iteration: 28 \t--- Loss: 1.332\n",
      "Iteration: 29 \t--- Loss: 1.357\n",
      "Iteration: 30 \t--- Loss: 1.324\n",
      "Iteration: 31 \t--- Loss: 1.400\n",
      "Iteration: 32 \t--- Loss: 1.343\n",
      "Iteration: 33 \t--- Loss: 1.373\n",
      "Iteration: 34 \t--- Loss: 1.369\n",
      "Iteration: 35 \t--- Loss: 1.383\n",
      "Iteration: 36 \t--- Loss: 1.395\n",
      "Iteration: 37 \t--- Loss: 1.379\n",
      "Iteration: 38 \t--- Loss: 1.351\n",
      "Iteration: 39 \t--- Loss: 1.364\n",
      "Iteration: 40 \t--- Loss: 1.377\n",
      "Iteration: 41 \t--- Loss: 1.364\n",
      "Iteration: 42 \t--- Loss: 1.335\n",
      "Iteration: 43 \t--- Loss: 1.377\n",
      "Iteration: 44 \t--- Loss: 1.347\n",
      "Iteration: 45 \t--- Loss: 1.371\n",
      "Iteration: 46 \t--- Loss: 1.385\n",
      "Iteration: 47 \t--- Loss: 1.318\n",
      "Iteration: 48 \t--- Loss: 1.368\n",
      "Iteration: 49 \t--- Loss: 1.343\n",
      "Iteration: 50 \t--- Loss: 1.364\n",
      "Iteration: 51 \t--- Loss: 1.369\n",
      "Iteration: 52 \t--- Loss: 1.363\n",
      "Iteration: 53 \t--- Loss: 1.344\n",
      "Iteration: 54 \t--- Loss: 1.335\n",
      "Iteration: 55 \t--- Loss: 1.352\n",
      "Iteration: 56 \t--- Loss: 1.344\n",
      "Iteration: 57 \t--- Loss: 1.350\n",
      "Iteration: 58 \t--- Loss: 1.376\n",
      "Iteration: 59 \t--- Loss: 1.356\n",
      "Iteration: 60 \t--- Loss: 1.369\n",
      "Iteration: 61 \t--- Loss: 1.369\n",
      "Iteration: 62 \t--- Loss: 1.365\n",
      "Iteration: 63 \t--- Loss: 1.337\n",
      "Iteration: 64 \t--- Loss: 1.361\n",
      "Iteration: 65 \t--- Loss: 1.366\n",
      "Iteration: 66 \t--- Loss: 1.381\n",
      "Iteration: 67 \t--- Loss: 1.382\n",
      "Iteration: 68 \t--- Loss: 1.360\n",
      "Iteration: 69 \t--- Loss: 1.358\n",
      "Iteration: 70 \t--- Loss: 1.364\n",
      "Iteration: 71 \t--- Loss: 1.354\n",
      "Iteration: 72 \t--- Loss: 1.361\n",
      "Iteration: 73 \t--- Loss: 1.368\n",
      "Iteration: 74 \t--- Loss: 1.319\n",
      "Iteration: 75 \t--- Loss: 1.357\n",
      "Iteration: 76 \t--- Loss: 1.352\n",
      "Iteration: 77 \t--- Loss: 1.338\n",
      "Iteration: 78 \t--- Loss: 1.339\n",
      "Iteration: 79 \t--- Loss: 1.370\n",
      "Iteration: 80 \t--- Loss: 1.360\n",
      "Iteration: 81 \t--- Loss: 1.347\n",
      "Iteration: 82 \t--- Loss: 1.350\n",
      "Iteration: 83 \t--- Loss: 1.350\n",
      "Iteration: 84 \t--- Loss: 1.340\n",
      "Iteration: 85 \t--- Loss: 1.356\n",
      "Iteration: 86 \t--- Loss: 1.373\n",
      "Iteration: 87 \t--- Loss: 1.369\n",
      "Iteration: 88 \t--- Loss: 1.399\n",
      "Iteration: 89 \t--- Loss: 1.349\n",
      "Iteration: 90 \t--- Loss: 1.349\n",
      "Iteration: 91 \t--- Loss: 1.363\n",
      "Iteration: 92 \t--- Loss: 1.349\n",
      "Iteration: 93 \t--- Loss: 1.369\n",
      "Iteration: 94 \t--- Loss: 1.403\n",
      "Iteration: 95 \t--- Loss: 1.363\n",
      "Iteration: 96 \t--- Loss: 1.362\n",
      "Iteration: 97 \t--- Loss: 1.370\n",
      "Iteration: 98 \t--- Loss: 1.359\n",
      "Iteration: 99 \t--- Loss: 1.359\n",
      "Iteration: 100 \t--- Loss: 1.357\n",
      "Iteration: 101 \t--- Loss: 1.390\n",
      "Iteration: 102 \t--- Loss: 1.417\n",
      "Iteration: 103 \t--- Loss: 1.371\n",
      "Iteration: 104 \t--- Loss: 1.377\n",
      "Iteration: 105 \t--- Loss: 1.352\n",
      "Iteration: 106 \t--- Loss: 1.324\n",
      "Iteration: 107 \t--- Loss: 1.331\n",
      "Iteration: 108 \t--- Loss: 1.366\n",
      "Iteration: 109 \t--- Loss: 1.355\n",
      "Iteration: 110 \t--- Loss: 1.358\n",
      "Iteration: 111 \t--- Loss: 1.360\n",
      "Iteration: 112 \t--- Loss: 1.366\n",
      "Iteration: 113 \t--- Loss: 1.353\n",
      "Iteration: 114 \t--- Loss: 1.358\n",
      "Iteration: 115 \t--- Loss: 1.362\n",
      "Iteration: 116 \t--- Loss: 1.337\n",
      "Iteration: 117 \t--- Loss: 1.356\n",
      "Iteration: 118 \t--- Loss: 1.417\n",
      "Iteration: 119 \t--- Loss: 1.371\n",
      "Iteration: 120 \t--- Loss: 1.365\n",
      "Iteration: 121 \t--- Loss: 1.372\n",
      "Iteration: 122 \t--- Loss: 1.368\n",
      "Iteration: 123 \t--- Loss: 1.376\n",
      "Iteration: 124 \t--- Loss: 1.354\n",
      "Iteration: 125 \t--- Loss: 1.383\n",
      "Iteration: 126 \t--- Loss: 1.348\n",
      "Iteration: 127 \t--- Loss: 1.347\n",
      "Iteration: 128 \t--- Loss: 1.353\n",
      "Iteration: 129 \t--- Loss: 1.356\n",
      "Iteration: 130 \t--- Loss: 1.363\n",
      "Iteration: 131 \t--- Loss: 1.382\n",
      "Iteration: 132 \t--- Loss: 1.345\n",
      "Iteration: 133 \t--- Loss: 1.367\n",
      "Iteration: 134 \t--- Loss: 1.367\n",
      "Iteration: 135 \t--- Loss: 1.372\n",
      "Iteration: 136 \t--- Loss: 1.373\n",
      "Iteration: 137 \t--- Loss: 1.376\n",
      "Iteration: 138 \t--- Loss: 1.393\n",
      "Iteration: 139 \t--- Loss: 1.329\n",
      "Iteration: 140 \t--- Loss: 1.398\n",
      "Iteration: 141 \t--- Loss: 1.392\n",
      "Iteration: 142 \t--- Loss: 1.400\n",
      "Iteration: 143 \t--- Loss: 1.383\n",
      "Iteration: 144 \t--- Loss: 1.358\n",
      "Iteration: 145 \t--- Loss: 1.335\n",
      "Iteration: 146 \t--- Loss: 1.333\n",
      "Iteration: 147 \t--- Loss: 1.352\n",
      "Iteration: 148 \t--- Loss: 1.363\n",
      "Iteration: 149 \t--- Loss: 1.327\n",
      "Iteration: 150 \t--- Loss: 1.385\n",
      "Iteration: 151 \t--- Loss: 1.379\n",
      "Iteration: 152 \t--- Loss: 1.377\n",
      "Iteration: 153 \t--- Loss: 1.380\n",
      "Iteration: 154 \t--- Loss: 1.383\n",
      "Iteration: 155 \t--- Loss: 1.399\n",
      "Iteration: 156 \t--- Loss: 1.362\n",
      "Iteration: 157 \t--- Loss: 1.373\n",
      "Iteration: 158 \t--- Loss: 1.376\n",
      "Iteration: 159 \t--- Loss: 1.368\n",
      "Iteration: 160 \t--- Loss: 1.382\n",
      "Iteration: 161 \t--- Loss: 1.336\n",
      "Iteration: 162 \t--- Loss: 1.358\n",
      "Iteration: 163 \t--- Loss: 1.352\n",
      "Iteration: 164 \t--- Loss: 1.342\n",
      "Iteration: 165 \t--- Loss: 1.385\n",
      "Iteration: 166 \t--- Loss: 1.326\n",
      "Iteration: 167 \t--- Loss: 1.338\n",
      "Iteration: 168 \t--- Loss: 1.358\n",
      "Iteration: 169 \t--- Loss: 1.350\n",
      "Iteration: 170 \t--- Loss: 1.349\n",
      "Iteration: 171 \t--- Loss: 1.364\n",
      "Iteration: 172 \t--- Loss: 1.337\n",
      "Iteration: 173 \t--- Loss: 1.356\n",
      "Iteration: 174 \t--- Loss: 1.348\n",
      "Iteration: 175 \t--- Loss: 1.331\n",
      "Iteration: 176 \t--- Loss: 1.369\n",
      "Iteration: 177 \t--- Loss: 1.371\n",
      "Iteration: 178 \t--- Loss: 1.351\n",
      "Iteration: 179 \t--- Loss: 1.336\n",
      "Iteration: 180 \t--- Loss: 1.388\n",
      "Iteration: 181 \t--- Loss: 1.358\n",
      "Iteration: 182 \t--- Loss: 1.361\n",
      "Iteration: 183 \t--- Loss: 1.374\n",
      "Iteration: 184 \t--- Loss: 1.363\n",
      "Iteration: 185 \t--- Loss: 1.334\n",
      "Iteration: 186 \t--- Loss: 1.375\n",
      "Iteration: 187 \t--- Loss: 1.388\n",
      "Iteration: 188 \t--- Loss: 1.363\n",
      "Iteration: 189 \t--- Loss: 1.387\n",
      "Iteration: 190 \t--- Loss: 1.361\n",
      "Iteration: 191 \t--- Loss: 1.336\n",
      "Iteration: 192 \t--- Loss: 1.327\n",
      "Iteration: 193 \t--- Loss: 1.367\n",
      "Iteration: 194 \t--- Loss: 1.342\n",
      "Iteration: 195 \t--- Loss: 1.371\n",
      "Iteration: 196 \t--- Loss: 1.395\n",
      "Iteration: 197 \t--- Loss: 1.349\n",
      "Iteration: 198 \t--- Loss: 1.339\n",
      "Iteration: 199 \t--- Loss: 1.336\n",
      "Iteration: 200 \t--- Loss: 1.331\n",
      "Iteration: 201 \t--- Loss: 1.355\n",
      "Iteration: 202 \t--- Loss: 1.419\n",
      "Iteration: 203 \t--- Loss: 1.359\n",
      "Iteration: 204 \t--- Loss: 1.361\n",
      "Iteration: 205 \t--- Loss: 1.336\n",
      "Iteration: 206 \t--- Loss: 1.360\n",
      "Iteration: 207 \t--- Loss: 1.351\n",
      "Iteration: 208 \t--- Loss: 1.358\n",
      "Iteration: 209 \t--- Loss: 1.377\n",
      "Iteration: 210 \t--- Loss: 1.332\n",
      "Iteration: 211 \t--- Loss: 1.403\n",
      "Iteration: 212 \t--- Loss: 1.357\n",
      "Iteration: 213 \t--- Loss: 1.344\n",
      "Iteration: 214 \t--- Loss: 1.338\n",
      "Iteration: 215 \t--- Loss: 1.352\n",
      "Iteration: 216 \t--- Loss: 1.384\n",
      "Iteration: 217 \t--- Loss: 1.370\n",
      "Iteration: 218 \t--- Loss: 1.373\n",
      "Iteration: 219 \t--- Loss: 1.371\n",
      "Iteration: 220 \t--- Loss: 1.350\n",
      "Iteration: 221 \t--- Loss: 1.355\n",
      "Iteration: 222 \t--- Loss: 1.382\n",
      "Iteration: 223 \t--- Loss: 1.358\n",
      "Iteration: 224 \t--- Loss: 1.376\n",
      "Iteration: 225 \t--- Loss: 1.355\n",
      "Iteration: 226 \t--- Loss: 1.361\n",
      "Iteration: 227 \t--- Loss: 1.377\n",
      "Iteration: 228 \t--- Loss: 1.365\n",
      "Iteration: 229 \t--- Loss: 1.346\n",
      "Iteration: 230 \t--- Loss: 1.349\n",
      "Iteration: 231 \t--- Loss: 1.395\n",
      "Iteration: 232 \t--- Loss: 1.372\n",
      "Iteration: 233 \t--- Loss: 1.374\n",
      "Iteration: 234 \t--- Loss: 1.336\n",
      "Iteration: 235 \t--- Loss: 1.334\n",
      "Iteration: 236 \t--- Loss: 1.367\n",
      "Iteration: 237 \t--- Loss: 1.376\n",
      "Iteration: 238 \t--- Loss: 1.360\n",
      "Iteration: 239 \t--- Loss: 1.394\n",
      "Iteration: 240 \t--- Loss: 1.367\n",
      "Iteration: 241 \t--- Loss: 1.380\n",
      "Iteration: 242 \t--- Loss: 1.308\n",
      "Iteration: 243 \t--- Loss: 1.341\n",
      "Iteration: 244 \t--- Loss: 1.366\n",
      "Iteration: 245 \t--- Loss: 1.377\n",
      "Iteration: 246 \t--- Loss: 1.339\n",
      "Iteration: 247 \t--- Loss: 1.374\n",
      "Iteration: 248 \t--- Loss: 1.340\n",
      "Iteration: 249 \t--- Loss: 1.373\n",
      "Iteration: 250 \t--- Loss: 1.379\n",
      "Iteration: 251 \t--- Loss: 1.332\n",
      "Iteration: 252 \t--- Loss: 1.383\n",
      "Iteration: 253 \t--- Loss: 1.368\n",
      "Iteration: 254 \t--- Loss: 1.369\n",
      "Iteration: 255 \t--- Loss: 1.358\n",
      "Iteration: 256 \t--- Loss: 1.338\n",
      "Iteration: 257 \t--- Loss: 1.381\n",
      "Iteration: 258 \t--- Loss: 1.375\n",
      "Iteration: 259 \t--- Loss: 1.382"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.50s/it][Parallel(n_jobs=5)]: Done  69 tasks      | elapsed: 44.2min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.45s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.25s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 1.397\n",
      "Iteration: 261 \t--- Loss: 1.441\n",
      "Iteration: 262 \t--- Loss: 1.347\n",
      "Iteration: 263 \t--- Loss: 1.345\n",
      "Iteration: 264 \t--- Loss: 1.360\n",
      "Iteration: 265 \t--- Loss: 1.336\n",
      "Iteration: 266 \t--- Loss: 1.358\n",
      "Iteration: 267 \t--- Loss: 1.306\n",
      "Iteration: 268 \t--- Loss: 1.357\n",
      "Iteration: 269 \t--- Loss: 1.296\n",
      "Iteration: 270 \t--- Loss: 1.355\n",
      "Iteration: 271 \t--- Loss: 1.356\n",
      "Iteration: 272 \t--- Loss: 1.354\n",
      "Iteration: 273 \t--- Loss: 1.326\n",
      "Iteration: 274 \t--- Loss: 1.332\n",
      "Iteration: 275 \t--- Loss: 1.393\n",
      "Iteration: 276 \t--- Loss: 1.354\n",
      "Iteration: 277 \t--- Loss: 1.323\n",
      "Iteration: 278 \t--- Loss: 1.355\n",
      "Iteration: 279 \t--- Loss: 1.367\n",
      "Iteration: 280 \t--- Loss: 1.374\n",
      "Iteration: 281 \t--- Loss: 1.355\n",
      "Iteration: 282 \t--- Loss: 1.371\n",
      "Iteration: 283 \t--- Loss: 1.372\n",
      "Iteration: 284 \t--- Loss: 1.410\n",
      "Iteration: 285 \t--- Loss: 1.381\n",
      "Iteration: 286 \t--- Loss: 1.379\n",
      "Iteration: 287 \t--- Loss: 1.368\n",
      "Iteration: 288 \t--- Loss: 1.374\n",
      "Iteration: 289 \t--- Loss: 1.356\n",
      "Iteration: 290 \t--- Loss: 1.354\n",
      "Iteration: 291 \t--- Loss: 1.385\n",
      "Iteration: 292 \t--- Loss: 1.326\n",
      "Iteration: 293 \t--- Loss: 1.372\n",
      "Iteration: 294 \t--- Loss: 1.361\n",
      "Iteration: 295 \t--- Loss: 1.391\n",
      "Iteration: 296 \t--- Loss: 1.374\n",
      "Iteration: 297 \t--- Loss: 1.351\n",
      "Iteration: 298 \t--- Loss: 1.398\n",
      "Iteration: 299 \t--- Loss: 1.360\n",
      "Iteration: 300 \t--- Loss: 1.385\n",
      "Iteration: 301 \t--- Loss: 1.356\n",
      "Iteration: 302 \t--- Loss: 1.314\n",
      "Iteration: 303 \t--- Loss: 1.363\n",
      "Iteration: 304 \t--- Loss: 1.370\n",
      "Iteration: 305 \t--- Loss: 1.368\n",
      "Iteration: 306 \t--- Loss: 1.367\n",
      "Iteration: 307 \t--- Loss: 1.382\n",
      "Iteration: 308 \t--- Loss: 1.348\n",
      "Iteration: 309 \t--- Loss: 1.355\n",
      "Iteration: 310 \t--- Loss: 1.366\n",
      "Iteration: 311 \t--- Loss: 1.383\n",
      "Iteration: 312 \t--- Loss: 1.342\n",
      "Iteration: 313 \t--- Loss: 1.339\n",
      "Iteration: 314 \t--- Loss: 1.356\n",
      "Iteration: 315 \t--- Loss: 1.386\n",
      "Iteration: 316 \t--- Loss: 1.375\n",
      "Iteration: 317 \t--- Loss: 1.401\n",
      "Iteration: 318 \t--- Loss: 1.338\n",
      "Iteration: 319 \t--- Loss: 1.397\n",
      "Iteration: 320 \t--- Loss: 1.353\n",
      "Iteration: 321 \t--- Loss: 1.354\n",
      "Iteration: 322 \t--- Loss: 1.349\n",
      "Iteration: 323 \t--- Loss: 1.362\n",
      "Iteration: 324 \t--- Loss: 1.360\n",
      "Iteration: 325 \t--- Loss: 1.414\n",
      "Iteration: 326 \t--- Loss: 1.380\n",
      "Iteration: 327 \t--- Loss: 1.362\n",
      "Iteration: 328 \t--- Loss: 1.381\n",
      "Iteration: 329 \t--- Loss: 1.368\n",
      "Iteration: 330 \t--- Loss: 1.343\n",
      "Iteration: 331 \t--- Loss: 1.349\n",
      "Iteration: 332 \t--- Loss: 1.352\n",
      "Iteration: 333 \t--- Loss: 1.351\n",
      "Iteration: 334 \t--- Loss: 1.359\n",
      "Iteration: 335 \t--- Loss: 1.370\n",
      "Iteration: 336 \t--- Loss: 1.351\n",
      "Iteration: 337 \t--- Loss: 1.426\n",
      "Iteration: 338 \t--- Loss: 1.359\n",
      "Iteration: 339 \t--- Loss: 1.352\n",
      "Iteration: 340 \t--- Loss: 1.357\n",
      "Iteration: 341 \t--- Loss: 1.359\n",
      "Iteration: 342 \t--- Loss: 1.375\n",
      "Iteration: 343 \t--- Loss: 1.372\n",
      "Iteration: 344 \t--- Loss: 1.338\n",
      "Iteration: 345 \t--- Loss: 1.352\n",
      "Iteration: 346 \t--- Loss: 1.390\n",
      "Iteration: 347 \t--- Loss: 1.327\n",
      "Iteration: 348 \t--- Loss: 1.351\n",
      "Iteration: 349 \t--- Loss: 1.395\n",
      "Iteration: 350 \t--- Loss: 1.373\n",
      "Iteration: 351 \t--- Loss: 1.342\n",
      "Iteration: 352 \t--- Loss: 1.366\n",
      "Iteration: 353 \t--- Loss: 1.314\n",
      "Iteration: 354 \t--- Loss: 1.386\n",
      "Iteration: 355 \t--- Loss: 1.369\n",
      "Iteration: 356 \t--- Loss: 1.358\n",
      "Iteration: 357 \t--- Loss: 1.320\n",
      "Iteration: 358 \t--- Loss: 1.330\n",
      "Iteration: 359 \t--- Loss: 1.363\n",
      "Iteration: 360 \t--- Loss: 1.358\n",
      "Iteration: 361 \t--- Loss: 1.361\n",
      "Iteration: 362 \t--- Loss: 1.361\n",
      "Iteration: 363 \t--- Loss: 1.328\n",
      "Iteration: 364 \t--- Loss: 1.381\n",
      "Iteration: 365 \t--- Loss: 1.369\n",
      "Iteration: 366 \t--- Loss: 1.358\n",
      "Iteration: 367 \t--- Loss: 1.389\n",
      "Iteration: 368 \t--- Loss: 1.393\n",
      "Iteration: 369 \t--- Loss: 1.361\n",
      "Iteration: 370 \t--- Loss: 1.362\n",
      "Iteration: 371 \t--- Loss: 1.348\n",
      "Iteration: 372 \t--- Loss: 1.386\n",
      "Iteration: 373 \t--- Loss: 1.359\n",
      "Iteration: 374 \t--- Loss: 1.366\n",
      "Iteration: 375 \t--- Loss: 1.379\n",
      "Iteration: 376 \t--- Loss: 1.394\n",
      "Iteration: 377 \t--- Loss: 1.380\n",
      "Iteration: 378 \t--- Loss: 1.341\n",
      "Iteration: 379 \t--- Loss: 1.390\n",
      "Iteration: 380 \t--- Loss: 1.373\n",
      "Iteration: 381 \t--- Loss: 1.352\n",
      "Iteration: 382 \t--- Loss: 1.384\n",
      "Iteration: 383 \t--- Loss: 1.367\n",
      "Iteration: 384 \t--- Loss: 1.381\n",
      "Iteration: 385 \t--- Loss: 1.419\n",
      "Iteration: 386 \t--- Loss: 1.377\n",
      "Iteration: 387 \t--- Loss: 1.359\n",
      "Iteration: 388 \t--- Loss: 1.361\n",
      "Iteration: 389 \t--- Loss: 1.386\n",
      "Iteration: 390 \t--- Loss: 1.371\n",
      "Iteration: 391 \t--- Loss: 1.359\n",
      "Iteration: 392 \t--- Loss: 1.385\n",
      "Iteration: 393 \t--- Loss: 1.333\n",
      "Iteration: 394 \t--- Loss: 1.385\n",
      "Iteration: 395 \t--- Loss: 1.320\n",
      "Iteration: 396 \t--- Loss: 1.357\n",
      "Iteration: 397 \t--- Loss: 1.339\n",
      "Iteration: 398 \t--- Loss: 1.369\n",
      "Iteration: 399 \t--- Loss: 1.330\n",
      "Iteration: 400 \t--- Loss: 1.385\n",
      "Iteration: 401 \t--- Loss: 1.404\n",
      "Iteration: 402 \t--- Loss: 1.381\n",
      "Iteration: 403 \t--- Loss: 1.349\n",
      "Iteration: 404 \t--- Loss: 1.351\n",
      "Iteration: 405 \t--- Loss: 1.364\n",
      "Iteration: 406 \t--- Loss: 1.395\n",
      "Iteration: 407 \t--- Loss: 1.381\n",
      "Iteration: 408 \t--- Loss: 1.367\n",
      "Iteration: 409 \t--- Loss: 1.372\n",
      "Iteration: 410 \t--- Loss: 1.360\n",
      "Iteration: 411 \t--- Loss: 1.337\n",
      "Iteration: 412 \t--- Loss: 1.382\n",
      "Iteration: 413 \t--- Loss: 1.341\n",
      "Iteration: 414 \t--- Loss: 1.389\n",
      "Iteration: 415 \t--- Loss: 1.376\n",
      "Iteration: 416 \t--- Loss: 1.366\n",
      "Iteration: 417 \t--- Loss: 1.389\n",
      "Iteration: 418 \t--- Loss: 1.352\n",
      "Iteration: 419 \t--- Loss: 1.356\n",
      "Iteration: 420 \t--- Loss: 1.408\n",
      "Iteration: 421 \t--- Loss: 1.381\n",
      "Iteration: 422 \t--- Loss: 1.330\n",
      "Iteration: 423 \t--- Loss: 1.377\n",
      "Iteration: 424 \t--- Loss: 1.424\n",
      "Iteration: 425 \t--- Loss: 1.375\n",
      "Iteration: 426 \t--- Loss: 1.344\n",
      "Iteration: 427 \t--- Loss: 1.380\n",
      "Iteration: 428 \t--- Loss: 1.348\n",
      "Iteration: 429 \t--- Loss: 1.352\n",
      "Iteration: 430 \t--- Loss: 1.354\n",
      "Iteration: 431 \t--- Loss: 1.355\n",
      "Iteration: 432 \t--- Loss: 1.342\n",
      "Iteration: 433 \t--- Loss: 1.347\n",
      "Iteration: 434 \t--- Loss: 1.356\n",
      "Iteration: 435 \t--- Loss: 1.316\n",
      "Iteration: 436 \t--- Loss: 1.349\n",
      "Iteration: 437 \t--- Loss: 1.404\n",
      "Iteration: 438 \t--- Loss: 1.343\n",
      "Iteration: 439 \t--- Loss: 1.323\n",
      "Iteration: 440 \t--- Loss: 1.375\n",
      "Iteration: 441 \t--- Loss: 1.400\n",
      "Iteration: 442 \t--- Loss: 1.356\n",
      "Iteration: 443 \t--- Loss: 1.361\n",
      "Iteration: 444 \t--- Loss: 1.332\n",
      "Iteration: 445 \t--- Loss: 1.347\n",
      "Iteration: 446 \t--- Loss: 1.372\n",
      "Iteration: 447 \t--- Loss: 1.400\n",
      "Iteration: 448 \t--- Loss: 1.354\n",
      "Iteration: 449 \t--- Loss: 1.343\n",
      "Iteration: 450 \t--- Loss: 1.346\n",
      "Iteration: 451 \t--- Loss: 1.365\n",
      "Iteration: 452 \t--- Loss: 1.337\n",
      "Iteration: 453 \t--- Loss: 1.335\n",
      "Iteration: 454 \t--- Loss: 1.366\n",
      "Iteration: 455 \t--- Loss: 1.389\n",
      "Iteration: 456 \t--- Loss: 1.352\n",
      "Iteration: 457 \t--- Loss: 1.387\n",
      "Iteration: 458 \t--- Loss: 1.348\n",
      "Iteration: 459 \t--- Loss: 1.381\n",
      "Iteration: 460 \t--- Loss: 1.355\n",
      "Iteration: 461 \t--- Loss: 1.376\n",
      "Iteration: 462 \t--- Loss: 1.402\n",
      "Iteration: 463 \t--- Loss: 1.363\n",
      "Iteration: 464 \t--- Loss: 1.376\n",
      "Iteration: 465 \t--- Loss: 1.355\n",
      "Iteration: 466 \t--- Loss: 1.364\n",
      "Iteration: 467 \t--- Loss: 1.364\n",
      "Iteration: 468 \t--- Loss: 1.392\n",
      "Iteration: 469 \t--- Loss: 1.373\n",
      "Iteration: 470 \t--- Loss: 1.343\n",
      "Iteration: 471 \t--- Loss: 1.359\n",
      "Iteration: 472 \t--- Loss: 1.354\n",
      "Iteration: 473 \t--- Loss: 1.329\n",
      "Iteration: 474 \t--- Loss: 1.389\n",
      "Iteration: 475 \t--- Loss: 1.350\n",
      "Iteration: 476 \t--- Loss: 1.356\n",
      "Iteration: 477 \t--- Loss: 1.319\n",
      "Iteration: 478 \t--- Loss: 1.369\n",
      "Iteration: 479 \t--- Loss: 1.370\n",
      "Iteration: 480 \t--- Loss: 1.359\n",
      "Iteration: 481 \t--- Loss: 1.388\n",
      "Iteration: 482 \t--- Loss: 1.369\n",
      "Iteration: 483 \t--- Loss: 1.380\n",
      "Iteration: 484 \t--- Loss: 1.306\n",
      "Iteration: 485 \t--- Loss: 1.382\n",
      "Iteration: 486 \t--- Loss: 1.354\n",
      "Iteration: 487 \t--- Loss: 1.394\n",
      "Iteration: 488 \t--- Loss: 1.375\n",
      "Iteration: 489 \t--- Loss: 1.394\n",
      "Iteration: 490 \t--- Loss: 1.349\n",
      "Iteration: 491 \t--- Loss: 1.343\n",
      "Iteration: 492 \t--- Loss: 1.347\n",
      "Iteration: 493 \t--- Loss: 1.326\n",
      "Iteration: 494 \t--- Loss: 1.371\n",
      "Iteration: 495 \t--- Loss: 1.349\n",
      "Iteration: 496 \t--- Loss: 1.408\n",
      "Iteration: 497 \t--- Loss: 1.381\n",
      "Iteration: 498 \t--- Loss: 1.353\n",
      "Iteration: 499 \t--- Loss: 1.375\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:11<00:03,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 2.223\n",
      "Iteration: 1 \t--- Loss: 2.037\n",
      "Iteration: 2 \t--- Loss: 1.942\n",
      "Iteration: 3 \t--- Loss: 1.809\n",
      "Iteration: 4 \t--- Loss: 1.669\n",
      "Iteration: 5 \t--- Loss: 1.612\n",
      "Iteration: 6 \t--- Loss: 1.652\n",
      "Iteration: 7 \t--- Loss: 1.608\n",
      "Iteration: 8 \t--- Loss: 1.567\n",
      "Iteration: 9 \t--- Loss: 1.579\n",
      "Iteration: 10 \t--- Loss: 1.555\n",
      "Iteration: 11 \t--- Loss: 1.555\n",
      "Iteration: 12 \t--- Loss: 1.500\n",
      "Iteration: 13 \t--- Loss: 1.559\n",
      "Iteration: 14 \t--- Loss: 1.451\n",
      "Iteration: 15 \t--- Loss: 1.507\n",
      "Iteration: 16 \t--- Loss: 1.493\n",
      "Iteration: 17 \t--- Loss: 1.549\n",
      "Iteration: 18 \t--- Loss: 1.533\n",
      "Iteration: 19 \t--- Loss: 1.530\n",
      "Iteration: 20 \t--- Loss: 1.475\n",
      "Iteration: 21 \t--- Loss: 1.454\n",
      "Iteration: 22 \t--- Loss: 1.479\n",
      "Iteration: 23 \t--- Loss: 1.571\n",
      "Iteration: 24 \t--- Loss: 1.478\n",
      "Iteration: 25 \t--- Loss: 1.522\n",
      "Iteration: 26 \t--- Loss: 1.543\n",
      "Iteration: 27 \t--- Loss: 1.538\n",
      "Iteration: 28 \t--- Loss: 1.532\n",
      "Iteration: 29 \t--- Loss: 1.543\n",
      "Iteration: 30 \t--- Loss: 1.495\n",
      "Iteration: 31 \t--- Loss: 1.459\n",
      "Iteration: 32 \t--- Loss: 1.469\n",
      "Iteration: 33 \t--- Loss: 1.530\n",
      "Iteration: 34 \t--- Loss: 1.463\n",
      "Iteration: 35 \t--- Loss: 1.514\n",
      "Iteration: 36 \t--- Loss: 1.560\n",
      "Iteration: 37 \t--- Loss: 1.552\n",
      "Iteration: 38 \t--- Loss: 1.514\n",
      "Iteration: 39 \t--- Loss: 1.545\n",
      "Iteration: 40 \t--- Loss: 1.495\n",
      "Iteration: 41 \t--- Loss: 1.475\n",
      "Iteration: 42 \t--- Loss: 1.489\n",
      "Iteration: 43 \t--- Loss: 1.482\n",
      "Iteration: 44 \t--- Loss: 1.458\n",
      "Iteration: 45 \t--- Loss: 1.528\n",
      "Iteration: 46 \t--- Loss: 1.462\n",
      "Iteration: 47 \t--- Loss: 1.475\n",
      "Iteration: 48 \t--- Loss: 1.509\n",
      "Iteration: 49 \t--- Loss: 1.460\n",
      "Iteration: 50 \t--- Loss: 1.480\n",
      "Iteration: 51 \t--- Loss: 1.513\n",
      "Iteration: 52 \t--- Loss: 1.480\n",
      "Iteration: 53 \t--- Loss: 1.482\n",
      "Iteration: 54 \t--- Loss: 1.463\n",
      "Iteration: 55 \t--- Loss: 1.487\n",
      "Iteration: 56 \t--- Loss: 1.506\n",
      "Iteration: 57 \t--- Loss: 1.460\n",
      "Iteration: 58 \t--- Loss: 1.530\n",
      "Iteration: 59 \t--- Loss: 1.489\n",
      "Iteration: 60 \t--- Loss: 1.474\n",
      "Iteration: 61 \t--- Loss: 1.486\n",
      "Iteration: 62 \t--- Loss: 1.523\n",
      "Iteration: 63 \t--- Loss: 1.537\n",
      "Iteration: 64 \t--- Loss: 1.518\n",
      "Iteration: 65 \t--- Loss: 1.454\n",
      "Iteration: 66 \t--- Loss: 1.537\n",
      "Iteration: 67 \t--- Loss: 1.495\n",
      "Iteration: 68 \t--- Loss: 1.489\n",
      "Iteration: 69 \t--- Loss: 1.444\n",
      "Iteration: 70 \t--- Loss: 1.496\n",
      "Iteration: 71 \t--- Loss: 1.498\n",
      "Iteration: 72 \t--- Loss: 1.493\n",
      "Iteration: 73 \t--- Loss: 1.438\n",
      "Iteration: 74 \t--- Loss: 1.493\n",
      "Iteration: 75 \t--- Loss: 1.547\n",
      "Iteration: 76 \t--- Loss: 1.494\n",
      "Iteration: 77 \t--- Loss: 1.530\n",
      "Iteration: 78 \t--- Loss: 1.476\n",
      "Iteration: 79 \t--- Loss: 1.495\n",
      "Iteration: 80 \t--- Loss: 1.495\n",
      "Iteration: 81 \t--- Loss: 1.445\n",
      "Iteration: 82 \t--- Loss: 1.510\n",
      "Iteration: 83 \t--- Loss: 1.490\n",
      "Iteration: 84 \t--- Loss: 1.491\n",
      "Iteration: 85 \t--- Loss: 1.543\n",
      "Iteration: 86 \t--- Loss: 1.537\n",
      "Iteration: 87 \t--- Loss: 1.473\n",
      "Iteration: 88 \t--- Loss: 1.537\n",
      "Iteration: 89 \t--- Loss: 1.496\n",
      "Iteration: 90 \t--- Loss: 1.502\n",
      "Iteration: 91 \t--- Loss: 1.519\n",
      "Iteration: 92 \t--- Loss: 1.463\n",
      "Iteration: 93 \t--- Loss: 1.477\n",
      "Iteration: 94 \t--- Loss: 1.506\n",
      "Iteration: 95 \t--- Loss: 1.498\n",
      "Iteration: 96 \t--- Loss: 1.529\n",
      "Iteration: 97 \t--- Loss: 1.575\n",
      "Iteration: 98 \t--- Loss: 1.447\n",
      "Iteration: 99 \t--- Loss: 1.485\n",
      "Iteration: 100 \t--- Loss: 1.450\n",
      "Iteration: 101 \t--- Loss: 1.488\n",
      "Iteration: 102 \t--- Loss: 1.503\n",
      "Iteration: 103 \t--- Loss: 1.492\n",
      "Iteration: 104 \t--- Loss: 1.521\n",
      "Iteration: 105 \t--- Loss: 1.542\n",
      "Iteration: 106 \t--- Loss: 1.478\n",
      "Iteration: 107 \t--- Loss: 1.513\n",
      "Iteration: 108 \t--- Loss: 1.513\n",
      "Iteration: 109 \t--- Loss: 1.530\n",
      "Iteration: 110 \t--- Loss: 1.491\n",
      "Iteration: 111 \t--- Loss: 1.527\n",
      "Iteration: 112 \t--- Loss: 1.503\n",
      "Iteration: 113 \t--- Loss: 1.527\n",
      "Iteration: 114 \t--- Loss: 1.529\n",
      "Iteration: 115 \t--- Loss: 1.487\n",
      "Iteration: 116 \t--- Loss: 1.519\n",
      "Iteration: 117 \t--- Loss: 1.461\n",
      "Iteration: 118 \t--- Loss: 1.491\n",
      "Iteration: 119 \t--- Loss: 1.484\n",
      "Iteration: 120 \t--- Loss: 1.536\n",
      "Iteration: 121 \t--- Loss: 1.487\n",
      "Iteration: 122 \t--- Loss: 1.486\n",
      "Iteration: 123 \t--- Loss: 1.473\n",
      "Iteration: 124 \t--- Loss: 1.528\n",
      "Iteration: 125 \t--- Loss: 1.475\n",
      "Iteration: 126 \t--- Loss: 1.556\n",
      "Iteration: 127 \t--- Loss: 1.492\n",
      "Iteration: 128 \t--- Loss: 1.503\n",
      "Iteration: 129 \t--- Loss: 1.488\n",
      "Iteration: 130 \t--- Loss: 1.503\n",
      "Iteration: 131 \t--- Loss: 1.505\n",
      "Iteration: 132 \t--- Loss: 1.569\n",
      "Iteration: 133 \t--- Loss: 1.554\n",
      "Iteration: 134 \t--- Loss: 1.496\n",
      "Iteration: 135 \t--- Loss: 1.485\n",
      "Iteration: 136 \t--- Loss: 1.432\n",
      "Iteration: 137 \t--- Loss: 1.513\n",
      "Iteration: 138 \t--- Loss: 1.515\n",
      "Iteration: 139 \t--- Loss: 1.467\n",
      "Iteration: 140 \t--- Loss: 1.462\n",
      "Iteration: 141 \t--- Loss: 1.494\n",
      "Iteration: 142 \t--- Loss: 1.468\n",
      "Iteration: 143 \t--- Loss: 1.519\n",
      "Iteration: 144 \t--- Loss: 1.476\n",
      "Iteration: 145 \t--- Loss: 1.503\n",
      "Iteration: 146 \t--- Loss: 1.514\n",
      "Iteration: 147 \t--- Loss: 1.488\n",
      "Iteration: 148 \t--- Loss: 1.481\n",
      "Iteration: 149 \t--- Loss: 1.473\n",
      "Iteration: 150 \t--- Loss: 1.525\n",
      "Iteration: 151 \t--- Loss: 1.441\n",
      "Iteration: 152 \t--- Loss: 1.472\n",
      "Iteration: 153 \t--- Loss: 1.453\n",
      "Iteration: 154 \t--- Loss: 1.542\n",
      "Iteration: 155 \t--- Loss: 1.560\n",
      "Iteration: 156 \t--- Loss: 1.542\n",
      "Iteration: 157 \t--- Loss: 1.551\n",
      "Iteration: 158 \t--- Loss: 1.458\n",
      "Iteration: 159 \t--- Loss: 1.462\n",
      "Iteration: 160 \t--- Loss: 1.506\n",
      "Iteration: 161 \t--- Loss: 1.493\n",
      "Iteration: 162 \t--- Loss: 1.533\n",
      "Iteration: 163 \t--- Loss: 1.540\n",
      "Iteration: 164 \t--- Loss: 1.491\n",
      "Iteration: 165 \t--- Loss: 1.508\n",
      "Iteration: 166 \t--- Loss: 1.554\n",
      "Iteration: 167 \t--- Loss: 1.465\n",
      "Iteration: 168 \t--- Loss: 1.532\n",
      "Iteration: 169 \t--- Loss: 1.494\n",
      "Iteration: 170 \t--- Loss: 1.575\n",
      "Iteration: 171 \t--- Loss: 1.509\n",
      "Iteration: 172 \t--- Loss: 1.529\n",
      "Iteration: 173 \t--- Loss: 1.489\n",
      "Iteration: 174 \t--- Loss: 1.541\n",
      "Iteration: 175 \t--- Loss: 1.486\n",
      "Iteration: 176 \t--- Loss: 1.526\n",
      "Iteration: 177 \t--- Loss: 1.519\n",
      "Iteration: 178 \t--- Loss: 1.536\n",
      "Iteration: 179 \t--- Loss: 1.552\n",
      "Iteration: 180 \t--- Loss: 1.524\n",
      "Iteration: 181 \t--- Loss: 1.490\n",
      "Iteration: 182 \t--- Loss: 1.484\n",
      "Iteration: 183 \t--- Loss: 1.499\n",
      "Iteration: 184 \t--- Loss: 1.470\n",
      "Iteration: 185 \t--- Loss: 1.487\n",
      "Iteration: 186 \t--- Loss: 1.488\n",
      "Iteration: 187 \t--- Loss: 1.527\n",
      "Iteration: 188 \t--- Loss: 1.510\n",
      "Iteration: 189 \t--- Loss: 1.532\n",
      "Iteration: 190 \t--- Loss: 1.512\n",
      "Iteration: 191 \t--- Loss: 1.483\n",
      "Iteration: 192 \t--- Loss: 1.475\n",
      "Iteration: 193 \t--- Loss: 1.507\n",
      "Iteration: 194 \t--- Loss: 1.471\n",
      "Iteration: 195 \t--- Loss: 1.526\n",
      "Iteration: 196 \t--- Loss: 1.519\n",
      "Iteration: 197 \t--- Loss: 1.480\n",
      "Iteration: 198 \t--- Loss: 1.521\n",
      "Iteration: 199 \t--- Loss: 1.528\n",
      "Iteration: 200 \t--- Loss: 1.505\n",
      "Iteration: 201 \t--- Loss: 1.485\n",
      "Iteration: 202 \t--- Loss: 1.502\n",
      "Iteration: 203 \t--- Loss: 1.490\n",
      "Iteration: 204 \t--- Loss: 1.496\n",
      "Iteration: 205 \t--- Loss: 1.470\n",
      "Iteration: 206 \t--- Loss: 1.455\n",
      "Iteration: 207 \t--- Loss: 1.506\n",
      "Iteration: 208 \t--- Loss: 1.479\n",
      "Iteration: 209 \t--- Loss: 1.450\n",
      "Iteration: 210 \t--- Loss: 1.492\n",
      "Iteration: 211 \t--- Loss: 1.423\n",
      "Iteration: 212 \t--- Loss: 1.484\n",
      "Iteration: 213 \t--- Loss: 1.518\n",
      "Iteration: 214 \t--- Loss: 1.480\n",
      "Iteration: 215 \t--- Loss: 1.471\n",
      "Iteration: 216 \t--- Loss: 1.461\n",
      "Iteration: 217 \t--- Loss: 1.491\n",
      "Iteration: 218 \t--- Loss: 1.505\n",
      "Iteration: 219 \t--- Loss: 1.498\n",
      "Iteration: 220 \t--- Loss: 1.519\n",
      "Iteration: 221 \t--- Loss: 1.504\n",
      "Iteration: 222 \t--- Loss: 1.520\n",
      "Iteration: 223 \t--- Loss: 1.508\n",
      "Iteration: 224 \t--- Loss: 1.519\n",
      "Iteration: 225 \t--- Loss: 1.561\n",
      "Iteration: 226 \t--- Loss: 1.520\n",
      "Iteration: 227 \t--- Loss: 1.515\n",
      "Iteration: 228 \t--- Loss: 1.554\n",
      "Iteration: 229 \t--- Loss: 1.535\n",
      "Iteration: 230 \t--- Loss: 1.497\n",
      "Iteration: 231 \t--- Loss: 1.481\n",
      "Iteration: 232 \t--- Loss: 1.495\n",
      "Iteration: 233 \t--- Loss: 1.543\n",
      "Iteration: 234 \t--- Loss: 1.514\n",
      "Iteration: 235 \t--- Loss: 1.524\n",
      "Iteration: 236 \t--- Loss: 1.568\n",
      "Iteration: 237 \t--- Loss: 1.473\n",
      "Iteration: 238 \t--- Loss: 1.499\n",
      "Iteration: 239 \t--- Loss: 1.523\n",
      "Iteration: 240 \t--- Loss: 1.501\n",
      "Iteration: 241 \t--- Loss: 1.477\n",
      "Iteration: 242 \t--- Loss: 1.502\n",
      "Iteration: 243 \t--- Loss: 1.541\n",
      "Iteration: 244 \t--- Loss: 1.498\n",
      "Iteration: 245 \t--- Loss: 1.526\n",
      "Iteration: 246 \t--- Loss: 1.523\n",
      "Iteration: 247 \t--- Loss: 1.540\n",
      "Iteration: 248 \t--- Loss: 1.517\n",
      "Iteration: 249 \t--- Loss: 1.504\n",
      "Iteration: 250 \t--- Loss: 1.553\n",
      "Iteration: 251 \t--- Loss: 1.488\n",
      "Iteration: 252 \t--- Loss: 1.490\n",
      "Iteration: 253 \t--- Loss: 1.438\n",
      "Iteration: 254 \t--- Loss: 1.477\n",
      "Iteration: 255 \t--- Loss: 1.499\n",
      "Iteration: 256 \t--- Loss: 1.486\n",
      "Iteration: 257 \t--- Loss: 1.494\n",
      "Iteration: 258 \t--- Loss: 1.522\n",
      "Iteration: 259 \t--- Loss: 1.476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.55s/it][Parallel(n_jobs=5)]: Done  70 tasks      | elapsed: 45.0min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.306\n",
      "Iteration: 1 \t--- Loss: 0.299\n",
      "Iteration: 2 \t--- Loss: 0.311\n",
      "Iteration: 3 \t--- Loss: 0.298\n",
      "Iteration: 4 \t--- Loss: 0.296\n",
      "Iteration: 5 \t--- Loss: 0.285\n",
      "Iteration: 6 \t--- Loss: 0.295\n",
      "Iteration: 7 \t--- Loss: 0.281\n",
      "Iteration: 8 \t--- Loss: 0.287\n",
      "Iteration: 9 \t--- Loss: 0.294\n",
      "Iteration: 10 \t--- Loss: 0.286\n",
      "Iteration: 11 \t--- Loss: 0.263\n",
      "Iteration: 12 \t--- Loss: 0.277\n",
      "Iteration: 13 \t--- Loss: 0.276\n",
      "Iteration: 14 \t--- Loss: 0.277\n",
      "Iteration: 15 \t--- Loss: 0.269\n",
      "Iteration: 16 \t--- Loss: 0.257\n",
      "Iteration: 17 \t--- Loss: 0.263\n",
      "Iteration: 18 \t--- Loss: 0.266\n",
      "Iteration: 19 \t--- Loss: 0.263\n",
      "Iteration: 20 \t--- Loss: 0.246\n",
      "Iteration: 21 \t--- Loss: 0.269\n",
      "Iteration: 22 \t--- Loss: 0.242\n",
      "Iteration: 23 \t--- Loss: 0.249\n",
      "Iteration: 24 \t--- Loss: 0.258\n",
      "Iteration: 25 \t--- Loss: 0.272\n",
      "Iteration: 26 \t--- Loss: 0.232\n",
      "Iteration: 27 \t--- Loss: 0.244\n",
      "Iteration: 28 \t--- Loss: 0.242\n",
      "Iteration: 29 \t--- Loss: 0.226\n",
      "Iteration: 30 \t--- Loss: 0.209\n",
      "Iteration: 31 \t--- Loss: 0.204\n",
      "Iteration: 32 \t--- Loss: 0.207\n",
      "Iteration: 33 \t--- Loss: 0.182\n",
      "Iteration: 34 \t--- Loss: 0.209\n",
      "Iteration: 35 \t--- Loss: 0.202\n",
      "Iteration: 36 \t--- Loss: 0.226\n",
      "Iteration: 37 \t--- Loss: 0.248\n",
      "Iteration: 38 \t--- Loss: 0.199\n",
      "Iteration: 39 \t--- Loss: 0.205\n",
      "Iteration: 40 \t--- Loss: 0.300\n",
      "Iteration: 41 \t--- Loss: 0.368\n",
      "Iteration: 42 \t--- Loss: 0.364\n",
      "Iteration: 43 \t--- Loss: 0.343\n",
      "Iteration: 44 \t--- Loss: 0.335\n",
      "Iteration: 45 \t--- Loss: 0.345\n",
      "Iteration: 46 \t--- Loss: 0.318\n",
      "Iteration: 47 \t--- Loss: 0.330\n",
      "Iteration: 48 \t--- Loss: 0.311\n",
      "Iteration: 49 \t--- Loss: 0.303\n",
      "Iteration: 50 \t--- Loss: 0.303\n",
      "Iteration: 51 \t--- Loss: 0.307\n",
      "Iteration: 52 \t--- Loss: 0.274\n",
      "Iteration: 53 \t--- Loss: 0.279\n",
      "Iteration: 54 \t--- Loss: 0.266\n",
      "Iteration: 55 \t--- Loss: 0.273\n",
      "Iteration: 56 \t--- Loss: 0.262\n",
      "Iteration: 57 \t--- Loss: 0.269\n",
      "Iteration: 58 \t--- Loss: 0.261\n",
      "Iteration: 59 \t--- Loss: 0.262\n",
      "Iteration: 60 \t--- Loss: 0.262\n",
      "Iteration: 61 \t--- Loss: 0.245\n",
      "Iteration: 62 \t--- Loss: 0.263\n",
      "Iteration: 63 \t--- Loss: 0.252\n",
      "Iteration: 64 \t--- Loss: 0.238\n",
      "Iteration: 65 \t--- Loss: 0.251\n",
      "Iteration: 66 \t--- Loss: 0.242\n",
      "Iteration: 67 \t--- Loss: 0.267\n",
      "Iteration: 68 \t--- Loss: 0.235\n",
      "Iteration: 69 \t--- Loss: 0.218\n",
      "Iteration: 70 \t--- Loss: 0.240\n",
      "Iteration: 71 \t--- Loss: 0.240\n",
      "Iteration: 72 \t--- Loss: 0.231\n",
      "Iteration: 73 \t--- Loss: 0.236\n",
      "Iteration: 74 \t--- Loss: 0.235\n",
      "Iteration: 75 \t--- Loss: 0.215\n",
      "Iteration: 76 \t--- Loss: 0.220\n",
      "Iteration: 77 \t--- Loss: 0.227\n",
      "Iteration: 78 \t--- Loss: 0.213\n",
      "Iteration: 79 \t--- Loss: 0.207\n",
      "Iteration: 80 \t--- Loss: 0.219\n",
      "Iteration: 81 \t--- Loss: 0.211\n",
      "Iteration: 82 \t--- Loss: 0.193\n",
      "Iteration: 83 \t--- Loss: 0.188\n",
      "Iteration: 84 \t--- Loss: 0.171\n",
      "Iteration: 85 \t--- Loss: 0.191\n",
      "Iteration: 86 \t--- Loss: 0.170\n",
      "Iteration: 87 \t--- Loss: 0.168\n",
      "Iteration: 88 \t--- Loss: 0.148\n",
      "Iteration: 89 \t--- Loss: 0.161\n",
      "Iteration: 90 \t--- Loss: 0.153\n",
      "Iteration: 91 \t--- Loss: 0.148\n",
      "Iteration: 92 \t--- Loss: 0.150\n",
      "Iteration: 93 \t--- Loss: 0.168\n",
      "Iteration: 94 \t--- Loss: 0.167\n",
      "Iteration: 95 \t--- Loss: 0.148\n",
      "Iteration: 96 \t--- Loss: 0.152\n",
      "Iteration: 97 \t--- Loss: 0.155\n",
      "Iteration: 98 \t--- Loss: 0.171\n",
      "Iteration: 99 \t--- Loss: 0.162\n",
      "Iteration: 100 \t--- Loss: 0.153\n",
      "Iteration: 101 \t--- Loss: 0.133\n",
      "Iteration: 102 \t--- Loss: 0.150\n",
      "Iteration: 103 \t--- Loss: 0.148\n",
      "Iteration: 104 \t--- Loss: 0.146\n",
      "Iteration: 105 \t--- Loss: 0.148\n",
      "Iteration: 106 \t--- Loss: 0.142\n",
      "Iteration: 107 \t--- Loss: 0.149\n",
      "Iteration: 108 \t--- Loss: 0.156\n",
      "Iteration: 109 \t--- Loss: 0.148\n",
      "Iteration: 110 \t--- Loss: 0.166\n",
      "Iteration: 111 \t--- Loss: 0.149\n",
      "Iteration: 112 \t--- Loss: 0.198\n",
      "Iteration: 113 \t--- Loss: 0.232\n",
      "Iteration: 114 \t--- Loss: 0.193\n",
      "Iteration: 115 \t--- Loss: 0.147\n",
      "Iteration: 116 \t--- Loss: 0.140\n",
      "Iteration: 117 \t--- Loss: 0.133\n",
      "Iteration: 118 \t--- Loss: 0.159\n",
      "Iteration: 119 \t--- Loss: 0.184\n",
      "Iteration: 120 \t--- Loss: 0.157\n",
      "Iteration: 121 \t--- Loss: 0.186\n",
      "Iteration: 122 \t--- Loss: 0.136\n",
      "Iteration: 123 \t--- Loss: 0.139\n",
      "Iteration: 124 \t--- Loss: 0.149\n",
      "Iteration: 125 \t--- Loss: 0.200\n",
      "Iteration: 126 \t--- Loss: 0.142\n",
      "Iteration: 127 \t--- Loss: 0.136\n",
      "Iteration: 128 \t--- Loss: 0.148\n",
      "Iteration: 129 \t--- Loss: 0.166\n",
      "Iteration: 130 \t--- Loss: 0.148\n",
      "Iteration: 131 \t--- Loss: 0.154\n",
      "Iteration: 132 \t--- Loss: 0.128\n",
      "Iteration: 133 \t--- Loss: 0.133\n",
      "Iteration: 134 \t--- Loss: 0.159\n",
      "Iteration: 135 \t--- Loss: 0.188\n",
      "Iteration: 136 \t--- Loss: 0.131\n",
      "Iteration: 137 \t--- Loss: 0.139\n",
      "Iteration: 138 \t--- Loss: 0.157\n",
      "Iteration: 139 \t--- Loss: 0.126\n",
      "Iteration: 140 \t--- Loss: 0.124\n",
      "Iteration: 141 \t--- Loss: 0.148\n",
      "Iteration: 142 \t--- Loss: 0.152\n",
      "Iteration: 143 \t--- Loss: 0.121\n",
      "Iteration: 144 \t--- Loss: 0.117\n",
      "Iteration: 145 \t--- Loss: 0.116\n",
      "Iteration: 146 \t--- Loss: 0.113\n",
      "Iteration: 147 \t--- Loss: 0.118\n",
      "Iteration: 148 \t--- Loss: 0.111\n",
      "Iteration: 149 \t--- Loss: 0.133\n",
      "Iteration: 150 \t--- Loss: 0.143\n",
      "Iteration: 151 \t--- Loss: 0.116\n",
      "Iteration: 152 \t--- Loss: 0.121\n",
      "Iteration: 153 \t--- Loss: 0.135\n",
      "Iteration: 154 \t--- Loss: 0.140\n",
      "Iteration: 155 \t--- Loss: 0.115\n",
      "Iteration: 156 \t--- Loss: 0.115\n",
      "Iteration: 157 \t--- Loss: 0.126\n",
      "Iteration: 158 \t--- Loss: 0.120\n",
      "Iteration: 159 \t--- Loss: 0.125\n",
      "Iteration: 160 \t--- Loss: 0.130\n",
      "Iteration: 161 \t--- Loss: 0.120\n",
      "Iteration: 162 \t--- Loss: 0.114\n",
      "Iteration: 163 \t--- Loss: 0.123\n",
      "Iteration: 164 \t--- Loss: 0.140\n",
      "Iteration: 165 \t--- Loss: 0.110\n",
      "Iteration: 166 \t--- Loss: 0.110\n",
      "Iteration: 167 \t--- Loss: 0.119\n",
      "Iteration: 168 \t--- Loss: 0.114\n",
      "Iteration: 169 \t--- Loss: 0.116\n",
      "Iteration: 170 \t--- Loss: 0.101\n",
      "Iteration: 171 \t--- Loss: 0.106\n",
      "Iteration: 172 \t--- Loss: 0.102\n",
      "Iteration: 173 \t--- Loss: 0.112\n",
      "Iteration: 174 \t--- Loss: 0.093\n",
      "Iteration: 175 \t--- Loss: 0.097\n",
      "Iteration: 176 \t--- Loss: 0.111\n",
      "Iteration: 177 \t--- Loss: 0.100\n",
      "Iteration: 178 \t--- Loss: 0.098\n",
      "Iteration: 179 \t--- Loss: 0.112\n",
      "Iteration: 180 \t--- Loss: 0.106\n",
      "Iteration: 181 \t--- Loss: 0.115\n",
      "Iteration: 182 \t--- Loss: 0.093\n",
      "Iteration: 183 \t--- Loss: 0.114\n",
      "Iteration: 184 \t--- Loss: 0.114\n",
      "Iteration: 185 \t--- Loss: 0.114\n",
      "Iteration: 186 \t--- Loss: 0.135\n",
      "Iteration: 187 \t--- Loss: 0.125\n",
      "Iteration: 188 \t--- Loss: 0.130\n",
      "Iteration: 189 \t--- Loss: 0.098\n",
      "Iteration: 190 \t--- Loss: 0.110\n",
      "Iteration: 191 \t--- Loss: 0.095\n",
      "Iteration: 192 \t--- Loss: 0.102\n",
      "Iteration: 193 \t--- Loss: 0.105\n",
      "Iteration: 194 \t--- Loss: 0.104\n",
      "Iteration: 195 \t--- Loss: 0.104\n",
      "Iteration: 196 \t--- Loss: 0.109\n",
      "Iteration: 197 \t--- Loss: 0.110\n",
      "Iteration: 198 \t--- Loss: 0.117\n",
      "Iteration: 199 \t--- Loss: 0.124\n",
      "Iteration: 200 \t--- Loss: 0.100\n",
      "Iteration: 201 \t--- Loss: 0.097\n",
      "Iteration: 202 \t--- Loss: 0.099\n",
      "Iteration: 203 \t--- Loss: 0.101\n",
      "Iteration: 204 \t--- Loss: 0.091\n",
      "Iteration: 205 \t--- Loss: 0.107\n",
      "Iteration: 206 \t--- Loss: 0.103\n",
      "Iteration: 207 \t--- Loss: 0.101\n",
      "Iteration: 208 \t--- Loss: 0.114\n",
      "Iteration: 209 \t--- Loss: 0.109\n",
      "Iteration: 210 \t--- Loss: 0.105\n",
      "Iteration: 211 \t--- Loss: 0.106\n",
      "Iteration: 212 \t--- Loss: 0.096\n",
      "Iteration: 213 \t--- Loss: 0.093\n",
      "Iteration: 214 \t--- Loss: 0.104\n",
      "Iteration: 215 \t--- Loss: 0.101\n",
      "Iteration: 216 \t--- Loss: 0.094\n",
      "Iteration: 217 \t--- Loss: 0.101\n",
      "Iteration: 218 \t--- Loss: 0.097\n",
      "Iteration: 219 \t--- Loss: 0.096\n",
      "Iteration: 220 \t--- Loss: 0.096\n",
      "Iteration: 221 \t--- Loss: 0.094\n",
      "Iteration: 222 \t--- Loss: 0.110\n",
      "Iteration: 223 \t--- Loss: 0.113\n",
      "Iteration: 224 \t--- Loss: 0.095\n",
      "Iteration: 225 \t--- Loss: 0.098\n",
      "Iteration: 226 \t--- Loss: 0.101\n",
      "Iteration: 227 \t--- Loss: 0.094\n",
      "Iteration: 228 \t--- Loss: 0.088\n",
      "Iteration: 229 \t--- Loss: 0.096\n",
      "Iteration: 230 \t--- Loss: 0.099\n",
      "Iteration: 231 \t--- Loss: 0.098\n",
      "Iteration: 232 \t--- Loss: 0.096\n",
      "Iteration: 233 \t--- Loss: 0.085\n",
      "Iteration: 234 \t--- Loss: 0.099\n",
      "Iteration: 235 \t--- Loss: 0.091\n",
      "Iteration: 236 \t--- Loss: 0.091\n",
      "Iteration: 237 \t--- Loss: 0.094\n",
      "Iteration: 238 \t--- Loss: 0.096\n",
      "Iteration: 239 \t--- Loss: 0.096\n",
      "Iteration: 240 \t--- Loss: 0.099\n",
      "Iteration: 241 \t--- Loss: 0.102\n",
      "Iteration: 242 \t--- Loss: 0.100\n",
      "Iteration: 243 \t--- Loss: 0.105\n",
      "Iteration: 244 \t--- Loss: 0.107\n",
      "Iteration: 245 \t--- Loss: 0.101\n",
      "Iteration: 246 \t--- Loss: 0.100\n",
      "Iteration: 247 \t--- Loss: 0.099\n",
      "Iteration: 248 \t--- Loss: 0.098\n",
      "Iteration: 249 \t--- Loss: 0.090\n",
      "Iteration: 250 \t--- Loss: 0.092\n",
      "Iteration: 251 \t--- Loss: 0.094\n",
      "Iteration: 252 \t--- Loss: 0.095\n",
      "Iteration: 253 \t--- Loss: 0.099\n",
      "Iteration: 254 \t--- Loss: 0.105\n",
      "Iteration: 255 \t--- Loss: 0.101\n",
      "Iteration: 256 \t--- Loss: 0.089\n",
      "Iteration: 257 \t--- Loss: 0.098\n",
      "Iteration: 258 \t--- Loss: 0.100\n",
      "Iteration: 259 \t--- Loss: 0.104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.44s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 1.474\n",
      "Iteration: 261 \t--- Loss: 1.513\n",
      "Iteration: 262 \t--- Loss: 1.541\n",
      "Iteration: 263 \t--- Loss: 1.476\n",
      "Iteration: 264 \t--- Loss: 1.521\n",
      "Iteration: 265 \t--- Loss: 1.497\n",
      "Iteration: 266 \t--- Loss: 1.468\n",
      "Iteration: 267 \t--- Loss: 1.520\n",
      "Iteration: 268 \t--- Loss: 1.511\n",
      "Iteration: 269 \t--- Loss: 1.510\n",
      "Iteration: 270 \t--- Loss: 1.487\n",
      "Iteration: 271 \t--- Loss: 1.486\n",
      "Iteration: 272 \t--- Loss: 1.530\n",
      "Iteration: 273 \t--- Loss: 1.484\n",
      "Iteration: 274 \t--- Loss: 1.480\n",
      "Iteration: 275 \t--- Loss: 1.590\n",
      "Iteration: 276 \t--- Loss: 1.493\n",
      "Iteration: 277 \t--- Loss: 1.506\n",
      "Iteration: 278 \t--- Loss: 1.484\n",
      "Iteration: 279 \t--- Loss: 1.461\n",
      "Iteration: 280 \t--- Loss: 1.509\n",
      "Iteration: 281 \t--- Loss: 1.483\n",
      "Iteration: 282 \t--- Loss: 1.482\n",
      "Iteration: 283 \t--- Loss: 1.517\n",
      "Iteration: 284 \t--- Loss: 1.512\n",
      "Iteration: 285 \t--- Loss: 1.502\n",
      "Iteration: 286 \t--- Loss: 1.483\n",
      "Iteration: 287 \t--- Loss: 1.541\n",
      "Iteration: 288 \t--- Loss: 1.517\n",
      "Iteration: 289 \t--- Loss: 1.516\n",
      "Iteration: 290 \t--- Loss: 1.521\n",
      "Iteration: 291 \t--- Loss: 1.468\n",
      "Iteration: 292 \t--- Loss: 1.538\n",
      "Iteration: 293 \t--- Loss: 1.453\n",
      "Iteration: 294 \t--- Loss: 1.547\n",
      "Iteration: 295 \t--- Loss: 1.539\n",
      "Iteration: 296 \t--- Loss: 1.513\n",
      "Iteration: 297 \t--- Loss: 1.499\n",
      "Iteration: 298 \t--- Loss: 1.456\n",
      "Iteration: 299 \t--- Loss: 1.518\n",
      "Iteration: 300 \t--- Loss: 1.521\n",
      "Iteration: 301 \t--- Loss: 1.478\n",
      "Iteration: 302 \t--- Loss: 1.483\n",
      "Iteration: 303 \t--- Loss: 1.529\n",
      "Iteration: 304 \t--- Loss: 1.574\n",
      "Iteration: 305 \t--- Loss: 1.512\n",
      "Iteration: 306 \t--- Loss: 1.518\n",
      "Iteration: 307 \t--- Loss: 1.465\n",
      "Iteration: 308 \t--- Loss: 1.490\n",
      "Iteration: 309 \t--- Loss: 1.558\n",
      "Iteration: 310 \t--- Loss: 1.498\n",
      "Iteration: 311 \t--- Loss: 1.498\n",
      "Iteration: 312 \t--- Loss: 1.482\n",
      "Iteration: 313 \t--- Loss: 1.486\n",
      "Iteration: 314 \t--- Loss: 1.557\n",
      "Iteration: 315 \t--- Loss: 1.484\n",
      "Iteration: 316 \t--- Loss: 1.517\n",
      "Iteration: 317 \t--- Loss: 1.439\n",
      "Iteration: 318 \t--- Loss: 1.461\n",
      "Iteration: 319 \t--- Loss: 1.505\n",
      "Iteration: 320 \t--- Loss: 1.488\n",
      "Iteration: 321 \t--- Loss: 1.500\n",
      "Iteration: 322 \t--- Loss: 1.527\n",
      "Iteration: 323 \t--- Loss: 1.457\n",
      "Iteration: 324 \t--- Loss: 1.512\n",
      "Iteration: 325 \t--- Loss: 1.532\n",
      "Iteration: 326 \t--- Loss: 1.521\n",
      "Iteration: 327 \t--- Loss: 1.480\n",
      "Iteration: 328 \t--- Loss: 1.496\n",
      "Iteration: 329 \t--- Loss: 1.530\n",
      "Iteration: 330 \t--- Loss: 1.464\n",
      "Iteration: 331 \t--- Loss: 1.501\n",
      "Iteration: 332 \t--- Loss: 1.532\n",
      "Iteration: 333 \t--- Loss: 1.435\n",
      "Iteration: 334 \t--- Loss: 1.478\n",
      "Iteration: 335 \t--- Loss: 1.520\n",
      "Iteration: 336 \t--- Loss: 1.540\n",
      "Iteration: 337 \t--- Loss: 1.536\n",
      "Iteration: 338 \t--- Loss: 1.481\n",
      "Iteration: 339 \t--- Loss: 1.543\n",
      "Iteration: 340 \t--- Loss: 1.514\n",
      "Iteration: 341 \t--- Loss: 1.481\n",
      "Iteration: 342 \t--- Loss: 1.526\n",
      "Iteration: 343 \t--- Loss: 1.501\n",
      "Iteration: 344 \t--- Loss: 1.537\n",
      "Iteration: 345 \t--- Loss: 1.541\n",
      "Iteration: 346 \t--- Loss: 1.475\n",
      "Iteration: 347 \t--- Loss: 1.488\n",
      "Iteration: 348 \t--- Loss: 1.504\n",
      "Iteration: 349 \t--- Loss: 1.512\n",
      "Iteration: 350 \t--- Loss: 1.492\n",
      "Iteration: 351 \t--- Loss: 1.535\n",
      "Iteration: 352 \t--- Loss: 1.510\n",
      "Iteration: 353 \t--- Loss: 1.462\n",
      "Iteration: 354 \t--- Loss: 1.517\n",
      "Iteration: 355 \t--- Loss: 1.508\n",
      "Iteration: 356 \t--- Loss: 1.513\n",
      "Iteration: 357 \t--- Loss: 1.477\n",
      "Iteration: 358 \t--- Loss: 1.480\n",
      "Iteration: 359 \t--- Loss: 1.533\n",
      "Iteration: 360 \t--- Loss: 1.527\n",
      "Iteration: 361 \t--- Loss: 1.531\n",
      "Iteration: 362 \t--- Loss: 1.475\n",
      "Iteration: 363 \t--- Loss: 1.515\n",
      "Iteration: 364 \t--- Loss: 1.509\n",
      "Iteration: 365 \t--- Loss: 1.504\n",
      "Iteration: 366 \t--- Loss: 1.514\n",
      "Iteration: 367 \t--- Loss: 1.501\n",
      "Iteration: 368 \t--- Loss: 1.541\n",
      "Iteration: 369 \t--- Loss: 1.520\n",
      "Iteration: 370 \t--- Loss: 1.501\n",
      "Iteration: 371 \t--- Loss: 1.498\n",
      "Iteration: 372 \t--- Loss: 1.449\n",
      "Iteration: 373 \t--- Loss: 1.547\n",
      "Iteration: 374 \t--- Loss: 1.530\n",
      "Iteration: 375 \t--- Loss: 1.534\n",
      "Iteration: 376 \t--- Loss: 1.479\n",
      "Iteration: 377 \t--- Loss: 1.486\n",
      "Iteration: 378 \t--- Loss: 1.474\n",
      "Iteration: 379 \t--- Loss: 1.540\n",
      "Iteration: 380 \t--- Loss: 1.508\n",
      "Iteration: 381 \t--- Loss: 1.485\n",
      "Iteration: 382 \t--- Loss: 1.405\n",
      "Iteration: 383 \t--- Loss: 1.535\n",
      "Iteration: 384 \t--- Loss: 1.515\n",
      "Iteration: 385 \t--- Loss: 1.542\n",
      "Iteration: 386 \t--- Loss: 1.538\n",
      "Iteration: 387 \t--- Loss: 1.507\n",
      "Iteration: 388 \t--- Loss: 1.495\n",
      "Iteration: 389 \t--- Loss: 1.527\n",
      "Iteration: 390 \t--- Loss: 1.482\n",
      "Iteration: 391 \t--- Loss: 1.489\n",
      "Iteration: 392 \t--- Loss: 1.549\n",
      "Iteration: 393 \t--- Loss: 1.526\n",
      "Iteration: 394 \t--- Loss: 1.505\n",
      "Iteration: 395 \t--- Loss: 1.509\n",
      "Iteration: 396 \t--- Loss: 1.486\n",
      "Iteration: 397 \t--- Loss: 1.448\n",
      "Iteration: 398 \t--- Loss: 1.479\n",
      "Iteration: 399 \t--- Loss: 1.487\n",
      "Iteration: 400 \t--- Loss: 1.461\n",
      "Iteration: 401 \t--- Loss: 1.487\n",
      "Iteration: 402 \t--- Loss: 1.549\n",
      "Iteration: 403 \t--- Loss: 1.526\n",
      "Iteration: 404 \t--- Loss: 1.481\n",
      "Iteration: 405 \t--- Loss: 1.505\n",
      "Iteration: 406 \t--- Loss: 1.583\n",
      "Iteration: 407 \t--- Loss: 1.504\n",
      "Iteration: 408 \t--- Loss: 1.483\n",
      "Iteration: 409 \t--- Loss: 1.533\n",
      "Iteration: 410 \t--- Loss: 1.506\n",
      "Iteration: 411 \t--- Loss: 1.528\n",
      "Iteration: 412 \t--- Loss: 1.514\n",
      "Iteration: 413 \t--- Loss: 1.513\n",
      "Iteration: 414 \t--- Loss: 1.515\n",
      "Iteration: 415 \t--- Loss: 1.461\n",
      "Iteration: 416 \t--- Loss: 1.461\n",
      "Iteration: 417 \t--- Loss: 1.516\n",
      "Iteration: 418 \t--- Loss: 1.531\n",
      "Iteration: 419 \t--- Loss: 1.442\n",
      "Iteration: 420 \t--- Loss: 1.509\n",
      "Iteration: 421 \t--- Loss: 1.450\n",
      "Iteration: 422 \t--- Loss: 1.481\n",
      "Iteration: 423 \t--- Loss: 1.465\n",
      "Iteration: 424 \t--- Loss: 1.544\n",
      "Iteration: 425 \t--- Loss: 1.539\n",
      "Iteration: 426 \t--- Loss: 1.529\n",
      "Iteration: 427 \t--- Loss: 1.494\n",
      "Iteration: 428 \t--- Loss: 1.538\n",
      "Iteration: 429 \t--- Loss: 1.557\n",
      "Iteration: 430 \t--- Loss: 1.475\n",
      "Iteration: 431 \t--- Loss: 1.511\n",
      "Iteration: 432 \t--- Loss: 1.569\n",
      "Iteration: 433 \t--- Loss: 1.479\n",
      "Iteration: 434 \t--- Loss: 1.439\n",
      "Iteration: 435 \t--- Loss: 1.497\n",
      "Iteration: 436 \t--- Loss: 1.503\n",
      "Iteration: 437 \t--- Loss: 1.484\n",
      "Iteration: 438 \t--- Loss: 1.490\n",
      "Iteration: 439 \t--- Loss: 1.528\n",
      "Iteration: 440 \t--- Loss: 1.507\n",
      "Iteration: 441 \t--- Loss: 1.492\n",
      "Iteration: 442 \t--- Loss: 1.511\n",
      "Iteration: 443 \t--- Loss: 1.479\n",
      "Iteration: 444 \t--- Loss: 1.519\n",
      "Iteration: 445 \t--- Loss: 1.501\n",
      "Iteration: 446 \t--- Loss: 1.523\n",
      "Iteration: 447 \t--- Loss: 1.451\n",
      "Iteration: 448 \t--- Loss: 1.424\n",
      "Iteration: 449 \t--- Loss: 1.500\n",
      "Iteration: 450 \t--- Loss: 1.507\n",
      "Iteration: 451 \t--- Loss: 1.509\n",
      "Iteration: 452 \t--- Loss: 1.507\n",
      "Iteration: 453 \t--- Loss: 1.523\n",
      "Iteration: 454 \t--- Loss: 1.505\n",
      "Iteration: 455 \t--- Loss: 1.525\n",
      "Iteration: 456 \t--- Loss: 1.479\n",
      "Iteration: 457 \t--- Loss: 1.496\n",
      "Iteration: 458 \t--- Loss: 1.506\n",
      "Iteration: 459 \t--- Loss: 1.492\n",
      "Iteration: 460 \t--- Loss: 1.514\n",
      "Iteration: 461 \t--- Loss: 1.544\n",
      "Iteration: 462 \t--- Loss: 1.484\n",
      "Iteration: 463 \t--- Loss: 1.515\n",
      "Iteration: 464 \t--- Loss: 1.520\n",
      "Iteration: 465 \t--- Loss: 1.503\n",
      "Iteration: 466 \t--- Loss: 1.463\n",
      "Iteration: 467 \t--- Loss: 1.497\n",
      "Iteration: 468 \t--- Loss: 1.493\n",
      "Iteration: 469 \t--- Loss: 1.500\n",
      "Iteration: 470 \t--- Loss: 1.523\n",
      "Iteration: 471 \t--- Loss: 1.531\n",
      "Iteration: 472 \t--- Loss: 1.449\n",
      "Iteration: 473 \t--- Loss: 1.493\n",
      "Iteration: 474 \t--- Loss: 1.487\n",
      "Iteration: 475 \t--- Loss: 1.514\n",
      "Iteration: 476 \t--- Loss: 1.524\n",
      "Iteration: 477 \t--- Loss: 1.506\n",
      "Iteration: 478 \t--- Loss: 1.534\n",
      "Iteration: 479 \t--- Loss: 1.480\n",
      "Iteration: 480 \t--- Loss: 1.485\n",
      "Iteration: 481 \t--- Loss: 1.564\n",
      "Iteration: 482 \t--- Loss: 1.448\n",
      "Iteration: 483 \t--- Loss: 1.468\n",
      "Iteration: 484 \t--- Loss: 1.482\n",
      "Iteration: 485 \t--- Loss: 1.507\n",
      "Iteration: 486 \t--- Loss: 1.459\n",
      "Iteration: 487 \t--- Loss: 1.441\n",
      "Iteration: 488 \t--- Loss: 1.545\n",
      "Iteration: 489 \t--- Loss: 1.469\n",
      "Iteration: 490 \t--- Loss: 1.558\n",
      "Iteration: 491 \t--- Loss: 1.526\n",
      "Iteration: 492 \t--- Loss: 1.507\n",
      "Iteration: 493 \t--- Loss: 1.514\n",
      "Iteration: 494 \t--- Loss: 1.474\n",
      "Iteration: 495 \t--- Loss: 1.534\n",
      "Iteration: 496 \t--- Loss: 1.470\n",
      "Iteration: 497 \t--- Loss: 1.556\n",
      "Iteration: 498 \t--- Loss: 1.531\n",
      "Iteration: 499 \t--- Loss: 1.442\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:10<00:04,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.148\n",
      "Iteration: 1 \t--- Loss: 0.113\n",
      "Iteration: 2 \t--- Loss: 0.118\n",
      "Iteration: 3 \t--- Loss: 0.104\n",
      "Iteration: 4 \t--- Loss: 0.091\n",
      "Iteration: 5 \t--- Loss: 0.097\n",
      "Iteration: 6 \t--- Loss: 0.095\n",
      "Iteration: 7 \t--- Loss: 0.108\n",
      "Iteration: 8 \t--- Loss: 0.087\n",
      "Iteration: 9 \t--- Loss: 0.083\n",
      "Iteration: 10 \t--- Loss: 0.081\n",
      "Iteration: 11 \t--- Loss: 0.069\n",
      "Iteration: 12 \t--- Loss: 0.069\n",
      "Iteration: 13 \t--- Loss: 0.086\n",
      "Iteration: 14 \t--- Loss: 0.067\n",
      "Iteration: 15 \t--- Loss: 0.068\n",
      "Iteration: 16 \t--- Loss: 0.065\n",
      "Iteration: 17 \t--- Loss: 0.064\n",
      "Iteration: 18 \t--- Loss: 0.069\n",
      "Iteration: 19 \t--- Loss: 0.064\n",
      "Iteration: 20 \t--- Loss: 0.065\n",
      "Iteration: 21 \t--- Loss: 0.064\n",
      "Iteration: 22 \t--- Loss: 0.064\n",
      "Iteration: 23 \t--- Loss: 0.064\n",
      "Iteration: 24 \t--- Loss: 0.069\n",
      "Iteration: 25 \t--- Loss: 0.063\n",
      "Iteration: 26 \t--- Loss: 0.063\n",
      "Iteration: 27 \t--- Loss: 0.060\n",
      "Iteration: 28 \t--- Loss: 0.058\n",
      "Iteration: 29 \t--- Loss: 0.064\n",
      "Iteration: 30 \t--- Loss: 0.060\n",
      "Iteration: 31 \t--- Loss: 0.068\n",
      "Iteration: 32 \t--- Loss: 0.065\n",
      "Iteration: 33 \t--- Loss: 0.056\n",
      "Iteration: 34 \t--- Loss: 0.061\n",
      "Iteration: 35 \t--- Loss: 0.060\n",
      "Iteration: 36 \t--- Loss: 0.055\n",
      "Iteration: 37 \t--- Loss: 0.066\n",
      "Iteration: 38 \t--- Loss: 0.061\n",
      "Iteration: 39 \t--- Loss: 0.052\n",
      "Iteration: 40 \t--- Loss: 0.060\n",
      "Iteration: 41 \t--- Loss: 0.060\n",
      "Iteration: 42 \t--- Loss: 0.056\n",
      "Iteration: 43 \t--- Loss: 0.057\n",
      "Iteration: 44 \t--- Loss: 0.054\n",
      "Iteration: 45 \t--- Loss: 0.053\n",
      "Iteration: 46 \t--- Loss: 0.054\n",
      "Iteration: 47 \t--- Loss: 0.058\n",
      "Iteration: 48 \t--- Loss: 0.054\n",
      "Iteration: 49 \t--- Loss: 0.056\n",
      "Iteration: 50 \t--- Loss: 0.058\n",
      "Iteration: 51 \t--- Loss: 0.054\n",
      "Iteration: 52 \t--- Loss: 0.057\n",
      "Iteration: 53 \t--- Loss: 0.062\n",
      "Iteration: 54 \t--- Loss: 0.062\n",
      "Iteration: 55 \t--- Loss: 0.052\n",
      "Iteration: 56 \t--- Loss: 0.056\n",
      "Iteration: 57 \t--- Loss: 0.055\n",
      "Iteration: 58 \t--- Loss: 0.055\n",
      "Iteration: 59 \t--- Loss: 0.056\n",
      "Iteration: 60 \t--- Loss: 0.053\n",
      "Iteration: 61 \t--- Loss: 0.056\n",
      "Iteration: 62 \t--- Loss: 0.053\n",
      "Iteration: 63 \t--- Loss: 0.060\n",
      "Iteration: 64 \t--- Loss: 0.057\n",
      "Iteration: 65 \t--- Loss: 0.059\n",
      "Iteration: 66 \t--- Loss: 0.060\n",
      "Iteration: 67 \t--- Loss: 0.059\n",
      "Iteration: 68 \t--- Loss: 0.058\n",
      "Iteration: 69 \t--- Loss: 0.056\n",
      "Iteration: 70 \t--- Loss: 0.061\n",
      "Iteration: 71 \t--- Loss: 0.054\n",
      "Iteration: 72 \t--- Loss: 0.054\n",
      "Iteration: 73 \t--- Loss: 0.056\n",
      "Iteration: 74 \t--- Loss: 0.055\n",
      "Iteration: 75 \t--- Loss: 0.056\n",
      "Iteration: 76 \t--- Loss: 0.053\n",
      "Iteration: 77 \t--- Loss: 0.054\n",
      "Iteration: 78 \t--- Loss: 0.055\n",
      "Iteration: 79 \t--- Loss: 0.052\n",
      "Iteration: 80 \t--- Loss: 0.052\n",
      "Iteration: 81 \t--- Loss: 0.057\n",
      "Iteration: 82 \t--- Loss: 0.058\n",
      "Iteration: 83 \t--- Loss: 0.055\n",
      "Iteration: 84 \t--- Loss: 0.060\n",
      "Iteration: 85 \t--- Loss: 0.057\n",
      "Iteration: 86 \t--- Loss: 0.054\n",
      "Iteration: 87 \t--- Loss: 0.058\n",
      "Iteration: 88 \t--- Loss: 0.053\n",
      "Iteration: 89 \t--- Loss: 0.057\n",
      "Iteration: 90 \t--- Loss: 0.056\n",
      "Iteration: 91 \t--- Loss: 0.057\n",
      "Iteration: 92 \t--- Loss: 0.055\n",
      "Iteration: 93 \t--- Loss: 0.058\n",
      "Iteration: 94 \t--- Loss: 0.059\n",
      "Iteration: 95 \t--- Loss: 0.057\n",
      "Iteration: 96 \t--- Loss: 0.058\n",
      "Iteration: 97 \t--- Loss: 0.056\n",
      "Iteration: 98 \t--- Loss: 0.057\n",
      "Iteration: 99 \t--- Loss: 0.056\n",
      "Iteration: 100 \t--- Loss: 0.058\n",
      "Iteration: 101 \t--- Loss: 0.059\n",
      "Iteration: 102 \t--- Loss: 0.055\n",
      "Iteration: 103 \t--- Loss: 0.056\n",
      "Iteration: 104 \t--- Loss: 0.054\n",
      "Iteration: 105 \t--- Loss: 0.058\n",
      "Iteration: 106 \t--- Loss: 0.057\n",
      "Iteration: 107 \t--- Loss: 0.057\n",
      "Iteration: 108 \t--- Loss: 0.054\n",
      "Iteration: 109 \t--- Loss: 0.054\n",
      "Iteration: 110 \t--- Loss: 0.056\n",
      "Iteration: 111 \t--- Loss: 0.056\n",
      "Iteration: 112 \t--- Loss: 0.058\n",
      "Iteration: 113 \t--- Loss: 0.055\n",
      "Iteration: 114 \t--- Loss: 0.051\n",
      "Iteration: 115 \t--- Loss: 0.054\n",
      "Iteration: 116 \t--- Loss: 0.054\n",
      "Iteration: 117 \t--- Loss: 0.062\n",
      "Iteration: 118 \t--- Loss: 0.059\n",
      "Iteration: 119 \t--- Loss: 0.057\n",
      "Iteration: 120 \t--- Loss: 0.059\n",
      "Iteration: 121 \t--- Loss: 0.054\n",
      "Iteration: 122 \t--- Loss: 0.054\n",
      "Iteration: 123 \t--- Loss: 0.054\n",
      "Iteration: 124 \t--- Loss: 0.057\n",
      "Iteration: 125 \t--- Loss: 0.053\n",
      "Iteration: 126 \t--- Loss: 0.054\n",
      "Iteration: 127 \t--- Loss: 0.054\n",
      "Iteration: 128 \t--- Loss: 0.052\n",
      "Iteration: 129 \t--- Loss: 0.051\n",
      "Iteration: 130 \t--- Loss: 0.057\n",
      "Iteration: 131 \t--- Loss: 0.057\n",
      "Iteration: 132 \t--- Loss: 0.057\n",
      "Iteration: 133 \t--- Loss: 0.054\n",
      "Iteration: 134 \t--- Loss: 0.057\n",
      "Iteration: 135 \t--- Loss: 0.058\n",
      "Iteration: 136 \t--- Loss: 0.050\n",
      "Iteration: 137 \t--- Loss: 0.055\n",
      "Iteration: 138 \t--- Loss: 0.055\n",
      "Iteration: 139 \t--- Loss: 0.056\n",
      "Iteration: 140 \t--- Loss: 0.057\n",
      "Iteration: 141 \t--- Loss: 0.057\n",
      "Iteration: 142 \t--- Loss: 0.054\n",
      "Iteration: 143 \t--- Loss: 0.054\n",
      "Iteration: 144 \t--- Loss: 0.051\n",
      "Iteration: 145 \t--- Loss: 0.059\n",
      "Iteration: 146 \t--- Loss: 0.057\n",
      "Iteration: 147 \t--- Loss: 0.056\n",
      "Iteration: 148 \t--- Loss: 0.061\n",
      "Iteration: 149 \t--- Loss: 0.054\n",
      "Iteration: 150 \t--- Loss: 0.058\n",
      "Iteration: 151 \t--- Loss: 0.062\n",
      "Iteration: 152 \t--- Loss: 0.059\n",
      "Iteration: 153 \t--- Loss: 0.058\n",
      "Iteration: 154 \t--- Loss: 0.052\n",
      "Iteration: 155 \t--- Loss: 0.058\n",
      "Iteration: 156 \t--- Loss: 0.056\n",
      "Iteration: 157 \t--- Loss: 0.056\n",
      "Iteration: 158 \t--- Loss: 0.051\n",
      "Iteration: 159 \t--- Loss: 0.057\n",
      "Iteration: 160 \t--- Loss: 0.050\n",
      "Iteration: 161 \t--- Loss: 0.053\n",
      "Iteration: 162 \t--- Loss: 0.059\n",
      "Iteration: 163 \t--- Loss: 0.058\n",
      "Iteration: 164 \t--- Loss: 0.054\n",
      "Iteration: 165 \t--- Loss: 0.055\n",
      "Iteration: 166 \t--- Loss: 0.056\n",
      "Iteration: 167 \t--- Loss: 0.054\n",
      "Iteration: 168 \t--- Loss: 0.056\n",
      "Iteration: 169 \t--- Loss: 0.057\n",
      "Iteration: 170 \t--- Loss: 0.054\n",
      "Iteration: 171 \t--- Loss: 0.058\n",
      "Iteration: 172 \t--- Loss: 0.054\n",
      "Iteration: 173 \t--- Loss: 0.060\n",
      "Iteration: 174 \t--- Loss: 0.056\n",
      "Iteration: 175 \t--- Loss: 0.059\n",
      "Iteration: 176 \t--- Loss: 0.056\n",
      "Iteration: 177 \t--- Loss: 0.058\n",
      "Iteration: 178 \t--- Loss: 0.057\n",
      "Iteration: 179 \t--- Loss: 0.059\n",
      "Iteration: 180 \t--- Loss: 0.057\n",
      "Iteration: 181 \t--- Loss: 0.057\n",
      "Iteration: 182 \t--- Loss: 0.056\n",
      "Iteration: 183 \t--- Loss: 0.055\n",
      "Iteration: 184 \t--- Loss: 0.055\n",
      "Iteration: 185 \t--- Loss: 0.055\n",
      "Iteration: 186 \t--- Loss: 0.055\n",
      "Iteration: 187 \t--- Loss: 0.057\n",
      "Iteration: 188 \t--- Loss: 0.055\n",
      "Iteration: 189 \t--- Loss: 0.058\n",
      "Iteration: 190 \t--- Loss: 0.055\n",
      "Iteration: 191 \t--- Loss: 0.056\n",
      "Iteration: 192 \t--- Loss: 0.052\n",
      "Iteration: 193 \t--- Loss: 0.056\n",
      "Iteration: 194 \t--- Loss: 0.053\n",
      "Iteration: 195 \t--- Loss: 0.061\n",
      "Iteration: 196 \t--- Loss: 0.054\n",
      "Iteration: 197 \t--- Loss: 0.057\n",
      "Iteration: 198 \t--- Loss: 0.053\n",
      "Iteration: 199 \t--- Loss: 0.054\n",
      "Iteration: 200 \t--- Loss: 0.056\n",
      "Iteration: 201 \t--- Loss: 0.054\n",
      "Iteration: 202 \t--- Loss: 0.056\n",
      "Iteration: 203 \t--- Loss: 0.053\n",
      "Iteration: 204 \t--- Loss: 0.058\n",
      "Iteration: 205 \t--- Loss: 0.054\n",
      "Iteration: 206 \t--- Loss: 0.062\n",
      "Iteration: 207 \t--- Loss: 0.055\n",
      "Iteration: 208 \t--- Loss: 0.058\n",
      "Iteration: 209 \t--- Loss: 0.057\n",
      "Iteration: 210 \t--- Loss: 0.055\n",
      "Iteration: 211 \t--- Loss: 0.052\n",
      "Iteration: 212 \t--- Loss: 0.056\n",
      "Iteration: 213 \t--- Loss: 0.055\n",
      "Iteration: 214 \t--- Loss: 0.053\n",
      "Iteration: 215 \t--- Loss: 0.056\n",
      "Iteration: 216 \t--- Loss: 0.055\n",
      "Iteration: 217 \t--- Loss: 0.057\n",
      "Iteration: 218 \t--- Loss: 0.054\n",
      "Iteration: 219 \t--- Loss: 0.058\n",
      "Iteration: 220 \t--- Loss: 0.050\n",
      "Iteration: 221 \t--- Loss: 0.051\n",
      "Iteration: 222 \t--- Loss: 0.053\n",
      "Iteration: 223 \t--- Loss: 0.058\n",
      "Iteration: 224 \t--- Loss: 0.055\n",
      "Iteration: 225 \t--- Loss: 0.057\n",
      "Iteration: 226 \t--- Loss: 0.055\n",
      "Iteration: 227 \t--- Loss: 0.056\n",
      "Iteration: 228 \t--- Loss: 0.054\n",
      "Iteration: 229 \t--- Loss: 0.053\n",
      "Iteration: 230 \t--- Loss: 0.054\n",
      "Iteration: 231 \t--- Loss: 0.051\n",
      "Iteration: 232 \t--- Loss: 0.052\n",
      "Iteration: 233 \t--- Loss: 0.052\n",
      "Iteration: 234 \t--- Loss: 0.055\n",
      "Iteration: 235 \t--- Loss: 0.056\n",
      "Iteration: 236 \t--- Loss: 0.055\n",
      "Iteration: 237 \t--- Loss: 0.060\n",
      "Iteration: 238 \t--- Loss: 0.053\n",
      "Iteration: 239 \t--- Loss: 0.054\n",
      "Iteration: 240 \t--- Loss: 0.055\n",
      "Iteration: 241 \t--- Loss: 0.056\n",
      "Iteration: 242 \t--- Loss: 0.053\n",
      "Iteration: 243 \t--- Loss: 0.053\n",
      "Iteration: 244 \t--- Loss: 0.056\n",
      "Iteration: 245 \t--- Loss: 0.055\n",
      "Iteration: 246 \t--- Loss: 0.056\n",
      "Iteration: 247 \t--- Loss: 0.056\n",
      "Iteration: 248 \t--- Loss: 0.053\n",
      "Iteration: 249 \t--- Loss: 0.061\n",
      "Iteration: 250 \t--- Loss: 0.059\n",
      "Iteration: 251 \t--- Loss: 0.054\n",
      "Iteration: 252 \t--- Loss: 0.055\n",
      "Iteration: 253 \t--- Loss: 0.055\n",
      "Iteration: 254 \t--- Loss: 0.055\n",
      "Iteration: 255 \t--- Loss: 0.056\n",
      "Iteration: 256 \t--- Loss: 0.056\n",
      "Iteration: 257 \t--- Loss: 0.057\n",
      "Iteration: 258 \t--- Loss: 0.060\n",
      "Iteration: 259 \t--- Loss: 0.059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.54s/it][Parallel(n_jobs=5)]: Done  71 tasks      | elapsed: 45.8min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.49s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:34<00:00, 94.30s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.060\n",
      "Iteration: 261 \t--- Loss: 0.055\n",
      "Iteration: 262 \t--- Loss: 0.059\n",
      "Iteration: 263 \t--- Loss: 0.055\n",
      "Iteration: 264 \t--- Loss: 0.054\n",
      "Iteration: 265 \t--- Loss: 0.056\n",
      "Iteration: 266 \t--- Loss: 0.056\n",
      "Iteration: 267 \t--- Loss: 0.059\n",
      "Iteration: 268 \t--- Loss: 0.060\n",
      "Iteration: 269 \t--- Loss: 0.058\n",
      "Iteration: 270 \t--- Loss: 0.057\n",
      "Iteration: 271 \t--- Loss: 0.057\n",
      "Iteration: 272 \t--- Loss: 0.054\n",
      "Iteration: 273 \t--- Loss: 0.058\n",
      "Iteration: 274 \t--- Loss: 0.049\n",
      "Iteration: 275 \t--- Loss: 0.055\n",
      "Iteration: 276 \t--- Loss: 0.053\n",
      "Iteration: 277 \t--- Loss: 0.052\n",
      "Iteration: 278 \t--- Loss: 0.053\n",
      "Iteration: 279 \t--- Loss: 0.056\n",
      "Iteration: 280 \t--- Loss: 0.056\n",
      "Iteration: 281 \t--- Loss: 0.055\n",
      "Iteration: 282 \t--- Loss: 0.058\n",
      "Iteration: 283 \t--- Loss: 0.055\n",
      "Iteration: 284 \t--- Loss: 0.054\n",
      "Iteration: 285 \t--- Loss: 0.057\n",
      "Iteration: 286 \t--- Loss: 0.058\n",
      "Iteration: 287 \t--- Loss: 0.054\n",
      "Iteration: 288 \t--- Loss: 0.057\n",
      "Iteration: 289 \t--- Loss: 0.055\n",
      "Iteration: 290 \t--- Loss: 0.054\n",
      "Iteration: 291 \t--- Loss: 0.054\n",
      "Iteration: 292 \t--- Loss: 0.058\n",
      "Iteration: 293 \t--- Loss: 0.056\n",
      "Iteration: 294 \t--- Loss: 0.054\n",
      "Iteration: 295 \t--- Loss: 0.056\n",
      "Iteration: 296 \t--- Loss: 0.056\n",
      "Iteration: 297 \t--- Loss: 0.055\n",
      "Iteration: 298 \t--- Loss: 0.055\n",
      "Iteration: 299 \t--- Loss: 0.055\n",
      "Iteration: 300 \t--- Loss: 0.056\n",
      "Iteration: 301 \t--- Loss: 0.053\n",
      "Iteration: 302 \t--- Loss: 0.056\n",
      "Iteration: 303 \t--- Loss: 0.056\n",
      "Iteration: 304 \t--- Loss: 0.055\n",
      "Iteration: 305 \t--- Loss: 0.053\n",
      "Iteration: 306 \t--- Loss: 0.054\n",
      "Iteration: 307 \t--- Loss: 0.055\n",
      "Iteration: 308 \t--- Loss: 0.055\n",
      "Iteration: 309 \t--- Loss: 0.055\n",
      "Iteration: 310 \t--- Loss: 0.057\n",
      "Iteration: 311 \t--- Loss: 0.055\n",
      "Iteration: 312 \t--- Loss: 0.055\n",
      "Iteration: 313 \t--- Loss: 0.053\n",
      "Iteration: 314 \t--- Loss: 0.056\n",
      "Iteration: 315 \t--- Loss: 0.056\n",
      "Iteration: 316 \t--- Loss: 0.056\n",
      "Iteration: 317 \t--- Loss: 0.057\n",
      "Iteration: 318 \t--- Loss: 0.054\n",
      "Iteration: 319 \t--- Loss: 0.058\n",
      "Iteration: 320 \t--- Loss: 0.061\n",
      "Iteration: 321 \t--- Loss: 0.055\n",
      "Iteration: 322 \t--- Loss: 0.052\n",
      "Iteration: 323 \t--- Loss: 0.056\n",
      "Iteration: 324 \t--- Loss: 0.054\n",
      "Iteration: 325 \t--- Loss: 0.060\n",
      "Iteration: 326 \t--- Loss: 0.058\n",
      "Iteration: 327 \t--- Loss: 0.055\n",
      "Iteration: 328 \t--- Loss: 0.056\n",
      "Iteration: 329 \t--- Loss: 0.055\n",
      "Iteration: 330 \t--- Loss: 0.054\n",
      "Iteration: 331 \t--- Loss: 0.055\n",
      "Iteration: 332 \t--- Loss: 0.054\n",
      "Iteration: 333 \t--- Loss: 0.060\n",
      "Iteration: 334 \t--- Loss: 0.059\n",
      "Iteration: 335 \t--- Loss: 0.054\n",
      "Iteration: 336 \t--- Loss: 0.056\n",
      "Iteration: 337 \t--- Loss: 0.058\n",
      "Iteration: 338 \t--- Loss: 0.058\n",
      "Iteration: 339 \t--- Loss: 0.056\n",
      "Iteration: 340 \t--- Loss: 0.061\n",
      "Iteration: 341 \t--- Loss: 0.054\n",
      "Iteration: 342 \t--- Loss: 0.057\n",
      "Iteration: 343 \t--- Loss: 0.055\n",
      "Iteration: 344 \t--- Loss: 0.054\n",
      "Iteration: 345 \t--- Loss: 0.057\n",
      "Iteration: 346 \t--- Loss: 0.055\n",
      "Iteration: 347 \t--- Loss: 0.063\n",
      "Iteration: 348 \t--- Loss: 0.053\n",
      "Iteration: 349 \t--- Loss: 0.059\n",
      "Iteration: 350 \t--- Loss: 0.058\n",
      "Iteration: 351 \t--- Loss: 0.055\n",
      "Iteration: 352 \t--- Loss: 0.053\n",
      "Iteration: 353 \t--- Loss: 0.056\n",
      "Iteration: 354 \t--- Loss: 0.052\n",
      "Iteration: 355 \t--- Loss: 0.057\n",
      "Iteration: 356 \t--- Loss: 0.057\n",
      "Iteration: 357 \t--- Loss: 0.052\n",
      "Iteration: 358 \t--- Loss: 0.060\n",
      "Iteration: 359 \t--- Loss: 0.056\n",
      "Iteration: 360 \t--- Loss: 0.056\n",
      "Iteration: 361 \t--- Loss: 0.052\n",
      "Iteration: 362 \t--- Loss: 0.053\n",
      "Iteration: 363 \t--- Loss: 0.056\n",
      "Iteration: 364 \t--- Loss: 0.054\n",
      "Iteration: 365 \t--- Loss: 0.057\n",
      "Iteration: 366 \t--- Loss: 0.057\n",
      "Iteration: 367 \t--- Loss: 0.052\n",
      "Iteration: 368 \t--- Loss: 0.057\n",
      "Iteration: 369 \t--- Loss: 0.056\n",
      "Iteration: 370 \t--- Loss: 0.052\n",
      "Iteration: 371 \t--- Loss: 0.054\n",
      "Iteration: 372 \t--- Loss: 0.056\n",
      "Iteration: 373 \t--- Loss: 0.057\n",
      "Iteration: 374 \t--- Loss: 0.055\n",
      "Iteration: 375 \t--- Loss: 0.052\n",
      "Iteration: 376 \t--- Loss: 0.059\n",
      "Iteration: 377 \t--- Loss: 0.056\n",
      "Iteration: 378 \t--- Loss: 0.058\n",
      "Iteration: 379 \t--- Loss: 0.055\n",
      "Iteration: 380 \t--- Loss: 0.057\n",
      "Iteration: 381 \t--- Loss: 0.054\n",
      "Iteration: 382 \t--- Loss: 0.056\n",
      "Iteration: 383 \t--- Loss: 0.050\n",
      "Iteration: 384 \t--- Loss: 0.052\n",
      "Iteration: 385 \t--- Loss: 0.052\n",
      "Iteration: 386 \t--- Loss: 0.054\n",
      "Iteration: 387 \t--- Loss: 0.054\n",
      "Iteration: 388 \t--- Loss: 0.055\n",
      "Iteration: 389 \t--- Loss: 0.057\n",
      "Iteration: 390 \t--- Loss: 0.054\n",
      "Iteration: 391 \t--- Loss: 0.055\n",
      "Iteration: 392 \t--- Loss: 0.055\n",
      "Iteration: 393 \t--- Loss: 0.056\n",
      "Iteration: 394 \t--- Loss: 0.052\n",
      "Iteration: 395 \t--- Loss: 0.056\n",
      "Iteration: 396 \t--- Loss: 0.056\n",
      "Iteration: 397 \t--- Loss: 0.053\n",
      "Iteration: 398 \t--- Loss: 0.055\n",
      "Iteration: 399 \t--- Loss: 0.056\n",
      "Iteration: 400 \t--- Loss: 0.057\n",
      "Iteration: 401 \t--- Loss: 0.058\n",
      "Iteration: 402 \t--- Loss: 0.057\n",
      "Iteration: 403 \t--- Loss: 0.054\n",
      "Iteration: 404 \t--- Loss: 0.055\n",
      "Iteration: 405 \t--- Loss: 0.055\n",
      "Iteration: 406 \t--- Loss: 0.054\n",
      "Iteration: 407 \t--- Loss: 0.052\n",
      "Iteration: 408 \t--- Loss: 0.054\n",
      "Iteration: 409 \t--- Loss: 0.057\n",
      "Iteration: 410 \t--- Loss: 0.052\n",
      "Iteration: 411 \t--- Loss: 0.050\n",
      "Iteration: 412 \t--- Loss: 0.056\n",
      "Iteration: 413 \t--- Loss: 0.054\n",
      "Iteration: 414 \t--- Loss: 0.055\n",
      "Iteration: 415 \t--- Loss: 0.054\n",
      "Iteration: 416 \t--- Loss: 0.052\n",
      "Iteration: 417 \t--- Loss: 0.057\n",
      "Iteration: 418 \t--- Loss: 0.061\n",
      "Iteration: 419 \t--- Loss: 0.056\n",
      "Iteration: 420 \t--- Loss: 0.057\n",
      "Iteration: 421 \t--- Loss: 0.059\n",
      "Iteration: 422 \t--- Loss: 0.057\n",
      "Iteration: 423 \t--- Loss: 0.057\n",
      "Iteration: 424 \t--- Loss: 0.053\n",
      "Iteration: 425 \t--- Loss: 0.054\n",
      "Iteration: 426 \t--- Loss: 0.060\n",
      "Iteration: 427 \t--- Loss: 0.056\n",
      "Iteration: 428 \t--- Loss: 0.059\n",
      "Iteration: 429 \t--- Loss: 0.056\n",
      "Iteration: 430 \t--- Loss: 0.057\n",
      "Iteration: 431 \t--- Loss: 0.053\n",
      "Iteration: 432 \t--- Loss: 0.060\n",
      "Iteration: 433 \t--- Loss: 0.052\n",
      "Iteration: 434 \t--- Loss: 0.056\n",
      "Iteration: 435 \t--- Loss: 0.056\n",
      "Iteration: 436 \t--- Loss: 0.053\n",
      "Iteration: 437 \t--- Loss: 0.052\n",
      "Iteration: 438 \t--- Loss: 0.056\n",
      "Iteration: 439 \t--- Loss: 0.053\n",
      "Iteration: 440 \t--- Loss: 0.060\n",
      "Iteration: 441 \t--- Loss: 0.054\n",
      "Iteration: 442 \t--- Loss: 0.053\n",
      "Iteration: 443 \t--- Loss: 0.053\n",
      "Iteration: 444 \t--- Loss: 0.055\n",
      "Iteration: 445 \t--- Loss: 0.058\n",
      "Iteration: 446 \t--- Loss: 0.057\n",
      "Iteration: 447 \t--- Loss: 0.056\n",
      "Iteration: 448 \t--- Loss: 0.056\n",
      "Iteration: 449 \t--- Loss: 0.057\n",
      "Iteration: 450 \t--- Loss: 0.055\n",
      "Iteration: 451 \t--- Loss: 0.055\n",
      "Iteration: 452 \t--- Loss: 0.055\n",
      "Iteration: 453 \t--- Loss: 0.057\n",
      "Iteration: 454 \t--- Loss: 0.058\n",
      "Iteration: 455 \t--- Loss: 0.056\n",
      "Iteration: 456 \t--- Loss: 0.056\n",
      "Iteration: 457 \t--- Loss: 0.054\n",
      "Iteration: 458 \t--- Loss: 0.057\n",
      "Iteration: 459 \t--- Loss: 0.054\n",
      "Iteration: 460 \t--- Loss: 0.055\n",
      "Iteration: 461 \t--- Loss: 0.054\n",
      "Iteration: 462 \t--- Loss: 0.054\n",
      "Iteration: 463 \t--- Loss: 0.051\n",
      "Iteration: 464 \t--- Loss: 0.056\n",
      "Iteration: 465 \t--- Loss: 0.059\n",
      "Iteration: 466 \t--- Loss: 0.060\n",
      "Iteration: 467 \t--- Loss: 0.058\n",
      "Iteration: 468 \t--- Loss: 0.057\n",
      "Iteration: 469 \t--- Loss: 0.057\n",
      "Iteration: 470 \t--- Loss: 0.055\n",
      "Iteration: 471 \t--- Loss: 0.056\n",
      "Iteration: 472 \t--- Loss: 0.054\n",
      "Iteration: 473 \t--- Loss: 0.056\n",
      "Iteration: 474 \t--- Loss: 0.050\n",
      "Iteration: 475 \t--- Loss: 0.056\n",
      "Iteration: 476 \t--- Loss: 0.058\n",
      "Iteration: 477 \t--- Loss: 0.054\n",
      "Iteration: 478 \t--- Loss: 0.053\n",
      "Iteration: 479 \t--- Loss: 0.054\n",
      "Iteration: 480 \t--- Loss: 0.057\n",
      "Iteration: 481 \t--- Loss: 0.058\n",
      "Iteration: 482 \t--- Loss: 0.054\n",
      "Iteration: 483 \t--- Loss: 0.050\n",
      "Iteration: 484 \t--- Loss: 0.056\n",
      "Iteration: 485 \t--- Loss: 0.059\n",
      "Iteration: 486 \t--- Loss: 0.058\n",
      "Iteration: 487 \t--- Loss: 0.052\n",
      "Iteration: 488 \t--- Loss: 0.052\n",
      "Iteration: 489 \t--- Loss: 0.053\n",
      "Iteration: 490 \t--- Loss: 0.053\n",
      "Iteration: 491 \t--- Loss: 0.053\n",
      "Iteration: 492 \t--- Loss: 0.055\n",
      "Iteration: 493 \t--- Loss: 0.054\n",
      "Iteration: 494 \t--- Loss: 0.054\n",
      "Iteration: 495 \t--- Loss: 0.055\n",
      "Iteration: 496 \t--- Loss: 0.056\n",
      "Iteration: 497 \t--- Loss: 0.058\n",
      "Iteration: 498 \t--- Loss: 0.053\n",
      "Iteration: 499 \t--- Loss: 0.053\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:07,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 1.527\n",
      "Iteration: 1 \t--- Loss: 1.332\n",
      "Iteration: 2 \t--- Loss: 1.280\n",
      "Iteration: 3 \t--- Loss: 1.161\n",
      "Iteration: 4 \t--- Loss: 1.130\n",
      "Iteration: 5 \t--- Loss: 1.112\n",
      "Iteration: 6 \t--- Loss: 1.044\n",
      "Iteration: 7 \t--- Loss: 1.005\n",
      "Iteration: 8 \t--- Loss: 1.019\n",
      "Iteration: 9 \t--- Loss: 0.973\n",
      "Iteration: 10 \t--- Loss: 0.949\n",
      "Iteration: 11 \t--- Loss: 0.957\n",
      "Iteration: 12 \t--- Loss: 0.961\n",
      "Iteration: 13 \t--- Loss: 0.921\n",
      "Iteration: 14 \t--- Loss: 0.955\n",
      "Iteration: 15 \t--- Loss: 0.916\n",
      "Iteration: 16 \t--- Loss: 0.960\n",
      "Iteration: 17 \t--- Loss: 0.958\n",
      "Iteration: 18 \t--- Loss: 0.940\n",
      "Iteration: 19 \t--- Loss: 0.885\n",
      "Iteration: 20 \t--- Loss: 0.930\n",
      "Iteration: 21 \t--- Loss: 0.923\n",
      "Iteration: 22 \t--- Loss: 0.944\n",
      "Iteration: 23 \t--- Loss: 0.917\n",
      "Iteration: 24 \t--- Loss: 0.925\n",
      "Iteration: 25 \t--- Loss: 0.949\n",
      "Iteration: 26 \t--- Loss: 0.917\n",
      "Iteration: 27 \t--- Loss: 0.886\n",
      "Iteration: 28 \t--- Loss: 0.908\n",
      "Iteration: 29 \t--- Loss: 0.909\n",
      "Iteration: 30 \t--- Loss: 0.917\n",
      "Iteration: 31 \t--- Loss: 0.942\n",
      "Iteration: 32 \t--- Loss: 0.889\n",
      "Iteration: 33 \t--- Loss: 0.922\n",
      "Iteration: 34 \t--- Loss: 0.899\n",
      "Iteration: 35 \t--- Loss: 0.911\n",
      "Iteration: 36 \t--- Loss: 0.938\n",
      "Iteration: 37 \t--- Loss: 0.941\n",
      "Iteration: 38 \t--- Loss: 0.903\n",
      "Iteration: 39 \t--- Loss: 0.941\n",
      "Iteration: 40 \t--- Loss: 0.928\n",
      "Iteration: 41 \t--- Loss: 0.939\n",
      "Iteration: 42 \t--- Loss: 0.901\n",
      "Iteration: 43 \t--- Loss: 0.913\n",
      "Iteration: 44 \t--- Loss: 0.904\n",
      "Iteration: 45 \t--- Loss: 0.933\n",
      "Iteration: 46 \t--- Loss: 0.900\n",
      "Iteration: 47 \t--- Loss: 0.927\n",
      "Iteration: 48 \t--- Loss: 0.936\n",
      "Iteration: 49 \t--- Loss: 0.884\n",
      "Iteration: 50 \t--- Loss: 0.924\n",
      "Iteration: 51 \t--- Loss: 0.896\n",
      "Iteration: 52 \t--- Loss: 0.944\n",
      "Iteration: 53 \t--- Loss: 0.924\n",
      "Iteration: 54 \t--- Loss: 0.914\n",
      "Iteration: 55 \t--- Loss: 0.925\n",
      "Iteration: 56 \t--- Loss: 0.927\n",
      "Iteration: 57 \t--- Loss: 0.928\n",
      "Iteration: 58 \t--- Loss: 0.926\n",
      "Iteration: 59 \t--- Loss: 0.899\n",
      "Iteration: 60 \t--- Loss: 0.878\n",
      "Iteration: 61 \t--- Loss: 0.899\n",
      "Iteration: 62 \t--- Loss: 0.877\n",
      "Iteration: 63 \t--- Loss: 0.877\n",
      "Iteration: 64 \t--- Loss: 0.909\n",
      "Iteration: 65 \t--- Loss: 0.967\n",
      "Iteration: 66 \t--- Loss: 0.934\n",
      "Iteration: 67 \t--- Loss: 0.890\n",
      "Iteration: 68 \t--- Loss: 0.931\n",
      "Iteration: 69 \t--- Loss: 0.919\n",
      "Iteration: 70 \t--- Loss: 0.922\n",
      "Iteration: 71 \t--- Loss: 0.931\n",
      "Iteration: 72 \t--- Loss: 0.911\n",
      "Iteration: 73 \t--- Loss: 0.935\n",
      "Iteration: 74 \t--- Loss: 0.898\n",
      "Iteration: 75 \t--- Loss: 0.901\n",
      "Iteration: 76 \t--- Loss: 0.922\n",
      "Iteration: 77 \t--- Loss: 0.905\n",
      "Iteration: 78 \t--- Loss: 0.903\n",
      "Iteration: 79 \t--- Loss: 0.911\n",
      "Iteration: 80 \t--- Loss: 0.905\n",
      "Iteration: 81 \t--- Loss: 0.905\n",
      "Iteration: 82 \t--- Loss: 0.949\n",
      "Iteration: 83 \t--- Loss: 0.909\n",
      "Iteration: 84 \t--- Loss: 0.924\n",
      "Iteration: 85 \t--- Loss: 0.942\n",
      "Iteration: 86 \t--- Loss: 0.883\n",
      "Iteration: 87 \t--- Loss: 0.926\n",
      "Iteration: 88 \t--- Loss: 0.912\n",
      "Iteration: 89 \t--- Loss: 0.920\n",
      "Iteration: 90 \t--- Loss: 0.900\n",
      "Iteration: 91 \t--- Loss: 0.917\n",
      "Iteration: 92 \t--- Loss: 0.926\n",
      "Iteration: 93 \t--- Loss: 0.946\n",
      "Iteration: 94 \t--- Loss: 0.926\n",
      "Iteration: 95 \t--- Loss: 0.924\n",
      "Iteration: 96 \t--- Loss: 0.935\n",
      "Iteration: 97 \t--- Loss: 0.940\n",
      "Iteration: 98 \t--- Loss: 0.887\n",
      "Iteration: 99 \t--- Loss: 0.877\n",
      "Iteration: 100 \t--- Loss: 0.898\n",
      "Iteration: 101 \t--- Loss: 0.938\n",
      "Iteration: 102 \t--- Loss: 0.940\n",
      "Iteration: 103 \t--- Loss: 0.923\n",
      "Iteration: 104 \t--- Loss: 0.920\n",
      "Iteration: 105 \t--- Loss: 0.931\n",
      "Iteration: 106 \t--- Loss: 0.920\n",
      "Iteration: 107 \t--- Loss: 0.889\n",
      "Iteration: 108 \t--- Loss: 0.955\n",
      "Iteration: 109 \t--- Loss: 0.891\n",
      "Iteration: 110 \t--- Loss: 0.928\n",
      "Iteration: 111 \t--- Loss: 0.923\n",
      "Iteration: 112 \t--- Loss: 0.946\n",
      "Iteration: 113 \t--- Loss: 0.928\n",
      "Iteration: 114 \t--- Loss: 0.941\n",
      "Iteration: 115 \t--- Loss: 0.919\n",
      "Iteration: 116 \t--- Loss: 0.900\n",
      "Iteration: 117 \t--- Loss: 0.893\n",
      "Iteration: 118 \t--- Loss: 0.930\n",
      "Iteration: 119 \t--- Loss: 0.922\n",
      "Iteration: 120 \t--- Loss: 0.927\n",
      "Iteration: 121 \t--- Loss: 0.918\n",
      "Iteration: 122 \t--- Loss: 0.902\n",
      "Iteration: 123 \t--- Loss: 0.896\n",
      "Iteration: 124 \t--- Loss: 0.902\n",
      "Iteration: 125 \t--- Loss: 0.891\n",
      "Iteration: 126 \t--- Loss: 0.932\n",
      "Iteration: 127 \t--- Loss: 0.909\n",
      "Iteration: 128 \t--- Loss: 0.872\n",
      "Iteration: 129 \t--- Loss: 0.925\n",
      "Iteration: 130 \t--- Loss: 0.942\n",
      "Iteration: 131 \t--- Loss: 0.910\n",
      "Iteration: 132 \t--- Loss: 0.917\n",
      "Iteration: 133 \t--- Loss: 0.926\n",
      "Iteration: 134 \t--- Loss: 0.906\n",
      "Iteration: 135 \t--- Loss: 0.901\n",
      "Iteration: 136 \t--- Loss: 0.906\n",
      "Iteration: 137 \t--- Loss: 0.933\n",
      "Iteration: 138 \t--- Loss: 0.907\n",
      "Iteration: 139 \t--- Loss: 0.912\n",
      "Iteration: 140 \t--- Loss: 0.902\n",
      "Iteration: 141 \t--- Loss: 0.936\n",
      "Iteration: 142 \t--- Loss: 0.958\n",
      "Iteration: 143 \t--- Loss: 0.904\n",
      "Iteration: 144 \t--- Loss: 0.914\n",
      "Iteration: 145 \t--- Loss: 0.910\n",
      "Iteration: 146 \t--- Loss: 0.936\n",
      "Iteration: 147 \t--- Loss: 0.950\n",
      "Iteration: 148 \t--- Loss: 0.907\n",
      "Iteration: 149 \t--- Loss: 0.877\n",
      "Iteration: 150 \t--- Loss: 0.933\n",
      "Iteration: 151 \t--- Loss: 0.931\n",
      "Iteration: 152 \t--- Loss: 0.942\n",
      "Iteration: 153 \t--- Loss: 0.902\n",
      "Iteration: 154 \t--- Loss: 0.898\n",
      "Iteration: 155 \t--- Loss: 0.934\n",
      "Iteration: 156 \t--- Loss: 0.901\n",
      "Iteration: 157 \t--- Loss: 0.925\n",
      "Iteration: 158 \t--- Loss: 0.956\n",
      "Iteration: 159 \t--- Loss: 0.912\n",
      "Iteration: 160 \t--- Loss: 0.886\n",
      "Iteration: 161 \t--- Loss: 0.939\n",
      "Iteration: 162 \t--- Loss: 0.912\n",
      "Iteration: 163 \t--- Loss: 0.910\n",
      "Iteration: 164 \t--- Loss: 0.904\n",
      "Iteration: 165 \t--- Loss: 0.873\n",
      "Iteration: 166 \t--- Loss: 0.939\n",
      "Iteration: 167 \t--- Loss: 0.904\n",
      "Iteration: 168 \t--- Loss: 0.925\n",
      "Iteration: 169 \t--- Loss: 0.925\n",
      "Iteration: 170 \t--- Loss: 0.917\n",
      "Iteration: 171 \t--- Loss: 0.918\n",
      "Iteration: 172 \t--- Loss: 0.935\n",
      "Iteration: 173 \t--- Loss: 0.910\n",
      "Iteration: 174 \t--- Loss: 0.927\n",
      "Iteration: 175 \t--- Loss: 0.875\n",
      "Iteration: 176 \t--- Loss: 0.927\n",
      "Iteration: 177 \t--- Loss: 0.891\n",
      "Iteration: 178 \t--- Loss: 0.911\n",
      "Iteration: 179 \t--- Loss: 0.901\n",
      "Iteration: 180 \t--- Loss: 0.911\n",
      "Iteration: 181 \t--- Loss: 0.902\n",
      "Iteration: 182 \t--- Loss: 0.912\n",
      "Iteration: 183 \t--- Loss: 0.925\n",
      "Iteration: 184 \t--- Loss: 0.928\n",
      "Iteration: 185 \t--- Loss: 0.933\n",
      "Iteration: 186 \t--- Loss: 0.878\n",
      "Iteration: 187 \t--- Loss: 0.932\n",
      "Iteration: 188 \t--- Loss: 0.916\n",
      "Iteration: 189 \t--- Loss: 0.940\n",
      "Iteration: 190 \t--- Loss: 0.920\n",
      "Iteration: 191 \t--- Loss: 0.922\n",
      "Iteration: 192 \t--- Loss: 0.916\n",
      "Iteration: 193 \t--- Loss: 0.935\n",
      "Iteration: 194 \t--- Loss: 0.934\n",
      "Iteration: 195 \t--- Loss: 0.895\n",
      "Iteration: 196 \t--- Loss: 0.907\n",
      "Iteration: 197 \t--- Loss: 0.910\n",
      "Iteration: 198 \t--- Loss: 0.903\n",
      "Iteration: 199 \t--- Loss: 0.885\n",
      "Iteration: 200 \t--- Loss: 0.944\n",
      "Iteration: 201 \t--- Loss: 0.977\n",
      "Iteration: 202 \t--- Loss: 0.927\n",
      "Iteration: 203 \t--- Loss: 0.878\n",
      "Iteration: 204 \t--- Loss: 0.912\n",
      "Iteration: 205 \t--- Loss: 0.903\n",
      "Iteration: 206 \t--- Loss: 0.930\n",
      "Iteration: 207 \t--- Loss: 0.945\n",
      "Iteration: 208 \t--- Loss: 0.923\n",
      "Iteration: 209 \t--- Loss: 0.903\n",
      "Iteration: 210 \t--- Loss: 0.904\n",
      "Iteration: 211 \t--- Loss: 0.905\n",
      "Iteration: 212 \t--- Loss: 0.927\n",
      "Iteration: 213 \t--- Loss: 0.911\n",
      "Iteration: 214 \t--- Loss: 0.909\n",
      "Iteration: 215 \t--- Loss: 0.940\n",
      "Iteration: 216 \t--- Loss: 0.935\n",
      "Iteration: 217 \t--- Loss: 0.933\n",
      "Iteration: 218 \t--- Loss: 0.956\n",
      "Iteration: 219 \t--- Loss: 0.874\n",
      "Iteration: 220 \t--- Loss: 0.950\n",
      "Iteration: 221 \t--- Loss: 0.950\n",
      "Iteration: 222 \t--- Loss: 0.902\n",
      "Iteration: 223 \t--- Loss: 0.907\n",
      "Iteration: 224 \t--- Loss: 0.916\n",
      "Iteration: 225 \t--- Loss: 0.925\n",
      "Iteration: 226 \t--- Loss: 0.950\n",
      "Iteration: 227 \t--- Loss: 0.930\n",
      "Iteration: 228 \t--- Loss: 0.924\n",
      "Iteration: 229 \t--- Loss: 0.903\n",
      "Iteration: 230 \t--- Loss: 0.907\n",
      "Iteration: 231 \t--- Loss: 0.939\n",
      "Iteration: 232 \t--- Loss: 0.896\n",
      "Iteration: 233 \t--- Loss: 0.901\n",
      "Iteration: 234 \t--- Loss: 0.932\n",
      "Iteration: 235 \t--- Loss: 0.934\n",
      "Iteration: 236 \t--- Loss: 0.930\n",
      "Iteration: 237 \t--- Loss: 0.922\n",
      "Iteration: 238 \t--- Loss: 0.920\n",
      "Iteration: 239 \t--- Loss: 0.887\n",
      "Iteration: 240 \t--- Loss: 0.891\n",
      "Iteration: 241 \t--- Loss: 0.885\n",
      "Iteration: 242 \t--- Loss: 0.931\n",
      "Iteration: 243 \t--- Loss: 0.910\n",
      "Iteration: 244 \t--- Loss: 0.904\n",
      "Iteration: 245 \t--- Loss: 0.943\n",
      "Iteration: 246 \t--- Loss: 0.934\n",
      "Iteration: 247 \t--- Loss: 0.889\n",
      "Iteration: 248 \t--- Loss: 0.894\n",
      "Iteration: 249 \t--- Loss: 0.905\n",
      "Iteration: 250 \t--- Loss: 0.917\n",
      "Iteration: 251 \t--- Loss: 0.927\n",
      "Iteration: 252 \t--- Loss: 0.897\n",
      "Iteration: 253 \t--- Loss: 0.927\n",
      "Iteration: 254 \t--- Loss: 0.954\n",
      "Iteration: 255 \t--- Loss: 0.927\n",
      "Iteration: 256 \t--- Loss: 0.890\n",
      "Iteration: 257 \t--- Loss: 0.919\n",
      "Iteration: 258 \t--- Loss: 0.910\n",
      "Iteration: 259 \t--- Loss: 0.930"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:12<00:03,  1.60s/it][Parallel(n_jobs=5)]: Done  72 tasks      | elapsed: 46.8min\n",
      " 90%|█████████ | 9/10 [00:13<00:01,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.747\n",
      "Iteration: 1 \t--- Loss: 0.718\n",
      "Iteration: 2 \t--- Loss: 0.684\n",
      "Iteration: 3 \t--- Loss: 0.655\n",
      "Iteration: 4 \t--- Loss: 0.634\n",
      "Iteration: 5 \t--- Loss: 0.610\n",
      "Iteration: 6 \t--- Loss: 0.598\n",
      "Iteration: 7 \t--- Loss: 0.590\n",
      "Iteration: 8 \t--- Loss: 0.583\n",
      "Iteration: 9 \t--- Loss: 0.557\n",
      "Iteration: 10 \t--- Loss: 0.538\n",
      "Iteration: 11 \t--- Loss: 0.530\n",
      "Iteration: 12 \t--- Loss: 0.534\n",
      "Iteration: 13 \t--- Loss: 0.515\n",
      "Iteration: 14 \t--- Loss: 0.509\n",
      "Iteration: 15 \t--- Loss: 0.521\n",
      "Iteration: 16 \t--- Loss: 0.491\n",
      "Iteration: 17 \t--- Loss: 0.455\n",
      "Iteration: 18 \t--- Loss: 0.482\n",
      "Iteration: 19 \t--- Loss: 0.486\n",
      "Iteration: 20 \t--- Loss: 0.473\n",
      "Iteration: 21 \t--- Loss: 0.446\n",
      "Iteration: 22 \t--- Loss: 0.464\n",
      "Iteration: 23 \t--- Loss: 0.429\n",
      "Iteration: 24 \t--- Loss: 0.425\n",
      "Iteration: 25 \t--- Loss: 0.399\n",
      "Iteration: 26 \t--- Loss: 0.415\n",
      "Iteration: 27 \t--- Loss: 0.394\n",
      "Iteration: 28 \t--- Loss: 0.410\n",
      "Iteration: 29 \t--- Loss: 0.357\n",
      "Iteration: 30 \t--- Loss: 0.344\n",
      "Iteration: 31 \t--- Loss: 0.379\n",
      "Iteration: 32 \t--- Loss: 0.366\n",
      "Iteration: 33 \t--- Loss: 0.364\n",
      "Iteration: 34 \t--- Loss: 0.550\n",
      "Iteration: 35 \t--- Loss: 1.193\n",
      "\n",
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it][Parallel(n_jobs=5)]: Done  73 tasks      | elapsed: 46.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [01:24<00:00, 84.22s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.920\n",
      "Iteration: 261 \t--- Loss: 0.894\n",
      "Iteration: 262 \t--- Loss: 0.909\n",
      "Iteration: 263 \t--- Loss: 0.910\n",
      "Iteration: 264 \t--- Loss: 0.941\n",
      "Iteration: 265 \t--- Loss: 0.920\n",
      "Iteration: 266 \t--- Loss: 0.939\n",
      "Iteration: 267 \t--- Loss: 0.938\n",
      "Iteration: 268 \t--- Loss: 0.907\n",
      "Iteration: 269 \t--- Loss: 0.941\n",
      "Iteration: 270 \t--- Loss: 0.916\n",
      "Iteration: 271 \t--- Loss: 0.908\n",
      "Iteration: 272 \t--- Loss: 0.909\n",
      "Iteration: 273 \t--- Loss: 0.944\n",
      "Iteration: 274 \t--- Loss: 0.951\n",
      "Iteration: 275 \t--- Loss: 0.916\n",
      "Iteration: 276 \t--- Loss: 0.933\n",
      "Iteration: 277 \t--- Loss: 0.923\n",
      "Iteration: 278 \t--- Loss: 0.904\n",
      "Iteration: 279 \t--- Loss: 0.951\n",
      "Iteration: 280 \t--- Loss: 0.904\n",
      "Iteration: 281 \t--- Loss: 0.939\n",
      "Iteration: 282 \t--- Loss: 0.902\n",
      "Iteration: 283 \t--- Loss: 0.879\n",
      "Iteration: 284 \t--- Loss: 0.908\n",
      "Iteration: 285 \t--- Loss: 0.897\n",
      "Iteration: 286 \t--- Loss: 0.885\n",
      "Iteration: 287 \t--- Loss: 0.932\n",
      "Iteration: 288 \t--- Loss: 0.873\n",
      "Iteration: 289 \t--- Loss: 0.924\n",
      "Iteration: 290 \t--- Loss: 0.935\n",
      "Iteration: 291 \t--- Loss: 0.898\n",
      "Iteration: 292 \t--- Loss: 0.901\n",
      "Iteration: 293 \t--- Loss: 0.913\n",
      "Iteration: 294 \t--- Loss: 0.922\n",
      "Iteration: 295 \t--- Loss: 0.903\n",
      "Iteration: 296 \t--- Loss: 0.932\n",
      "Iteration: 297 \t--- Loss: 0.892\n",
      "Iteration: 298 \t--- Loss: 0.908\n",
      "Iteration: 299 \t--- Loss: 0.906\n",
      "Iteration: 300 \t--- Loss: 0.919\n",
      "Iteration: 301 \t--- Loss: 0.923\n",
      "Iteration: 302 \t--- Loss: 0.928\n",
      "Iteration: 303 \t--- Loss: 0.864\n",
      "Iteration: 304 \t--- Loss: 0.902\n",
      "Iteration: 305 \t--- Loss: 0.935\n",
      "Iteration: 306 \t--- Loss: 0.942\n",
      "Iteration: 307 \t--- Loss: 0.946\n",
      "Iteration: 308 \t--- Loss: 0.953\n",
      "Iteration: 309 \t--- Loss: 0.907\n",
      "Iteration: 310 \t--- Loss: 0.895\n",
      "Iteration: 311 \t--- Loss: 0.916\n",
      "Iteration: 312 \t--- Loss: 0.900\n",
      "Iteration: 313 \t--- Loss: 0.890\n",
      "Iteration: 314 \t--- Loss: 0.924\n",
      "Iteration: 315 \t--- Loss: 0.921\n",
      "Iteration: 316 \t--- Loss: 0.923\n",
      "Iteration: 317 \t--- Loss: 0.914\n",
      "Iteration: 318 \t--- Loss: 0.867\n",
      "Iteration: 319 \t--- Loss: 0.929\n",
      "Iteration: 320 \t--- Loss: 0.912\n",
      "Iteration: 321 \t--- Loss: 0.938\n",
      "Iteration: 322 \t--- Loss: 0.940\n",
      "Iteration: 323 \t--- Loss: 0.884\n",
      "Iteration: 324 \t--- Loss: 0.913\n",
      "Iteration: 325 \t--- Loss: 0.917\n",
      "Iteration: 326 \t--- Loss: 0.929\n",
      "Iteration: 327 \t--- Loss: 0.901\n",
      "Iteration: 328 \t--- Loss: 0.926\n",
      "Iteration: 329 \t--- Loss: 0.897\n",
      "Iteration: 330 \t--- Loss: 0.906\n",
      "Iteration: 331 \t--- Loss: 0.880\n",
      "Iteration: 332 \t--- Loss: 0.913\n",
      "Iteration: 333 \t--- Loss: 0.925\n",
      "Iteration: 334 \t--- Loss: 0.901\n",
      "Iteration: 335 \t--- Loss: 0.899\n",
      "Iteration: 336 \t--- Loss: 0.941\n",
      "Iteration: 337 \t--- Loss: 0.909\n",
      "Iteration: 338 \t--- Loss: 0.909\n",
      "Iteration: 339 \t--- Loss: 0.912\n",
      "Iteration: 340 \t--- Loss: 0.903\n",
      "Iteration: 341 \t--- Loss: 0.930\n",
      "Iteration: 342 \t--- Loss: 0.954\n",
      "Iteration: 343 \t--- Loss: 0.962\n",
      "Iteration: 344 \t--- Loss: 0.960\n",
      "Iteration: 345 \t--- Loss: 0.906\n",
      "Iteration: 346 \t--- Loss: 0.927\n",
      "Iteration: 347 \t--- Loss: 0.923\n",
      "Iteration: 348 \t--- Loss: 0.909\n",
      "Iteration: 349 \t--- Loss: 0.926\n",
      "Iteration: 350 \t--- Loss: 0.949\n",
      "Iteration: 351 \t--- Loss: 0.907\n",
      "Iteration: 352 \t--- Loss: 0.885\n",
      "Iteration: 353 \t--- Loss: 0.926\n",
      "Iteration: 354 \t--- Loss: 0.915\n",
      "Iteration: 355 \t--- Loss: 0.877\n",
      "Iteration: 356 \t--- Loss: 0.889\n",
      "Iteration: 357 \t--- Loss: 0.933\n",
      "Iteration: 358 \t--- Loss: 0.927\n",
      "Iteration: 359 \t--- Loss: 0.905\n",
      "Iteration: 360 \t--- Loss: 0.927\n",
      "Iteration: 361 \t--- Loss: 0.917\n",
      "Iteration: 362 \t--- Loss: 0.917\n",
      "Iteration: 363 \t--- Loss: 0.924\n",
      "Iteration: 364 \t--- Loss: 0.918\n",
      "Iteration: 365 \t--- Loss: 0.909\n",
      "Iteration: 366 \t--- Loss: 0.931\n",
      "Iteration: 367 \t--- Loss: 0.965\n",
      "Iteration: 368 \t--- Loss: 0.921\n",
      "Iteration: 369 \t--- Loss: 0.910\n",
      "Iteration: 370 \t--- Loss: 0.911\n",
      "Iteration: 371 \t--- Loss: 0.894\n",
      "Iteration: 372 \t--- Loss: 0.927\n",
      "Iteration: 373 \t--- Loss: 0.898\n",
      "Iteration: 374 \t--- Loss: 0.875\n",
      "Iteration: 375 \t--- Loss: 0.944\n",
      "Iteration: 376 \t--- Loss: 0.890\n",
      "Iteration: 377 \t--- Loss: 0.931\n",
      "Iteration: 378 \t--- Loss: 0.924\n",
      "Iteration: 379 \t--- Loss: 0.890\n",
      "Iteration: 380 \t--- Loss: 0.923\n",
      "Iteration: 381 \t--- Loss: 0.909\n",
      "Iteration: 382 \t--- Loss: 0.937\n",
      "Iteration: 383 \t--- Loss: 0.878\n",
      "Iteration: 384 \t--- Loss: 0.910\n",
      "Iteration: 385 \t--- Loss: 0.896\n",
      "Iteration: 386 \t--- Loss: 0.867\n",
      "Iteration: 387 \t--- Loss: 0.894\n",
      "Iteration: 388 \t--- Loss: 0.940\n",
      "Iteration: 389 \t--- Loss: 0.918\n",
      "Iteration: 390 \t--- Loss: 0.911\n",
      "Iteration: 391 \t--- Loss: 0.916\n",
      "Iteration: 392 \t--- Loss: 0.900\n",
      "Iteration: 393 \t--- Loss: 0.900\n",
      "Iteration: 394 \t--- Loss: 0.925\n",
      "Iteration: 395 \t--- Loss: 0.919\n",
      "Iteration: 396 \t--- Loss: 0.930\n",
      "Iteration: 397 \t--- Loss: 0.922\n",
      "Iteration: 398 \t--- Loss: 0.927\n",
      "Iteration: 399 \t--- Loss: 0.928\n",
      "Iteration: 400 \t--- Loss: 0.913\n",
      "Iteration: 401 \t--- Loss: 0.925\n",
      "Iteration: 402 \t--- Loss: 0.899\n",
      "Iteration: 403 \t--- Loss: 0.929\n",
      "Iteration: 404 \t--- Loss: 0.933\n",
      "Iteration: 405 \t--- Loss: 0.927\n",
      "Iteration: 406 \t--- Loss: 0.872\n",
      "Iteration: 407 \t--- Loss: 0.928\n",
      "Iteration: 408 \t--- Loss: 0.903\n",
      "Iteration: 409 \t--- Loss: 0.949\n",
      "Iteration: 410 \t--- Loss: 0.933\n",
      "Iteration: 411 \t--- Loss: 0.934\n",
      "Iteration: 412 \t--- Loss: 0.882\n",
      "Iteration: 413 \t--- Loss: 0.958\n",
      "Iteration: 414 \t--- Loss: 0.898\n",
      "Iteration: 415 \t--- Loss: 0.913\n",
      "Iteration: 416 \t--- Loss: 0.935\n",
      "Iteration: 417 \t--- Loss: 0.902\n",
      "Iteration: 418 \t--- Loss: 0.934\n",
      "Iteration: 419 \t--- Loss: 0.941\n",
      "Iteration: 420 \t--- Loss: 0.915\n",
      "Iteration: 421 \t--- Loss: 0.947\n",
      "Iteration: 422 \t--- Loss: 0.935\n",
      "Iteration: 423 \t--- Loss: 0.912\n",
      "Iteration: 424 \t--- Loss: 0.904\n",
      "Iteration: 425 \t--- Loss: 0.898\n",
      "Iteration: 426 \t--- Loss: 0.902\n",
      "Iteration: 427 \t--- Loss: 0.886\n",
      "Iteration: 428 \t--- Loss: 0.923\n",
      "Iteration: 429 \t--- Loss: 0.899\n",
      "Iteration: 430 \t--- Loss: 0.901\n",
      "Iteration: 431 \t--- Loss: 0.905\n",
      "Iteration: 432 \t--- Loss: 0.926\n",
      "Iteration: 433 \t--- Loss: 0.913\n",
      "Iteration: 434 \t--- Loss: 0.918\n",
      "Iteration: 435 \t--- Loss: 0.925\n",
      "Iteration: 436 \t--- Loss: 0.915\n",
      "Iteration: 437 \t--- Loss: 0.906\n",
      "Iteration: 438 \t--- Loss: 0.924\n",
      "Iteration: 439 \t--- Loss: 0.904\n",
      "Iteration: 440 \t--- Loss: 0.879\n",
      "Iteration: 441 \t--- Loss: 0.957\n",
      "Iteration: 442 \t--- Loss: 0.939\n",
      "Iteration: 443 \t--- Loss: 0.897\n",
      "Iteration: 444 \t--- Loss: 0.925\n",
      "Iteration: 445 \t--- Loss: 0.908\n",
      "Iteration: 446 \t--- Loss: 0.902\n",
      "Iteration: 447 \t--- Loss: 0.886\n",
      "Iteration: 448 \t--- Loss: 0.930\n",
      "Iteration: 449 \t--- Loss: 0.871\n",
      "Iteration: 450 \t--- Loss: 0.948\n",
      "Iteration: 451 \t--- Loss: 0.946\n",
      "Iteration: 452 \t--- Loss: 0.922\n",
      "Iteration: 453 \t--- Loss: 0.913\n",
      "Iteration: 454 \t--- Loss: 0.905\n",
      "Iteration: 455 \t--- Loss: 0.934\n",
      "Iteration: 456 \t--- Loss: 0.897\n",
      "Iteration: 457 \t--- Loss: 0.921\n",
      "Iteration: 458 \t--- Loss: 0.901\n",
      "Iteration: 459 \t--- Loss: 0.921\n",
      "Iteration: 460 \t--- Loss: 0.915\n",
      "Iteration: 461 \t--- Loss: 0.913\n",
      "Iteration: 462 \t--- Loss: 0.919\n",
      "Iteration: 463 \t--- Loss: 0.913\n",
      "Iteration: 464 \t--- Loss: 0.909\n",
      "Iteration: 465 \t--- Loss: 0.913\n",
      "Iteration: 466 \t--- Loss: 0.919\n",
      "Iteration: 467 \t--- Loss: 0.897\n",
      "Iteration: 468 \t--- Loss: 0.902\n",
      "Iteration: 469 \t--- Loss: 0.928\n",
      "Iteration: 470 \t--- Loss: 0.912\n",
      "Iteration: 471 \t--- Loss: 0.944\n",
      "Iteration: 472 \t--- Loss: 0.912\n",
      "Iteration: 473 \t--- Loss: 0.925\n",
      "Iteration: 474 \t--- Loss: 0.878\n",
      "Iteration: 475 \t--- Loss: 0.950\n",
      "Iteration: 476 \t--- Loss: 0.945\n",
      "Iteration: 477 \t--- Loss: 0.906\n",
      "Iteration: 478 \t--- Loss: 0.915\n",
      "Iteration: 479 \t--- Loss: 0.927\n",
      "Iteration: 480 \t--- Loss: 0.912\n",
      "Iteration: 481 \t--- Loss: 0.923\n",
      "Iteration: 482 \t--- Loss: 0.892\n",
      "Iteration: 483 \t--- Loss: 0.921\n",
      "Iteration: 484 \t--- Loss: 0.882\n",
      "Iteration: 485 \t--- Loss: 0.917\n",
      "Iteration: 486 \t--- Loss: 0.924\n",
      "Iteration: 487 \t--- Loss: 0.920\n",
      "Iteration: 488 \t--- Loss: 0.931\n",
      "Iteration: 489 \t--- Loss: 0.900\n",
      "Iteration: 490 \t--- Loss: 0.953\n",
      "Iteration: 491 \t--- Loss: 0.894\n",
      "Iteration: 492 \t--- Loss: 0.907\n",
      "Iteration: 493 \t--- Loss: 0.870\n",
      "Iteration: 494 \t--- Loss: 0.938\n",
      "Iteration: 495 \t--- Loss: 0.908\n",
      "Iteration: 496 \t--- Loss: 0.904\n",
      "Iteration: 497 \t--- Loss: 0.903\n",
      "Iteration: 498 \t--- Loss: 0.921\n",
      "Iteration: 499 \t--- Loss: 0.919\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [09:59<?, ?it/s][Parallel(n_jobs=5)]: Done  74 tasks      | elapsed: 47.4min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 1.372\n",
      "Iteration: 1 \t--- Loss: 1.270\n",
      "Iteration: 2 \t--- Loss: 1.189\n",
      "Iteration: 3 \t--- Loss: 1.175\n",
      "Iteration: 4 \t--- Loss: 1.137\n",
      "Iteration: 5 \t--- Loss: 1.119\n",
      "Iteration: 6 \t--- Loss: 1.087\n",
      "Iteration: 7 \t--- Loss: 1.034\n",
      "Iteration: 8 \t--- Loss: 1.089\n",
      "Iteration: 9 \t--- Loss: 0.993\n",
      "Iteration: 10 \t--- Loss: 1.067\n",
      "Iteration: 11 \t--- Loss: 1.031\n",
      "Iteration: 12 \t--- Loss: 1.008\n",
      "Iteration: 13 \t--- Loss: 0.972\n",
      "Iteration: 14 \t--- Loss: 0.994\n",
      "Iteration: 15 \t--- Loss: 0.988\n",
      "Iteration: 16 \t--- Loss: 0.981\n",
      "Iteration: 17 \t--- Loss: 0.978\n",
      "Iteration: 18 \t--- Loss: 0.938\n",
      "Iteration: 19 \t--- Loss: 0.914\n",
      "Iteration: 20 \t--- Loss: 0.918\n",
      "Iteration: 21 \t--- Loss: 0.886\n",
      "Iteration: 22 \t--- Loss: 0.915\n",
      "Iteration: 23 \t--- Loss: 0.891\n",
      "Iteration: 24 \t--- Loss: 0.910\n",
      "Iteration: 25 \t--- Loss: 0.850\n",
      "Iteration: 26 \t--- Loss: 0.865\n",
      "Iteration: 27 \t--- Loss: 0.812\n",
      "Iteration: 28 \t--- Loss: 0.863\n",
      "Iteration: 29 \t--- Loss: 0.807\n",
      "Iteration: 30 \t--- Loss: 0.830\n",
      "Iteration: 31 \t--- Loss: 0.822\n",
      "Iteration: 32 \t--- Loss: 0.805\n",
      "Iteration: 33 \t--- Loss: 0.780\n",
      "Iteration: 34 \t--- Loss: 0.743\n",
      "Iteration: 35 \t--- Loss: 0.766\n",
      "Iteration: 36 \t--- Loss: 0.722\n",
      "Iteration: 37 \t--- Loss: 0.705\n",
      "Iteration: 38 \t--- Loss: 0.665\n",
      "Iteration: 39 \t--- Loss: 0.668\n",
      "Iteration: 40 \t--- Loss: 0.699\n",
      "Iteration: 41 \t--- Loss: 0.856\n",
      "Iteration: 42 \t--- Loss: 0.763\n",
      "Iteration: 43 \t--- Loss: 0.641\n",
      "Iteration: 44 \t--- Loss: 0.672\n",
      "Iteration: 45 \t--- Loss: 1.434\n",
      "Iteration: 46 \t--- Loss: 1.904\n",
      "\n",
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it][Parallel(n_jobs=5)]: Done  75 tasks      | elapsed: 47.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.599\n",
      "Iteration: 1 \t--- Loss: 0.465\n",
      "Iteration: 2 \t--- Loss: 0.482\n",
      "Iteration: 3 \t--- Loss: 0.353\n",
      "Iteration: 4 \t--- Loss: 0.441\n",
      "Iteration: 5 \t--- Loss: 0.372\n",
      "Iteration: 6 \t--- Loss: 0.410\n",
      "Iteration: 7 \t--- Loss: 0.342\n",
      "Iteration: 8 \t--- Loss: 0.385\n",
      "Iteration: 9 \t--- Loss: 0.352\n",
      "Iteration: 10 \t--- Loss: 0.334\n",
      "Iteration: 11 \t--- Loss: 0.363\n",
      "Iteration: 12 \t--- Loss: 0.302\n",
      "Iteration: 13 \t--- Loss: 0.307\n",
      "Iteration: 14 \t--- Loss: 0.306\n",
      "Iteration: 15 \t--- Loss: 0.305\n",
      "Iteration: 16 \t--- Loss: 0.299\n",
      "Iteration: 17 \t--- Loss: 0.283\n",
      "Iteration: 18 \t--- Loss: 0.292\n",
      "Iteration: 19 \t--- Loss: 0.265\n",
      "Iteration: 20 \t--- Loss: 0.283\n",
      "Iteration: 21 \t--- Loss: 0.254\n",
      "Iteration: 22 \t--- Loss: 0.294\n",
      "Iteration: 23 \t--- Loss: 0.288\n",
      "Iteration: 24 \t--- Loss: 0.256\n",
      "Iteration: 25 \t--- Loss: 0.277\n",
      "Iteration: 26 \t--- Loss: 0.317\n",
      "Iteration: 27 \t--- Loss: 0.282\n",
      "Iteration: 28 \t--- Loss: 0.320\n",
      "Iteration: 29 \t--- Loss: 0.285\n",
      "Iteration: 30 \t--- Loss: 0.302\n",
      "Iteration: 31 \t--- Loss: 0.290\n",
      "Iteration: 32 \t--- Loss: 0.281\n",
      "Iteration: 33 \t--- Loss: 0.297\n",
      "Iteration: 34 \t--- Loss: 0.244\n",
      "Iteration: 35 \t--- Loss: 0.288\n",
      "Iteration: 36 \t--- Loss: 0.257\n",
      "Iteration: 37 \t--- Loss: 0.235\n",
      "Iteration: 38 \t--- Loss: 0.265\n",
      "Iteration: 39 \t--- Loss: 0.288\n",
      "Iteration: 40 \t--- Loss: 0.235\n",
      "Iteration: 41 \t--- Loss: 0.270\n",
      "Iteration: 42 \t--- Loss: 0.251\n",
      "Iteration: 43 \t--- Loss: 0.239\n",
      "Iteration: 44 \t--- Loss: 0.259\n",
      "Iteration: 45 \t--- Loss: 0.273\n",
      "Iteration: 46 \t--- Loss: 0.263\n",
      "Iteration: 47 \t--- Loss: 0.293\n",
      "Iteration: 48 \t--- Loss: 0.259\n",
      "Iteration: 49 \t--- Loss: 0.285\n",
      "Iteration: 50 \t--- Loss: 0.296\n",
      "Iteration: 51 \t--- Loss: 0.263\n",
      "Iteration: 52 \t--- Loss: 0.288\n",
      "Iteration: 53 \t--- Loss: 0.247\n",
      "Iteration: 54 \t--- Loss: 0.256\n",
      "Iteration: 55 \t--- Loss: 0.263\n",
      "Iteration: 56 \t--- Loss: 0.270\n",
      "Iteration: 57 \t--- Loss: 0.265\n",
      "Iteration: 58 \t--- Loss: 0.268\n",
      "Iteration: 59 \t--- Loss: 0.283\n",
      "Iteration: 60 \t--- Loss: 0.284\n",
      "Iteration: 61 \t--- Loss: 0.268\n",
      "Iteration: 62 \t--- Loss: 0.280\n",
      "Iteration: 63 \t--- Loss: 0.264\n",
      "Iteration: 64 \t--- Loss: 0.258\n",
      "Iteration: 65 \t--- Loss: 0.243\n",
      "Iteration: 66 \t--- Loss: 0.277\n",
      "Iteration: 67 \t--- Loss: 0.256\n",
      "Iteration: 68 \t--- Loss: 0.250\n",
      "Iteration: 69 \t--- Loss: 0.289\n",
      "Iteration: 70 \t--- Loss: 0.281\n",
      "Iteration: 71 \t--- Loss: 0.263\n",
      "Iteration: 72 \t--- Loss: 0.256\n",
      "Iteration: 73 \t--- Loss: 0.259\n",
      "Iteration: 74 \t--- Loss: 0.269\n",
      "Iteration: 75 \t--- Loss: 0.288\n",
      "Iteration: 76 \t--- Loss: 0.269\n",
      "Iteration: 77 \t--- Loss: 0.266\n",
      "Iteration: 78 \t--- Loss: 0.274\n",
      "Iteration: 79 \t--- Loss: 0.301\n",
      "Iteration: 80 \t--- Loss: 0.283\n",
      "Iteration: 81 \t--- Loss: 0.234\n",
      "Iteration: 82 \t--- Loss: 0.273\n",
      "Iteration: 83 \t--- Loss: 0.239\n",
      "Iteration: 84 \t--- Loss: 0.264\n",
      "Iteration: 85 \t--- Loss: 0.267\n",
      "Iteration: 86 \t--- Loss: 0.304\n",
      "Iteration: 87 \t--- Loss: 0.255\n",
      "Iteration: 88 \t--- Loss: 0.249\n",
      "Iteration: 89 \t--- Loss: 0.296\n",
      "Iteration: 90 \t--- Loss: 0.253\n",
      "Iteration: 91 \t--- Loss: 0.252\n",
      "Iteration: 92 \t--- Loss: 0.268\n",
      "Iteration: 93 \t--- Loss: 0.253\n",
      "Iteration: 94 \t--- Loss: 0.263\n",
      "Iteration: 95 \t--- Loss: 0.243\n",
      "Iteration: 96 \t--- Loss: 0.250\n",
      "Iteration: 97 \t--- Loss: 0.279\n",
      "Iteration: 98 \t--- Loss: 0.259\n",
      "Iteration: 99 \t--- Loss: 0.239\n",
      "Iteration: 100 \t--- Loss: 0.244\n",
      "Iteration: 101 \t--- Loss: 0.273\n",
      "Iteration: 102 \t--- Loss: 0.264\n",
      "Iteration: 103 \t--- Loss: 0.254\n",
      "Iteration: 104 \t--- Loss: 0.259\n",
      "Iteration: 105 \t--- Loss: 0.294\n",
      "Iteration: 106 \t--- Loss: 0.241\n",
      "Iteration: 107 \t--- Loss: 0.261\n",
      "Iteration: 108 \t--- Loss: 0.251\n",
      "Iteration: 109 \t--- Loss: 0.226\n",
      "Iteration: 110 \t--- Loss: 0.259\n",
      "Iteration: 111 \t--- Loss: 0.254\n",
      "Iteration: 112 \t--- Loss: 0.286\n",
      "Iteration: 113 \t--- Loss: 0.260\n",
      "Iteration: 114 \t--- Loss: 0.221\n",
      "Iteration: 115 \t--- Loss: 0.234\n",
      "Iteration: 116 \t--- Loss: 0.255\n",
      "Iteration: 117 \t--- Loss: 0.282\n",
      "Iteration: 118 \t--- Loss: 0.261\n",
      "Iteration: 119 \t--- Loss: 0.249\n",
      "Iteration: 120 \t--- Loss: 0.283\n",
      "Iteration: 121 \t--- Loss: 0.264\n",
      "Iteration: 122 \t--- Loss: 0.259\n",
      "Iteration: 123 \t--- Loss: 0.236\n",
      "Iteration: 124 \t--- Loss: 0.254\n",
      "Iteration: 125 \t--- Loss: 0.261\n",
      "Iteration: 126 \t--- Loss: 0.263\n",
      "Iteration: 127 \t--- Loss: 0.280\n",
      "Iteration: 128 \t--- Loss: 0.240\n",
      "Iteration: 129 \t--- Loss: 0.246\n",
      "Iteration: 130 \t--- Loss: 0.287\n",
      "Iteration: 131 \t--- Loss: 0.273\n",
      "Iteration: 132 \t--- Loss: 0.257\n",
      "Iteration: 133 \t--- Loss: 0.269\n",
      "Iteration: 134 \t--- Loss: 0.249\n",
      "Iteration: 135 \t--- Loss: 0.248\n",
      "Iteration: 136 \t--- Loss: 0.254\n",
      "Iteration: 137 \t--- Loss: 0.254\n",
      "Iteration: 138 \t--- Loss: 0.262\n",
      "Iteration: 139 \t--- Loss: 0.262\n",
      "Iteration: 140 \t--- Loss: 0.263\n",
      "Iteration: 141 \t--- Loss: 0.277\n",
      "Iteration: 142 \t--- Loss: 0.281\n",
      "Iteration: 143 \t--- Loss: 0.247\n",
      "Iteration: 144 \t--- Loss: 0.227\n",
      "Iteration: 145 \t--- Loss: 0.239\n",
      "Iteration: 146 \t--- Loss: 0.244\n",
      "Iteration: 147 \t--- Loss: 0.281\n",
      "Iteration: 148 \t--- Loss: 0.267\n",
      "Iteration: 149 \t--- Loss: 0.216\n",
      "Iteration: 150 \t--- Loss: 0.267\n",
      "Iteration: 151 \t--- Loss: 0.270\n",
      "Iteration: 152 \t--- Loss: 0.255\n",
      "Iteration: 153 \t--- Loss: 0.265\n",
      "Iteration: 154 \t--- Loss: 0.259\n",
      "Iteration: 155 \t--- Loss: 0.268\n",
      "Iteration: 156 \t--- Loss: 0.231\n",
      "Iteration: 157 \t--- Loss: 0.249\n",
      "Iteration: 158 \t--- Loss: 0.251\n",
      "Iteration: 159 \t--- Loss: 0.245\n",
      "Iteration: 160 \t--- Loss: 0.251\n",
      "Iteration: 161 \t--- Loss: 0.253\n",
      "Iteration: 162 \t--- Loss: 0.272\n",
      "Iteration: 163 \t--- Loss: 0.269\n",
      "Iteration: 164 \t--- Loss: 0.268\n",
      "Iteration: 165 \t--- Loss: 0.248\n",
      "Iteration: 166 \t--- Loss: 0.246\n",
      "Iteration: 167 \t--- Loss: 0.299\n",
      "Iteration: 168 \t--- Loss: 0.246\n",
      "Iteration: 169 \t--- Loss: 0.301\n",
      "Iteration: 170 \t--- Loss: 0.257\n",
      "Iteration: 171 \t--- Loss: 0.248\n",
      "Iteration: 172 \t--- Loss: 0.244\n",
      "Iteration: 173 \t--- Loss: 0.264\n",
      "Iteration: 174 \t--- Loss: 0.261\n",
      "Iteration: 175 \t--- Loss: 0.253\n",
      "Iteration: 176 \t--- Loss: 0.281\n",
      "Iteration: 177 \t--- Loss: 0.277\n",
      "Iteration: 178 \t--- Loss: 0.284\n",
      "Iteration: 179 \t--- Loss: 0.269\n",
      "Iteration: 180 \t--- Loss: 0.225\n",
      "Iteration: 181 \t--- Loss: 0.252\n",
      "Iteration: 182 \t--- Loss: 0.254\n",
      "Iteration: 183 \t--- Loss: 0.234\n",
      "Iteration: 184 \t--- Loss: 0.244\n",
      "Iteration: 185 \t--- Loss: 0.250\n",
      "Iteration: 186 \t--- Loss: 0.258\n",
      "Iteration: 187 \t--- Loss: 0.276\n",
      "Iteration: 188 \t--- Loss: 0.240\n",
      "Iteration: 189 \t--- Loss: 0.274\n",
      "Iteration: 190 \t--- Loss: 0.291\n",
      "Iteration: 191 \t--- Loss: 0.251\n",
      "Iteration: 192 \t--- Loss: 0.255\n",
      "Iteration: 193 \t--- Loss: 0.264\n",
      "Iteration: 194 \t--- Loss: 0.292\n",
      "Iteration: 195 \t--- Loss: 0.302\n",
      "Iteration: 196 \t--- Loss: 0.224\n",
      "Iteration: 197 \t--- Loss: 0.246\n",
      "Iteration: 198 \t--- Loss: 0.281\n",
      "Iteration: 199 \t--- Loss: 0.255\n",
      "Iteration: 200 \t--- Loss: 0.305\n",
      "Iteration: 201 \t--- Loss: 0.264\n",
      "Iteration: 202 \t--- Loss: 0.288\n",
      "Iteration: 203 \t--- Loss: 0.270\n",
      "Iteration: 204 \t--- Loss: 0.268\n",
      "Iteration: 205 \t--- Loss: 0.250\n",
      "Iteration: 206 \t--- Loss: 0.248\n",
      "Iteration: 207 \t--- Loss: 0.298\n",
      "Iteration: 208 \t--- Loss: 0.260\n",
      "Iteration: 209 \t--- Loss: 0.264\n",
      "Iteration: 210 \t--- Loss: 0.252\n",
      "Iteration: 211 \t--- Loss: 0.271\n",
      "Iteration: 212 \t--- Loss: 0.267\n",
      "Iteration: 213 \t--- Loss: 0.261\n",
      "Iteration: 214 \t--- Loss: 0.272\n",
      "Iteration: 215 \t--- Loss: 0.216\n",
      "Iteration: 216 \t--- Loss: 0.258\n",
      "Iteration: 217 \t--- Loss: 0.295\n",
      "Iteration: 218 \t--- Loss: 0.260\n",
      "Iteration: 219 \t--- Loss: 0.259\n",
      "Iteration: 220 \t--- Loss: 0.265\n",
      "Iteration: 221 \t--- Loss: 0.248\n",
      "Iteration: 222 \t--- Loss: 0.248\n",
      "Iteration: 223 \t--- Loss: 0.270\n",
      "Iteration: 224 \t--- Loss: 0.249\n",
      "Iteration: 225 \t--- Loss: 0.260\n",
      "Iteration: 226 \t--- Loss: 0.284\n",
      "Iteration: 227 \t--- Loss: 0.313\n",
      "Iteration: 228 \t--- Loss: 0.274\n",
      "Iteration: 229 \t--- Loss: 0.259\n",
      "Iteration: 230 \t--- Loss: 0.269\n",
      "Iteration: 231 \t--- Loss: 0.312\n",
      "Iteration: 232 \t--- Loss: 0.285\n",
      "Iteration: 233 \t--- Loss: 0.271\n",
      "Iteration: 234 \t--- Loss: 0.283\n",
      "Iteration: 235 \t--- Loss: 0.243\n",
      "Iteration: 236 \t--- Loss: 0.263\n",
      "Iteration: 237 \t--- Loss: 0.281\n",
      "Iteration: 238 \t--- Loss: 0.267\n",
      "Iteration: 239 \t--- Loss: 0.285\n",
      "Iteration: 240 \t--- Loss: 0.244\n",
      "Iteration: 241 \t--- Loss: 0.251\n",
      "Iteration: 242 \t--- Loss: 0.272\n",
      "Iteration: 243 \t--- Loss: 0.228\n",
      "Iteration: 244 \t--- Loss: 0.253\n",
      "Iteration: 245 \t--- Loss: 0.255\n",
      "Iteration: 246 \t--- Loss: 0.258\n",
      "Iteration: 247 \t--- Loss: 0.263\n",
      "Iteration: 248 \t--- Loss: 0.261\n",
      "Iteration: 249 \t--- Loss: 0.283\n",
      "Iteration: 250 \t--- Loss: 0.223\n",
      "Iteration: 251 \t--- Loss: 0.285\n",
      "Iteration: 252 \t--- Loss: 0.254\n",
      "Iteration: 253 \t--- Loss: 0.305\n",
      "Iteration: 254 \t--- Loss: 0.281\n",
      "Iteration: 255 \t--- Loss: 0.251\n",
      "Iteration: 256 \t--- Loss: 0.280\n",
      "Iteration: 257 \t--- Loss: 0.287\n",
      "Iteration: 258 \t--- Loss: 0.263\n",
      "Iteration: 259 \t--- Loss: 0.278Iteration: 0 \t--- Loss: 1.278\n",
      "Iteration: 1 \t--- Loss: 1.180\n",
      "Iteration: 2 \t--- Loss: 1.060\n",
      "Iteration: 3 \t--- Loss: 1.001\n",
      "Iteration: 4 \t--- Loss: 0.941\n",
      "Iteration: 5 \t--- Loss: 0.903\n",
      "Iteration: 6 \t--- Loss: 0.872\n",
      "Iteration: 7 \t--- Loss: 0.845\n",
      "Iteration: 8 \t--- Loss: 0.824\n",
      "Iteration: 9 \t--- Loss: 0.814\n",
      "Iteration: 10 \t--- Loss: 0.797\n",
      "Iteration: 11 \t--- Loss: 0.783\n",
      "Iteration: 12 \t--- Loss: 0.778\n",
      "Iteration: 13 \t--- Loss: 0.773\n",
      "Iteration: 14 \t--- Loss: 0.770\n",
      "Iteration: 15 \t--- Loss: 0.760\n",
      "Iteration: 16 \t--- Loss: 0.757\n",
      "Iteration: 17 \t--- Loss: 0.762\n",
      "Iteration: 18 \t--- Loss: 0.754\n",
      "Iteration: 19 \t--- Loss: 0.758\n",
      "Iteration: 20 \t--- Loss: 0.749\n",
      "Iteration: 21 \t--- Loss: 0.749\n",
      "Iteration: 22 \t--- Loss: 0.755\n",
      "Iteration: 23 \t--- Loss: 0.746\n",
      "Iteration: 24 \t--- Loss: 0.747\n",
      "Iteration: 25 \t--- Loss: 0.744\n",
      "Iteration: 26 \t--- Loss: 0.754\n",
      "Iteration: 27 \t--- Loss: 0.740\n",
      "Iteration: 28 \t--- Loss: 0.749\n",
      "Iteration: 29 \t--- Loss: 0.740\n",
      "Iteration: 30 \t--- Loss: 0.737\n",
      "Iteration: 31 \t--- Loss: 0.743\n",
      "Iteration: 32 \t--- Loss: 0.748\n",
      "Iteration: 33 \t--- Loss: 0.744\n",
      "Iteration: 34 \t--- Loss: 0.744\n",
      "Iteration: 35 \t--- Loss: 0.741\n",
      "Iteration: 36 \t--- Loss: 0.742\n",
      "Iteration: 37 \t--- Loss: 0.742\n",
      "Iteration: 38 \t--- Loss: 0.743\n",
      "Iteration: 39 \t--- Loss: 0.748\n",
      "Iteration: 40 \t--- Loss: 0.740\n",
      "Iteration: 41 \t--- Loss: 0.740\n",
      "Iteration: 42 \t--- Loss: 0.741\n",
      "Iteration: 43 \t--- Loss: 0.749\n",
      "Iteration: 44 \t--- Loss: 0.735\n",
      "Iteration: 45 \t--- Loss: 0.748\n",
      "Iteration: 46 \t--- Loss: 0.739\n",
      "Iteration: 47 \t--- Loss: 0.744\n",
      "Iteration: 48 \t--- Loss: 0.731\n",
      "Iteration: 49 \t--- Loss: 0.734\n",
      "Iteration: 50 \t--- Loss: 0.739\n",
      "Iteration: 51 \t--- Loss: 0.742\n",
      "Iteration: 52 \t--- Loss: 0.747\n",
      "Iteration: 53 \t--- Loss: 0.743\n",
      "Iteration: 54 \t--- Loss: 0.744\n",
      "Iteration: 55 \t--- Loss: 0.736\n",
      "Iteration: 56 \t--- Loss: 0.744\n",
      "Iteration: 57 \t--- Loss: 0.737\n",
      "Iteration: 58 \t--- Loss: 0.749\n",
      "Iteration: 59 \t--- Loss: 0.738\n",
      "Iteration: 60 \t--- Loss: 0.732\n",
      "Iteration: 61 \t--- Loss: 0.743\n",
      "Iteration: 62 \t--- Loss: 0.749\n",
      "Iteration: 63 \t--- Loss: 0.743\n",
      "Iteration: 64 \t--- Loss: 0.743\n",
      "Iteration: 65 \t--- Loss: 0.736\n",
      "Iteration: 66 \t--- Loss: 0.742\n",
      "Iteration: 67 \t--- Loss: 0.740\n",
      "Iteration: 68 \t--- Loss: 0.742\n",
      "Iteration: 69 \t--- Loss: 0.742\n",
      "Iteration: 70 \t--- Loss: 0.736\n",
      "Iteration: 71 \t--- Loss: 0.736\n",
      "Iteration: 72 \t--- Loss: 0.741\n",
      "Iteration: 73 \t--- Loss: 0.738\n",
      "Iteration: 74 \t--- Loss: 0.743\n",
      "Iteration: 75 \t--- Loss: 0.750\n",
      "Iteration: 76 \t--- Loss: 0.738\n",
      "Iteration: 77 \t--- Loss: 0.742\n",
      "Iteration: 78 \t--- Loss: 0.742\n",
      "Iteration: 79 \t--- Loss: 0.741\n",
      "Iteration: 80 \t--- Loss: 0.743\n",
      "Iteration: 81 \t--- Loss: 0.740\n",
      "Iteration: 82 \t--- Loss: 0.741\n",
      "Iteration: 83 \t--- Loss: 0.741\n",
      "Iteration: 84 \t--- Loss: 0.743\n",
      "Iteration: 85 \t--- Loss: 0.742\n",
      "Iteration: 86 \t--- Loss: 0.744\n",
      "Iteration: 87 \t--- Loss: 0.743\n",
      "Iteration: 88 \t--- Loss: 0.736\n",
      "Iteration: 89 \t--- Loss: 0.734\n",
      "Iteration: 90 \t--- Loss: 0.746\n",
      "Iteration: 91 \t--- Loss: 0.744\n",
      "Iteration: 92 \t--- Loss: 0.751\n",
      "Iteration: 93 \t--- Loss: 0.744\n",
      "Iteration: 94 \t--- Loss: 0.742\n",
      "Iteration: 95 \t--- Loss: 0.745\n",
      "Iteration: 96 \t--- Loss: 0.744\n",
      "Iteration: 97 \t--- Loss: 0.748\n",
      "Iteration: 98 \t--- Loss: 0.732\n",
      "Iteration: 99 \t--- Loss: 0.742\n",
      "Iteration: 100 \t--- Loss: 0.750\n",
      "Iteration: 101 \t--- Loss: 0.742\n",
      "Iteration: 102 \t--- Loss: 0.743\n",
      "Iteration: 103 \t--- Loss: 0.738\n",
      "Iteration: 104 \t--- Loss: 0.744\n",
      "Iteration: 105 \t--- Loss: 0.736\n",
      "Iteration: 106 \t--- Loss: 0.745\n",
      "Iteration: 107 \t--- Loss: 0.748\n",
      "Iteration: 108 \t--- Loss: 0.741\n",
      "Iteration: 109 \t--- Loss: 0.747\n",
      "Iteration: 110 \t--- Loss: 0.745\n",
      "Iteration: 111 \t--- Loss: 0.734\n",
      "Iteration: 112 \t--- Loss: 0.741\n",
      "Iteration: 113 \t--- Loss: 0.747\n",
      "Iteration: 114 \t--- Loss: 0.742\n",
      "Iteration: 115 \t--- Loss: 0.732\n",
      "Iteration: 116 \t--- Loss: 0.745\n",
      "Iteration: 117 \t--- Loss: 0.740\n",
      "Iteration: 118 \t--- Loss: 0.745\n",
      "Iteration: 119 \t--- Loss: 0.735\n",
      "Iteration: 120 \t--- Loss: 0.738\n",
      "Iteration: 121 \t--- Loss: 0.755\n",
      "Iteration: 122 \t--- Loss: 0.748\n",
      "Iteration: 123 \t--- Loss: 0.737\n",
      "Iteration: 124 \t--- Loss: 0.737\n",
      "Iteration: 125 \t--- Loss: 0.739\n",
      "Iteration: 126 \t--- Loss: 0.746\n",
      "Iteration: 127 \t--- Loss: 0.738\n",
      "Iteration: 128 \t--- Loss: 0.736\n",
      "Iteration: 129 \t--- Loss: 0.741\n",
      "Iteration: 130 \t--- Loss: 0.744\n",
      "Iteration: 131 \t--- Loss: 0.742\n",
      "Iteration: 132 \t--- Loss: 0.738\n",
      "Iteration: 133 \t--- Loss: 0.739\n",
      "Iteration: 134 \t--- Loss: 0.736\n",
      "Iteration: 135 \t--- Loss: 0.739\n",
      "Iteration: 136 \t--- Loss: 0.742\n",
      "Iteration: 137 \t--- Loss: 0.743\n",
      "Iteration: 138 \t--- Loss: 0.747\n",
      "Iteration: 139 \t--- Loss: 0.735\n",
      "Iteration: 140 \t--- Loss: 0.745\n",
      "Iteration: 141 \t--- Loss: 0.730\n",
      "Iteration: 142 \t--- Loss: 0.746\n",
      "Iteration: 143 \t--- Loss: 0.747\n",
      "Iteration: 144 \t--- Loss: 0.741\n",
      "Iteration: 145 \t--- Loss: 0.743\n",
      "Iteration: 146 \t--- Loss: 0.747\n",
      "Iteration: 147 \t--- Loss: 0.733\n",
      "Iteration: 148 \t--- Loss: 0.743\n",
      "Iteration: 149 \t--- Loss: 0.734\n",
      "Iteration: 150 \t--- Loss: 0.739\n",
      "Iteration: 151 \t--- Loss: 0.734\n",
      "Iteration: 152 \t--- Loss: 0.742\n",
      "Iteration: 153 \t--- Loss: 0.738\n",
      "Iteration: 154 \t--- Loss: 0.735\n",
      "Iteration: 155 \t--- Loss: 0.744\n",
      "Iteration: 156 \t--- Loss: 0.743\n",
      "Iteration: 157 \t--- Loss: 0.738\n",
      "Iteration: 158 \t--- Loss: 0.734\n",
      "Iteration: 159 \t--- Loss: 0.748\n",
      "Iteration: 160 \t--- Loss: 0.743\n",
      "Iteration: 161 \t--- Loss: 0.743\n",
      "Iteration: 162 \t--- Loss: 0.741\n",
      "Iteration: 163 \t--- Loss: 0.748\n",
      "Iteration: 164 \t--- Loss: 0.747\n",
      "Iteration: 165 \t--- Loss: 0.743\n",
      "Iteration: 166 \t--- Loss: 0.748\n",
      "Iteration: 167 \t--- Loss: 0.738\n",
      "Iteration: 168 \t--- Loss: 0.734\n",
      "Iteration: 169 \t--- Loss: 0.733\n",
      "Iteration: 170 \t--- Loss: 0.741\n",
      "Iteration: 171 \t--- Loss: 0.733\n",
      "Iteration: 172 \t--- Loss: 0.741\n",
      "Iteration: 173 \t--- Loss: 0.742\n",
      "Iteration: 174 \t--- Loss: 0.748\n",
      "Iteration: 175 \t--- Loss: 0.744\n",
      "Iteration: 176 \t--- Loss: 0.739\n",
      "Iteration: 177 \t--- Loss: 0.736\n",
      "Iteration: 178 \t--- Loss: 0.741\n",
      "Iteration: 179 \t--- Loss: 0.738\n",
      "Iteration: 180 \t--- Loss: 0.743\n",
      "Iteration: 181 \t--- Loss: 0.745\n",
      "Iteration: 182 \t--- Loss: 0.744\n",
      "Iteration: 183 \t--- Loss: 0.737\n",
      "Iteration: 184 \t--- Loss: 0.745\n",
      "Iteration: 185 \t--- Loss: 0.735\n",
      "Iteration: 186 \t--- Loss: 0.735\n",
      "Iteration: 187 \t--- Loss: 0.741\n",
      "Iteration: 188 \t--- Loss: 0.740\n",
      "Iteration: 189 \t--- Loss: 0.740\n",
      "Iteration: 190 \t--- Loss: 0.741\n",
      "Iteration: 191 \t--- Loss: 0.745\n",
      "Iteration: 192 \t--- Loss: 0.735\n",
      "Iteration: 193 \t--- Loss: 0.736\n",
      "Iteration: 194 \t--- Loss: 0.749\n",
      "Iteration: 195 \t--- Loss: 0.753\n",
      "Iteration: 196 \t--- Loss: 0.745\n",
      "Iteration: 197 \t--- Loss: 0.738\n",
      "Iteration: 198 \t--- Loss: 0.748\n",
      "Iteration: 199 \t--- Loss: 0.741\n",
      "Iteration: 200 \t--- Loss: 0.740\n",
      "Iteration: 201 \t--- Loss: 0.742\n",
      "Iteration: 202 \t--- Loss: 0.734\n",
      "Iteration: 203 \t--- Loss: 0.745\n",
      "Iteration: 204 \t--- Loss: 0.742\n",
      "Iteration: 205 \t--- Loss: 0.739\n",
      "Iteration: 206 \t--- Loss: 0.742\n",
      "Iteration: 207 \t--- Loss: 0.741\n",
      "Iteration: 208 \t--- Loss: 0.743\n",
      "Iteration: 209 \t--- Loss: 0.751\n",
      "Iteration: 210 \t--- Loss: 0.737\n",
      "Iteration: 211 \t--- Loss: 0.736\n",
      "Iteration: 212 \t--- Loss: 0.737\n",
      "Iteration: 213 \t--- Loss: 0.743\n",
      "Iteration: 214 \t--- Loss: 0.735\n",
      "Iteration: 215 \t--- Loss: 0.743\n",
      "Iteration: 216 \t--- Loss: 0.745\n",
      "Iteration: 217 \t--- Loss: 0.741\n",
      "Iteration: 218 \t--- Loss: 0.741\n",
      "Iteration: 219 \t--- Loss: 0.751\n",
      "Iteration: 220 \t--- Loss: 0.751\n",
      "Iteration: 221 \t--- Loss: 0.738\n",
      "Iteration: 222 \t--- Loss: 0.745\n",
      "Iteration: 223 \t--- Loss: 0.740\n",
      "Iteration: 224 \t--- Loss: 0.742\n",
      "Iteration: 225 \t--- Loss: 0.740\n",
      "Iteration: 226 \t--- Loss: 0.736\n",
      "Iteration: 227 \t--- Loss: 0.742\n",
      "Iteration: 228 \t--- Loss: 0.741\n",
      "Iteration: 229 \t--- Loss: 0.740\n",
      "Iteration: 230 \t--- Loss: 0.738\n",
      "Iteration: 231 \t--- Loss: 0.743\n",
      "Iteration: 232 \t--- Loss: 0.742\n",
      "Iteration: 233 \t--- Loss: 0.753\n",
      "Iteration: 234 \t--- Loss: 0.741\n",
      "Iteration: 235 \t--- Loss: 0.742\n",
      "Iteration: 236 \t--- Loss: 0.738\n",
      "Iteration: 237 \t--- Loss: 0.742\n",
      "Iteration: 238 \t--- Loss: 0.730\n",
      "Iteration: 239 \t--- Loss: 0.738\n",
      "Iteration: 240 \t--- Loss: 0.744\n",
      "Iteration: 241 \t--- Loss: 0.744\n",
      "Iteration: 242 \t--- Loss: 0.748\n",
      "Iteration: 243 \t--- Loss: 0.731\n",
      "Iteration: 244 \t--- Loss: 0.743\n",
      "Iteration: 245 \t--- Loss: 0.744\n",
      "Iteration: 246 \t--- Loss: 0.740\n",
      "Iteration: 247 \t--- Loss: 0.743\n",
      "Iteration: 248 \t--- Loss: 0.731\n",
      "Iteration: 249 \t--- Loss: 0.744\n",
      "Iteration: 250 \t--- Loss: 0.745\n",
      "Iteration: 251 \t--- Loss: 0.738\n",
      "Iteration: 252 \t--- Loss: 0.747\n",
      "Iteration: 253 \t--- Loss: 0.739\n",
      "Iteration: 254 \t--- Loss: 0.744\n",
      "Iteration: 255 \t--- Loss: 0.743\n",
      "Iteration: 256 \t--- Loss: 0.746\n",
      "Iteration: 257 \t--- Loss: 0.740\n",
      "Iteration: 258 \t--- Loss: 0.741\n",
      "Iteration: 259 \t--- Loss: 0.736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:24<00:00, 84.29s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.242\n",
      "Iteration: 261 \t--- Loss: 0.254\n",
      "Iteration: 262 \t--- Loss: 0.254\n",
      "Iteration: 263 \t--- Loss: 0.252\n",
      "Iteration: 264 \t--- Loss: 0.248\n",
      "Iteration: 265 \t--- Loss: 0.250\n",
      "Iteration: 266 \t--- Loss: 0.247\n",
      "Iteration: 267 \t--- Loss: 0.280\n",
      "Iteration: 268 \t--- Loss: 0.264\n",
      "Iteration: 269 \t--- Loss: 0.264\n",
      "Iteration: 270 \t--- Loss: 0.274\n",
      "Iteration: 271 \t--- Loss: 0.242\n",
      "Iteration: 272 \t--- Loss: 0.263\n",
      "Iteration: 273 \t--- Loss: 0.289\n",
      "Iteration: 274 \t--- Loss: 0.246\n",
      "Iteration: 275 \t--- Loss: 0.261\n",
      "Iteration: 276 \t--- Loss: 0.275\n",
      "Iteration: 277 \t--- Loss: 0.254\n",
      "Iteration: 278 \t--- Loss: 0.255\n",
      "Iteration: 279 \t--- Loss: 0.276\n",
      "Iteration: 280 \t--- Loss: 0.275\n",
      "Iteration: 281 \t--- Loss: 0.263\n",
      "Iteration: 282 \t--- Loss: 0.250\n",
      "Iteration: 283 \t--- Loss: 0.257\n",
      "Iteration: 284 \t--- Loss: 0.235\n",
      "Iteration: 285 \t--- Loss: 0.281\n",
      "Iteration: 286 \t--- Loss: 0.235\n",
      "Iteration: 287 \t--- Loss: 0.268\n",
      "Iteration: 288 \t--- Loss: 0.303\n",
      "Iteration: 289 \t--- Loss: 0.271\n",
      "Iteration: 290 \t--- Loss: 0.276\n",
      "Iteration: 291 \t--- Loss: 0.244\n",
      "Iteration: 292 \t--- Loss: 0.248\n",
      "Iteration: 293 \t--- Loss: 0.267\n",
      "Iteration: 294 \t--- Loss: 0.258\n",
      "Iteration: 295 \t--- Loss: 0.287\n",
      "Iteration: 296 \t--- Loss: 0.267\n",
      "Iteration: 297 \t--- Loss: 0.284\n",
      "Iteration: 298 \t--- Loss: 0.251\n",
      "Iteration: 299 \t--- Loss: 0.281\n",
      "Iteration: 300 \t--- Loss: 0.277\n",
      "Iteration: 301 \t--- Loss: 0.246\n",
      "Iteration: 302 \t--- Loss: 0.233\n",
      "Iteration: 303 \t--- Loss: 0.270\n",
      "Iteration: 304 \t--- Loss: 0.241\n",
      "Iteration: 305 \t--- Loss: 0.271\n",
      "Iteration: 306 \t--- Loss: 0.266\n",
      "Iteration: 307 \t--- Loss: 0.276\n",
      "Iteration: 308 \t--- Loss: 0.285\n",
      "Iteration: 309 \t--- Loss: 0.270\n",
      "Iteration: 310 \t--- Loss: 0.265\n",
      "Iteration: 311 \t--- Loss: 0.278\n",
      "Iteration: 312 \t--- Loss: 0.280\n",
      "Iteration: 313 \t--- Loss: 0.241\n",
      "Iteration: 314 \t--- Loss: 0.221\n",
      "Iteration: 315 \t--- Loss: 0.248\n",
      "Iteration: 316 \t--- Loss: 0.256\n",
      "Iteration: 317 \t--- Loss: 0.275\n",
      "Iteration: 318 \t--- Loss: 0.266\n",
      "Iteration: 319 \t--- Loss: 0.250\n",
      "Iteration: 320 \t--- Loss: 0.235\n",
      "Iteration: 321 \t--- Loss: 0.252\n",
      "Iteration: 322 \t--- Loss: 0.251\n",
      "Iteration: 323 \t--- Loss: 0.270\n",
      "Iteration: 324 \t--- Loss: 0.241\n",
      "Iteration: 325 \t--- Loss: 0.228\n",
      "Iteration: 326 \t--- Loss: 0.276\n",
      "Iteration: 327 \t--- Loss: 0.252\n",
      "Iteration: 328 \t--- Loss: 0.271\n",
      "Iteration: 329 \t--- Loss: 0.267\n",
      "Iteration: 330 \t--- Loss: 0.243\n",
      "Iteration: 331 \t--- Loss: 0.254\n",
      "Iteration: 332 \t--- Loss: 0.264\n",
      "Iteration: 333 \t--- Loss: 0.278\n",
      "Iteration: 334 \t--- Loss: 0.263\n",
      "Iteration: 335 \t--- Loss: 0.285\n",
      "Iteration: 336 \t--- Loss: 0.247\n",
      "Iteration: 337 \t--- Loss: 0.264\n",
      "Iteration: 338 \t--- Loss: 0.268\n",
      "Iteration: 339 \t--- Loss: 0.257\n",
      "Iteration: 340 \t--- Loss: 0.252\n",
      "Iteration: 341 \t--- Loss: 0.236\n",
      "Iteration: 342 \t--- Loss: 0.254\n",
      "Iteration: 343 \t--- Loss: 0.265\n",
      "Iteration: 344 \t--- Loss: 0.260\n",
      "Iteration: 345 \t--- Loss: 0.289\n",
      "Iteration: 346 \t--- Loss: 0.232\n",
      "Iteration: 347 \t--- Loss: 0.306\n",
      "Iteration: 348 \t--- Loss: 0.289\n",
      "Iteration: 349 \t--- Loss: 0.266\n",
      "Iteration: 350 \t--- Loss: 0.250\n",
      "Iteration: 351 \t--- Loss: 0.287\n",
      "Iteration: 352 \t--- Loss: 0.268\n",
      "Iteration: 353 \t--- Loss: 0.272\n",
      "Iteration: 354 \t--- Loss: 0.265\n",
      "Iteration: 355 \t--- Loss: 0.254\n",
      "Iteration: 356 \t--- Loss: 0.256\n",
      "Iteration: 357 \t--- Loss: 0.253\n",
      "Iteration: 358 \t--- Loss: 0.283\n",
      "Iteration: 359 \t--- Loss: 0.265\n",
      "Iteration: 360 \t--- Loss: 0.295\n",
      "Iteration: 361 \t--- Loss: 0.250\n",
      "Iteration: 362 \t--- Loss: 0.250\n",
      "Iteration: 363 \t--- Loss: 0.257\n",
      "Iteration: 364 \t--- Loss: 0.235\n",
      "Iteration: 365 \t--- Loss: 0.280\n",
      "Iteration: 366 \t--- Loss: 0.250\n",
      "Iteration: 367 \t--- Loss: 0.245\n",
      "Iteration: 368 \t--- Loss: 0.290\n",
      "Iteration: 369 \t--- Loss: 0.264\n",
      "Iteration: 370 \t--- Loss: 0.269\n",
      "Iteration: 371 \t--- Loss: 0.245\n",
      "Iteration: 372 \t--- Loss: 0.261\n",
      "Iteration: 373 \t--- Loss: 0.250\n",
      "Iteration: 374 \t--- Loss: 0.245\n",
      "Iteration: 375 \t--- Loss: 0.283\n",
      "Iteration: 376 \t--- Loss: 0.263\n",
      "Iteration: 377 \t--- Loss: 0.283\n",
      "Iteration: 378 \t--- Loss: 0.263\n",
      "Iteration: 379 \t--- Loss: 0.243\n",
      "Iteration: 380 \t--- Loss: 0.283\n",
      "Iteration: 381 \t--- Loss: 0.251\n",
      "Iteration: 382 \t--- Loss: 0.238\n",
      "Iteration: 383 \t--- Loss: 0.287\n",
      "Iteration: 384 \t--- Loss: 0.283\n",
      "Iteration: 385 \t--- Loss: 0.276\n",
      "Iteration: 386 \t--- Loss: 0.256\n",
      "Iteration: 387 \t--- Loss: 0.257\n",
      "Iteration: 388 \t--- Loss: 0.259\n",
      "Iteration: 389 \t--- Loss: 0.242\n",
      "Iteration: 390 \t--- Loss: 0.248\n",
      "Iteration: 391 \t--- Loss: 0.234\n",
      "Iteration: 392 \t--- Loss: 0.277\n",
      "Iteration: 393 \t--- Loss: 0.284\n",
      "Iteration: 394 \t--- Loss: 0.282\n",
      "Iteration: 395 \t--- Loss: 0.291\n",
      "Iteration: 396 \t--- Loss: 0.258\n",
      "Iteration: 397 \t--- Loss: 0.220\n",
      "Iteration: 398 \t--- Loss: 0.286\n",
      "Iteration: 399 \t--- Loss: 0.253\n",
      "Iteration: 400 \t--- Loss: 0.276\n",
      "Iteration: 401 \t--- Loss: 0.223\n",
      "Iteration: 402 \t--- Loss: 0.265\n",
      "Iteration: 403 \t--- Loss: 0.250\n",
      "Iteration: 404 \t--- Loss: 0.257\n",
      "Iteration: 405 \t--- Loss: 0.256\n",
      "Iteration: 406 \t--- Loss: 0.230\n",
      "Iteration: 407 \t--- Loss: 0.252\n",
      "Iteration: 408 \t--- Loss: 0.238\n",
      "Iteration: 409 \t--- Loss: 0.270\n",
      "Iteration: 410 \t--- Loss: 0.252\n",
      "Iteration: 411 \t--- Loss: 0.281\n",
      "Iteration: 412 \t--- Loss: 0.267\n",
      "Iteration: 413 \t--- Loss: 0.247\n",
      "Iteration: 414 \t--- Loss: 0.272\n",
      "Iteration: 415 \t--- Loss: 0.256\n",
      "Iteration: 416 \t--- Loss: 0.249\n",
      "Iteration: 417 \t--- Loss: 0.261\n",
      "Iteration: 418 \t--- Loss: 0.252\n",
      "Iteration: 419 \t--- Loss: 0.240\n",
      "Iteration: 420 \t--- Loss: 0.254\n",
      "Iteration: 421 \t--- Loss: 0.233\n",
      "Iteration: 422 \t--- Loss: 0.236\n",
      "Iteration: 423 \t--- Loss: 0.225\n",
      "Iteration: 424 \t--- Loss: 0.288\n",
      "Iteration: 425 \t--- Loss: 0.243\n",
      "Iteration: 426 \t--- Loss: 0.262\n",
      "Iteration: 427 \t--- Loss: 0.258\n",
      "Iteration: 428 \t--- Loss: 0.298\n",
      "Iteration: 429 \t--- Loss: 0.267\n",
      "Iteration: 430 \t--- Loss: 0.268\n",
      "Iteration: 431 \t--- Loss: 0.258\n",
      "Iteration: 432 \t--- Loss: 0.282\n",
      "Iteration: 433 \t--- Loss: 0.256\n",
      "Iteration: 434 \t--- Loss: 0.284\n",
      "Iteration: 435 \t--- Loss: 0.263\n",
      "Iteration: 436 \t--- Loss: 0.267\n",
      "Iteration: 437 \t--- Loss: 0.256\n",
      "Iteration: 438 \t--- Loss: 0.260\n",
      "Iteration: 439 \t--- Loss: 0.287\n",
      "Iteration: 440 \t--- Loss: 0.276\n",
      "Iteration: 441 \t--- Loss: 0.249\n",
      "Iteration: 442 \t--- Loss: 0.266\n",
      "Iteration: 443 \t--- Loss: 0.269\n",
      "Iteration: 444 \t--- Loss: 0.235\n",
      "Iteration: 445 \t--- Loss: 0.231\n",
      "Iteration: 446 \t--- Loss: 0.285\n",
      "Iteration: 447 \t--- Loss: 0.268\n",
      "Iteration: 448 \t--- Loss: 0.228\n",
      "Iteration: 449 \t--- Loss: 0.301\n",
      "Iteration: 450 \t--- Loss: 0.264\n",
      "Iteration: 451 \t--- Loss: 0.293\n",
      "Iteration: 452 \t--- Loss: 0.255\n",
      "Iteration: 453 \t--- Loss: 0.293\n",
      "Iteration: 454 \t--- Loss: 0.265\n",
      "Iteration: 455 \t--- Loss: 0.245\n",
      "Iteration: 456 \t--- Loss: 0.266\n",
      "Iteration: 457 \t--- Loss: 0.273\n",
      "Iteration: 458 \t--- Loss: 0.252\n",
      "Iteration: 459 \t--- Loss: 0.263\n",
      "Iteration: 460 \t--- Loss: 0.247\n",
      "Iteration: 461 \t--- Loss: 0.272\n",
      "Iteration: 462 \t--- Loss: 0.269\n",
      "Iteration: 463 \t--- Loss: 0.251\n",
      "Iteration: 464 \t--- Loss: 0.258\n",
      "Iteration: 465 \t--- Loss: 0.250\n",
      "Iteration: 466 \t--- Loss: 0.263\n",
      "Iteration: 467 \t--- Loss: 0.292\n",
      "Iteration: 468 \t--- Loss: 0.278\n",
      "Iteration: 469 \t--- Loss: 0.240\n",
      "Iteration: 470 \t--- Loss: 0.295\n",
      "Iteration: 471 \t--- Loss: 0.261\n",
      "Iteration: 472 \t--- Loss: 0.273\n",
      "Iteration: 473 \t--- Loss: 0.279\n",
      "Iteration: 474 \t--- Loss: 0.253\n",
      "Iteration: 475 \t--- Loss: 0.239\n",
      "Iteration: 476 \t--- Loss: 0.262\n",
      "Iteration: 477 \t--- Loss: 0.267\n",
      "Iteration: 478 \t--- Loss: 0.286\n",
      "Iteration: 479 \t--- Loss: 0.262\n",
      "Iteration: 480 \t--- Loss: 0.270\n",
      "Iteration: 481 \t--- Loss: 0.277\n",
      "Iteration: 482 \t--- Loss: 0.260\n",
      "Iteration: 483 \t--- Loss: 0.245\n",
      "Iteration: 484 \t--- Loss: 0.257\n",
      "Iteration: 485 \t--- Loss: 0.267\n",
      "Iteration: 486 \t--- Loss: 0.262\n",
      "Iteration: 487 \t--- Loss: 0.256\n",
      "Iteration: 488 \t--- Loss: 0.263\n",
      "Iteration: 489 \t--- Loss: 0.252\n",
      "Iteration: 490 \t--- Loss: 0.256\n",
      "Iteration: 491 \t--- Loss: 0.253\n",
      "Iteration: 492 \t--- Loss: 0.272\n",
      "Iteration: 493 \t--- Loss: 0.286\n",
      "Iteration: 494 \t--- Loss: 0.275\n",
      "Iteration: 495 \t--- Loss: 0.226\n",
      "Iteration: 496 \t--- Loss: 0.247\n",
      "Iteration: 497 \t--- Loss: 0.269\n",
      "Iteration: 498 \t--- Loss: 0.245\n",
      "Iteration: 499 \t--- Loss: 0.290\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.79s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.741\n",
      "Iteration: 261 \t--- Loss: 0.741\n",
      "Iteration: 262 \t--- Loss: 0.743\n",
      "Iteration: 263 \t--- Loss: 0.751\n",
      "Iteration: 264 \t--- Loss: 0.746\n",
      "Iteration: 265 \t--- Loss: 0.752\n",
      "Iteration: 266 \t--- Loss: 0.737\n",
      "Iteration: 267 \t--- Loss: 0.743\n",
      "Iteration: 268 \t--- Loss: 0.737\n",
      "Iteration: 269 \t--- Loss: 0.739\n",
      "Iteration: 270 \t--- Loss: 0.743\n",
      "Iteration: 271 \t--- Loss: 0.748\n",
      "Iteration: 272 \t--- Loss: 0.741\n",
      "Iteration: 273 \t--- Loss: 0.741\n",
      "Iteration: 274 \t--- Loss: 0.744\n",
      "Iteration: 275 \t--- Loss: 0.743\n",
      "Iteration: 276 \t--- Loss: 0.739\n",
      "Iteration: 277 \t--- Loss: 0.741\n",
      "Iteration: 278 \t--- Loss: 0.743\n",
      "Iteration: 279 \t--- Loss: 0.738\n",
      "Iteration: 280 \t--- Loss: 0.745\n",
      "Iteration: 281 \t--- Loss: 0.735\n",
      "Iteration: 282 \t--- Loss: 0.749\n",
      "Iteration: 283 \t--- Loss: 0.743\n",
      "Iteration: 284 \t--- Loss: 0.735\n",
      "Iteration: 285 \t--- Loss: 0.747\n",
      "Iteration: 286 \t--- Loss: 0.736\n",
      "Iteration: 287 \t--- Loss: 0.740\n",
      "Iteration: 288 \t--- Loss: 0.739\n",
      "Iteration: 289 \t--- Loss: 0.738\n",
      "Iteration: 290 \t--- Loss: 0.739\n",
      "Iteration: 291 \t--- Loss: 0.734\n",
      "Iteration: 292 \t--- Loss: 0.744\n",
      "Iteration: 293 \t--- Loss: 0.738\n",
      "Iteration: 294 \t--- Loss: 0.743\n",
      "Iteration: 295 \t--- Loss: 0.752\n",
      "Iteration: 296 \t--- Loss: 0.741\n",
      "Iteration: 297 \t--- Loss: 0.746\n",
      "Iteration: 298 \t--- Loss: 0.739\n",
      "Iteration: 299 \t--- Loss: 0.741\n",
      "Iteration: 300 \t--- Loss: 0.741\n",
      "Iteration: 301 \t--- Loss: 0.739\n",
      "Iteration: 302 \t--- Loss: 0.742\n",
      "Iteration: 303 \t--- Loss: 0.737\n",
      "Iteration: 304 \t--- Loss: 0.743\n",
      "Iteration: 305 \t--- Loss: 0.745\n",
      "Iteration: 306 \t--- Loss: 0.742\n",
      "Iteration: 307 \t--- Loss: 0.740\n",
      "Iteration: 308 \t--- Loss: 0.736\n",
      "Iteration: 309 \t--- Loss: 0.733\n",
      "Iteration: 310 \t--- Loss: 0.732\n",
      "Iteration: 311 \t--- Loss: 0.737\n",
      "Iteration: 312 \t--- Loss: 0.739\n",
      "Iteration: 313 \t--- Loss: 0.741\n",
      "Iteration: 314 \t--- Loss: 0.744\n",
      "Iteration: 315 \t--- Loss: 0.741\n",
      "Iteration: 316 \t--- Loss: 0.735\n",
      "Iteration: 317 \t--- Loss: 0.737\n",
      "Iteration: 318 \t--- Loss: 0.744\n",
      "Iteration: 319 \t--- Loss: 0.745\n",
      "Iteration: 320 \t--- Loss: 0.735\n",
      "Iteration: 321 \t--- Loss: 0.742\n",
      "Iteration: 322 \t--- Loss: 0.743\n",
      "Iteration: 323 \t--- Loss: 0.738\n",
      "Iteration: 324 \t--- Loss: 0.747\n",
      "Iteration: 325 \t--- Loss: 0.744\n",
      "Iteration: 326 \t--- Loss: 0.741\n",
      "Iteration: 327 \t--- Loss: 0.738\n",
      "Iteration: 328 \t--- Loss: 0.733\n",
      "Iteration: 329 \t--- Loss: 0.737\n",
      "Iteration: 330 \t--- Loss: 0.738\n",
      "Iteration: 331 \t--- Loss: 0.744\n",
      "Iteration: 332 \t--- Loss: 0.744\n",
      "Iteration: 333 \t--- Loss: 0.741\n",
      "Iteration: 334 \t--- Loss: 0.732\n",
      "Iteration: 335 \t--- Loss: 0.738\n",
      "Iteration: 336 \t--- Loss: 0.746\n",
      "Iteration: 337 \t--- Loss: 0.748\n",
      "Iteration: 338 \t--- Loss: 0.741\n",
      "Iteration: 339 \t--- Loss: 0.732\n",
      "Iteration: 340 \t--- Loss: 0.746\n",
      "Iteration: 341 \t--- Loss: 0.742\n",
      "Iteration: 342 \t--- Loss: 0.737\n",
      "Iteration: 343 \t--- Loss: 0.745\n",
      "Iteration: 344 \t--- Loss: 0.745\n",
      "Iteration: 345 \t--- Loss: 0.736\n",
      "Iteration: 346 \t--- Loss: 0.746\n",
      "Iteration: 347 \t--- Loss: 0.742\n",
      "Iteration: 348 \t--- Loss: 0.748\n",
      "Iteration: 349 \t--- Loss: 0.740\n",
      "Iteration: 350 \t--- Loss: 0.740\n",
      "Iteration: 351 \t--- Loss: 0.747\n",
      "Iteration: 352 \t--- Loss: 0.741\n",
      "Iteration: 353 \t--- Loss: 0.740\n",
      "Iteration: 354 \t--- Loss: 0.737\n",
      "Iteration: 355 \t--- Loss: 0.740\n",
      "Iteration: 356 \t--- Loss: 0.745\n",
      "Iteration: 357 \t--- Loss: 0.740\n",
      "Iteration: 358 \t--- Loss: 0.744\n",
      "Iteration: 359 \t--- Loss: 0.735\n",
      "Iteration: 360 \t--- Loss: 0.750\n",
      "Iteration: 361 \t--- Loss: 0.743\n",
      "Iteration: 362 \t--- Loss: 0.740\n",
      "Iteration: 363 \t--- Loss: 0.746\n",
      "Iteration: 364 \t--- Loss: 0.736\n",
      "Iteration: 365 \t--- Loss: 0.738\n",
      "Iteration: 366 \t--- Loss: 0.734\n",
      "Iteration: 367 \t--- Loss: 0.743\n",
      "Iteration: 368 \t--- Loss: 0.743\n",
      "Iteration: 369 \t--- Loss: 0.744\n",
      "Iteration: 370 \t--- Loss: 0.743\n",
      "Iteration: 371 \t--- Loss: 0.743\n",
      "Iteration: 372 \t--- Loss: 0.744\n",
      "Iteration: 373 \t--- Loss: 0.741\n",
      "Iteration: 374 \t--- Loss: 0.745\n",
      "Iteration: 375 \t--- Loss: 0.733\n",
      "Iteration: 376 \t--- Loss: 0.729\n",
      "Iteration: 377 \t--- Loss: 0.745\n",
      "Iteration: 378 \t--- Loss: 0.742\n",
      "Iteration: 379 \t--- Loss: 0.742\n",
      "Iteration: 380 \t--- Loss: 0.742\n",
      "Iteration: 381 \t--- Loss: 0.748\n",
      "Iteration: 382 \t--- Loss: 0.743\n",
      "Iteration: 383 \t--- Loss: 0.746\n",
      "Iteration: 384 \t--- Loss: 0.736\n",
      "Iteration: 385 \t--- Loss: 0.738\n",
      "Iteration: 386 \t--- Loss: 0.736\n",
      "Iteration: 387 \t--- Loss: 0.741\n",
      "Iteration: 388 \t--- Loss: 0.737\n",
      "Iteration: 389 \t--- Loss: 0.739\n",
      "Iteration: 390 \t--- Loss: 0.742\n",
      "Iteration: 391 \t--- Loss: 0.743\n",
      "Iteration: 392 \t--- Loss: 0.736\n",
      "Iteration: 393 \t--- Loss: 0.742\n",
      "Iteration: 394 \t--- Loss: 0.737\n",
      "Iteration: 395 \t--- Loss: 0.737\n",
      "Iteration: 396 \t--- Loss: 0.742\n",
      "Iteration: 397 \t--- Loss: 0.733\n",
      "Iteration: 398 \t--- Loss: 0.751\n",
      "Iteration: 399 \t--- Loss: 0.743\n",
      "Iteration: 400 \t--- Loss: 0.743\n",
      "Iteration: 401 \t--- Loss: 0.754\n",
      "Iteration: 402 \t--- Loss: 0.737\n",
      "Iteration: 403 \t--- Loss: 0.746\n",
      "Iteration: 404 \t--- Loss: 0.737\n",
      "Iteration: 405 \t--- Loss: 0.736\n",
      "Iteration: 406 \t--- Loss: 0.746\n",
      "Iteration: 407 \t--- Loss: 0.749\n",
      "Iteration: 408 \t--- Loss: 0.742\n",
      "Iteration: 409 \t--- Loss: 0.736\n",
      "Iteration: 410 \t--- Loss: 0.739\n",
      "Iteration: 411 \t--- Loss: 0.746\n",
      "Iteration: 412 \t--- Loss: 0.748\n",
      "Iteration: 413 \t--- Loss: 0.742\n",
      "Iteration: 414 \t--- Loss: 0.737\n",
      "Iteration: 415 \t--- Loss: 0.745\n",
      "Iteration: 416 \t--- Loss: 0.739\n",
      "Iteration: 417 \t--- Loss: 0.749\n",
      "Iteration: 418 \t--- Loss: 0.742\n",
      "Iteration: 419 \t--- Loss: 0.737\n",
      "Iteration: 420 \t--- Loss: 0.743\n",
      "Iteration: 421 \t--- Loss: 0.740\n",
      "Iteration: 422 \t--- Loss: 0.743\n",
      "Iteration: 423 \t--- Loss: 0.741\n",
      "Iteration: 424 \t--- Loss: 0.740\n",
      "Iteration: 425 \t--- Loss: 0.744\n",
      "Iteration: 426 \t--- Loss: 0.744\n",
      "Iteration: 427 \t--- Loss: 0.737\n",
      "Iteration: 428 \t--- Loss: 0.741\n",
      "Iteration: 429 \t--- Loss: 0.735\n",
      "Iteration: 430 \t--- Loss: 0.739\n",
      "Iteration: 431 \t--- Loss: 0.738\n",
      "Iteration: 432 \t--- Loss: 0.739\n",
      "Iteration: 433 \t--- Loss: 0.744\n",
      "Iteration: 434 \t--- Loss: 0.742\n",
      "Iteration: 435 \t--- Loss: 0.739\n",
      "Iteration: 436 \t--- Loss: 0.740\n",
      "Iteration: 437 \t--- Loss: 0.742\n",
      "Iteration: 438 \t--- Loss: 0.736\n",
      "Iteration: 439 \t--- Loss: 0.751\n",
      "Iteration: 440 \t--- Loss: 0.741\n",
      "Iteration: 441 \t--- Loss: 0.740\n",
      "Iteration: 442 \t--- Loss: 0.747\n",
      "Iteration: 443 \t--- Loss: 0.738\n",
      "Iteration: 444 \t--- Loss: 0.743\n",
      "Iteration: 445 \t--- Loss: 0.735\n",
      "Iteration: 446 \t--- Loss: 0.743\n",
      "Iteration: 447 \t--- Loss: 0.742\n",
      "Iteration: 448 \t--- Loss: 0.741\n",
      "Iteration: 449 \t--- Loss: 0.740\n",
      "Iteration: 450 \t--- Loss: 0.739\n",
      "Iteration: 451 \t--- Loss: 0.734\n",
      "Iteration: 452 \t--- Loss: 0.745\n",
      "Iteration: 453 \t--- Loss: 0.743\n",
      "Iteration: 454 \t--- Loss: 0.739\n",
      "Iteration: 455 \t--- Loss: 0.741\n",
      "Iteration: 456 \t--- Loss: 0.739\n",
      "Iteration: 457 \t--- Loss: 0.742\n",
      "Iteration: 458 \t--- Loss: 0.744\n",
      "Iteration: 459 \t--- Loss: 0.744\n",
      "Iteration: 460 \t--- Loss: 0.740\n",
      "Iteration: 461 \t--- Loss: 0.745\n",
      "Iteration: 462 \t--- Loss: 0.733\n",
      "Iteration: 463 \t--- Loss: 0.739\n",
      "Iteration: 464 \t--- Loss: 0.739\n",
      "Iteration: 465 \t--- Loss: 0.742\n",
      "Iteration: 466 \t--- Loss: 0.742\n",
      "Iteration: 467 \t--- Loss: 0.737\n",
      "Iteration: 468 \t--- Loss: 0.746\n",
      "Iteration: 469 \t--- Loss: 0.742\n",
      "Iteration: 470 \t--- Loss: 0.745\n",
      "Iteration: 471 \t--- Loss: 0.738\n",
      "Iteration: 472 \t--- Loss: 0.735\n",
      "Iteration: 473 \t--- Loss: 0.739\n",
      "Iteration: 474 \t--- Loss: 0.739\n",
      "Iteration: 475 \t--- Loss: 0.740\n",
      "Iteration: 476 \t--- Loss: 0.733\n",
      "Iteration: 477 \t--- Loss: 0.739\n",
      "Iteration: 478 \t--- Loss: 0.744\n",
      "Iteration: 479 \t--- Loss: 0.742\n",
      "Iteration: 480 \t--- Loss: 0.739\n",
      "Iteration: 481 \t--- Loss: 0.742\n",
      "Iteration: 482 \t--- Loss: 0.743\n",
      "Iteration: 483 \t--- Loss: 0.737\n",
      "Iteration: 484 \t--- Loss: 0.737\n",
      "Iteration: 485 \t--- Loss: 0.747\n",
      "Iteration: 486 \t--- Loss: 0.737\n",
      "Iteration: 487 \t--- Loss: 0.741\n",
      "Iteration: 488 \t--- Loss: 0.744\n",
      "Iteration: 489 \t--- Loss: 0.739\n",
      "Iteration: 490 \t--- Loss: 0.742\n",
      "Iteration: 491 \t--- Loss: 0.744\n",
      "Iteration: 492 \t--- Loss: 0.738\n",
      "Iteration: 493 \t--- Loss: 0.737\n",
      "Iteration: 494 \t--- Loss: 0.739\n",
      "Iteration: 495 \t--- Loss: 0.744\n",
      "Iteration: 496 \t--- Loss: 0.738\n",
      "Iteration: 497 \t--- Loss: 0.737\n",
      "Iteration: 498 \t--- Loss: 0.743\n",
      "Iteration: 499 \t--- Loss: 0.749\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:09,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.753\n",
      "Iteration: 1 \t--- Loss: 0.690\n",
      "Iteration: 2 \t--- Loss: 0.598\n",
      "Iteration: 3 \t--- Loss: 0.590\n",
      "Iteration: 4 \t--- Loss: 0.537\n",
      "Iteration: 5 \t--- Loss: 0.504\n",
      "Iteration: 6 \t--- Loss: 0.481\n",
      "Iteration: 7 \t--- Loss: 0.449\n",
      "Iteration: 8 \t--- Loss: 0.433\n",
      "Iteration: 9 \t--- Loss: 0.426\n",
      "Iteration: 10 \t--- Loss: 0.414\n",
      "Iteration: 11 \t--- Loss: 0.406\n",
      "Iteration: 12 \t--- Loss: 0.393\n",
      "Iteration: 13 \t--- Loss: 0.394\n",
      "Iteration: 14 \t--- Loss: 0.388\n",
      "Iteration: 15 \t--- Loss: 0.381\n",
      "Iteration: 16 \t--- Loss: 0.375\n",
      "Iteration: 17 \t--- Loss: 0.373\n",
      "Iteration: 18 \t--- Loss: 0.374\n",
      "Iteration: 19 \t--- Loss: 0.371\n",
      "Iteration: 20 \t--- Loss: 0.372\n",
      "Iteration: 21 \t--- Loss: 0.362\n",
      "Iteration: 22 \t--- Loss: 0.362\n",
      "Iteration: 23 \t--- Loss: 0.362\n",
      "Iteration: 24 \t--- Loss: 0.359\n",
      "Iteration: 25 \t--- Loss: 0.360\n",
      "Iteration: 26 \t--- Loss: 0.356\n",
      "Iteration: 27 \t--- Loss: 0.356\n",
      "Iteration: 28 \t--- Loss: 0.359\n",
      "Iteration: 29 \t--- Loss: 0.354\n",
      "Iteration: 30 \t--- Loss: 0.354\n",
      "Iteration: 31 \t--- Loss: 0.356\n",
      "Iteration: 32 \t--- Loss: 0.357\n",
      "Iteration: 33 \t--- Loss: 0.353\n",
      "Iteration: 34 \t--- Loss: 0.355\n",
      "Iteration: 35 \t--- Loss: 0.351\n",
      "Iteration: 36 \t--- Loss: 0.352\n",
      "Iteration: 37 \t--- Loss: 0.353\n",
      "Iteration: 38 \t--- Loss: 0.354\n",
      "Iteration: 39 \t--- Loss: 0.352\n",
      "Iteration: 40 \t--- Loss: 0.350\n",
      "Iteration: 41 \t--- Loss: 0.351\n",
      "Iteration: 42 \t--- Loss: 0.349\n",
      "Iteration: 43 \t--- Loss: 0.351\n",
      "Iteration: 44 \t--- Loss: 0.350\n",
      "Iteration: 45 \t--- Loss: 0.352\n",
      "Iteration: 46 \t--- Loss: 0.353\n",
      "Iteration: 47 \t--- Loss: 0.349\n",
      "Iteration: 48 \t--- Loss: 0.352\n",
      "Iteration: 49 \t--- Loss: 0.350\n",
      "Iteration: 50 \t--- Loss: 0.350\n",
      "Iteration: 51 \t--- Loss: 0.351\n",
      "Iteration: 52 \t--- Loss: 0.350\n",
      "Iteration: 53 \t--- Loss: 0.353\n",
      "Iteration: 54 \t--- Loss: 0.349\n",
      "Iteration: 55 \t--- Loss: 0.349\n",
      "Iteration: 56 \t--- Loss: 0.350\n",
      "Iteration: 57 \t--- Loss: 0.350\n",
      "Iteration: 58 \t--- Loss: 0.350\n",
      "Iteration: 59 \t--- Loss: 0.351\n",
      "Iteration: 60 \t--- Loss: 0.348\n",
      "Iteration: 61 \t--- Loss: 0.349\n",
      "Iteration: 62 \t--- Loss: 0.349\n",
      "Iteration: 63 \t--- Loss: 0.351\n",
      "Iteration: 64 \t--- Loss: 0.349\n",
      "Iteration: 65 \t--- Loss: 0.350\n",
      "Iteration: 66 \t--- Loss: 0.350\n",
      "Iteration: 67 \t--- Loss: 0.351\n",
      "Iteration: 68 \t--- Loss: 0.350\n",
      "Iteration: 69 \t--- Loss: 0.348\n",
      "Iteration: 70 \t--- Loss: 0.348\n",
      "Iteration: 71 \t--- Loss: 0.351\n",
      "Iteration: 72 \t--- Loss: 0.348\n",
      "Iteration: 73 \t--- Loss: 0.348\n",
      "Iteration: 74 \t--- Loss: 0.349\n",
      "Iteration: 75 \t--- Loss: 0.350\n",
      "Iteration: 76 \t--- Loss: 0.349\n",
      "Iteration: 77 \t--- Loss: 0.350\n",
      "Iteration: 78 \t--- Loss: 0.348\n",
      "Iteration: 79 \t--- Loss: 0.350\n",
      "Iteration: 80 \t--- Loss: 0.349\n",
      "Iteration: 81 \t--- Loss: 0.348\n",
      "Iteration: 82 \t--- Loss: 0.350\n",
      "Iteration: 83 \t--- Loss: 0.349\n",
      "Iteration: 84 \t--- Loss: 0.348\n",
      "Iteration: 85 \t--- Loss: 0.349\n",
      "Iteration: 86 \t--- Loss: 0.351\n",
      "Iteration: 87 \t--- Loss: 0.348\n",
      "Iteration: 88 \t--- Loss: 0.347\n",
      "Iteration: 89 \t--- Loss: 0.349\n",
      "Iteration: 90 \t--- Loss: 0.347\n",
      "Iteration: 91 \t--- Loss: 0.351\n",
      "Iteration: 92 \t--- Loss: 0.348\n",
      "Iteration: 93 \t--- Loss: 0.349\n",
      "Iteration: 94 \t--- Loss: 0.349\n",
      "Iteration: 95 \t--- Loss: 0.349\n",
      "Iteration: 96 \t--- Loss: 0.351\n",
      "Iteration: 97 \t--- Loss: 0.348\n",
      "Iteration: 98 \t--- Loss: 0.349\n",
      "Iteration: 99 \t--- Loss: 0.349\n",
      "Iteration: 100 \t--- Loss: 0.349\n",
      "Iteration: 101 \t--- Loss: 0.349\n",
      "Iteration: 102 \t--- Loss: 0.351\n",
      "Iteration: 103 \t--- Loss: 0.350\n",
      "Iteration: 104 \t--- Loss: 0.351\n",
      "Iteration: 105 \t--- Loss: 0.350\n",
      "Iteration: 106 \t--- Loss: 0.350\n",
      "Iteration: 107 \t--- Loss: 0.351\n",
      "Iteration: 108 \t--- Loss: 0.349\n",
      "Iteration: 109 \t--- Loss: 0.351\n",
      "Iteration: 110 \t--- Loss: 0.351\n",
      "Iteration: 111 \t--- Loss: 0.347\n",
      "Iteration: 112 \t--- Loss: 0.348\n",
      "Iteration: 113 \t--- Loss: 0.349\n",
      "Iteration: 114 \t--- Loss: 0.350\n",
      "Iteration: 115 \t--- Loss: 0.348\n",
      "Iteration: 116 \t--- Loss: 0.349\n",
      "Iteration: 117 \t--- Loss: 0.348\n",
      "Iteration: 118 \t--- Loss: 0.349\n",
      "Iteration: 119 \t--- Loss: 0.349\n",
      "Iteration: 120 \t--- Loss: 0.350\n",
      "Iteration: 121 \t--- Loss: 0.349\n",
      "Iteration: 122 \t--- Loss: 0.349\n",
      "Iteration: 123 \t--- Loss: 0.349\n",
      "Iteration: 124 \t--- Loss: 0.348\n",
      "Iteration: 125 \t--- Loss: 0.353\n",
      "Iteration: 126 \t--- Loss: 0.351\n",
      "Iteration: 127 \t--- Loss: 0.349\n",
      "Iteration: 128 \t--- Loss: 0.350\n",
      "Iteration: 129 \t--- Loss: 0.350\n",
      "Iteration: 130 \t--- Loss: 0.347\n",
      "Iteration: 131 \t--- Loss: 0.347\n",
      "Iteration: 132 \t--- Loss: 0.351\n",
      "Iteration: 133 \t--- Loss: 0.348\n",
      "Iteration: 134 \t--- Loss: 0.350\n",
      "Iteration: 135 \t--- Loss: 0.349\n",
      "Iteration: 136 \t--- Loss: 0.348\n",
      "Iteration: 137 \t--- Loss: 0.348\n",
      "Iteration: 138 \t--- Loss: 0.348\n",
      "Iteration: 139 \t--- Loss: 0.348\n",
      "Iteration: 140 \t--- Loss: 0.348\n",
      "Iteration: 141 \t--- Loss: 0.349\n",
      "Iteration: 142 \t--- Loss: 0.348\n",
      "Iteration: 143 \t--- Loss: 0.348\n",
      "Iteration: 144 \t--- Loss: 0.348\n",
      "Iteration: 145 \t--- Loss: 0.348\n",
      "Iteration: 146 \t--- Loss: 0.350\n",
      "Iteration: 147 \t--- Loss: 0.352\n",
      "Iteration: 148 \t--- Loss: 0.350\n",
      "Iteration: 149 \t--- Loss: 0.351\n",
      "Iteration: 150 \t--- Loss: 0.351\n",
      "Iteration: 151 \t--- Loss: 0.349\n",
      "Iteration: 152 \t--- Loss: 0.349\n",
      "Iteration: 153 \t--- Loss: 0.350\n",
      "Iteration: 154 \t--- Loss: 0.349\n",
      "Iteration: 155 \t--- Loss: 0.348\n",
      "Iteration: 156 \t--- Loss: 0.347\n",
      "Iteration: 157 \t--- Loss: 0.348\n",
      "Iteration: 158 \t--- Loss: 0.350\n",
      "Iteration: 159 \t--- Loss: 0.346\n",
      "Iteration: 160 \t--- Loss: 0.350\n",
      "Iteration: 161 \t--- Loss: 0.351\n",
      "Iteration: 162 \t--- Loss: 0.349\n",
      "Iteration: 163 \t--- Loss: 0.347\n",
      "Iteration: 164 \t--- Loss: 0.349\n",
      "Iteration: 165 \t--- Loss: 0.349\n",
      "Iteration: 166 \t--- Loss: 0.349\n",
      "Iteration: 167 \t--- Loss: 0.349\n",
      "Iteration: 168 \t--- Loss: 0.350\n",
      "Iteration: 169 \t--- Loss: 0.347\n",
      "Iteration: 170 \t--- Loss: 0.353\n",
      "Iteration: 171 \t--- Loss: 0.349\n",
      "Iteration: 172 \t--- Loss: 0.349\n",
      "Iteration: 173 \t--- Loss: 0.349\n",
      "Iteration: 174 \t--- Loss: 0.350\n",
      "Iteration: 175 \t--- Loss: 0.349\n",
      "Iteration: 176 \t--- Loss: 0.347\n",
      "Iteration: 177 \t--- Loss: 0.349\n",
      "Iteration: 178 \t--- Loss: 0.348\n",
      "Iteration: 179 \t--- Loss: 0.351\n",
      "Iteration: 180 \t--- Loss: 0.351\n",
      "Iteration: 181 \t--- Loss: 0.350\n",
      "Iteration: 182 \t--- Loss: 0.349\n",
      "Iteration: 183 \t--- Loss: 0.346\n",
      "Iteration: 184 \t--- Loss: 0.349\n",
      "Iteration: 185 \t--- Loss: 0.350\n",
      "Iteration: 186 \t--- Loss: 0.349\n",
      "Iteration: 187 \t--- Loss: 0.350\n",
      "Iteration: 188 \t--- Loss: 0.350\n",
      "Iteration: 189 \t--- Loss: 0.348\n",
      "Iteration: 190 \t--- Loss: 0.348\n",
      "Iteration: 191 \t--- Loss: 0.350\n",
      "Iteration: 192 \t--- Loss: 0.348\n",
      "Iteration: 193 \t--- Loss: 0.350\n",
      "Iteration: 194 \t--- Loss: 0.351\n",
      "Iteration: 195 \t--- Loss: 0.350\n",
      "Iteration: 196 \t--- Loss: 0.347\n",
      "Iteration: 197 \t--- Loss: 0.349\n",
      "Iteration: 198 \t--- Loss: 0.351\n",
      "Iteration: 199 \t--- Loss: 0.351\n",
      "Iteration: 200 \t--- Loss: 0.350\n",
      "Iteration: 201 \t--- Loss: 0.349\n",
      "Iteration: 202 \t--- Loss: 0.351\n",
      "Iteration: 203 \t--- Loss: 0.348\n",
      "Iteration: 204 \t--- Loss: 0.348\n",
      "Iteration: 205 \t--- Loss: 0.348\n",
      "Iteration: 206 \t--- Loss: 0.348\n",
      "Iteration: 207 \t--- Loss: 0.351\n",
      "Iteration: 208 \t--- Loss: 0.350\n",
      "Iteration: 209 \t--- Loss: 0.347\n",
      "Iteration: 210 \t--- Loss: 0.348\n",
      "Iteration: 211 \t--- Loss: 0.350\n",
      "Iteration: 212 \t--- Loss: 0.348\n",
      "Iteration: 213 \t--- Loss: 0.348\n",
      "Iteration: 214 \t--- Loss: 0.349\n",
      "Iteration: 215 \t--- Loss: 0.349\n",
      "Iteration: 216 \t--- Loss: 0.350\n",
      "Iteration: 217 \t--- Loss: 0.352\n",
      "Iteration: 218 \t--- Loss: 0.349\n",
      "Iteration: 219 \t--- Loss: 0.346\n",
      "Iteration: 220 \t--- Loss: 0.348\n",
      "Iteration: 221 \t--- Loss: 0.347\n",
      "Iteration: 222 \t--- Loss: 0.348\n",
      "Iteration: 223 \t--- Loss: 0.350\n",
      "Iteration: 224 \t--- Loss: 0.347\n",
      "Iteration: 225 \t--- Loss: 0.349\n",
      "Iteration: 226 \t--- Loss: 0.348\n",
      "Iteration: 227 \t--- Loss: 0.349\n",
      "Iteration: 228 \t--- Loss: 0.348\n",
      "Iteration: 229 \t--- Loss: 0.348\n",
      "Iteration: 230 \t--- Loss: 0.348\n",
      "Iteration: 231 \t--- Loss: 0.351\n",
      "Iteration: 232 \t--- Loss: 0.348\n",
      "Iteration: 233 \t--- Loss: 0.351\n",
      "Iteration: 234 \t--- Loss: 0.347\n",
      "Iteration: 235 \t--- Loss: 0.349\n",
      "Iteration: 236 \t--- Loss: 0.349\n",
      "Iteration: 237 \t--- Loss: 0.348\n",
      "Iteration: 238 \t--- Loss: 0.349\n",
      "Iteration: 239 \t--- Loss: 0.351\n",
      "Iteration: 240 \t--- Loss: 0.349\n",
      "Iteration: 241 \t--- Loss: 0.350\n",
      "Iteration: 242 \t--- Loss: 0.348\n",
      "Iteration: 243 \t--- Loss: 0.350\n",
      "Iteration: 244 \t--- Loss: 0.350\n",
      "Iteration: 245 \t--- Loss: 0.350\n",
      "Iteration: 246 \t--- Loss: 0.351\n",
      "Iteration: 247 \t--- Loss: 0.351\n",
      "Iteration: 248 \t--- Loss: 0.351\n",
      "Iteration: 249 \t--- Loss: 0.351\n",
      "Iteration: 250 \t--- Loss: 0.351\n",
      "Iteration: 251 \t--- Loss: 0.349\n",
      "Iteration: 252 \t--- Loss: 0.346\n",
      "Iteration: 253 \t--- Loss: 0.350\n",
      "Iteration: 254 \t--- Loss: 0.348\n",
      "Iteration: 255 \t--- Loss: 0.349\n",
      "Iteration: 256 \t--- Loss: 0.350\n",
      "Iteration: 257 \t--- Loss: 0.347\n",
      "Iteration: 258 \t--- Loss: 0.350\n",
      "Iteration: 259 \t--- Loss: 0.351"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.45s/it][Parallel(n_jobs=5)]: Done  76 tasks      | elapsed: 48.4min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.53s/it][Parallel(n_jobs=5)]: Done  77 tasks      | elapsed: 48.4min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:25<00:00, 85.14s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.351\n",
      "Iteration: 261 \t--- Loss: 0.347\n",
      "Iteration: 262 \t--- Loss: 0.348\n",
      "Iteration: 263 \t--- Loss: 0.348\n",
      "Iteration: 264 \t--- Loss: 0.348\n",
      "Iteration: 265 \t--- Loss: 0.349\n",
      "Iteration: 266 \t--- Loss: 0.348\n",
      "Iteration: 267 \t--- Loss: 0.349\n",
      "Iteration: 268 \t--- Loss: 0.352\n",
      "Iteration: 269 \t--- Loss: 0.350\n",
      "Iteration: 270 \t--- Loss: 0.348\n",
      "Iteration: 271 \t--- Loss: 0.348\n",
      "Iteration: 272 \t--- Loss: 0.348\n",
      "Iteration: 273 \t--- Loss: 0.349\n",
      "Iteration: 274 \t--- Loss: 0.350\n",
      "Iteration: 275 \t--- Loss: 0.350\n",
      "Iteration: 276 \t--- Loss: 0.347\n",
      "Iteration: 277 \t--- Loss: 0.351\n",
      "Iteration: 278 \t--- Loss: 0.350\n",
      "Iteration: 279 \t--- Loss: 0.349\n",
      "Iteration: 280 \t--- Loss: 0.350\n",
      "Iteration: 281 \t--- Loss: 0.349\n",
      "Iteration: 282 \t--- Loss: 0.349\n",
      "Iteration: 283 \t--- Loss: 0.349\n",
      "Iteration: 284 \t--- Loss: 0.349\n",
      "Iteration: 285 \t--- Loss: 0.348\n",
      "Iteration: 286 \t--- Loss: 0.350\n",
      "Iteration: 287 \t--- Loss: 0.348\n",
      "Iteration: 288 \t--- Loss: 0.348\n",
      "Iteration: 289 \t--- Loss: 0.348\n",
      "Iteration: 290 \t--- Loss: 0.348\n",
      "Iteration: 291 \t--- Loss: 0.348\n",
      "Iteration: 292 \t--- Loss: 0.352\n",
      "Iteration: 293 \t--- Loss: 0.350\n",
      "Iteration: 294 \t--- Loss: 0.348\n",
      "Iteration: 295 \t--- Loss: 0.348\n",
      "Iteration: 296 \t--- Loss: 0.348\n",
      "Iteration: 297 \t--- Loss: 0.349\n",
      "Iteration: 298 \t--- Loss: 0.351\n",
      "Iteration: 299 \t--- Loss: 0.349\n",
      "Iteration: 300 \t--- Loss: 0.347\n",
      "Iteration: 301 \t--- Loss: 0.349\n",
      "Iteration: 302 \t--- Loss: 0.351\n",
      "Iteration: 303 \t--- Loss: 0.351\n",
      "Iteration: 304 \t--- Loss: 0.350\n",
      "Iteration: 305 \t--- Loss: 0.350\n",
      "Iteration: 306 \t--- Loss: 0.350\n",
      "Iteration: 307 \t--- Loss: 0.349\n",
      "Iteration: 308 \t--- Loss: 0.348\n",
      "Iteration: 309 \t--- Loss: 0.348\n",
      "Iteration: 310 \t--- Loss: 0.349\n",
      "Iteration: 311 \t--- Loss: 0.346\n",
      "Iteration: 312 \t--- Loss: 0.349\n",
      "Iteration: 313 \t--- Loss: 0.348\n",
      "Iteration: 314 \t--- Loss: 0.348\n",
      "Iteration: 315 \t--- Loss: 0.349\n",
      "Iteration: 316 \t--- Loss: 0.349\n",
      "Iteration: 317 \t--- Loss: 0.349\n",
      "Iteration: 318 \t--- Loss: 0.349\n",
      "Iteration: 319 \t--- Loss: 0.348\n",
      "Iteration: 320 \t--- Loss: 0.348\n",
      "Iteration: 321 \t--- Loss: 0.349\n",
      "Iteration: 322 \t--- Loss: 0.352\n",
      "Iteration: 323 \t--- Loss: 0.350\n",
      "Iteration: 324 \t--- Loss: 0.350\n",
      "Iteration: 325 \t--- Loss: 0.348\n",
      "Iteration: 326 \t--- Loss: 0.347\n",
      "Iteration: 327 \t--- Loss: 0.349\n",
      "Iteration: 328 \t--- Loss: 0.348\n",
      "Iteration: 329 \t--- Loss: 0.348\n",
      "Iteration: 330 \t--- Loss: 0.345\n",
      "Iteration: 331 \t--- Loss: 0.351\n",
      "Iteration: 332 \t--- Loss: 0.349\n",
      "Iteration: 333 \t--- Loss: 0.349\n",
      "Iteration: 334 \t--- Loss: 0.345\n",
      "Iteration: 335 \t--- Loss: 0.349\n",
      "Iteration: 336 \t--- Loss: 0.350\n",
      "Iteration: 337 \t--- Loss: 0.351\n",
      "Iteration: 338 \t--- Loss: 0.350\n",
      "Iteration: 339 \t--- Loss: 0.348\n",
      "Iteration: 340 \t--- Loss: 0.349\n",
      "Iteration: 341 \t--- Loss: 0.350\n",
      "Iteration: 342 \t--- Loss: 0.350\n",
      "Iteration: 343 \t--- Loss: 0.348\n",
      "Iteration: 344 \t--- Loss: 0.347\n",
      "Iteration: 345 \t--- Loss: 0.350\n",
      "Iteration: 346 \t--- Loss: 0.351\n",
      "Iteration: 347 \t--- Loss: 0.350\n",
      "Iteration: 348 \t--- Loss: 0.348\n",
      "Iteration: 349 \t--- Loss: 0.350\n",
      "Iteration: 350 \t--- Loss: 0.350\n",
      "Iteration: 351 \t--- Loss: 0.350\n",
      "Iteration: 352 \t--- Loss: 0.351\n",
      "Iteration: 353 \t--- Loss: 0.347\n",
      "Iteration: 354 \t--- Loss: 0.350\n",
      "Iteration: 355 \t--- Loss: 0.350\n",
      "Iteration: 356 \t--- Loss: 0.349\n",
      "Iteration: 357 \t--- Loss: 0.351\n",
      "Iteration: 358 \t--- Loss: 0.349\n",
      "Iteration: 359 \t--- Loss: 0.350\n",
      "Iteration: 360 \t--- Loss: 0.347\n",
      "Iteration: 361 \t--- Loss: 0.350\n",
      "Iteration: 362 \t--- Loss: 0.351\n",
      "Iteration: 363 \t--- Loss: 0.348\n",
      "Iteration: 364 \t--- Loss: 0.350\n",
      "Iteration: 365 \t--- Loss: 0.347\n",
      "Iteration: 366 \t--- Loss: 0.349\n",
      "Iteration: 367 \t--- Loss: 0.348\n",
      "Iteration: 368 \t--- Loss: 0.347\n",
      "Iteration: 369 \t--- Loss: 0.351\n",
      "Iteration: 370 \t--- Loss: 0.352\n",
      "Iteration: 371 \t--- Loss: 0.349\n",
      "Iteration: 372 \t--- Loss: 0.349\n",
      "Iteration: 373 \t--- Loss: 0.348\n",
      "Iteration: 374 \t--- Loss: 0.350\n",
      "Iteration: 375 \t--- Loss: 0.350\n",
      "Iteration: 376 \t--- Loss: 0.348\n",
      "Iteration: 377 \t--- Loss: 0.351\n",
      "Iteration: 378 \t--- Loss: 0.350\n",
      "Iteration: 379 \t--- Loss: 0.349\n",
      "Iteration: 380 \t--- Loss: 0.349\n",
      "Iteration: 381 \t--- Loss: 0.348\n",
      "Iteration: 382 \t--- Loss: 0.349\n",
      "Iteration: 383 \t--- Loss: 0.349\n",
      "Iteration: 384 \t--- Loss: 0.347\n",
      "Iteration: 385 \t--- Loss: 0.348\n",
      "Iteration: 386 \t--- Loss: 0.351\n",
      "Iteration: 387 \t--- Loss: 0.349\n",
      "Iteration: 388 \t--- Loss: 0.350\n",
      "Iteration: 389 \t--- Loss: 0.348\n",
      "Iteration: 390 \t--- Loss: 0.349\n",
      "Iteration: 391 \t--- Loss: 0.348\n",
      "Iteration: 392 \t--- Loss: 0.351\n",
      "Iteration: 393 \t--- Loss: 0.348\n",
      "Iteration: 394 \t--- Loss: 0.348\n",
      "Iteration: 395 \t--- Loss: 0.349\n",
      "Iteration: 396 \t--- Loss: 0.350\n",
      "Iteration: 397 \t--- Loss: 0.349\n",
      "Iteration: 398 \t--- Loss: 0.351\n",
      "Iteration: 399 \t--- Loss: 0.350\n",
      "Iteration: 400 \t--- Loss: 0.350\n",
      "Iteration: 401 \t--- Loss: 0.351\n",
      "Iteration: 402 \t--- Loss: 0.349\n",
      "Iteration: 403 \t--- Loss: 0.347\n",
      "Iteration: 404 \t--- Loss: 0.350\n",
      "Iteration: 405 \t--- Loss: 0.351\n",
      "Iteration: 406 \t--- Loss: 0.348\n",
      "Iteration: 407 \t--- Loss: 0.349\n",
      "Iteration: 408 \t--- Loss: 0.348\n",
      "Iteration: 409 \t--- Loss: 0.350\n",
      "Iteration: 410 \t--- Loss: 0.348\n",
      "Iteration: 411 \t--- Loss: 0.347\n",
      "Iteration: 412 \t--- Loss: 0.350\n",
      "Iteration: 413 \t--- Loss: 0.349\n",
      "Iteration: 414 \t--- Loss: 0.350\n",
      "Iteration: 415 \t--- Loss: 0.348\n",
      "Iteration: 416 \t--- Loss: 0.349\n",
      "Iteration: 417 \t--- Loss: 0.350\n",
      "Iteration: 418 \t--- Loss: 0.349\n",
      "Iteration: 419 \t--- Loss: 0.351\n",
      "Iteration: 420 \t--- Loss: 0.349\n",
      "Iteration: 421 \t--- Loss: 0.349\n",
      "Iteration: 422 \t--- Loss: 0.350\n",
      "Iteration: 423 \t--- Loss: 0.351\n",
      "Iteration: 424 \t--- Loss: 0.349\n",
      "Iteration: 425 \t--- Loss: 0.350\n",
      "Iteration: 426 \t--- Loss: 0.349\n",
      "Iteration: 427 \t--- Loss: 0.349\n",
      "Iteration: 428 \t--- Loss: 0.348\n",
      "Iteration: 429 \t--- Loss: 0.350\n",
      "Iteration: 430 \t--- Loss: 0.348\n",
      "Iteration: 431 \t--- Loss: 0.349\n",
      "Iteration: 432 \t--- Loss: 0.347\n",
      "Iteration: 433 \t--- Loss: 0.348\n",
      "Iteration: 434 \t--- Loss: 0.347\n",
      "Iteration: 435 \t--- Loss: 0.349\n",
      "Iteration: 436 \t--- Loss: 0.351\n",
      "Iteration: 437 \t--- Loss: 0.350\n",
      "Iteration: 438 \t--- Loss: 0.349\n",
      "Iteration: 439 \t--- Loss: 0.351\n",
      "Iteration: 440 \t--- Loss: 0.348\n",
      "Iteration: 441 \t--- Loss: 0.348\n",
      "Iteration: 442 \t--- Loss: 0.350\n",
      "Iteration: 443 \t--- Loss: 0.350\n",
      "Iteration: 444 \t--- Loss: 0.349\n",
      "Iteration: 445 \t--- Loss: 0.347\n",
      "Iteration: 446 \t--- Loss: 0.351\n",
      "Iteration: 447 \t--- Loss: 0.348\n",
      "Iteration: 448 \t--- Loss: 0.348\n",
      "Iteration: 449 \t--- Loss: 0.350\n",
      "Iteration: 450 \t--- Loss: 0.349\n",
      "Iteration: 451 \t--- Loss: 0.349\n",
      "Iteration: 452 \t--- Loss: 0.348\n",
      "Iteration: 453 \t--- Loss: 0.349\n",
      "Iteration: 454 \t--- Loss: 0.350\n",
      "Iteration: 455 \t--- Loss: 0.348\n",
      "Iteration: 456 \t--- Loss: 0.351\n",
      "Iteration: 457 \t--- Loss: 0.351\n",
      "Iteration: 458 \t--- Loss: 0.348\n",
      "Iteration: 459 \t--- Loss: 0.349\n",
      "Iteration: 460 \t--- Loss: 0.347\n",
      "Iteration: 461 \t--- Loss: 0.349\n",
      "Iteration: 462 \t--- Loss: 0.351\n",
      "Iteration: 463 \t--- Loss: 0.348\n",
      "Iteration: 464 \t--- Loss: 0.348\n",
      "Iteration: 465 \t--- Loss: 0.349\n",
      "Iteration: 466 \t--- Loss: 0.345\n",
      "Iteration: 467 \t--- Loss: 0.349\n",
      "Iteration: 468 \t--- Loss: 0.351\n",
      "Iteration: 469 \t--- Loss: 0.349\n",
      "Iteration: 470 \t--- Loss: 0.349\n",
      "Iteration: 471 \t--- Loss: 0.351\n",
      "Iteration: 472 \t--- Loss: 0.348\n",
      "Iteration: 473 \t--- Loss: 0.351\n",
      "Iteration: 474 \t--- Loss: 0.349\n",
      "Iteration: 475 \t--- Loss: 0.351\n",
      "Iteration: 476 \t--- Loss: 0.350\n",
      "Iteration: 477 \t--- Loss: 0.346\n",
      "Iteration: 478 \t--- Loss: 0.349\n",
      "Iteration: 479 \t--- Loss: 0.349\n",
      "Iteration: 480 \t--- Loss: 0.349\n",
      "Iteration: 481 \t--- Loss: 0.350\n",
      "Iteration: 482 \t--- Loss: 0.347\n",
      "Iteration: 483 \t--- Loss: 0.349\n",
      "Iteration: 484 \t--- Loss: 0.352\n",
      "Iteration: 485 \t--- Loss: 0.350\n",
      "Iteration: 486 \t--- Loss: 0.350\n",
      "Iteration: 487 \t--- Loss: 0.350\n",
      "Iteration: 488 \t--- Loss: 0.350\n",
      "Iteration: 489 \t--- Loss: 0.350\n",
      "Iteration: 490 \t--- Loss: 0.348\n",
      "Iteration: 491 \t--- Loss: 0.351\n",
      "Iteration: 492 \t--- Loss: 0.351\n",
      "Iteration: 493 \t--- Loss: 0.350\n",
      "Iteration: 494 \t--- Loss: 0.347\n",
      "Iteration: 495 \t--- Loss: 0.348\n",
      "Iteration: 496 \t--- Loss: 0.349\n",
      "Iteration: 497 \t--- Loss: 0.349\n",
      "Iteration: 498 \t--- Loss: 0.348\n",
      "Iteration: 499 \t--- Loss: 0.348\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 2.708\n",
      "Iteration: 1 \t--- Loss: 2.582\n",
      "Iteration: 2 \t--- Loss: 2.139\n",
      "Iteration: 3 \t--- Loss: 2.349\n",
      "Iteration: 4 \t--- Loss: 2.049\n",
      "Iteration: 5 \t--- Loss: 1.894\n",
      "Iteration: 6 \t--- Loss: 1.967\n",
      "Iteration: 7 \t--- Loss: 2.087\n",
      "Iteration: 8 \t--- Loss: 1.883\n",
      "Iteration: 9 \t--- Loss: 1.902\n",
      "Iteration: 10 \t--- Loss: 1.967\n",
      "Iteration: 11 \t--- Loss: 1.870\n",
      "Iteration: 12 \t--- Loss: 2.018\n",
      "Iteration: 13 \t--- Loss: 1.924\n",
      "Iteration: 14 \t--- Loss: 1.873\n",
      "Iteration: 15 \t--- Loss: 1.806\n",
      "Iteration: 16 \t--- Loss: 1.939\n",
      "Iteration: 17 \t--- Loss: 1.919\n",
      "Iteration: 18 \t--- Loss: 1.894\n",
      "Iteration: 19 \t--- Loss: 1.887\n",
      "Iteration: 20 \t--- Loss: 1.871\n",
      "Iteration: 21 \t--- Loss: 1.885\n",
      "Iteration: 22 \t--- Loss: 1.926\n",
      "Iteration: 23 \t--- Loss: 1.865\n",
      "Iteration: 24 \t--- Loss: 2.034\n",
      "Iteration: 25 \t--- Loss: 1.897\n",
      "Iteration: 26 \t--- Loss: 1.981\n",
      "Iteration: 27 \t--- Loss: 1.907\n",
      "Iteration: 28 \t--- Loss: 1.868\n",
      "Iteration: 29 \t--- Loss: 1.961\n",
      "Iteration: 30 \t--- Loss: 1.877\n",
      "Iteration: 31 \t--- Loss: 1.864\n",
      "Iteration: 32 \t--- Loss: 1.821\n",
      "Iteration: 33 \t--- Loss: 1.932\n",
      "Iteration: 34 \t--- Loss: 2.017\n",
      "Iteration: 35 \t--- Loss: 2.038\n",
      "Iteration: 36 \t--- Loss: 1.892\n",
      "Iteration: 37 \t--- Loss: 1.868\n",
      "Iteration: 38 \t--- Loss: 1.928\n",
      "Iteration: 39 \t--- Loss: 1.836\n",
      "Iteration: 40 \t--- Loss: 1.910\n",
      "Iteration: 41 \t--- Loss: 1.869\n",
      "Iteration: 42 \t--- Loss: 1.872\n",
      "Iteration: 43 \t--- Loss: 1.804\n",
      "Iteration: 44 \t--- Loss: 1.965\n",
      "Iteration: 45 \t--- Loss: 1.878\n",
      "Iteration: 46 \t--- Loss: 1.908\n",
      "Iteration: 47 \t--- Loss: 1.917\n",
      "Iteration: 48 \t--- Loss: 1.894\n",
      "Iteration: 49 \t--- Loss: 1.826\n",
      "Iteration: 50 \t--- Loss: 1.933\n",
      "Iteration: 51 \t--- Loss: 1.909\n",
      "Iteration: 52 \t--- Loss: 1.924\n",
      "Iteration: 53 \t--- Loss: 1.859\n",
      "Iteration: 54 \t--- Loss: 1.917\n",
      "Iteration: 55 \t--- Loss: 1.912\n",
      "Iteration: 56 \t--- Loss: 1.832\n",
      "Iteration: 57 \t--- Loss: 1.817\n",
      "Iteration: 58 \t--- Loss: 1.784\n",
      "Iteration: 59 \t--- Loss: 1.864\n",
      "Iteration: 60 \t--- Loss: 1.891\n",
      "Iteration: 61 \t--- Loss: 1.881\n",
      "Iteration: 62 \t--- Loss: 1.949\n",
      "Iteration: 63 \t--- Loss: 1.933\n",
      "Iteration: 64 \t--- Loss: 1.998\n",
      "Iteration: 65 \t--- Loss: 1.875\n",
      "Iteration: 66 \t--- Loss: 1.910\n",
      "Iteration: 67 \t--- Loss: 1.932\n",
      "Iteration: 68 \t--- Loss: 1.987\n",
      "Iteration: 69 \t--- Loss: 1.864\n",
      "Iteration: 70 \t--- Loss: 1.861\n",
      "Iteration: 71 \t--- Loss: 1.919\n",
      "Iteration: 72 \t--- Loss: 1.860\n",
      "Iteration: 73 \t--- Loss: 1.873\n",
      "Iteration: 74 \t--- Loss: 1.814\n",
      "Iteration: 75 \t--- Loss: 1.924\n",
      "Iteration: 76 \t--- Loss: 1.904\n",
      "Iteration: 77 \t--- Loss: 1.904\n",
      "Iteration: 78 \t--- Loss: 1.845\n",
      "Iteration: 79 \t--- Loss: 1.888\n",
      "Iteration: 80 \t--- Loss: 1.938\n",
      "Iteration: 81 \t--- Loss: 1.794\n",
      "Iteration: 82 \t--- Loss: 1.855\n",
      "Iteration: 83 \t--- Loss: 1.766\n",
      "Iteration: 84 \t--- Loss: 1.881\n",
      "Iteration: 85 \t--- Loss: 1.927\n",
      "Iteration: 86 \t--- Loss: 1.949\n",
      "Iteration: 87 \t--- Loss: 1.876\n",
      "Iteration: 88 \t--- Loss: 1.891\n",
      "Iteration: 89 \t--- Loss: 1.836\n",
      "Iteration: 90 \t--- Loss: 1.913\n",
      "Iteration: 91 \t--- Loss: 1.830\n",
      "Iteration: 92 \t--- Loss: 1.919\n",
      "Iteration: 93 \t--- Loss: 1.925\n",
      "Iteration: 94 \t--- Loss: 1.914\n",
      "Iteration: 95 \t--- Loss: 1.735\n",
      "Iteration: 96 \t--- Loss: 1.916\n",
      "Iteration: 97 \t--- Loss: 1.911\n",
      "Iteration: 98 \t--- Loss: 1.869\n",
      "Iteration: 99 \t--- Loss: 1.995\n",
      "Iteration: 100 \t--- Loss: 1.885\n",
      "Iteration: 101 \t--- Loss: 1.818\n",
      "Iteration: 102 \t--- Loss: 1.880\n",
      "Iteration: 103 \t--- Loss: 1.879\n",
      "Iteration: 104 \t--- Loss: 1.921\n",
      "Iteration: 105 \t--- Loss: 1.887\n",
      "Iteration: 106 \t--- Loss: 1.820\n",
      "Iteration: 107 \t--- Loss: 1.842\n",
      "Iteration: 108 \t--- Loss: 1.942\n",
      "Iteration: 109 \t--- Loss: 1.964\n",
      "Iteration: 110 \t--- Loss: 1.988\n",
      "Iteration: 111 \t--- Loss: 1.888\n",
      "Iteration: 112 \t--- Loss: 1.880\n",
      "Iteration: 113 \t--- Loss: 1.803\n",
      "Iteration: 114 \t--- Loss: 1.902\n",
      "Iteration: 115 \t--- Loss: 1.863\n",
      "Iteration: 116 \t--- Loss: 1.913\n",
      "Iteration: 117 \t--- Loss: 1.919\n",
      "Iteration: 118 \t--- Loss: 1.839\n",
      "Iteration: 119 \t--- Loss: 1.884\n",
      "Iteration: 120 \t--- Loss: 1.907\n",
      "Iteration: 121 \t--- Loss: 1.994\n",
      "Iteration: 122 \t--- Loss: 1.913\n",
      "Iteration: 123 \t--- Loss: 1.895\n",
      "Iteration: 124 \t--- Loss: 1.998\n",
      "Iteration: 125 \t--- Loss: 1.818\n",
      "Iteration: 126 \t--- Loss: 1.959\n",
      "Iteration: 127 \t--- Loss: 1.930\n",
      "Iteration: 128 \t--- Loss: 1.805\n",
      "Iteration: 129 \t--- Loss: 1.923\n",
      "Iteration: 130 \t--- Loss: 1.887\n",
      "Iteration: 131 \t--- Loss: 1.847\n",
      "Iteration: 132 \t--- Loss: 1.735\n",
      "Iteration: 133 \t--- Loss: 1.798\n",
      "Iteration: 134 \t--- Loss: 1.915\n",
      "Iteration: 135 \t--- Loss: 1.850\n",
      "Iteration: 136 \t--- Loss: 1.837\n",
      "Iteration: 137 \t--- Loss: 1.859\n",
      "Iteration: 138 \t--- Loss: 1.914\n",
      "Iteration: 139 \t--- Loss: 1.889\n",
      "Iteration: 140 \t--- Loss: 1.857\n",
      "Iteration: 141 \t--- Loss: 1.870\n",
      "Iteration: 142 \t--- Loss: 1.896\n",
      "Iteration: 143 \t--- Loss: 1.868\n",
      "Iteration: 144 \t--- Loss: 1.896\n",
      "Iteration: 145 \t--- Loss: 1.970\n",
      "Iteration: 146 \t--- Loss: 1.909\n",
      "Iteration: 147 \t--- Loss: 1.844\n",
      "Iteration: 148 \t--- Loss: 1.848\n",
      "Iteration: 149 \t--- Loss: 1.858\n",
      "Iteration: 150 \t--- Loss: 1.852\n",
      "Iteration: 151 \t--- Loss: 1.893\n",
      "Iteration: 152 \t--- Loss: 1.845\n",
      "Iteration: 153 \t--- Loss: 1.939\n",
      "Iteration: 154 \t--- Loss: 1.923\n",
      "Iteration: 155 \t--- Loss: 1.868\n",
      "Iteration: 156 \t--- Loss: 1.893\n",
      "Iteration: 157 \t--- Loss: 1.921\n",
      "Iteration: 158 \t--- Loss: 1.937\n",
      "Iteration: 159 \t--- Loss: 1.943\n",
      "Iteration: 160 \t--- Loss: 1.891\n",
      "Iteration: 161 \t--- Loss: 1.965\n",
      "Iteration: 162 \t--- Loss: 1.810\n",
      "Iteration: 163 \t--- Loss: 1.923\n",
      "Iteration: 164 \t--- Loss: 1.920\n",
      "Iteration: 165 \t--- Loss: 1.866\n",
      "Iteration: 166 \t--- Loss: 1.915\n",
      "Iteration: 167 \t--- Loss: 2.008\n",
      "Iteration: 168 \t--- Loss: 1.844\n",
      "Iteration: 169 \t--- Loss: 1.892\n",
      "Iteration: 170 \t--- Loss: 1.892\n",
      "Iteration: 171 \t--- Loss: 1.913\n",
      "Iteration: 172 \t--- Loss: 1.859\n",
      "Iteration: 173 \t--- Loss: 2.047\n",
      "Iteration: 174 \t--- Loss: 1.838\n",
      "Iteration: 175 \t--- Loss: 1.837\n",
      "Iteration: 176 \t--- Loss: 2.028\n",
      "Iteration: 177 \t--- Loss: 1.902\n",
      "Iteration: 178 \t--- Loss: 1.867\n",
      "Iteration: 179 \t--- Loss: 1.895\n",
      "Iteration: 180 \t--- Loss: 1.862\n",
      "Iteration: 181 \t--- Loss: 1.857\n",
      "Iteration: 182 \t--- Loss: 1.890\n",
      "Iteration: 183 \t--- Loss: 1.981\n",
      "Iteration: 184 \t--- Loss: 1.902\n",
      "Iteration: 185 \t--- Loss: 1.958\n",
      "Iteration: 186 \t--- Loss: 1.991\n",
      "Iteration: 187 \t--- Loss: 1.878\n",
      "Iteration: 188 \t--- Loss: 1.917\n",
      "Iteration: 189 \t--- Loss: 1.933\n",
      "Iteration: 190 \t--- Loss: 1.897\n",
      "Iteration: 191 \t--- Loss: 1.942\n",
      "Iteration: 192 \t--- Loss: 1.975\n",
      "Iteration: 193 \t--- Loss: 1.994\n",
      "Iteration: 194 \t--- Loss: 1.880\n",
      "Iteration: 195 \t--- Loss: 1.919\n",
      "Iteration: 196 \t--- Loss: 1.903\n",
      "Iteration: 197 \t--- Loss: 1.902\n",
      "Iteration: 198 \t--- Loss: 1.900\n",
      "Iteration: 199 \t--- Loss: 1.929\n",
      "Iteration: 200 \t--- Loss: 1.821\n",
      "Iteration: 201 \t--- Loss: 1.891\n",
      "Iteration: 202 \t--- Loss: 1.912\n",
      "Iteration: 203 \t--- Loss: 1.806\n",
      "Iteration: 204 \t--- Loss: 1.888\n",
      "Iteration: 205 \t--- Loss: 1.912\n",
      "Iteration: 206 \t--- Loss: 1.805\n",
      "Iteration: 207 \t--- Loss: 1.796\n",
      "Iteration: 208 \t--- Loss: 1.973\n",
      "Iteration: 209 \t--- Loss: 1.959\n",
      "Iteration: 210 \t--- Loss: 1.808\n",
      "Iteration: 211 \t--- Loss: 1.936\n",
      "Iteration: 212 \t--- Loss: 1.968\n",
      "Iteration: 213 \t--- Loss: 1.960\n",
      "Iteration: 214 \t--- Loss: 1.866\n",
      "Iteration: 215 \t--- Loss: 1.884\n",
      "Iteration: 216 \t--- Loss: 1.895\n",
      "Iteration: 217 \t--- Loss: 1.864\n",
      "Iteration: 218 \t--- Loss: 1.966\n",
      "Iteration: 219 \t--- Loss: 1.846\n",
      "Iteration: 220 \t--- Loss: 1.948\n",
      "Iteration: 221 \t--- Loss: 1.871\n",
      "Iteration: 222 \t--- Loss: 1.877\n",
      "Iteration: 223 \t--- Loss: 1.782\n",
      "Iteration: 224 \t--- Loss: 1.980\n",
      "Iteration: 225 \t--- Loss: 1.887\n",
      "Iteration: 226 \t--- Loss: 1.797\n",
      "Iteration: 227 \t--- Loss: 1.892\n",
      "Iteration: 228 \t--- Loss: 1.925\n",
      "Iteration: 229 \t--- Loss: 1.863\n",
      "Iteration: 230 \t--- Loss: 1.941\n",
      "Iteration: 231 \t--- Loss: 1.981\n",
      "Iteration: 232 \t--- Loss: 1.921\n",
      "Iteration: 233 \t--- Loss: 1.935\n",
      "Iteration: 234 \t--- Loss: 1.897\n",
      "Iteration: 235 \t--- Loss: 1.815\n",
      "Iteration: 236 \t--- Loss: 1.969\n",
      "Iteration: 237 \t--- Loss: 1.889\n",
      "Iteration: 238 \t--- Loss: 1.891\n",
      "Iteration: 239 \t--- Loss: 1.766\n",
      "Iteration: 240 \t--- Loss: 1.881\n",
      "Iteration: 241 \t--- Loss: 1.925\n",
      "Iteration: 242 \t--- Loss: 1.895\n",
      "Iteration: 243 \t--- Loss: 1.910\n",
      "Iteration: 244 \t--- Loss: 1.907\n",
      "Iteration: 245 \t--- Loss: 1.848\n",
      "Iteration: 246 \t--- Loss: 1.952\n",
      "Iteration: 247 \t--- Loss: 1.827\n",
      "Iteration: 248 \t--- Loss: 1.921\n",
      "Iteration: 249 \t--- Loss: 1.932\n",
      "Iteration: 250 \t--- Loss: 1.902\n",
      "Iteration: 251 \t--- Loss: 1.857\n",
      "Iteration: 252 \t--- Loss: 1.860\n",
      "Iteration: 253 \t--- Loss: 1.970\n",
      "Iteration: 254 \t--- Loss: 1.907\n",
      "Iteration: 255 \t--- Loss: 1.841\n",
      "Iteration: 256 \t--- Loss: 1.926\n",
      "Iteration: 257 \t--- Loss: 1.955\n",
      "Iteration: 258 \t--- Loss: 1.904\n",
      "Iteration: 259 \t--- Loss: 1.902"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it][Parallel(n_jobs=5)]: Done  78 tasks      | elapsed: 49.2min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.246\n",
      "Iteration: 1 \t--- Loss: 0.264\n",
      "Iteration: 2 \t--- Loss: 0.202\n",
      "Iteration: 3 \t--- Loss: 0.187\n",
      "Iteration: 4 \t--- Loss: 0.164\n",
      "Iteration: 5 \t--- Loss: 0.177\n",
      "Iteration: 6 \t--- Loss: 0.156\n",
      "Iteration: 7 \t--- Loss: 0.160\n",
      "Iteration: 8 \t--- Loss: 0.134\n",
      "Iteration: 9 \t--- Loss: 0.114\n",
      "Iteration: 10 \t--- Loss: 0.102\n",
      "Iteration: 11 \t--- Loss: 0.109\n",
      "Iteration: 12 \t--- Loss: 0.087\n",
      "Iteration: 13 \t--- Loss: 0.102\n",
      "Iteration: 14 \t--- Loss: 0.090\n",
      "Iteration: 15 \t--- Loss: 0.091\n",
      "Iteration: 16 \t--- Loss: 0.088\n",
      "Iteration: 17 \t--- Loss: 0.093\n",
      "Iteration: 18 \t--- Loss: 0.080\n",
      "Iteration: 19 \t--- Loss: 0.074\n",
      "Iteration: 20 \t--- Loss: 0.086\n",
      "Iteration: 21 \t--- Loss: 0.076\n",
      "Iteration: 22 \t--- Loss: 0.080\n",
      "Iteration: 23 \t--- Loss: 0.073\n",
      "Iteration: 24 \t--- Loss: 0.082\n",
      "Iteration: 25 \t--- Loss: 0.069\n",
      "Iteration: 26 \t--- Loss: 0.075\n",
      "Iteration: 27 \t--- Loss: 0.066\n",
      "Iteration: 28 \t--- Loss: 0.073\n",
      "Iteration: 29 \t--- Loss: 0.064\n",
      "Iteration: 30 \t--- Loss: 0.066\n",
      "Iteration: 31 \t--- Loss: 0.070\n",
      "Iteration: 32 \t--- Loss: 0.071\n",
      "Iteration: 33 \t--- Loss: 0.069\n",
      "Iteration: 34 \t--- Loss: 0.065\n",
      "Iteration: 35 \t--- Loss: 0.063\n",
      "Iteration: 36 \t--- Loss: 0.067\n",
      "Iteration: 37 \t--- Loss: 0.060\n",
      "Iteration: 38 \t--- Loss: 0.057\n",
      "Iteration: 39 \t--- Loss: 0.068\n",
      "Iteration: 40 \t--- Loss: 0.060\n",
      "Iteration: 41 \t--- Loss: 0.063\n",
      "Iteration: 42 \t--- Loss: 0.072\n",
      "Iteration: 43 \t--- Loss: 0.060\n",
      "Iteration: 44 \t--- Loss: 0.064\n",
      "Iteration: 45 \t--- Loss: 0.064\n",
      "Iteration: 46 \t--- Loss: 0.059\n",
      "Iteration: 47 \t--- Loss: 0.063\n",
      "Iteration: 48 \t--- Loss: 0.052\n",
      "Iteration: 49 \t--- Loss: 0.061\n",
      "Iteration: 50 \t--- Loss: 0.057\n",
      "Iteration: 51 \t--- Loss: 0.065\n",
      "Iteration: 52 \t--- Loss: 0.058\n",
      "Iteration: 53 \t--- Loss: 0.056\n",
      "Iteration: 54 \t--- Loss: 0.061\n",
      "Iteration: 55 \t--- Loss: 0.063\n",
      "Iteration: 56 \t--- Loss: 0.057\n",
      "Iteration: 57 \t--- Loss: 0.064\n",
      "Iteration: 58 \t--- Loss: 0.056\n",
      "Iteration: 59 \t--- Loss: 0.057\n",
      "Iteration: 60 \t--- Loss: 0.061\n",
      "Iteration: 61 \t--- Loss: 0.058\n",
      "Iteration: 62 \t--- Loss: 0.061\n",
      "Iteration: 63 \t--- Loss: 0.059\n",
      "Iteration: 64 \t--- Loss: 0.059\n",
      "Iteration: 65 \t--- Loss: 0.058\n",
      "Iteration: 66 \t--- Loss: 0.056\n",
      "Iteration: 67 \t--- Loss: 0.062\n",
      "Iteration: 68 \t--- Loss: 0.055\n",
      "Iteration: 69 \t--- Loss: 0.053\n",
      "Iteration: 70 \t--- Loss: 0.056\n",
      "Iteration: 71 \t--- Loss: 0.060\n",
      "Iteration: 72 \t--- Loss: 0.055\n",
      "Iteration: 73 \t--- Loss: 0.058\n",
      "Iteration: 74 \t--- Loss: 0.058\n",
      "Iteration: 75 \t--- Loss: 0.053\n",
      "Iteration: 76 \t--- Loss: 0.064\n",
      "Iteration: 77 \t--- Loss: 0.057\n",
      "Iteration: 78 \t--- Loss: 0.055\n",
      "Iteration: 79 \t--- Loss: 0.051\n",
      "Iteration: 80 \t--- Loss: 0.058\n",
      "Iteration: 81 \t--- Loss: 0.055\n",
      "Iteration: 82 \t--- Loss: 0.057\n",
      "Iteration: 83 \t--- Loss: 0.059\n",
      "Iteration: 84 \t--- Loss: 0.054\n",
      "Iteration: 85 \t--- Loss: 0.061\n",
      "Iteration: 86 \t--- Loss: 0.053\n",
      "Iteration: 87 \t--- Loss: 0.052\n",
      "Iteration: 88 \t--- Loss: 0.051\n",
      "Iteration: 89 \t--- Loss: 0.053\n",
      "Iteration: 90 \t--- Loss: 0.055\n",
      "Iteration: 91 \t--- Loss: 0.055\n",
      "Iteration: 92 \t--- Loss: 0.051\n",
      "Iteration: 93 \t--- Loss: 0.056\n",
      "Iteration: 94 \t--- Loss: 0.055\n",
      "Iteration: 95 \t--- Loss: 0.055\n",
      "Iteration: 96 \t--- Loss: 0.059\n",
      "Iteration: 97 \t--- Loss: 0.056\n",
      "Iteration: 98 \t--- Loss: 0.060\n",
      "Iteration: 99 \t--- Loss: 0.059\n",
      "Iteration: 100 \t--- Loss: 0.053\n",
      "Iteration: 101 \t--- Loss: 0.056\n",
      "Iteration: 102 \t--- Loss: 0.058\n",
      "Iteration: 103 \t--- Loss: 0.056\n",
      "Iteration: 104 \t--- Loss: 0.052\n",
      "Iteration: 105 \t--- Loss: 0.053\n",
      "Iteration: 106 \t--- Loss: 0.053\n",
      "Iteration: 107 \t--- Loss: 0.047\n",
      "Iteration: 108 \t--- Loss: 0.055\n",
      "Iteration: 109 \t--- Loss: 0.052\n",
      "Iteration: 110 \t--- Loss: 0.052\n",
      "Iteration: 111 \t--- Loss: 0.056\n",
      "Iteration: 112 \t--- Loss: 0.060\n",
      "Iteration: 113 \t--- Loss: 0.053\n",
      "Iteration: 114 \t--- Loss: 0.056\n",
      "Iteration: 115 \t--- Loss: 0.050\n",
      "Iteration: 116 \t--- Loss: 0.056\n",
      "Iteration: 117 \t--- Loss: 0.055\n",
      "Iteration: 118 \t--- Loss: 0.054\n",
      "Iteration: 119 \t--- Loss: 0.055\n",
      "Iteration: 120 \t--- Loss: 0.056\n",
      "Iteration: 121 \t--- Loss: 0.052\n",
      "Iteration: 122 \t--- Loss: 0.057\n",
      "Iteration: 123 \t--- Loss: 0.052\n",
      "Iteration: 124 \t--- Loss: 0.055\n",
      "Iteration: 125 \t--- Loss: 0.054\n",
      "Iteration: 126 \t--- Loss: 0.058\n",
      "Iteration: 127 \t--- Loss: 0.054\n",
      "Iteration: 128 \t--- Loss: 0.054\n",
      "Iteration: 129 \t--- Loss: 0.058\n",
      "Iteration: 130 \t--- Loss: 0.054\n",
      "Iteration: 131 \t--- Loss: 0.057\n",
      "Iteration: 132 \t--- Loss: 0.058\n",
      "Iteration: 133 \t--- Loss: 0.055\n",
      "Iteration: 134 \t--- Loss: 0.052\n",
      "Iteration: 135 \t--- Loss: 0.055\n",
      "Iteration: 136 \t--- Loss: 0.049\n",
      "Iteration: 137 \t--- Loss: 0.057\n",
      "Iteration: 138 \t--- Loss: 0.055\n",
      "Iteration: 139 \t--- Loss: 0.053\n",
      "Iteration: 140 \t--- Loss: 0.052\n",
      "Iteration: 141 \t--- Loss: 0.057\n",
      "Iteration: 142 \t--- Loss: 0.054\n",
      "Iteration: 143 \t--- Loss: 0.056\n",
      "Iteration: 144 \t--- Loss: 0.056\n",
      "Iteration: 145 \t--- Loss: 0.052\n",
      "Iteration: 146 \t--- Loss: 0.056\n",
      "Iteration: 147 \t--- Loss: 0.061\n",
      "Iteration: 148 \t--- Loss: 0.060\n",
      "Iteration: 149 \t--- Loss: 0.054\n",
      "Iteration: 150 \t--- Loss: 0.051\n",
      "Iteration: 151 \t--- Loss: 0.056\n",
      "Iteration: 152 \t--- Loss: 0.057\n",
      "Iteration: 153 \t--- Loss: 0.059\n",
      "Iteration: 154 \t--- Loss: 0.055\n",
      "Iteration: 155 \t--- Loss: 0.054\n",
      "Iteration: 156 \t--- Loss: 0.055\n",
      "Iteration: 157 \t--- Loss: 0.053\n",
      "Iteration: 158 \t--- Loss: 0.056\n",
      "Iteration: 159 \t--- Loss: 0.057\n",
      "Iteration: 160 \t--- Loss: 0.057\n",
      "Iteration: 161 \t--- Loss: 0.050\n",
      "Iteration: 162 \t--- Loss: 0.050\n",
      "Iteration: 163 \t--- Loss: 0.058\n",
      "Iteration: 164 \t--- Loss: 0.050\n",
      "Iteration: 165 \t--- Loss: 0.048\n",
      "Iteration: 166 \t--- Loss: 0.052\n",
      "Iteration: 167 \t--- Loss: 0.053\n",
      "Iteration: 168 \t--- Loss: 0.054\n",
      "Iteration: 169 \t--- Loss: 0.049\n",
      "Iteration: 170 \t--- Loss: 0.053\n",
      "Iteration: 171 \t--- Loss: 0.057\n",
      "Iteration: 172 \t--- Loss: 0.054\n",
      "Iteration: 173 \t--- Loss: 0.056\n",
      "Iteration: 174 \t--- Loss: 0.052\n",
      "Iteration: 175 \t--- Loss: 0.052\n",
      "Iteration: 176 \t--- Loss: 0.055\n",
      "Iteration: 177 \t--- Loss: 0.057\n",
      "Iteration: 178 \t--- Loss: 0.054\n",
      "Iteration: 179 \t--- Loss: 0.055\n",
      "Iteration: 180 \t--- Loss: 0.056\n",
      "Iteration: 181 \t--- Loss: 0.054\n",
      "Iteration: 182 \t--- Loss: 0.053\n",
      "Iteration: 183 \t--- Loss: 0.049\n",
      "Iteration: 184 \t--- Loss: 0.051\n",
      "Iteration: 185 \t--- Loss: 0.057\n",
      "Iteration: 186 \t--- Loss: 0.054\n",
      "Iteration: 187 \t--- Loss: 0.050\n",
      "Iteration: 188 \t--- Loss: 0.047\n",
      "Iteration: 189 \t--- Loss: 0.061\n",
      "Iteration: 190 \t--- Loss: 0.057\n",
      "Iteration: 191 \t--- Loss: 0.053\n",
      "Iteration: 192 \t--- Loss: 0.056\n",
      "Iteration: 193 \t--- Loss: 0.054\n",
      "Iteration: 194 \t--- Loss: 0.054\n",
      "Iteration: 195 \t--- Loss: 0.059\n",
      "Iteration: 196 \t--- Loss: 0.050\n",
      "Iteration: 197 \t--- Loss: 0.051\n",
      "Iteration: 198 \t--- Loss: 0.050\n",
      "Iteration: 199 \t--- Loss: 0.060\n",
      "Iteration: 200 \t--- Loss: 0.053\n",
      "Iteration: 201 \t--- Loss: 0.056\n",
      "Iteration: 202 \t--- Loss: 0.052\n",
      "Iteration: 203 \t--- Loss: 0.047\n",
      "Iteration: 204 \t--- Loss: 0.060\n",
      "Iteration: 205 \t--- Loss: 0.050\n",
      "Iteration: 206 \t--- Loss: 0.055\n",
      "Iteration: 207 \t--- Loss: 0.059\n",
      "Iteration: 208 \t--- Loss: 0.052\n",
      "Iteration: 209 \t--- Loss: 0.058\n",
      "Iteration: 210 \t--- Loss: 0.061\n",
      "Iteration: 211 \t--- Loss: 0.057\n",
      "Iteration: 212 \t--- Loss: 0.049\n",
      "Iteration: 213 \t--- Loss: 0.053\n",
      "Iteration: 214 \t--- Loss: 0.056\n",
      "Iteration: 215 \t--- Loss: 0.058\n",
      "Iteration: 216 \t--- Loss: 0.052\n",
      "Iteration: 217 \t--- Loss: 0.053\n",
      "Iteration: 218 \t--- Loss: 0.055\n",
      "Iteration: 219 \t--- Loss: 0.050\n",
      "Iteration: 220 \t--- Loss: 0.050\n",
      "Iteration: 221 \t--- Loss: 0.052\n",
      "Iteration: 222 \t--- Loss: 0.057\n",
      "Iteration: 223 \t--- Loss: 0.052\n",
      "Iteration: 224 \t--- Loss: 0.052\n",
      "Iteration: 225 \t--- Loss: 0.055\n",
      "Iteration: 226 \t--- Loss: 0.053\n",
      "Iteration: 227 \t--- Loss: 0.055\n",
      "Iteration: 228 \t--- Loss: 0.052\n",
      "Iteration: 229 \t--- Loss: 0.053\n",
      "Iteration: 230 \t--- Loss: 0.051\n",
      "Iteration: 231 \t--- Loss: 0.053\n",
      "Iteration: 232 \t--- Loss: 0.061\n",
      "Iteration: 233 \t--- Loss: 0.052\n",
      "Iteration: 234 \t--- Loss: 0.055\n",
      "Iteration: 235 \t--- Loss: 0.050\n",
      "Iteration: 236 \t--- Loss: 0.064\n",
      "Iteration: 237 \t--- Loss: 0.057\n",
      "Iteration: 238 \t--- Loss: 0.057\n",
      "Iteration: 239 \t--- Loss: 0.058\n",
      "Iteration: 240 \t--- Loss: 0.051\n",
      "Iteration: 241 \t--- Loss: 0.053\n",
      "Iteration: 242 \t--- Loss: 0.055\n",
      "Iteration: 243 \t--- Loss: 0.051\n",
      "Iteration: 244 \t--- Loss: 0.050\n",
      "Iteration: 245 \t--- Loss: 0.057\n",
      "Iteration: 246 \t--- Loss: 0.056\n",
      "Iteration: 247 \t--- Loss: 0.049\n",
      "Iteration: 248 \t--- Loss: 0.057\n",
      "Iteration: 249 \t--- Loss: 0.058\n",
      "Iteration: 250 \t--- Loss: 0.054\n",
      "Iteration: 251 \t--- Loss: 0.051\n",
      "Iteration: 252 \t--- Loss: 0.054\n",
      "Iteration: 253 \t--- Loss: 0.055\n",
      "Iteration: 254 \t--- Loss: 0.054\n",
      "Iteration: 255 \t--- Loss: 0.054\n",
      "Iteration: 256 \t--- Loss: 0.054\n",
      "Iteration: 257 \t--- Loss: 0.060\n",
      "Iteration: 258 \t--- Loss: 0.054\n",
      "Iteration: 259 \t--- Loss: 0.059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.61s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 1.932\n",
      "Iteration: 261 \t--- Loss: 1.814\n",
      "Iteration: 262 \t--- Loss: 1.880\n",
      "Iteration: 263 \t--- Loss: 1.908\n",
      "Iteration: 264 \t--- Loss: 1.965\n",
      "Iteration: 265 \t--- Loss: 1.894\n",
      "Iteration: 266 \t--- Loss: 1.830\n",
      "Iteration: 267 \t--- Loss: 1.912\n",
      "Iteration: 268 \t--- Loss: 1.789\n",
      "Iteration: 269 \t--- Loss: 1.824\n",
      "Iteration: 270 \t--- Loss: 1.902\n",
      "Iteration: 271 \t--- Loss: 1.848\n",
      "Iteration: 272 \t--- Loss: 1.894\n",
      "Iteration: 273 \t--- Loss: 1.879\n",
      "Iteration: 274 \t--- Loss: 1.835\n",
      "Iteration: 275 \t--- Loss: 1.914\n",
      "Iteration: 276 \t--- Loss: 1.974\n",
      "Iteration: 277 \t--- Loss: 1.846\n",
      "Iteration: 278 \t--- Loss: 1.897\n",
      "Iteration: 279 \t--- Loss: 1.996\n",
      "Iteration: 280 \t--- Loss: 1.825\n",
      "Iteration: 281 \t--- Loss: 1.713\n",
      "Iteration: 282 \t--- Loss: 1.873\n",
      "Iteration: 283 \t--- Loss: 1.877\n",
      "Iteration: 284 \t--- Loss: 1.892\n",
      "Iteration: 285 \t--- Loss: 1.970\n",
      "Iteration: 286 \t--- Loss: 1.908\n",
      "Iteration: 287 \t--- Loss: 1.805\n",
      "Iteration: 288 \t--- Loss: 1.982\n",
      "Iteration: 289 \t--- Loss: 1.944\n",
      "Iteration: 290 \t--- Loss: 1.897\n",
      "Iteration: 291 \t--- Loss: 1.934\n",
      "Iteration: 292 \t--- Loss: 1.903\n",
      "Iteration: 293 \t--- Loss: 1.875\n",
      "Iteration: 294 \t--- Loss: 1.884\n",
      "Iteration: 295 \t--- Loss: 1.970\n",
      "Iteration: 296 \t--- Loss: 1.950\n",
      "Iteration: 297 \t--- Loss: 1.929\n",
      "Iteration: 298 \t--- Loss: 1.946\n",
      "Iteration: 299 \t--- Loss: 1.853\n",
      "Iteration: 300 \t--- Loss: 1.851\n",
      "Iteration: 301 \t--- Loss: 1.959\n",
      "Iteration: 302 \t--- Loss: 1.890\n",
      "Iteration: 303 \t--- Loss: 1.895\n",
      "Iteration: 304 \t--- Loss: 1.916\n",
      "Iteration: 305 \t--- Loss: 1.946\n",
      "Iteration: 306 \t--- Loss: 1.893\n",
      "Iteration: 307 \t--- Loss: 1.842\n",
      "Iteration: 308 \t--- Loss: 1.885\n",
      "Iteration: 309 \t--- Loss: 1.841\n",
      "Iteration: 310 \t--- Loss: 1.861\n",
      "Iteration: 311 \t--- Loss: 1.883\n",
      "Iteration: 312 \t--- Loss: 1.796\n",
      "Iteration: 313 \t--- Loss: 1.996\n",
      "Iteration: 314 \t--- Loss: 1.892\n",
      "Iteration: 315 \t--- Loss: 1.881\n",
      "Iteration: 316 \t--- Loss: 1.885\n",
      "Iteration: 317 \t--- Loss: 1.935\n",
      "Iteration: 318 \t--- Loss: 1.839\n",
      "Iteration: 319 \t--- Loss: 1.948\n",
      "Iteration: 320 \t--- Loss: 1.950\n",
      "Iteration: 321 \t--- Loss: 1.882\n",
      "Iteration: 322 \t--- Loss: 1.939\n",
      "Iteration: 323 \t--- Loss: 1.903\n",
      "Iteration: 324 \t--- Loss: 1.892\n",
      "Iteration: 325 \t--- Loss: 1.849\n",
      "Iteration: 326 \t--- Loss: 1.848\n",
      "Iteration: 327 \t--- Loss: 1.953\n",
      "Iteration: 328 \t--- Loss: 1.913\n",
      "Iteration: 329 \t--- Loss: 1.819\n",
      "Iteration: 330 \t--- Loss: 1.913\n",
      "Iteration: 331 \t--- Loss: 1.992\n",
      "Iteration: 332 \t--- Loss: 1.898\n",
      "Iteration: 333 \t--- Loss: 1.881\n",
      "Iteration: 334 \t--- Loss: 1.903\n",
      "Iteration: 335 \t--- Loss: 1.905\n",
      "Iteration: 336 \t--- Loss: 1.946\n",
      "Iteration: 337 \t--- Loss: 1.950\n",
      "Iteration: 338 \t--- Loss: 1.954\n",
      "Iteration: 339 \t--- Loss: 1.917\n",
      "Iteration: 340 \t--- Loss: 1.922\n",
      "Iteration: 341 \t--- Loss: 1.941\n",
      "Iteration: 342 \t--- Loss: 1.926\n",
      "Iteration: 343 \t--- Loss: 1.775\n",
      "Iteration: 344 \t--- Loss: 1.845\n",
      "Iteration: 345 \t--- Loss: 1.872\n",
      "Iteration: 346 \t--- Loss: 1.907\n",
      "Iteration: 347 \t--- Loss: 1.967\n",
      "Iteration: 348 \t--- Loss: 1.957\n",
      "Iteration: 349 \t--- Loss: 1.881\n",
      "Iteration: 350 \t--- Loss: 2.063\n",
      "Iteration: 351 \t--- Loss: 1.937\n",
      "Iteration: 352 \t--- Loss: 1.955\n",
      "Iteration: 353 \t--- Loss: 1.905\n",
      "Iteration: 354 \t--- Loss: 1.871\n",
      "Iteration: 355 \t--- Loss: 2.038\n",
      "Iteration: 356 \t--- Loss: 1.927\n",
      "Iteration: 357 \t--- Loss: 1.892\n",
      "Iteration: 358 \t--- Loss: 1.978\n",
      "Iteration: 359 \t--- Loss: 1.909\n",
      "Iteration: 360 \t--- Loss: 1.948\n",
      "Iteration: 361 \t--- Loss: 1.977\n",
      "Iteration: 362 \t--- Loss: 1.953\n",
      "Iteration: 363 \t--- Loss: 1.910\n",
      "Iteration: 364 \t--- Loss: 1.918\n",
      "Iteration: 365 \t--- Loss: 1.858\n",
      "Iteration: 366 \t--- Loss: 1.925\n",
      "Iteration: 367 \t--- Loss: 1.869\n",
      "Iteration: 368 \t--- Loss: 1.890\n",
      "Iteration: 369 \t--- Loss: 1.882\n",
      "Iteration: 370 \t--- Loss: 1.912\n",
      "Iteration: 371 \t--- Loss: 2.009\n",
      "Iteration: 372 \t--- Loss: 1.941\n",
      "Iteration: 373 \t--- Loss: 1.875\n",
      "Iteration: 374 \t--- Loss: 1.921\n",
      "Iteration: 375 \t--- Loss: 1.852\n",
      "Iteration: 376 \t--- Loss: 1.984\n",
      "Iteration: 377 \t--- Loss: 1.810\n",
      "Iteration: 378 \t--- Loss: 1.853\n",
      "Iteration: 379 \t--- Loss: 1.937\n",
      "Iteration: 380 \t--- Loss: 1.845\n",
      "Iteration: 381 \t--- Loss: 1.908\n",
      "Iteration: 382 \t--- Loss: 1.994\n",
      "Iteration: 383 \t--- Loss: 1.866\n",
      "Iteration: 384 \t--- Loss: 1.842\n",
      "Iteration: 385 \t--- Loss: 1.944\n",
      "Iteration: 386 \t--- Loss: 2.015\n",
      "Iteration: 387 \t--- Loss: 1.958\n",
      "Iteration: 388 \t--- Loss: 1.824\n",
      "Iteration: 389 \t--- Loss: 1.895\n",
      "Iteration: 390 \t--- Loss: 1.975\n",
      "Iteration: 391 \t--- Loss: 1.797\n",
      "Iteration: 392 \t--- Loss: 1.834\n",
      "Iteration: 393 \t--- Loss: 1.851\n",
      "Iteration: 394 \t--- Loss: 1.860\n",
      "Iteration: 395 \t--- Loss: 1.902\n",
      "Iteration: 396 \t--- Loss: 1.877\n",
      "Iteration: 397 \t--- Loss: 1.900\n",
      "Iteration: 398 \t--- Loss: 1.959\n",
      "Iteration: 399 \t--- Loss: 1.944\n",
      "Iteration: 400 \t--- Loss: 1.904\n",
      "Iteration: 401 \t--- Loss: 1.933\n",
      "Iteration: 402 \t--- Loss: 1.887\n",
      "Iteration: 403 \t--- Loss: 1.929\n",
      "Iteration: 404 \t--- Loss: 1.952\n",
      "Iteration: 405 \t--- Loss: 1.869\n",
      "Iteration: 406 \t--- Loss: 1.823\n",
      "Iteration: 407 \t--- Loss: 1.899\n",
      "Iteration: 408 \t--- Loss: 1.911\n",
      "Iteration: 409 \t--- Loss: 1.915\n",
      "Iteration: 410 \t--- Loss: 1.935\n",
      "Iteration: 411 \t--- Loss: 1.892\n",
      "Iteration: 412 \t--- Loss: 1.873\n",
      "Iteration: 413 \t--- Loss: 1.863\n",
      "Iteration: 414 \t--- Loss: 1.830\n",
      "Iteration: 415 \t--- Loss: 1.906\n",
      "Iteration: 416 \t--- Loss: 1.917\n",
      "Iteration: 417 \t--- Loss: 1.896\n",
      "Iteration: 418 \t--- Loss: 1.789\n",
      "Iteration: 419 \t--- Loss: 1.959\n",
      "Iteration: 420 \t--- Loss: 1.947\n",
      "Iteration: 421 \t--- Loss: 1.967\n",
      "Iteration: 422 \t--- Loss: 1.862\n",
      "Iteration: 423 \t--- Loss: 1.904\n",
      "Iteration: 424 \t--- Loss: 1.799\n",
      "Iteration: 425 \t--- Loss: 1.890\n",
      "Iteration: 426 \t--- Loss: 1.859\n",
      "Iteration: 427 \t--- Loss: 1.808\n",
      "Iteration: 428 \t--- Loss: 1.796\n",
      "Iteration: 429 \t--- Loss: 1.913\n",
      "Iteration: 430 \t--- Loss: 1.886\n",
      "Iteration: 431 \t--- Loss: 1.914\n",
      "Iteration: 432 \t--- Loss: 1.940\n",
      "Iteration: 433 \t--- Loss: 1.848\n",
      "Iteration: 434 \t--- Loss: 1.952\n",
      "Iteration: 435 \t--- Loss: 1.880\n",
      "Iteration: 436 \t--- Loss: 1.977\n",
      "Iteration: 437 \t--- Loss: 1.848\n",
      "Iteration: 438 \t--- Loss: 1.818\n",
      "Iteration: 439 \t--- Loss: 1.951\n",
      "Iteration: 440 \t--- Loss: 1.913\n",
      "Iteration: 441 \t--- Loss: 1.942\n",
      "Iteration: 442 \t--- Loss: 1.826\n",
      "Iteration: 443 \t--- Loss: 1.910\n",
      "Iteration: 444 \t--- Loss: 1.950\n",
      "Iteration: 445 \t--- Loss: 1.885\n",
      "Iteration: 446 \t--- Loss: 1.944\n",
      "Iteration: 447 \t--- Loss: 1.939\n",
      "Iteration: 448 \t--- Loss: 1.909\n",
      "Iteration: 449 \t--- Loss: 1.887\n",
      "Iteration: 450 \t--- Loss: 1.810\n",
      "Iteration: 451 \t--- Loss: 1.811\n",
      "Iteration: 452 \t--- Loss: 1.889\n",
      "Iteration: 453 \t--- Loss: 1.800\n",
      "Iteration: 454 \t--- Loss: 1.867\n",
      "Iteration: 455 \t--- Loss: 1.819\n",
      "Iteration: 456 \t--- Loss: 1.920\n",
      "Iteration: 457 \t--- Loss: 1.985\n",
      "Iteration: 458 \t--- Loss: 1.954\n",
      "Iteration: 459 \t--- Loss: 1.867\n",
      "Iteration: 460 \t--- Loss: 1.844\n",
      "Iteration: 461 \t--- Loss: 1.885\n",
      "Iteration: 462 \t--- Loss: 1.950\n",
      "Iteration: 463 \t--- Loss: 1.888\n",
      "Iteration: 464 \t--- Loss: 1.819\n",
      "Iteration: 465 \t--- Loss: 1.919\n",
      "Iteration: 466 \t--- Loss: 1.957\n",
      "Iteration: 467 \t--- Loss: 1.884\n",
      "Iteration: 468 \t--- Loss: 1.938\n",
      "Iteration: 469 \t--- Loss: 1.908\n",
      "Iteration: 470 \t--- Loss: 1.872\n",
      "Iteration: 471 \t--- Loss: 1.959\n",
      "Iteration: 472 \t--- Loss: 1.852\n",
      "Iteration: 473 \t--- Loss: 1.831\n",
      "Iteration: 474 \t--- Loss: 1.918\n",
      "Iteration: 475 \t--- Loss: 1.850\n",
      "Iteration: 476 \t--- Loss: 1.938\n",
      "Iteration: 477 \t--- Loss: 1.841\n",
      "Iteration: 478 \t--- Loss: 1.817\n",
      "Iteration: 479 \t--- Loss: 1.855\n",
      "Iteration: 480 \t--- Loss: 1.880\n",
      "Iteration: 481 \t--- Loss: 1.904\n",
      "Iteration: 482 \t--- Loss: 1.888\n",
      "Iteration: 483 \t--- Loss: 1.876\n",
      "Iteration: 484 \t--- Loss: 1.881\n",
      "Iteration: 485 \t--- Loss: 1.863\n",
      "Iteration: 486 \t--- Loss: 1.886\n",
      "Iteration: 487 \t--- Loss: 1.874\n",
      "Iteration: 488 \t--- Loss: 1.917\n",
      "Iteration: 489 \t--- Loss: 1.964\n",
      "Iteration: 490 \t--- Loss: 1.965\n",
      "Iteration: 491 \t--- Loss: 1.852\n",
      "Iteration: 492 \t--- Loss: 1.782\n",
      "Iteration: 493 \t--- Loss: 1.912\n",
      "Iteration: 494 \t--- Loss: 1.980\n",
      "Iteration: 495 \t--- Loss: 1.903\n",
      "Iteration: 496 \t--- Loss: 1.878\n",
      "Iteration: 497 \t--- Loss: 1.879\n",
      "Iteration: 498 \t--- Loss: 1.821\n",
      "Iteration: 499 \t--- Loss: 1.866\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:24<00:00, 84.09s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.056\n",
      "Iteration: 261 \t--- Loss: 0.053\n",
      "Iteration: 262 \t--- Loss: 0.057\n",
      "Iteration: 263 \t--- Loss: 0.059\n",
      "Iteration: 264 \t--- Loss: 0.056\n",
      "Iteration: 265 \t--- Loss: 0.060\n",
      "Iteration: 266 \t--- Loss: 0.059\n",
      "Iteration: 267 \t--- Loss: 0.053\n",
      "Iteration: 268 \t--- Loss: 0.055\n",
      "Iteration: 269 \t--- Loss: 0.056\n",
      "Iteration: 270 \t--- Loss: 0.055\n",
      "Iteration: 271 \t--- Loss: 0.053\n",
      "Iteration: 272 \t--- Loss: 0.058\n",
      "Iteration: 273 \t--- Loss: 0.054\n",
      "Iteration: 274 \t--- Loss: 0.052\n",
      "Iteration: 275 \t--- Loss: 0.054\n",
      "Iteration: 276 \t--- Loss: 0.057\n",
      "Iteration: 277 \t--- Loss: 0.056\n",
      "Iteration: 278 \t--- Loss: 0.056\n",
      "Iteration: 279 \t--- Loss: 0.054\n",
      "Iteration: 280 \t--- Loss: 0.052\n",
      "Iteration: 281 \t--- Loss: 0.054\n",
      "Iteration: 282 \t--- Loss: 0.058\n",
      "Iteration: 283 \t--- Loss: 0.056\n",
      "Iteration: 284 \t--- Loss: 0.057\n",
      "Iteration: 285 \t--- Loss: 0.049\n",
      "Iteration: 286 \t--- Loss: 0.054\n",
      "Iteration: 287 \t--- Loss: 0.060\n",
      "Iteration: 288 \t--- Loss: 0.051\n",
      "Iteration: 289 \t--- Loss: 0.057\n",
      "Iteration: 290 \t--- Loss: 0.054\n",
      "Iteration: 291 \t--- Loss: 0.052\n",
      "Iteration: 292 \t--- Loss: 0.052\n",
      "Iteration: 293 \t--- Loss: 0.050\n",
      "Iteration: 294 \t--- Loss: 0.056\n",
      "Iteration: 295 \t--- Loss: 0.049\n",
      "Iteration: 296 \t--- Loss: 0.057\n",
      "Iteration: 297 \t--- Loss: 0.057\n",
      "Iteration: 298 \t--- Loss: 0.059\n",
      "Iteration: 299 \t--- Loss: 0.062\n",
      "Iteration: 300 \t--- Loss: 0.055\n",
      "Iteration: 301 \t--- Loss: 0.051\n",
      "Iteration: 302 \t--- Loss: 0.052\n",
      "Iteration: 303 \t--- Loss: 0.056\n",
      "Iteration: 304 \t--- Loss: 0.051\n",
      "Iteration: 305 \t--- Loss: 0.055\n",
      "Iteration: 306 \t--- Loss: 0.058\n",
      "Iteration: 307 \t--- Loss: 0.051\n",
      "Iteration: 308 \t--- Loss: 0.064\n",
      "Iteration: 309 \t--- Loss: 0.058\n",
      "Iteration: 310 \t--- Loss: 0.053\n",
      "Iteration: 311 \t--- Loss: 0.057\n",
      "Iteration: 312 \t--- Loss: 0.049\n",
      "Iteration: 313 \t--- Loss: 0.057\n",
      "Iteration: 314 \t--- Loss: 0.053\n",
      "Iteration: 315 \t--- Loss: 0.053\n",
      "Iteration: 316 \t--- Loss: 0.052\n",
      "Iteration: 317 \t--- Loss: 0.050\n",
      "Iteration: 318 \t--- Loss: 0.058\n",
      "Iteration: 319 \t--- Loss: 0.062\n",
      "Iteration: 320 \t--- Loss: 0.054\n",
      "Iteration: 321 \t--- Loss: 0.053\n",
      "Iteration: 322 \t--- Loss: 0.050\n",
      "Iteration: 323 \t--- Loss: 0.056\n",
      "Iteration: 324 \t--- Loss: 0.053\n",
      "Iteration: 325 \t--- Loss: 0.054\n",
      "Iteration: 326 \t--- Loss: 0.056\n",
      "Iteration: 327 \t--- Loss: 0.052\n",
      "Iteration: 328 \t--- Loss: 0.060\n",
      "Iteration: 329 \t--- Loss: 0.051\n",
      "Iteration: 330 \t--- Loss: 0.056\n",
      "Iteration: 331 \t--- Loss: 0.061\n",
      "Iteration: 332 \t--- Loss: 0.056\n",
      "Iteration: 333 \t--- Loss: 0.050\n",
      "Iteration: 334 \t--- Loss: 0.056\n",
      "Iteration: 335 \t--- Loss: 0.055\n",
      "Iteration: 336 \t--- Loss: 0.057\n",
      "Iteration: 337 \t--- Loss: 0.051\n",
      "Iteration: 338 \t--- Loss: 0.053\n",
      "Iteration: 339 \t--- Loss: 0.060\n",
      "Iteration: 340 \t--- Loss: 0.053\n",
      "Iteration: 341 \t--- Loss: 0.058\n",
      "Iteration: 342 \t--- Loss: 0.058\n",
      "Iteration: 343 \t--- Loss: 0.050\n",
      "Iteration: 344 \t--- Loss: 0.053\n",
      "Iteration: 345 \t--- Loss: 0.051\n",
      "Iteration: 346 \t--- Loss: 0.058\n",
      "Iteration: 347 \t--- Loss: 0.060\n",
      "Iteration: 348 \t--- Loss: 0.051\n",
      "Iteration: 349 \t--- Loss: 0.054\n",
      "Iteration: 350 \t--- Loss: 0.060\n",
      "Iteration: 351 \t--- Loss: 0.054\n",
      "Iteration: 352 \t--- Loss: 0.054\n",
      "Iteration: 353 \t--- Loss: 0.053\n",
      "Iteration: 354 \t--- Loss: 0.059\n",
      "Iteration: 355 \t--- Loss: 0.052\n",
      "Iteration: 356 \t--- Loss: 0.053\n",
      "Iteration: 357 \t--- Loss: 0.051\n",
      "Iteration: 358 \t--- Loss: 0.054\n",
      "Iteration: 359 \t--- Loss: 0.055\n",
      "Iteration: 360 \t--- Loss: 0.056\n",
      "Iteration: 361 \t--- Loss: 0.054\n",
      "Iteration: 362 \t--- Loss: 0.054\n",
      "Iteration: 363 \t--- Loss: 0.053\n",
      "Iteration: 364 \t--- Loss: 0.057\n",
      "Iteration: 365 \t--- Loss: 0.059\n",
      "Iteration: 366 \t--- Loss: 0.052\n",
      "Iteration: 367 \t--- Loss: 0.050\n",
      "Iteration: 368 \t--- Loss: 0.059\n",
      "Iteration: 369 \t--- Loss: 0.054\n",
      "Iteration: 370 \t--- Loss: 0.051\n",
      "Iteration: 371 \t--- Loss: 0.056\n",
      "Iteration: 372 \t--- Loss: 0.056\n",
      "Iteration: 373 \t--- Loss: 0.059\n",
      "Iteration: 374 \t--- Loss: 0.056\n",
      "Iteration: 375 \t--- Loss: 0.051\n",
      "Iteration: 376 \t--- Loss: 0.051\n",
      "Iteration: 377 \t--- Loss: 0.050\n",
      "Iteration: 378 \t--- Loss: 0.056\n",
      "Iteration: 379 \t--- Loss: 0.056\n",
      "Iteration: 380 \t--- Loss: 0.052\n",
      "Iteration: 381 \t--- Loss: 0.051\n",
      "Iteration: 382 \t--- Loss: 0.054\n",
      "Iteration: 383 \t--- Loss: 0.054\n",
      "Iteration: 384 \t--- Loss: 0.054\n",
      "Iteration: 385 \t--- Loss: 0.056\n",
      "Iteration: 386 \t--- Loss: 0.056\n",
      "Iteration: 387 \t--- Loss: 0.054\n",
      "Iteration: 388 \t--- Loss: 0.053\n",
      "Iteration: 389 \t--- Loss: 0.063\n",
      "Iteration: 390 \t--- Loss: 0.052\n",
      "Iteration: 391 \t--- Loss: 0.058\n",
      "Iteration: 392 \t--- Loss: 0.055\n",
      "Iteration: 393 \t--- Loss: 0.053\n",
      "Iteration: 394 \t--- Loss: 0.053\n",
      "Iteration: 395 \t--- Loss: 0.053\n",
      "Iteration: 396 \t--- Loss: 0.054\n",
      "Iteration: 397 \t--- Loss: 0.053\n",
      "Iteration: 398 \t--- Loss: 0.058\n",
      "Iteration: 399 \t--- Loss: 0.057\n",
      "Iteration: 400 \t--- Loss: 0.057\n",
      "Iteration: 401 \t--- Loss: 0.058\n",
      "Iteration: 402 \t--- Loss: 0.052\n",
      "Iteration: 403 \t--- Loss: 0.053\n",
      "Iteration: 404 \t--- Loss: 0.053\n",
      "Iteration: 405 \t--- Loss: 0.055\n",
      "Iteration: 406 \t--- Loss: 0.058\n",
      "Iteration: 407 \t--- Loss: 0.057\n",
      "Iteration: 408 \t--- Loss: 0.047\n",
      "Iteration: 409 \t--- Loss: 0.053\n",
      "Iteration: 410 \t--- Loss: 0.057\n",
      "Iteration: 411 \t--- Loss: 0.055\n",
      "Iteration: 412 \t--- Loss: 0.051\n",
      "Iteration: 413 \t--- Loss: 0.047\n",
      "Iteration: 414 \t--- Loss: 0.051\n",
      "Iteration: 415 \t--- Loss: 0.055\n",
      "Iteration: 416 \t--- Loss: 0.054\n",
      "Iteration: 417 \t--- Loss: 0.061\n",
      "Iteration: 418 \t--- Loss: 0.056\n",
      "Iteration: 419 \t--- Loss: 0.054\n",
      "Iteration: 420 \t--- Loss: 0.054\n",
      "Iteration: 421 \t--- Loss: 0.049\n",
      "Iteration: 422 \t--- Loss: 0.055\n",
      "Iteration: 423 \t--- Loss: 0.054\n",
      "Iteration: 424 \t--- Loss: 0.054\n",
      "Iteration: 425 \t--- Loss: 0.058\n",
      "Iteration: 426 \t--- Loss: 0.054\n",
      "Iteration: 427 \t--- Loss: 0.054\n",
      "Iteration: 428 \t--- Loss: 0.048\n",
      "Iteration: 429 \t--- Loss: 0.053\n",
      "Iteration: 430 \t--- Loss: 0.050\n",
      "Iteration: 431 \t--- Loss: 0.050\n",
      "Iteration: 432 \t--- Loss: 0.054\n",
      "Iteration: 433 \t--- Loss: 0.056\n",
      "Iteration: 434 \t--- Loss: 0.051\n",
      "Iteration: 435 \t--- Loss: 0.053\n",
      "Iteration: 436 \t--- Loss: 0.058\n",
      "Iteration: 437 \t--- Loss: 0.054\n",
      "Iteration: 438 \t--- Loss: 0.054\n",
      "Iteration: 439 \t--- Loss: 0.053\n",
      "Iteration: 440 \t--- Loss: 0.057\n",
      "Iteration: 441 \t--- Loss: 0.055\n",
      "Iteration: 442 \t--- Loss: 0.051\n",
      "Iteration: 443 \t--- Loss: 0.059\n",
      "Iteration: 444 \t--- Loss: 0.058\n",
      "Iteration: 445 \t--- Loss: 0.056\n",
      "Iteration: 446 \t--- Loss: 0.056\n",
      "Iteration: 447 \t--- Loss: 0.055\n",
      "Iteration: 448 \t--- Loss: 0.052\n",
      "Iteration: 449 \t--- Loss: 0.055\n",
      "Iteration: 450 \t--- Loss: 0.058\n",
      "Iteration: 451 \t--- Loss: 0.059\n",
      "Iteration: 452 \t--- Loss: 0.053\n",
      "Iteration: 453 \t--- Loss: 0.056\n",
      "Iteration: 454 \t--- Loss: 0.051\n",
      "Iteration: 455 \t--- Loss: 0.057\n",
      "Iteration: 456 \t--- Loss: 0.053\n",
      "Iteration: 457 \t--- Loss: 0.056\n",
      "Iteration: 458 \t--- Loss: 0.047\n",
      "Iteration: 459 \t--- Loss: 0.050\n",
      "Iteration: 460 \t--- Loss: 0.053\n",
      "Iteration: 461 \t--- Loss: 0.054\n",
      "Iteration: 462 \t--- Loss: 0.054\n",
      "Iteration: 463 \t--- Loss: 0.054\n",
      "Iteration: 464 \t--- Loss: 0.053\n",
      "Iteration: 465 \t--- Loss: 0.058\n",
      "Iteration: 466 \t--- Loss: 0.049\n",
      "Iteration: 467 \t--- Loss: 0.055\n",
      "Iteration: 468 \t--- Loss: 0.060\n",
      "Iteration: 469 \t--- Loss: 0.053\n",
      "Iteration: 470 \t--- Loss: 0.057\n",
      "Iteration: 471 \t--- Loss: 0.052\n",
      "Iteration: 472 \t--- Loss: 0.054\n",
      "Iteration: 473 \t--- Loss: 0.062\n",
      "Iteration: 474 \t--- Loss: 0.054\n",
      "Iteration: 475 \t--- Loss: 0.058\n",
      "Iteration: 476 \t--- Loss: 0.052\n",
      "Iteration: 477 \t--- Loss: 0.053\n",
      "Iteration: 478 \t--- Loss: 0.054\n",
      "Iteration: 479 \t--- Loss: 0.049\n",
      "Iteration: 480 \t--- Loss: 0.054\n",
      "Iteration: 481 \t--- Loss: 0.051\n",
      "Iteration: 482 \t--- Loss: 0.050\n",
      "Iteration: 483 \t--- Loss: 0.052\n",
      "Iteration: 484 \t--- Loss: 0.057\n",
      "Iteration: 485 \t--- Loss: 0.053\n",
      "Iteration: 486 \t--- Loss: 0.056\n",
      "Iteration: 487 \t--- Loss: 0.063\n",
      "Iteration: 488 \t--- Loss: 0.058\n",
      "Iteration: 489 \t--- Loss: 0.057\n",
      "Iteration: 490 \t--- Loss: 0.058\n",
      "Iteration: 491 \t--- Loss: 0.058\n",
      "Iteration: 492 \t--- Loss: 0.057\n",
      "Iteration: 493 \t--- Loss: 0.050\n",
      "Iteration: 494 \t--- Loss: 0.061\n",
      "Iteration: 495 \t--- Loss: 0.050\n",
      "Iteration: 496 \t--- Loss: 0.057\n",
      "Iteration: 497 \t--- Loss: 0.055\n",
      "Iteration: 498 \t--- Loss: 0.052\n",
      "Iteration: 499 \t--- Loss: 0.054\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:09,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.829\n",
      "Iteration: 1 \t--- Loss: 0.731\n",
      "Iteration: 2 \t--- Loss: 0.724\n",
      "Iteration: 3 \t--- Loss: 0.648\n",
      "Iteration: 4 \t--- Loss: 0.599\n",
      "Iteration: 5 \t--- Loss: 0.567\n",
      "Iteration: 6 \t--- Loss: 0.540\n",
      "Iteration: 7 \t--- Loss: 0.495\n",
      "Iteration: 8 \t--- Loss: 0.490\n",
      "Iteration: 9 \t--- Loss: 0.487\n",
      "Iteration: 10 \t--- Loss: 0.483\n",
      "Iteration: 11 \t--- Loss: 0.481\n",
      "Iteration: 12 \t--- Loss: 0.464\n",
      "Iteration: 13 \t--- Loss: 0.444\n",
      "Iteration: 14 \t--- Loss: 0.432\n",
      "Iteration: 15 \t--- Loss: 0.424\n",
      "Iteration: 16 \t--- Loss: 0.439\n",
      "Iteration: 17 \t--- Loss: 0.430\n",
      "Iteration: 18 \t--- Loss: 0.435\n",
      "Iteration: 19 \t--- Loss: 0.426\n",
      "Iteration: 20 \t--- Loss: 0.435\n",
      "Iteration: 21 \t--- Loss: 0.425\n",
      "Iteration: 22 \t--- Loss: 0.418\n",
      "Iteration: 23 \t--- Loss: 0.425\n",
      "Iteration: 24 \t--- Loss: 0.420\n",
      "Iteration: 25 \t--- Loss: 0.411\n",
      "Iteration: 26 \t--- Loss: 0.419\n",
      "Iteration: 27 \t--- Loss: 0.413\n",
      "Iteration: 28 \t--- Loss: 0.428\n",
      "Iteration: 29 \t--- Loss: 0.407\n",
      "Iteration: 30 \t--- Loss: 0.415\n",
      "Iteration: 31 \t--- Loss: 0.412\n",
      "Iteration: 32 \t--- Loss: 0.417\n",
      "Iteration: 33 \t--- Loss: 0.412\n",
      "Iteration: 34 \t--- Loss: 0.415\n",
      "Iteration: 35 \t--- Loss: 0.407\n",
      "Iteration: 36 \t--- Loss: 0.420\n",
      "Iteration: 37 \t--- Loss: 0.400\n",
      "Iteration: 38 \t--- Loss: 0.417\n",
      "Iteration: 39 \t--- Loss: 0.409\n",
      "Iteration: 40 \t--- Loss: 0.411\n",
      "Iteration: 41 \t--- Loss: 0.425\n",
      "Iteration: 42 \t--- Loss: 0.423\n",
      "Iteration: 43 \t--- Loss: 0.405\n",
      "Iteration: 44 \t--- Loss: 0.410\n",
      "Iteration: 45 \t--- Loss: 0.422\n",
      "Iteration: 46 \t--- Loss: 0.411\n",
      "Iteration: 47 \t--- Loss: 0.415\n",
      "Iteration: 48 \t--- Loss: 0.415\n",
      "Iteration: 49 \t--- Loss: 0.404\n",
      "Iteration: 50 \t--- Loss: 0.411\n",
      "Iteration: 51 \t--- Loss: 0.415\n",
      "Iteration: 52 \t--- Loss: 0.414\n",
      "Iteration: 53 \t--- Loss: 0.405\n",
      "Iteration: 54 \t--- Loss: 0.404\n",
      "Iteration: 55 \t--- Loss: 0.413\n",
      "Iteration: 56 \t--- Loss: 0.414\n",
      "Iteration: 57 \t--- Loss: 0.404\n",
      "Iteration: 58 \t--- Loss: 0.413\n",
      "Iteration: 59 \t--- Loss: 0.397\n",
      "Iteration: 60 \t--- Loss: 0.412\n",
      "Iteration: 61 \t--- Loss: 0.412\n",
      "Iteration: 62 \t--- Loss: 0.402\n",
      "Iteration: 63 \t--- Loss: 0.414\n",
      "Iteration: 64 \t--- Loss: 0.412\n",
      "Iteration: 65 \t--- Loss: 0.411\n",
      "Iteration: 66 \t--- Loss: 0.403\n",
      "Iteration: 67 \t--- Loss: 0.413\n",
      "Iteration: 68 \t--- Loss: 0.404\n",
      "Iteration: 69 \t--- Loss: 0.403\n",
      "Iteration: 70 \t--- Loss: 0.409\n",
      "Iteration: 71 \t--- Loss: 0.408\n",
      "Iteration: 72 \t--- Loss: 0.401\n",
      "Iteration: 73 \t--- Loss: 0.406\n",
      "Iteration: 74 \t--- Loss: 0.396\n",
      "Iteration: 75 \t--- Loss: 0.401\n",
      "Iteration: 76 \t--- Loss: 0.396\n",
      "Iteration: 77 \t--- Loss: 0.408\n",
      "Iteration: 78 \t--- Loss: 0.416\n",
      "Iteration: 79 \t--- Loss: 0.408\n",
      "Iteration: 80 \t--- Loss: 0.407\n",
      "Iteration: 81 \t--- Loss: 0.403\n",
      "Iteration: 82 \t--- Loss: 0.420\n",
      "Iteration: 83 \t--- Loss: 0.413\n",
      "Iteration: 84 \t--- Loss: 0.404\n",
      "Iteration: 85 \t--- Loss: 0.396\n",
      "Iteration: 86 \t--- Loss: 0.406\n",
      "Iteration: 87 \t--- Loss: 0.411\n",
      "Iteration: 88 \t--- Loss: 0.398\n",
      "Iteration: 89 \t--- Loss: 0.414\n",
      "Iteration: 90 \t--- Loss: 0.421\n",
      "Iteration: 91 \t--- Loss: 0.402\n",
      "Iteration: 92 \t--- Loss: 0.412\n",
      "Iteration: 93 \t--- Loss: 0.406\n",
      "Iteration: 94 \t--- Loss: 0.403\n",
      "Iteration: 95 \t--- Loss: 0.406\n",
      "Iteration: 96 \t--- Loss: 0.414\n",
      "Iteration: 97 \t--- Loss: 0.408\n",
      "Iteration: 98 \t--- Loss: 0.402\n",
      "Iteration: 99 \t--- Loss: 0.412\n",
      "Iteration: 100 \t--- Loss: 0.403\n",
      "Iteration: 101 \t--- Loss: 0.413\n",
      "Iteration: 102 \t--- Loss: 0.411\n",
      "Iteration: 103 \t--- Loss: 0.412\n",
      "Iteration: 104 \t--- Loss: 0.393\n",
      "Iteration: 105 \t--- Loss: 0.403\n",
      "Iteration: 106 \t--- Loss: 0.407\n",
      "Iteration: 107 \t--- Loss: 0.412\n",
      "Iteration: 108 \t--- Loss: 0.409\n",
      "Iteration: 109 \t--- Loss: 0.413\n",
      "Iteration: 110 \t--- Loss: 0.406\n",
      "Iteration: 111 \t--- Loss: 0.413\n",
      "Iteration: 112 \t--- Loss: 0.413\n",
      "Iteration: 113 \t--- Loss: 0.420\n",
      "Iteration: 114 \t--- Loss: 0.403\n",
      "Iteration: 115 \t--- Loss: 0.406\n",
      "Iteration: 116 \t--- Loss: 0.417\n",
      "Iteration: 117 \t--- Loss: 0.408\n",
      "Iteration: 118 \t--- Loss: 0.419\n",
      "Iteration: 119 \t--- Loss: 0.418\n",
      "Iteration: 120 \t--- Loss: 0.415\n",
      "Iteration: 121 \t--- Loss: 0.410\n",
      "Iteration: 122 \t--- Loss: 0.404\n",
      "Iteration: 123 \t--- Loss: 0.415\n",
      "Iteration: 124 \t--- Loss: 0.410\n",
      "Iteration: 125 \t--- Loss: 0.398\n",
      "Iteration: 126 \t--- Loss: 0.412\n",
      "Iteration: 127 \t--- Loss: 0.406\n",
      "Iteration: 128 \t--- Loss: 0.410\n",
      "Iteration: 129 \t--- Loss: 0.407\n",
      "Iteration: 130 \t--- Loss: 0.409\n",
      "Iteration: 131 \t--- Loss: 0.412\n",
      "Iteration: 132 \t--- Loss: 0.419\n",
      "Iteration: 133 \t--- Loss: 0.397\n",
      "Iteration: 134 \t--- Loss: 0.398\n",
      "Iteration: 135 \t--- Loss: 0.411\n",
      "Iteration: 136 \t--- Loss: 0.405\n",
      "Iteration: 137 \t--- Loss: 0.409\n",
      "Iteration: 138 \t--- Loss: 0.406\n",
      "Iteration: 139 \t--- Loss: 0.401\n",
      "Iteration: 140 \t--- Loss: 0.409\n",
      "Iteration: 141 \t--- Loss: 0.404\n",
      "Iteration: 142 \t--- Loss: 0.412\n",
      "Iteration: 143 \t--- Loss: 0.405\n",
      "Iteration: 144 \t--- Loss: 0.407\n",
      "Iteration: 145 \t--- Loss: 0.414\n",
      "Iteration: 146 \t--- Loss: 0.405\n",
      "Iteration: 147 \t--- Loss: 0.397\n",
      "Iteration: 148 \t--- Loss: 0.415\n",
      "Iteration: 149 \t--- Loss: 0.405\n",
      "Iteration: 150 \t--- Loss: 0.407\n",
      "Iteration: 151 \t--- Loss: 0.407\n",
      "Iteration: 152 \t--- Loss: 0.413\n",
      "Iteration: 153 \t--- Loss: 0.409\n",
      "Iteration: 154 \t--- Loss: 0.405\n",
      "Iteration: 155 \t--- Loss: 0.395\n",
      "Iteration: 156 \t--- Loss: 0.412\n",
      "Iteration: 157 \t--- Loss: 0.403\n",
      "Iteration: 158 \t--- Loss: 0.408\n",
      "Iteration: 159 \t--- Loss: 0.409\n",
      "Iteration: 160 \t--- Loss: 0.400\n",
      "Iteration: 161 \t--- Loss: 0.403\n",
      "Iteration: 162 \t--- Loss: 0.395\n",
      "Iteration: 163 \t--- Loss: 0.402\n",
      "Iteration: 164 \t--- Loss: 0.408\n",
      "Iteration: 165 \t--- Loss: 0.413\n",
      "Iteration: 166 \t--- Loss: 0.409\n",
      "Iteration: 167 \t--- Loss: 0.408\n",
      "Iteration: 168 \t--- Loss: 0.401\n",
      "Iteration: 169 \t--- Loss: 0.407\n",
      "Iteration: 170 \t--- Loss: 0.410\n",
      "Iteration: 171 \t--- Loss: 0.410\n",
      "Iteration: 172 \t--- Loss: 0.409\n",
      "Iteration: 173 \t--- Loss: 0.407\n",
      "Iteration: 174 \t--- Loss: 0.413\n",
      "Iteration: 175 \t--- Loss: 0.405\n",
      "Iteration: 176 \t--- Loss: 0.404\n",
      "Iteration: 177 \t--- Loss: 0.418\n",
      "Iteration: 178 \t--- Loss: 0.415\n",
      "Iteration: 179 \t--- Loss: 0.410\n",
      "Iteration: 180 \t--- Loss: 0.411\n",
      "Iteration: 181 \t--- Loss: 0.402\n",
      "Iteration: 182 \t--- Loss: 0.403\n",
      "Iteration: 183 \t--- Loss: 0.414\n",
      "Iteration: 184 \t--- Loss: 0.414\n",
      "Iteration: 185 \t--- Loss: 0.406\n",
      "Iteration: 186 \t--- Loss: 0.399\n",
      "Iteration: 187 \t--- Loss: 0.410\n",
      "Iteration: 188 \t--- Loss: 0.414\n",
      "Iteration: 189 \t--- Loss: 0.397\n",
      "Iteration: 190 \t--- Loss: 0.399\n",
      "Iteration: 191 \t--- Loss: 0.411\n",
      "Iteration: 192 \t--- Loss: 0.411\n",
      "Iteration: 193 \t--- Loss: 0.412\n",
      "Iteration: 194 \t--- Loss: 0.403\n",
      "Iteration: 195 \t--- Loss: 0.407\n",
      "Iteration: 196 \t--- Loss: 0.410\n",
      "Iteration: 197 \t--- Loss: 0.399\n",
      "Iteration: 198 \t--- Loss: 0.413\n",
      "Iteration: 199 \t--- Loss: 0.401\n",
      "Iteration: 200 \t--- Loss: 0.401\n",
      "Iteration: 201 \t--- Loss: 0.410\n",
      "Iteration: 202 \t--- Loss: 0.403\n",
      "Iteration: 203 \t--- Loss: 0.402\n",
      "Iteration: 204 \t--- Loss: 0.410\n",
      "Iteration: 205 \t--- Loss: 0.405\n",
      "Iteration: 206 \t--- Loss: 0.407\n",
      "Iteration: 207 \t--- Loss: 0.401\n",
      "Iteration: 208 \t--- Loss: 0.417\n",
      "Iteration: 209 \t--- Loss: 0.399\n",
      "Iteration: 210 \t--- Loss: 0.404\n",
      "Iteration: 211 \t--- Loss: 0.414\n",
      "Iteration: 212 \t--- Loss: 0.401\n",
      "Iteration: 213 \t--- Loss: 0.410\n",
      "Iteration: 214 \t--- Loss: 0.409\n",
      "Iteration: 215 \t--- Loss: 0.405\n",
      "Iteration: 216 \t--- Loss: 0.409\n",
      "Iteration: 217 \t--- Loss: 0.406\n",
      "Iteration: 218 \t--- Loss: 0.411\n",
      "Iteration: 219 \t--- Loss: 0.417\n",
      "Iteration: 220 \t--- Loss: 0.397\n",
      "Iteration: 221 \t--- Loss: 0.409\n",
      "Iteration: 222 \t--- Loss: 0.399\n",
      "Iteration: 223 \t--- Loss: 0.407\n",
      "Iteration: 224 \t--- Loss: 0.406\n",
      "Iteration: 225 \t--- Loss: 0.414\n",
      "Iteration: 226 \t--- Loss: 0.407\n",
      "Iteration: 227 \t--- Loss: 0.407\n",
      "Iteration: 228 \t--- Loss: 0.411\n",
      "Iteration: 229 \t--- Loss: 0.413\n",
      "Iteration: 230 \t--- Loss: 0.404\n",
      "Iteration: 231 \t--- Loss: 0.409\n",
      "Iteration: 232 \t--- Loss: 0.413\n",
      "Iteration: 233 \t--- Loss: 0.398\n",
      "Iteration: 234 \t--- Loss: 0.409\n",
      "Iteration: 235 \t--- Loss: 0.415\n",
      "Iteration: 236 \t--- Loss: 0.408\n",
      "Iteration: 237 \t--- Loss: 0.407\n",
      "Iteration: 238 \t--- Loss: 0.419\n",
      "Iteration: 239 \t--- Loss: 0.403\n",
      "Iteration: 240 \t--- Loss: 0.399\n",
      "Iteration: 241 \t--- Loss: 0.399\n",
      "Iteration: 242 \t--- Loss: 0.411\n",
      "Iteration: 243 \t--- Loss: 0.406\n",
      "Iteration: 244 \t--- Loss: 0.416\n",
      "Iteration: 245 \t--- Loss: 0.413\n",
      "Iteration: 246 \t--- Loss: 0.415\n",
      "Iteration: 247 \t--- Loss: 0.407\n",
      "Iteration: 248 \t--- Loss: 0.400\n",
      "Iteration: 249 \t--- Loss: 0.404\n",
      "Iteration: 250 \t--- Loss: 0.402\n",
      "Iteration: 251 \t--- Loss: 0.403\n",
      "Iteration: 252 \t--- Loss: 0.406\n",
      "Iteration: 253 \t--- Loss: 0.412\n",
      "Iteration: 254 \t--- Loss: 0.404\n",
      "Iteration: 255 \t--- Loss: 0.404\n",
      "Iteration: 256 \t--- Loss: 0.406\n",
      "Iteration: 257 \t--- Loss: 0.404\n",
      "Iteration: 258 \t--- Loss: 0.397\n",
      "Iteration: 259 \t--- Loss: 0.411"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.50s/it][Parallel(n_jobs=5)]: Done  79 tasks      | elapsed: 50.1min\n",
      " 90%|█████████ | 9/10 [00:13<00:01,  1.52s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it][Parallel(n_jobs=5)]: Done  80 tasks      | elapsed: 50.1min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:26<00:00, 86.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.396\n",
      "Iteration: 261 \t--- Loss: 0.399\n",
      "Iteration: 262 \t--- Loss: 0.402\n",
      "Iteration: 263 \t--- Loss: 0.411\n",
      "Iteration: 264 \t--- Loss: 0.398\n",
      "Iteration: 265 \t--- Loss: 0.406\n",
      "Iteration: 266 \t--- Loss: 0.401\n",
      "Iteration: 267 \t--- Loss: 0.408\n",
      "Iteration: 268 \t--- Loss: 0.415\n",
      "Iteration: 269 \t--- Loss: 0.413\n",
      "Iteration: 270 \t--- Loss: 0.409\n",
      "Iteration: 271 \t--- Loss: 0.414\n",
      "Iteration: 272 \t--- Loss: 0.396\n",
      "Iteration: 273 \t--- Loss: 0.401\n",
      "Iteration: 274 \t--- Loss: 0.406\n",
      "Iteration: 275 \t--- Loss: 0.405\n",
      "Iteration: 276 \t--- Loss: 0.412\n",
      "Iteration: 277 \t--- Loss: 0.408\n",
      "Iteration: 278 \t--- Loss: 0.410\n",
      "Iteration: 279 \t--- Loss: 0.410\n",
      "Iteration: 280 \t--- Loss: 0.410\n",
      "Iteration: 281 \t--- Loss: 0.406\n",
      "Iteration: 282 \t--- Loss: 0.406\n",
      "Iteration: 283 \t--- Loss: 0.419\n",
      "Iteration: 284 \t--- Loss: 0.398\n",
      "Iteration: 285 \t--- Loss: 0.424\n",
      "Iteration: 286 \t--- Loss: 0.408\n",
      "Iteration: 287 \t--- Loss: 0.420\n",
      "Iteration: 288 \t--- Loss: 0.405\n",
      "Iteration: 289 \t--- Loss: 0.410\n",
      "Iteration: 290 \t--- Loss: 0.401\n",
      "Iteration: 291 \t--- Loss: 0.406\n",
      "Iteration: 292 \t--- Loss: 0.414\n",
      "Iteration: 293 \t--- Loss: 0.407\n",
      "Iteration: 294 \t--- Loss: 0.410\n",
      "Iteration: 295 \t--- Loss: 0.405\n",
      "Iteration: 296 \t--- Loss: 0.408\n",
      "Iteration: 297 \t--- Loss: 0.410\n",
      "Iteration: 298 \t--- Loss: 0.411\n",
      "Iteration: 299 \t--- Loss: 0.395\n",
      "Iteration: 300 \t--- Loss: 0.396\n",
      "Iteration: 301 \t--- Loss: 0.402\n",
      "Iteration: 302 \t--- Loss: 0.402\n",
      "Iteration: 303 \t--- Loss: 0.408\n",
      "Iteration: 304 \t--- Loss: 0.409\n",
      "Iteration: 305 \t--- Loss: 0.411\n",
      "Iteration: 306 \t--- Loss: 0.406\n",
      "Iteration: 307 \t--- Loss: 0.407\n",
      "Iteration: 308 \t--- Loss: 0.408\n",
      "Iteration: 309 \t--- Loss: 0.408\n",
      "Iteration: 310 \t--- Loss: 0.409\n",
      "Iteration: 311 \t--- Loss: 0.412\n",
      "Iteration: 312 \t--- Loss: 0.400\n",
      "Iteration: 313 \t--- Loss: 0.411\n",
      "Iteration: 314 \t--- Loss: 0.399\n",
      "Iteration: 315 \t--- Loss: 0.408\n",
      "Iteration: 316 \t--- Loss: 0.409\n",
      "Iteration: 317 \t--- Loss: 0.397\n",
      "Iteration: 318 \t--- Loss: 0.414\n",
      "Iteration: 319 \t--- Loss: 0.399\n",
      "Iteration: 320 \t--- Loss: 0.414\n",
      "Iteration: 321 \t--- Loss: 0.408\n",
      "Iteration: 322 \t--- Loss: 0.408\n",
      "Iteration: 323 \t--- Loss: 0.412\n",
      "Iteration: 324 \t--- Loss: 0.415\n",
      "Iteration: 325 \t--- Loss: 0.406\n",
      "Iteration: 326 \t--- Loss: 0.408\n",
      "Iteration: 327 \t--- Loss: 0.412\n",
      "Iteration: 328 \t--- Loss: 0.402\n",
      "Iteration: 329 \t--- Loss: 0.402\n",
      "Iteration: 330 \t--- Loss: 0.417\n",
      "Iteration: 331 \t--- Loss: 0.402\n",
      "Iteration: 332 \t--- Loss: 0.410\n",
      "Iteration: 333 \t--- Loss: 0.413\n",
      "Iteration: 334 \t--- Loss: 0.416\n",
      "Iteration: 335 \t--- Loss: 0.411\n",
      "Iteration: 336 \t--- Loss: 0.400\n",
      "Iteration: 337 \t--- Loss: 0.403\n",
      "Iteration: 338 \t--- Loss: 0.414\n",
      "Iteration: 339 \t--- Loss: 0.406\n",
      "Iteration: 340 \t--- Loss: 0.399\n",
      "Iteration: 341 \t--- Loss: 0.408\n",
      "Iteration: 342 \t--- Loss: 0.404\n",
      "Iteration: 343 \t--- Loss: 0.415\n",
      "Iteration: 344 \t--- Loss: 0.405\n",
      "Iteration: 345 \t--- Loss: 0.413\n",
      "Iteration: 346 \t--- Loss: 0.405\n",
      "Iteration: 347 \t--- Loss: 0.406\n",
      "Iteration: 348 \t--- Loss: 0.410\n",
      "Iteration: 349 \t--- Loss: 0.407\n",
      "Iteration: 350 \t--- Loss: 0.398\n",
      "Iteration: 351 \t--- Loss: 0.419\n",
      "Iteration: 352 \t--- Loss: 0.413\n",
      "Iteration: 353 \t--- Loss: 0.418\n",
      "Iteration: 354 \t--- Loss: 0.406\n",
      "Iteration: 355 \t--- Loss: 0.402\n",
      "Iteration: 356 \t--- Loss: 0.405\n",
      "Iteration: 357 \t--- Loss: 0.402\n",
      "Iteration: 358 \t--- Loss: 0.403\n",
      "Iteration: 359 \t--- Loss: 0.413\n",
      "Iteration: 360 \t--- Loss: 0.411\n",
      "Iteration: 361 \t--- Loss: 0.422\n",
      "Iteration: 362 \t--- Loss: 0.410\n",
      "Iteration: 363 \t--- Loss: 0.407\n",
      "Iteration: 364 \t--- Loss: 0.407\n",
      "Iteration: 365 \t--- Loss: 0.406\n",
      "Iteration: 366 \t--- Loss: 0.407\n",
      "Iteration: 367 \t--- Loss: 0.410\n",
      "Iteration: 368 \t--- Loss: 0.407\n",
      "Iteration: 369 \t--- Loss: 0.404\n",
      "Iteration: 370 \t--- Loss: 0.405\n",
      "Iteration: 371 \t--- Loss: 0.397\n",
      "Iteration: 372 \t--- Loss: 0.408\n",
      "Iteration: 373 \t--- Loss: 0.408\n",
      "Iteration: 374 \t--- Loss: 0.408\n",
      "Iteration: 375 \t--- Loss: 0.406\n",
      "Iteration: 376 \t--- Loss: 0.403\n",
      "Iteration: 377 \t--- Loss: 0.408\n",
      "Iteration: 378 \t--- Loss: 0.411\n",
      "Iteration: 379 \t--- Loss: 0.402\n",
      "Iteration: 380 \t--- Loss: 0.413\n",
      "Iteration: 381 \t--- Loss: 0.397\n",
      "Iteration: 382 \t--- Loss: 0.399\n",
      "Iteration: 383 \t--- Loss: 0.411\n",
      "Iteration: 384 \t--- Loss: 0.416\n",
      "Iteration: 385 \t--- Loss: 0.407\n",
      "Iteration: 386 \t--- Loss: 0.403\n",
      "Iteration: 387 \t--- Loss: 0.408\n",
      "Iteration: 388 \t--- Loss: 0.412\n",
      "Iteration: 389 \t--- Loss: 0.403\n",
      "Iteration: 390 \t--- Loss: 0.411\n",
      "Iteration: 391 \t--- Loss: 0.408\n",
      "Iteration: 392 \t--- Loss: 0.405\n",
      "Iteration: 393 \t--- Loss: 0.400\n",
      "Iteration: 394 \t--- Loss: 0.402\n",
      "Iteration: 395 \t--- Loss: 0.406\n",
      "Iteration: 396 \t--- Loss: 0.408\n",
      "Iteration: 397 \t--- Loss: 0.406\n",
      "Iteration: 398 \t--- Loss: 0.410\n",
      "Iteration: 399 \t--- Loss: 0.412\n",
      "Iteration: 400 \t--- Loss: 0.415\n",
      "Iteration: 401 \t--- Loss: 0.408\n",
      "Iteration: 402 \t--- Loss: 0.407\n",
      "Iteration: 403 \t--- Loss: 0.405\n",
      "Iteration: 404 \t--- Loss: 0.419\n",
      "Iteration: 405 \t--- Loss: 0.404\n",
      "Iteration: 406 \t--- Loss: 0.411\n",
      "Iteration: 407 \t--- Loss: 0.403\n",
      "Iteration: 408 \t--- Loss: 0.404\n",
      "Iteration: 409 \t--- Loss: 0.405\n",
      "Iteration: 410 \t--- Loss: 0.412\n",
      "Iteration: 411 \t--- Loss: 0.413\n",
      "Iteration: 412 \t--- Loss: 0.411\n",
      "Iteration: 413 \t--- Loss: 0.408\n",
      "Iteration: 414 \t--- Loss: 0.402\n",
      "Iteration: 415 \t--- Loss: 0.413\n",
      "Iteration: 416 \t--- Loss: 0.416\n",
      "Iteration: 417 \t--- Loss: 0.408\n",
      "Iteration: 418 \t--- Loss: 0.401\n",
      "Iteration: 419 \t--- Loss: 0.409\n",
      "Iteration: 420 \t--- Loss: 0.405\n",
      "Iteration: 421 \t--- Loss: 0.411\n",
      "Iteration: 422 \t--- Loss: 0.403\n",
      "Iteration: 423 \t--- Loss: 0.413\n",
      "Iteration: 424 \t--- Loss: 0.411\n",
      "Iteration: 425 \t--- Loss: 0.408\n",
      "Iteration: 426 \t--- Loss: 0.401\n",
      "Iteration: 427 \t--- Loss: 0.413\n",
      "Iteration: 428 \t--- Loss: 0.407\n",
      "Iteration: 429 \t--- Loss: 0.404\n",
      "Iteration: 430 \t--- Loss: 0.411\n",
      "Iteration: 431 \t--- Loss: 0.405\n",
      "Iteration: 432 \t--- Loss: 0.414\n",
      "Iteration: 433 \t--- Loss: 0.406\n",
      "Iteration: 434 \t--- Loss: 0.409\n",
      "Iteration: 435 \t--- Loss: 0.403\n",
      "Iteration: 436 \t--- Loss: 0.410\n",
      "Iteration: 437 \t--- Loss: 0.408\n",
      "Iteration: 438 \t--- Loss: 0.404\n",
      "Iteration: 439 \t--- Loss: 0.409\n",
      "Iteration: 440 \t--- Loss: 0.399\n",
      "Iteration: 441 \t--- Loss: 0.411\n",
      "Iteration: 442 \t--- Loss: 0.408\n",
      "Iteration: 443 \t--- Loss: 0.409\n",
      "Iteration: 444 \t--- Loss: 0.403\n",
      "Iteration: 445 \t--- Loss: 0.397\n",
      "Iteration: 446 \t--- Loss: 0.404\n",
      "Iteration: 447 \t--- Loss: 0.410\n",
      "Iteration: 448 \t--- Loss: 0.416\n",
      "Iteration: 449 \t--- Loss: 0.405\n",
      "Iteration: 450 \t--- Loss: 0.405\n",
      "Iteration: 451 \t--- Loss: 0.401\n",
      "Iteration: 452 \t--- Loss: 0.406\n",
      "Iteration: 453 \t--- Loss: 0.397\n",
      "Iteration: 454 \t--- Loss: 0.400\n",
      "Iteration: 455 \t--- Loss: 0.417\n",
      "Iteration: 456 \t--- Loss: 0.409\n",
      "Iteration: 457 \t--- Loss: 0.401\n",
      "Iteration: 458 \t--- Loss: 0.410\n",
      "Iteration: 459 \t--- Loss: 0.409\n",
      "Iteration: 460 \t--- Loss: 0.412\n",
      "Iteration: 461 \t--- Loss: 0.403\n",
      "Iteration: 462 \t--- Loss: 0.406\n",
      "Iteration: 463 \t--- Loss: 0.407\n",
      "Iteration: 464 \t--- Loss: 0.406\n",
      "Iteration: 465 \t--- Loss: 0.404\n",
      "Iteration: 466 \t--- Loss: 0.408\n",
      "Iteration: 467 \t--- Loss: 0.406\n",
      "Iteration: 468 \t--- Loss: 0.404\n",
      "Iteration: 469 \t--- Loss: 0.400\n",
      "Iteration: 470 \t--- Loss: 0.409\n",
      "Iteration: 471 \t--- Loss: 0.403\n",
      "Iteration: 472 \t--- Loss: 0.415\n",
      "Iteration: 473 \t--- Loss: 0.401\n",
      "Iteration: 474 \t--- Loss: 0.407\n",
      "Iteration: 475 \t--- Loss: 0.405\n",
      "Iteration: 476 \t--- Loss: 0.411\n",
      "Iteration: 477 \t--- Loss: 0.407\n",
      "Iteration: 478 \t--- Loss: 0.405\n",
      "Iteration: 479 \t--- Loss: 0.417\n",
      "Iteration: 480 \t--- Loss: 0.409\n",
      "Iteration: 481 \t--- Loss: 0.406\n",
      "Iteration: 482 \t--- Loss: 0.413\n",
      "Iteration: 483 \t--- Loss: 0.408\n",
      "Iteration: 484 \t--- Loss: 0.403\n",
      "Iteration: 485 \t--- Loss: 0.415\n",
      "Iteration: 486 \t--- Loss: 0.405\n",
      "Iteration: 487 \t--- Loss: 0.413\n",
      "Iteration: 488 \t--- Loss: 0.406\n",
      "Iteration: 489 \t--- Loss: 0.405\n",
      "Iteration: 490 \t--- Loss: 0.413\n",
      "Iteration: 491 \t--- Loss: 0.412\n",
      "Iteration: 492 \t--- Loss: 0.401\n",
      "Iteration: 493 \t--- Loss: 0.412\n",
      "Iteration: 494 \t--- Loss: 0.412\n",
      "Iteration: 495 \t--- Loss: 0.401\n",
      "Iteration: 496 \t--- Loss: 0.412\n",
      "Iteration: 497 \t--- Loss: 0.414\n",
      "Iteration: 498 \t--- Loss: 0.400\n",
      "Iteration: 499 \t--- Loss: 0.393\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.159\n",
      "Iteration: 1 \t--- Loss: 0.152\n",
      "Iteration: 2 \t--- Loss: 0.136\n",
      "Iteration: 3 \t--- Loss: 0.127\n",
      "Iteration: 4 \t--- Loss: 0.104\n",
      "Iteration: 5 \t--- Loss: 0.103\n",
      "Iteration: 6 \t--- Loss: 0.096\n",
      "Iteration: 7 \t--- Loss: 0.086\n",
      "Iteration: 8 \t--- Loss: 0.078\n",
      "Iteration: 9 \t--- Loss: 0.071\n",
      "Iteration: 10 \t--- Loss: 0.066\n",
      "Iteration: 11 \t--- Loss: 0.063\n",
      "Iteration: 12 \t--- Loss: 0.057\n",
      "Iteration: 13 \t--- Loss: 0.057\n",
      "Iteration: 14 \t--- Loss: 0.053\n",
      "Iteration: 15 \t--- Loss: 0.047\n",
      "Iteration: 16 \t--- Loss: 0.047\n",
      "Iteration: 17 \t--- Loss: 0.045\n",
      "Iteration: 18 \t--- Loss: 0.043\n",
      "Iteration: 19 \t--- Loss: 0.043\n",
      "Iteration: 20 \t--- Loss: 0.042\n",
      "Iteration: 21 \t--- Loss: 0.039\n",
      "Iteration: 22 \t--- Loss: 0.040\n",
      "Iteration: 23 \t--- Loss: 0.034\n",
      "Iteration: 24 \t--- Loss: 0.036\n",
      "Iteration: 25 \t--- Loss: 0.036\n",
      "Iteration: 26 \t--- Loss: 0.033\n",
      "Iteration: 27 \t--- Loss: 0.035\n",
      "Iteration: 28 \t--- Loss: 0.033\n",
      "Iteration: 29 \t--- Loss: 0.032\n",
      "Iteration: 30 \t--- Loss: 0.032\n",
      "Iteration: 31 \t--- Loss: 0.030\n",
      "Iteration: 32 \t--- Loss: 0.030\n",
      "Iteration: 33 \t--- Loss: 0.030\n",
      "Iteration: 34 \t--- Loss: 0.028\n",
      "Iteration: 35 \t--- Loss: 0.030\n",
      "Iteration: 36 \t--- Loss: 0.026\n",
      "Iteration: 37 \t--- Loss: 0.027\n",
      "Iteration: 38 \t--- Loss: 0.027\n",
      "Iteration: 39 \t--- Loss: 0.027\n",
      "Iteration: 40 \t--- Loss: 0.027\n",
      "Iteration: 41 \t--- Loss: 0.024\n",
      "Iteration: 42 \t--- Loss: 0.026\n",
      "Iteration: 43 \t--- Loss: 0.026\n",
      "Iteration: 44 \t--- Loss: 0.027\n",
      "Iteration: 45 \t--- Loss: 0.024\n",
      "Iteration: 46 \t--- Loss: 0.027\n",
      "Iteration: 47 \t--- Loss: 0.026\n",
      "Iteration: 48 \t--- Loss: 0.024\n",
      "Iteration: 49 \t--- Loss: 0.024\n",
      "Iteration: 50 \t--- Loss: 0.024\n",
      "Iteration: 51 \t--- Loss: 0.023\n",
      "Iteration: 52 \t--- Loss: 0.024\n",
      "Iteration: 53 \t--- Loss: 0.023\n",
      "Iteration: 54 \t--- Loss: 0.023\n",
      "Iteration: 55 \t--- Loss: 0.026\n",
      "Iteration: 56 \t--- Loss: 0.023\n",
      "Iteration: 57 \t--- Loss: 0.025\n",
      "Iteration: 58 \t--- Loss: 0.023\n",
      "Iteration: 59 \t--- Loss: 0.023\n",
      "Iteration: 60 \t--- Loss: 0.024\n",
      "Iteration: 61 \t--- Loss: 0.025\n",
      "Iteration: 62 \t--- Loss: 0.022\n",
      "Iteration: 63 \t--- Loss: 0.023\n",
      "Iteration: 64 \t--- Loss: 0.022\n",
      "Iteration: 65 \t--- Loss: 0.023\n",
      "Iteration: 66 \t--- Loss: 0.023\n",
      "Iteration: 67 \t--- Loss: 0.024\n",
      "Iteration: 68 \t--- Loss: 0.023\n",
      "Iteration: 69 \t--- Loss: 0.025\n",
      "Iteration: 70 \t--- Loss: 0.023\n",
      "Iteration: 71 \t--- Loss: 0.023\n",
      "Iteration: 72 \t--- Loss: 0.021\n",
      "Iteration: 73 \t--- Loss: 0.021\n",
      "Iteration: 74 \t--- Loss: 0.022\n",
      "Iteration: 75 \t--- Loss: 0.021\n",
      "Iteration: 76 \t--- Loss: 0.022\n",
      "Iteration: 77 \t--- Loss: 0.021\n",
      "Iteration: 78 \t--- Loss: 0.019\n",
      "Iteration: 79 \t--- Loss: 0.022\n",
      "Iteration: 80 \t--- Loss: 0.020\n",
      "Iteration: 81 \t--- Loss: 0.020\n",
      "Iteration: 82 \t--- Loss: 0.021\n",
      "Iteration: 83 \t--- Loss: 0.020\n",
      "Iteration: 84 \t--- Loss: 0.019\n",
      "Iteration: 85 \t--- Loss: 0.022\n",
      "Iteration: 86 \t--- Loss: 0.021\n",
      "Iteration: 87 \t--- Loss: 0.023\n",
      "Iteration: 88 \t--- Loss: 0.020\n",
      "Iteration: 89 \t--- Loss: 0.020\n",
      "Iteration: 90 \t--- Loss: 0.019\n",
      "Iteration: 91 \t--- Loss: 0.021\n",
      "Iteration: 92 \t--- Loss: 0.021\n",
      "Iteration: 93 \t--- Loss: 0.019\n",
      "Iteration: 94 \t--- Loss: 0.019\n",
      "Iteration: 95 \t--- Loss: 0.020\n",
      "Iteration: 96 \t--- Loss: 0.021\n",
      "Iteration: 97 \t--- Loss: 0.020\n",
      "Iteration: 98 \t--- Loss: 0.021\n",
      "Iteration: 99 \t--- Loss: 0.019\n",
      "Iteration: 100 \t--- Loss: 0.020\n",
      "Iteration: 101 \t--- Loss: 0.021\n",
      "Iteration: 102 \t--- Loss: 0.020\n",
      "Iteration: 103 \t--- Loss: 0.021\n",
      "Iteration: 104 \t--- Loss: 0.021\n",
      "Iteration: 105 \t--- Loss: 0.019\n",
      "Iteration: 106 \t--- Loss: 0.020\n",
      "Iteration: 107 \t--- Loss: 0.019\n",
      "Iteration: 108 \t--- Loss: 0.021\n",
      "Iteration: 109 \t--- Loss: 0.019\n",
      "Iteration: 110 \t--- Loss: 0.020\n",
      "Iteration: 111 \t--- Loss: 0.019\n",
      "Iteration: 112 \t--- Loss: 0.020\n",
      "Iteration: 113 \t--- Loss: 0.019\n",
      "Iteration: 114 \t--- Loss: 0.020\n",
      "Iteration: 115 \t--- Loss: 0.018\n",
      "Iteration: 116 \t--- Loss: 0.021\n",
      "Iteration: 117 \t--- Loss: 0.019\n",
      "Iteration: 118 \t--- Loss: 0.019\n",
      "Iteration: 119 \t--- Loss: 0.021\n",
      "Iteration: 120 \t--- Loss: 0.020\n",
      "Iteration: 121 \t--- Loss: 0.019\n",
      "Iteration: 122 \t--- Loss: 0.018\n",
      "Iteration: 123 \t--- Loss: 0.021\n",
      "Iteration: 124 \t--- Loss: 0.017\n",
      "Iteration: 125 \t--- Loss: 0.018\n",
      "Iteration: 126 \t--- Loss: 0.019\n",
      "Iteration: 127 \t--- Loss: 0.019\n",
      "Iteration: 128 \t--- Loss: 0.019\n",
      "Iteration: 129 \t--- Loss: 0.018\n",
      "Iteration: 130 \t--- Loss: 0.019\n",
      "Iteration: 131 \t--- Loss: 0.020\n",
      "Iteration: 132 \t--- Loss: 0.019\n",
      "Iteration: 133 \t--- Loss: 0.020\n",
      "Iteration: 134 \t--- Loss: 0.018\n",
      "Iteration: 135 \t--- Loss: 0.018\n",
      "Iteration: 136 \t--- Loss: 0.018\n",
      "Iteration: 137 \t--- Loss: 0.020\n",
      "Iteration: 138 \t--- Loss: 0.019\n",
      "Iteration: 139 \t--- Loss: 0.020\n",
      "Iteration: 140 \t--- Loss: 0.020\n",
      "Iteration: 141 \t--- Loss: 0.021\n",
      "Iteration: 142 \t--- Loss: 0.020\n",
      "Iteration: 143 \t--- Loss: 0.019\n",
      "Iteration: 144 \t--- Loss: 0.019\n",
      "Iteration: 145 \t--- Loss: 0.020\n",
      "Iteration: 146 \t--- Loss: 0.018\n",
      "Iteration: 147 \t--- Loss: 0.019\n",
      "Iteration: 148 \t--- Loss: 0.019\n",
      "Iteration: 149 \t--- Loss: 0.020\n",
      "Iteration: 150 \t--- Loss: 0.019\n",
      "Iteration: 151 \t--- Loss: 0.020\n",
      "Iteration: 152 \t--- Loss: 0.018\n",
      "Iteration: 153 \t--- Loss: 0.020\n",
      "Iteration: 154 \t--- Loss: 0.020\n",
      "Iteration: 155 \t--- Loss: 0.017\n",
      "Iteration: 156 \t--- Loss: 0.019\n",
      "Iteration: 157 \t--- Loss: 0.019\n",
      "Iteration: 158 \t--- Loss: 0.019\n",
      "Iteration: 159 \t--- Loss: 0.020\n",
      "Iteration: 160 \t--- Loss: 0.020\n",
      "Iteration: 161 \t--- Loss: 0.018\n",
      "Iteration: 162 \t--- Loss: 0.020\n",
      "Iteration: 163 \t--- Loss: 0.019\n",
      "Iteration: 164 \t--- Loss: 0.020\n",
      "Iteration: 165 \t--- Loss: 0.019\n",
      "Iteration: 166 \t--- Loss: 0.020\n",
      "Iteration: 167 \t--- Loss: 0.020\n",
      "Iteration: 168 \t--- Loss: 0.020\n",
      "Iteration: 169 \t--- Loss: 0.018\n",
      "Iteration: 170 \t--- Loss: 0.019\n",
      "Iteration: 171 \t--- Loss: 0.018\n",
      "Iteration: 172 \t--- Loss: 0.020\n",
      "Iteration: 173 \t--- Loss: 0.020\n",
      "Iteration: 174 \t--- Loss: 0.018\n",
      "Iteration: 175 \t--- Loss: 0.018\n",
      "Iteration: 176 \t--- Loss: 0.020\n",
      "Iteration: 177 \t--- Loss: 0.019\n",
      "Iteration: 178 \t--- Loss: 0.020\n",
      "Iteration: 179 \t--- Loss: 0.019\n",
      "Iteration: 180 \t--- Loss: 0.019\n",
      "Iteration: 181 \t--- Loss: 0.019\n",
      "Iteration: 182 \t--- Loss: 0.019\n",
      "Iteration: 183 \t--- Loss: 0.018\n",
      "Iteration: 184 \t--- Loss: 0.017\n",
      "Iteration: 185 \t--- Loss: 0.019\n",
      "Iteration: 186 \t--- Loss: 0.018\n",
      "Iteration: 187 \t--- Loss: 0.020\n",
      "Iteration: 188 \t--- Loss: 0.019\n",
      "Iteration: 189 \t--- Loss: 0.018\n",
      "Iteration: 190 \t--- Loss: 0.019\n",
      "Iteration: 191 \t--- Loss: 0.019\n",
      "Iteration: 192 \t--- Loss: 0.019\n",
      "Iteration: 193 \t--- Loss: 0.018\n",
      "Iteration: 194 \t--- Loss: 0.021\n",
      "Iteration: 195 \t--- Loss: 0.020\n",
      "Iteration: 196 \t--- Loss: 0.019\n",
      "Iteration: 197 \t--- Loss: 0.020\n",
      "Iteration: 198 \t--- Loss: 0.019\n",
      "Iteration: 199 \t--- Loss: 0.019\n",
      "Iteration: 200 \t--- Loss: 0.018\n",
      "Iteration: 201 \t--- Loss: 0.018\n",
      "Iteration: 202 \t--- Loss: 0.019\n",
      "Iteration: 203 \t--- Loss: 0.018\n",
      "Iteration: 204 \t--- Loss: 0.018\n",
      "Iteration: 205 \t--- Loss: 0.019\n",
      "Iteration: 206 \t--- Loss: 0.017\n",
      "Iteration: 207 \t--- Loss: 0.018\n",
      "Iteration: 208 \t--- Loss: 0.018\n",
      "Iteration: 209 \t--- Loss: 0.019\n",
      "Iteration: 210 \t--- Loss: 0.020\n",
      "Iteration: 211 \t--- Loss: 0.019\n",
      "Iteration: 212 \t--- Loss: 0.020\n",
      "Iteration: 213 \t--- Loss: 0.020\n",
      "Iteration: 214 \t--- Loss: 0.019\n",
      "Iteration: 215 \t--- Loss: 0.018\n",
      "Iteration: 216 \t--- Loss: 0.019\n",
      "Iteration: 217 \t--- Loss: 0.018\n",
      "Iteration: 218 \t--- Loss: 0.018\n",
      "Iteration: 219 \t--- Loss: 0.018\n",
      "Iteration: 220 \t--- Loss: 0.019\n",
      "Iteration: 221 \t--- Loss: 0.019\n",
      "Iteration: 222 \t--- Loss: 0.018\n",
      "Iteration: 223 \t--- Loss: 0.019\n",
      "Iteration: 224 \t--- Loss: 0.019\n",
      "Iteration: 225 \t--- Loss: 0.017\n",
      "Iteration: 226 \t--- Loss: 0.018\n",
      "Iteration: 227 \t--- Loss: 0.019\n",
      "Iteration: 228 \t--- Loss: 0.019\n",
      "Iteration: 229 \t--- Loss: 0.019\n",
      "Iteration: 230 \t--- Loss: 0.018\n",
      "Iteration: 231 \t--- Loss: 0.021\n",
      "Iteration: 232 \t--- Loss: 0.019\n",
      "Iteration: 233 \t--- Loss: 0.020\n",
      "Iteration: 234 \t--- Loss: 0.018\n",
      "Iteration: 235 \t--- Loss: 0.019\n",
      "Iteration: 236 \t--- Loss: 0.018\n",
      "Iteration: 237 \t--- Loss: 0.019\n",
      "Iteration: 238 \t--- Loss: 0.018\n",
      "Iteration: 239 \t--- Loss: 0.019\n",
      "Iteration: 240 \t--- Loss: 0.019\n",
      "Iteration: 241 \t--- Loss: 0.018\n",
      "Iteration: 242 \t--- Loss: 0.019\n",
      "Iteration: 243 \t--- Loss: 0.021\n",
      "Iteration: 244 \t--- Loss: 0.018\n",
      "Iteration: 245 \t--- Loss: 0.019\n",
      "Iteration: 246 \t--- Loss: 0.019\n",
      "Iteration: 247 \t--- Loss: 0.016\n",
      "Iteration: 248 \t--- Loss: 0.017\n",
      "Iteration: 249 \t--- Loss: 0.020\n",
      "Iteration: 250 \t--- Loss: 0.021\n",
      "Iteration: 251 \t--- Loss: 0.018\n",
      "Iteration: 252 \t--- Loss: 0.019\n",
      "Iteration: 253 \t--- Loss: 0.016\n",
      "Iteration: 254 \t--- Loss: 0.019\n",
      "Iteration: 255 \t--- Loss: 0.018\n",
      "Iteration: 256 \t--- Loss: 0.020\n",
      "Iteration: 257 \t--- Loss: 0.018\n",
      "Iteration: 258 \t--- Loss: 0.018\n",
      "Iteration: 259 \t--- Loss: 0.019"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it][Parallel(n_jobs=5)]: Done  81 tasks      | elapsed: 50.9min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:45<00:00, 585.48s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.101\n",
      "Iteration: 261 \t--- Loss: 0.094\n",
      "Iteration: 262 \t--- Loss: 0.109\n",
      "Iteration: 263 \t--- Loss: 0.113\n",
      "Iteration: 264 \t--- Loss: 0.127\n",
      "Iteration: 265 \t--- Loss: 0.085\n",
      "Iteration: 266 \t--- Loss: 0.097\n",
      "Iteration: 267 \t--- Loss: 0.098\n",
      "Iteration: 268 \t--- Loss: 0.090\n",
      "Iteration: 269 \t--- Loss: 0.083\n",
      "Iteration: 270 \t--- Loss: 0.101\n",
      "Iteration: 271 \t--- Loss: 0.099\n",
      "Iteration: 272 \t--- Loss: 0.102\n",
      "Iteration: 273 \t--- Loss: 0.108\n",
      "Iteration: 274 \t--- Loss: 0.095\n",
      "Iteration: 275 \t--- Loss: 0.094\n",
      "Iteration: 276 \t--- Loss: 0.094\n",
      "Iteration: 277 \t--- Loss: 0.092\n",
      "Iteration: 278 \t--- Loss: 0.095\n",
      "Iteration: 279 \t--- Loss: 0.100\n",
      "Iteration: 280 \t--- Loss: 0.094\n",
      "Iteration: 281 \t--- Loss: 0.088\n",
      "Iteration: 282 \t--- Loss: 0.087\n",
      "Iteration: 283 \t--- Loss: 0.099\n",
      "Iteration: 284 \t--- Loss: 0.090\n",
      "Iteration: 285 \t--- Loss: 0.090\n",
      "Iteration: 286 \t--- Loss: 0.088\n",
      "Iteration: 287 \t--- Loss: 0.085\n",
      "Iteration: 288 \t--- Loss: 0.090\n",
      "Iteration: 289 \t--- Loss: 0.096\n",
      "Iteration: 290 \t--- Loss: 0.092\n",
      "Iteration: 291 \t--- Loss: 0.085\n",
      "Iteration: 292 \t--- Loss: 0.094\n",
      "Iteration: 293 \t--- Loss: 0.088\n",
      "Iteration: 294 \t--- Loss: 0.090\n",
      "Iteration: 295 \t--- Loss: 0.087\n",
      "Iteration: 296 \t--- Loss: 0.096\n",
      "Iteration: 297 \t--- Loss: 0.087\n",
      "Iteration: 298 \t--- Loss: 0.095\n",
      "Iteration: 299 \t--- Loss: 0.093\n",
      "Iteration: 300 \t--- Loss: 0.095\n",
      "Iteration: 301 \t--- Loss: 0.092\n",
      "Iteration: 302 \t--- Loss: 0.079\n",
      "Iteration: 303 \t--- Loss: 0.093\n",
      "Iteration: 304 \t--- Loss: 0.091\n",
      "Iteration: 305 \t--- Loss: 0.099\n",
      "Iteration: 306 \t--- Loss: 0.093\n",
      "Iteration: 307 \t--- Loss: 0.096\n",
      "Iteration: 308 \t--- Loss: 0.086\n",
      "Iteration: 309 \t--- Loss: 0.084\n",
      "Iteration: 310 \t--- Loss: 0.088\n",
      "Iteration: 311 \t--- Loss: 0.090\n",
      "Iteration: 312 \t--- Loss: 0.091\n",
      "Iteration: 313 \t--- Loss: 0.086\n",
      "Iteration: 314 \t--- Loss: 0.103\n",
      "Iteration: 315 \t--- Loss: 0.093\n",
      "Iteration: 316 \t--- Loss: 0.103\n",
      "Iteration: 317 \t--- Loss: 0.103\n",
      "Iteration: 318 \t--- Loss: 0.087\n",
      "Iteration: 319 \t--- Loss: 0.081\n",
      "Iteration: 320 \t--- Loss: 0.089\n",
      "Iteration: 321 \t--- Loss: 0.089\n",
      "Iteration: 322 \t--- Loss: 0.077\n",
      "Iteration: 323 \t--- Loss: 0.094\n",
      "Iteration: 324 \t--- Loss: 0.089\n",
      "Iteration: 325 \t--- Loss: 0.088\n",
      "Iteration: 326 \t--- Loss: 0.086\n",
      "Iteration: 327 \t--- Loss: 0.077\n",
      "Iteration: 328 \t--- Loss: 0.095\n",
      "Iteration: 329 \t--- Loss: 0.087\n",
      "Iteration: 330 \t--- Loss: 0.091\n",
      "Iteration: 331 \t--- Loss: 0.086\n",
      "Iteration: 332 \t--- Loss: 0.088\n",
      "Iteration: 333 \t--- Loss: 0.100\n",
      "Iteration: 334 \t--- Loss: 0.090\n",
      "Iteration: 335 \t--- Loss: 0.096\n",
      "Iteration: 336 \t--- Loss: 0.094\n",
      "Iteration: 337 \t--- Loss: 0.088\n",
      "Iteration: 338 \t--- Loss: 0.087\n",
      "Iteration: 339 \t--- Loss: 0.086\n",
      "Iteration: 340 \t--- Loss: 0.093\n",
      "Iteration: 341 \t--- Loss: 0.088\n",
      "Iteration: 342 \t--- Loss: 0.085\n",
      "Iteration: 343 \t--- Loss: 0.083\n",
      "Iteration: 344 \t--- Loss: 0.083\n",
      "Iteration: 345 \t--- Loss: 0.085\n",
      "Iteration: 346 \t--- Loss: 0.087\n",
      "Iteration: 347 \t--- Loss: 0.090\n",
      "Iteration: 348 \t--- Loss: 0.088\n",
      "Iteration: 349 \t--- Loss: 0.083\n",
      "Iteration: 350 \t--- Loss: 0.095\n",
      "Iteration: 351 \t--- Loss: 0.081\n",
      "Iteration: 352 \t--- Loss: 0.092\n",
      "Iteration: 353 \t--- Loss: 0.091\n",
      "Iteration: 354 \t--- Loss: 0.095\n",
      "Iteration: 355 \t--- Loss: 0.090\n",
      "Iteration: 356 \t--- Loss: 0.095\n",
      "Iteration: 357 \t--- Loss: 0.093\n",
      "Iteration: 358 \t--- Loss: 0.095\n",
      "Iteration: 359 \t--- Loss: 0.085\n",
      "Iteration: 360 \t--- Loss: 0.086\n",
      "Iteration: 361 \t--- Loss: 0.080\n",
      "Iteration: 362 \t--- Loss: 0.091\n",
      "Iteration: 363 \t--- Loss: 0.091\n",
      "Iteration: 364 \t--- Loss: 0.084\n",
      "Iteration: 365 \t--- Loss: 0.091\n",
      "Iteration: 366 \t--- Loss: 0.083\n",
      "Iteration: 367 \t--- Loss: 0.087\n",
      "Iteration: 368 \t--- Loss: 0.082\n",
      "Iteration: 369 \t--- Loss: 0.090\n",
      "Iteration: 370 \t--- Loss: 0.090\n",
      "Iteration: 371 \t--- Loss: 0.092\n",
      "Iteration: 372 \t--- Loss: 0.094\n",
      "Iteration: 373 \t--- Loss: 0.085\n",
      "Iteration: 374 \t--- Loss: 0.092\n",
      "Iteration: 375 \t--- Loss: 0.094\n",
      "Iteration: 376 \t--- Loss: 0.087\n",
      "Iteration: 377 \t--- Loss: 0.085\n",
      "Iteration: 378 \t--- Loss: 0.084\n",
      "Iteration: 379 \t--- Loss: 0.081\n",
      "Iteration: 380 \t--- Loss: 0.080\n",
      "Iteration: 381 \t--- Loss: 0.093\n",
      "Iteration: 382 \t--- Loss: 0.089\n",
      "Iteration: 383 \t--- Loss: 0.092\n",
      "Iteration: 384 \t--- Loss: 0.085\n",
      "Iteration: 385 \t--- Loss: 0.091\n",
      "Iteration: 386 \t--- Loss: 0.089\n",
      "Iteration: 387 \t--- Loss: 0.091\n",
      "Iteration: 388 \t--- Loss: 0.093\n",
      "Iteration: 389 \t--- Loss: 0.089\n",
      "Iteration: 390 \t--- Loss: 0.093\n",
      "Iteration: 391 \t--- Loss: 0.089\n",
      "Iteration: 392 \t--- Loss: 0.096\n",
      "Iteration: 393 \t--- Loss: 0.094\n",
      "Iteration: 394 \t--- Loss: 0.089\n",
      "Iteration: 395 \t--- Loss: 0.092\n",
      "Iteration: 396 \t--- Loss: 0.088\n",
      "Iteration: 397 \t--- Loss: 0.082\n",
      "Iteration: 398 \t--- Loss: 0.084\n",
      "Iteration: 399 \t--- Loss: 0.089\n",
      "Iteration: 400 \t--- Loss: 0.080\n",
      "Iteration: 401 \t--- Loss: 0.086\n",
      "Iteration: 402 \t--- Loss: 0.087\n",
      "Iteration: 403 \t--- Loss: 0.094\n",
      "Iteration: 404 \t--- Loss: 0.097\n",
      "Iteration: 405 \t--- Loss: 0.084\n",
      "Iteration: 406 \t--- Loss: 0.088\n",
      "Iteration: 407 \t--- Loss: 0.091\n",
      "Iteration: 408 \t--- Loss: 0.094\n",
      "Iteration: 409 \t--- Loss: 0.086\n",
      "Iteration: 410 \t--- Loss: 0.087\n",
      "Iteration: 411 \t--- Loss: 0.093\n",
      "Iteration: 412 \t--- Loss: 0.088\n",
      "Iteration: 413 \t--- Loss: 0.082\n",
      "Iteration: 414 \t--- Loss: 0.085\n",
      "Iteration: 415 \t--- Loss: 0.084\n",
      "Iteration: 416 \t--- Loss: 0.091\n",
      "Iteration: 417 \t--- Loss: 0.093\n",
      "Iteration: 418 \t--- Loss: 0.093\n",
      "Iteration: 419 \t--- Loss: 0.092\n",
      "Iteration: 420 \t--- Loss: 0.087\n",
      "Iteration: 421 \t--- Loss: 0.086\n",
      "Iteration: 422 \t--- Loss: 0.089\n",
      "Iteration: 423 \t--- Loss: 0.089\n",
      "Iteration: 424 \t--- Loss: 0.089\n",
      "Iteration: 425 \t--- Loss: 0.083\n",
      "Iteration: 426 \t--- Loss: 0.096\n",
      "Iteration: 427 \t--- Loss: 0.089\n",
      "Iteration: 428 \t--- Loss: 0.095\n",
      "Iteration: 429 \t--- Loss: 0.093\n",
      "Iteration: 430 \t--- Loss: 0.091\n",
      "Iteration: 431 \t--- Loss: 0.083\n",
      "Iteration: 432 \t--- Loss: 0.090\n",
      "Iteration: 433 \t--- Loss: 0.081\n",
      "Iteration: 434 \t--- Loss: 0.079\n",
      "Iteration: 435 \t--- Loss: 0.086\n",
      "Iteration: 436 \t--- Loss: 0.091\n",
      "Iteration: 437 \t--- Loss: 0.094\n",
      "Iteration: 438 \t--- Loss: 0.097\n",
      "Iteration: 439 \t--- Loss: 0.088\n",
      "Iteration: 440 \t--- Loss: 0.091\n",
      "Iteration: 441 \t--- Loss: 0.091\n",
      "Iteration: 442 \t--- Loss: 0.080\n",
      "Iteration: 443 \t--- Loss: 0.094\n",
      "Iteration: 444 \t--- Loss: 0.093\n",
      "Iteration: 445 \t--- Loss: 0.090\n",
      "Iteration: 446 \t--- Loss: 0.088\n",
      "Iteration: 447 \t--- Loss: 0.086\n",
      "Iteration: 448 \t--- Loss: 0.091\n",
      "Iteration: 449 \t--- Loss: 0.089\n",
      "Iteration: 450 \t--- Loss: 0.093\n",
      "Iteration: 451 \t--- Loss: 0.093\n",
      "Iteration: 452 \t--- Loss: 0.083\n",
      "Iteration: 453 \t--- Loss: 0.091\n",
      "Iteration: 454 \t--- Loss: 0.092\n",
      "Iteration: 455 \t--- Loss: 0.089\n",
      "Iteration: 456 \t--- Loss: 0.082\n",
      "Iteration: 457 \t--- Loss: 0.092\n",
      "Iteration: 458 \t--- Loss: 0.087\n",
      "Iteration: 459 \t--- Loss: 0.089\n",
      "Iteration: 460 \t--- Loss: 0.087\n",
      "Iteration: 461 \t--- Loss: 0.087\n",
      "Iteration: 462 \t--- Loss: 0.098\n",
      "Iteration: 463 \t--- Loss: 0.090\n",
      "Iteration: 464 \t--- Loss: 0.092\n",
      "Iteration: 465 \t--- Loss: 0.083\n",
      "Iteration: 466 \t--- Loss: 0.093\n",
      "Iteration: 467 \t--- Loss: 0.085\n",
      "Iteration: 468 \t--- Loss: 0.092\n",
      "Iteration: 469 \t--- Loss: 0.094\n",
      "Iteration: 470 \t--- Loss: 0.088\n",
      "Iteration: 471 \t--- Loss: 0.087\n",
      "Iteration: 472 \t--- Loss: 0.085\n",
      "Iteration: 473 \t--- Loss: 0.086\n",
      "Iteration: 474 \t--- Loss: 0.094\n",
      "Iteration: 475 \t--- Loss: 0.087\n",
      "Iteration: 476 \t--- Loss: 0.099\n",
      "Iteration: 477 \t--- Loss: 0.082\n",
      "Iteration: 478 \t--- Loss: 0.088\n",
      "Iteration: 479 \t--- Loss: 0.088\n",
      "Iteration: 480 \t--- Loss: 0.079\n",
      "Iteration: 481 \t--- Loss: 0.091\n",
      "Iteration: 482 \t--- Loss: 0.083\n",
      "Iteration: 483 \t--- Loss: 0.087\n",
      "Iteration: 484 \t--- Loss: 0.085\n",
      "Iteration: 485 \t--- Loss: 0.083\n",
      "Iteration: 486 \t--- Loss: 0.086\n",
      "Iteration: 487 \t--- Loss: 0.086\n",
      "Iteration: 488 \t--- Loss: 0.081\n",
      "Iteration: 489 \t--- Loss: 0.086\n",
      "Iteration: 490 \t--- Loss: 0.087\n",
      "Iteration: 491 \t--- Loss: 0.086\n",
      "Iteration: 492 \t--- Loss: 0.085\n",
      "Iteration: 493 \t--- Loss: 0.087\n",
      "Iteration: 494 \t--- Loss: 0.084\n",
      "Iteration: 495 \t--- Loss: 0.084\n",
      "Iteration: 496 \t--- Loss: 0.089\n",
      "Iteration: 497 \t--- Loss: 0.086\n",
      "Iteration: 498 \t--- Loss: 0.083\n",
      "Iteration: 499 \t--- Loss: 0.089\n",
      "----  Optimizing the metamodel  ----\n",
      "Iteration: 0 \t--- Loss: 0.080\n",
      "Iteration: 1 \t--- Loss: 0.081\n",
      "Iteration: 2 \t--- Loss: 0.080\n",
      "Iteration: 3 \t--- Loss: 0.083\n",
      "Iteration: 4 \t--- Loss: 0.079\n",
      "Iteration: 5 \t--- Loss: 0.075\n",
      "Iteration: 6 \t--- Loss: 0.085\n",
      "Iteration: 7 \t--- Loss: 0.080\n",
      "Iteration: 8 \t--- Loss: 0.076\n",
      "Iteration: 9 \t--- Loss: 0.078\n",
      "Iteration: 10 \t--- Loss: 0.076\n",
      "Iteration: 11 \t--- Loss: 0.076\n",
      "Iteration: 12 \t--- Loss: 0.076\n",
      "Iteration: 13 \t--- Loss: 0.072\n",
      "Iteration: 14 \t--- Loss: 0.072\n",
      "Iteration: 15 \t--- Loss: 0.075\n",
      "Iteration: 16 \t--- Loss: 0.076\n",
      "Iteration: 17 \t--- Loss: 0.075\n",
      "Iteration: 18 \t--- Loss: 0.072\n",
      "Iteration: 19 \t--- Loss: 0.077\n",
      "Iteration: 20 \t--- Loss: 0.070\n",
      "Iteration: 21 \t--- Loss: 0.076\n",
      "Iteration: 22 \t--- Loss: 0.076\n",
      "Iteration: 23 \t--- Loss: 0.076\n",
      "Iteration: 24 \t--- Loss: 0.072\n",
      "Iteration: 25 \t--- Loss: 0.076\n",
      "Iteration: 26 \t--- Loss: 0.073\n",
      "Iteration: 27 \t--- Loss: 0.070\n",
      "Iteration: 28 \t--- Loss: 0.077\n",
      "Iteration: 29 \t--- Loss: 0.076\n",
      "Iteration: 30 \t--- Loss: 0.074\n",
      "Iteration: 31 \t--- Loss: 0.069\n",
      "Iteration: 32 \t--- Loss: 0.072\n",
      "Iteration: 33 \t--- Loss: 0.075\n",
      "Iteration: 34 \t--- Loss: 0.074\n",
      "Iteration: 35 \t--- Loss: 0.073\n",
      "Iteration: 36 \t--- Loss: 0.070\n",
      "Iteration: 37 \t--- Loss: 0.074\n",
      "Iteration: 38 \t--- Loss: 0.076\n",
      "Iteration: 39 \t--- Loss: 0.071\n",
      "Iteration: 40 \t--- Loss: 0.070\n",
      "Iteration: 41 \t--- Loss: 0.075\n",
      "Iteration: 42 \t--- Loss: 0.073\n",
      "Iteration: 43 \t--- Loss: 0.073\n",
      "Iteration: 44 \t--- Loss: 0.071\n",
      "Iteration: 45 \t--- Loss: 0.076\n",
      "Iteration: 46 \t--- Loss: 0.073\n",
      "Iteration: 47 \t--- Loss: 0.071\n",
      "Iteration: 48 \t--- Loss: 0.071\n",
      "Iteration: 49 \t--- Loss: 0.073\n",
      "Iteration: 50 \t--- Loss: 0.071\n",
      "Iteration: 51 \t--- Loss: 0.070\n",
      "Iteration: 52 \t--- Loss: 0.068\n",
      "Iteration: 53 \t--- Loss: 0.067\n",
      "Iteration: 54 \t--- Loss: 0.069\n",
      "Iteration: 55 \t--- Loss: 0.071\n",
      "Iteration: 56 \t--- Loss: 0.072\n",
      "Iteration: 57 \t--- Loss: 0.066\n",
      "Iteration: 58 \t--- Loss: 0.066\n",
      "Iteration: 59 \t--- Loss: 0.072\n",
      "Iteration: 60 \t--- Loss: 0.076\n",
      "Iteration: 61 \t--- Loss: 0.070\n",
      "Iteration: 62 \t--- Loss: 0.069\n",
      "Iteration: 63 \t--- Loss: 0.072\n",
      "Iteration: 64 \t--- Loss: 0.074\n",
      "Iteration: 65 \t--- Loss: 0.066\n",
      "Iteration: 66 \t--- Loss: 0.069\n",
      "Iteration: 67 \t--- Loss: 0.070\n",
      "Iteration: 68 \t--- Loss: 0.075\n",
      "Iteration: 69 \t--- Loss: 0.072\n",
      "Iteration: 70 \t--- Loss: 0.070\n",
      "Iteration: 71 \t--- Loss: 0.073\n",
      "Iteration: 72 \t--- Loss: 0.074\n",
      "Iteration: 73 \t--- Loss: 0.076\n",
      "Iteration: 74 \t--- Loss: 0.071\n",
      "Iteration: 75 \t--- Loss: 0.073\n",
      "Iteration: 76 \t--- Loss: 0.067\n",
      "Iteration: 77 \t--- Loss: 0.071\n",
      "Iteration: 78 \t--- Loss: 0.071\n",
      "Iteration: 79 \t--- Loss: 0.070\n",
      "Iteration: 80 \t--- Loss: 0.078\n",
      "Iteration: 81 \t--- Loss: 0.071\n",
      "Iteration: 82 \t--- Loss: 0.073\n",
      "Iteration: 83 \t--- Loss: 0.074\n",
      "Iteration: 84 \t--- Loss: 0.077\n",
      "Iteration: 85 \t--- Loss: 0.069\n",
      "Iteration: 86 \t--- Loss: 0.071\n",
      "Iteration: 87 \t--- Loss: 0.076\n",
      "Iteration: 88 \t--- Loss: 0.074\n",
      "Iteration: 89 \t--- Loss: 0.071\n",
      "Iteration: 90 \t--- Loss: 0.071\n",
      "Iteration: 91 \t--- Loss: 0.071\n",
      "Iteration: 92 \t--- Loss: 0.069\n",
      "Iteration: 93 \t--- Loss: 0.073\n",
      "Iteration: 94 \t--- Loss: 0.073\n",
      "Iteration: 95 \t--- Loss: 0.071\n",
      "Iteration: 96 \t--- Loss: 0.069\n",
      "Iteration: 97 \t--- Loss: 0.067\n",
      "Iteration: 98 \t--- Loss: 0.073\n",
      "Iteration: 99 \t--- Loss: 0.076\n",
      "Iteration: 100 \t--- Loss: 0.071\n",
      "Iteration: 101 \t--- Loss: 0.072\n",
      "Iteration: 102 \t--- Loss: 0.070\n",
      "Iteration: 103 \t--- Loss: 0.072\n",
      "Iteration: 104 \t--- Loss: 0.075\n",
      "Iteration: 105 \t--- Loss: 0.074\n",
      "Iteration: 106 \t--- Loss: 0.072\n",
      "Iteration: 107 \t--- Loss: 0.074\n",
      "Iteration: 108 \t--- Loss: 0.070\n",
      "Iteration: 109 \t--- Loss: 0.069\n",
      "Iteration: 110 \t--- Loss: 0.069\n",
      "Iteration: 111 \t--- Loss: 0.072\n",
      "Iteration: 112 \t--- Loss: 0.078\n",
      "Iteration: 113 \t--- Loss: 0.073\n",
      "Iteration: 114 \t--- Loss: 0.075\n",
      "Iteration: 115 \t--- Loss: 0.067\n",
      "Iteration: 116 \t--- Loss: 0.072\n",
      "Iteration: 117 \t--- Loss: 0.072\n",
      "Iteration: 118 \t--- Loss: 0.069\n",
      "Iteration: 119 \t--- Loss: 0.077\n",
      "Iteration: 120 \t--- Loss: 0.066\n",
      "Iteration: 121 \t--- Loss: 0.071\n",
      "Iteration: 122 \t--- Loss: 0.073\n",
      "Iteration: 123 \t--- Loss: 0.069\n",
      "Iteration: 124 \t--- Loss: 0.072\n",
      "Iteration: 125 \t--- Loss: 0.064\n",
      "Iteration: 126 \t--- Loss: 0.069\n",
      "Iteration: 127 \t--- Loss: 0.074\n",
      "Iteration: 128 \t--- Loss: 0.069\n",
      "Iteration: 129 \t--- Loss: 0.075\n",
      "Iteration: 130 \t--- Loss: 0.073\n",
      "Iteration: 131 \t--- Loss: 0.067\n",
      "Iteration: 132 \t--- Loss: 0.067\n",
      "Iteration: 133 \t--- Loss: 0.071\n",
      "Iteration: 134 \t--- Loss: 0.072\n",
      "Iteration: 135 \t--- Loss: 0.071\n",
      "Iteration: 136 \t--- Loss: 0.071\n",
      "Iteration: 137 \t--- Loss: 0.066\n",
      "Iteration: 138 \t--- Loss: 0.070\n",
      "Iteration: 139 \t--- Loss: 0.068\n",
      "Iteration: 140 \t--- Loss: 0.076\n",
      "Iteration: 141 \t--- Loss: 0.072\n",
      "Iteration: 142 \t--- Loss: 0.074\n",
      "Iteration: 143 \t--- Loss: 0.072\n",
      "Iteration: 144 \t--- Loss: 0.070\n",
      "Iteration: 145 \t--- Loss: 0.070\n",
      "Iteration: 146 \t--- Loss: 0.068\n",
      "Iteration: 147 \t--- Loss: 0.070\n",
      "Iteration: 148 \t--- Loss: 0.069\n",
      "Iteration: 149 \t--- Loss: 0.071\n",
      "Iteration: 150 \t--- Loss: 0.070\n",
      "Iteration: 151 \t--- Loss: 0.071\n",
      "Iteration: 152 \t--- Loss: 0.071\n",
      "Iteration: 153 \t--- Loss: 0.072\n",
      "Iteration: 154 \t--- Loss: 0.075\n",
      "Iteration: 155 \t--- Loss: 0.068\n",
      "Iteration: 156 \t--- Loss: 0.075\n",
      "Iteration: 157 \t--- Loss: 0.068\n",
      "Iteration: 158 \t--- Loss: 0.070\n",
      "Iteration: 159 \t--- Loss: 0.067\n",
      "Iteration: 160 \t--- Loss: 0.067\n",
      "Iteration: 161 \t--- Loss: 0.071\n",
      "Iteration: 162 \t--- Loss: 0.072\n",
      "Iteration: 163 \t--- Loss: 0.075\n",
      "Iteration: 164 \t--- Loss: 0.072\n",
      "Iteration: 165 \t--- Loss: 0.073\n",
      "Iteration: 166 \t--- Loss: 0.070\n",
      "Iteration: 167 \t--- Loss: 0.070\n",
      "Iteration: 168 \t--- Loss: 0.071\n",
      "Iteration: 169 \t--- Loss: 0.067\n",
      "Iteration: 170 \t--- Loss: 0.074\n",
      "Iteration: 171 \t--- Loss: 0.066\n",
      "Iteration: 172 \t--- Loss: 0.071\n",
      "Iteration: 173 \t--- Loss: 0.071\n",
      "Iteration: 174 \t--- Loss: 0.071\n",
      "Iteration: 175 \t--- Loss: 0.069\n",
      "Iteration: 176 \t--- Loss: 0.071\n",
      "Iteration: 177 \t--- Loss: 0.072\n",
      "Iteration: 178 \t--- Loss: 0.070\n",
      "Iteration: 179 \t--- Loss: 0.069\n",
      "Iteration: 180 \t--- Loss: 0.069\n",
      "Iteration: 181 \t--- Loss: 0.068\n",
      "Iteration: 182 \t--- Loss: 0.068\n",
      "Iteration: 183 \t--- Loss: 0.068\n",
      "Iteration: 184 \t--- Loss: 0.072\n",
      "Iteration: 185 \t--- Loss: 0.070\n",
      "Iteration: 186 \t--- Loss: 0.069\n",
      "Iteration: 187 \t--- Loss: 0.069\n",
      "Iteration: 188 \t--- Loss: 0.068\n",
      "Iteration: 189 \t--- Loss: 0.069\n",
      "Iteration: 190 \t--- Loss: 0.068\n",
      "Iteration: 191 \t--- Loss: 0.069\n",
      "Iteration: 192 \t--- Loss: 0.063\n",
      "Iteration: 193 \t--- Loss: 0.069\n",
      "Iteration: 194 \t--- Loss: 0.069\n",
      "Iteration: 195 \t--- Loss: 0.069\n",
      "Iteration: 196 \t--- Loss: 0.065\n",
      "Iteration: 197 \t--- Loss: 0.071\n",
      "Iteration: 198 \t--- Loss: 0.071\n",
      "Iteration: 199 \t--- Loss: 0.068\n",
      "Iteration: 200 \t--- Loss: 0.074\n",
      "Iteration: 201 \t--- Loss: 0.067\n",
      "Iteration: 202 \t--- Loss: 0.070\n",
      "Iteration: 203 \t--- Loss: 0.063\n",
      "Iteration: 204 \t--- Loss: 0.067\n",
      "Iteration: 205 \t--- Loss: 0.064\n",
      "Iteration: 206 \t--- Loss: 0.071\n",
      "Iteration: 207 \t--- Loss: 0.067\n",
      "Iteration: 208 \t--- Loss: 0.067\n",
      "Iteration: 209 \t--- Loss: 0.066\n",
      "Iteration: 210 \t--- Loss: 0.072\n",
      "Iteration: 211 \t--- Loss: 0.065\n",
      "Iteration: 212 \t--- Loss: 0.068\n",
      "Iteration: 213 \t--- Loss: 0.069\n",
      "Iteration: 214 \t--- Loss: 0.067\n",
      "Iteration: 215 \t--- Loss: 0.069\n",
      "Iteration: 216 \t--- Loss: 0.066\n",
      "Iteration: 217 \t--- Loss: 0.067\n",
      "Iteration: 218 \t--- Loss: 0.067\n",
      "Iteration: 219 \t--- Loss: 0.068\n",
      "Iteration: 220 \t--- Loss: 0.064\n",
      "Iteration: 221 \t--- Loss: 0.071\n",
      "Iteration: 222 \t--- Loss: 0.065\n",
      "Iteration: 223 \t--- Loss: 0.070\n",
      "Iteration: 224 \t--- Loss: 0.067\n",
      "Iteration: 225 \t--- Loss: 0.068\n",
      "Iteration: 226 \t--- Loss: 0.066\n",
      "Iteration: 227 \t--- Loss: 0.071\n",
      "Iteration: 228 \t--- Loss: 0.067\n",
      "Iteration: 229 \t--- Loss: 0.063\n",
      "Iteration: 230 \t--- Loss: 0.068\n",
      "Iteration: 231 \t--- Loss: 0.067\n",
      "Iteration: 232 \t--- Loss: 0.067\n",
      "Iteration: 233 \t--- Loss: 0.064\n",
      "Iteration: 234 \t--- Loss: 0.067\n",
      "Iteration: 235 \t--- Loss: 0.071\n",
      "Iteration: 236 \t--- Loss: 0.073\n",
      "Iteration: 237 \t--- Loss: 0.067\n",
      "Iteration: 238 \t--- Loss: 0.067\n",
      "Iteration: 239 \t--- Loss: 0.067\n",
      "Iteration: 240 \t--- Loss: 0.069\n",
      "Iteration: 241 \t--- Loss: 0.064\n",
      "Iteration: 242 \t--- Loss: 0.073\n",
      "Iteration: 243 \t--- Loss: 0.070\n",
      "Iteration: 244 \t--- Loss: 0.067\n",
      "Iteration: 245 \t--- Loss: 0.063\n",
      "Iteration: 246 \t--- Loss: 0.075\n",
      "Iteration: 247 \t--- Loss: 0.066\n",
      "Iteration: 248 \t--- Loss: 0.070\n",
      "Iteration: 249 \t--- Loss: 0.068\n",
      "Iteration: 250 \t--- Loss: 0.067\n",
      "Iteration: 251 \t--- Loss: 0.064\n",
      "Iteration: 252 \t--- Loss: 0.065\n",
      "Iteration: 253 \t--- Loss: 0.070\n",
      "Iteration: 254 \t--- Loss: 0.062\n",
      "Iteration: 255 \t--- Loss: 0.068\n",
      "Iteration: 256 \t--- Loss: 0.067\n",
      "Iteration: 257 \t--- Loss: 0.064\n",
      "Iteration: 258 \t--- Loss: 0.066\n",
      "Iteration: 259 \t--- Loss: 0.070"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:09<00:23,  3.34s/it][Parallel(n_jobs=5)]: Done  82 tasks      | elapsed: 51.1min\n",
      " 30%|███       | 3/10 [00:12<00:29,  4.24s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:24<00:00, 84.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.020\n",
      "Iteration: 261 \t--- Loss: 0.019\n",
      "Iteration: 262 \t--- Loss: 0.019\n",
      "Iteration: 263 \t--- Loss: 0.017\n",
      "Iteration: 264 \t--- Loss: 0.018\n",
      "Iteration: 265 \t--- Loss: 0.019\n",
      "Iteration: 266 \t--- Loss: 0.018\n",
      "Iteration: 267 \t--- Loss: 0.016\n",
      "Iteration: 268 \t--- Loss: 0.020\n",
      "Iteration: 269 \t--- Loss: 0.017\n",
      "Iteration: 270 \t--- Loss: 0.020\n",
      "Iteration: 271 \t--- Loss: 0.019\n",
      "Iteration: 272 \t--- Loss: 0.019\n",
      "Iteration: 273 \t--- Loss: 0.020\n",
      "Iteration: 274 \t--- Loss: 0.019\n",
      "Iteration: 275 \t--- Loss: 0.018\n",
      "Iteration: 276 \t--- Loss: 0.018\n",
      "Iteration: 277 \t--- Loss: 0.017\n",
      "Iteration: 278 \t--- Loss: 0.018\n",
      "Iteration: 279 \t--- Loss: 0.018\n",
      "Iteration: 280 \t--- Loss: 0.017\n",
      "Iteration: 281 \t--- Loss: 0.020\n",
      "Iteration: 282 \t--- Loss: 0.019\n",
      "Iteration: 283 \t--- Loss: 0.018\n",
      "Iteration: 284 \t--- Loss: 0.018\n",
      "Iteration: 285 \t--- Loss: 0.020\n",
      "Iteration: 286 \t--- Loss: 0.019\n",
      "Iteration: 287 \t--- Loss: 0.019\n",
      "Iteration: 288 \t--- Loss: 0.019\n",
      "Iteration: 289 \t--- Loss: 0.017\n",
      "Iteration: 290 \t--- Loss: 0.020\n",
      "Iteration: 291 \t--- Loss: 0.018\n",
      "Iteration: 292 \t--- Loss: 0.019\n",
      "Iteration: 293 \t--- Loss: 0.020\n",
      "Iteration: 294 \t--- Loss: 0.019\n",
      "Iteration: 295 \t--- Loss: 0.019\n",
      "Iteration: 296 \t--- Loss: 0.018\n",
      "Iteration: 297 \t--- Loss: 0.017\n",
      "Iteration: 298 \t--- Loss: 0.017\n",
      "Iteration: 299 \t--- Loss: 0.018\n",
      "Iteration: 300 \t--- Loss: 0.018\n",
      "Iteration: 301 \t--- Loss: 0.018\n",
      "Iteration: 302 \t--- Loss: 0.018\n",
      "Iteration: 303 \t--- Loss: 0.019\n",
      "Iteration: 304 \t--- Loss: 0.018\n",
      "Iteration: 305 \t--- Loss: 0.019\n",
      "Iteration: 306 \t--- Loss: 0.018\n",
      "Iteration: 307 \t--- Loss: 0.018\n",
      "Iteration: 308 \t--- Loss: 0.019\n",
      "Iteration: 309 \t--- Loss: 0.019\n",
      "Iteration: 310 \t--- Loss: 0.018\n",
      "Iteration: 311 \t--- Loss: 0.018\n",
      "Iteration: 312 \t--- Loss: 0.018\n",
      "Iteration: 313 \t--- Loss: 0.018\n",
      "Iteration: 314 \t--- Loss: 0.019\n",
      "Iteration: 315 \t--- Loss: 0.018\n",
      "Iteration: 316 \t--- Loss: 0.017\n",
      "Iteration: 317 \t--- Loss: 0.019\n",
      "Iteration: 318 \t--- Loss: 0.019\n",
      "Iteration: 319 \t--- Loss: 0.018\n",
      "Iteration: 320 \t--- Loss: 0.019\n",
      "Iteration: 321 \t--- Loss: 0.019\n",
      "Iteration: 322 \t--- Loss: 0.019\n",
      "Iteration: 323 \t--- Loss: 0.018\n",
      "Iteration: 324 \t--- Loss: 0.018\n",
      "Iteration: 325 \t--- Loss: 0.018\n",
      "Iteration: 326 \t--- Loss: 0.019\n",
      "Iteration: 327 \t--- Loss: 0.018\n",
      "Iteration: 328 \t--- Loss: 0.018\n",
      "Iteration: 329 \t--- Loss: 0.017\n",
      "Iteration: 330 \t--- Loss: 0.019\n",
      "Iteration: 331 \t--- Loss: 0.018\n",
      "Iteration: 332 \t--- Loss: 0.019\n",
      "Iteration: 333 \t--- Loss: 0.018\n",
      "Iteration: 334 \t--- Loss: 0.018\n",
      "Iteration: 335 \t--- Loss: 0.019\n",
      "Iteration: 336 \t--- Loss: 0.017\n",
      "Iteration: 337 \t--- Loss: 0.019\n",
      "Iteration: 338 \t--- Loss: 0.019\n",
      "Iteration: 339 \t--- Loss: 0.020\n",
      "Iteration: 340 \t--- Loss: 0.018\n",
      "Iteration: 341 \t--- Loss: 0.019\n",
      "Iteration: 342 \t--- Loss: 0.019\n",
      "Iteration: 343 \t--- Loss: 0.018\n",
      "Iteration: 344 \t--- Loss: 0.019\n",
      "Iteration: 345 \t--- Loss: 0.018\n",
      "Iteration: 346 \t--- Loss: 0.019\n",
      "Iteration: 347 \t--- Loss: 0.019\n",
      "Iteration: 348 \t--- Loss: 0.018\n",
      "Iteration: 349 \t--- Loss: 0.018\n",
      "Iteration: 350 \t--- Loss: 0.020\n",
      "Iteration: 351 \t--- Loss: 0.018\n",
      "Iteration: 352 \t--- Loss: 0.020\n",
      "Iteration: 353 \t--- Loss: 0.019\n",
      "Iteration: 354 \t--- Loss: 0.019\n",
      "Iteration: 355 \t--- Loss: 0.018\n",
      "Iteration: 356 \t--- Loss: 0.019\n",
      "Iteration: 357 \t--- Loss: 0.019\n",
      "Iteration: 358 \t--- Loss: 0.018\n",
      "Iteration: 359 \t--- Loss: 0.020\n",
      "Iteration: 360 \t--- Loss: 0.019\n",
      "Iteration: 361 \t--- Loss: 0.019\n",
      "Iteration: 362 \t--- Loss: 0.019\n",
      "Iteration: 363 \t--- Loss: 0.019\n",
      "Iteration: 364 \t--- Loss: 0.019\n",
      "Iteration: 365 \t--- Loss: 0.017\n",
      "Iteration: 366 \t--- Loss: 0.021\n",
      "Iteration: 367 \t--- Loss: 0.018\n",
      "Iteration: 368 \t--- Loss: 0.018\n",
      "Iteration: 369 \t--- Loss: 0.020\n",
      "Iteration: 370 \t--- Loss: 0.018\n",
      "Iteration: 371 \t--- Loss: 0.018\n",
      "Iteration: 372 \t--- Loss: 0.019\n",
      "Iteration: 373 \t--- Loss: 0.018\n",
      "Iteration: 374 \t--- Loss: 0.020\n",
      "Iteration: 375 \t--- Loss: 0.017\n",
      "Iteration: 376 \t--- Loss: 0.018\n",
      "Iteration: 377 \t--- Loss: 0.018\n",
      "Iteration: 378 \t--- Loss: 0.018\n",
      "Iteration: 379 \t--- Loss: 0.017\n",
      "Iteration: 380 \t--- Loss: 0.018\n",
      "Iteration: 381 \t--- Loss: 0.016\n",
      "Iteration: 382 \t--- Loss: 0.018\n",
      "Iteration: 383 \t--- Loss: 0.018\n",
      "Iteration: 384 \t--- Loss: 0.019\n",
      "Iteration: 385 \t--- Loss: 0.019\n",
      "Iteration: 386 \t--- Loss: 0.018\n",
      "Iteration: 387 \t--- Loss: 0.016\n",
      "Iteration: 388 \t--- Loss: 0.018\n",
      "Iteration: 389 \t--- Loss: 0.019\n",
      "Iteration: 390 \t--- Loss: 0.019\n",
      "Iteration: 391 \t--- Loss: 0.017\n",
      "Iteration: 392 \t--- Loss: 0.019\n",
      "Iteration: 393 \t--- Loss: 0.018\n",
      "Iteration: 394 \t--- Loss: 0.017\n",
      "Iteration: 395 \t--- Loss: 0.019\n",
      "Iteration: 396 \t--- Loss: 0.018\n",
      "Iteration: 397 \t--- Loss: 0.018\n",
      "Iteration: 398 \t--- Loss: 0.018\n",
      "Iteration: 399 \t--- Loss: 0.019\n",
      "Iteration: 400 \t--- Loss: 0.019\n",
      "Iteration: 401 \t--- Loss: 0.018\n",
      "Iteration: 402 \t--- Loss: 0.019\n",
      "Iteration: 403 \t--- Loss: 0.017\n",
      "Iteration: 404 \t--- Loss: 0.019\n",
      "Iteration: 405 \t--- Loss: 0.018\n",
      "Iteration: 406 \t--- Loss: 0.018\n",
      "Iteration: 407 \t--- Loss: 0.019\n",
      "Iteration: 408 \t--- Loss: 0.017\n",
      "Iteration: 409 \t--- Loss: 0.020\n",
      "Iteration: 410 \t--- Loss: 0.018\n",
      "Iteration: 411 \t--- Loss: 0.018\n",
      "Iteration: 412 \t--- Loss: 0.018\n",
      "Iteration: 413 \t--- Loss: 0.019\n",
      "Iteration: 414 \t--- Loss: 0.019\n",
      "Iteration: 415 \t--- Loss: 0.020\n",
      "Iteration: 416 \t--- Loss: 0.018\n",
      "Iteration: 417 \t--- Loss: 0.018\n",
      "Iteration: 418 \t--- Loss: 0.018\n",
      "Iteration: 419 \t--- Loss: 0.019\n",
      "Iteration: 420 \t--- Loss: 0.018\n",
      "Iteration: 421 \t--- Loss: 0.018\n",
      "Iteration: 422 \t--- Loss: 0.020\n",
      "Iteration: 423 \t--- Loss: 0.019\n",
      "Iteration: 424 \t--- Loss: 0.017\n",
      "Iteration: 425 \t--- Loss: 0.017\n",
      "Iteration: 426 \t--- Loss: 0.017\n",
      "Iteration: 427 \t--- Loss: 0.019\n",
      "Iteration: 428 \t--- Loss: 0.019\n",
      "Iteration: 429 \t--- Loss: 0.018\n",
      "Iteration: 430 \t--- Loss: 0.019\n",
      "Iteration: 431 \t--- Loss: 0.017\n",
      "Iteration: 432 \t--- Loss: 0.018\n",
      "Iteration: 433 \t--- Loss: 0.020\n",
      "Iteration: 434 \t--- Loss: 0.018\n",
      "Iteration: 435 \t--- Loss: 0.020\n",
      "Iteration: 436 \t--- Loss: 0.018\n",
      "Iteration: 437 \t--- Loss: 0.019\n",
      "Iteration: 438 \t--- Loss: 0.017\n",
      "Iteration: 439 \t--- Loss: 0.019\n",
      "Iteration: 440 \t--- Loss: 0.019\n",
      "Iteration: 441 \t--- Loss: 0.018\n",
      "Iteration: 442 \t--- Loss: 0.018\n",
      "Iteration: 443 \t--- Loss: 0.018\n",
      "Iteration: 444 \t--- Loss: 0.018\n",
      "Iteration: 445 \t--- Loss: 0.017\n",
      "Iteration: 446 \t--- Loss: 0.019\n",
      "Iteration: 447 \t--- Loss: 0.019\n",
      "Iteration: 448 \t--- Loss: 0.019\n",
      "Iteration: 449 \t--- Loss: 0.020\n",
      "Iteration: 450 \t--- Loss: 0.018\n",
      "Iteration: 451 \t--- Loss: 0.021\n",
      "Iteration: 452 \t--- Loss: 0.016\n",
      "Iteration: 453 \t--- Loss: 0.020\n",
      "Iteration: 454 \t--- Loss: 0.018\n",
      "Iteration: 455 \t--- Loss: 0.018\n",
      "Iteration: 456 \t--- Loss: 0.018\n",
      "Iteration: 457 \t--- Loss: 0.019\n",
      "Iteration: 458 \t--- Loss: 0.019\n",
      "Iteration: 459 \t--- Loss: 0.018\n",
      "Iteration: 460 \t--- Loss: 0.017\n",
      "Iteration: 461 \t--- Loss: 0.017\n",
      "Iteration: 462 \t--- Loss: 0.021\n",
      "Iteration: 463 \t--- Loss: 0.015\n",
      "Iteration: 464 \t--- Loss: 0.018\n",
      "Iteration: 465 \t--- Loss: 0.018\n",
      "Iteration: 466 \t--- Loss: 0.017\n",
      "Iteration: 467 \t--- Loss: 0.017\n",
      "Iteration: 468 \t--- Loss: 0.019\n",
      "Iteration: 469 \t--- Loss: 0.018\n",
      "Iteration: 470 \t--- Loss: 0.018\n",
      "Iteration: 471 \t--- Loss: 0.018\n",
      "Iteration: 472 \t--- Loss: 0.018\n",
      "Iteration: 473 \t--- Loss: 0.018\n",
      "Iteration: 474 \t--- Loss: 0.018\n",
      "Iteration: 475 \t--- Loss: 0.018\n",
      "Iteration: 476 \t--- Loss: 0.018\n",
      "Iteration: 477 \t--- Loss: 0.019\n",
      "Iteration: 478 \t--- Loss: 0.017\n",
      "Iteration: 479 \t--- Loss: 0.017\n",
      "Iteration: 480 \t--- Loss: 0.017\n",
      "Iteration: 481 \t--- Loss: 0.018\n",
      "Iteration: 482 \t--- Loss: 0.017\n",
      "Iteration: 483 \t--- Loss: 0.019\n",
      "Iteration: 484 \t--- Loss: 0.019\n",
      "Iteration: 485 \t--- Loss: 0.019\n",
      "Iteration: 486 \t--- Loss: 0.018\n",
      "Iteration: 487 \t--- Loss: 0.017\n",
      "Iteration: 488 \t--- Loss: 0.018\n",
      "Iteration: 489 \t--- Loss: 0.018\n",
      "Iteration: 490 \t--- Loss: 0.018\n",
      "Iteration: 491 \t--- Loss: 0.018\n",
      "Iteration: 492 \t--- Loss: 0.019\n",
      "Iteration: 493 \t--- Loss: 0.019\n",
      "Iteration: 494 \t--- Loss: 0.018\n",
      "Iteration: 495 \t--- Loss: 0.019\n",
      "Iteration: 496 \t--- Loss: 0.019\n",
      "Iteration: 497 \t--- Loss: 0.019\n",
      "Iteration: 498 \t--- Loss: 0.021\n",
      "Iteration: 499 \t--- Loss: 0.018\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 3/10 [00:03<00:09,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.514\n",
      "Iteration: 1 \t--- Loss: 0.466\n",
      "Iteration: 2 \t--- Loss: 0.431\n",
      "Iteration: 3 \t--- Loss: 0.400\n",
      "Iteration: 4 \t--- Loss: 0.368\n",
      "Iteration: 5 \t--- Loss: 0.341\n",
      "Iteration: 6 \t--- Loss: 0.319\n",
      "Iteration: 7 \t--- Loss: 0.305\n",
      "Iteration: 8 \t--- Loss: 0.286\n",
      "Iteration: 9 \t--- Loss: 0.275\n",
      "Iteration: 10 \t--- Loss: 0.263\n",
      "Iteration: 11 \t--- Loss: 0.262\n",
      "Iteration: 12 \t--- Loss: 0.253\n",
      "Iteration: 13 \t--- Loss: 0.243\n",
      "Iteration: 14 \t--- Loss: 0.240\n",
      "Iteration: 15 \t--- Loss: 0.240\n",
      "Iteration: 16 \t--- Loss: 0.235\n",
      "Iteration: 17 \t--- Loss: 0.226\n",
      "Iteration: 18 \t--- Loss: 0.228\n",
      "Iteration: 19 \t--- Loss: 0.224\n",
      "Iteration: 20 \t--- Loss: 0.220\n",
      "Iteration: 21 \t--- Loss: 0.217\n",
      "Iteration: 22 \t--- Loss: 0.218\n",
      "Iteration: 23 \t--- Loss: 0.216\n",
      "Iteration: 24 \t--- Loss: 0.211\n",
      "Iteration: 25 \t--- Loss: 0.216\n",
      "Iteration: 26 \t--- Loss: 0.207\n",
      "Iteration: 27 \t--- Loss: 0.211\n",
      "Iteration: 28 \t--- Loss: 0.205\n",
      "Iteration: 29 \t--- Loss: 0.220\n",
      "Iteration: 30 \t--- Loss: 0.209\n",
      "Iteration: 31 \t--- Loss: 0.213\n",
      "Iteration: 32 \t--- Loss: 0.212\n",
      "Iteration: 33 \t--- Loss: 0.204\n",
      "Iteration: 34 \t--- Loss: 0.204\n",
      "Iteration: 35 \t--- Loss: 0.204\n",
      "Iteration: 36 \t--- Loss: 0.195\n",
      "Iteration: 37 \t--- Loss: 0.200\n",
      "Iteration: 38 \t--- Loss: 0.202\n",
      "Iteration: 39 \t--- Loss: 0.200\n",
      "Iteration: 40 \t--- Loss: 0.201\n",
      "Iteration: 41 \t--- Loss: 0.203\n",
      "Iteration: 42 \t--- Loss: 0.209\n",
      "Iteration: 43 \t--- Loss: 0.203\n",
      "Iteration: 44 \t--- Loss: 0.194\n",
      "Iteration: 45 \t--- Loss: 0.204\n",
      "Iteration: 46 \t--- Loss: 0.209\n",
      "Iteration: 47 \t--- Loss: 0.204\n",
      "Iteration: 48 \t--- Loss: 0.196\n",
      "Iteration: 49 \t--- Loss: 0.196\n",
      "Iteration: 50 \t--- Loss: 0.206\n",
      "Iteration: 51 \t--- Loss: 0.202\n",
      "Iteration: 52 \t--- Loss: 0.201\n",
      "Iteration: 53 \t--- Loss: 0.191\n",
      "Iteration: 54 \t--- Loss: 0.205\n",
      "Iteration: 55 \t--- Loss: 0.197\n",
      "Iteration: 56 \t--- Loss: 0.206\n",
      "Iteration: 57 \t--- Loss: 0.202\n",
      "Iteration: 58 \t--- Loss: 0.206\n",
      "Iteration: 59 \t--- Loss: 0.202\n",
      "Iteration: 60 \t--- Loss: 0.198\n",
      "Iteration: 61 \t--- Loss: 0.198\n",
      "Iteration: 62 \t--- Loss: 0.200\n",
      "Iteration: 63 \t--- Loss: 0.198\n",
      "Iteration: 64 \t--- Loss: 0.204\n",
      "Iteration: 65 \t--- Loss: 0.193\n",
      "Iteration: 66 \t--- Loss: 0.204\n",
      "Iteration: 67 \t--- Loss: 0.196\n",
      "Iteration: 68 \t--- Loss: 0.205\n",
      "Iteration: 69 \t--- Loss: 0.201\n",
      "Iteration: 70 \t--- Loss: 0.203\n",
      "Iteration: 71 \t--- Loss: 0.201\n",
      "Iteration: 72 \t--- Loss: 0.197\n",
      "Iteration: 73 \t--- Loss: 0.206\n",
      "Iteration: 74 \t--- Loss: 0.213\n",
      "Iteration: 75 \t--- Loss: 0.197\n",
      "Iteration: 76 \t--- Loss: 0.208\n",
      "Iteration: 77 \t--- Loss: 0.202\n",
      "Iteration: 78 \t--- Loss: 0.199\n",
      "Iteration: 79 \t--- Loss: 0.202\n",
      "Iteration: 80 \t--- Loss: 0.201\n",
      "Iteration: 81 \t--- Loss: 0.198\n",
      "Iteration: 82 \t--- Loss: 0.202\n",
      "Iteration: 83 \t--- Loss: 0.207\n",
      "Iteration: 84 \t--- Loss: 0.202\n",
      "Iteration: 85 \t--- Loss: 0.206\n",
      "Iteration: 86 \t--- Loss: 0.204\n",
      "Iteration: 87 \t--- Loss: 0.206\n",
      "Iteration: 88 \t--- Loss: 0.200\n",
      "Iteration: 89 \t--- Loss: 0.204\n",
      "Iteration: 90 \t--- Loss: 0.209\n",
      "Iteration: 91 \t--- Loss: 0.200\n",
      "Iteration: 92 \t--- Loss: 0.196\n",
      "Iteration: 93 \t--- Loss: 0.201\n",
      "Iteration: 94 \t--- Loss: 0.203\n",
      "Iteration: 95 \t--- Loss: 0.194\n",
      "Iteration: 96 \t--- Loss: 0.189\n",
      "Iteration: 97 \t--- Loss: 0.199\n",
      "Iteration: 98 \t--- Loss: 0.197\n",
      "Iteration: 99 \t--- Loss: 0.194\n",
      "Iteration: 100 \t--- Loss: 0.198\n",
      "Iteration: 101 \t--- Loss: 0.201\n",
      "Iteration: 102 \t--- Loss: 0.201\n",
      "Iteration: 103 \t--- Loss: 0.199\n",
      "Iteration: 104 \t--- Loss: 0.196\n",
      "Iteration: 105 \t--- Loss: 0.201\n",
      "Iteration: 106 \t--- Loss: 0.207\n",
      "Iteration: 107 \t--- Loss: 0.198\n",
      "Iteration: 108 \t--- Loss: 0.202\n",
      "Iteration: 109 \t--- Loss: 0.202\n",
      "Iteration: 110 \t--- Loss: 0.203\n",
      "Iteration: 111 \t--- Loss: 0.195\n",
      "Iteration: 112 \t--- Loss: 0.199\n",
      "Iteration: 113 \t--- Loss: 0.195\n",
      "Iteration: 114 \t--- Loss: 0.198\n",
      "Iteration: 115 \t--- Loss: 0.197\n",
      "Iteration: 116 \t--- Loss: 0.202\n",
      "Iteration: 117 \t--- Loss: 0.200\n",
      "Iteration: 118 \t--- Loss: 0.203\n",
      "Iteration: 119 \t--- Loss: 0.200\n",
      "Iteration: 120 \t--- Loss: 0.200\n",
      "Iteration: 121 \t--- Loss: 0.210\n",
      "Iteration: 122 \t--- Loss: 0.205\n",
      "Iteration: 123 \t--- Loss: 0.199\n",
      "Iteration: 124 \t--- Loss: 0.193\n",
      "Iteration: 125 \t--- Loss: 0.198\n",
      "Iteration: 126 \t--- Loss: 0.203\n",
      "Iteration: 127 \t--- Loss: 0.197\n",
      "Iteration: 128 \t--- Loss: 0.195\n",
      "Iteration: 129 \t--- Loss: 0.198\n",
      "Iteration: 130 \t--- Loss: 0.198\n",
      "Iteration: 131 \t--- Loss: 0.200\n",
      "Iteration: 132 \t--- Loss: 0.199\n",
      "Iteration: 133 \t--- Loss: 0.204\n",
      "Iteration: 134 \t--- Loss: 0.195\n",
      "Iteration: 135 \t--- Loss: 0.198\n",
      "Iteration: 136 \t--- Loss: 0.204\n",
      "Iteration: 137 \t--- Loss: 0.195\n",
      "Iteration: 138 \t--- Loss: 0.206\n",
      "Iteration: 139 \t--- Loss: 0.198\n",
      "Iteration: 140 \t--- Loss: 0.199\n",
      "Iteration: 141 \t--- Loss: 0.189\n",
      "Iteration: 142 \t--- Loss: 0.190\n",
      "Iteration: 143 \t--- Loss: 0.199\n",
      "Iteration: 144 \t--- Loss: 0.198\n",
      "Iteration: 145 \t--- Loss: 0.201\n",
      "Iteration: 146 \t--- Loss: 0.203\n",
      "Iteration: 147 \t--- Loss: 0.202\n",
      "Iteration: 148 \t--- Loss: 0.201\n",
      "Iteration: 149 \t--- Loss: 0.206\n",
      "Iteration: 150 \t--- Loss: 0.197\n",
      "Iteration: 151 \t--- Loss: 0.194\n",
      "Iteration: 152 \t--- Loss: 0.202\n",
      "Iteration: 153 \t--- Loss: 0.200\n",
      "Iteration: 154 \t--- Loss: 0.195\n",
      "Iteration: 155 \t--- Loss: 0.204\n",
      "Iteration: 156 \t--- Loss: 0.195\n",
      "Iteration: 157 \t--- Loss: 0.194\n",
      "Iteration: 158 \t--- Loss: 0.202\n",
      "Iteration: 159 \t--- Loss: 0.202\n",
      "Iteration: 160 \t--- Loss: 0.192\n",
      "Iteration: 161 \t--- Loss: 0.204\n",
      "Iteration: 162 \t--- Loss: 0.195\n",
      "Iteration: 163 \t--- Loss: 0.197\n",
      "Iteration: 164 \t--- Loss: 0.205\n",
      "Iteration: 165 \t--- Loss: 0.204\n",
      "Iteration: 166 \t--- Loss: 0.209\n",
      "Iteration: 167 \t--- Loss: 0.200\n",
      "Iteration: 168 \t--- Loss: 0.200\n",
      "Iteration: 169 \t--- Loss: 0.200\n",
      "Iteration: 170 \t--- Loss: 0.206\n",
      "Iteration: 171 \t--- Loss: 0.193\n",
      "Iteration: 172 \t--- Loss: 0.206\n",
      "Iteration: 173 \t--- Loss: 0.197\n",
      "Iteration: 174 \t--- Loss: 0.200\n",
      "Iteration: 175 \t--- Loss: 0.195\n",
      "Iteration: 176 \t--- Loss: 0.199\n",
      "Iteration: 177 \t--- Loss: 0.193\n",
      "Iteration: 178 \t--- Loss: 0.211\n",
      "Iteration: 179 \t--- Loss: 0.201\n",
      "Iteration: 180 \t--- Loss: 0.202\n",
      "Iteration: 181 \t--- Loss: 0.199\n",
      "Iteration: 182 \t--- Loss: 0.198\n",
      "Iteration: 183 \t--- Loss: 0.191\n",
      "Iteration: 184 \t--- Loss: 0.197\n",
      "Iteration: 185 \t--- Loss: 0.202\n",
      "Iteration: 186 \t--- Loss: 0.197\n",
      "Iteration: 187 \t--- Loss: 0.198\n",
      "Iteration: 188 \t--- Loss: 0.205\n",
      "Iteration: 189 \t--- Loss: 0.207\n",
      "Iteration: 190 \t--- Loss: 0.188\n",
      "Iteration: 191 \t--- Loss: 0.210\n",
      "Iteration: 192 \t--- Loss: 0.198\n",
      "Iteration: 193 \t--- Loss: 0.207\n",
      "Iteration: 194 \t--- Loss: 0.207\n",
      "Iteration: 195 \t--- Loss: 0.200\n",
      "Iteration: 196 \t--- Loss: 0.202\n",
      "Iteration: 197 \t--- Loss: 0.191\n",
      "Iteration: 198 \t--- Loss: 0.199\n",
      "Iteration: 199 \t--- Loss: 0.202\n",
      "Iteration: 200 \t--- Loss: 0.201\n",
      "Iteration: 201 \t--- Loss: 0.203\n",
      "Iteration: 202 \t--- Loss: 0.200\n",
      "Iteration: 203 \t--- Loss: 0.206\n",
      "Iteration: 204 \t--- Loss: 0.206\n",
      "Iteration: 205 \t--- Loss: 0.200\n",
      "Iteration: 206 \t--- Loss: 0.202\n",
      "Iteration: 207 \t--- Loss: 0.197\n",
      "Iteration: 208 \t--- Loss: 0.198\n",
      "Iteration: 209 \t--- Loss: 0.194\n",
      "Iteration: 210 \t--- Loss: 0.203\n",
      "Iteration: 211 \t--- Loss: 0.200\n",
      "Iteration: 212 \t--- Loss: 0.195\n",
      "Iteration: 213 \t--- Loss: 0.197\n",
      "Iteration: 214 \t--- Loss: 0.212\n",
      "Iteration: 215 \t--- Loss: 0.200\n",
      "Iteration: 216 \t--- Loss: 0.200\n",
      "Iteration: 217 \t--- Loss: 0.189\n",
      "Iteration: 218 \t--- Loss: 0.207\n",
      "Iteration: 219 \t--- Loss: 0.191\n",
      "Iteration: 220 \t--- Loss: 0.197\n",
      "Iteration: 221 \t--- Loss: 0.206\n",
      "Iteration: 222 \t--- Loss: 0.202\n",
      "Iteration: 223 \t--- Loss: 0.191\n",
      "Iteration: 224 \t--- Loss: 0.205\n",
      "Iteration: 225 \t--- Loss: 0.197\n",
      "Iteration: 226 \t--- Loss: 0.204\n",
      "Iteration: 227 \t--- Loss: 0.198\n",
      "Iteration: 228 \t--- Loss: 0.202\n",
      "Iteration: 229 \t--- Loss: 0.202\n",
      "Iteration: 230 \t--- Loss: 0.186\n",
      "Iteration: 231 \t--- Loss: 0.197\n",
      "Iteration: 232 \t--- Loss: 0.201\n",
      "Iteration: 233 \t--- Loss: 0.195\n",
      "Iteration: 234 \t--- Loss: 0.187\n",
      "Iteration: 235 \t--- Loss: 0.203\n",
      "Iteration: 236 \t--- Loss: 0.200\n",
      "Iteration: 237 \t--- Loss: 0.192\n",
      "Iteration: 238 \t--- Loss: 0.188\n",
      "Iteration: 239 \t--- Loss: 0.202\n",
      "Iteration: 240 \t--- Loss: 0.200\n",
      "Iteration: 241 \t--- Loss: 0.200\n",
      "Iteration: 242 \t--- Loss: 0.201\n",
      "Iteration: 243 \t--- Loss: 0.200\n",
      "Iteration: 244 \t--- Loss: 0.202\n",
      "Iteration: 245 \t--- Loss: 0.194\n",
      "Iteration: 246 \t--- Loss: 0.193\n",
      "Iteration: 247 \t--- Loss: 0.197\n",
      "Iteration: 248 \t--- Loss: 0.195\n",
      "Iteration: 249 \t--- Loss: 0.191\n",
      "Iteration: 250 \t--- Loss: 0.208\n",
      "Iteration: 251 \t--- Loss: 0.203\n",
      "Iteration: 252 \t--- Loss: 0.200\n",
      "Iteration: 253 \t--- Loss: 0.202\n",
      "Iteration: 254 \t--- Loss: 0.196\n",
      "Iteration: 255 \t--- Loss: 0.205\n",
      "Iteration: 256 \t--- Loss: 0.203\n",
      "Iteration: 257 \t--- Loss: 0.200\n",
      "Iteration: 258 \t--- Loss: 0.199\n",
      "Iteration: 259 \t--- Loss: 0.194"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:36<00:00, 96.23s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.066\n",
      "Iteration: 261 \t--- Loss: 0.064\n",
      "Iteration: 262 \t--- Loss: 0.071\n",
      "Iteration: 263 \t--- Loss: 0.065\n",
      "Iteration: 264 \t--- Loss: 0.068\n",
      "Iteration: 265 \t--- Loss: 0.068\n",
      "Iteration: 266 \t--- Loss: 0.067\n",
      "Iteration: 267 \t--- Loss: 0.063\n",
      "Iteration: 268 \t--- Loss: 0.067\n",
      "Iteration: 269 \t--- Loss: 0.064\n",
      "Iteration: 270 \t--- Loss: 0.066\n",
      "Iteration: 271 \t--- Loss: 0.067\n",
      "Iteration: 272 \t--- Loss: 0.069\n",
      "Iteration: 273 \t--- Loss: 0.068\n",
      "Iteration: 274 \t--- Loss: 0.068\n",
      "Iteration: 275 \t--- Loss: 0.067\n",
      "Iteration: 276 \t--- Loss: 0.064\n",
      "Iteration: 277 \t--- Loss: 0.068\n",
      "Iteration: 278 \t--- Loss: 0.068\n",
      "Iteration: 279 \t--- Loss: 0.070\n",
      "Iteration: 280 \t--- Loss: 0.065\n",
      "Iteration: 281 \t--- Loss: 0.062\n",
      "Iteration: 282 \t--- Loss: 0.068\n",
      "Iteration: 283 \t--- Loss: 0.064\n",
      "Iteration: 284 \t--- Loss: 0.061\n",
      "Iteration: 285 \t--- Loss: 0.062\n",
      "Iteration: 286 \t--- Loss: 0.065\n",
      "Iteration: 287 \t--- Loss: 0.066\n",
      "Iteration: 288 \t--- Loss: 0.067\n",
      "Iteration: 289 \t--- Loss: 0.065\n",
      "Iteration: 290 \t--- Loss: 0.062\n",
      "Iteration: 291 \t--- Loss: 0.066\n",
      "Iteration: 292 \t--- Loss: 0.069\n",
      "Iteration: 293 \t--- Loss: 0.064\n",
      "Iteration: 294 \t--- Loss: 0.063\n",
      "Iteration: 295 \t--- Loss: 0.066\n",
      "Iteration: 296 \t--- Loss: 0.068\n",
      "Iteration: 297 \t--- Loss: 0.061\n",
      "Iteration: 298 \t--- Loss: 0.067\n",
      "Iteration: 299 \t--- Loss: 0.065\n",
      "Iteration: 300 \t--- Loss: 0.064\n",
      "Iteration: 301 \t--- Loss: 0.066\n",
      "Iteration: 302 \t--- Loss: 0.068\n",
      "Iteration: 303 \t--- Loss: 0.063\n",
      "Iteration: 304 \t--- Loss: 0.062\n",
      "Iteration: 305 \t--- Loss: 0.059\n",
      "Iteration: 306 \t--- Loss: 0.065\n",
      "Iteration: 307 \t--- Loss: 0.058\n",
      "Iteration: 308 \t--- Loss: 0.061\n",
      "Iteration: 309 \t--- Loss: 0.061\n",
      "Iteration: 310 \t--- Loss: 0.062\n",
      "Iteration: 311 \t--- Loss: 0.062\n",
      "Iteration: 312 \t--- Loss: 0.064\n",
      "Iteration: 313 \t--- Loss: 0.066\n",
      "Iteration: 314 \t--- Loss: 0.061\n",
      "Iteration: 315 \t--- Loss: 0.063\n",
      "Iteration: 316 \t--- Loss: 0.065\n",
      "Iteration: 317 \t--- Loss: 0.066\n",
      "Iteration: 318 \t--- Loss: 0.068\n",
      "Iteration: 319 \t--- Loss: 0.063\n",
      "Iteration: 320 \t--- Loss: 0.066\n",
      "Iteration: 321 \t--- Loss: 0.059\n",
      "Iteration: 322 \t--- Loss: 0.058\n",
      "Iteration: 323 \t--- Loss: 0.059\n",
      "Iteration: 324 \t--- Loss: 0.065\n",
      "Iteration: 325 \t--- Loss: 0.063\n",
      "Iteration: 326 \t--- Loss: 0.060\n",
      "Iteration: 327 \t--- Loss: 0.062\n",
      "Iteration: 328 \t--- Loss: 0.063\n",
      "Iteration: 329 \t--- Loss: 0.064\n",
      "Iteration: 330 \t--- Loss: 0.061\n",
      "Iteration: 331 \t--- Loss: 0.064\n",
      "Iteration: 332 \t--- Loss: 0.061\n",
      "Iteration: 333 \t--- Loss: 0.061\n",
      "Iteration: 334 \t--- Loss: 0.059\n",
      "Iteration: 335 \t--- Loss: 0.057\n",
      "Iteration: 336 \t--- Loss: 0.058\n",
      "Iteration: 337 \t--- Loss: 0.062\n",
      "Iteration: 338 \t--- Loss: 0.059\n",
      "Iteration: 339 \t--- Loss: 0.057\n",
      "Iteration: 340 \t--- Loss: 0.064\n",
      "Iteration: 341 \t--- Loss: 0.059\n",
      "Iteration: 342 \t--- Loss: 0.059\n",
      "Iteration: 343 \t--- Loss: 0.055\n",
      "Iteration: 344 \t--- Loss: 0.058\n",
      "Iteration: 345 \t--- Loss: 0.057\n",
      "Iteration: 346 \t--- Loss: 0.059\n",
      "Iteration: 347 \t--- Loss: 0.058\n",
      "Iteration: 348 \t--- Loss: 0.057\n",
      "Iteration: 349 \t--- Loss: 0.061\n",
      "Iteration: 350 \t--- Loss: 0.057\n",
      "Iteration: 351 \t--- Loss: 0.055\n",
      "Iteration: 352 \t--- Loss: 0.059\n",
      "Iteration: 353 \t--- Loss: 0.057\n",
      "Iteration: 354 \t--- Loss: 0.062\n",
      "Iteration: 355 \t--- Loss: 0.056\n",
      "Iteration: 356 \t--- Loss: 0.056\n",
      "Iteration: 357 \t--- Loss: 0.054\n",
      "Iteration: 358 \t--- Loss: 0.058\n",
      "Iteration: 359 \t--- Loss: 0.055\n",
      "Iteration: 360 \t--- Loss: 0.050\n",
      "Iteration: 361 \t--- Loss: 0.060\n",
      "Iteration: 362 \t--- Loss: 0.056\n",
      "Iteration: 363 \t--- Loss: 0.051\n",
      "Iteration: 364 \t--- Loss: 0.053\n",
      "Iteration: 365 \t--- Loss: 0.055\n",
      "Iteration: 366 \t--- Loss: 0.055\n",
      "Iteration: 367 \t--- Loss: 0.052\n",
      "Iteration: 368 \t--- Loss: 0.056\n",
      "Iteration: 369 \t--- Loss: 0.054\n",
      "Iteration: 370 \t--- Loss: 0.056\n",
      "Iteration: 371 \t--- Loss: 0.054\n",
      "Iteration: 372 \t--- Loss: 0.049\n",
      "Iteration: 373 \t--- Loss: 0.053\n",
      "Iteration: 374 \t--- Loss: 0.050\n",
      "Iteration: 375 \t--- Loss: 0.051\n",
      "Iteration: 376 \t--- Loss: 0.052\n",
      "Iteration: 377 \t--- Loss: 0.048\n",
      "Iteration: 378 \t--- Loss: 0.051\n",
      "Iteration: 379 \t--- Loss: 0.054\n",
      "Iteration: 380 \t--- Loss: 0.049\n",
      "Iteration: 381 \t--- Loss: 0.051\n",
      "Iteration: 382 \t--- Loss: 0.051\n",
      "Iteration: 383 \t--- Loss: 0.047\n",
      "Iteration: 384 \t--- Loss: 0.051\n",
      "Iteration: 385 \t--- Loss: 0.051\n",
      "Iteration: 386 \t--- Loss: 0.052\n",
      "Iteration: 387 \t--- Loss: 0.054\n",
      "Iteration: 388 \t--- Loss: 0.052\n",
      "Iteration: 389 \t--- Loss: 0.048\n",
      "Iteration: 390 \t--- Loss: 0.050\n",
      "Iteration: 391 \t--- Loss: 0.050\n",
      "Iteration: 392 \t--- Loss: 0.046\n",
      "Iteration: 393 \t--- Loss: 0.049\n",
      "Iteration: 394 \t--- Loss: 0.049\n",
      "Iteration: 395 \t--- Loss: 0.049\n",
      "Iteration: 396 \t--- Loss: 0.052\n",
      "Iteration: 397 \t--- Loss: 0.048\n",
      "Iteration: 398 \t--- Loss: 0.048\n",
      "Iteration: 399 \t--- Loss: 0.047\n",
      "Iteration: 400 \t--- Loss: 0.048\n",
      "Iteration: 401 \t--- Loss: 0.049\n",
      "Iteration: 402 \t--- Loss: 0.049\n",
      "Iteration: 403 \t--- Loss: 0.048\n",
      "Iteration: 404 \t--- Loss: 0.047\n",
      "Iteration: 405 \t--- Loss: 0.048\n",
      "Iteration: 406 \t--- Loss: 0.053\n",
      "Iteration: 407 \t--- Loss: 0.049\n",
      "Iteration: 408 \t--- Loss: 0.048\n",
      "Iteration: 409 \t--- Loss: 0.051\n",
      "Iteration: 410 \t--- Loss: 0.050\n",
      "Iteration: 411 \t--- Loss: 0.050\n",
      "Iteration: 412 \t--- Loss: 0.051\n",
      "Iteration: 413 \t--- Loss: 0.049\n",
      "Iteration: 414 \t--- Loss: 0.050\n",
      "Iteration: 415 \t--- Loss: 0.047\n",
      "Iteration: 416 \t--- Loss: 0.047\n",
      "Iteration: 417 \t--- Loss: 0.046\n",
      "Iteration: 418 \t--- Loss: 0.046\n",
      "Iteration: 419 \t--- Loss: 0.046\n",
      "Iteration: 420 \t--- Loss: 0.048\n",
      "Iteration: 421 \t--- Loss: 0.046\n",
      "Iteration: 422 \t--- Loss: 0.047\n",
      "Iteration: 423 \t--- Loss: 0.048\n",
      "Iteration: 424 \t--- Loss: 0.048\n",
      "Iteration: 425 \t--- Loss: 0.047\n",
      "Iteration: 426 \t--- Loss: 0.043\n",
      "Iteration: 427 \t--- Loss: 0.045\n",
      "Iteration: 428 \t--- Loss: 0.044\n",
      "Iteration: 429 \t--- Loss: 0.049\n",
      "Iteration: 430 \t--- Loss: 0.046\n",
      "Iteration: 431 \t--- Loss: 0.045\n",
      "Iteration: 432 \t--- Loss: 0.045\n",
      "Iteration: 433 \t--- Loss: 0.050\n",
      "Iteration: 434 \t--- Loss: 0.049\n",
      "Iteration: 435 \t--- Loss: 0.047\n",
      "Iteration: 436 \t--- Loss: 0.044\n",
      "Iteration: 437 \t--- Loss: 0.046\n",
      "Iteration: 438 \t--- Loss: 0.043\n",
      "Iteration: 439 \t--- Loss: 0.048\n",
      "Iteration: 440 \t--- Loss: 0.043\n",
      "Iteration: 441 \t--- Loss: 0.047\n",
      "Iteration: 442 \t--- Loss: 0.044\n",
      "Iteration: 443 \t--- Loss: 0.043\n",
      "Iteration: 444 \t--- Loss: 0.045\n",
      "Iteration: 445 \t--- Loss: 0.045\n",
      "Iteration: 446 \t--- Loss: 0.044\n",
      "Iteration: 447 \t--- Loss: 0.043\n",
      "Iteration: 448 \t--- Loss: 0.044\n",
      "Iteration: 449 \t--- Loss: 0.046\n",
      "Iteration: 450 \t--- Loss: 0.046\n",
      "Iteration: 451 \t--- Loss: 0.048\n",
      "Iteration: 452 \t--- Loss: 0.042\n",
      "Iteration: 453 \t--- Loss: 0.045\n",
      "Iteration: 454 \t--- Loss: 0.046\n",
      "Iteration: 455 \t--- Loss: 0.045\n",
      "Iteration: 456 \t--- Loss: 0.045\n",
      "Iteration: 457 \t--- Loss: 0.046\n",
      "Iteration: 458 \t--- Loss: 0.043\n",
      "Iteration: 459 \t--- Loss: 0.045\n",
      "Iteration: 460 \t--- Loss: 0.043\n",
      "Iteration: 461 \t--- Loss: 0.044\n",
      "Iteration: 462 \t--- Loss: 0.044\n",
      "Iteration: 463 \t--- Loss: 0.046\n",
      "Iteration: 464 \t--- Loss: 0.044\n",
      "Iteration: 465 \t--- Loss: 0.044\n",
      "Iteration: 466 \t--- Loss: 0.041\n",
      "Iteration: 467 \t--- Loss: 0.045\n",
      "Iteration: 468 \t--- Loss: 0.045\n",
      "Iteration: 469 \t--- Loss: 0.044\n",
      "Iteration: 470 \t--- Loss: 0.042\n",
      "Iteration: 471 \t--- Loss: 0.045\n",
      "Iteration: 472 \t--- Loss: 0.046\n",
      "Iteration: 473 \t--- Loss: 0.043\n",
      "Iteration: 474 \t--- Loss: 0.042\n",
      "Iteration: 475 \t--- Loss: 0.043\n",
      "Iteration: 476 \t--- Loss: 0.041\n",
      "Iteration: 477 \t--- Loss: 0.043\n",
      "Iteration: 478 \t--- Loss: 0.040\n",
      "Iteration: 479 \t--- Loss: 0.042\n",
      "Iteration: 480 \t--- Loss: 0.045\n",
      "Iteration: 481 \t--- Loss: 0.042\n",
      "Iteration: 482 \t--- Loss: 0.045\n",
      "Iteration: 483 \t--- Loss: 0.041\n",
      "Iteration: 484 \t--- Loss: 0.040\n",
      "Iteration: 485 \t--- Loss: 0.041\n",
      "Iteration: 486 \t--- Loss: 0.041\n",
      "Iteration: 487 \t--- Loss: 0.042\n",
      "Iteration: 488 \t--- Loss: 0.045\n",
      "Iteration: 489 \t--- Loss: 0.043\n",
      "Iteration: 490 \t--- Loss: 0.043\n",
      "Iteration: 491 \t--- Loss: 0.045\n",
      "Iteration: 492 \t--- Loss: 0.040\n",
      "Iteration: 493 \t--- Loss: 0.039\n",
      "Iteration: 494 \t--- Loss: 0.041\n",
      "Iteration: 495 \t--- Loss: 0.038\n",
      "Iteration: 496 \t--- Loss: 0.040\n",
      "Iteration: 497 \t--- Loss: 0.043\n",
      "Iteration: 498 \t--- Loss: 0.039\n",
      "Iteration: 499 \t--- Loss: 0.043\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it][Parallel(n_jobs=5)]: Done  83 tasks      | elapsed: 51.8min\n",
      " 40%|████      | 4/10 [00:05<00:08,  1.46s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:11<00:03,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 1.268\n",
      "Iteration: 1 \t--- Loss: 1.195\n",
      "Iteration: 2 \t--- Loss: 1.084\n",
      "Iteration: 3 \t--- Loss: 1.055\n",
      "Iteration: 4 \t--- Loss: 0.995\n",
      "Iteration: 5 \t--- Loss: 0.940\n",
      "Iteration: 6 \t--- Loss: 0.907\n",
      "Iteration: 7 \t--- Loss: 0.880\n",
      "Iteration: 8 \t--- Loss: 0.856\n",
      "Iteration: 9 \t--- Loss: 0.834\n",
      "Iteration: 10 \t--- Loss: 0.826\n",
      "Iteration: 11 \t--- Loss: 0.817\n",
      "Iteration: 12 \t--- Loss: 0.811\n",
      "Iteration: 13 \t--- Loss: 0.796\n",
      "Iteration: 14 \t--- Loss: 0.798\n",
      "Iteration: 15 \t--- Loss: 0.794\n",
      "Iteration: 16 \t--- Loss: 0.790\n",
      "Iteration: 17 \t--- Loss: 0.789\n",
      "Iteration: 18 \t--- Loss: 0.787\n",
      "Iteration: 19 \t--- Loss: 0.787\n",
      "Iteration: 20 \t--- Loss: 0.785\n",
      "Iteration: 21 \t--- Loss: 0.783\n",
      "Iteration: 22 \t--- Loss: 0.786\n",
      "Iteration: 23 \t--- Loss: 0.783\n",
      "Iteration: 24 \t--- Loss: 0.782\n",
      "Iteration: 25 \t--- Loss: 0.781\n",
      "Iteration: 26 \t--- Loss: 0.781\n",
      "Iteration: 27 \t--- Loss: 0.777\n",
      "Iteration: 28 \t--- Loss: 0.777\n",
      "Iteration: 29 \t--- Loss: 0.774\n",
      "Iteration: 30 \t--- Loss: 0.777\n",
      "Iteration: 31 \t--- Loss: 0.771\n",
      "Iteration: 32 \t--- Loss: 0.774\n",
      "Iteration: 33 \t--- Loss: 0.774\n",
      "Iteration: 34 \t--- Loss: 0.772\n",
      "Iteration: 35 \t--- Loss: 0.777\n",
      "Iteration: 36 \t--- Loss: 0.778\n",
      "Iteration: 37 \t--- Loss: 0.774\n",
      "Iteration: 38 \t--- Loss: 0.772\n",
      "Iteration: 39 \t--- Loss: 0.775\n",
      "Iteration: 40 \t--- Loss: 0.771\n",
      "Iteration: 41 \t--- Loss: 0.772\n",
      "Iteration: 42 \t--- Loss: 0.775\n",
      "Iteration: 43 \t--- Loss: 0.778\n",
      "Iteration: 44 \t--- Loss: 0.773\n",
      "Iteration: 45 \t--- Loss: 0.773\n",
      "Iteration: 46 \t--- Loss: 0.773\n",
      "Iteration: 47 \t--- Loss: 0.769\n",
      "Iteration: 48 \t--- Loss: 0.773\n",
      "Iteration: 49 \t--- Loss: 0.767\n",
      "Iteration: 50 \t--- Loss: 0.780\n",
      "Iteration: 51 \t--- Loss: 0.771\n",
      "Iteration: 52 \t--- Loss: 0.773\n",
      "Iteration: 53 \t--- Loss: 0.770\n",
      "Iteration: 54 \t--- Loss: 0.780\n",
      "Iteration: 55 \t--- Loss: 0.770\n",
      "Iteration: 56 \t--- Loss: 0.774\n",
      "Iteration: 57 \t--- Loss: 0.772\n",
      "Iteration: 58 \t--- Loss: 0.777\n",
      "Iteration: 59 \t--- Loss: 0.774\n",
      "Iteration: 60 \t--- Loss: 0.772\n",
      "Iteration: 61 \t--- Loss: 0.773\n",
      "Iteration: 62 \t--- Loss: 0.768\n",
      "Iteration: 63 \t--- Loss: 0.778\n",
      "Iteration: 64 \t--- Loss: 0.765\n",
      "Iteration: 65 \t--- Loss: 0.773\n",
      "Iteration: 66 \t--- Loss: 0.773\n",
      "Iteration: 67 \t--- Loss: 0.778\n",
      "Iteration: 68 \t--- Loss: 0.772\n",
      "Iteration: 69 \t--- Loss: 0.775\n",
      "Iteration: 70 \t--- Loss: 0.772\n",
      "Iteration: 71 \t--- Loss: 0.777\n",
      "Iteration: 72 \t--- Loss: 0.771\n",
      "Iteration: 73 \t--- Loss: 0.767\n",
      "Iteration: 74 \t--- Loss: 0.775\n",
      "Iteration: 75 \t--- Loss: 0.773\n",
      "Iteration: 76 \t--- Loss: 0.770\n",
      "Iteration: 77 \t--- Loss: 0.769\n",
      "Iteration: 78 \t--- Loss: 0.774\n",
      "Iteration: 79 \t--- Loss: 0.775\n",
      "Iteration: 80 \t--- Loss: 0.774\n",
      "Iteration: 81 \t--- Loss: 0.771\n",
      "Iteration: 82 \t--- Loss: 0.772\n",
      "Iteration: 83 \t--- Loss: 0.776\n",
      "Iteration: 84 \t--- Loss: 0.771\n",
      "Iteration: 85 \t--- Loss: 0.770\n",
      "Iteration: 86 \t--- Loss: 0.769\n",
      "Iteration: 87 \t--- Loss: 0.772\n",
      "Iteration: 88 \t--- Loss: 0.769\n",
      "Iteration: 89 \t--- Loss: 0.772\n",
      "Iteration: 90 \t--- Loss: 0.775\n",
      "Iteration: 91 \t--- Loss: 0.769\n",
      "Iteration: 92 \t--- Loss: 0.783\n",
      "Iteration: 93 \t--- Loss: 0.768\n",
      "Iteration: 94 \t--- Loss: 0.766\n",
      "Iteration: 95 \t--- Loss: 0.775\n",
      "Iteration: 96 \t--- Loss: 0.771\n",
      "Iteration: 97 \t--- Loss: 0.775\n",
      "Iteration: 98 \t--- Loss: 0.770\n",
      "Iteration: 99 \t--- Loss: 0.773\n",
      "Iteration: 100 \t--- Loss: 0.768\n",
      "Iteration: 101 \t--- Loss: 0.771\n",
      "Iteration: 102 \t--- Loss: 0.773\n",
      "Iteration: 103 \t--- Loss: 0.773\n",
      "Iteration: 104 \t--- Loss: 0.777\n",
      "Iteration: 105 \t--- Loss: 0.772\n",
      "Iteration: 106 \t--- Loss: 0.772\n",
      "Iteration: 107 \t--- Loss: 0.778\n",
      "Iteration: 108 \t--- Loss: 0.778\n",
      "Iteration: 109 \t--- Loss: 0.774\n",
      "Iteration: 110 \t--- Loss: 0.769\n",
      "Iteration: 111 \t--- Loss: 0.774\n",
      "Iteration: 112 \t--- Loss: 0.770\n",
      "Iteration: 113 \t--- Loss: 0.772\n",
      "Iteration: 114 \t--- Loss: 0.782\n",
      "Iteration: 115 \t--- Loss: 0.774\n",
      "Iteration: 116 \t--- Loss: 0.771\n",
      "Iteration: 117 \t--- Loss: 0.771\n",
      "Iteration: 118 \t--- Loss: 0.768\n",
      "Iteration: 119 \t--- Loss: 0.775\n",
      "Iteration: 120 \t--- Loss: 0.772\n",
      "Iteration: 121 \t--- Loss: 0.775\n",
      "Iteration: 122 \t--- Loss: 0.772\n",
      "Iteration: 123 \t--- Loss: 0.766\n",
      "Iteration: 124 \t--- Loss: 0.778\n",
      "Iteration: 125 \t--- Loss: 0.780\n",
      "Iteration: 126 \t--- Loss: 0.768\n",
      "Iteration: 127 \t--- Loss: 0.769\n",
      "Iteration: 128 \t--- Loss: 0.777\n",
      "Iteration: 129 \t--- Loss: 0.772\n",
      "Iteration: 130 \t--- Loss: 0.776\n",
      "Iteration: 131 \t--- Loss: 0.775\n",
      "Iteration: 132 \t--- Loss: 0.775\n",
      "Iteration: 133 \t--- Loss: 0.773\n",
      "Iteration: 134 \t--- Loss: 0.777\n",
      "Iteration: 135 \t--- Loss: 0.772\n",
      "Iteration: 136 \t--- Loss: 0.771\n",
      "Iteration: 137 \t--- Loss: 0.774\n",
      "Iteration: 138 \t--- Loss: 0.769\n",
      "Iteration: 139 \t--- Loss: 0.771\n",
      "Iteration: 140 \t--- Loss: 0.775\n",
      "Iteration: 141 \t--- Loss: 0.774\n",
      "Iteration: 142 \t--- Loss: 0.776\n",
      "Iteration: 143 \t--- Loss: 0.764\n",
      "Iteration: 144 \t--- Loss: 0.777\n",
      "Iteration: 145 \t--- Loss: 0.772\n",
      "Iteration: 146 \t--- Loss: 0.770\n",
      "Iteration: 147 \t--- Loss: 0.776\n",
      "Iteration: 148 \t--- Loss: 0.773\n",
      "Iteration: 149 \t--- Loss: 0.769\n",
      "Iteration: 150 \t--- Loss: 0.771\n",
      "Iteration: 151 \t--- Loss: 0.772\n",
      "Iteration: 152 \t--- Loss: 0.770\n",
      "Iteration: 153 \t--- Loss: 0.768\n",
      "Iteration: 154 \t--- Loss: 0.775\n",
      "Iteration: 155 \t--- Loss: 0.776\n",
      "Iteration: 156 \t--- Loss: 0.768\n",
      "Iteration: 157 \t--- Loss: 0.771\n",
      "Iteration: 158 \t--- Loss: 0.774\n",
      "Iteration: 159 \t--- Loss: 0.772\n",
      "Iteration: 160 \t--- Loss: 0.778\n",
      "Iteration: 161 \t--- Loss: 0.775\n",
      "Iteration: 162 \t--- Loss: 0.774\n",
      "Iteration: 163 \t--- Loss: 0.767\n",
      "Iteration: 164 \t--- Loss: 0.767\n",
      "Iteration: 165 \t--- Loss: 0.772\n",
      "Iteration: 166 \t--- Loss: 0.775\n",
      "Iteration: 167 \t--- Loss: 0.773\n",
      "Iteration: 168 \t--- Loss: 0.772\n",
      "Iteration: 169 \t--- Loss: 0.774\n",
      "Iteration: 170 \t--- Loss: 0.772\n",
      "Iteration: 171 \t--- Loss: 0.773\n",
      "Iteration: 172 \t--- Loss: 0.780\n",
      "Iteration: 173 \t--- Loss: 0.777\n",
      "Iteration: 174 \t--- Loss: 0.776\n",
      "Iteration: 175 \t--- Loss: 0.768\n",
      "Iteration: 176 \t--- Loss: 0.768\n",
      "Iteration: 177 \t--- Loss: 0.774\n",
      "Iteration: 178 \t--- Loss: 0.774\n",
      "Iteration: 179 \t--- Loss: 0.770\n",
      "Iteration: 180 \t--- Loss: 0.766\n",
      "Iteration: 181 \t--- Loss: 0.774\n",
      "Iteration: 182 \t--- Loss: 0.773\n",
      "Iteration: 183 \t--- Loss: 0.776\n",
      "Iteration: 184 \t--- Loss: 0.775\n",
      "Iteration: 185 \t--- Loss: 0.774\n",
      "Iteration: 186 \t--- Loss: 0.770\n",
      "Iteration: 187 \t--- Loss: 0.776\n",
      "Iteration: 188 \t--- Loss: 0.781\n",
      "Iteration: 189 \t--- Loss: 0.771\n",
      "Iteration: 190 \t--- Loss: 0.772\n",
      "Iteration: 191 \t--- Loss: 0.776\n",
      "Iteration: 192 \t--- Loss: 0.768\n",
      "Iteration: 193 \t--- Loss: 0.773\n",
      "Iteration: 194 \t--- Loss: 0.773\n",
      "Iteration: 195 \t--- Loss: 0.771\n",
      "Iteration: 196 \t--- Loss: 0.772\n",
      "Iteration: 197 \t--- Loss: 0.774\n",
      "Iteration: 198 \t--- Loss: 0.770\n",
      "Iteration: 199 \t--- Loss: 0.775\n",
      "Iteration: 200 \t--- Loss: 0.775\n",
      "Iteration: 201 \t--- Loss: 0.772\n",
      "Iteration: 202 \t--- Loss: 0.769\n",
      "Iteration: 203 \t--- Loss: 0.771\n",
      "Iteration: 204 \t--- Loss: 0.770\n",
      "Iteration: 205 \t--- Loss: 0.771\n",
      "Iteration: 206 \t--- Loss: 0.771\n",
      "Iteration: 207 \t--- Loss: 0.774\n",
      "Iteration: 208 \t--- Loss: 0.773\n",
      "Iteration: 209 \t--- Loss: 0.769\n",
      "Iteration: 210 \t--- Loss: 0.767\n",
      "Iteration: 211 \t--- Loss: 0.774\n",
      "Iteration: 212 \t--- Loss: 0.773\n",
      "Iteration: 213 \t--- Loss: 0.774\n",
      "Iteration: 214 \t--- Loss: 0.770\n",
      "Iteration: 215 \t--- Loss: 0.775\n",
      "Iteration: 216 \t--- Loss: 0.773\n",
      "Iteration: 217 \t--- Loss: 0.775\n",
      "Iteration: 218 \t--- Loss: 0.769\n",
      "Iteration: 219 \t--- Loss: 0.777\n",
      "Iteration: 220 \t--- Loss: 0.769\n",
      "Iteration: 221 \t--- Loss: 0.770\n",
      "Iteration: 222 \t--- Loss: 0.767\n",
      "Iteration: 223 \t--- Loss: 0.778\n",
      "Iteration: 224 \t--- Loss: 0.774\n",
      "Iteration: 225 \t--- Loss: 0.765\n",
      "Iteration: 226 \t--- Loss: 0.771\n",
      "Iteration: 227 \t--- Loss: 0.770\n",
      "Iteration: 228 \t--- Loss: 0.773\n",
      "Iteration: 229 \t--- Loss: 0.772\n",
      "Iteration: 230 \t--- Loss: 0.773\n",
      "Iteration: 231 \t--- Loss: 0.771\n",
      "Iteration: 232 \t--- Loss: 0.775\n",
      "Iteration: 233 \t--- Loss: 0.776\n",
      "Iteration: 234 \t--- Loss: 0.776\n",
      "Iteration: 235 \t--- Loss: 0.772\n",
      "Iteration: 236 \t--- Loss: 0.767\n",
      "Iteration: 237 \t--- Loss: 0.770\n",
      "Iteration: 238 \t--- Loss: 0.773\n",
      "Iteration: 239 \t--- Loss: 0.765\n",
      "Iteration: 240 \t--- Loss: 0.774\n",
      "Iteration: 241 \t--- Loss: 0.772\n",
      "Iteration: 242 \t--- Loss: 0.778\n",
      "Iteration: 243 \t--- Loss: 0.775\n",
      "Iteration: 244 \t--- Loss: 0.772\n",
      "Iteration: 245 \t--- Loss: 0.776\n",
      "Iteration: 246 \t--- Loss: 0.770\n",
      "Iteration: 247 \t--- Loss: 0.772\n",
      "Iteration: 248 \t--- Loss: 0.770\n",
      "Iteration: 249 \t--- Loss: 0.776\n",
      "Iteration: 250 \t--- Loss: 0.775\n",
      "Iteration: 251 \t--- Loss: 0.770\n",
      "Iteration: 252 \t--- Loss: 0.772\n",
      "Iteration: 253 \t--- Loss: 0.771\n",
      "Iteration: 254 \t--- Loss: 0.770\n",
      "Iteration: 255 \t--- Loss: 0.775\n",
      "Iteration: 256 \t--- Loss: 0.773\n",
      "Iteration: 257 \t--- Loss: 0.770\n",
      "Iteration: 258 \t--- Loss: 0.774\n",
      "Iteration: 259 \t--- Loss: 0.773"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it][Parallel(n_jobs=5)]: Done  84 tasks      | elapsed: 51.9min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:26<00:00, 86.30s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.209\n",
      "Iteration: 261 \t--- Loss: 0.198\n",
      "Iteration: 262 \t--- Loss: 0.200\n",
      "Iteration: 263 \t--- Loss: 0.195\n",
      "Iteration: 264 \t--- Loss: 0.199\n",
      "Iteration: 265 \t--- Loss: 0.195\n",
      "Iteration: 266 \t--- Loss: 0.202\n",
      "Iteration: 267 \t--- Loss: 0.193\n",
      "Iteration: 268 \t--- Loss: 0.202\n",
      "Iteration: 269 \t--- Loss: 0.199\n",
      "Iteration: 270 \t--- Loss: 0.195\n",
      "Iteration: 271 \t--- Loss: 0.199\n",
      "Iteration: 272 \t--- Loss: 0.204\n",
      "Iteration: 273 \t--- Loss: 0.201\n",
      "Iteration: 274 \t--- Loss: 0.189\n",
      "Iteration: 275 \t--- Loss: 0.207\n",
      "Iteration: 276 \t--- Loss: 0.201\n",
      "Iteration: 277 \t--- Loss: 0.196\n",
      "Iteration: 278 \t--- Loss: 0.200\n",
      "Iteration: 279 \t--- Loss: 0.207\n",
      "Iteration: 280 \t--- Loss: 0.193\n",
      "Iteration: 281 \t--- Loss: 0.190\n",
      "Iteration: 282 \t--- Loss: 0.196\n",
      "Iteration: 283 \t--- Loss: 0.194\n",
      "Iteration: 284 \t--- Loss: 0.193\n",
      "Iteration: 285 \t--- Loss: 0.202\n",
      "Iteration: 286 \t--- Loss: 0.194\n",
      "Iteration: 287 \t--- Loss: 0.205\n",
      "Iteration: 288 \t--- Loss: 0.203\n",
      "Iteration: 289 \t--- Loss: 0.195\n",
      "Iteration: 290 \t--- Loss: 0.203\n",
      "Iteration: 291 \t--- Loss: 0.199\n",
      "Iteration: 292 \t--- Loss: 0.199\n",
      "Iteration: 293 \t--- Loss: 0.208\n",
      "Iteration: 294 \t--- Loss: 0.203\n",
      "Iteration: 295 \t--- Loss: 0.199\n",
      "Iteration: 296 \t--- Loss: 0.199\n",
      "Iteration: 297 \t--- Loss: 0.196\n",
      "Iteration: 298 \t--- Loss: 0.198\n",
      "Iteration: 299 \t--- Loss: 0.197\n",
      "Iteration: 300 \t--- Loss: 0.204\n",
      "Iteration: 301 \t--- Loss: 0.205\n",
      "Iteration: 302 \t--- Loss: 0.205\n",
      "Iteration: 303 \t--- Loss: 0.188\n",
      "Iteration: 304 \t--- Loss: 0.213\n",
      "Iteration: 305 \t--- Loss: 0.197\n",
      "Iteration: 306 \t--- Loss: 0.202\n",
      "Iteration: 307 \t--- Loss: 0.197\n",
      "Iteration: 308 \t--- Loss: 0.194\n",
      "Iteration: 309 \t--- Loss: 0.197\n",
      "Iteration: 310 \t--- Loss: 0.200\n",
      "Iteration: 311 \t--- Loss: 0.198\n",
      "Iteration: 312 \t--- Loss: 0.195\n",
      "Iteration: 313 \t--- Loss: 0.200\n",
      "Iteration: 314 \t--- Loss: 0.203\n",
      "Iteration: 315 \t--- Loss: 0.197\n",
      "Iteration: 316 \t--- Loss: 0.205\n",
      "Iteration: 317 \t--- Loss: 0.199\n",
      "Iteration: 318 \t--- Loss: 0.200\n",
      "Iteration: 319 \t--- Loss: 0.200\n",
      "Iteration: 320 \t--- Loss: 0.198\n",
      "Iteration: 321 \t--- Loss: 0.207\n",
      "Iteration: 322 \t--- Loss: 0.206\n",
      "Iteration: 323 \t--- Loss: 0.200\n",
      "Iteration: 324 \t--- Loss: 0.202\n",
      "Iteration: 325 \t--- Loss: 0.203\n",
      "Iteration: 326 \t--- Loss: 0.199\n",
      "Iteration: 327 \t--- Loss: 0.200\n",
      "Iteration: 328 \t--- Loss: 0.199\n",
      "Iteration: 329 \t--- Loss: 0.193\n",
      "Iteration: 330 \t--- Loss: 0.194\n",
      "Iteration: 331 \t--- Loss: 0.195\n",
      "Iteration: 332 \t--- Loss: 0.198\n",
      "Iteration: 333 \t--- Loss: 0.201\n",
      "Iteration: 334 \t--- Loss: 0.199\n",
      "Iteration: 335 \t--- Loss: 0.205\n",
      "Iteration: 336 \t--- Loss: 0.202\n",
      "Iteration: 337 \t--- Loss: 0.204\n",
      "Iteration: 338 \t--- Loss: 0.195\n",
      "Iteration: 339 \t--- Loss: 0.196\n",
      "Iteration: 340 \t--- Loss: 0.200\n",
      "Iteration: 341 \t--- Loss: 0.197\n",
      "Iteration: 342 \t--- Loss: 0.190\n",
      "Iteration: 343 \t--- Loss: 0.205\n",
      "Iteration: 344 \t--- Loss: 0.202\n",
      "Iteration: 345 \t--- Loss: 0.190\n",
      "Iteration: 346 \t--- Loss: 0.205\n",
      "Iteration: 347 \t--- Loss: 0.201\n",
      "Iteration: 348 \t--- Loss: 0.208\n",
      "Iteration: 349 \t--- Loss: 0.200\n",
      "Iteration: 350 \t--- Loss: 0.202\n",
      "Iteration: 351 \t--- Loss: 0.190\n",
      "Iteration: 352 \t--- Loss: 0.200\n",
      "Iteration: 353 \t--- Loss: 0.194\n",
      "Iteration: 354 \t--- Loss: 0.193\n",
      "Iteration: 355 \t--- Loss: 0.196\n",
      "Iteration: 356 \t--- Loss: 0.199\n",
      "Iteration: 357 \t--- Loss: 0.197\n",
      "Iteration: 358 \t--- Loss: 0.204\n",
      "Iteration: 359 \t--- Loss: 0.191\n",
      "Iteration: 360 \t--- Loss: 0.208\n",
      "Iteration: 361 \t--- Loss: 0.209\n",
      "Iteration: 362 \t--- Loss: 0.200\n",
      "Iteration: 363 \t--- Loss: 0.215\n",
      "Iteration: 364 \t--- Loss: 0.194\n",
      "Iteration: 365 \t--- Loss: 0.201\n",
      "Iteration: 366 \t--- Loss: 0.199\n",
      "Iteration: 367 \t--- Loss: 0.198\n",
      "Iteration: 368 \t--- Loss: 0.200\n",
      "Iteration: 369 \t--- Loss: 0.200\n",
      "Iteration: 370 \t--- Loss: 0.199\n",
      "Iteration: 371 \t--- Loss: 0.195\n",
      "Iteration: 372 \t--- Loss: 0.204\n",
      "Iteration: 373 \t--- Loss: 0.197\n",
      "Iteration: 374 \t--- Loss: 0.204\n",
      "Iteration: 375 \t--- Loss: 0.200\n",
      "Iteration: 376 \t--- Loss: 0.193\n",
      "Iteration: 377 \t--- Loss: 0.200\n",
      "Iteration: 378 \t--- Loss: 0.193\n",
      "Iteration: 379 \t--- Loss: 0.195\n",
      "Iteration: 380 \t--- Loss: 0.206\n",
      "Iteration: 381 \t--- Loss: 0.200\n",
      "Iteration: 382 \t--- Loss: 0.194\n",
      "Iteration: 383 \t--- Loss: 0.193\n",
      "Iteration: 384 \t--- Loss: 0.201\n",
      "Iteration: 385 \t--- Loss: 0.191\n",
      "Iteration: 386 \t--- Loss: 0.196\n",
      "Iteration: 387 \t--- Loss: 0.198\n",
      "Iteration: 388 \t--- Loss: 0.195\n",
      "Iteration: 389 \t--- Loss: 0.199\n",
      "Iteration: 390 \t--- Loss: 0.201\n",
      "Iteration: 391 \t--- Loss: 0.199\n",
      "Iteration: 392 \t--- Loss: 0.203\n",
      "Iteration: 393 \t--- Loss: 0.200\n",
      "Iteration: 394 \t--- Loss: 0.190\n",
      "Iteration: 395 \t--- Loss: 0.204\n",
      "Iteration: 396 \t--- Loss: 0.197\n",
      "Iteration: 397 \t--- Loss: 0.200\n",
      "Iteration: 398 \t--- Loss: 0.197\n",
      "Iteration: 399 \t--- Loss: 0.209\n",
      "Iteration: 400 \t--- Loss: 0.193\n",
      "Iteration: 401 \t--- Loss: 0.199\n",
      "Iteration: 402 \t--- Loss: 0.197\n",
      "Iteration: 403 \t--- Loss: 0.193\n",
      "Iteration: 404 \t--- Loss: 0.205\n",
      "Iteration: 405 \t--- Loss: 0.205\n",
      "Iteration: 406 \t--- Loss: 0.199\n",
      "Iteration: 407 \t--- Loss: 0.202\n",
      "Iteration: 408 \t--- Loss: 0.196\n",
      "Iteration: 409 \t--- Loss: 0.199\n",
      "Iteration: 410 \t--- Loss: 0.202\n",
      "Iteration: 411 \t--- Loss: 0.198\n",
      "Iteration: 412 \t--- Loss: 0.196\n",
      "Iteration: 413 \t--- Loss: 0.201\n",
      "Iteration: 414 \t--- Loss: 0.198\n",
      "Iteration: 415 \t--- Loss: 0.197\n",
      "Iteration: 416 \t--- Loss: 0.204\n",
      "Iteration: 417 \t--- Loss: 0.201\n",
      "Iteration: 418 \t--- Loss: 0.201\n",
      "Iteration: 419 \t--- Loss: 0.193\n",
      "Iteration: 420 \t--- Loss: 0.202\n",
      "Iteration: 421 \t--- Loss: 0.200\n",
      "Iteration: 422 \t--- Loss: 0.205\n",
      "Iteration: 423 \t--- Loss: 0.199\n",
      "Iteration: 424 \t--- Loss: 0.198\n",
      "Iteration: 425 \t--- Loss: 0.201\n",
      "Iteration: 426 \t--- Loss: 0.196\n",
      "Iteration: 427 \t--- Loss: 0.202\n",
      "Iteration: 428 \t--- Loss: 0.204\n",
      "Iteration: 429 \t--- Loss: 0.195\n",
      "Iteration: 430 \t--- Loss: 0.197\n",
      "Iteration: 431 \t--- Loss: 0.202\n",
      "Iteration: 432 \t--- Loss: 0.194\n",
      "Iteration: 433 \t--- Loss: 0.200\n",
      "Iteration: 434 \t--- Loss: 0.200\n",
      "Iteration: 435 \t--- Loss: 0.198\n",
      "Iteration: 436 \t--- Loss: 0.198\n",
      "Iteration: 437 \t--- Loss: 0.200\n",
      "Iteration: 438 \t--- Loss: 0.204\n",
      "Iteration: 439 \t--- Loss: 0.191\n",
      "Iteration: 440 \t--- Loss: 0.196\n",
      "Iteration: 441 \t--- Loss: 0.195\n",
      "Iteration: 442 \t--- Loss: 0.198\n",
      "Iteration: 443 \t--- Loss: 0.203\n",
      "Iteration: 444 \t--- Loss: 0.198\n",
      "Iteration: 445 \t--- Loss: 0.194\n",
      "Iteration: 446 \t--- Loss: 0.193\n",
      "Iteration: 447 \t--- Loss: 0.197\n",
      "Iteration: 448 \t--- Loss: 0.200\n",
      "Iteration: 449 \t--- Loss: 0.199\n",
      "Iteration: 450 \t--- Loss: 0.197\n",
      "Iteration: 451 \t--- Loss: 0.194\n",
      "Iteration: 452 \t--- Loss: 0.202\n",
      "Iteration: 453 \t--- Loss: 0.200\n",
      "Iteration: 454 \t--- Loss: 0.202\n",
      "Iteration: 455 \t--- Loss: 0.201\n",
      "Iteration: 456 \t--- Loss: 0.204\n",
      "Iteration: 457 \t--- Loss: 0.203\n",
      "Iteration: 458 \t--- Loss: 0.207\n",
      "Iteration: 459 \t--- Loss: 0.199\n",
      "Iteration: 460 \t--- Loss: 0.190\n",
      "Iteration: 461 \t--- Loss: 0.198\n",
      "Iteration: 462 \t--- Loss: 0.202\n",
      "Iteration: 463 \t--- Loss: 0.196\n",
      "Iteration: 464 \t--- Loss: 0.192\n",
      "Iteration: 465 \t--- Loss: 0.199\n",
      "Iteration: 466 \t--- Loss: 0.198\n",
      "Iteration: 467 \t--- Loss: 0.206\n",
      "Iteration: 468 \t--- Loss: 0.195\n",
      "Iteration: 469 \t--- Loss: 0.196\n",
      "Iteration: 470 \t--- Loss: 0.198\n",
      "Iteration: 471 \t--- Loss: 0.205\n",
      "Iteration: 472 \t--- Loss: 0.197\n",
      "Iteration: 473 \t--- Loss: 0.202\n",
      "Iteration: 474 \t--- Loss: 0.203\n",
      "Iteration: 475 \t--- Loss: 0.209\n",
      "Iteration: 476 \t--- Loss: 0.199\n",
      "Iteration: 477 \t--- Loss: 0.199\n",
      "Iteration: 478 \t--- Loss: 0.202\n",
      "Iteration: 479 \t--- Loss: 0.202\n",
      "Iteration: 480 \t--- Loss: 0.195\n",
      "Iteration: 481 \t--- Loss: 0.194\n",
      "Iteration: 482 \t--- Loss: 0.201\n",
      "Iteration: 483 \t--- Loss: 0.192\n",
      "Iteration: 484 \t--- Loss: 0.196\n",
      "Iteration: 485 \t--- Loss: 0.190\n",
      "Iteration: 486 \t--- Loss: 0.187\n",
      "Iteration: 487 \t--- Loss: 0.206\n",
      "Iteration: 488 \t--- Loss: 0.205\n",
      "Iteration: 489 \t--- Loss: 0.196\n",
      "Iteration: 490 \t--- Loss: 0.202\n",
      "Iteration: 491 \t--- Loss: 0.205\n",
      "Iteration: 492 \t--- Loss: 0.194\n",
      "Iteration: 493 \t--- Loss: 0.202\n",
      "Iteration: 494 \t--- Loss: 0.194\n",
      "Iteration: 495 \t--- Loss: 0.194\n",
      "Iteration: 496 \t--- Loss: 0.199\n",
      "Iteration: 497 \t--- Loss: 0.199\n",
      "Iteration: 498 \t--- Loss: 0.201\n",
      "Iteration: 499 \t--- Loss: 0.194\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:24<00:00, 84.38s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.771\n",
      "Iteration: 261 \t--- Loss: 0.768\n",
      "Iteration: 262 \t--- Loss: 0.775\n",
      "Iteration: 263 \t--- Loss: 0.774\n",
      "Iteration: 264 \t--- Loss: 0.776\n",
      "Iteration: 265 \t--- Loss: 0.775\n",
      "Iteration: 266 \t--- Loss: 0.776\n",
      "Iteration: 267 \t--- Loss: 0.775\n",
      "Iteration: 268 \t--- Loss: 0.769\n",
      "Iteration: 269 \t--- Loss: 0.770\n",
      "Iteration: 270 \t--- Loss: 0.772\n",
      "Iteration: 271 \t--- Loss: 0.771\n",
      "Iteration: 272 \t--- Loss: 0.770\n",
      "Iteration: 273 \t--- Loss: 0.776\n",
      "Iteration: 274 \t--- Loss: 0.770\n",
      "Iteration: 275 \t--- Loss: 0.770\n",
      "Iteration: 276 \t--- Loss: 0.780\n",
      "Iteration: 277 \t--- Loss: 0.775\n",
      "Iteration: 278 \t--- Loss: 0.776\n",
      "Iteration: 279 \t--- Loss: 0.772\n",
      "Iteration: 280 \t--- Loss: 0.778\n",
      "Iteration: 281 \t--- Loss: 0.767\n",
      "Iteration: 282 \t--- Loss: 0.770\n",
      "Iteration: 283 \t--- Loss: 0.771\n",
      "Iteration: 284 \t--- Loss: 0.766\n",
      "Iteration: 285 \t--- Loss: 0.777\n",
      "Iteration: 286 \t--- Loss: 0.774\n",
      "Iteration: 287 \t--- Loss: 0.774\n",
      "Iteration: 288 \t--- Loss: 0.771\n",
      "Iteration: 289 \t--- Loss: 0.776\n",
      "Iteration: 290 \t--- Loss: 0.774\n",
      "Iteration: 291 \t--- Loss: 0.773\n",
      "Iteration: 292 \t--- Loss: 0.773\n",
      "Iteration: 293 \t--- Loss: 0.776\n",
      "Iteration: 294 \t--- Loss: 0.775\n",
      "Iteration: 295 \t--- Loss: 0.772\n",
      "Iteration: 296 \t--- Loss: 0.774\n",
      "Iteration: 297 \t--- Loss: 0.768\n",
      "Iteration: 298 \t--- Loss: 0.771\n",
      "Iteration: 299 \t--- Loss: 0.772\n",
      "Iteration: 300 \t--- Loss: 0.773\n",
      "Iteration: 301 \t--- Loss: 0.777\n",
      "Iteration: 302 \t--- Loss: 0.774\n",
      "Iteration: 303 \t--- Loss: 0.772\n",
      "Iteration: 304 \t--- Loss: 0.773\n",
      "Iteration: 305 \t--- Loss: 0.770\n",
      "Iteration: 306 \t--- Loss: 0.770\n",
      "Iteration: 307 \t--- Loss: 0.776\n",
      "Iteration: 308 \t--- Loss: 0.768\n",
      "Iteration: 309 \t--- Loss: 0.769\n",
      "Iteration: 310 \t--- Loss: 0.767\n",
      "Iteration: 311 \t--- Loss: 0.771\n",
      "Iteration: 312 \t--- Loss: 0.776\n",
      "Iteration: 313 \t--- Loss: 0.776\n",
      "Iteration: 314 \t--- Loss: 0.774\n",
      "Iteration: 315 \t--- Loss: 0.771\n",
      "Iteration: 316 \t--- Loss: 0.773\n",
      "Iteration: 317 \t--- Loss: 0.774\n",
      "Iteration: 318 \t--- Loss: 0.772\n",
      "Iteration: 319 \t--- Loss: 0.771\n",
      "Iteration: 320 \t--- Loss: 0.776\n",
      "Iteration: 321 \t--- Loss: 0.769\n",
      "Iteration: 322 \t--- Loss: 0.768\n",
      "Iteration: 323 \t--- Loss: 0.772\n",
      "Iteration: 324 \t--- Loss: 0.774\n",
      "Iteration: 325 \t--- Loss: 0.772\n",
      "Iteration: 326 \t--- Loss: 0.774\n",
      "Iteration: 327 \t--- Loss: 0.769\n",
      "Iteration: 328 \t--- Loss: 0.770\n",
      "Iteration: 329 \t--- Loss: 0.775\n",
      "Iteration: 330 \t--- Loss: 0.774\n",
      "Iteration: 331 \t--- Loss: 0.771\n",
      "Iteration: 332 \t--- Loss: 0.776\n",
      "Iteration: 333 \t--- Loss: 0.774\n",
      "Iteration: 334 \t--- Loss: 0.774\n",
      "Iteration: 335 \t--- Loss: 0.779\n",
      "Iteration: 336 \t--- Loss: 0.771\n",
      "Iteration: 337 \t--- Loss: 0.773\n",
      "Iteration: 338 \t--- Loss: 0.776\n",
      "Iteration: 339 \t--- Loss: 0.775\n",
      "Iteration: 340 \t--- Loss: 0.773\n",
      "Iteration: 341 \t--- Loss: 0.776\n",
      "Iteration: 342 \t--- Loss: 0.767\n",
      "Iteration: 343 \t--- Loss: 0.771\n",
      "Iteration: 344 \t--- Loss: 0.772\n",
      "Iteration: 345 \t--- Loss: 0.778\n",
      "Iteration: 346 \t--- Loss: 0.773\n",
      "Iteration: 347 \t--- Loss: 0.773\n",
      "Iteration: 348 \t--- Loss: 0.778\n",
      "Iteration: 349 \t--- Loss: 0.782\n",
      "Iteration: 350 \t--- Loss: 0.768\n",
      "Iteration: 351 \t--- Loss: 0.769\n",
      "Iteration: 352 \t--- Loss: 0.769\n",
      "Iteration: 353 \t--- Loss: 0.767\n",
      "Iteration: 354 \t--- Loss: 0.770\n",
      "Iteration: 355 \t--- Loss: 0.776\n",
      "Iteration: 356 \t--- Loss: 0.773\n",
      "Iteration: 357 \t--- Loss: 0.769\n",
      "Iteration: 358 \t--- Loss: 0.771\n",
      "Iteration: 359 \t--- Loss: 0.775\n",
      "Iteration: 360 \t--- Loss: 0.775\n",
      "Iteration: 361 \t--- Loss: 0.770\n",
      "Iteration: 362 \t--- Loss: 0.774\n",
      "Iteration: 363 \t--- Loss: 0.774\n",
      "Iteration: 364 \t--- Loss: 0.762\n",
      "Iteration: 365 \t--- Loss: 0.770\n",
      "Iteration: 366 \t--- Loss: 0.771\n",
      "Iteration: 367 \t--- Loss: 0.775\n",
      "Iteration: 368 \t--- Loss: 0.773\n",
      "Iteration: 369 \t--- Loss: 0.771\n",
      "Iteration: 370 \t--- Loss: 0.776\n",
      "Iteration: 371 \t--- Loss: 0.768\n",
      "Iteration: 372 \t--- Loss: 0.768\n",
      "Iteration: 373 \t--- Loss: 0.775\n",
      "Iteration: 374 \t--- Loss: 0.769\n",
      "Iteration: 375 \t--- Loss: 0.768\n",
      "Iteration: 376 \t--- Loss: 0.780\n",
      "Iteration: 377 \t--- Loss: 0.775\n",
      "Iteration: 378 \t--- Loss: 0.778\n",
      "Iteration: 379 \t--- Loss: 0.777\n",
      "Iteration: 380 \t--- Loss: 0.768\n",
      "Iteration: 381 \t--- Loss: 0.771\n",
      "Iteration: 382 \t--- Loss: 0.774\n",
      "Iteration: 383 \t--- Loss: 0.773\n",
      "Iteration: 384 \t--- Loss: 0.778\n",
      "Iteration: 385 \t--- Loss: 0.769\n",
      "Iteration: 386 \t--- Loss: 0.768\n",
      "Iteration: 387 \t--- Loss: 0.768\n",
      "Iteration: 388 \t--- Loss: 0.772\n",
      "Iteration: 389 \t--- Loss: 0.770\n",
      "Iteration: 390 \t--- Loss: 0.769\n",
      "Iteration: 391 \t--- Loss: 0.770\n",
      "Iteration: 392 \t--- Loss: 0.773\n",
      "Iteration: 393 \t--- Loss: 0.766\n",
      "Iteration: 394 \t--- Loss: 0.778\n",
      "Iteration: 395 \t--- Loss: 0.771\n",
      "Iteration: 396 \t--- Loss: 0.772\n",
      "Iteration: 397 \t--- Loss: 0.778\n",
      "Iteration: 398 \t--- Loss: 0.776\n",
      "Iteration: 399 \t--- Loss: 0.775\n",
      "Iteration: 400 \t--- Loss: 0.775\n",
      "Iteration: 401 \t--- Loss: 0.771\n",
      "Iteration: 402 \t--- Loss: 0.779\n",
      "Iteration: 403 \t--- Loss: 0.771\n",
      "Iteration: 404 \t--- Loss: 0.769\n",
      "Iteration: 405 \t--- Loss: 0.775\n",
      "Iteration: 406 \t--- Loss: 0.773\n",
      "Iteration: 407 \t--- Loss: 0.774\n",
      "Iteration: 408 \t--- Loss: 0.778\n",
      "Iteration: 409 \t--- Loss: 0.774\n",
      "Iteration: 410 \t--- Loss: 0.772\n",
      "Iteration: 411 \t--- Loss: 0.773\n",
      "Iteration: 412 \t--- Loss: 0.772\n",
      "Iteration: 413 \t--- Loss: 0.774\n",
      "Iteration: 414 \t--- Loss: 0.774\n",
      "Iteration: 415 \t--- Loss: 0.776\n",
      "Iteration: 416 \t--- Loss: 0.772\n",
      "Iteration: 417 \t--- Loss: 0.774\n",
      "Iteration: 418 \t--- Loss: 0.773\n",
      "Iteration: 419 \t--- Loss: 0.773\n",
      "Iteration: 420 \t--- Loss: 0.770\n",
      "Iteration: 421 \t--- Loss: 0.773\n",
      "Iteration: 422 \t--- Loss: 0.770\n",
      "Iteration: 423 \t--- Loss: 0.770\n",
      "Iteration: 424 \t--- Loss: 0.775\n",
      "Iteration: 425 \t--- Loss: 0.776\n",
      "Iteration: 426 \t--- Loss: 0.770\n",
      "Iteration: 427 \t--- Loss: 0.776\n",
      "Iteration: 428 \t--- Loss: 0.769\n",
      "Iteration: 429 \t--- Loss: 0.772\n",
      "Iteration: 430 \t--- Loss: 0.769\n",
      "Iteration: 431 \t--- Loss: 0.770\n",
      "Iteration: 432 \t--- Loss: 0.775\n",
      "Iteration: 433 \t--- Loss: 0.777\n",
      "Iteration: 434 \t--- Loss: 0.772\n",
      "Iteration: 435 \t--- Loss: 0.776\n",
      "Iteration: 436 \t--- Loss: 0.775\n",
      "Iteration: 437 \t--- Loss: 0.775\n",
      "Iteration: 438 \t--- Loss: 0.776\n",
      "Iteration: 439 \t--- Loss: 0.771\n",
      "Iteration: 440 \t--- Loss: 0.770\n",
      "Iteration: 441 \t--- Loss: 0.771\n",
      "Iteration: 442 \t--- Loss: 0.771\n",
      "Iteration: 443 \t--- Loss: 0.769\n",
      "Iteration: 444 \t--- Loss: 0.772\n",
      "Iteration: 445 \t--- Loss: 0.776\n",
      "Iteration: 446 \t--- Loss: 0.775\n",
      "Iteration: 447 \t--- Loss: 0.773\n",
      "Iteration: 448 \t--- Loss: 0.774\n",
      "Iteration: 449 \t--- Loss: 0.775\n",
      "Iteration: 450 \t--- Loss: 0.770\n",
      "Iteration: 451 \t--- Loss: 0.774\n",
      "Iteration: 452 \t--- Loss: 0.781\n",
      "Iteration: 453 \t--- Loss: 0.772\n",
      "Iteration: 454 \t--- Loss: 0.769\n",
      "Iteration: 455 \t--- Loss: 0.768\n",
      "Iteration: 456 \t--- Loss: 0.773\n",
      "Iteration: 457 \t--- Loss: 0.772\n",
      "Iteration: 458 \t--- Loss: 0.775\n",
      "Iteration: 459 \t--- Loss: 0.773\n",
      "Iteration: 460 \t--- Loss: 0.775\n",
      "Iteration: 461 \t--- Loss: 0.767\n",
      "Iteration: 462 \t--- Loss: 0.776\n",
      "Iteration: 463 \t--- Loss: 0.774\n",
      "Iteration: 464 \t--- Loss: 0.774\n",
      "Iteration: 465 \t--- Loss: 0.773\n",
      "Iteration: 466 \t--- Loss: 0.771\n",
      "Iteration: 467 \t--- Loss: 0.771\n",
      "Iteration: 468 \t--- Loss: 0.772\n",
      "Iteration: 469 \t--- Loss: 0.770\n",
      "Iteration: 470 \t--- Loss: 0.771\n",
      "Iteration: 471 \t--- Loss: 0.774\n",
      "Iteration: 472 \t--- Loss: 0.781\n",
      "Iteration: 473 \t--- Loss: 0.775\n",
      "Iteration: 474 \t--- Loss: 0.778\n",
      "Iteration: 475 \t--- Loss: 0.764\n",
      "Iteration: 476 \t--- Loss: 0.772\n",
      "Iteration: 477 \t--- Loss: 0.775\n",
      "Iteration: 478 \t--- Loss: 0.771\n",
      "Iteration: 479 \t--- Loss: 0.775\n",
      "Iteration: 480 \t--- Loss: 0.774\n",
      "Iteration: 481 \t--- Loss: 0.779\n",
      "Iteration: 482 \t--- Loss: 0.777\n",
      "Iteration: 483 \t--- Loss: 0.771\n",
      "Iteration: 484 \t--- Loss: 0.768\n",
      "Iteration: 485 \t--- Loss: 0.770\n",
      "Iteration: 486 \t--- Loss: 0.772\n",
      "Iteration: 487 \t--- Loss: 0.775\n",
      "Iteration: 488 \t--- Loss: 0.773\n",
      "Iteration: 489 \t--- Loss: 0.773\n",
      "Iteration: 490 \t--- Loss: 0.771\n",
      "Iteration: 491 \t--- Loss: 0.773\n",
      "Iteration: 492 \t--- Loss: 0.772\n",
      "Iteration: 493 \t--- Loss: 0.773\n",
      "Iteration: 494 \t--- Loss: 0.770\n",
      "Iteration: 495 \t--- Loss: 0.775\n",
      "Iteration: 496 \t--- Loss: 0.774\n",
      "Iteration: 497 \t--- Loss: 0.773\n",
      "Iteration: 498 \t--- Loss: 0.772\n",
      "Iteration: 499 \t--- Loss: 0.773\n",
      "----  Optimizing the metamodel  ----\n",
      "Iteration: 0 \t--- Loss: 1.200\n",
      "Iteration: 1 \t--- Loss: 1.159\n",
      "Iteration: 2 \t--- Loss: 1.067\n",
      "Iteration: 3 \t--- Loss: 0.985\n",
      "Iteration: 4 \t--- Loss: 0.874\n",
      "Iteration: 5 \t--- Loss: 0.845\n",
      "Iteration: 6 \t--- Loss: 0.816\n",
      "Iteration: 7 \t--- Loss: 0.813\n",
      "Iteration: 8 \t--- Loss: 0.775\n",
      "Iteration: 9 \t--- Loss: 0.758\n",
      "Iteration: 10 \t--- Loss: 0.766\n",
      "Iteration: 11 \t--- Loss: 0.759\n",
      "Iteration: 12 \t--- Loss: 0.769\n",
      "Iteration: 13 \t--- Loss: 0.745\n",
      "Iteration: 14 \t--- Loss: 0.719\n",
      "Iteration: 15 \t--- Loss: 0.700\n",
      "Iteration: 16 \t--- Loss: 0.723\n",
      "Iteration: 17 \t--- Loss: 0.726\n",
      "Iteration: 18 \t--- Loss: 0.710\n",
      "Iteration: 19 \t--- Loss: 0.692\n",
      "Iteration: 20 \t--- Loss: 0.706\n",
      "Iteration: 21 \t--- Loss: 0.696\n",
      "Iteration: 22 \t--- Loss: 0.691\n",
      "Iteration: 23 \t--- Loss: 0.717\n",
      "Iteration: 24 \t--- Loss: 0.706\n",
      "Iteration: 25 \t--- Loss: 0.715\n",
      "Iteration: 26 \t--- Loss: 0.731\n",
      "Iteration: 27 \t--- Loss: 0.718\n",
      "Iteration: 28 \t--- Loss: 0.700\n",
      "Iteration: 29 \t--- Loss: 0.693\n",
      "Iteration: 30 \t--- Loss: 0.720\n",
      "Iteration: 31 \t--- Loss: 0.689\n",
      "Iteration: 32 \t--- Loss: 0.711\n",
      "Iteration: 33 \t--- Loss: 0.705\n",
      "Iteration: 34 \t--- Loss: 0.697\n",
      "Iteration: 35 \t--- Loss: 0.707\n",
      "Iteration: 36 \t--- Loss: 0.693\n",
      "Iteration: 37 \t--- Loss: 0.688\n",
      "Iteration: 38 \t--- Loss: 0.689\n",
      "Iteration: 39 \t--- Loss: 0.711\n",
      "Iteration: 40 \t--- Loss: 0.676\n",
      "Iteration: 41 \t--- Loss: 0.700\n",
      "Iteration: 42 \t--- Loss: 0.718\n",
      "Iteration: 43 \t--- Loss: 0.691\n",
      "Iteration: 44 \t--- Loss: 0.704\n",
      "Iteration: 45 \t--- Loss: 0.719\n",
      "Iteration: 46 \t--- Loss: 0.708\n",
      "Iteration: 47 \t--- Loss: 0.675\n",
      "Iteration: 48 \t--- Loss: 0.719\n",
      "Iteration: 49 \t--- Loss: 0.712\n",
      "Iteration: 50 \t--- Loss: 0.692\n",
      "Iteration: 51 \t--- Loss: 0.700\n",
      "Iteration: 52 \t--- Loss: 0.680\n",
      "Iteration: 53 \t--- Loss: 0.721\n",
      "Iteration: 54 \t--- Loss: 0.712\n",
      "Iteration: 55 \t--- Loss: 0.712\n",
      "Iteration: 56 \t--- Loss: 0.684\n",
      "Iteration: 57 \t--- Loss: 0.698\n",
      "Iteration: 58 \t--- Loss: 0.704\n",
      "Iteration: 59 \t--- Loss: 0.723\n",
      "Iteration: 60 \t--- Loss: 0.687\n",
      "Iteration: 61 \t--- Loss: 0.705\n",
      "Iteration: 62 \t--- Loss: 0.684\n",
      "Iteration: 63 \t--- Loss: 0.711\n",
      "Iteration: 64 \t--- Loss: 0.683\n",
      "Iteration: 65 \t--- Loss: 0.706\n",
      "Iteration: 66 \t--- Loss: 0.699\n",
      "Iteration: 67 \t--- Loss: 0.717\n",
      "Iteration: 68 \t--- Loss: 0.705\n",
      "Iteration: 69 \t--- Loss: 0.696\n",
      "Iteration: 70 \t--- Loss: 0.716\n",
      "Iteration: 71 \t--- Loss: 0.694\n",
      "Iteration: 72 \t--- Loss: 0.679\n",
      "Iteration: 73 \t--- Loss: 0.707\n",
      "Iteration: 74 \t--- Loss: 0.693\n",
      "Iteration: 75 \t--- Loss: 0.688\n",
      "Iteration: 76 \t--- Loss: 0.702\n",
      "Iteration: 77 \t--- Loss: 0.706\n",
      "Iteration: 78 \t--- Loss: 0.688\n",
      "Iteration: 79 \t--- Loss: 0.709\n",
      "Iteration: 80 \t--- Loss: 0.702\n",
      "Iteration: 81 \t--- Loss: 0.686\n",
      "Iteration: 82 \t--- Loss: 0.709\n",
      "Iteration: 83 \t--- Loss: 0.701\n",
      "Iteration: 84 \t--- Loss: 0.681\n",
      "Iteration: 85 \t--- Loss: 0.719\n",
      "Iteration: 86 \t--- Loss: 0.689\n",
      "Iteration: 87 \t--- Loss: 0.710\n",
      "Iteration: 88 \t--- Loss: 0.709\n",
      "Iteration: 89 \t--- Loss: 0.703\n",
      "Iteration: 90 \t--- Loss: 0.705\n",
      "Iteration: 91 \t--- Loss: 0.699\n",
      "Iteration: 92 \t--- Loss: 0.682\n",
      "Iteration: 93 \t--- Loss: 0.687\n",
      "Iteration: 94 \t--- Loss: 0.715\n",
      "Iteration: 95 \t--- Loss: 0.692\n",
      "Iteration: 96 \t--- Loss: 0.691\n",
      "Iteration: 97 \t--- Loss: 0.680\n",
      "Iteration: 98 \t--- Loss: 0.700\n",
      "Iteration: 99 \t--- Loss: 0.716\n",
      "Iteration: 100 \t--- Loss: 0.692\n",
      "Iteration: 101 \t--- Loss: 0.704\n",
      "Iteration: 102 \t--- Loss: 0.737\n",
      "Iteration: 103 \t--- Loss: 0.692\n",
      "Iteration: 104 \t--- Loss: 0.711\n",
      "Iteration: 105 \t--- Loss: 0.729\n",
      "Iteration: 106 \t--- Loss: 0.709\n",
      "Iteration: 107 \t--- Loss: 0.703\n",
      "Iteration: 108 \t--- Loss: 0.714\n",
      "Iteration: 109 \t--- Loss: 0.707\n",
      "Iteration: 110 \t--- Loss: 0.697\n",
      "Iteration: 111 \t--- Loss: 0.675\n",
      "Iteration: 112 \t--- Loss: 0.693\n",
      "Iteration: 113 \t--- Loss: 0.695\n",
      "Iteration: 114 \t--- Loss: 0.689\n",
      "Iteration: 115 \t--- Loss: 0.716\n",
      "Iteration: 116 \t--- Loss: 0.712\n",
      "Iteration: 117 \t--- Loss: 0.700\n",
      "Iteration: 118 \t--- Loss: 0.709\n",
      "Iteration: 119 \t--- Loss: 0.702\n",
      "Iteration: 120 \t--- Loss: 0.693\n",
      "Iteration: 121 \t--- Loss: 0.703\n",
      "Iteration: 122 \t--- Loss: 0.702\n",
      "Iteration: 123 \t--- Loss: 0.686\n",
      "Iteration: 124 \t--- Loss: 0.692\n",
      "Iteration: 125 \t--- Loss: 0.704\n",
      "Iteration: 126 \t--- Loss: 0.688\n",
      "Iteration: 127 \t--- Loss: 0.712\n",
      "Iteration: 128 \t--- Loss: 0.697\n",
      "Iteration: 129 \t--- Loss: 0.706\n",
      "Iteration: 130 \t--- Loss: 0.686\n",
      "Iteration: 131 \t--- Loss: 0.702\n",
      "Iteration: 132 \t--- Loss: 0.702\n",
      "Iteration: 133 \t--- Loss: 0.697\n",
      "Iteration: 134 \t--- Loss: 0.668\n",
      "Iteration: 135 \t--- Loss: 0.705\n",
      "Iteration: 136 \t--- Loss: 0.692\n",
      "Iteration: 137 \t--- Loss: 0.703\n",
      "Iteration: 138 \t--- Loss: 0.706\n",
      "Iteration: 139 \t--- Loss: 0.694\n",
      "Iteration: 140 \t--- Loss: 0.694\n",
      "Iteration: 141 \t--- Loss: 0.695\n",
      "Iteration: 142 \t--- Loss: 0.700\n",
      "Iteration: 143 \t--- Loss: 0.704\n",
      "Iteration: 144 \t--- Loss: 0.706\n",
      "Iteration: 145 \t--- Loss: 0.735\n",
      "Iteration: 146 \t--- Loss: 0.687\n",
      "Iteration: 147 \t--- Loss: 0.674\n",
      "Iteration: 148 \t--- Loss: 0.703\n",
      "Iteration: 149 \t--- Loss: 0.701\n",
      "Iteration: 150 \t--- Loss: 0.698\n",
      "Iteration: 151 \t--- Loss: 0.714\n",
      "Iteration: 152 \t--- Loss: 0.679\n",
      "Iteration: 153 \t--- Loss: 0.699\n",
      "Iteration: 154 \t--- Loss: 0.697\n",
      "Iteration: 155 \t--- Loss: 0.693\n",
      "Iteration: 156 \t--- Loss: 0.696\n",
      "Iteration: 157 \t--- Loss: 0.717\n",
      "Iteration: 158 \t--- Loss: 0.716\n",
      "Iteration: 159 \t--- Loss: 0.695\n",
      "Iteration: 160 \t--- Loss: 0.697\n",
      "Iteration: 161 \t--- Loss: 0.696\n",
      "Iteration: 162 \t--- Loss: 0.711\n",
      "Iteration: 163 \t--- Loss: 0.697\n",
      "Iteration: 164 \t--- Loss: 0.701\n",
      "Iteration: 165 \t--- Loss: 0.713\n",
      "Iteration: 166 \t--- Loss: 0.688\n",
      "Iteration: 167 \t--- Loss: 0.691\n",
      "Iteration: 168 \t--- Loss: 0.712\n",
      "Iteration: 169 \t--- Loss: 0.693\n",
      "Iteration: 170 \t--- Loss: 0.715\n",
      "Iteration: 171 \t--- Loss: 0.679\n",
      "Iteration: 172 \t--- Loss: 0.689\n",
      "Iteration: 173 \t--- Loss: 0.681\n",
      "Iteration: 174 \t--- Loss: 0.680\n",
      "Iteration: 175 \t--- Loss: 0.708\n",
      "Iteration: 176 \t--- Loss: 0.706\n",
      "Iteration: 177 \t--- Loss: 0.705\n",
      "Iteration: 178 \t--- Loss: 0.698\n",
      "Iteration: 179 \t--- Loss: 0.712\n",
      "Iteration: 180 \t--- Loss: 0.698\n",
      "Iteration: 181 \t--- Loss: 0.700\n",
      "Iteration: 182 \t--- Loss: 0.708\n",
      "Iteration: 183 \t--- Loss: 0.698\n",
      "Iteration: 184 \t--- Loss: 0.664\n",
      "Iteration: 185 \t--- Loss: 0.700\n",
      "Iteration: 186 \t--- Loss: 0.707\n",
      "Iteration: 187 \t--- Loss: 0.686\n",
      "Iteration: 188 \t--- Loss: 0.714\n",
      "Iteration: 189 \t--- Loss: 0.706\n",
      "Iteration: 190 \t--- Loss: 0.714\n",
      "Iteration: 191 \t--- Loss: 0.697\n",
      "Iteration: 192 \t--- Loss: 0.685\n",
      "Iteration: 193 \t--- Loss: 0.711\n",
      "Iteration: 194 \t--- Loss: 0.708\n",
      "Iteration: 195 \t--- Loss: 0.693\n",
      "Iteration: 196 \t--- Loss: 0.708\n",
      "Iteration: 197 \t--- Loss: 0.712\n",
      "Iteration: 198 \t--- Loss: 0.702\n",
      "Iteration: 199 \t--- Loss: 0.686\n",
      "Iteration: 200 \t--- Loss: 0.703\n",
      "Iteration: 201 \t--- Loss: 0.704\n",
      "Iteration: 202 \t--- Loss: 0.687\n",
      "Iteration: 203 \t--- Loss: 0.699\n",
      "Iteration: 204 \t--- Loss: 0.690\n",
      "Iteration: 205 \t--- Loss: 0.694\n",
      "Iteration: 206 \t--- Loss: 0.715\n",
      "Iteration: 207 \t--- Loss: 0.712\n",
      "Iteration: 208 \t--- Loss: 0.695\n",
      "Iteration: 209 \t--- Loss: 0.692\n",
      "Iteration: 210 \t--- Loss: 0.726\n",
      "Iteration: 211 \t--- Loss: 0.692\n",
      "Iteration: 212 \t--- Loss: 0.727\n",
      "Iteration: 213 \t--- Loss: 0.689\n",
      "Iteration: 214 \t--- Loss: 0.712\n",
      "Iteration: 215 \t--- Loss: 0.708\n",
      "Iteration: 216 \t--- Loss: 0.689\n",
      "Iteration: 217 \t--- Loss: 0.707\n",
      "Iteration: 218 \t--- Loss: 0.706\n",
      "Iteration: 219 \t--- Loss: 0.691\n",
      "Iteration: 220 \t--- Loss: 0.688\n",
      "Iteration: 221 \t--- Loss: 0.708\n",
      "Iteration: 222 \t--- Loss: 0.715\n",
      "Iteration: 223 \t--- Loss: 0.704\n",
      "Iteration: 224 \t--- Loss: 0.700\n",
      "Iteration: 225 \t--- Loss: 0.712\n",
      "Iteration: 226 \t--- Loss: 0.682\n",
      "Iteration: 227 \t--- Loss: 0.701\n",
      "Iteration: 228 \t--- Loss: 0.702\n",
      "Iteration: 229 \t--- Loss: 0.724\n",
      "Iteration: 230 \t--- Loss: 0.708\n",
      "Iteration: 231 \t--- Loss: 0.680\n",
      "Iteration: 232 \t--- Loss: 0.699\n",
      "Iteration: 233 \t--- Loss: 0.694\n",
      "Iteration: 234 \t--- Loss: 0.688\n",
      "Iteration: 235 \t--- Loss: 0.691\n",
      "Iteration: 236 \t--- Loss: 0.696\n",
      "Iteration: 237 \t--- Loss: 0.704\n",
      "Iteration: 238 \t--- Loss: 0.689\n",
      "Iteration: 239 \t--- Loss: 0.684\n",
      "Iteration: 240 \t--- Loss: 0.715\n",
      "Iteration: 241 \t--- Loss: 0.695\n",
      "Iteration: 242 \t--- Loss: 0.713\n",
      "Iteration: 243 \t--- Loss: 0.707\n",
      "Iteration: 244 \t--- Loss: 0.697\n",
      "Iteration: 245 \t--- Loss: 0.702\n",
      "Iteration: 246 \t--- Loss: 0.693\n",
      "Iteration: 247 \t--- Loss: 0.691\n",
      "Iteration: 248 \t--- Loss: 0.700\n",
      "Iteration: 249 \t--- Loss: 0.706\n",
      "Iteration: 250 \t--- Loss: 0.709\n",
      "Iteration: 251 \t--- Loss: 0.709\n",
      "Iteration: 252 \t--- Loss: 0.710\n",
      "Iteration: 253 \t--- Loss: 0.703\n",
      "Iteration: 254 \t--- Loss: 0.706\n",
      "Iteration: 255 \t--- Loss: 0.703\n",
      "Iteration: 256 \t--- Loss: 0.685\n",
      "Iteration: 257 \t--- Loss: 0.693\n",
      "Iteration: 258 \t--- Loss: 0.705\n",
      "Iteration: 259 \t--- Loss: 0.682"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it][Parallel(n_jobs=5)]: Done  85 tasks      | elapsed: 52.6min\n",
      " 10%|█         | 1/10 [00:00<00:08,  1.04it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:07<00:07,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 1.042\n",
      "Iteration: 1 \t--- Loss: 0.854\n",
      "Iteration: 2 \t--- Loss: 0.813\n",
      "Iteration: 3 \t--- Loss: 0.801\n",
      "Iteration: 4 \t--- Loss: 0.734\n",
      "Iteration: 5 \t--- Loss: 0.719\n",
      "Iteration: 6 \t--- Loss: 0.698\n",
      "Iteration: 7 \t--- Loss: 0.641\n",
      "Iteration: 8 \t--- Loss: 0.637\n",
      "Iteration: 9 \t--- Loss: 0.633\n",
      "Iteration: 10 \t--- Loss: 0.604\n",
      "Iteration: 11 \t--- Loss: 0.596\n",
      "Iteration: 12 \t--- Loss: 0.566\n",
      "Iteration: 13 \t--- Loss: 0.585\n",
      "Iteration: 14 \t--- Loss: 0.569\n",
      "Iteration: 15 \t--- Loss: 0.559\n",
      "Iteration: 16 \t--- Loss: 0.560\n",
      "Iteration: 17 \t--- Loss: 0.580\n",
      "Iteration: 18 \t--- Loss: 0.569\n",
      "Iteration: 19 \t--- Loss: 0.554\n",
      "Iteration: 20 \t--- Loss: 0.549\n",
      "Iteration: 21 \t--- Loss: 0.543\n",
      "Iteration: 22 \t--- Loss: 0.563\n",
      "Iteration: 23 \t--- Loss: 0.567\n",
      "Iteration: 24 \t--- Loss: 0.557\n",
      "Iteration: 25 \t--- Loss: 0.557\n",
      "Iteration: 26 \t--- Loss: 0.536\n",
      "Iteration: 27 \t--- Loss: 0.544\n",
      "Iteration: 28 \t--- Loss: 0.568\n",
      "Iteration: 29 \t--- Loss: 0.548\n",
      "Iteration: 30 \t--- Loss: 0.569\n",
      "Iteration: 31 \t--- Loss: 0.527\n",
      "Iteration: 32 \t--- Loss: 0.563\n",
      "Iteration: 33 \t--- Loss: 0.565\n",
      "Iteration: 34 \t--- Loss: 0.550\n",
      "Iteration: 35 \t--- Loss: 0.541\n",
      "Iteration: 36 \t--- Loss: 0.568\n",
      "Iteration: 37 \t--- Loss: 0.563\n",
      "Iteration: 38 \t--- Loss: 0.559\n",
      "Iteration: 39 \t--- Loss: 0.532\n",
      "Iteration: 40 \t--- Loss: 0.553\n",
      "Iteration: 41 \t--- Loss: 0.540\n",
      "Iteration: 42 \t--- Loss: 0.555\n",
      "Iteration: 43 \t--- Loss: 0.544\n",
      "Iteration: 44 \t--- Loss: 0.562\n",
      "Iteration: 45 \t--- Loss: 0.546\n",
      "Iteration: 46 \t--- Loss: 0.561\n",
      "Iteration: 47 \t--- Loss: 0.545\n",
      "Iteration: 48 \t--- Loss: 0.563\n",
      "Iteration: 49 \t--- Loss: 0.545\n",
      "Iteration: 50 \t--- Loss: 0.552\n",
      "Iteration: 51 \t--- Loss: 0.569\n",
      "Iteration: 52 \t--- Loss: 0.543\n",
      "Iteration: 53 \t--- Loss: 0.529\n",
      "Iteration: 54 \t--- Loss: 0.545\n",
      "Iteration: 55 \t--- Loss: 0.549\n",
      "Iteration: 56 \t--- Loss: 0.525\n",
      "Iteration: 57 \t--- Loss: 0.554\n",
      "Iteration: 58 \t--- Loss: 0.531\n",
      "Iteration: 59 \t--- Loss: 0.517\n",
      "Iteration: 60 \t--- Loss: 0.542\n",
      "Iteration: 61 \t--- Loss: 0.542\n",
      "Iteration: 62 \t--- Loss: 0.548\n",
      "Iteration: 63 \t--- Loss: 0.558\n",
      "Iteration: 64 \t--- Loss: 0.565\n",
      "Iteration: 65 \t--- Loss: 0.541\n",
      "Iteration: 66 \t--- Loss: 0.562\n",
      "Iteration: 67 \t--- Loss: 0.561\n",
      "Iteration: 68 \t--- Loss: 0.562\n",
      "Iteration: 69 \t--- Loss: 0.546\n",
      "Iteration: 70 \t--- Loss: 0.553\n",
      "Iteration: 71 \t--- Loss: 0.539\n",
      "Iteration: 72 \t--- Loss: 0.540\n",
      "Iteration: 73 \t--- Loss: 0.567\n",
      "Iteration: 74 \t--- Loss: 0.527\n",
      "Iteration: 75 \t--- Loss: 0.563\n",
      "Iteration: 76 \t--- Loss: 0.564\n",
      "Iteration: 77 \t--- Loss: 0.565\n",
      "Iteration: 78 \t--- Loss: 0.550\n",
      "Iteration: 79 \t--- Loss: 0.564\n",
      "Iteration: 80 \t--- Loss: 0.550\n",
      "Iteration: 81 \t--- Loss: 0.573\n",
      "Iteration: 82 \t--- Loss: 0.524\n",
      "Iteration: 83 \t--- Loss: 0.559\n",
      "Iteration: 84 \t--- Loss: 0.563\n",
      "Iteration: 85 \t--- Loss: 0.526\n",
      "Iteration: 86 \t--- Loss: 0.545\n",
      "Iteration: 87 \t--- Loss: 0.551\n",
      "Iteration: 88 \t--- Loss: 0.568\n",
      "Iteration: 89 \t--- Loss: 0.566\n",
      "Iteration: 90 \t--- Loss: 0.542\n",
      "Iteration: 91 \t--- Loss: 0.552\n",
      "Iteration: 92 \t--- Loss: 0.551\n",
      "Iteration: 93 \t--- Loss: 0.540\n",
      "Iteration: 94 \t--- Loss: 0.527\n",
      "Iteration: 95 \t--- Loss: 0.564\n",
      "Iteration: 96 \t--- Loss: 0.520\n",
      "Iteration: 97 \t--- Loss: 0.560\n",
      "Iteration: 98 \t--- Loss: 0.578\n",
      "Iteration: 99 \t--- Loss: 0.544\n",
      "Iteration: 100 \t--- Loss: 0.531\n",
      "Iteration: 101 \t--- Loss: 0.521\n",
      "Iteration: 102 \t--- Loss: 0.550\n",
      "Iteration: 103 \t--- Loss: 0.549\n",
      "Iteration: 104 \t--- Loss: 0.561\n",
      "Iteration: 105 \t--- Loss: 0.556\n",
      "Iteration: 106 \t--- Loss: 0.534\n",
      "Iteration: 107 \t--- Loss: 0.556\n",
      "Iteration: 108 \t--- Loss: 0.528\n",
      "Iteration: 109 \t--- Loss: 0.572\n",
      "Iteration: 110 \t--- Loss: 0.547\n",
      "Iteration: 111 \t--- Loss: 0.516\n",
      "Iteration: 112 \t--- Loss: 0.546\n",
      "Iteration: 113 \t--- Loss: 0.539\n",
      "Iteration: 114 \t--- Loss: 0.541\n",
      "Iteration: 115 \t--- Loss: 0.547\n",
      "Iteration: 116 \t--- Loss: 0.554\n",
      "Iteration: 117 \t--- Loss: 0.554\n",
      "Iteration: 118 \t--- Loss: 0.534\n",
      "Iteration: 119 \t--- Loss: 0.529\n",
      "Iteration: 120 \t--- Loss: 0.554\n",
      "Iteration: 121 \t--- Loss: 0.563\n",
      "Iteration: 122 \t--- Loss: 0.532\n",
      "Iteration: 123 \t--- Loss: 0.571\n",
      "Iteration: 124 \t--- Loss: 0.542\n",
      "Iteration: 125 \t--- Loss: 0.552\n",
      "Iteration: 126 \t--- Loss: 0.532\n",
      "Iteration: 127 \t--- Loss: 0.580\n",
      "Iteration: 128 \t--- Loss: 0.563\n",
      "Iteration: 129 \t--- Loss: 0.532\n",
      "Iteration: 130 \t--- Loss: 0.551\n",
      "Iteration: 131 \t--- Loss: 0.547\n",
      "Iteration: 132 \t--- Loss: 0.556\n",
      "Iteration: 133 \t--- Loss: 0.547\n",
      "Iteration: 134 \t--- Loss: 0.538\n",
      "Iteration: 135 \t--- Loss: 0.544\n",
      "Iteration: 136 \t--- Loss: 0.563\n",
      "Iteration: 137 \t--- Loss: 0.524\n",
      "Iteration: 138 \t--- Loss: 0.541\n",
      "Iteration: 139 \t--- Loss: 0.535\n",
      "Iteration: 140 \t--- Loss: 0.542\n",
      "Iteration: 141 \t--- Loss: 0.551\n",
      "Iteration: 142 \t--- Loss: 0.535\n",
      "Iteration: 143 \t--- Loss: 0.544\n",
      "Iteration: 144 \t--- Loss: 0.567\n",
      "Iteration: 145 \t--- Loss: 0.535\n",
      "Iteration: 146 \t--- Loss: 0.552\n",
      "Iteration: 147 \t--- Loss: 0.541\n",
      "Iteration: 148 \t--- Loss: 0.548\n",
      "Iteration: 149 \t--- Loss: 0.541\n",
      "Iteration: 150 \t--- Loss: 0.550\n",
      "Iteration: 151 \t--- Loss: 0.545\n",
      "Iteration: 152 \t--- Loss: 0.563\n",
      "Iteration: 153 \t--- Loss: 0.572\n",
      "Iteration: 154 \t--- Loss: 0.568\n",
      "Iteration: 155 \t--- Loss: 0.535\n",
      "Iteration: 156 \t--- Loss: 0.540\n",
      "Iteration: 157 \t--- Loss: 0.551\n",
      "Iteration: 158 \t--- Loss: 0.561\n",
      "Iteration: 159 \t--- Loss: 0.560\n",
      "Iteration: 160 \t--- Loss: 0.532\n",
      "Iteration: 161 \t--- Loss: 0.550\n",
      "Iteration: 162 \t--- Loss: 0.540\n",
      "Iteration: 163 \t--- Loss: 0.521\n",
      "Iteration: 164 \t--- Loss: 0.531\n",
      "Iteration: 165 \t--- Loss: 0.552\n",
      "Iteration: 166 \t--- Loss: 0.550\n",
      "Iteration: 167 \t--- Loss: 0.560\n",
      "Iteration: 168 \t--- Loss: 0.546\n",
      "Iteration: 169 \t--- Loss: 0.528\n",
      "Iteration: 170 \t--- Loss: 0.547\n",
      "Iteration: 171 \t--- Loss: 0.521\n",
      "Iteration: 172 \t--- Loss: 0.527\n",
      "Iteration: 173 \t--- Loss: 0.549\n",
      "Iteration: 174 \t--- Loss: 0.554\n",
      "Iteration: 175 \t--- Loss: 0.552\n",
      "Iteration: 176 \t--- Loss: 0.547\n",
      "Iteration: 177 \t--- Loss: 0.534\n",
      "Iteration: 178 \t--- Loss: 0.536\n",
      "Iteration: 179 \t--- Loss: 0.550\n",
      "Iteration: 180 \t--- Loss: 0.546\n",
      "Iteration: 181 \t--- Loss: 0.530\n",
      "Iteration: 182 \t--- Loss: 0.539\n",
      "Iteration: 183 \t--- Loss: 0.547\n",
      "Iteration: 184 \t--- Loss: 0.562\n",
      "Iteration: 185 \t--- Loss: 0.548\n",
      "Iteration: 186 \t--- Loss: 0.560\n",
      "Iteration: 187 \t--- Loss: 0.560\n",
      "Iteration: 188 \t--- Loss: 0.543\n",
      "Iteration: 189 \t--- Loss: 0.547\n",
      "Iteration: 190 \t--- Loss: 0.559\n",
      "Iteration: 191 \t--- Loss: 0.550\n",
      "Iteration: 192 \t--- Loss: 0.548\n",
      "Iteration: 193 \t--- Loss: 0.554\n",
      "Iteration: 194 \t--- Loss: 0.546\n",
      "Iteration: 195 \t--- Loss: 0.539\n",
      "Iteration: 196 \t--- Loss: 0.550\n",
      "Iteration: 197 \t--- Loss: 0.547\n",
      "Iteration: 198 \t--- Loss: 0.539\n",
      "Iteration: 199 \t--- Loss: 0.528\n",
      "Iteration: 200 \t--- Loss: 0.543\n",
      "Iteration: 201 \t--- Loss: 0.548\n",
      "Iteration: 202 \t--- Loss: 0.521\n",
      "Iteration: 203 \t--- Loss: 0.543\n",
      "Iteration: 204 \t--- Loss: 0.558\n",
      "Iteration: 205 \t--- Loss: 0.546\n",
      "Iteration: 206 \t--- Loss: 0.542\n",
      "Iteration: 207 \t--- Loss: 0.546\n",
      "Iteration: 208 \t--- Loss: 0.554\n",
      "Iteration: 209 \t--- Loss: 0.548\n",
      "Iteration: 210 \t--- Loss: 0.527\n",
      "Iteration: 211 \t--- Loss: 0.540\n",
      "Iteration: 212 \t--- Loss: 0.547\n",
      "Iteration: 213 \t--- Loss: 0.536\n",
      "Iteration: 214 \t--- Loss: 0.553\n",
      "Iteration: 215 \t--- Loss: 0.545\n",
      "Iteration: 216 \t--- Loss: 0.539\n",
      "Iteration: 217 \t--- Loss: 0.535\n",
      "Iteration: 218 \t--- Loss: 0.555\n",
      "Iteration: 219 \t--- Loss: 0.571\n",
      "Iteration: 220 \t--- Loss: 0.551\n",
      "Iteration: 221 \t--- Loss: 0.540\n",
      "Iteration: 222 \t--- Loss: 0.559\n",
      "Iteration: 223 \t--- Loss: 0.559\n",
      "Iteration: 224 \t--- Loss: 0.561\n",
      "Iteration: 225 \t--- Loss: 0.533\n",
      "Iteration: 226 \t--- Loss: 0.546\n",
      "Iteration: 227 \t--- Loss: 0.542\n",
      "Iteration: 228 \t--- Loss: 0.551\n",
      "Iteration: 229 \t--- Loss: 0.540\n",
      "Iteration: 230 \t--- Loss: 0.555\n",
      "Iteration: 231 \t--- Loss: 0.563\n",
      "Iteration: 232 \t--- Loss: 0.528\n",
      "Iteration: 233 \t--- Loss: 0.525\n",
      "Iteration: 234 \t--- Loss: 0.547\n",
      "Iteration: 235 \t--- Loss: 0.555\n",
      "Iteration: 236 \t--- Loss: 0.541\n",
      "Iteration: 237 \t--- Loss: 0.539\n",
      "Iteration: 238 \t--- Loss: 0.541\n",
      "Iteration: 239 \t--- Loss: 0.554\n",
      "Iteration: 240 \t--- Loss: 0.546\n",
      "Iteration: 241 \t--- Loss: 0.564\n",
      "Iteration: 242 \t--- Loss: 0.564\n",
      "Iteration: 243 \t--- Loss: 0.535\n",
      "Iteration: 244 \t--- Loss: 0.549\n",
      "Iteration: 245 \t--- Loss: 0.548\n",
      "Iteration: 246 \t--- Loss: 0.562\n",
      "Iteration: 247 \t--- Loss: 0.550\n",
      "Iteration: 248 \t--- Loss: 0.553\n",
      "Iteration: 249 \t--- Loss: 0.535\n",
      "Iteration: 250 \t--- Loss: 0.553\n",
      "Iteration: 251 \t--- Loss: 0.536\n",
      "Iteration: 252 \t--- Loss: 0.552\n",
      "Iteration: 253 \t--- Loss: 0.568\n",
      "Iteration: 254 \t--- Loss: 0.541\n",
      "Iteration: 255 \t--- Loss: 0.552\n",
      "Iteration: 256 \t--- Loss: 0.566\n",
      "Iteration: 257 \t--- Loss: 0.546\n",
      "Iteration: 258 \t--- Loss: 0.543\n",
      "Iteration: 259 \t--- Loss: 0.542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/it][Parallel(n_jobs=5)]: Done  86 tasks      | elapsed: 52.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [01:27<00:00, 87.79s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.693\n",
      "Iteration: 261 \t--- Loss: 0.690\n",
      "Iteration: 262 \t--- Loss: 0.685\n",
      "Iteration: 263 \t--- Loss: 0.720\n",
      "Iteration: 264 \t--- Loss: 0.706\n",
      "Iteration: 265 \t--- Loss: 0.701\n",
      "Iteration: 266 \t--- Loss: 0.718\n",
      "Iteration: 267 \t--- Loss: 0.712\n",
      "Iteration: 268 \t--- Loss: 0.699\n",
      "Iteration: 269 \t--- Loss: 0.691\n",
      "Iteration: 270 \t--- Loss: 0.690\n",
      "Iteration: 271 \t--- Loss: 0.733\n",
      "Iteration: 272 \t--- Loss: 0.690\n",
      "Iteration: 273 \t--- Loss: 0.703\n",
      "Iteration: 274 \t--- Loss: 0.702\n",
      "Iteration: 275 \t--- Loss: 0.694\n",
      "Iteration: 276 \t--- Loss: 0.697\n",
      "Iteration: 277 \t--- Loss: 0.717\n",
      "Iteration: 278 \t--- Loss: 0.709\n",
      "Iteration: 279 \t--- Loss: 0.701\n",
      "Iteration: 280 \t--- Loss: 0.700\n",
      "Iteration: 281 \t--- Loss: 0.726\n",
      "Iteration: 282 \t--- Loss: 0.670\n",
      "Iteration: 283 \t--- Loss: 0.739\n",
      "Iteration: 284 \t--- Loss: 0.695\n",
      "Iteration: 285 \t--- Loss: 0.692\n",
      "Iteration: 286 \t--- Loss: 0.707\n",
      "Iteration: 287 \t--- Loss: 0.696\n",
      "Iteration: 288 \t--- Loss: 0.696\n",
      "Iteration: 289 \t--- Loss: 0.722\n",
      "Iteration: 290 \t--- Loss: 0.711\n",
      "Iteration: 291 \t--- Loss: 0.671\n",
      "Iteration: 292 \t--- Loss: 0.691\n",
      "Iteration: 293 \t--- Loss: 0.688\n",
      "Iteration: 294 \t--- Loss: 0.702\n",
      "Iteration: 295 \t--- Loss: 0.693\n",
      "Iteration: 296 \t--- Loss: 0.709\n",
      "Iteration: 297 \t--- Loss: 0.709\n",
      "Iteration: 298 \t--- Loss: 0.698\n",
      "Iteration: 299 \t--- Loss: 0.699\n",
      "Iteration: 300 \t--- Loss: 0.683\n",
      "Iteration: 301 \t--- Loss: 0.698\n",
      "Iteration: 302 \t--- Loss: 0.692\n",
      "Iteration: 303 \t--- Loss: 0.695\n",
      "Iteration: 304 \t--- Loss: 0.706\n",
      "Iteration: 305 \t--- Loss: 0.710\n",
      "Iteration: 306 \t--- Loss: 0.720\n",
      "Iteration: 307 \t--- Loss: 0.700\n",
      "Iteration: 308 \t--- Loss: 0.697\n",
      "Iteration: 309 \t--- Loss: 0.698\n",
      "Iteration: 310 \t--- Loss: 0.716\n",
      "Iteration: 311 \t--- Loss: 0.697\n",
      "Iteration: 312 \t--- Loss: 0.682\n",
      "Iteration: 313 \t--- Loss: 0.695\n",
      "Iteration: 314 \t--- Loss: 0.694\n",
      "Iteration: 315 \t--- Loss: 0.667\n",
      "Iteration: 316 \t--- Loss: 0.704\n",
      "Iteration: 317 \t--- Loss: 0.714\n",
      "Iteration: 318 \t--- Loss: 0.703\n",
      "Iteration: 319 \t--- Loss: 0.685\n",
      "Iteration: 320 \t--- Loss: 0.687\n",
      "Iteration: 321 \t--- Loss: 0.708\n",
      "Iteration: 322 \t--- Loss: 0.709\n",
      "Iteration: 323 \t--- Loss: 0.676\n",
      "Iteration: 324 \t--- Loss: 0.694\n",
      "Iteration: 325 \t--- Loss: 0.711\n",
      "Iteration: 326 \t--- Loss: 0.702\n",
      "Iteration: 327 \t--- Loss: 0.709\n",
      "Iteration: 328 \t--- Loss: 0.692\n",
      "Iteration: 329 \t--- Loss: 0.692\n",
      "Iteration: 330 \t--- Loss: 0.707\n",
      "Iteration: 331 \t--- Loss: 0.688\n",
      "Iteration: 332 \t--- Loss: 0.696\n",
      "Iteration: 333 \t--- Loss: 0.709\n",
      "Iteration: 334 \t--- Loss: 0.710\n",
      "Iteration: 335 \t--- Loss: 0.698\n",
      "Iteration: 336 \t--- Loss: 0.727\n",
      "Iteration: 337 \t--- Loss: 0.681\n",
      "Iteration: 338 \t--- Loss: 0.685\n",
      "Iteration: 339 \t--- Loss: 0.690\n",
      "Iteration: 340 \t--- Loss: 0.715\n",
      "Iteration: 341 \t--- Loss: 0.701\n",
      "Iteration: 342 \t--- Loss: 0.714\n",
      "Iteration: 343 \t--- Loss: 0.727\n",
      "Iteration: 344 \t--- Loss: 0.686\n",
      "Iteration: 345 \t--- Loss: 0.710\n",
      "Iteration: 346 \t--- Loss: 0.673\n",
      "Iteration: 347 \t--- Loss: 0.686\n",
      "Iteration: 348 \t--- Loss: 0.704\n",
      "Iteration: 349 \t--- Loss: 0.681\n",
      "Iteration: 350 \t--- Loss: 0.701\n",
      "Iteration: 351 \t--- Loss: 0.699\n",
      "Iteration: 352 \t--- Loss: 0.670\n",
      "Iteration: 353 \t--- Loss: 0.712\n",
      "Iteration: 354 \t--- Loss: 0.696\n",
      "Iteration: 355 \t--- Loss: 0.685\n",
      "Iteration: 356 \t--- Loss: 0.699\n",
      "Iteration: 357 \t--- Loss: 0.688\n",
      "Iteration: 358 \t--- Loss: 0.693\n",
      "Iteration: 359 \t--- Loss: 0.691\n",
      "Iteration: 360 \t--- Loss: 0.689\n",
      "Iteration: 361 \t--- Loss: 0.705\n",
      "Iteration: 362 \t--- Loss: 0.706\n",
      "Iteration: 363 \t--- Loss: 0.681\n",
      "Iteration: 364 \t--- Loss: 0.726\n",
      "Iteration: 365 \t--- Loss: 0.714\n",
      "Iteration: 366 \t--- Loss: 0.717\n",
      "Iteration: 367 \t--- Loss: 0.695\n",
      "Iteration: 368 \t--- Loss: 0.685\n",
      "Iteration: 369 \t--- Loss: 0.690\n",
      "Iteration: 370 \t--- Loss: 0.683\n",
      "Iteration: 371 \t--- Loss: 0.688\n",
      "Iteration: 372 \t--- Loss: 0.673\n",
      "Iteration: 373 \t--- Loss: 0.713\n",
      "Iteration: 374 \t--- Loss: 0.714\n",
      "Iteration: 375 \t--- Loss: 0.692\n",
      "Iteration: 376 \t--- Loss: 0.701\n",
      "Iteration: 377 \t--- Loss: 0.719\n",
      "Iteration: 378 \t--- Loss: 0.688\n",
      "Iteration: 379 \t--- Loss: 0.733\n",
      "Iteration: 380 \t--- Loss: 0.702\n",
      "Iteration: 381 \t--- Loss: 0.698\n",
      "Iteration: 382 \t--- Loss: 0.696\n",
      "Iteration: 383 \t--- Loss: 0.684\n",
      "Iteration: 384 \t--- Loss: 0.698\n",
      "Iteration: 385 \t--- Loss: 0.725\n",
      "Iteration: 386 \t--- Loss: 0.696\n",
      "Iteration: 387 \t--- Loss: 0.691\n",
      "Iteration: 388 \t--- Loss: 0.718\n",
      "Iteration: 389 \t--- Loss: 0.697\n",
      "Iteration: 390 \t--- Loss: 0.692\n",
      "Iteration: 391 \t--- Loss: 0.680\n",
      "Iteration: 392 \t--- Loss: 0.700\n",
      "Iteration: 393 \t--- Loss: 0.691\n",
      "Iteration: 394 \t--- Loss: 0.686\n",
      "Iteration: 395 \t--- Loss: 0.700\n",
      "Iteration: 396 \t--- Loss: 0.708\n",
      "Iteration: 397 \t--- Loss: 0.735\n",
      "Iteration: 398 \t--- Loss: 0.691\n",
      "Iteration: 399 \t--- Loss: 0.684\n",
      "Iteration: 400 \t--- Loss: 0.698\n",
      "Iteration: 401 \t--- Loss: 0.712\n",
      "Iteration: 402 \t--- Loss: 0.686\n",
      "Iteration: 403 \t--- Loss: 0.709\n",
      "Iteration: 404 \t--- Loss: 0.686\n",
      "Iteration: 405 \t--- Loss: 0.700\n",
      "Iteration: 406 \t--- Loss: 0.721\n",
      "Iteration: 407 \t--- Loss: 0.700\n",
      "Iteration: 408 \t--- Loss: 0.711\n",
      "Iteration: 409 \t--- Loss: 0.697\n",
      "Iteration: 410 \t--- Loss: 0.703\n",
      "Iteration: 411 \t--- Loss: 0.694\n",
      "Iteration: 412 \t--- Loss: 0.684\n",
      "Iteration: 413 \t--- Loss: 0.727\n",
      "Iteration: 414 \t--- Loss: 0.686\n",
      "Iteration: 415 \t--- Loss: 0.684\n",
      "Iteration: 416 \t--- Loss: 0.704\n",
      "Iteration: 417 \t--- Loss: 0.704\n",
      "Iteration: 418 \t--- Loss: 0.702\n",
      "Iteration: 419 \t--- Loss: 0.699\n",
      "Iteration: 420 \t--- Loss: 0.693\n",
      "Iteration: 421 \t--- Loss: 0.710\n",
      "Iteration: 422 \t--- Loss: 0.709\n",
      "Iteration: 423 \t--- Loss: 0.712\n",
      "Iteration: 424 \t--- Loss: 0.693\n",
      "Iteration: 425 \t--- Loss: 0.688\n",
      "Iteration: 426 \t--- Loss: 0.702\n",
      "Iteration: 427 \t--- Loss: 0.706\n",
      "Iteration: 428 \t--- Loss: 0.714\n",
      "Iteration: 429 \t--- Loss: 0.674\n",
      "Iteration: 430 \t--- Loss: 0.694\n",
      "Iteration: 431 \t--- Loss: 0.722\n",
      "Iteration: 432 \t--- Loss: 0.696\n",
      "Iteration: 433 \t--- Loss: 0.712\n",
      "Iteration: 434 \t--- Loss: 0.728\n",
      "Iteration: 435 \t--- Loss: 0.698\n",
      "Iteration: 436 \t--- Loss: 0.711\n",
      "Iteration: 437 \t--- Loss: 0.695\n",
      "Iteration: 438 \t--- Loss: 0.709\n",
      "Iteration: 439 \t--- Loss: 0.693\n",
      "Iteration: 440 \t--- Loss: 0.694\n",
      "Iteration: 441 \t--- Loss: 0.678\n",
      "Iteration: 442 \t--- Loss: 0.677\n",
      "Iteration: 443 \t--- Loss: 0.698\n",
      "Iteration: 444 \t--- Loss: 0.693\n",
      "Iteration: 445 \t--- Loss: 0.690\n",
      "Iteration: 446 \t--- Loss: 0.691\n",
      "Iteration: 447 \t--- Loss: 0.709\n",
      "Iteration: 448 \t--- Loss: 0.708\n",
      "Iteration: 449 \t--- Loss: 0.696\n",
      "Iteration: 450 \t--- Loss: 0.690\n",
      "Iteration: 451 \t--- Loss: 0.689\n",
      "Iteration: 452 \t--- Loss: 0.694\n",
      "Iteration: 453 \t--- Loss: 0.681\n",
      "Iteration: 454 \t--- Loss: 0.689\n",
      "Iteration: 455 \t--- Loss: 0.679\n",
      "Iteration: 456 \t--- Loss: 0.700\n",
      "Iteration: 457 \t--- Loss: 0.697\n",
      "Iteration: 458 \t--- Loss: 0.700\n",
      "Iteration: 459 \t--- Loss: 0.727\n",
      "Iteration: 460 \t--- Loss: 0.694\n",
      "Iteration: 461 \t--- Loss: 0.689\n",
      "Iteration: 462 \t--- Loss: 0.698\n",
      "Iteration: 463 \t--- Loss: 0.703\n",
      "Iteration: 464 \t--- Loss: 0.691\n",
      "Iteration: 465 \t--- Loss: 0.706\n",
      "Iteration: 466 \t--- Loss: 0.698\n",
      "Iteration: 467 \t--- Loss: 0.699\n",
      "Iteration: 468 \t--- Loss: 0.698\n",
      "Iteration: 469 \t--- Loss: 0.721\n",
      "Iteration: 470 \t--- Loss: 0.707\n",
      "Iteration: 471 \t--- Loss: 0.693\n",
      "Iteration: 472 \t--- Loss: 0.700\n",
      "Iteration: 473 \t--- Loss: 0.692\n",
      "Iteration: 474 \t--- Loss: 0.719\n",
      "Iteration: 475 \t--- Loss: 0.693\n",
      "Iteration: 476 \t--- Loss: 0.695\n",
      "Iteration: 477 \t--- Loss: 0.708\n",
      "Iteration: 478 \t--- Loss: 0.699\n",
      "Iteration: 479 \t--- Loss: 0.707\n",
      "Iteration: 480 \t--- Loss: 0.696\n",
      "Iteration: 481 \t--- Loss: 0.710\n",
      "Iteration: 482 \t--- Loss: 0.722\n",
      "Iteration: 483 \t--- Loss: 0.702\n",
      "Iteration: 484 \t--- Loss: 0.714\n",
      "Iteration: 485 \t--- Loss: 0.706\n",
      "Iteration: 486 \t--- Loss: 0.691\n",
      "Iteration: 487 \t--- Loss: 0.680\n",
      "Iteration: 488 \t--- Loss: 0.698\n",
      "Iteration: 489 \t--- Loss: 0.686\n",
      "Iteration: 490 \t--- Loss: 0.688\n",
      "Iteration: 491 \t--- Loss: 0.686\n",
      "Iteration: 492 \t--- Loss: 0.687\n",
      "Iteration: 493 \t--- Loss: 0.692\n",
      "Iteration: 494 \t--- Loss: 0.711\n",
      "Iteration: 495 \t--- Loss: 0.719\n",
      "Iteration: 496 \t--- Loss: 0.709\n",
      "Iteration: 497 \t--- Loss: 0.712\n",
      "Iteration: 498 \t--- Loss: 0.733\n",
      "Iteration: 499 \t--- Loss: 0.701\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:25<00:00, 85.33s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.554\n",
      "Iteration: 261 \t--- Loss: 0.537\n",
      "Iteration: 262 \t--- Loss: 0.558\n",
      "Iteration: 263 \t--- Loss: 0.541\n",
      "Iteration: 264 \t--- Loss: 0.516\n",
      "Iteration: 265 \t--- Loss: 0.535\n",
      "Iteration: 266 \t--- Loss: 0.538\n",
      "Iteration: 267 \t--- Loss: 0.556\n",
      "Iteration: 268 \t--- Loss: 0.580\n",
      "Iteration: 269 \t--- Loss: 0.558\n",
      "Iteration: 270 \t--- Loss: 0.541\n",
      "Iteration: 271 \t--- Loss: 0.562\n",
      "Iteration: 272 \t--- Loss: 0.566\n",
      "Iteration: 273 \t--- Loss: 0.537\n",
      "Iteration: 274 \t--- Loss: 0.550\n",
      "Iteration: 275 \t--- Loss: 0.542\n",
      "Iteration: 276 \t--- Loss: 0.548\n",
      "Iteration: 277 \t--- Loss: 0.524\n",
      "Iteration: 278 \t--- Loss: 0.520\n",
      "Iteration: 279 \t--- Loss: 0.536\n",
      "Iteration: 280 \t--- Loss: 0.542\n",
      "Iteration: 281 \t--- Loss: 0.553\n",
      "Iteration: 282 \t--- Loss: 0.536\n",
      "Iteration: 283 \t--- Loss: 0.524\n",
      "Iteration: 284 \t--- Loss: 0.538\n",
      "Iteration: 285 \t--- Loss: 0.545\n",
      "Iteration: 286 \t--- Loss: 0.530\n",
      "Iteration: 287 \t--- Loss: 0.553\n",
      "Iteration: 288 \t--- Loss: 0.547\n",
      "Iteration: 289 \t--- Loss: 0.534\n",
      "Iteration: 290 \t--- Loss: 0.513\n",
      "Iteration: 291 \t--- Loss: 0.542\n",
      "Iteration: 292 \t--- Loss: 0.557\n",
      "Iteration: 293 \t--- Loss: 0.550\n",
      "Iteration: 294 \t--- Loss: 0.545\n",
      "Iteration: 295 \t--- Loss: 0.519\n",
      "Iteration: 296 \t--- Loss: 0.539\n",
      "Iteration: 297 \t--- Loss: 0.565\n",
      "Iteration: 298 \t--- Loss: 0.550\n",
      "Iteration: 299 \t--- Loss: 0.539\n",
      "Iteration: 300 \t--- Loss: 0.543\n",
      "Iteration: 301 \t--- Loss: 0.570\n",
      "Iteration: 302 \t--- Loss: 0.534\n",
      "Iteration: 303 \t--- Loss: 0.564\n",
      "Iteration: 304 \t--- Loss: 0.533\n",
      "Iteration: 305 \t--- Loss: 0.534\n",
      "Iteration: 306 \t--- Loss: 0.543\n",
      "Iteration: 307 \t--- Loss: 0.528\n",
      "Iteration: 308 \t--- Loss: 0.560\n",
      "Iteration: 309 \t--- Loss: 0.534\n",
      "Iteration: 310 \t--- Loss: 0.521\n",
      "Iteration: 311 \t--- Loss: 0.534\n",
      "Iteration: 312 \t--- Loss: 0.546\n",
      "Iteration: 313 \t--- Loss: 0.543\n",
      "Iteration: 314 \t--- Loss: 0.540\n",
      "Iteration: 315 \t--- Loss: 0.556\n",
      "Iteration: 316 \t--- Loss: 0.539\n",
      "Iteration: 317 \t--- Loss: 0.546\n",
      "Iteration: 318 \t--- Loss: 0.559\n",
      "Iteration: 319 \t--- Loss: 0.535\n",
      "Iteration: 320 \t--- Loss: 0.540\n",
      "Iteration: 321 \t--- Loss: 0.561\n",
      "Iteration: 322 \t--- Loss: 0.544\n",
      "Iteration: 323 \t--- Loss: 0.545\n",
      "Iteration: 324 \t--- Loss: 0.548\n",
      "Iteration: 325 \t--- Loss: 0.570\n",
      "Iteration: 326 \t--- Loss: 0.565\n",
      "Iteration: 327 \t--- Loss: 0.552\n",
      "Iteration: 328 \t--- Loss: 0.534\n",
      "Iteration: 329 \t--- Loss: 0.534\n",
      "Iteration: 330 \t--- Loss: 0.546\n",
      "Iteration: 331 \t--- Loss: 0.548\n",
      "Iteration: 332 \t--- Loss: 0.562\n",
      "Iteration: 333 \t--- Loss: 0.528\n",
      "Iteration: 334 \t--- Loss: 0.542\n",
      "Iteration: 335 \t--- Loss: 0.535\n",
      "Iteration: 336 \t--- Loss: 0.554\n",
      "Iteration: 337 \t--- Loss: 0.550\n",
      "Iteration: 338 \t--- Loss: 0.549\n",
      "Iteration: 339 \t--- Loss: 0.568\n",
      "Iteration: 340 \t--- Loss: 0.545\n",
      "Iteration: 341 \t--- Loss: 0.519\n",
      "Iteration: 342 \t--- Loss: 0.567\n",
      "Iteration: 343 \t--- Loss: 0.536\n",
      "Iteration: 344 \t--- Loss: 0.544\n",
      "Iteration: 345 \t--- Loss: 0.544\n",
      "Iteration: 346 \t--- Loss: 0.540\n",
      "Iteration: 347 \t--- Loss: 0.564\n",
      "Iteration: 348 \t--- Loss: 0.562\n",
      "Iteration: 349 \t--- Loss: 0.533\n",
      "Iteration: 350 \t--- Loss: 0.554\n",
      "Iteration: 351 \t--- Loss: 0.564\n",
      "Iteration: 352 \t--- Loss: 0.525\n",
      "Iteration: 353 \t--- Loss: 0.561\n",
      "Iteration: 354 \t--- Loss: 0.536\n",
      "Iteration: 355 \t--- Loss: 0.539\n",
      "Iteration: 356 \t--- Loss: 0.566\n",
      "Iteration: 357 \t--- Loss: 0.536\n",
      "Iteration: 358 \t--- Loss: 0.557\n",
      "Iteration: 359 \t--- Loss: 0.542\n",
      "Iteration: 360 \t--- Loss: 0.570\n",
      "Iteration: 361 \t--- Loss: 0.529\n",
      "Iteration: 362 \t--- Loss: 0.541\n",
      "Iteration: 363 \t--- Loss: 0.574\n",
      "Iteration: 364 \t--- Loss: 0.542\n",
      "Iteration: 365 \t--- Loss: 0.547\n",
      "Iteration: 366 \t--- Loss: 0.554\n",
      "Iteration: 367 \t--- Loss: 0.555\n",
      "Iteration: 368 \t--- Loss: 0.560\n",
      "Iteration: 369 \t--- Loss: 0.556\n",
      "Iteration: 370 \t--- Loss: 0.542\n",
      "Iteration: 371 \t--- Loss: 0.554\n",
      "Iteration: 372 \t--- Loss: 0.561\n",
      "Iteration: 373 \t--- Loss: 0.541\n",
      "Iteration: 374 \t--- Loss: 0.541\n",
      "Iteration: 375 \t--- Loss: 0.557\n",
      "Iteration: 376 \t--- Loss: 0.558\n",
      "Iteration: 377 \t--- Loss: 0.541\n",
      "Iteration: 378 \t--- Loss: 0.564\n",
      "Iteration: 379 \t--- Loss: 0.532\n",
      "Iteration: 380 \t--- Loss: 0.549\n",
      "Iteration: 381 \t--- Loss: 0.547\n",
      "Iteration: 382 \t--- Loss: 0.530\n",
      "Iteration: 383 \t--- Loss: 0.547\n",
      "Iteration: 384 \t--- Loss: 0.565\n",
      "Iteration: 385 \t--- Loss: 0.532\n",
      "Iteration: 386 \t--- Loss: 0.547\n",
      "Iteration: 387 \t--- Loss: 0.546\n",
      "Iteration: 388 \t--- Loss: 0.530\n",
      "Iteration: 389 \t--- Loss: 0.543\n",
      "Iteration: 390 \t--- Loss: 0.555\n",
      "Iteration: 391 \t--- Loss: 0.552\n",
      "Iteration: 392 \t--- Loss: 0.539\n",
      "Iteration: 393 \t--- Loss: 0.539\n",
      "Iteration: 394 \t--- Loss: 0.547\n",
      "Iteration: 395 \t--- Loss: 0.541\n",
      "Iteration: 396 \t--- Loss: 0.563\n",
      "Iteration: 397 \t--- Loss: 0.528\n",
      "Iteration: 398 \t--- Loss: 0.565\n",
      "Iteration: 399 \t--- Loss: 0.560\n",
      "Iteration: 400 \t--- Loss: 0.544\n",
      "Iteration: 401 \t--- Loss: 0.550\n",
      "Iteration: 402 \t--- Loss: 0.521\n",
      "Iteration: 403 \t--- Loss: 0.583\n",
      "Iteration: 404 \t--- Loss: 0.574\n",
      "Iteration: 405 \t--- Loss: 0.547\n",
      "Iteration: 406 \t--- Loss: 0.543\n",
      "Iteration: 407 \t--- Loss: 0.537\n",
      "Iteration: 408 \t--- Loss: 0.528\n",
      "Iteration: 409 \t--- Loss: 0.515\n",
      "Iteration: 410 \t--- Loss: 0.557\n",
      "Iteration: 411 \t--- Loss: 0.531\n",
      "Iteration: 412 \t--- Loss: 0.571\n",
      "Iteration: 413 \t--- Loss: 0.545\n",
      "Iteration: 414 \t--- Loss: 0.549\n",
      "Iteration: 415 \t--- Loss: 0.556\n",
      "Iteration: 416 \t--- Loss: 0.543\n",
      "Iteration: 417 \t--- Loss: 0.537\n",
      "Iteration: 418 \t--- Loss: 0.537\n",
      "Iteration: 419 \t--- Loss: 0.551\n",
      "Iteration: 420 \t--- Loss: 0.543\n",
      "Iteration: 421 \t--- Loss: 0.540\n",
      "Iteration: 422 \t--- Loss: 0.538\n",
      "Iteration: 423 \t--- Loss: 0.545\n",
      "Iteration: 424 \t--- Loss: 0.536\n",
      "Iteration: 425 \t--- Loss: 0.552\n",
      "Iteration: 426 \t--- Loss: 0.528\n",
      "Iteration: 427 \t--- Loss: 0.554\n",
      "Iteration: 428 \t--- Loss: 0.580\n",
      "Iteration: 429 \t--- Loss: 0.561\n",
      "Iteration: 430 \t--- Loss: 0.566\n",
      "Iteration: 431 \t--- Loss: 0.529\n",
      "Iteration: 432 \t--- Loss: 0.567\n",
      "Iteration: 433 \t--- Loss: 0.531\n",
      "Iteration: 434 \t--- Loss: 0.546\n",
      "Iteration: 435 \t--- Loss: 0.552\n",
      "Iteration: 436 \t--- Loss: 0.541\n",
      "Iteration: 437 \t--- Loss: 0.529\n",
      "Iteration: 438 \t--- Loss: 0.553\n",
      "Iteration: 439 \t--- Loss: 0.561\n",
      "Iteration: 440 \t--- Loss: 0.563\n",
      "Iteration: 441 \t--- Loss: 0.556\n",
      "Iteration: 442 \t--- Loss: 0.543\n",
      "Iteration: 443 \t--- Loss: 0.561\n",
      "Iteration: 444 \t--- Loss: 0.534\n",
      "Iteration: 445 \t--- Loss: 0.538\n",
      "Iteration: 446 \t--- Loss: 0.520\n",
      "Iteration: 447 \t--- Loss: 0.523\n",
      "Iteration: 448 \t--- Loss: 0.531\n",
      "Iteration: 449 \t--- Loss: 0.548\n",
      "Iteration: 450 \t--- Loss: 0.537\n",
      "Iteration: 451 \t--- Loss: 0.541\n",
      "Iteration: 452 \t--- Loss: 0.548\n",
      "Iteration: 453 \t--- Loss: 0.567\n",
      "Iteration: 454 \t--- Loss: 0.553\n",
      "Iteration: 455 \t--- Loss: 0.542\n",
      "Iteration: 456 \t--- Loss: 0.563\n",
      "Iteration: 457 \t--- Loss: 0.556\n",
      "Iteration: 458 \t--- Loss: 0.532\n",
      "Iteration: 459 \t--- Loss: 0.573\n",
      "Iteration: 460 \t--- Loss: 0.537\n",
      "Iteration: 461 \t--- Loss: 0.546\n",
      "Iteration: 462 \t--- Loss: 0.565\n",
      "Iteration: 463 \t--- Loss: 0.534\n",
      "Iteration: 464 \t--- Loss: 0.550\n",
      "Iteration: 465 \t--- Loss: 0.542\n",
      "Iteration: 466 \t--- Loss: 0.548\n",
      "Iteration: 467 \t--- Loss: 0.523\n",
      "Iteration: 468 \t--- Loss: 0.540\n",
      "Iteration: 469 \t--- Loss: 0.550\n",
      "Iteration: 470 \t--- Loss: 0.536\n",
      "Iteration: 471 \t--- Loss: 0.566\n",
      "Iteration: 472 \t--- Loss: 0.530\n",
      "Iteration: 473 \t--- Loss: 0.553\n",
      "Iteration: 474 \t--- Loss: 0.555\n",
      "Iteration: 475 \t--- Loss: 0.550\n",
      "Iteration: 476 \t--- Loss: 0.521\n",
      "Iteration: 477 \t--- Loss: 0.554\n",
      "Iteration: 478 \t--- Loss: 0.547\n",
      "Iteration: 479 \t--- Loss: 0.567\n",
      "Iteration: 480 \t--- Loss: 0.569\n",
      "Iteration: 481 \t--- Loss: 0.542\n",
      "Iteration: 482 \t--- Loss: 0.543\n",
      "Iteration: 483 \t--- Loss: 0.559\n",
      "Iteration: 484 \t--- Loss: 0.550\n",
      "Iteration: 485 \t--- Loss: 0.558\n",
      "Iteration: 486 \t--- Loss: 0.550\n",
      "Iteration: 487 \t--- Loss: 0.524\n",
      "Iteration: 488 \t--- Loss: 0.547\n",
      "Iteration: 489 \t--- Loss: 0.542\n",
      "Iteration: 490 \t--- Loss: 0.546\n",
      "Iteration: 491 \t--- Loss: 0.542\n",
      "Iteration: 492 \t--- Loss: 0.505\n",
      "Iteration: 493 \t--- Loss: 0.545\n",
      "Iteration: 494 \t--- Loss: 0.542\n",
      "Iteration: 495 \t--- Loss: 0.547\n",
      "Iteration: 496 \t--- Loss: 0.557\n",
      "Iteration: 497 \t--- Loss: 0.547\n",
      "Iteration: 498 \t--- Loss: 0.552\n",
      "Iteration: 499 \t--- Loss: 0.527\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:10<00:04,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.205\n",
      "Iteration: 1 \t--- Loss: 0.215\n",
      "Iteration: 2 \t--- Loss: 0.207\n",
      "Iteration: 3 \t--- Loss: 0.209\n",
      "Iteration: 4 \t--- Loss: 0.194\n",
      "Iteration: 5 \t--- Loss: 0.204\n",
      "Iteration: 6 \t--- Loss: 0.217\n",
      "Iteration: 7 \t--- Loss: 0.206\n",
      "Iteration: 8 \t--- Loss: 0.230\n",
      "Iteration: 9 \t--- Loss: 0.201\n",
      "Iteration: 10 \t--- Loss: 0.218\n",
      "Iteration: 11 \t--- Loss: 0.206\n",
      "Iteration: 12 \t--- Loss: 0.201\n",
      "Iteration: 13 \t--- Loss: 0.201\n",
      "Iteration: 14 \t--- Loss: 0.219\n",
      "Iteration: 15 \t--- Loss: 0.204\n",
      "Iteration: 16 \t--- Loss: 0.206\n",
      "Iteration: 17 \t--- Loss: 0.199\n",
      "Iteration: 18 \t--- Loss: 0.218\n",
      "Iteration: 19 \t--- Loss: 0.205\n",
      "Iteration: 20 \t--- Loss: 0.203\n",
      "Iteration: 21 \t--- Loss: 0.197\n",
      "Iteration: 22 \t--- Loss: 0.199\n",
      "Iteration: 23 \t--- Loss: 0.216\n",
      "Iteration: 24 \t--- Loss: 0.210\n",
      "Iteration: 25 \t--- Loss: 0.201\n",
      "Iteration: 26 \t--- Loss: 0.199\n",
      "Iteration: 27 \t--- Loss: 0.200\n",
      "Iteration: 28 \t--- Loss: 0.199\n",
      "Iteration: 29 \t--- Loss: 0.216\n",
      "Iteration: 30 \t--- Loss: 0.197\n",
      "Iteration: 31 \t--- Loss: 0.215\n",
      "Iteration: 32 \t--- Loss: 0.204\n",
      "Iteration: 33 \t--- Loss: 0.196\n",
      "Iteration: 34 \t--- Loss: 0.198\n",
      "Iteration: 35 \t--- Loss: 0.198\n",
      "Iteration: 36 \t--- Loss: 0.211\n",
      "Iteration: 37 \t--- Loss: 0.198\n",
      "Iteration: 38 \t--- Loss: 0.205\n",
      "Iteration: 39 \t--- Loss: 0.205\n",
      "Iteration: 40 \t--- Loss: 0.206\n",
      "Iteration: 41 \t--- Loss: 0.206\n",
      "Iteration: 42 \t--- Loss: 0.224\n",
      "Iteration: 43 \t--- Loss: 0.200\n",
      "Iteration: 44 \t--- Loss: 0.193\n",
      "Iteration: 45 \t--- Loss: 0.187\n",
      "Iteration: 46 \t--- Loss: 0.198\n",
      "Iteration: 47 \t--- Loss: 0.196\n",
      "Iteration: 48 \t--- Loss: 0.209\n",
      "Iteration: 49 \t--- Loss: 0.198\n",
      "Iteration: 50 \t--- Loss: 0.210\n",
      "Iteration: 51 \t--- Loss: 0.211\n",
      "Iteration: 52 \t--- Loss: 0.206\n",
      "Iteration: 53 \t--- Loss: 0.217\n",
      "Iteration: 54 \t--- Loss: 0.203\n",
      "Iteration: 55 \t--- Loss: 0.209\n",
      "Iteration: 56 \t--- Loss: 0.194\n",
      "Iteration: 57 \t--- Loss: 0.206\n",
      "Iteration: 58 \t--- Loss: 0.211\n",
      "Iteration: 59 \t--- Loss: 0.205\n",
      "Iteration: 60 \t--- Loss: 0.196\n",
      "Iteration: 61 \t--- Loss: 0.207\n",
      "Iteration: 62 \t--- Loss: 0.207\n",
      "Iteration: 63 \t--- Loss: 0.208\n",
      "Iteration: 64 \t--- Loss: 0.210\n",
      "Iteration: 65 \t--- Loss: 0.200\n",
      "Iteration: 66 \t--- Loss: 0.223\n",
      "Iteration: 67 \t--- Loss: 0.203\n",
      "Iteration: 68 \t--- Loss: 0.190\n",
      "Iteration: 69 \t--- Loss: 0.201\n",
      "Iteration: 70 \t--- Loss: 0.198\n",
      "Iteration: 71 \t--- Loss: 0.204\n",
      "Iteration: 72 \t--- Loss: 0.202\n",
      "Iteration: 73 \t--- Loss: 0.210\n",
      "Iteration: 74 \t--- Loss: 0.207\n",
      "Iteration: 75 \t--- Loss: 0.223\n",
      "Iteration: 76 \t--- Loss: 0.196\n",
      "Iteration: 77 \t--- Loss: 0.186\n",
      "Iteration: 78 \t--- Loss: 0.201\n",
      "Iteration: 79 \t--- Loss: 0.185\n",
      "Iteration: 80 \t--- Loss: 0.185\n",
      "Iteration: 81 \t--- Loss: 0.178\n",
      "Iteration: 82 \t--- Loss: 0.160\n",
      "Iteration: 83 \t--- Loss: 0.167\n",
      "Iteration: 84 \t--- Loss: 0.169\n",
      "Iteration: 85 \t--- Loss: 0.180\n",
      "Iteration: 86 \t--- Loss: 0.166\n",
      "Iteration: 87 \t--- Loss: 0.161\n",
      "Iteration: 88 \t--- Loss: 0.183\n",
      "Iteration: 89 \t--- Loss: 0.183\n",
      "Iteration: 90 \t--- Loss: 0.176\n",
      "Iteration: 91 \t--- Loss: 0.169\n",
      "Iteration: 92 \t--- Loss: 0.166\n",
      "Iteration: 93 \t--- Loss: 0.184\n",
      "Iteration: 94 \t--- Loss: 0.173\n",
      "Iteration: 95 \t--- Loss: 0.167\n",
      "Iteration: 96 \t--- Loss: 0.171\n",
      "Iteration: 97 \t--- Loss: 0.170\n",
      "Iteration: 98 \t--- Loss: 0.176\n",
      "Iteration: 99 \t--- Loss: 0.156\n",
      "Iteration: 100 \t--- Loss: 0.168\n",
      "Iteration: 101 \t--- Loss: 0.160\n",
      "Iteration: 102 \t--- Loss: 0.171\n",
      "Iteration: 103 \t--- Loss: 0.174\n",
      "Iteration: 104 \t--- Loss: 0.167\n",
      "Iteration: 105 \t--- Loss: 0.165\n",
      "Iteration: 106 \t--- Loss: 0.186\n",
      "Iteration: 107 \t--- Loss: 0.200\n",
      "Iteration: 108 \t--- Loss: 0.172\n",
      "Iteration: 109 \t--- Loss: 0.186\n",
      "Iteration: 110 \t--- Loss: 0.201\n",
      "Iteration: 111 \t--- Loss: 0.245\n",
      "Iteration: 112 \t--- Loss: 0.208\n",
      "Iteration: 113 \t--- Loss: 0.169\n",
      "Iteration: 114 \t--- Loss: 0.166\n",
      "Iteration: 115 \t--- Loss: 0.172\n",
      "Iteration: 116 \t--- Loss: 0.192\n",
      "Iteration: 117 \t--- Loss: 0.162\n",
      "Iteration: 118 \t--- Loss: 0.171\n",
      "Iteration: 119 \t--- Loss: 0.185\n",
      "Iteration: 120 \t--- Loss: 0.209\n",
      "Iteration: 121 \t--- Loss: 0.149\n",
      "Iteration: 122 \t--- Loss: 0.170\n",
      "Iteration: 123 \t--- Loss: 0.180\n",
      "Iteration: 124 \t--- Loss: 0.166\n",
      "Iteration: 125 \t--- Loss: 0.187\n",
      "Iteration: 126 \t--- Loss: 0.162\n",
      "Iteration: 127 \t--- Loss: 0.177\n",
      "Iteration: 128 \t--- Loss: 0.174\n",
      "Iteration: 129 \t--- Loss: 0.201\n",
      "Iteration: 130 \t--- Loss: 0.165\n",
      "Iteration: 131 \t--- Loss: 0.163\n",
      "Iteration: 132 \t--- Loss: 0.187\n",
      "Iteration: 133 \t--- Loss: 0.225\n",
      "Iteration: 134 \t--- Loss: 0.154\n",
      "Iteration: 135 \t--- Loss: 0.169\n",
      "Iteration: 136 \t--- Loss: 0.159\n",
      "Iteration: 137 \t--- Loss: 0.179\n",
      "Iteration: 138 \t--- Loss: 0.189\n",
      "Iteration: 139 \t--- Loss: 0.178\n",
      "Iteration: 140 \t--- Loss: 0.160\n",
      "Iteration: 141 \t--- Loss: 0.167\n",
      "Iteration: 142 \t--- Loss: 0.178\n",
      "Iteration: 143 \t--- Loss: 0.165\n",
      "Iteration: 144 \t--- Loss: 0.182\n",
      "Iteration: 145 \t--- Loss: 0.157\n",
      "Iteration: 146 \t--- Loss: 0.150\n",
      "Iteration: 147 \t--- Loss: 0.154\n",
      "Iteration: 148 \t--- Loss: 0.160\n",
      "Iteration: 149 \t--- Loss: 0.189\n",
      "Iteration: 150 \t--- Loss: 0.191\n",
      "Iteration: 151 \t--- Loss: 0.165\n",
      "Iteration: 152 \t--- Loss: 0.166\n",
      "Iteration: 153 \t--- Loss: 0.151\n",
      "Iteration: 154 \t--- Loss: 0.165\n",
      "Iteration: 155 \t--- Loss: 0.166\n",
      "Iteration: 156 \t--- Loss: 0.195\n",
      "Iteration: 157 \t--- Loss: 0.162\n",
      "Iteration: 158 \t--- Loss: 0.200\n",
      "Iteration: 159 \t--- Loss: 0.143\n",
      "Iteration: 160 \t--- Loss: 0.150\n",
      "Iteration: 161 \t--- Loss: 0.173\n",
      "Iteration: 162 \t--- Loss: 0.180\n",
      "Iteration: 163 \t--- Loss: 0.151\n",
      "Iteration: 164 \t--- Loss: 0.177\n",
      "Iteration: 165 \t--- Loss: 0.169\n",
      "Iteration: 166 \t--- Loss: 0.186\n",
      "Iteration: 167 \t--- Loss: 0.147\n",
      "Iteration: 168 \t--- Loss: 0.163\n",
      "Iteration: 169 \t--- Loss: 0.166\n",
      "Iteration: 170 \t--- Loss: 0.176\n",
      "Iteration: 171 \t--- Loss: 0.139\n",
      "Iteration: 172 \t--- Loss: 0.153\n",
      "Iteration: 173 \t--- Loss: 0.161\n",
      "Iteration: 174 \t--- Loss: 0.166\n",
      "Iteration: 175 \t--- Loss: 0.166\n",
      "Iteration: 176 \t--- Loss: 0.180\n",
      "Iteration: 177 \t--- Loss: 0.164\n",
      "Iteration: 178 \t--- Loss: 0.152\n",
      "Iteration: 179 \t--- Loss: 0.153\n",
      "Iteration: 180 \t--- Loss: 0.162\n",
      "Iteration: 181 \t--- Loss: 0.145\n",
      "Iteration: 182 \t--- Loss: 0.154\n",
      "Iteration: 183 \t--- Loss: 0.154\n",
      "Iteration: 184 \t--- Loss: 0.156\n",
      "Iteration: 185 \t--- Loss: 0.172\n",
      "Iteration: 186 \t--- Loss: 0.203\n",
      "Iteration: 187 \t--- Loss: 0.136\n",
      "Iteration: 188 \t--- Loss: 0.153\n",
      "Iteration: 189 \t--- Loss: 0.149\n",
      "Iteration: 190 \t--- Loss: 0.147\n",
      "Iteration: 191 \t--- Loss: 0.145\n",
      "Iteration: 192 \t--- Loss: 0.150\n",
      "Iteration: 193 \t--- Loss: 0.152\n",
      "Iteration: 194 \t--- Loss: 0.151\n",
      "Iteration: 195 \t--- Loss: 0.156\n",
      "Iteration: 196 \t--- Loss: 0.151\n",
      "Iteration: 197 \t--- Loss: 0.142\n",
      "Iteration: 198 \t--- Loss: 0.142\n",
      "Iteration: 199 \t--- Loss: 0.148\n",
      "Iteration: 200 \t--- Loss: 0.140\n",
      "Iteration: 201 \t--- Loss: 0.129\n",
      "Iteration: 202 \t--- Loss: 0.137\n",
      "Iteration: 203 \t--- Loss: 0.142\n",
      "Iteration: 204 \t--- Loss: 0.127\n",
      "Iteration: 205 \t--- Loss: 0.135\n",
      "Iteration: 206 \t--- Loss: 0.139\n",
      "Iteration: 207 \t--- Loss: 0.149\n",
      "Iteration: 208 \t--- Loss: 0.151\n",
      "Iteration: 209 \t--- Loss: 0.148\n",
      "Iteration: 210 \t--- Loss: 0.148\n",
      "Iteration: 211 \t--- Loss: 0.158\n",
      "Iteration: 212 \t--- Loss: 0.159\n",
      "Iteration: 213 \t--- Loss: 0.187\n",
      "Iteration: 214 \t--- Loss: 0.138\n",
      "Iteration: 215 \t--- Loss: 0.149\n",
      "Iteration: 216 \t--- Loss: 0.134\n",
      "Iteration: 217 \t--- Loss: 0.147\n",
      "Iteration: 218 \t--- Loss: 0.133\n",
      "Iteration: 219 \t--- Loss: 0.145\n",
      "Iteration: 220 \t--- Loss: 0.131\n",
      "Iteration: 221 \t--- Loss: 0.143\n",
      "Iteration: 222 \t--- Loss: 0.144\n",
      "Iteration: 223 \t--- Loss: 0.143\n",
      "Iteration: 224 \t--- Loss: 0.155\n",
      "Iteration: 225 \t--- Loss: 0.137\n",
      "Iteration: 226 \t--- Loss: 0.139\n",
      "Iteration: 227 \t--- Loss: 0.161\n",
      "Iteration: 228 \t--- Loss: 0.148\n",
      "Iteration: 229 \t--- Loss: 0.155\n",
      "Iteration: 230 \t--- Loss: 0.149\n",
      "Iteration: 231 \t--- Loss: 0.140\n",
      "Iteration: 232 \t--- Loss: 0.140\n",
      "Iteration: 233 \t--- Loss: 0.131\n",
      "Iteration: 234 \t--- Loss: 0.141\n",
      "Iteration: 235 \t--- Loss: 0.157\n",
      "Iteration: 236 \t--- Loss: 0.152\n",
      "Iteration: 237 \t--- Loss: 0.152\n",
      "Iteration: 238 \t--- Loss: 0.149\n",
      "Iteration: 239 \t--- Loss: 0.136\n",
      "Iteration: 240 \t--- Loss: 0.150\n",
      "Iteration: 241 \t--- Loss: 0.128\n",
      "Iteration: 242 \t--- Loss: 0.138\n",
      "Iteration: 243 \t--- Loss: 0.142\n",
      "Iteration: 244 \t--- Loss: 0.143\n",
      "Iteration: 245 \t--- Loss: 0.140\n",
      "Iteration: 246 \t--- Loss: 0.134\n",
      "Iteration: 247 \t--- Loss: 0.143\n",
      "Iteration: 248 \t--- Loss: 0.145\n",
      "Iteration: 249 \t--- Loss: 0.135\n",
      "Iteration: 250 \t--- Loss: 0.136\n",
      "Iteration: 251 \t--- Loss: 0.151\n",
      "Iteration: 252 \t--- Loss: 0.141\n",
      "Iteration: 253 \t--- Loss: 0.145\n",
      "Iteration: 254 \t--- Loss: 0.147\n",
      "Iteration: 255 \t--- Loss: 0.148\n",
      "Iteration: 256 \t--- Loss: 0.149\n",
      "Iteration: 257 \t--- Loss: 0.155\n",
      "Iteration: 258 \t--- Loss: 0.154\n",
      "Iteration: 259 \t--- Loss: 0.155"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it][Parallel(n_jobs=5)]: Done  87 tasks      | elapsed: 53.5min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it][Parallel(n_jobs=5)]: Done  88 tasks      | elapsed: 53.6min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 1.313\n",
      "Iteration: 1 \t--- Loss: 1.283\n",
      "Iteration: 2 \t--- Loss: 1.207\n",
      "Iteration: 3 \t--- Loss: 1.163\n",
      "Iteration: 4 \t--- Loss: 1.112\n",
      "Iteration: 5 \t--- Loss: 1.088\n",
      "Iteration: 6 \t--- Loss: 1.051\n",
      "Iteration: 7 \t--- Loss: 1.033\n",
      "Iteration: 8 \t--- Loss: 1.007\n",
      "Iteration: 9 \t--- Loss: 0.980\n",
      "Iteration: 10 \t--- Loss: 0.995\n",
      "Iteration: 11 \t--- Loss: 0.973\n",
      "Iteration: 12 \t--- Loss: 0.942\n",
      "Iteration: 13 \t--- Loss: 0.940\n",
      "Iteration: 14 \t--- Loss: 0.943\n",
      "Iteration: 15 \t--- Loss: 0.914\n",
      "Iteration: 16 \t--- Loss: 0.897\n",
      "Iteration: 17 \t--- Loss: 0.883\n",
      "Iteration: 18 \t--- Loss: 0.886\n",
      "Iteration: 19 \t--- Loss: 0.884\n",
      "Iteration: 20 \t--- Loss: 0.868\n",
      "Iteration: 21 \t--- Loss: 0.866\n",
      "Iteration: 22 \t--- Loss: 0.885\n",
      "Iteration: 23 \t--- Loss: 0.856\n",
      "Iteration: 24 \t--- Loss: 0.880\n",
      "Iteration: 25 \t--- Loss: 0.834\n",
      "Iteration: 26 \t--- Loss: 0.844\n",
      "Iteration: 27 \t--- Loss: 0.847\n",
      "Iteration: 28 \t--- Loss: 0.818\n",
      "Iteration: 29 \t--- Loss: 0.791\n",
      "Iteration: 30 \t--- Loss: 0.800\n",
      "Iteration: 31 \t--- Loss: 0.754\n",
      "Iteration: 32 \t--- Loss: 0.779\n",
      "Iteration: 33 \t--- Loss: 0.731\n",
      "Iteration: 34 \t--- Loss: 0.672\n",
      "Iteration: 35 \t--- Loss: 0.680\n",
      "Iteration: 36 \t--- Loss: 0.743\n",
      "Iteration: 37 \t--- Loss: 0.706\n",
      "Iteration: 38 \t--- Loss: 0.647\n",
      "Iteration: 39 \t--- Loss: 0.632\n",
      "Iteration: 40 \t--- Loss: 0.601\n",
      "Iteration: 41 \t--- Loss: 0.616\n",
      "Iteration: 42 \t--- Loss: 0.713\n",
      "Iteration: 43 \t--- Loss: 1.801\n",
      "Iteration: 44 \t--- Loss: 1.867\n",
      "Iteration: 45 \t--- Loss: 1.812\n",
      "Iteration: 46 \t--- Loss: 1.894\n",
      "Iteration: 47 \t--- Loss: 1.881\n",
      "Iteration: 48 \t--- Loss: 1.879\n",
      "Iteration: 49 \t--- Loss: 1.936\n",
      "Iteration: 50 \t--- Loss: 1.803\n",
      "Iteration: 51 \t--- Loss: 1.872\n",
      "Iteration: 52 \t--- Loss: 1.853\n",
      "Iteration: 53 \t--- Loss: 1.819\n",
      "Iteration: 54 \t--- Loss: 1.805\n",
      "Iteration: 55 \t--- Loss: 1.825\n",
      "Iteration: 56 \t--- Loss: 1.839\n",
      "Iteration: 57 \t--- Loss: 1.831\n",
      "Iteration: 58 \t--- Loss: 1.845\n",
      "Iteration: 59 \t--- Loss: 1.801\n",
      "Iteration: 60 \t--- Loss: 1.740\n",
      "Iteration: 61 \t--- Loss: 1.717\n",
      "Iteration: 62 \t--- Loss: 1.671\n",
      "Iteration: 63 \t--- Loss: 1.768\n",
      "Iteration: 64 \t--- Loss: 1.684\n",
      "Iteration: 65 \t--- Loss: 1.649\n",
      "Iteration: 66 \t--- Loss: 1.692\n",
      "Iteration: 67 \t--- Loss: 1.619\n",
      "Iteration: 68 \t--- Loss: 1.593\n",
      "Iteration: 69 \t--- Loss: 1.591\n",
      "Iteration: 70 \t--- Loss: 1.517\n",
      "Iteration: 71 \t--- Loss: 1.510\n",
      "Iteration: 72 \t--- Loss: 1.433\n",
      "Iteration: 73 \t--- Loss: 1.402\n",
      "Iteration: 74 \t--- Loss: 1.368\n",
      "Iteration: 75 \t--- Loss: 1.278\n",
      "Iteration: 76 \t--- Loss: 1.224\n",
      "Iteration: 77 \t--- Loss: 1.164\n",
      "Iteration: 78 \t--- Loss: 1.112\n",
      "Iteration: 79 \t--- Loss: 1.058\n",
      "Iteration: 80 \t--- Loss: 1.028\n",
      "Iteration: 81 \t--- Loss: 0.986\n",
      "Iteration: 82 \t--- Loss: 0.956\n",
      "Iteration: 83 \t--- Loss: 0.929\n",
      "Iteration: 84 \t--- Loss: 0.940\n",
      "Iteration: 85 \t--- Loss: 0.894\n",
      "Iteration: 86 \t--- Loss: 0.875\n",
      "Iteration: 87 \t--- Loss: 0.877\n",
      "Iteration: 88 \t--- Loss: 0.867\n",
      "Iteration: 89 \t--- Loss: 0.855\n",
      "Iteration: 90 \t--- Loss: 0.843\n",
      "Iteration: 91 \t--- Loss: 0.825\n",
      "Iteration: 92 \t--- Loss: 0.836\n",
      "Iteration: 93 \t--- Loss: 0.817\n",
      "Iteration: 94 \t--- Loss: 0.836\n",
      "Iteration: 95 \t--- Loss: 0.816\n",
      "Iteration: 96 \t--- Loss: 0.828\n",
      "Iteration: 97 \t--- Loss: 0.830\n",
      "Iteration: 98 \t--- Loss: 0.797\n",
      "Iteration: 99 \t--- Loss: 0.797\n",
      "Iteration: 100 \t--- Loss: 0.809\n",
      "Iteration: 101 \t--- Loss: 0.802\n",
      "Iteration: 102 \t--- Loss: 0.807\n",
      "Iteration: 103 \t--- Loss: 0.817\n",
      "Iteration: 104 \t--- Loss: 0.812\n",
      "Iteration: 105 \t--- Loss: 0.797\n",
      "Iteration: 106 \t--- Loss: 0.786\n",
      "Iteration: 107 \t--- Loss: 0.812\n",
      "Iteration: 108 \t--- Loss: 0.788\n",
      "Iteration: 109 \t--- Loss: 0.802\n",
      "Iteration: 110 \t--- Loss: 0.772\n",
      "Iteration: 111 \t--- Loss: 0.792\n",
      "Iteration: 112 \t--- Loss: 0.811\n",
      "Iteration: 113 \t--- Loss: 0.775\n",
      "Iteration: 114 \t--- Loss: 0.774\n",
      "Iteration: 115 \t--- Loss: 0.774\n",
      "Iteration: 116 \t--- Loss: 0.774\n",
      "Iteration: 117 \t--- Loss: 0.794\n",
      "Iteration: 118 \t--- Loss: 0.730\n",
      "Iteration: 119 \t--- Loss: 0.760\n",
      "Iteration: 120 \t--- Loss: 0.735\n",
      "Iteration: 121 \t--- Loss: 0.741\n",
      "Iteration: 122 \t--- Loss: 0.751\n",
      "Iteration: 123 \t--- Loss: 0.732\n",
      "Iteration: 124 \t--- Loss: 0.752\n",
      "Iteration: 125 \t--- Loss: 0.746\n",
      "Iteration: 126 \t--- Loss: 0.735\n",
      "Iteration: 127 \t--- Loss: 0.673\n",
      "Iteration: 128 \t--- Loss: 0.685\n",
      "Iteration: 129 \t--- Loss: 0.687\n",
      "Iteration: 130 \t--- Loss: 0.681\n",
      "Iteration: 131 \t--- Loss: 0.693\n",
      "Iteration: 132 \t--- Loss: 0.686\n",
      "Iteration: 133 \t--- Loss: 0.635\n",
      "Iteration: 134 \t--- Loss: 0.580\n",
      "Iteration: 135 \t--- Loss: 0.613\n",
      "Iteration: 136 \t--- Loss: 0.528\n",
      "Iteration: 137 \t--- Loss: 0.562\n",
      "Iteration: 138 \t--- Loss: 0.509\n",
      "Iteration: 139 \t--- Loss: 0.433\n",
      "Iteration: 140 \t--- Loss: 0.412\n",
      "Iteration: 141 \t--- Loss: 0.396\n",
      "Iteration: 142 \t--- Loss: 0.432\n",
      "Iteration: 143 \t--- Loss: 0.403\n",
      "Iteration: 144 \t--- Loss: 0.307\n",
      "Iteration: 145 \t--- Loss: 0.278\n",
      "Iteration: 146 \t--- Loss: 0.248\n",
      "Iteration: 147 \t--- Loss: 0.233\n",
      "Iteration: 148 \t--- Loss: 0.236\n",
      "Iteration: 149 \t--- Loss: 0.226\n",
      "Iteration: 150 \t--- Loss: 0.218\n",
      "Iteration: 151 \t--- Loss: 0.207\n",
      "Iteration: 152 \t--- Loss: 0.199\n",
      "Iteration: 153 \t--- Loss: 0.188\n",
      "Iteration: 154 \t--- Loss: 0.172\n",
      "Iteration: 155 \t--- Loss: 0.174\n",
      "Iteration: 156 \t--- Loss: 0.148\n",
      "Iteration: 157 \t--- Loss: 0.176\n",
      "Iteration: 158 \t--- Loss: 0.165\n",
      "Iteration: 159 \t--- Loss: 0.156\n",
      "Iteration: 160 \t--- Loss: 0.150\n",
      "Iteration: 161 \t--- Loss: 0.159\n",
      "Iteration: 162 \t--- Loss: 0.157\n",
      "Iteration: 163 \t--- Loss: 0.145\n",
      "Iteration: 164 \t--- Loss: 0.178\n",
      "Iteration: 165 \t--- Loss: 0.149\n",
      "Iteration: 166 \t--- Loss: 0.150\n",
      "Iteration: 167 \t--- Loss: 0.138\n",
      "Iteration: 168 \t--- Loss: 0.156\n",
      "Iteration: 169 \t--- Loss: 0.147\n",
      "Iteration: 170 \t--- Loss: 0.202\n",
      "Iteration: 171 \t--- Loss: 0.161\n",
      "Iteration: 172 \t--- Loss: 0.256\n",
      "Iteration: 173 \t--- Loss: 0.158\n",
      "Iteration: 174 \t--- Loss: 0.208\n",
      "Iteration: 175 \t--- Loss: 0.176\n",
      "Iteration: 176 \t--- Loss: 0.252\n",
      "Iteration: 177 \t--- Loss: 0.157\n",
      "Iteration: 178 \t--- Loss: 0.213\n",
      "Iteration: 179 \t--- Loss: 0.148\n",
      "Iteration: 180 \t--- Loss: 0.161\n",
      "Iteration: 181 \t--- Loss: 0.135\n",
      "Iteration: 182 \t--- Loss: 0.158\n",
      "Iteration: 183 \t--- Loss: 0.149\n",
      "Iteration: 184 \t--- Loss: 0.201\n",
      "Iteration: 185 \t--- Loss: 0.173\n",
      "Iteration: 186 \t--- Loss: 0.321\n",
      "Iteration: 187 \t--- Loss: 0.138\n",
      "Iteration: 188 \t--- Loss: 0.150\n",
      "Iteration: 189 \t--- Loss: 0.137\n",
      "Iteration: 190 \t--- Loss: 0.158\n",
      "Iteration: 191 \t--- Loss: 0.156\n",
      "Iteration: 192 \t--- Loss: 0.192\n",
      "Iteration: 193 \t--- Loss: 0.205\n",
      "Iteration: 194 \t--- Loss: 0.450\n",
      "Iteration: 195 \t--- Loss: 0.138\n",
      "Iteration: 196 \t--- Loss: 0.122\n",
      "Iteration: 197 \t--- Loss: 0.138\n",
      "Iteration: 198 \t--- Loss: 0.138\n",
      "Iteration: 199 \t--- Loss: 0.135\n",
      "Iteration: 200 \t--- Loss: 0.124\n",
      "Iteration: 201 \t--- Loss: 0.191\n",
      "Iteration: 202 \t--- Loss: 0.161\n",
      "Iteration: 203 \t--- Loss: 0.271\n",
      "Iteration: 204 \t--- Loss: 0.153\n",
      "Iteration: 205 \t--- Loss: 0.244\n",
      "Iteration: 206 \t--- Loss: 0.169\n",
      "Iteration: 207 \t--- Loss: 0.244\n",
      "Iteration: 208 \t--- Loss: 0.155\n",
      "Iteration: 209 \t--- Loss: 0.230\n",
      "Iteration: 210 \t--- Loss: 0.159\n",
      "Iteration: 211 \t--- Loss: 0.240\n",
      "Iteration: 212 \t--- Loss: 0.159\n",
      "Iteration: 213 \t--- Loss: 0.220\n",
      "Iteration: 214 \t--- Loss: 0.156\n",
      "Iteration: 215 \t--- Loss: 0.192\n",
      "Iteration: 216 \t--- Loss: 0.148\n",
      "Iteration: 217 \t--- Loss: 0.200\n",
      "Iteration: 218 \t--- Loss: 0.151\n",
      "Iteration: 219 \t--- Loss: 0.168\n",
      "Iteration: 220 \t--- Loss: 0.157\n",
      "Iteration: 221 \t--- Loss: 0.240\n",
      "Iteration: 222 \t--- Loss: 0.155\n",
      "Iteration: 223 \t--- Loss: 0.163\n",
      "Iteration: 224 \t--- Loss: 0.147\n",
      "Iteration: 225 \t--- Loss: 0.229\n",
      "Iteration: 226 \t--- Loss: 0.143\n",
      "Iteration: 227 \t--- Loss: 0.161\n",
      "Iteration: 228 \t--- Loss: 0.155\n",
      "Iteration: 229 \t--- Loss: 0.208\n",
      "Iteration: 230 \t--- Loss: 0.163\n",
      "Iteration: 231 \t--- Loss: 0.210\n",
      "Iteration: 232 \t--- Loss: 0.153\n",
      "Iteration: 233 \t--- Loss: 0.250\n",
      "Iteration: 234 \t--- Loss: 0.203\n",
      "Iteration: 235 \t--- Loss: 0.393\n",
      "Iteration: 236 \t--- Loss: 0.146\n",
      "Iteration: 237 \t--- Loss: 0.137\n",
      "Iteration: 238 \t--- Loss: 0.139\n",
      "Iteration: 239 \t--- Loss: 0.130\n",
      "Iteration: 240 \t--- Loss: 0.145\n",
      "Iteration: 241 \t--- Loss: 0.136\n",
      "Iteration: 242 \t--- Loss: 0.191\n",
      "Iteration: 243 \t--- Loss: 0.199\n",
      "Iteration: 244 \t--- Loss: 0.466\n",
      "Iteration: 245 \t--- Loss: 0.148\n",
      "Iteration: 246 \t--- Loss: 0.139\n",
      "Iteration: 247 \t--- Loss: 0.145\n",
      "Iteration: 248 \t--- Loss: 0.154\n",
      "Iteration: 249 \t--- Loss: 0.168\n",
      "Iteration: 250 \t--- Loss: 0.168\n",
      "Iteration: 251 \t--- Loss: 0.249\n",
      "Iteration: 252 \t--- Loss: 0.134\n",
      "Iteration: 253 \t--- Loss: 0.155\n",
      "Iteration: 254 \t--- Loss: 0.150\n",
      "Iteration: 255 \t--- Loss: 0.214\n",
      "Iteration: 256 \t--- Loss: 0.169\n",
      "Iteration: 257 \t--- Loss: 0.335\n",
      "Iteration: 258 \t--- Loss: 0.143\n",
      "Iteration: 259 \t--- Loss: 0.140"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:36<00:00, 96.07s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.143\n",
      "Iteration: 261 \t--- Loss: 0.134\n",
      "Iteration: 262 \t--- Loss: 0.138\n",
      "Iteration: 263 \t--- Loss: 0.130\n",
      "Iteration: 264 \t--- Loss: 0.136\n",
      "Iteration: 265 \t--- Loss: 0.144\n",
      "Iteration: 266 \t--- Loss: 0.137\n",
      "Iteration: 267 \t--- Loss: 0.140\n",
      "Iteration: 268 \t--- Loss: 0.145\n",
      "Iteration: 269 \t--- Loss: 0.144\n",
      "Iteration: 270 \t--- Loss: 0.144\n",
      "Iteration: 271 \t--- Loss: 0.140\n",
      "Iteration: 272 \t--- Loss: 0.129\n",
      "Iteration: 273 \t--- Loss: 0.141\n",
      "Iteration: 274 \t--- Loss: 0.139\n",
      "Iteration: 275 \t--- Loss: 0.135\n",
      "Iteration: 276 \t--- Loss: 0.131\n",
      "Iteration: 277 \t--- Loss: 0.144\n",
      "Iteration: 278 \t--- Loss: 0.132\n",
      "Iteration: 279 \t--- Loss: 0.152\n",
      "Iteration: 280 \t--- Loss: 0.139\n",
      "Iteration: 281 \t--- Loss: 0.124\n",
      "Iteration: 282 \t--- Loss: 0.135\n",
      "Iteration: 283 \t--- Loss: 0.144\n",
      "Iteration: 284 \t--- Loss: 0.136\n",
      "Iteration: 285 \t--- Loss: 0.141\n",
      "Iteration: 286 \t--- Loss: 0.142\n",
      "Iteration: 287 \t--- Loss: 0.142\n",
      "Iteration: 288 \t--- Loss: 0.130\n",
      "Iteration: 289 \t--- Loss: 0.137\n",
      "Iteration: 290 \t--- Loss: 0.142\n",
      "Iteration: 291 \t--- Loss: 0.146\n",
      "Iteration: 292 \t--- Loss: 0.138\n",
      "Iteration: 293 \t--- Loss: 0.133\n",
      "Iteration: 294 \t--- Loss: 0.142\n",
      "Iteration: 295 \t--- Loss: 0.146\n",
      "Iteration: 296 \t--- Loss: 0.142\n",
      "Iteration: 297 \t--- Loss: 0.142\n",
      "Iteration: 298 \t--- Loss: 0.132\n",
      "Iteration: 299 \t--- Loss: 0.132\n",
      "Iteration: 300 \t--- Loss: 0.138\n",
      "Iteration: 301 \t--- Loss: 0.132\n",
      "Iteration: 302 \t--- Loss: 0.132\n",
      "Iteration: 303 \t--- Loss: 0.132\n",
      "Iteration: 304 \t--- Loss: 0.145\n",
      "Iteration: 305 \t--- Loss: 0.151\n",
      "Iteration: 306 \t--- Loss: 0.141\n",
      "Iteration: 307 \t--- Loss: 0.144\n",
      "Iteration: 308 \t--- Loss: 0.132\n",
      "Iteration: 309 \t--- Loss: 0.138\n",
      "Iteration: 310 \t--- Loss: 0.137\n",
      "Iteration: 311 \t--- Loss: 0.129\n",
      "Iteration: 312 \t--- Loss: 0.132\n",
      "Iteration: 313 \t--- Loss: 0.137\n",
      "Iteration: 314 \t--- Loss: 0.144\n",
      "Iteration: 315 \t--- Loss: 0.137\n",
      "Iteration: 316 \t--- Loss: 0.135\n",
      "Iteration: 317 \t--- Loss: 0.135\n",
      "Iteration: 318 \t--- Loss: 0.127\n",
      "Iteration: 319 \t--- Loss: 0.131\n",
      "Iteration: 320 \t--- Loss: 0.135\n",
      "Iteration: 321 \t--- Loss: 0.133\n",
      "Iteration: 322 \t--- Loss: 0.144\n",
      "Iteration: 323 \t--- Loss: 0.143\n",
      "Iteration: 324 \t--- Loss: 0.137\n",
      "Iteration: 325 \t--- Loss: 0.132\n",
      "Iteration: 326 \t--- Loss: 0.125\n",
      "Iteration: 327 \t--- Loss: 0.146\n",
      "Iteration: 328 \t--- Loss: 0.127\n",
      "Iteration: 329 \t--- Loss: 0.143\n",
      "Iteration: 330 \t--- Loss: 0.130\n",
      "Iteration: 331 \t--- Loss: 0.140\n",
      "Iteration: 332 \t--- Loss: 0.145\n",
      "Iteration: 333 \t--- Loss: 0.135\n",
      "Iteration: 334 \t--- Loss: 0.135\n",
      "Iteration: 335 \t--- Loss: 0.139\n",
      "Iteration: 336 \t--- Loss: 0.138\n",
      "Iteration: 337 \t--- Loss: 0.143\n",
      "Iteration: 338 \t--- Loss: 0.141\n",
      "Iteration: 339 \t--- Loss: 0.138\n",
      "Iteration: 340 \t--- Loss: 0.145\n",
      "Iteration: 341 \t--- Loss: 0.129\n",
      "Iteration: 342 \t--- Loss: 0.146\n",
      "Iteration: 343 \t--- Loss: 0.133\n",
      "Iteration: 344 \t--- Loss: 0.133\n",
      "Iteration: 345 \t--- Loss: 0.135\n",
      "Iteration: 346 \t--- Loss: 0.129\n",
      "Iteration: 347 \t--- Loss: 0.134\n",
      "Iteration: 348 \t--- Loss: 0.143\n",
      "Iteration: 349 \t--- Loss: 0.141\n",
      "Iteration: 350 \t--- Loss: 0.130\n",
      "Iteration: 351 \t--- Loss: 0.129\n",
      "Iteration: 352 \t--- Loss: 0.128\n",
      "Iteration: 353 \t--- Loss: 0.140\n",
      "Iteration: 354 \t--- Loss: 0.139\n",
      "Iteration: 355 \t--- Loss: 0.136\n",
      "Iteration: 356 \t--- Loss: 0.143\n",
      "Iteration: 357 \t--- Loss: 0.131\n",
      "Iteration: 358 \t--- Loss: 0.134\n",
      "Iteration: 359 \t--- Loss: 0.142\n",
      "Iteration: 360 \t--- Loss: 0.136\n",
      "Iteration: 361 \t--- Loss: 0.126\n",
      "Iteration: 362 \t--- Loss: 0.143\n",
      "Iteration: 363 \t--- Loss: 0.128\n",
      "Iteration: 364 \t--- Loss: 0.137\n",
      "Iteration: 365 \t--- Loss: 0.128\n",
      "Iteration: 366 \t--- Loss: 0.124\n",
      "Iteration: 367 \t--- Loss: 0.135\n",
      "Iteration: 368 \t--- Loss: 0.125\n",
      "Iteration: 369 \t--- Loss: 0.139\n",
      "Iteration: 370 \t--- Loss: 0.122\n",
      "Iteration: 371 \t--- Loss: 0.123\n",
      "Iteration: 372 \t--- Loss: 0.132\n",
      "Iteration: 373 \t--- Loss: 0.127\n",
      "Iteration: 374 \t--- Loss: 0.132\n",
      "Iteration: 375 \t--- Loss: 0.133\n",
      "Iteration: 376 \t--- Loss: 0.132\n",
      "Iteration: 377 \t--- Loss: 0.137\n",
      "Iteration: 378 \t--- Loss: 0.141\n",
      "Iteration: 379 \t--- Loss: 0.145\n",
      "Iteration: 380 \t--- Loss: 0.141\n",
      "Iteration: 381 \t--- Loss: 0.141\n",
      "Iteration: 382 \t--- Loss: 0.132\n",
      "Iteration: 383 \t--- Loss: 0.135\n",
      "Iteration: 384 \t--- Loss: 0.137\n",
      "Iteration: 385 \t--- Loss: 0.134\n",
      "Iteration: 386 \t--- Loss: 0.126\n",
      "Iteration: 387 \t--- Loss: 0.140\n",
      "Iteration: 388 \t--- Loss: 0.131\n",
      "Iteration: 389 \t--- Loss: 0.133\n",
      "Iteration: 390 \t--- Loss: 0.141\n",
      "Iteration: 391 \t--- Loss: 0.125\n",
      "Iteration: 392 \t--- Loss: 0.148\n",
      "Iteration: 393 \t--- Loss: 0.135\n",
      "Iteration: 394 \t--- Loss: 0.133\n",
      "Iteration: 395 \t--- Loss: 0.141\n",
      "Iteration: 396 \t--- Loss: 0.141\n",
      "Iteration: 397 \t--- Loss: 0.138\n",
      "Iteration: 398 \t--- Loss: 0.145\n",
      "Iteration: 399 \t--- Loss: 0.140\n",
      "Iteration: 400 \t--- Loss: 0.130\n",
      "Iteration: 401 \t--- Loss: 0.140\n",
      "Iteration: 402 \t--- Loss: 0.136\n",
      "Iteration: 403 \t--- Loss: 0.133\n",
      "Iteration: 404 \t--- Loss: 0.132\n",
      "Iteration: 405 \t--- Loss: 0.136\n",
      "Iteration: 406 \t--- Loss: 0.138\n",
      "Iteration: 407 \t--- Loss: 0.134\n",
      "Iteration: 408 \t--- Loss: 0.148\n",
      "Iteration: 409 \t--- Loss: 0.140\n",
      "Iteration: 410 \t--- Loss: 0.130\n",
      "Iteration: 411 \t--- Loss: 0.121\n",
      "Iteration: 412 \t--- Loss: 0.122\n",
      "Iteration: 413 \t--- Loss: 0.129\n",
      "Iteration: 414 \t--- Loss: 0.140\n",
      "Iteration: 415 \t--- Loss: 0.137\n",
      "Iteration: 416 \t--- Loss: 0.131\n",
      "Iteration: 417 \t--- Loss: 0.124\n",
      "Iteration: 418 \t--- Loss: 0.145\n",
      "Iteration: 419 \t--- Loss: 0.151\n",
      "Iteration: 420 \t--- Loss: 0.148\n",
      "Iteration: 421 \t--- Loss: 0.137\n",
      "Iteration: 422 \t--- Loss: 0.133\n",
      "Iteration: 423 \t--- Loss: 0.135\n",
      "Iteration: 424 \t--- Loss: 0.140\n",
      "Iteration: 425 \t--- Loss: 0.147\n",
      "Iteration: 426 \t--- Loss: 0.136\n",
      "Iteration: 427 \t--- Loss: 0.141\n",
      "Iteration: 428 \t--- Loss: 0.142\n",
      "Iteration: 429 \t--- Loss: 0.134\n",
      "Iteration: 430 \t--- Loss: 0.134\n",
      "Iteration: 431 \t--- Loss: 0.142\n",
      "Iteration: 432 \t--- Loss: 0.132\n",
      "Iteration: 433 \t--- Loss: 0.130\n",
      "Iteration: 434 \t--- Loss: 0.139\n",
      "Iteration: 435 \t--- Loss: 0.134\n",
      "Iteration: 436 \t--- Loss: 0.137\n",
      "Iteration: 437 \t--- Loss: 0.141\n",
      "Iteration: 438 \t--- Loss: 0.121\n",
      "Iteration: 439 \t--- Loss: 0.137\n",
      "Iteration: 440 \t--- Loss: 0.146\n",
      "Iteration: 441 \t--- Loss: 0.145\n",
      "Iteration: 442 \t--- Loss: 0.127\n",
      "Iteration: 443 \t--- Loss: 0.136\n",
      "Iteration: 444 \t--- Loss: 0.132\n",
      "Iteration: 445 \t--- Loss: 0.132\n",
      "Iteration: 446 \t--- Loss: 0.140\n",
      "Iteration: 447 \t--- Loss: 0.132\n",
      "Iteration: 448 \t--- Loss: 0.134\n",
      "Iteration: 449 \t--- Loss: 0.141\n",
      "Iteration: 450 \t--- Loss: 0.143\n",
      "Iteration: 451 \t--- Loss: 0.142\n",
      "Iteration: 452 \t--- Loss: 0.145\n",
      "Iteration: 453 \t--- Loss: 0.133\n",
      "Iteration: 454 \t--- Loss: 0.141\n",
      "Iteration: 455 \t--- Loss: 0.136\n",
      "Iteration: 456 \t--- Loss: 0.138\n",
      "Iteration: 457 \t--- Loss: 0.136\n",
      "Iteration: 458 \t--- Loss: 0.134\n",
      "Iteration: 459 \t--- Loss: 0.144\n",
      "Iteration: 460 \t--- Loss: 0.135\n",
      "Iteration: 461 \t--- Loss: 0.137\n",
      "Iteration: 462 \t--- Loss: 0.137\n",
      "Iteration: 463 \t--- Loss: 0.139\n",
      "Iteration: 464 \t--- Loss: 0.143\n",
      "Iteration: 465 \t--- Loss: 0.142\n",
      "Iteration: 466 \t--- Loss: 0.132\n",
      "Iteration: 467 \t--- Loss: 0.130\n",
      "Iteration: 468 \t--- Loss: 0.132\n",
      "Iteration: 469 \t--- Loss: 0.147\n",
      "Iteration: 470 \t--- Loss: 0.132\n",
      "Iteration: 471 \t--- Loss: 0.132\n",
      "Iteration: 472 \t--- Loss: 0.139\n",
      "Iteration: 473 \t--- Loss: 0.149\n",
      "Iteration: 474 \t--- Loss: 0.141\n",
      "Iteration: 475 \t--- Loss: 0.135\n",
      "Iteration: 476 \t--- Loss: 0.135\n",
      "Iteration: 477 \t--- Loss: 0.128\n",
      "Iteration: 478 \t--- Loss: 0.131\n",
      "Iteration: 479 \t--- Loss: 0.143\n",
      "Iteration: 480 \t--- Loss: 0.133\n",
      "Iteration: 481 \t--- Loss: 0.138\n",
      "Iteration: 482 \t--- Loss: 0.137\n",
      "Iteration: 483 \t--- Loss: 0.138\n",
      "Iteration: 484 \t--- Loss: 0.137\n",
      "Iteration: 485 \t--- Loss: 0.143\n",
      "Iteration: 486 \t--- Loss: 0.136\n",
      "Iteration: 487 \t--- Loss: 0.142\n",
      "Iteration: 488 \t--- Loss: 0.128\n",
      "Iteration: 489 \t--- Loss: 0.141\n",
      "Iteration: 490 \t--- Loss: 0.144\n",
      "Iteration: 491 \t--- Loss: 0.133\n",
      "Iteration: 492 \t--- Loss: 0.136\n",
      "Iteration: 493 \t--- Loss: 0.130\n",
      "Iteration: 494 \t--- Loss: 0.142\n",
      "Iteration: 495 \t--- Loss: 0.143\n",
      "Iteration: 496 \t--- Loss: 0.141\n",
      "Iteration: 497 \t--- Loss: 0.135\n",
      "Iteration: 498 \t--- Loss: 0.128\n",
      "Iteration: 499 \t--- Loss: 0.137\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.53s/it][Parallel(n_jobs=5)]: Done  89 tasks      | elapsed: 54.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.085\n",
      "Iteration: 1 \t--- Loss: 0.088\n",
      "Iteration: 2 \t--- Loss: 0.088\n",
      "Iteration: 3 \t--- Loss: 0.090\n",
      "Iteration: 4 \t--- Loss: 0.088\n",
      "Iteration: 5 \t--- Loss: 0.093\n",
      "Iteration: 6 \t--- Loss: 0.087\n",
      "Iteration: 7 \t--- Loss: 0.086\n",
      "Iteration: 8 \t--- Loss: 0.087\n",
      "Iteration: 9 \t--- Loss: 0.090\n",
      "Iteration: 10 \t--- Loss: 0.086\n",
      "Iteration: 11 \t--- Loss: 0.085\n",
      "Iteration: 12 \t--- Loss: 0.085\n",
      "Iteration: 13 \t--- Loss: 0.085\n",
      "Iteration: 14 \t--- Loss: 0.084\n",
      "Iteration: 15 \t--- Loss: 0.086\n",
      "Iteration: 16 \t--- Loss: 0.089\n",
      "Iteration: 17 \t--- Loss: 0.090\n",
      "Iteration: 18 \t--- Loss: 0.083\n",
      "Iteration: 19 \t--- Loss: 0.084\n",
      "Iteration: 20 \t--- Loss: 0.083\n",
      "Iteration: 21 \t--- Loss: 0.087\n",
      "Iteration: 22 \t--- Loss: 0.092\n",
      "Iteration: 23 \t--- Loss: 0.084\n",
      "Iteration: 24 \t--- Loss: 0.085\n",
      "Iteration: 25 \t--- Loss: 0.086\n",
      "Iteration: 26 \t--- Loss: 0.081\n",
      "Iteration: 27 \t--- Loss: 0.089\n",
      "Iteration: 28 \t--- Loss: 0.089\n",
      "Iteration: 29 \t--- Loss: 0.087\n",
      "Iteration: 30 \t--- Loss: 0.088\n",
      "Iteration: 31 \t--- Loss: 0.089\n",
      "Iteration: 32 \t--- Loss: 0.085\n",
      "Iteration: 33 \t--- Loss: 0.086\n",
      "Iteration: 34 \t--- Loss: 0.081\n",
      "Iteration: 35 \t--- Loss: 0.084\n",
      "Iteration: 36 \t--- Loss: 0.088\n",
      "Iteration: 37 \t--- Loss: 0.084\n",
      "Iteration: 38 \t--- Loss: 0.089\n",
      "Iteration: 39 \t--- Loss: 0.082\n",
      "Iteration: 40 \t--- Loss: 0.082\n",
      "Iteration: 41 \t--- Loss: 0.084\n",
      "Iteration: 42 \t--- Loss: 0.080\n",
      "Iteration: 43 \t--- Loss: 0.083\n",
      "Iteration: 44 \t--- Loss: 0.083\n",
      "Iteration: 45 \t--- Loss: 0.082\n",
      "Iteration: 46 \t--- Loss: 0.087\n",
      "Iteration: 47 \t--- Loss: 0.083\n",
      "Iteration: 48 \t--- Loss: 0.084\n",
      "Iteration: 49 \t--- Loss: 0.085\n",
      "Iteration: 50 \t--- Loss: 0.085\n",
      "Iteration: 51 \t--- Loss: 0.088\n",
      "Iteration: 52 \t--- Loss: 0.090\n",
      "Iteration: 53 \t--- Loss: 0.087\n",
      "Iteration: 54 \t--- Loss: 0.091\n",
      "Iteration: 55 \t--- Loss: 0.090\n",
      "Iteration: 56 \t--- Loss: 0.092\n",
      "Iteration: 57 \t--- Loss: 0.088\n",
      "Iteration: 58 \t--- Loss: 0.085\n",
      "Iteration: 59 \t--- Loss: 0.083\n",
      "Iteration: 60 \t--- Loss: 0.083\n",
      "Iteration: 61 \t--- Loss: 0.087\n",
      "Iteration: 62 \t--- Loss: 0.079\n",
      "Iteration: 63 \t--- Loss: 0.072\n",
      "Iteration: 64 \t--- Loss: 0.066\n",
      "Iteration: 65 \t--- Loss: 0.053\n",
      "Iteration: 66 \t--- Loss: 0.048\n",
      "Iteration: 67 \t--- Loss: 0.051\n",
      "Iteration: 68 \t--- Loss: 0.046\n",
      "Iteration: 69 \t--- Loss: 0.049\n",
      "Iteration: 70 \t--- Loss: 0.048\n",
      "Iteration: 71 \t--- Loss: 0.050\n",
      "Iteration: 72 \t--- Loss: 0.049\n",
      "Iteration: 73 \t--- Loss: 0.047\n",
      "Iteration: 74 \t--- Loss: 0.046\n",
      "Iteration: 75 \t--- Loss: 0.046\n",
      "Iteration: 76 \t--- Loss: 0.045\n",
      "Iteration: 77 \t--- Loss: 0.048\n",
      "Iteration: 78 \t--- Loss: 0.045\n",
      "Iteration: 79 \t--- Loss: 0.046\n",
      "Iteration: 80 \t--- Loss: 0.046\n",
      "Iteration: 81 \t--- Loss: 0.042\n",
      "Iteration: 82 \t--- Loss: 0.046\n",
      "Iteration: 83 \t--- Loss: 0.045\n",
      "Iteration: 84 \t--- Loss: 0.048\n",
      "Iteration: 85 \t--- Loss: 0.044\n",
      "Iteration: 86 \t--- Loss: 0.060\n",
      "Iteration: 87 \t--- Loss: 0.110\n",
      "Iteration: 88 \t--- Loss: 0.054\n",
      "Iteration: 89 \t--- Loss: 0.080\n",
      "Iteration: 90 \t--- Loss: 0.079\n",
      "Iteration: 91 \t--- Loss: 0.145\n",
      "Iteration: 92 \t--- Loss: 0.100\n",
      "Iteration: 93 \t--- Loss: 0.039\n",
      "Iteration: 94 \t--- Loss: 0.042\n",
      "Iteration: 95 \t--- Loss: 0.041\n",
      "Iteration: 96 \t--- Loss: 0.066\n",
      "Iteration: 97 \t--- Loss: 0.094\n",
      "Iteration: 98 \t--- Loss: 0.040\n",
      "Iteration: 99 \t--- Loss: 0.051\n",
      "Iteration: 100 \t--- Loss: 0.067\n",
      "Iteration: 101 \t--- Loss: 0.067\n",
      "Iteration: 102 \t--- Loss: 0.118\n",
      "Iteration: 103 \t--- Loss: 0.068\n",
      "Iteration: 104 \t--- Loss: 0.033\n",
      "Iteration: 105 \t--- Loss: 0.035\n",
      "Iteration: 106 \t--- Loss: 0.033\n",
      "Iteration: 107 \t--- Loss: 0.044\n",
      "Iteration: 108 \t--- Loss: 0.053\n",
      "Iteration: 109 \t--- Loss: 0.048\n",
      "Iteration: 110 \t--- Loss: 0.076\n",
      "Iteration: 111 \t--- Loss: 0.044\n",
      "Iteration: 112 \t--- Loss: 0.051\n",
      "Iteration: 113 \t--- Loss: 0.054\n",
      "Iteration: 114 \t--- Loss: 0.082\n",
      "Iteration: 115 \t--- Loss: 0.043\n",
      "Iteration: 116 \t--- Loss: 0.054\n",
      "Iteration: 117 \t--- Loss: 0.048\n",
      "Iteration: 118 \t--- Loss: 0.069\n",
      "Iteration: 119 \t--- Loss: 0.034\n",
      "Iteration: 120 \t--- Loss: 0.037\n",
      "Iteration: 121 \t--- Loss: 0.050\n",
      "Iteration: 122 \t--- Loss: 0.075\n",
      "Iteration: 123 \t--- Loss: 0.031\n",
      "Iteration: 124 \t--- Loss: 0.028\n",
      "Iteration: 125 \t--- Loss: 0.038\n",
      "Iteration: 126 \t--- Loss: 0.049\n",
      "Iteration: 127 \t--- Loss: 0.051\n",
      "Iteration: 128 \t--- Loss: 0.091\n",
      "Iteration: 129 \t--- Loss: 0.025\n",
      "Iteration: 130 \t--- Loss: 0.030\n",
      "Iteration: 131 \t--- Loss: 0.028\n",
      "Iteration: 132 \t--- Loss: 0.036\n",
      "Iteration: 133 \t--- Loss: 0.045\n",
      "Iteration: 134 \t--- Loss: 0.045\n",
      "Iteration: 135 \t--- Loss: 0.070\n",
      "Iteration: 136 \t--- Loss: 0.033\n",
      "Iteration: 137 \t--- Loss: 0.031\n",
      "Iteration: 138 \t--- Loss: 0.035\n",
      "Iteration: 139 \t--- Loss: 0.046\n",
      "Iteration: 140 \t--- Loss: 0.040\n",
      "Iteration: 141 \t--- Loss: 0.063\n",
      "Iteration: 142 \t--- Loss: 0.028\n",
      "Iteration: 143 \t--- Loss: 0.032\n",
      "Iteration: 144 \t--- Loss: 0.032\n",
      "Iteration: 145 \t--- Loss: 0.037\n",
      "Iteration: 146 \t--- Loss: 0.037\n",
      "Iteration: 147 \t--- Loss: 0.054\n",
      "Iteration: 148 \t--- Loss: 0.025\n",
      "Iteration: 149 \t--- Loss: 0.023\n",
      "Iteration: 150 \t--- Loss: 0.029\n",
      "Iteration: 151 \t--- Loss: 0.030\n",
      "Iteration: 152 \t--- Loss: 0.034\n",
      "Iteration: 153 \t--- Loss: 0.049\n",
      "Iteration: 154 \t--- Loss: 0.022\n",
      "Iteration: 155 \t--- Loss: 0.021\n",
      "Iteration: 156 \t--- Loss: 0.024\n",
      "Iteration: 157 \t--- Loss: 0.022\n",
      "Iteration: 158 \t--- Loss: 0.028\n",
      "Iteration: 159 \t--- Loss: 0.046\n",
      "Iteration: 160 \t--- Loss: 0.034\n",
      "Iteration: 161 \t--- Loss: 0.053\n",
      "Iteration: 162 \t--- Loss: 0.025\n",
      "Iteration: 163 \t--- Loss: 0.034\n",
      "Iteration: 164 \t--- Loss: 0.025\n",
      "Iteration: 165 \t--- Loss: 0.027\n",
      "Iteration: 166 \t--- Loss: 0.031\n",
      "Iteration: 167 \t--- Loss: 0.042\n",
      "Iteration: 168 \t--- Loss: 0.026\n",
      "Iteration: 169 \t--- Loss: 0.029\n",
      "Iteration: 170 \t--- Loss: 0.025\n",
      "Iteration: 171 \t--- Loss: 0.025\n",
      "Iteration: 172 \t--- Loss: 0.023\n",
      "Iteration: 173 \t--- Loss: 0.024\n",
      "Iteration: 174 \t--- Loss: 0.023\n",
      "Iteration: 175 \t--- Loss: 0.025\n",
      "Iteration: 176 \t--- Loss: 0.021\n",
      "Iteration: 177 \t--- Loss: 0.023\n",
      "Iteration: 178 \t--- Loss: 0.023\n",
      "Iteration: 179 \t--- Loss: 0.029\n",
      "Iteration: 180 \t--- Loss: 0.018\n",
      "Iteration: 181 \t--- Loss: 0.017\n",
      "Iteration: 182 \t--- Loss: 0.017\n",
      "Iteration: 183 \t--- Loss: 0.015\n",
      "Iteration: 184 \t--- Loss: 0.017\n",
      "Iteration: 185 \t--- Loss: 0.016\n",
      "Iteration: 186 \t--- Loss: 0.018\n",
      "Iteration: 187 \t--- Loss: 0.018\n",
      "Iteration: 188 \t--- Loss: 0.020\n",
      "Iteration: 189 \t--- Loss: 0.024\n",
      "Iteration: 190 \t--- Loss: 0.019\n",
      "Iteration: 191 \t--- Loss: 0.015\n",
      "Iteration: 192 \t--- Loss: 0.017\n",
      "Iteration: 193 \t--- Loss: 0.022\n",
      "Iteration: 194 \t--- Loss: 0.016\n",
      "Iteration: 195 \t--- Loss: 0.019\n",
      "Iteration: 196 \t--- Loss: 0.018\n",
      "Iteration: 197 \t--- Loss: 0.018\n",
      "Iteration: 198 \t--- Loss: 0.017\n",
      "Iteration: 199 \t--- Loss: 0.017\n",
      "Iteration: 200 \t--- Loss: 0.018\n",
      "Iteration: 201 \t--- Loss: 0.019\n",
      "Iteration: 202 \t--- Loss: 0.017\n",
      "Iteration: 203 \t--- Loss: 0.018\n",
      "Iteration: 204 \t--- Loss: 0.016\n",
      "Iteration: 205 \t--- Loss: 0.018\n",
      "Iteration: 206 \t--- Loss: 0.017\n",
      "Iteration: 207 \t--- Loss: 0.017\n",
      "Iteration: 208 \t--- Loss: 0.018\n",
      "Iteration: 209 \t--- Loss: 0.019\n",
      "Iteration: 210 \t--- Loss: 0.016\n",
      "Iteration: 211 \t--- Loss: 0.014\n",
      "Iteration: 212 \t--- Loss: 0.013\n",
      "Iteration: 213 \t--- Loss: 0.013\n",
      "Iteration: 214 \t--- Loss: 0.013\n",
      "Iteration: 215 \t--- Loss: 0.014\n",
      "Iteration: 216 \t--- Loss: 0.014\n",
      "Iteration: 217 \t--- Loss: 0.012\n",
      "Iteration: 218 \t--- Loss: 0.012\n",
      "Iteration: 219 \t--- Loss: 0.012\n",
      "Iteration: 220 \t--- Loss: 0.011\n",
      "Iteration: 221 \t--- Loss: 0.011\n",
      "Iteration: 222 \t--- Loss: 0.011\n",
      "Iteration: 223 \t--- Loss: 0.011\n",
      "Iteration: 224 \t--- Loss: 0.011\n",
      "Iteration: 225 \t--- Loss: 0.011\n",
      "Iteration: 226 \t--- Loss: 0.011\n",
      "Iteration: 227 \t--- Loss: 0.011\n",
      "Iteration: 228 \t--- Loss: 0.013\n",
      "Iteration: 229 \t--- Loss: 0.015\n",
      "Iteration: 230 \t--- Loss: 0.014\n",
      "Iteration: 231 \t--- Loss: 0.018\n",
      "Iteration: 232 \t--- Loss: 0.016\n",
      "Iteration: 233 \t--- Loss: 0.019\n",
      "Iteration: 234 \t--- Loss: 0.014\n",
      "Iteration: 235 \t--- Loss: 0.014\n",
      "Iteration: 236 \t--- Loss: 0.013\n",
      "Iteration: 237 \t--- Loss: 0.012\n",
      "Iteration: 238 \t--- Loss: 0.011\n",
      "Iteration: 239 \t--- Loss: 0.009\n",
      "Iteration: 240 \t--- Loss: 0.011\n",
      "Iteration: 241 \t--- Loss: 0.010\n",
      "Iteration: 242 \t--- Loss: 0.011\n",
      "Iteration: 243 \t--- Loss: 0.010\n",
      "Iteration: 244 \t--- Loss: 0.011\n",
      "Iteration: 245 \t--- Loss: 0.011\n",
      "Iteration: 246 \t--- Loss: 0.012\n",
      "Iteration: 247 \t--- Loss: 0.011\n",
      "Iteration: 248 \t--- Loss: 0.010\n",
      "Iteration: 249 \t--- Loss: 0.010\n",
      "Iteration: 250 \t--- Loss: 0.011\n",
      "Iteration: 251 \t--- Loss: 0.010\n",
      "Iteration: 252 \t--- Loss: 0.011\n",
      "Iteration: 253 \t--- Loss: 0.010\n",
      "Iteration: 254 \t--- Loss: 0.011\n",
      "Iteration: 255 \t--- Loss: 0.011\n",
      "Iteration: 256 \t--- Loss: 0.011\n",
      "Iteration: 257 \t--- Loss: 0.012\n",
      "Iteration: 258 \t--- Loss: 0.011\n",
      "Iteration: 259 \t--- Loss: 0.011---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:37<00:00, 97.22s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.146\n",
      "Iteration: 261 \t--- Loss: 0.137\n",
      "Iteration: 262 \t--- Loss: 0.125\n",
      "Iteration: 263 \t--- Loss: 0.191\n",
      "Iteration: 264 \t--- Loss: 0.211\n",
      "Iteration: 265 \t--- Loss: 0.546\n",
      "Iteration: 266 \t--- Loss: 0.206\n",
      "Iteration: 267 \t--- Loss: 0.145\n",
      "Iteration: 268 \t--- Loss: 0.129\n",
      "Iteration: 269 \t--- Loss: 0.134\n",
      "Iteration: 270 \t--- Loss: 0.151\n",
      "Iteration: 271 \t--- Loss: 0.133\n",
      "Iteration: 272 \t--- Loss: 0.210\n",
      "Iteration: 273 \t--- Loss: 0.171\n",
      "Iteration: 274 \t--- Loss: 0.319\n",
      "Iteration: 275 \t--- Loss: 0.135\n",
      "Iteration: 276 \t--- Loss: 0.128\n",
      "Iteration: 277 \t--- Loss: 0.125\n",
      "Iteration: 278 \t--- Loss: 0.131\n",
      "Iteration: 279 \t--- Loss: 0.126\n",
      "Iteration: 280 \t--- Loss: 0.126\n",
      "Iteration: 281 \t--- Loss: 0.154\n",
      "Iteration: 282 \t--- Loss: 0.199\n",
      "Iteration: 283 \t--- Loss: 0.474\n",
      "Iteration: 284 \t--- Loss: 0.157\n",
      "Iteration: 285 \t--- Loss: 0.148\n",
      "Iteration: 286 \t--- Loss: 0.140\n",
      "Iteration: 287 \t--- Loss: 0.135\n",
      "Iteration: 288 \t--- Loss: 0.200\n",
      "Iteration: 289 \t--- Loss: 0.181\n",
      "Iteration: 290 \t--- Loss: 0.327\n",
      "Iteration: 291 \t--- Loss: 0.132\n",
      "Iteration: 292 \t--- Loss: 0.160\n",
      "Iteration: 293 \t--- Loss: 0.143\n",
      "Iteration: 294 \t--- Loss: 0.152\n",
      "Iteration: 295 \t--- Loss: 0.146\n",
      "Iteration: 296 \t--- Loss: 0.241\n",
      "Iteration: 297 \t--- Loss: 0.162\n",
      "Iteration: 298 \t--- Loss: 0.307\n",
      "Iteration: 299 \t--- Loss: 0.141\n",
      "Iteration: 300 \t--- Loss: 0.161\n",
      "Iteration: 301 \t--- Loss: 0.149\n",
      "Iteration: 302 \t--- Loss: 0.216\n",
      "Iteration: 303 \t--- Loss: 0.152\n",
      "Iteration: 304 \t--- Loss: 0.189\n",
      "Iteration: 305 \t--- Loss: 0.157\n",
      "Iteration: 306 \t--- Loss: 0.294\n",
      "Iteration: 307 \t--- Loss: 0.148\n",
      "Iteration: 308 \t--- Loss: 0.160\n",
      "Iteration: 309 \t--- Loss: 0.148\n",
      "Iteration: 310 \t--- Loss: 0.180\n",
      "Iteration: 311 \t--- Loss: 0.140\n",
      "Iteration: 312 \t--- Loss: 0.184\n",
      "Iteration: 313 \t--- Loss: 0.162\n",
      "Iteration: 314 \t--- Loss: 0.293\n",
      "Iteration: 315 \t--- Loss: 0.144\n",
      "Iteration: 316 \t--- Loss: 0.156\n",
      "Iteration: 317 \t--- Loss: 0.154\n",
      "Iteration: 318 \t--- Loss: 0.178\n",
      "Iteration: 319 \t--- Loss: 0.161\n",
      "Iteration: 320 \t--- Loss: 0.264\n",
      "Iteration: 321 \t--- Loss: 0.142\n",
      "Iteration: 322 \t--- Loss: 0.140\n",
      "Iteration: 323 \t--- Loss: 0.136\n",
      "Iteration: 324 \t--- Loss: 0.142\n",
      "Iteration: 325 \t--- Loss: 0.157\n",
      "Iteration: 326 \t--- Loss: 0.283\n",
      "Iteration: 327 \t--- Loss: 0.159\n",
      "Iteration: 328 \t--- Loss: 0.250\n",
      "Iteration: 329 \t--- Loss: 0.147\n",
      "Iteration: 330 \t--- Loss: 0.219\n",
      "Iteration: 331 \t--- Loss: 0.144\n",
      "Iteration: 332 \t--- Loss: 0.181\n",
      "Iteration: 333 \t--- Loss: 0.157\n",
      "Iteration: 334 \t--- Loss: 0.213\n",
      "Iteration: 335 \t--- Loss: 0.161\n",
      "Iteration: 336 \t--- Loss: 0.255\n",
      "Iteration: 337 \t--- Loss: 0.154\n",
      "Iteration: 338 \t--- Loss: 0.194\n",
      "Iteration: 339 \t--- Loss: 0.161\n",
      "Iteration: 340 \t--- Loss: 0.243\n",
      "Iteration: 341 \t--- Loss: 0.157\n",
      "Iteration: 342 \t--- Loss: 0.229\n",
      "Iteration: 343 \t--- Loss: 0.160\n",
      "Iteration: 344 \t--- Loss: 0.143\n",
      "Iteration: 345 \t--- Loss: 0.143\n",
      "Iteration: 346 \t--- Loss: 0.202\n",
      "Iteration: 347 \t--- Loss: 0.164\n",
      "Iteration: 348 \t--- Loss: 0.250\n",
      "Iteration: 349 \t--- Loss: 0.142\n",
      "Iteration: 350 \t--- Loss: 0.237\n",
      "Iteration: 351 \t--- Loss: 0.149\n",
      "Iteration: 352 \t--- Loss: 0.185\n",
      "Iteration: 353 \t--- Loss: 0.160\n",
      "Iteration: 354 \t--- Loss: 0.221\n",
      "Iteration: 355 \t--- Loss: 0.146\n",
      "Iteration: 356 \t--- Loss: 0.215\n",
      "Iteration: 357 \t--- Loss: 0.169\n",
      "Iteration: 358 \t--- Loss: 0.255\n",
      "Iteration: 359 \t--- Loss: 0.159\n",
      "Iteration: 360 \t--- Loss: 0.253\n",
      "Iteration: 361 \t--- Loss: 0.151\n",
      "Iteration: 362 \t--- Loss: 0.227\n",
      "Iteration: 363 \t--- Loss: 0.170\n",
      "Iteration: 364 \t--- Loss: 0.242\n",
      "Iteration: 365 \t--- Loss: 0.149\n",
      "Iteration: 366 \t--- Loss: 0.245\n",
      "Iteration: 367 \t--- Loss: 0.151\n",
      "Iteration: 368 \t--- Loss: 0.202\n",
      "Iteration: 369 \t--- Loss: 0.142\n",
      "Iteration: 370 \t--- Loss: 0.144\n",
      "Iteration: 371 \t--- Loss: 0.138\n",
      "Iteration: 372 \t--- Loss: 0.199\n",
      "Iteration: 373 \t--- Loss: 0.165\n",
      "Iteration: 374 \t--- Loss: 0.254\n",
      "Iteration: 375 \t--- Loss: 0.141\n",
      "Iteration: 376 \t--- Loss: 0.195\n",
      "Iteration: 377 \t--- Loss: 0.150\n",
      "Iteration: 378 \t--- Loss: 0.207\n",
      "Iteration: 379 \t--- Loss: 0.159\n",
      "Iteration: 380 \t--- Loss: 0.256\n",
      "Iteration: 381 \t--- Loss: 0.148\n",
      "Iteration: 382 \t--- Loss: 0.180\n",
      "Iteration: 383 \t--- Loss: 0.149\n",
      "Iteration: 384 \t--- Loss: 0.132\n",
      "Iteration: 385 \t--- Loss: 0.136\n",
      "Iteration: 386 \t--- Loss: 0.163\n",
      "Iteration: 387 \t--- Loss: 0.151\n",
      "Iteration: 388 \t--- Loss: 0.274\n",
      "Iteration: 389 \t--- Loss: 0.171\n",
      "Iteration: 390 \t--- Loss: 0.280\n",
      "Iteration: 391 \t--- Loss: 0.142\n",
      "Iteration: 392 \t--- Loss: 0.164\n",
      "Iteration: 393 \t--- Loss: 0.146\n",
      "Iteration: 394 \t--- Loss: 0.218\n",
      "Iteration: 395 \t--- Loss: 0.162\n",
      "Iteration: 396 \t--- Loss: 0.239\n",
      "Iteration: 397 \t--- Loss: 0.147\n",
      "Iteration: 398 \t--- Loss: 0.155\n",
      "Iteration: 399 \t--- Loss: 0.158\n",
      "Iteration: 400 \t--- Loss: 0.189\n",
      "Iteration: 401 \t--- Loss: 0.152\n",
      "Iteration: 402 \t--- Loss: 0.229\n",
      "Iteration: 403 \t--- Loss: 0.147\n",
      "Iteration: 404 \t--- Loss: 0.201\n",
      "Iteration: 405 \t--- Loss: 0.168\n",
      "Iteration: 406 \t--- Loss: 0.326\n",
      "Iteration: 407 \t--- Loss: 0.148\n",
      "Iteration: 408 \t--- Loss: 0.141\n",
      "Iteration: 409 \t--- Loss: 0.147\n",
      "Iteration: 410 \t--- Loss: 0.193\n",
      "Iteration: 411 \t--- Loss: 0.162\n",
      "Iteration: 412 \t--- Loss: 0.221\n",
      "Iteration: 413 \t--- Loss: 0.148\n",
      "Iteration: 414 \t--- Loss: 0.204\n",
      "Iteration: 415 \t--- Loss: 0.156\n",
      "Iteration: 416 \t--- Loss: 0.190\n",
      "Iteration: 417 \t--- Loss: 0.137\n",
      "Iteration: 418 \t--- Loss: 0.185\n",
      "Iteration: 419 \t--- Loss: 0.161\n",
      "Iteration: 420 \t--- Loss: 0.257\n",
      "Iteration: 421 \t--- Loss: 0.152\n",
      "Iteration: 422 \t--- Loss: 0.205\n",
      "Iteration: 423 \t--- Loss: 0.143\n",
      "Iteration: 424 \t--- Loss: 0.164\n",
      "Iteration: 425 \t--- Loss: 0.171\n",
      "Iteration: 426 \t--- Loss: 0.248\n",
      "Iteration: 427 \t--- Loss: 0.133\n",
      "Iteration: 428 \t--- Loss: 0.144\n",
      "Iteration: 429 \t--- Loss: 0.150\n",
      "Iteration: 430 \t--- Loss: 0.242\n",
      "Iteration: 431 \t--- Loss: 0.172\n",
      "Iteration: 432 \t--- Loss: 0.274\n",
      "Iteration: 433 \t--- Loss: 0.146\n",
      "Iteration: 434 \t--- Loss: 0.179\n",
      "Iteration: 435 \t--- Loss: 0.168\n",
      "Iteration: 436 \t--- Loss: 0.286\n",
      "Iteration: 437 \t--- Loss: 0.136\n",
      "Iteration: 438 \t--- Loss: 0.159\n",
      "Iteration: 439 \t--- Loss: 0.135\n",
      "Iteration: 440 \t--- Loss: 0.174\n",
      "Iteration: 441 \t--- Loss: 0.150\n",
      "Iteration: 442 \t--- Loss: 0.220\n",
      "Iteration: 443 \t--- Loss: 0.160\n",
      "Iteration: 444 \t--- Loss: 0.258\n",
      "Iteration: 445 \t--- Loss: 0.148\n",
      "Iteration: 446 \t--- Loss: 0.158\n",
      "Iteration: 447 \t--- Loss: 0.150\n",
      "Iteration: 448 \t--- Loss: 0.173\n",
      "Iteration: 449 \t--- Loss: 0.149\n",
      "Iteration: 450 \t--- Loss: 0.239\n",
      "Iteration: 451 \t--- Loss: 0.179\n",
      "Iteration: 452 \t--- Loss: 0.330\n",
      "Iteration: 453 \t--- Loss: 0.129\n",
      "Iteration: 454 \t--- Loss: 0.133\n",
      "Iteration: 455 \t--- Loss: 0.139\n",
      "Iteration: 456 \t--- Loss: 0.127\n",
      "Iteration: 457 \t--- Loss: 0.122\n",
      "Iteration: 458 \t--- Loss: 0.125\n",
      "Iteration: 459 \t--- Loss: 0.143\n",
      "Iteration: 460 \t--- Loss: 0.149\n",
      "Iteration: 461 \t--- Loss: 0.322\n",
      "Iteration: 462 \t--- Loss: 0.187\n",
      "Iteration: 463 \t--- Loss: 0.375\n",
      "Iteration: 464 \t--- Loss: 0.140\n",
      "Iteration: 465 \t--- Loss: 0.132\n",
      "Iteration: 466 \t--- Loss: 0.139\n",
      "Iteration: 467 \t--- Loss: 0.145\n",
      "Iteration: 468 \t--- Loss: 0.143\n",
      "Iteration: 469 \t--- Loss: 0.172\n",
      "Iteration: 470 \t--- Loss: 0.163\n",
      "Iteration: 471 \t--- Loss: 0.284\n",
      "Iteration: 472 \t--- Loss: 0.161\n",
      "Iteration: 473 \t--- Loss: 0.230\n",
      "Iteration: 474 \t--- Loss: 0.140\n",
      "Iteration: 475 \t--- Loss: 0.176\n",
      "Iteration: 476 \t--- Loss: 0.149\n",
      "Iteration: 477 \t--- Loss: 0.208\n",
      "Iteration: 478 \t--- Loss: 0.155\n",
      "Iteration: 479 \t--- Loss: 0.245\n",
      "Iteration: 480 \t--- Loss: 0.164\n",
      "Iteration: 481 \t--- Loss: 0.260\n",
      "Iteration: 482 \t--- Loss: 0.148\n",
      "Iteration: 483 \t--- Loss: 0.177\n",
      "Iteration: 484 \t--- Loss: 0.142\n",
      "Iteration: 485 \t--- Loss: 0.137\n",
      "Iteration: 486 \t--- Loss: 0.148\n",
      "Iteration: 487 \t--- Loss: 0.202\n",
      "Iteration: 488 \t--- Loss: 0.207\n",
      "Iteration: 489 \t--- Loss: 0.480\n",
      "Iteration: 490 \t--- Loss: 0.155\n",
      "Iteration: 491 \t--- Loss: 0.148\n",
      "Iteration: 492 \t--- Loss: 0.148\n",
      "Iteration: 493 \t--- Loss: 0.140\n",
      "Iteration: 494 \t--- Loss: 0.187\n",
      "Iteration: 495 \t--- Loss: 0.154\n",
      "Iteration: 496 \t--- Loss: 0.189\n",
      "Iteration: 497 \t--- Loss: 0.153\n",
      "Iteration: 498 \t--- Loss: 0.194\n",
      "Iteration: 499 \t--- Loss: 0.166\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:12<00:01,  1.50s/it][Parallel(n_jobs=5)]: Done  90 tasks      | elapsed: 54.7min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:33<00:00, 93.32s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.011\n",
      "Iteration: 261 \t--- Loss: 0.012\n",
      "Iteration: 262 \t--- Loss: 0.011\n",
      "Iteration: 263 \t--- Loss: 0.011\n",
      "Iteration: 264 \t--- Loss: 0.010\n",
      "Iteration: 265 \t--- Loss: 0.010\n",
      "Iteration: 266 \t--- Loss: 0.010\n",
      "Iteration: 267 \t--- Loss: 0.010\n",
      "Iteration: 268 \t--- Loss: 0.010\n",
      "Iteration: 269 \t--- Loss: 0.011\n",
      "Iteration: 270 \t--- Loss: 0.009\n",
      "Iteration: 271 \t--- Loss: 0.010\n",
      "Iteration: 272 \t--- Loss: 0.009\n",
      "Iteration: 273 \t--- Loss: 0.009\n",
      "Iteration: 274 \t--- Loss: 0.010\n",
      "Iteration: 275 \t--- Loss: 0.010\n",
      "Iteration: 276 \t--- Loss: 0.010\n",
      "Iteration: 277 \t--- Loss: 0.011\n",
      "Iteration: 278 \t--- Loss: 0.010\n",
      "Iteration: 279 \t--- Loss: 0.009\n",
      "Iteration: 280 \t--- Loss: 0.009\n",
      "Iteration: 281 \t--- Loss: 0.008\n",
      "Iteration: 282 \t--- Loss: 0.008\n",
      "Iteration: 283 \t--- Loss: 0.009\n",
      "Iteration: 284 \t--- Loss: 0.008\n",
      "Iteration: 285 \t--- Loss: 0.008\n",
      "Iteration: 286 \t--- Loss: 0.008\n",
      "Iteration: 287 \t--- Loss: 0.008\n",
      "Iteration: 288 \t--- Loss: 0.009\n",
      "Iteration: 289 \t--- Loss: 0.008\n",
      "Iteration: 290 \t--- Loss: 0.009\n",
      "Iteration: 291 \t--- Loss: 0.008\n",
      "Iteration: 292 \t--- Loss: 0.008\n",
      "Iteration: 293 \t--- Loss: 0.009\n",
      "Iteration: 294 \t--- Loss: 0.008\n",
      "Iteration: 295 \t--- Loss: 0.009\n",
      "Iteration: 296 \t--- Loss: 0.008\n",
      "Iteration: 297 \t--- Loss: 0.009\n",
      "Iteration: 298 \t--- Loss: 0.009\n",
      "Iteration: 299 \t--- Loss: 0.008\n",
      "Iteration: 300 \t--- Loss: 0.008\n",
      "Iteration: 301 \t--- Loss: 0.008\n",
      "Iteration: 302 \t--- Loss: 0.008\n",
      "Iteration: 303 \t--- Loss: 0.009\n",
      "Iteration: 304 \t--- Loss: 0.008\n",
      "Iteration: 305 \t--- Loss: 0.009\n",
      "Iteration: 306 \t--- Loss: 0.007\n",
      "Iteration: 307 \t--- Loss: 0.008\n",
      "Iteration: 308 \t--- Loss: 0.008\n",
      "Iteration: 309 \t--- Loss: 0.008\n",
      "Iteration: 310 \t--- Loss: 0.008\n",
      "Iteration: 311 \t--- Loss: 0.009\n",
      "Iteration: 312 \t--- Loss: 0.007\n",
      "Iteration: 313 \t--- Loss: 0.008\n",
      "Iteration: 314 \t--- Loss: 0.008\n",
      "Iteration: 315 \t--- Loss: 0.008\n",
      "Iteration: 316 \t--- Loss: 0.008\n",
      "Iteration: 317 \t--- Loss: 0.007\n",
      "Iteration: 318 \t--- Loss: 0.008\n",
      "Iteration: 319 \t--- Loss: 0.008\n",
      "Iteration: 320 \t--- Loss: 0.008\n",
      "Iteration: 321 \t--- Loss: 0.008\n",
      "Iteration: 322 \t--- Loss: 0.008\n",
      "Iteration: 323 \t--- Loss: 0.008\n",
      "Iteration: 324 \t--- Loss: 0.008\n",
      "Iteration: 325 \t--- Loss: 0.008\n",
      "Iteration: 326 \t--- Loss: 0.008\n",
      "Iteration: 327 \t--- Loss: 0.008\n",
      "Iteration: 328 \t--- Loss: 0.008\n",
      "Iteration: 329 \t--- Loss: 0.008\n",
      "Iteration: 330 \t--- Loss: 0.008\n",
      "Iteration: 331 \t--- Loss: 0.007\n",
      "Iteration: 332 \t--- Loss: 0.008\n",
      "Iteration: 333 \t--- Loss: 0.008\n",
      "Iteration: 334 \t--- Loss: 0.008\n",
      "Iteration: 335 \t--- Loss: 0.008\n",
      "Iteration: 336 \t--- Loss: 0.008\n",
      "Iteration: 337 \t--- Loss: 0.007\n",
      "Iteration: 338 \t--- Loss: 0.007\n",
      "Iteration: 339 \t--- Loss: 0.008\n",
      "Iteration: 340 \t--- Loss: 0.008\n",
      "Iteration: 341 \t--- Loss: 0.008\n",
      "Iteration: 342 \t--- Loss: 0.008\n",
      "Iteration: 343 \t--- Loss: 0.009\n",
      "Iteration: 344 \t--- Loss: 0.008\n",
      "Iteration: 345 \t--- Loss: 0.008\n",
      "Iteration: 346 \t--- Loss: 0.009\n",
      "Iteration: 347 \t--- Loss: 0.008\n",
      "Iteration: 348 \t--- Loss: 0.008\n",
      "Iteration: 349 \t--- Loss: 0.009\n",
      "Iteration: 350 \t--- Loss: 0.008\n",
      "Iteration: 351 \t--- Loss: 0.007\n",
      "Iteration: 352 \t--- Loss: 0.008\n",
      "Iteration: 353 \t--- Loss: 0.008\n",
      "Iteration: 354 \t--- Loss: 0.008\n",
      "Iteration: 355 \t--- Loss: 0.008\n",
      "Iteration: 356 \t--- Loss: 0.008\n",
      "Iteration: 357 \t--- Loss: 0.008\n",
      "Iteration: 358 \t--- Loss: 0.008\n",
      "Iteration: 359 \t--- Loss: 0.007\n",
      "Iteration: 360 \t--- Loss: 0.007\n",
      "Iteration: 361 \t--- Loss: 0.008\n",
      "Iteration: 362 \t--- Loss: 0.008\n",
      "Iteration: 363 \t--- Loss: 0.007\n",
      "Iteration: 364 \t--- Loss: 0.008\n",
      "Iteration: 365 \t--- Loss: 0.008\n",
      "Iteration: 366 \t--- Loss: 0.007\n",
      "Iteration: 367 \t--- Loss: 0.007\n",
      "Iteration: 368 \t--- Loss: 0.007\n",
      "Iteration: 369 \t--- Loss: 0.008\n",
      "Iteration: 370 \t--- Loss: 0.008\n",
      "Iteration: 371 \t--- Loss: 0.008\n",
      "Iteration: 372 \t--- Loss: 0.007\n",
      "Iteration: 373 \t--- Loss: 0.008\n",
      "Iteration: 374 \t--- Loss: 0.007\n",
      "Iteration: 375 \t--- Loss: 0.007\n",
      "Iteration: 376 \t--- Loss: 0.007\n",
      "Iteration: 377 \t--- Loss: 0.007\n",
      "Iteration: 378 \t--- Loss: 0.007\n",
      "Iteration: 379 \t--- Loss: 0.007\n",
      "Iteration: 380 \t--- Loss: 0.007\n",
      "Iteration: 381 \t--- Loss: 0.008\n",
      "Iteration: 382 \t--- Loss: 0.007\n",
      "Iteration: 383 \t--- Loss: 0.007\n",
      "Iteration: 384 \t--- Loss: 0.008\n",
      "Iteration: 385 \t--- Loss: 0.007\n",
      "Iteration: 386 \t--- Loss: 0.007\n",
      "Iteration: 387 \t--- Loss: 0.008\n",
      "Iteration: 388 \t--- Loss: 0.007\n",
      "Iteration: 389 \t--- Loss: 0.007\n",
      "Iteration: 390 \t--- Loss: 0.007\n",
      "Iteration: 391 \t--- Loss: 0.008\n",
      "Iteration: 392 \t--- Loss: 0.008\n",
      "Iteration: 393 \t--- Loss: 0.007\n",
      "Iteration: 394 \t--- Loss: 0.007\n",
      "Iteration: 395 \t--- Loss: 0.008\n",
      "Iteration: 396 \t--- Loss: 0.007\n",
      "Iteration: 397 \t--- Loss: 0.007\n",
      "Iteration: 398 \t--- Loss: 0.008\n",
      "Iteration: 399 \t--- Loss: 0.007\n",
      "Iteration: 400 \t--- Loss: 0.008\n",
      "Iteration: 401 \t--- Loss: 0.007\n",
      "Iteration: 402 \t--- Loss: 0.006\n",
      "Iteration: 403 \t--- Loss: 0.007\n",
      "Iteration: 404 \t--- Loss: 0.007\n",
      "Iteration: 405 \t--- Loss: 0.007\n",
      "Iteration: 406 \t--- Loss: 0.007\n",
      "Iteration: 407 \t--- Loss: 0.007\n",
      "Iteration: 408 \t--- Loss: 0.008\n",
      "Iteration: 409 \t--- Loss: 0.007\n",
      "Iteration: 410 \t--- Loss: 0.007\n",
      "Iteration: 411 \t--- Loss: 0.008\n",
      "Iteration: 412 \t--- Loss: 0.007\n",
      "Iteration: 413 \t--- Loss: 0.007\n",
      "Iteration: 414 \t--- Loss: 0.008\n",
      "Iteration: 415 \t--- Loss: 0.008\n",
      "Iteration: 416 \t--- Loss: 0.007\n",
      "Iteration: 417 \t--- Loss: 0.007\n",
      "Iteration: 418 \t--- Loss: 0.007\n",
      "Iteration: 419 \t--- Loss: 0.007\n",
      "Iteration: 420 \t--- Loss: 0.007\n",
      "Iteration: 421 \t--- Loss: 0.008\n",
      "Iteration: 422 \t--- Loss: 0.008\n",
      "Iteration: 423 \t--- Loss: 0.007\n",
      "Iteration: 424 \t--- Loss: 0.007\n",
      "Iteration: 425 \t--- Loss: 0.007\n",
      "Iteration: 426 \t--- Loss: 0.008\n",
      "Iteration: 427 \t--- Loss: 0.007\n",
      "Iteration: 428 \t--- Loss: 0.007\n",
      "Iteration: 429 \t--- Loss: 0.007\n",
      "Iteration: 430 \t--- Loss: 0.008\n",
      "Iteration: 431 \t--- Loss: 0.007\n",
      "Iteration: 432 \t--- Loss: 0.008\n",
      "Iteration: 433 \t--- Loss: 0.007\n",
      "Iteration: 434 \t--- Loss: 0.007\n",
      "Iteration: 435 \t--- Loss: 0.007\n",
      "Iteration: 436 \t--- Loss: 0.007\n",
      "Iteration: 437 \t--- Loss: 0.007\n",
      "Iteration: 438 \t--- Loss: 0.008\n",
      "Iteration: 439 \t--- Loss: 0.007\n",
      "Iteration: 440 \t--- Loss: 0.007\n",
      "Iteration: 441 \t--- Loss: 0.008\n",
      "Iteration: 442 \t--- Loss: 0.007\n",
      "Iteration: 443 \t--- Loss: 0.008\n",
      "Iteration: 444 \t--- Loss: 0.008\n",
      "Iteration: 445 \t--- Loss: 0.007\n",
      "Iteration: 446 \t--- Loss: 0.007\n",
      "Iteration: 447 \t--- Loss: 0.007\n",
      "Iteration: 448 \t--- Loss: 0.008\n",
      "Iteration: 449 \t--- Loss: 0.007\n",
      "Iteration: 450 \t--- Loss: 0.007\n",
      "Iteration: 451 \t--- Loss: 0.007\n",
      "Iteration: 452 \t--- Loss: 0.007\n",
      "Iteration: 453 \t--- Loss: 0.007\n",
      "Iteration: 454 \t--- Loss: 0.007\n",
      "Iteration: 455 \t--- Loss: 0.008\n",
      "Iteration: 456 \t--- Loss: 0.008\n",
      "Iteration: 457 \t--- Loss: 0.007\n",
      "Iteration: 458 \t--- Loss: 0.007\n",
      "Iteration: 459 \t--- Loss: 0.008\n",
      "Iteration: 460 \t--- Loss: 0.007\n",
      "Iteration: 461 \t--- Loss: 0.007\n",
      "Iteration: 462 \t--- Loss: 0.007\n",
      "Iteration: 463 \t--- Loss: 0.008\n",
      "Iteration: 464 \t--- Loss: 0.007\n",
      "Iteration: 465 \t--- Loss: 0.007\n",
      "Iteration: 466 \t--- Loss: 0.007\n",
      "Iteration: 467 \t--- Loss: 0.007\n",
      "Iteration: 468 \t--- Loss: 0.006\n",
      "Iteration: 469 \t--- Loss: 0.008\n",
      "Iteration: 470 \t--- Loss: 0.007\n",
      "Iteration: 471 \t--- Loss: 0.007\n",
      "Iteration: 472 \t--- Loss: 0.007\n",
      "Iteration: 473 \t--- Loss: 0.008\n",
      "Iteration: 474 \t--- Loss: 0.007\n",
      "Iteration: 475 \t--- Loss: 0.008\n",
      "Iteration: 476 \t--- Loss: 0.007\n",
      "Iteration: 477 \t--- Loss: 0.008\n",
      "Iteration: 478 \t--- Loss: 0.007\n",
      "Iteration: 479 \t--- Loss: 0.008\n",
      "Iteration: 480 \t--- Loss: 0.007\n",
      "Iteration: 481 \t--- Loss: 0.007\n",
      "Iteration: 482 \t--- Loss: 0.007\n",
      "Iteration: 483 \t--- Loss: 0.007\n",
      "Iteration: 484 \t--- Loss: 0.007\n",
      "Iteration: 485 \t--- Loss: 0.007\n",
      "Iteration: 486 \t--- Loss: 0.008\n",
      "Iteration: 487 \t--- Loss: 0.007\n",
      "Iteration: 488 \t--- Loss: 0.007\n",
      "Iteration: 489 \t--- Loss: 0.007\n",
      "Iteration: 490 \t--- Loss: 0.007\n",
      "Iteration: 491 \t--- Loss: 0.008\n",
      "Iteration: 492 \t--- Loss: 0.007\n",
      "Iteration: 493 \t--- Loss: 0.008\n",
      "Iteration: 494 \t--- Loss: 0.007\n",
      "Iteration: 495 \t--- Loss: 0.007\n",
      "Iteration: 496 \t--- Loss: 0.007\n",
      "Iteration: 497 \t--- Loss: 0.008\n",
      "Iteration: 498 \t--- Loss: 0.007\n",
      "Iteration: 499 \t--- Loss: 0.007\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it][Parallel(n_jobs=5)]: Done  91 tasks      | elapsed: 55.4min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.060\n",
      "Iteration: 1 \t--- Loss: 0.063\n",
      "Iteration: 2 \t--- Loss: 0.060\n",
      "Iteration: 3 \t--- Loss: 0.052\n",
      "Iteration: 4 \t--- Loss: 0.051\n",
      "Iteration: 5 \t--- Loss: 0.043\n",
      "Iteration: 6 \t--- Loss: 0.040\n",
      "Iteration: 7 \t--- Loss: 0.039\n",
      "Iteration: 8 \t--- Loss: 0.036\n",
      "Iteration: 9 \t--- Loss: 0.029\n",
      "Iteration: 10 \t--- Loss: 0.028\n",
      "Iteration: 11 \t--- Loss: 0.027\n",
      "Iteration: 12 \t--- Loss: 0.027\n",
      "Iteration: 13 \t--- Loss: 0.028\n",
      "Iteration: 14 \t--- Loss: 0.022\n",
      "Iteration: 15 \t--- Loss: 0.024\n",
      "Iteration: 16 \t--- Loss: 0.021\n",
      "Iteration: 17 \t--- Loss: 0.021\n",
      "Iteration: 18 \t--- Loss: 0.019\n",
      "Iteration: 19 \t--- Loss: 0.018\n",
      "Iteration: 20 \t--- Loss: 0.015\n",
      "Iteration: 21 \t--- Loss: 0.015\n",
      "Iteration: 22 \t--- Loss: 0.014\n",
      "Iteration: 23 \t--- Loss: 0.017\n",
      "Iteration: 24 \t--- Loss: 0.015\n",
      "Iteration: 25 \t--- Loss: 0.013\n",
      "Iteration: 26 \t--- Loss: 0.012\n",
      "Iteration: 27 \t--- Loss: 0.012\n",
      "Iteration: 28 \t--- Loss: 0.012\n",
      "Iteration: 29 \t--- Loss: 0.011\n",
      "Iteration: 30 \t--- Loss: 0.012\n",
      "Iteration: 31 \t--- Loss: 0.011\n",
      "Iteration: 32 \t--- Loss: 0.010\n",
      "Iteration: 33 \t--- Loss: 0.011\n",
      "Iteration: 34 \t--- Loss: 0.010\n",
      "Iteration: 35 \t--- Loss: 0.010\n",
      "Iteration: 36 \t--- Loss: 0.009\n",
      "Iteration: 37 \t--- Loss: 0.009\n",
      "Iteration: 38 \t--- Loss: 0.009\n",
      "Iteration: 39 \t--- Loss: 0.010\n",
      "Iteration: 40 \t--- Loss: 0.009\n",
      "Iteration: 41 \t--- Loss: 0.009\n",
      "Iteration: 42 \t--- Loss: 0.009\n",
      "Iteration: 43 \t--- Loss: 0.009\n",
      "Iteration: 44 \t--- Loss: 0.010\n",
      "Iteration: 45 \t--- Loss: 0.009\n",
      "Iteration: 46 \t--- Loss: 0.009\n",
      "Iteration: 47 \t--- Loss: 0.008\n",
      "Iteration: 48 \t--- Loss: 0.008\n",
      "Iteration: 49 \t--- Loss: 0.008\n",
      "Iteration: 50 \t--- Loss: 0.009\n",
      "Iteration: 51 \t--- Loss: 0.008\n",
      "Iteration: 52 \t--- Loss: 0.008\n",
      "Iteration: 53 \t--- Loss: 0.008\n",
      "Iteration: 54 \t--- Loss: 0.008\n",
      "Iteration: 55 \t--- Loss: 0.008\n",
      "Iteration: 56 \t--- Loss: 0.008\n",
      "Iteration: 57 \t--- Loss: 0.009\n",
      "Iteration: 58 \t--- Loss: 0.008\n",
      "Iteration: 59 \t--- Loss: 0.008\n",
      "Iteration: 60 \t--- Loss: 0.008\n",
      "Iteration: 61 \t--- Loss: 0.008\n",
      "Iteration: 62 \t--- Loss: 0.007\n",
      "Iteration: 63 \t--- Loss: 0.008\n",
      "Iteration: 64 \t--- Loss: 0.008\n",
      "Iteration: 65 \t--- Loss: 0.008\n",
      "Iteration: 66 \t--- Loss: 0.007\n",
      "Iteration: 67 \t--- Loss: 0.008\n",
      "Iteration: 68 \t--- Loss: 0.008\n",
      "Iteration: 69 \t--- Loss: 0.008\n",
      "Iteration: 70 \t--- Loss: 0.008\n",
      "Iteration: 71 \t--- Loss: 0.008\n",
      "Iteration: 72 \t--- Loss: 0.008\n",
      "Iteration: 73 \t--- Loss: 0.008\n",
      "Iteration: 74 \t--- Loss: 0.007\n",
      "Iteration: 75 \t--- Loss: 0.008\n",
      "Iteration: 76 \t--- Loss: 0.007\n",
      "Iteration: 77 \t--- Loss: 0.007\n",
      "Iteration: 78 \t--- Loss: 0.008\n",
      "Iteration: 79 \t--- Loss: 0.008\n",
      "Iteration: 80 \t--- Loss: 0.008\n",
      "Iteration: 81 \t--- Loss: 0.008\n",
      "Iteration: 82 \t--- Loss: 0.007\n",
      "Iteration: 83 \t--- Loss: 0.008\n",
      "Iteration: 84 \t--- Loss: 0.008\n",
      "Iteration: 85 \t--- Loss: 0.007\n",
      "Iteration: 86 \t--- Loss: 0.007\n",
      "Iteration: 87 \t--- Loss: 0.007\n",
      "Iteration: 88 \t--- Loss: 0.007\n",
      "Iteration: 89 \t--- Loss: 0.007\n",
      "Iteration: 90 \t--- Loss: 0.007\n",
      "Iteration: 91 \t--- Loss: 0.008\n",
      "Iteration: 92 \t--- Loss: 0.007\n",
      "Iteration: 93 \t--- Loss: 0.007\n",
      "Iteration: 94 \t--- Loss: 0.007\n",
      "Iteration: 95 \t--- Loss: 0.007\n",
      "Iteration: 96 \t--- Loss: 0.008\n",
      "Iteration: 97 \t--- Loss: 0.008\n",
      "Iteration: 98 \t--- Loss: 0.007\n",
      "Iteration: 99 \t--- Loss: 0.008\n",
      "Iteration: 100 \t--- Loss: 0.008\n",
      "Iteration: 101 \t--- Loss: 0.008\n",
      "Iteration: 102 \t--- Loss: 0.007\n",
      "Iteration: 103 \t--- Loss: 0.007\n",
      "Iteration: 104 \t--- Loss: 0.007\n",
      "Iteration: 105 \t--- Loss: 0.008\n",
      "Iteration: 106 \t--- Loss: 0.007\n",
      "Iteration: 107 \t--- Loss: 0.007\n",
      "Iteration: 108 \t--- Loss: 0.007\n",
      "Iteration: 109 \t--- Loss: 0.007\n",
      "Iteration: 110 \t--- Loss: 0.007\n",
      "Iteration: 111 \t--- Loss: 0.007\n",
      "Iteration: 112 \t--- Loss: 0.007\n",
      "Iteration: 113 \t--- Loss: 0.007\n",
      "Iteration: 114 \t--- Loss: 0.007\n",
      "Iteration: 115 \t--- Loss: 0.007\n",
      "Iteration: 116 \t--- Loss: 0.007\n",
      "Iteration: 117 \t--- Loss: 0.008\n",
      "Iteration: 118 \t--- Loss: 0.007\n",
      "Iteration: 119 \t--- Loss: 0.007\n",
      "Iteration: 120 \t--- Loss: 0.008\n",
      "Iteration: 121 \t--- Loss: 0.008\n",
      "Iteration: 122 \t--- Loss: 0.008\n",
      "Iteration: 123 \t--- Loss: 0.007\n",
      "Iteration: 124 \t--- Loss: 0.008\n",
      "Iteration: 125 \t--- Loss: 0.007\n",
      "Iteration: 126 \t--- Loss: 0.008\n",
      "Iteration: 127 \t--- Loss: 0.007\n",
      "Iteration: 128 \t--- Loss: 0.007\n",
      "Iteration: 129 \t--- Loss: 0.007\n",
      "Iteration: 130 \t--- Loss: 0.008\n",
      "Iteration: 131 \t--- Loss: 0.008\n",
      "Iteration: 132 \t--- Loss: 0.007\n",
      "Iteration: 133 \t--- Loss: 0.007\n",
      "Iteration: 134 \t--- Loss: 0.007\n",
      "Iteration: 135 \t--- Loss: 0.007\n",
      "Iteration: 136 \t--- Loss: 0.007\n",
      "Iteration: 137 \t--- Loss: 0.008\n",
      "Iteration: 138 \t--- Loss: 0.007\n",
      "Iteration: 139 \t--- Loss: 0.008\n",
      "Iteration: 140 \t--- Loss: 0.007\n",
      "Iteration: 141 \t--- Loss: 0.008\n",
      "Iteration: 142 \t--- Loss: 0.007\n",
      "Iteration: 143 \t--- Loss: 0.007\n",
      "Iteration: 144 \t--- Loss: 0.007\n",
      "Iteration: 145 \t--- Loss: 0.007\n",
      "Iteration: 146 \t--- Loss: 0.007\n",
      "Iteration: 147 \t--- Loss: 0.007\n",
      "Iteration: 148 \t--- Loss: 0.007\n",
      "Iteration: 149 \t--- Loss: 0.008\n",
      "Iteration: 150 \t--- Loss: 0.007\n",
      "Iteration: 151 \t--- Loss: 0.007\n",
      "Iteration: 152 \t--- Loss: 0.008\n",
      "Iteration: 153 \t--- Loss: 0.008\n",
      "Iteration: 154 \t--- Loss: 0.007\n",
      "Iteration: 155 \t--- Loss: 0.007\n",
      "Iteration: 156 \t--- Loss: 0.007\n",
      "Iteration: 157 \t--- Loss: 0.007\n",
      "Iteration: 158 \t--- Loss: 0.007\n",
      "Iteration: 159 \t--- Loss: 0.008\n",
      "Iteration: 160 \t--- Loss: 0.007\n",
      "Iteration: 161 \t--- Loss: 0.007\n",
      "Iteration: 162 \t--- Loss: 0.007\n",
      "Iteration: 163 \t--- Loss: 0.008\n",
      "Iteration: 164 \t--- Loss: 0.008\n",
      "Iteration: 165 \t--- Loss: 0.007\n",
      "Iteration: 166 \t--- Loss: 0.007\n",
      "Iteration: 167 \t--- Loss: 0.007\n",
      "Iteration: 168 \t--- Loss: 0.008\n",
      "Iteration: 169 \t--- Loss: 0.007\n",
      "Iteration: 170 \t--- Loss: 0.007\n",
      "Iteration: 171 \t--- Loss: 0.007\n",
      "Iteration: 172 \t--- Loss: 0.007\n",
      "Iteration: 173 \t--- Loss: 0.007\n",
      "Iteration: 174 \t--- Loss: 0.007\n",
      "Iteration: 175 \t--- Loss: 0.007\n",
      "Iteration: 176 \t--- Loss: 0.007\n",
      "Iteration: 177 \t--- Loss: 0.007\n",
      "Iteration: 178 \t--- Loss: 0.007\n",
      "Iteration: 179 \t--- Loss: 0.008\n",
      "Iteration: 180 \t--- Loss: 0.007\n",
      "Iteration: 181 \t--- Loss: 0.008\n",
      "Iteration: 182 \t--- Loss: 0.007\n",
      "Iteration: 183 \t--- Loss: 0.007\n",
      "Iteration: 184 \t--- Loss: 0.008\n",
      "Iteration: 185 \t--- Loss: 0.007\n",
      "Iteration: 186 \t--- Loss: 0.007\n",
      "Iteration: 187 \t--- Loss: 0.007\n",
      "Iteration: 188 \t--- Loss: 0.007\n",
      "Iteration: 189 \t--- Loss: 0.007\n",
      "Iteration: 190 \t--- Loss: 0.008\n",
      "Iteration: 191 \t--- Loss: 0.007\n",
      "Iteration: 192 \t--- Loss: 0.008\n",
      "Iteration: 193 \t--- Loss: 0.007\n",
      "Iteration: 194 \t--- Loss: 0.007\n",
      "Iteration: 195 \t--- Loss: 0.007\n",
      "Iteration: 196 \t--- Loss: 0.007\n",
      "Iteration: 197 \t--- Loss: 0.007\n",
      "Iteration: 198 \t--- Loss: 0.008\n",
      "Iteration: 199 \t--- Loss: 0.008\n",
      "Iteration: 200 \t--- Loss: 0.007\n",
      "Iteration: 201 \t--- Loss: 0.007\n",
      "Iteration: 202 \t--- Loss: 0.007\n",
      "Iteration: 203 \t--- Loss: 0.007\n",
      "Iteration: 204 \t--- Loss: 0.007\n",
      "Iteration: 205 \t--- Loss: 0.007\n",
      "Iteration: 206 \t--- Loss: 0.007\n",
      "Iteration: 207 \t--- Loss: 0.007\n",
      "Iteration: 208 \t--- Loss: 0.007\n",
      "Iteration: 209 \t--- Loss: 0.007\n",
      "Iteration: 210 \t--- Loss: 0.007\n",
      "Iteration: 211 \t--- Loss: 0.007\n",
      "Iteration: 212 \t--- Loss: 0.007\n",
      "Iteration: 213 \t--- Loss: 0.007\n",
      "Iteration: 214 \t--- Loss: 0.007\n",
      "Iteration: 215 \t--- Loss: 0.007\n",
      "Iteration: 216 \t--- Loss: 0.007\n",
      "Iteration: 217 \t--- Loss: 0.007\n",
      "Iteration: 218 \t--- Loss: 0.007\n",
      "Iteration: 219 \t--- Loss: 0.007\n",
      "Iteration: 220 \t--- Loss: 0.007\n",
      "Iteration: 221 \t--- Loss: 0.007\n",
      "Iteration: 222 \t--- Loss: 0.007\n",
      "Iteration: 223 \t--- Loss: 0.007\n",
      "Iteration: 224 \t--- Loss: 0.007\n",
      "Iteration: 225 \t--- Loss: 0.007\n",
      "Iteration: 226 \t--- Loss: 0.007\n",
      "Iteration: 227 \t--- Loss: 0.007\n",
      "Iteration: 228 \t--- Loss: 0.007\n",
      "Iteration: 229 \t--- Loss: 0.007\n",
      "Iteration: 230 \t--- Loss: 0.007\n",
      "Iteration: 231 \t--- Loss: 0.007\n",
      "Iteration: 232 \t--- Loss: 0.007\n",
      "Iteration: 233 \t--- Loss: 0.007\n",
      "Iteration: 234 \t--- Loss: 0.008\n",
      "Iteration: 235 \t--- Loss: 0.007\n",
      "Iteration: 236 \t--- Loss: 0.008\n",
      "Iteration: 237 \t--- Loss: 0.007\n",
      "Iteration: 238 \t--- Loss: 0.007\n",
      "Iteration: 239 \t--- Loss: 0.007\n",
      "Iteration: 240 \t--- Loss: 0.007\n",
      "Iteration: 241 \t--- Loss: 0.007\n",
      "Iteration: 242 \t--- Loss: 0.008\n",
      "Iteration: 243 \t--- Loss: 0.007\n",
      "Iteration: 244 \t--- Loss: 0.008\n",
      "Iteration: 245 \t--- Loss: 0.007\n",
      "Iteration: 246 \t--- Loss: 0.007\n",
      "Iteration: 247 \t--- Loss: 0.007\n",
      "Iteration: 248 \t--- Loss: 0.008\n",
      "Iteration: 249 \t--- Loss: 0.008\n",
      "Iteration: 250 \t--- Loss: 0.007\n",
      "Iteration: 251 \t--- Loss: 0.007\n",
      "Iteration: 252 \t--- Loss: 0.007\n",
      "Iteration: 253 \t--- Loss: 0.007\n",
      "Iteration: 254 \t--- Loss: 0.007\n",
      "Iteration: 255 \t--- Loss: 0.007\n",
      "Iteration: 256 \t--- Loss: 0.008\n",
      "Iteration: 257 \t--- Loss: 0.007\n",
      "Iteration: 258 \t--- Loss: 0.007\n",
      "Iteration: 259 \t--- Loss: 0.008Iteration: 0 \t--- Loss: 1.163\n",
      "Iteration: 1 \t--- Loss: 1.161\n",
      "Iteration: 2 \t--- Loss: 1.038\n",
      "Iteration: 3 \t--- Loss: 1.048\n",
      "Iteration: 4 \t--- Loss: 0.958\n",
      "Iteration: 5 \t--- Loss: 0.957\n",
      "Iteration: 6 \t--- Loss: 0.868\n",
      "Iteration: 7 \t--- Loss: 0.906\n",
      "Iteration: 8 \t--- Loss: 0.807\n",
      "Iteration: 9 \t--- Loss: 0.790\n",
      "Iteration: 10 \t--- Loss: 0.825\n",
      "Iteration: 11 \t--- Loss: 0.786\n",
      "Iteration: 12 \t--- Loss: 0.774\n",
      "Iteration: 13 \t--- Loss: 0.771\n",
      "Iteration: 14 \t--- Loss: 0.787\n",
      "Iteration: 15 \t--- Loss: 0.782\n",
      "Iteration: 16 \t--- Loss: 0.797\n",
      "Iteration: 17 \t--- Loss: 0.812\n",
      "Iteration: 18 \t--- Loss: 0.735\n",
      "Iteration: 19 \t--- Loss: 0.734\n",
      "Iteration: 20 \t--- Loss: 0.743\n",
      "Iteration: 21 \t--- Loss: 0.721\n",
      "Iteration: 22 \t--- Loss: 0.733\n",
      "Iteration: 23 \t--- Loss: 0.719\n",
      "Iteration: 24 \t--- Loss: 0.755\n",
      "Iteration: 25 \t--- Loss: 0.714\n",
      "Iteration: 26 \t--- Loss: 0.704\n",
      "Iteration: 27 \t--- Loss: 0.722\n",
      "Iteration: 28 \t--- Loss: 0.706\n",
      "Iteration: 29 \t--- Loss: 0.734\n",
      "Iteration: 30 \t--- Loss: 0.730\n",
      "Iteration: 31 \t--- Loss: 0.749\n",
      "Iteration: 32 \t--- Loss: 0.745\n",
      "Iteration: 33 \t--- Loss: 0.738\n",
      "Iteration: 34 \t--- Loss: 0.732\n",
      "Iteration: 35 \t--- Loss: 0.780\n",
      "Iteration: 36 \t--- Loss: 0.765\n",
      "Iteration: 37 \t--- Loss: 0.738\n",
      "Iteration: 38 \t--- Loss: 0.694\n",
      "Iteration: 39 \t--- Loss: 0.678\n",
      "Iteration: 40 \t--- Loss: 0.719\n",
      "Iteration: 41 \t--- Loss: 0.793\n",
      "Iteration: 42 \t--- Loss: 0.726\n",
      "Iteration: 43 \t--- Loss: 0.722\n",
      "Iteration: 44 \t--- Loss: 0.758\n",
      "Iteration: 45 \t--- Loss: 0.686\n",
      "Iteration: 46 \t--- Loss: 0.715\n",
      "Iteration: 47 \t--- Loss: 0.730\n",
      "Iteration: 48 \t--- Loss: 0.702\n",
      "Iteration: 49 \t--- Loss: 0.680\n",
      "Iteration: 50 \t--- Loss: 0.692\n",
      "Iteration: 51 \t--- Loss: 0.675\n",
      "Iteration: 52 \t--- Loss: 0.756\n",
      "Iteration: 53 \t--- Loss: 0.706\n",
      "Iteration: 54 \t--- Loss: 0.815\n",
      "Iteration: 55 \t--- Loss: 0.680\n",
      "Iteration: 56 \t--- Loss: 0.713\n",
      "Iteration: 57 \t--- Loss: 0.749\n",
      "Iteration: 58 \t--- Loss: 0.771\n",
      "Iteration: 59 \t--- Loss: 0.687\n",
      "Iteration: 60 \t--- Loss: 0.793\n",
      "Iteration: 61 \t--- Loss: 0.726\n",
      "Iteration: 62 \t--- Loss: 0.702\n",
      "Iteration: 63 \t--- Loss: 0.708\n",
      "Iteration: 64 \t--- Loss: 0.770\n",
      "Iteration: 65 \t--- Loss: 0.765\n",
      "Iteration: 66 \t--- Loss: 0.692\n",
      "Iteration: 67 \t--- Loss: 0.674\n",
      "Iteration: 68 \t--- Loss: 0.800\n",
      "Iteration: 69 \t--- Loss: 0.773\n",
      "Iteration: 70 \t--- Loss: 0.779\n",
      "Iteration: 71 \t--- Loss: 0.682\n",
      "Iteration: 72 \t--- Loss: 0.759\n",
      "Iteration: 73 \t--- Loss: 0.749\n",
      "Iteration: 74 \t--- Loss: 0.709\n",
      "Iteration: 75 \t--- Loss: 0.718\n",
      "Iteration: 76 \t--- Loss: 0.743\n",
      "Iteration: 77 \t--- Loss: 0.718\n",
      "Iteration: 78 \t--- Loss: 0.748\n",
      "Iteration: 79 \t--- Loss: 0.750\n",
      "Iteration: 80 \t--- Loss: 0.772\n",
      "Iteration: 81 \t--- Loss: 0.822\n",
      "Iteration: 82 \t--- Loss: 0.720\n",
      "Iteration: 83 \t--- Loss: 0.737\n",
      "Iteration: 84 \t--- Loss: 0.762\n",
      "Iteration: 85 \t--- Loss: 0.715\n",
      "Iteration: 86 \t--- Loss: 0.751\n",
      "Iteration: 87 \t--- Loss: 0.780\n",
      "Iteration: 88 \t--- Loss: 0.708\n",
      "Iteration: 89 \t--- Loss: 0.764\n",
      "Iteration: 90 \t--- Loss: 0.706\n",
      "Iteration: 91 \t--- Loss: 0.740\n",
      "Iteration: 92 \t--- Loss: 0.745\n",
      "Iteration: 93 \t--- Loss: 0.765\n",
      "Iteration: 94 \t--- Loss: 0.813\n",
      "Iteration: 95 \t--- Loss: 0.751\n",
      "Iteration: 96 \t--- Loss: 0.779\n",
      "Iteration: 97 \t--- Loss: 0.728\n",
      "Iteration: 98 \t--- Loss: 0.707\n",
      "Iteration: 99 \t--- Loss: 0.719\n",
      "Iteration: 100 \t--- Loss: 0.737\n",
      "Iteration: 101 \t--- Loss: 0.715\n",
      "Iteration: 102 \t--- Loss: 0.731\n",
      "Iteration: 103 \t--- Loss: 0.690\n",
      "Iteration: 104 \t--- Loss: 0.732\n",
      "Iteration: 105 \t--- Loss: 0.783\n",
      "Iteration: 106 \t--- Loss: 0.726\n",
      "Iteration: 107 \t--- Loss: 0.709\n",
      "Iteration: 108 \t--- Loss: 0.749\n",
      "Iteration: 109 \t--- Loss: 0.710\n",
      "Iteration: 110 \t--- Loss: 0.737\n",
      "Iteration: 111 \t--- Loss: 0.708\n",
      "Iteration: 112 \t--- Loss: 0.760\n",
      "Iteration: 113 \t--- Loss: 0.728\n",
      "Iteration: 114 \t--- Loss: 0.726\n",
      "Iteration: 115 \t--- Loss: 0.725\n",
      "Iteration: 116 \t--- Loss: 0.797\n",
      "Iteration: 117 \t--- Loss: 0.742\n",
      "Iteration: 118 \t--- Loss: 0.713\n",
      "Iteration: 119 \t--- Loss: 0.716\n",
      "Iteration: 120 \t--- Loss: 0.699\n",
      "Iteration: 121 \t--- Loss: 0.761\n",
      "Iteration: 122 \t--- Loss: 0.742\n",
      "Iteration: 123 \t--- Loss: 0.778\n",
      "Iteration: 124 \t--- Loss: 0.758\n",
      "Iteration: 125 \t--- Loss: 0.723\n",
      "Iteration: 126 \t--- Loss: 0.759\n",
      "Iteration: 127 \t--- Loss: 0.741\n",
      "Iteration: 128 \t--- Loss: 0.776\n",
      "Iteration: 129 \t--- Loss: 0.707\n",
      "Iteration: 130 \t--- Loss: 0.796\n",
      "Iteration: 131 \t--- Loss: 0.724\n",
      "Iteration: 132 \t--- Loss: 0.717\n",
      "Iteration: 133 \t--- Loss: 0.742\n",
      "Iteration: 134 \t--- Loss: 0.669\n",
      "Iteration: 135 \t--- Loss: 0.706\n",
      "Iteration: 136 \t--- Loss: 0.737\n",
      "Iteration: 137 \t--- Loss: 0.759\n",
      "Iteration: 138 \t--- Loss: 0.744\n",
      "Iteration: 139 \t--- Loss: 0.742\n",
      "Iteration: 140 \t--- Loss: 0.716\n",
      "Iteration: 141 \t--- Loss: 0.736\n",
      "Iteration: 142 \t--- Loss: 0.720\n",
      "Iteration: 143 \t--- Loss: 0.796\n",
      "Iteration: 144 \t--- Loss: 0.692\n",
      "Iteration: 145 \t--- Loss: 0.797\n",
      "Iteration: 146 \t--- Loss: 0.781\n",
      "Iteration: 147 \t--- Loss: 0.814\n",
      "Iteration: 148 \t--- Loss: 0.729\n",
      "Iteration: 149 \t--- Loss: 0.719\n",
      "Iteration: 150 \t--- Loss: 0.792\n",
      "Iteration: 151 \t--- Loss: 0.731\n",
      "Iteration: 152 \t--- Loss: 0.738\n",
      "Iteration: 153 \t--- Loss: 0.733\n",
      "Iteration: 154 \t--- Loss: 0.702\n",
      "Iteration: 155 \t--- Loss: 0.741\n",
      "Iteration: 156 \t--- Loss: 0.764\n",
      "Iteration: 157 \t--- Loss: 0.708\n",
      "Iteration: 158 \t--- Loss: 0.739\n",
      "Iteration: 159 \t--- Loss: 0.702\n",
      "Iteration: 160 \t--- Loss: 0.765\n",
      "Iteration: 161 \t--- Loss: 0.802\n",
      "Iteration: 162 \t--- Loss: 0.717\n",
      "Iteration: 163 \t--- Loss: 0.748\n",
      "Iteration: 164 \t--- Loss: 0.732\n",
      "Iteration: 165 \t--- Loss: 0.718\n",
      "Iteration: 166 \t--- Loss: 0.749\n",
      "Iteration: 167 \t--- Loss: 0.766\n",
      "Iteration: 168 \t--- Loss: 0.688\n",
      "Iteration: 169 \t--- Loss: 0.761\n",
      "Iteration: 170 \t--- Loss: 0.732\n",
      "Iteration: 171 \t--- Loss: 0.752\n",
      "Iteration: 172 \t--- Loss: 0.775\n",
      "Iteration: 173 \t--- Loss: 0.749\n",
      "Iteration: 174 \t--- Loss: 0.769\n",
      "Iteration: 175 \t--- Loss: 0.769\n",
      "Iteration: 176 \t--- Loss: 0.739\n",
      "Iteration: 177 \t--- Loss: 0.756\n",
      "Iteration: 178 \t--- Loss: 0.759\n",
      "Iteration: 179 \t--- Loss: 0.722\n",
      "Iteration: 180 \t--- Loss: 0.693\n",
      "Iteration: 181 \t--- Loss: 0.739\n",
      "Iteration: 182 \t--- Loss: 0.700\n",
      "Iteration: 183 \t--- Loss: 0.812\n",
      "Iteration: 184 \t--- Loss: 0.770\n",
      "Iteration: 185 \t--- Loss: 0.655\n",
      "Iteration: 186 \t--- Loss: 0.730\n",
      "Iteration: 187 \t--- Loss: 0.739\n",
      "Iteration: 188 \t--- Loss: 0.733\n",
      "Iteration: 189 \t--- Loss: 0.749\n",
      "Iteration: 190 \t--- Loss: 0.788\n",
      "Iteration: 191 \t--- Loss: 0.744\n",
      "Iteration: 192 \t--- Loss: 0.734\n",
      "Iteration: 193 \t--- Loss: 0.708\n",
      "Iteration: 194 \t--- Loss: 0.793\n",
      "Iteration: 195 \t--- Loss: 0.757\n",
      "Iteration: 196 \t--- Loss: 0.815\n",
      "Iteration: 197 \t--- Loss: 0.758\n",
      "Iteration: 198 \t--- Loss: 0.742\n",
      "Iteration: 199 \t--- Loss: 0.724\n",
      "Iteration: 200 \t--- Loss: 0.734\n",
      "Iteration: 201 \t--- Loss: 0.702\n",
      "Iteration: 202 \t--- Loss: 0.728\n",
      "Iteration: 203 \t--- Loss: 0.714\n",
      "Iteration: 204 \t--- Loss: 0.727\n",
      "Iteration: 205 \t--- Loss: 0.759\n",
      "Iteration: 206 \t--- Loss: 0.721\n",
      "Iteration: 207 \t--- Loss: 0.693\n",
      "Iteration: 208 \t--- Loss: 0.808\n",
      "Iteration: 209 \t--- Loss: 0.728\n",
      "Iteration: 210 \t--- Loss: 0.750\n",
      "Iteration: 211 \t--- Loss: 0.780\n",
      "Iteration: 212 \t--- Loss: 0.728\n",
      "Iteration: 213 \t--- Loss: 0.713\n",
      "Iteration: 214 \t--- Loss: 0.702\n",
      "Iteration: 215 \t--- Loss: 0.728\n",
      "Iteration: 216 \t--- Loss: 0.803\n",
      "Iteration: 217 \t--- Loss: 0.750\n",
      "Iteration: 218 \t--- Loss: 0.754\n",
      "Iteration: 219 \t--- Loss: 0.738\n",
      "Iteration: 220 \t--- Loss: 0.756\n",
      "Iteration: 221 \t--- Loss: 0.663\n",
      "Iteration: 222 \t--- Loss: 0.737\n",
      "Iteration: 223 \t--- Loss: 0.762\n",
      "Iteration: 224 \t--- Loss: 0.759\n",
      "Iteration: 225 \t--- Loss: 0.740\n",
      "Iteration: 226 \t--- Loss: 0.748\n",
      "Iteration: 227 \t--- Loss: 0.715\n",
      "Iteration: 228 \t--- Loss: 0.746\n",
      "Iteration: 229 \t--- Loss: 0.730\n",
      "Iteration: 230 \t--- Loss: 0.747\n",
      "Iteration: 231 \t--- Loss: 0.755\n",
      "Iteration: 232 \t--- Loss: 0.736\n",
      "Iteration: 233 \t--- Loss: 0.756\n",
      "Iteration: 234 \t--- Loss: 0.676\n",
      "Iteration: 235 \t--- Loss: 0.715\n",
      "Iteration: 236 \t--- Loss: 0.664\n",
      "Iteration: 237 \t--- Loss: 0.708\n",
      "Iteration: 238 \t--- Loss: 0.715\n",
      "Iteration: 239 \t--- Loss: 0.724\n",
      "Iteration: 240 \t--- Loss: 0.750\n",
      "Iteration: 241 \t--- Loss: 0.803\n",
      "Iteration: 242 \t--- Loss: 0.735\n",
      "Iteration: 243 \t--- Loss: 0.725\n",
      "Iteration: 244 \t--- Loss: 0.733\n",
      "Iteration: 245 \t--- Loss: 0.781\n",
      "Iteration: 246 \t--- Loss: 0.755\n",
      "Iteration: 247 \t--- Loss: 0.713\n",
      "Iteration: 248 \t--- Loss: 0.771\n",
      "Iteration: 249 \t--- Loss: 0.768\n",
      "Iteration: 250 \t--- Loss: 0.699\n",
      "Iteration: 251 \t--- Loss: 0.712\n",
      "Iteration: 252 \t--- Loss: 0.789\n",
      "Iteration: 253 \t--- Loss: 0.760\n",
      "Iteration: 254 \t--- Loss: 0.776\n",
      "Iteration: 255 \t--- Loss: 0.763\n",
      "Iteration: 256 \t--- Loss: 0.754\n",
      "Iteration: 257 \t--- Loss: 0.752\n",
      "Iteration: 258 \t--- Loss: 0.734\n",
      "Iteration: 259 \t--- Loss: 0.736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:33<00:00, 93.75s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.007\n",
      "Iteration: 261 \t--- Loss: 0.008\n",
      "Iteration: 262 \t--- Loss: 0.007\n",
      "Iteration: 263 \t--- Loss: 0.008\n",
      "Iteration: 264 \t--- Loss: 0.007\n",
      "Iteration: 265 \t--- Loss: 0.007\n",
      "Iteration: 266 \t--- Loss: 0.007\n",
      "Iteration: 267 \t--- Loss: 0.007\n",
      "Iteration: 268 \t--- Loss: 0.007\n",
      "Iteration: 269 \t--- Loss: 0.007\n",
      "Iteration: 270 \t--- Loss: 0.007\n",
      "Iteration: 271 \t--- Loss: 0.007\n",
      "Iteration: 272 \t--- Loss: 0.007\n",
      "Iteration: 273 \t--- Loss: 0.008\n",
      "Iteration: 274 \t--- Loss: 0.007\n",
      "Iteration: 275 \t--- Loss: 0.007\n",
      "Iteration: 276 \t--- Loss: 0.007\n",
      "Iteration: 277 \t--- Loss: 0.007\n",
      "Iteration: 278 \t--- Loss: 0.007\n",
      "Iteration: 279 \t--- Loss: 0.007\n",
      "Iteration: 280 \t--- Loss: 0.008\n",
      "Iteration: 281 \t--- Loss: 0.007\n",
      "Iteration: 282 \t--- Loss: 0.007\n",
      "Iteration: 283 \t--- Loss: 0.008\n",
      "Iteration: 284 \t--- Loss: 0.007\n",
      "Iteration: 285 \t--- Loss: 0.007\n",
      "Iteration: 286 \t--- Loss: 0.008\n",
      "Iteration: 287 \t--- Loss: 0.008\n",
      "Iteration: 288 \t--- Loss: 0.007\n",
      "Iteration: 289 \t--- Loss: 0.008\n",
      "Iteration: 290 \t--- Loss: 0.007\n",
      "Iteration: 291 \t--- Loss: 0.007\n",
      "Iteration: 292 \t--- Loss: 0.007\n",
      "Iteration: 293 \t--- Loss: 0.007\n",
      "Iteration: 294 \t--- Loss: 0.007\n",
      "Iteration: 295 \t--- Loss: 0.007\n",
      "Iteration: 296 \t--- Loss: 0.007\n",
      "Iteration: 297 \t--- Loss: 0.007\n",
      "Iteration: 298 \t--- Loss: 0.008\n",
      "Iteration: 299 \t--- Loss: 0.007\n",
      "Iteration: 300 \t--- Loss: 0.007\n",
      "Iteration: 301 \t--- Loss: 0.007\n",
      "Iteration: 302 \t--- Loss: 0.008\n",
      "Iteration: 303 \t--- Loss: 0.007\n",
      "Iteration: 304 \t--- Loss: 0.007\n",
      "Iteration: 305 \t--- Loss: 0.007\n",
      "Iteration: 306 \t--- Loss: 0.007\n",
      "Iteration: 307 \t--- Loss: 0.007\n",
      "Iteration: 308 \t--- Loss: 0.007\n",
      "Iteration: 309 \t--- Loss: 0.007\n",
      "Iteration: 310 \t--- Loss: 0.007\n",
      "Iteration: 311 \t--- Loss: 0.007\n",
      "Iteration: 312 \t--- Loss: 0.007\n",
      "Iteration: 313 \t--- Loss: 0.007\n",
      "Iteration: 314 \t--- Loss: 0.007\n",
      "Iteration: 315 \t--- Loss: 0.007\n",
      "Iteration: 316 \t--- Loss: 0.007\n",
      "Iteration: 317 \t--- Loss: 0.007\n",
      "Iteration: 318 \t--- Loss: 0.007\n",
      "Iteration: 319 \t--- Loss: 0.008\n",
      "Iteration: 320 \t--- Loss: 0.007\n",
      "Iteration: 321 \t--- Loss: 0.008\n",
      "Iteration: 322 \t--- Loss: 0.007\n",
      "Iteration: 323 \t--- Loss: 0.007\n",
      "Iteration: 324 \t--- Loss: 0.007\n",
      "Iteration: 325 \t--- Loss: 0.007\n",
      "Iteration: 326 \t--- Loss: 0.007\n",
      "Iteration: 327 \t--- Loss: 0.008\n",
      "Iteration: 328 \t--- Loss: 0.007\n",
      "Iteration: 329 \t--- Loss: 0.007\n",
      "Iteration: 330 \t--- Loss: 0.007\n",
      "Iteration: 331 \t--- Loss: 0.007\n",
      "Iteration: 332 \t--- Loss: 0.007\n",
      "Iteration: 333 \t--- Loss: 0.008\n",
      "Iteration: 334 \t--- Loss: 0.007\n",
      "Iteration: 335 \t--- Loss: 0.007\n",
      "Iteration: 336 \t--- Loss: 0.007\n",
      "Iteration: 337 \t--- Loss: 0.007\n",
      "Iteration: 338 \t--- Loss: 0.007\n",
      "Iteration: 339 \t--- Loss: 0.007\n",
      "Iteration: 340 \t--- Loss: 0.007\n",
      "Iteration: 341 \t--- Loss: 0.007\n",
      "Iteration: 342 \t--- Loss: 0.007\n",
      "Iteration: 343 \t--- Loss: 0.007\n",
      "Iteration: 344 \t--- Loss: 0.007\n",
      "Iteration: 345 \t--- Loss: 0.007\n",
      "Iteration: 346 \t--- Loss: 0.007\n",
      "Iteration: 347 \t--- Loss: 0.007\n",
      "Iteration: 348 \t--- Loss: 0.007\n",
      "Iteration: 349 \t--- Loss: 0.008\n",
      "Iteration: 350 \t--- Loss: 0.007\n",
      "Iteration: 351 \t--- Loss: 0.007\n",
      "Iteration: 352 \t--- Loss: 0.007\n",
      "Iteration: 353 \t--- Loss: 0.007\n",
      "Iteration: 354 \t--- Loss: 0.007\n",
      "Iteration: 355 \t--- Loss: 0.007\n",
      "Iteration: 356 \t--- Loss: 0.007\n",
      "Iteration: 357 \t--- Loss: 0.007\n",
      "Iteration: 358 \t--- Loss: 0.008\n",
      "Iteration: 359 \t--- Loss: 0.007\n",
      "Iteration: 360 \t--- Loss: 0.007\n",
      "Iteration: 361 \t--- Loss: 0.007\n",
      "Iteration: 362 \t--- Loss: 0.007\n",
      "Iteration: 363 \t--- Loss: 0.007\n",
      "Iteration: 364 \t--- Loss: 0.007\n",
      "Iteration: 365 \t--- Loss: 0.007\n",
      "Iteration: 366 \t--- Loss: 0.007\n",
      "Iteration: 367 \t--- Loss: 0.007\n",
      "Iteration: 368 \t--- Loss: 0.007\n",
      "Iteration: 369 \t--- Loss: 0.007\n",
      "Iteration: 370 \t--- Loss: 0.007\n",
      "Iteration: 371 \t--- Loss: 0.007\n",
      "Iteration: 372 \t--- Loss: 0.007\n",
      "Iteration: 373 \t--- Loss: 0.007\n",
      "Iteration: 374 \t--- Loss: 0.007\n",
      "Iteration: 375 \t--- Loss: 0.007\n",
      "Iteration: 376 \t--- Loss: 0.007\n",
      "Iteration: 377 \t--- Loss: 0.007\n",
      "Iteration: 378 \t--- Loss: 0.007\n",
      "Iteration: 379 \t--- Loss: 0.007\n",
      "Iteration: 380 \t--- Loss: 0.007\n",
      "Iteration: 381 \t--- Loss: 0.007\n",
      "Iteration: 382 \t--- Loss: 0.007\n",
      "Iteration: 383 \t--- Loss: 0.007\n",
      "Iteration: 384 \t--- Loss: 0.008\n",
      "Iteration: 385 \t--- Loss: 0.007\n",
      "Iteration: 386 \t--- Loss: 0.007\n",
      "Iteration: 387 \t--- Loss: 0.007\n",
      "Iteration: 388 \t--- Loss: 0.007\n",
      "Iteration: 389 \t--- Loss: 0.007\n",
      "Iteration: 390 \t--- Loss: 0.007\n",
      "Iteration: 391 \t--- Loss: 0.007\n",
      "Iteration: 392 \t--- Loss: 0.007\n",
      "Iteration: 393 \t--- Loss: 0.007\n",
      "Iteration: 394 \t--- Loss: 0.007\n",
      "Iteration: 395 \t--- Loss: 0.007\n",
      "Iteration: 396 \t--- Loss: 0.007\n",
      "Iteration: 397 \t--- Loss: 0.007\n",
      "Iteration: 398 \t--- Loss: 0.007\n",
      "Iteration: 399 \t--- Loss: 0.007\n",
      "Iteration: 400 \t--- Loss: 0.007\n",
      "Iteration: 401 \t--- Loss: 0.007\n",
      "Iteration: 402 \t--- Loss: 0.007\n",
      "Iteration: 403 \t--- Loss: 0.007\n",
      "Iteration: 404 \t--- Loss: 0.007\n",
      "Iteration: 405 \t--- Loss: 0.007\n",
      "Iteration: 406 \t--- Loss: 0.007\n",
      "Iteration: 407 \t--- Loss: 0.007\n",
      "Iteration: 408 \t--- Loss: 0.007\n",
      "Iteration: 409 \t--- Loss: 0.007\n",
      "Iteration: 410 \t--- Loss: 0.007\n",
      "Iteration: 411 \t--- Loss: 0.007\n",
      "Iteration: 412 \t--- Loss: 0.007\n",
      "Iteration: 413 \t--- Loss: 0.007\n",
      "Iteration: 414 \t--- Loss: 0.008\n",
      "Iteration: 415 \t--- Loss: 0.008\n",
      "Iteration: 416 \t--- Loss: 0.007\n",
      "Iteration: 417 \t--- Loss: 0.007\n",
      "Iteration: 418 \t--- Loss: 0.007\n",
      "Iteration: 419 \t--- Loss: 0.007\n",
      "Iteration: 420 \t--- Loss: 0.007\n",
      "Iteration: 421 \t--- Loss: 0.007\n",
      "Iteration: 422 \t--- Loss: 0.007\n",
      "Iteration: 423 \t--- Loss: 0.007\n",
      "Iteration: 424 \t--- Loss: 0.007\n",
      "Iteration: 425 \t--- Loss: 0.007\n",
      "Iteration: 426 \t--- Loss: 0.007\n",
      "Iteration: 427 \t--- Loss: 0.007\n",
      "Iteration: 428 \t--- Loss: 0.007\n",
      "Iteration: 429 \t--- Loss: 0.007\n",
      "Iteration: 430 \t--- Loss: 0.007\n",
      "Iteration: 431 \t--- Loss: 0.007\n",
      "Iteration: 432 \t--- Loss: 0.007\n",
      "Iteration: 433 \t--- Loss: 0.007\n",
      "Iteration: 434 \t--- Loss: 0.007\n",
      "Iteration: 435 \t--- Loss: 0.007\n",
      "Iteration: 436 \t--- Loss: 0.007\n",
      "Iteration: 437 \t--- Loss: 0.007\n",
      "Iteration: 438 \t--- Loss: 0.007\n",
      "Iteration: 439 \t--- Loss: 0.007\n",
      "Iteration: 440 \t--- Loss: 0.007\n",
      "Iteration: 441 \t--- Loss: 0.007\n",
      "Iteration: 442 \t--- Loss: 0.008\n",
      "Iteration: 443 \t--- Loss: 0.007\n",
      "Iteration: 444 \t--- Loss: 0.007\n",
      "Iteration: 445 \t--- Loss: 0.007\n",
      "Iteration: 446 \t--- Loss: 0.007\n",
      "Iteration: 447 \t--- Loss: 0.007\n",
      "Iteration: 448 \t--- Loss: 0.007\n",
      "Iteration: 449 \t--- Loss: 0.007\n",
      "Iteration: 450 \t--- Loss: 0.007\n",
      "Iteration: 451 \t--- Loss: 0.007\n",
      "Iteration: 452 \t--- Loss: 0.007\n",
      "Iteration: 453 \t--- Loss: 0.007\n",
      "Iteration: 454 \t--- Loss: 0.007\n",
      "Iteration: 455 \t--- Loss: 0.007\n",
      "Iteration: 456 \t--- Loss: 0.007\n",
      "Iteration: 457 \t--- Loss: 0.007\n",
      "Iteration: 458 \t--- Loss: 0.007\n",
      "Iteration: 459 \t--- Loss: 0.007\n",
      "Iteration: 460 \t--- Loss: 0.007\n",
      "Iteration: 461 \t--- Loss: 0.007\n",
      "Iteration: 462 \t--- Loss: 0.007\n",
      "Iteration: 463 \t--- Loss: 0.007\n",
      "Iteration: 464 \t--- Loss: 0.007\n",
      "Iteration: 465 \t--- Loss: 0.007\n",
      "Iteration: 466 \t--- Loss: 0.007\n",
      "Iteration: 467 \t--- Loss: 0.007\n",
      "Iteration: 468 \t--- Loss: 0.007\n",
      "Iteration: 469 \t--- Loss: 0.007\n",
      "Iteration: 470 \t--- Loss: 0.007\n",
      "Iteration: 471 \t--- Loss: 0.007\n",
      "Iteration: 472 \t--- Loss: 0.007\n",
      "Iteration: 473 \t--- Loss: 0.007\n",
      "Iteration: 474 \t--- Loss: 0.007\n",
      "Iteration: 475 \t--- Loss: 0.007\n",
      "Iteration: 476 \t--- Loss: 0.007\n",
      "Iteration: 477 \t--- Loss: 0.007\n",
      "Iteration: 478 \t--- Loss: 0.007\n",
      "Iteration: 479 \t--- Loss: 0.007\n",
      "Iteration: 480 \t--- Loss: 0.007\n",
      "Iteration: 481 \t--- Loss: 0.007\n",
      "Iteration: 482 \t--- Loss: 0.007\n",
      "Iteration: 483 \t--- Loss: 0.007\n",
      "Iteration: 484 \t--- Loss: 0.007\n",
      "Iteration: 485 \t--- Loss: 0.007\n",
      "Iteration: 486 \t--- Loss: 0.007\n",
      "Iteration: 487 \t--- Loss: 0.007\n",
      "Iteration: 488 \t--- Loss: 0.007\n",
      "Iteration: 489 \t--- Loss: 0.007\n",
      "Iteration: 490 \t--- Loss: 0.007\n",
      "Iteration: 491 \t--- Loss: 0.007\n",
      "Iteration: 492 \t--- Loss: 0.007\n",
      "Iteration: 493 \t--- Loss: 0.007\n",
      "Iteration: 494 \t--- Loss: 0.007\n",
      "Iteration: 495 \t--- Loss: 0.007\n",
      "Iteration: 496 \t--- Loss: 0.007\n",
      "Iteration: 497 \t--- Loss: 0.007\n",
      "Iteration: 498 \t--- Loss: 0.007\n",
      "Iteration: 499 \t--- Loss: 0.007\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:10,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.534\n",
      "Iteration: 1 \t--- Loss: 0.540\n",
      "Iteration: 2 \t--- Loss: 0.478\n",
      "Iteration: 3 \t--- Loss: 0.485\n",
      "Iteration: 4 \t--- Loss: 0.460\n",
      "Iteration: 5 \t--- Loss: 0.459\n",
      "Iteration: 6 \t--- Loss: 0.430\n",
      "Iteration: 7 \t--- Loss: 0.439\n",
      "Iteration: 8 \t--- Loss: 0.415\n",
      "Iteration: 9 \t--- Loss: 0.447\n",
      "Iteration: 10 \t--- Loss: 0.386\n",
      "Iteration: 11 \t--- Loss: 0.400\n",
      "Iteration: 12 \t--- Loss: 0.402\n",
      "Iteration: 13 \t--- Loss: 0.377\n",
      "Iteration: 14 \t--- Loss: 0.383\n",
      "Iteration: 15 \t--- Loss: 0.372\n",
      "Iteration: 16 \t--- Loss: 0.392\n",
      "Iteration: 17 \t--- Loss: 0.352\n",
      "Iteration: 18 \t--- Loss: 0.367\n",
      "Iteration: 19 \t--- Loss: 0.345\n",
      "Iteration: 20 \t--- Loss: 0.326\n",
      "Iteration: 21 \t--- Loss: 0.325\n",
      "Iteration: 22 \t--- Loss: 0.313\n",
      "Iteration: 23 \t--- Loss: 0.299\n",
      "Iteration: 24 \t--- Loss: 0.316\n",
      "Iteration: 25 \t--- Loss: 0.296\n",
      "Iteration: 26 \t--- Loss: 0.293\n",
      "Iteration: 27 \t--- Loss: 0.274\n",
      "Iteration: 28 \t--- Loss: 0.252\n",
      "Iteration: 29 \t--- Loss: 0.255\n",
      "Iteration: 30 \t--- Loss: 0.251\n",
      "Iteration: 31 \t--- Loss: 0.234\n",
      "Iteration: 32 \t--- Loss: 0.246\n",
      "Iteration: 33 \t--- Loss: 0.350\n",
      "Iteration: 34 \t--- Loss: 0.759\n",
      "Iteration: 35 \t--- Loss: 0.750\n",
      "Iteration: 36 \t--- Loss: 0.733\n",
      "Iteration: 37 \t--- Loss: 0.713\n",
      "Iteration: 38 \t--- Loss: 0.681\n",
      "Iteration: 39 \t--- Loss: 0.668\n",
      "Iteration: 40 \t--- Loss: 0.651\n",
      "Iteration: 41 \t--- Loss: 0.623\n",
      "Iteration: 42 \t--- Loss: 0.603\n",
      "Iteration: 43 \t--- Loss: 0.575\n",
      "Iteration: 44 \t--- Loss: 0.551\n",
      "Iteration: 45 \t--- Loss: 0.516\n",
      "Iteration: 46 \t--- Loss: 0.501\n",
      "Iteration: 47 \t--- Loss: 0.473\n",
      "Iteration: 48 \t--- Loss: 0.469\n",
      "Iteration: 49 \t--- Loss: 0.444\n",
      "Iteration: 50 \t--- Loss: 0.443\n",
      "Iteration: 51 \t--- Loss: 0.424\n",
      "Iteration: 52 \t--- Loss: 0.406\n",
      "Iteration: 53 \t--- Loss: 0.404\n",
      "Iteration: 54 \t--- Loss: 0.390\n",
      "Iteration: 55 \t--- Loss: 0.380\n",
      "Iteration: 56 \t--- Loss: 0.354\n",
      "Iteration: 57 \t--- Loss: 0.370\n",
      "Iteration: 58 \t--- Loss: 0.341\n",
      "Iteration: 59 \t--- Loss: 0.339\n",
      "Iteration: 60 \t--- Loss: 0.325\n",
      "Iteration: 61 \t--- Loss: 0.326\n",
      "Iteration: 62 \t--- Loss: 0.317\n",
      "Iteration: 63 \t--- Loss: 0.301\n",
      "Iteration: 64 \t--- Loss: 0.294\n",
      "Iteration: 65 \t--- Loss: 0.301\n",
      "Iteration: 66 \t--- Loss: 0.306\n",
      "Iteration: 67 \t--- Loss: 0.274\n",
      "Iteration: 68 \t--- Loss: 0.285\n",
      "Iteration: 69 \t--- Loss: 0.267\n",
      "Iteration: 70 \t--- Loss: 0.260\n",
      "Iteration: 71 \t--- Loss: 0.238\n",
      "Iteration: 72 \t--- Loss: 0.258\n",
      "Iteration: 73 \t--- Loss: 0.261\n",
      "Iteration: 74 \t--- Loss: 0.254\n",
      "Iteration: 75 \t--- Loss: 0.245\n",
      "Iteration: 76 \t--- Loss: 0.231\n",
      "Iteration: 77 \t--- Loss: 0.209\n",
      "Iteration: 78 \t--- Loss: 0.211\n",
      "Iteration: 79 \t--- Loss: 0.208\n",
      "Iteration: 80 \t--- Loss: 0.200\n",
      "Iteration: 81 \t--- Loss: 0.193\n",
      "Iteration: 82 \t--- Loss: 0.205\n",
      "Iteration: 83 \t--- Loss: 0.185\n",
      "Iteration: 84 \t--- Loss: 0.186\n",
      "Iteration: 85 \t--- Loss: 0.175\n",
      "Iteration: 86 \t--- Loss: 0.173\n",
      "Iteration: 87 \t--- Loss: 0.188\n",
      "Iteration: 88 \t--- Loss: 0.185\n",
      "Iteration: 89 \t--- Loss: 0.172\n",
      "Iteration: 90 \t--- Loss: 0.155\n",
      "Iteration: 91 \t--- Loss: 0.179\n",
      "Iteration: 92 \t--- Loss: 0.177\n",
      "Iteration: 93 \t--- Loss: 0.161\n",
      "Iteration: 94 \t--- Loss: 0.156\n",
      "Iteration: 95 \t--- Loss: 0.160\n",
      "Iteration: 96 \t--- Loss: 0.147\n",
      "Iteration: 97 \t--- Loss: 0.162\n",
      "Iteration: 98 \t--- Loss: 0.157\n",
      "Iteration: 99 \t--- Loss: 0.150\n",
      "Iteration: 100 \t--- Loss: 0.162\n",
      "Iteration: 101 \t--- Loss: 0.164\n",
      "Iteration: 102 \t--- Loss: 0.154\n",
      "Iteration: 103 \t--- Loss: 0.155\n",
      "Iteration: 104 \t--- Loss: 0.142\n",
      "Iteration: 105 \t--- Loss: 0.151\n",
      "Iteration: 106 \t--- Loss: 0.160\n",
      "Iteration: 107 \t--- Loss: 0.147\n",
      "Iteration: 108 \t--- Loss: 0.152\n",
      "Iteration: 109 \t--- Loss: 0.148\n",
      "Iteration: 110 \t--- Loss: 0.132\n",
      "Iteration: 111 \t--- Loss: 0.153\n",
      "Iteration: 112 \t--- Loss: 0.150\n",
      "Iteration: 113 \t--- Loss: 0.149\n",
      "Iteration: 114 \t--- Loss: 0.139\n",
      "Iteration: 115 \t--- Loss: 0.137\n",
      "Iteration: 116 \t--- Loss: 0.143\n",
      "Iteration: 117 \t--- Loss: 0.125\n",
      "Iteration: 118 \t--- Loss: 0.133\n",
      "Iteration: 119 \t--- Loss: 0.132\n",
      "Iteration: 120 \t--- Loss: 0.133\n",
      "Iteration: 121 \t--- Loss: 0.141\n",
      "Iteration: 122 \t--- Loss: 0.129\n",
      "Iteration: 123 \t--- Loss: 0.134\n",
      "Iteration: 124 \t--- Loss: 0.128\n",
      "Iteration: 125 \t--- Loss: 0.125\n",
      "Iteration: 126 \t--- Loss: 0.130\n",
      "Iteration: 127 \t--- Loss: 0.126\n",
      "Iteration: 128 \t--- Loss: 0.125\n",
      "Iteration: 129 \t--- Loss: 0.124\n",
      "Iteration: 130 \t--- Loss: 0.104\n",
      "Iteration: 131 \t--- Loss: 0.118\n",
      "Iteration: 132 \t--- Loss: 0.129\n",
      "Iteration: 133 \t--- Loss: 0.135\n",
      "Iteration: 134 \t--- Loss: 0.122\n",
      "Iteration: 135 \t--- Loss: 0.125\n",
      "Iteration: 136 \t--- Loss: 0.126\n",
      "Iteration: 137 \t--- Loss: 0.128\n",
      "Iteration: 138 \t--- Loss: 0.125\n",
      "Iteration: 139 \t--- Loss: 0.114\n",
      "Iteration: 140 \t--- Loss: 0.113\n",
      "Iteration: 141 \t--- Loss: 0.118\n",
      "Iteration: 142 \t--- Loss: 0.114\n",
      "Iteration: 143 \t--- Loss: 0.122\n",
      "Iteration: 144 \t--- Loss: 0.107\n",
      "Iteration: 145 \t--- Loss: 0.114\n",
      "Iteration: 146 \t--- Loss: 0.105\n",
      "Iteration: 147 \t--- Loss: 0.112\n",
      "Iteration: 148 \t--- Loss: 0.117\n",
      "Iteration: 149 \t--- Loss: 0.116\n",
      "Iteration: 150 \t--- Loss: 0.114\n",
      "Iteration: 151 \t--- Loss: 0.121\n",
      "Iteration: 152 \t--- Loss: 0.111\n",
      "Iteration: 153 \t--- Loss: 0.104\n",
      "Iteration: 154 \t--- Loss: 0.118\n",
      "Iteration: 155 \t--- Loss: 0.111\n",
      "Iteration: 156 \t--- Loss: 0.116\n",
      "Iteration: 157 \t--- Loss: 0.106\n",
      "Iteration: 158 \t--- Loss: 0.116\n",
      "Iteration: 159 \t--- Loss: 0.096\n",
      "Iteration: 160 \t--- Loss: 0.127\n",
      "Iteration: 161 \t--- Loss: 0.121\n",
      "Iteration: 162 \t--- Loss: 0.117\n",
      "Iteration: 163 \t--- Loss: 0.121\n",
      "Iteration: 164 \t--- Loss: 0.124\n",
      "Iteration: 165 \t--- Loss: 0.125\n",
      "Iteration: 166 \t--- Loss: 0.147\n",
      "Iteration: 167 \t--- Loss: 0.115\n",
      "Iteration: 168 \t--- Loss: 0.127\n",
      "Iteration: 169 \t--- Loss: 0.155\n",
      "Iteration: 170 \t--- Loss: 0.210\n",
      "Iteration: 171 \t--- Loss: 0.209\n",
      "Iteration: 172 \t--- Loss: 0.183\n",
      "Iteration: 173 \t--- Loss: 0.124\n",
      "Iteration: 174 \t--- Loss: 0.109\n",
      "Iteration: 175 \t--- Loss: 0.100\n",
      "Iteration: 176 \t--- Loss: 0.093\n",
      "Iteration: 177 \t--- Loss: 0.095\n",
      "Iteration: 178 \t--- Loss: 0.092\n",
      "Iteration: 179 \t--- Loss: 0.083\n",
      "Iteration: 180 \t--- Loss: 0.089\n",
      "Iteration: 181 \t--- Loss: 0.090\n",
      "Iteration: 182 \t--- Loss: 0.091\n",
      "Iteration: 183 \t--- Loss: 0.091\n",
      "Iteration: 184 \t--- Loss: 0.092\n",
      "Iteration: 185 \t--- Loss: 0.105\n",
      "Iteration: 186 \t--- Loss: 0.121\n",
      "Iteration: 187 \t--- Loss: 0.169\n",
      "Iteration: 188 \t--- Loss: 0.089\n",
      "Iteration: 189 \t--- Loss: 0.099\n",
      "Iteration: 190 \t--- Loss: 0.111\n",
      "Iteration: 191 \t--- Loss: 0.215\n",
      "Iteration: 192 \t--- Loss: 0.499\n",
      "Iteration: 193 \t--- Loss: 0.492\n",
      "Iteration: 194 \t--- Loss: 0.481\n",
      "Iteration: 195 \t--- Loss: 0.473\n",
      "Iteration: 196 \t--- Loss: 0.433\n",
      "Iteration: 197 \t--- Loss: 0.426\n",
      "Iteration: 198 \t--- Loss: 0.412\n",
      "Iteration: 199 \t--- Loss: 0.408\n",
      "Iteration: 200 \t--- Loss: 0.396\n",
      "Iteration: 201 \t--- Loss: 0.371\n",
      "Iteration: 202 \t--- Loss: 0.382\n",
      "Iteration: 203 \t--- Loss: 0.359\n",
      "Iteration: 204 \t--- Loss: 0.341\n",
      "Iteration: 205 \t--- Loss: 0.335\n",
      "Iteration: 206 \t--- Loss: 0.308\n",
      "Iteration: 207 \t--- Loss: 0.311\n",
      "Iteration: 208 \t--- Loss: 0.293\n",
      "Iteration: 209 \t--- Loss: 0.285\n",
      "Iteration: 210 \t--- Loss: 0.280\n",
      "Iteration: 211 \t--- Loss: 0.248\n",
      "Iteration: 212 \t--- Loss: 0.260\n",
      "Iteration: 213 \t--- Loss: 0.243\n",
      "Iteration: 214 \t--- Loss: 0.238\n",
      "Iteration: 215 \t--- Loss: 0.214\n",
      "Iteration: 216 \t--- Loss: 0.222\n",
      "Iteration: 217 \t--- Loss: 0.222\n",
      "Iteration: 218 \t--- Loss: 0.207\n",
      "Iteration: 219 \t--- Loss: 0.197\n",
      "Iteration: 220 \t--- Loss: 0.184\n",
      "Iteration: 221 \t--- Loss: 0.188\n",
      "Iteration: 222 \t--- Loss: 0.175\n",
      "Iteration: 223 \t--- Loss: 0.179\n",
      "Iteration: 224 \t--- Loss: 0.172\n",
      "Iteration: 225 \t--- Loss: 0.163\n",
      "Iteration: 226 \t--- Loss: 0.150\n",
      "Iteration: 227 \t--- Loss: 0.141\n",
      "Iteration: 228 \t--- Loss: 0.143\n",
      "Iteration: 229 \t--- Loss: 0.138\n",
      "Iteration: 230 \t--- Loss: 0.137\n",
      "Iteration: 231 \t--- Loss: 0.137\n",
      "Iteration: 232 \t--- Loss: 0.125\n",
      "Iteration: 233 \t--- Loss: 0.120\n",
      "Iteration: 234 \t--- Loss: 0.113\n",
      "Iteration: 235 \t--- Loss: 0.110\n",
      "Iteration: 236 \t--- Loss: 0.102\n",
      "Iteration: 237 \t--- Loss: 0.102\n",
      "Iteration: 238 \t--- Loss: 0.095\n",
      "Iteration: 239 \t--- Loss: 0.093\n",
      "Iteration: 240 \t--- Loss: 0.089\n",
      "Iteration: 241 \t--- Loss: 0.086\n",
      "Iteration: 242 \t--- Loss: 0.081\n",
      "Iteration: 243 \t--- Loss: 0.078\n",
      "Iteration: 244 \t--- Loss: 0.074\n",
      "Iteration: 245 \t--- Loss: 0.072\n",
      "Iteration: 246 \t--- Loss: 0.069\n",
      "Iteration: 247 \t--- Loss: 0.064\n",
      "Iteration: 248 \t--- Loss: 0.061\n",
      "Iteration: 249 \t--- Loss: 0.058\n",
      "Iteration: 250 \t--- Loss: 0.056\n",
      "Iteration: 251 \t--- Loss: 0.052\n",
      "Iteration: 252 \t--- Loss: 0.050\n",
      "Iteration: 253 \t--- Loss: 0.051\n",
      "Iteration: 254 \t--- Loss: 0.047\n",
      "Iteration: 255 \t--- Loss: 0.043\n",
      "Iteration: 256 \t--- Loss: 0.041\n",
      "Iteration: 257 \t--- Loss: 0.042\n",
      "Iteration: 258 \t--- Loss: 0.039\n",
      "Iteration: 259 \t--- Loss: 0.036"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:10,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.516\n",
      "Iteration: 1 \t--- Loss: 0.504\n",
      "Iteration: 2 \t--- Loss: 0.485\n",
      "Iteration: 3 \t--- Loss: 0.483\n",
      "Iteration: 4 \t--- Loss: 0.438\n",
      "Iteration: 5 \t--- Loss: 0.445\n",
      "Iteration: 6 \t--- Loss: 0.423\n",
      "Iteration: 7 \t--- Loss: 0.408\n",
      "Iteration: 8 \t--- Loss: 0.411\n",
      "Iteration: 9 \t--- Loss: 0.392\n",
      "Iteration: 10 \t--- Loss: 0.390\n",
      "Iteration: 11 \t--- Loss: 0.387\n",
      "Iteration: 12 \t--- Loss: 0.373\n",
      "Iteration: 13 \t--- Loss: 0.367\n",
      "Iteration: 14 \t--- Loss: 0.351\n",
      "Iteration: 15 \t--- Loss: 0.379\n",
      "Iteration: 16 \t--- Loss: 0.361\n",
      "Iteration: 17 \t--- Loss: 0.306\n",
      "Iteration: 18 \t--- Loss: 0.345\n",
      "Iteration: 19 \t--- Loss: 0.324\n",
      "Iteration: 20 \t--- Loss: 0.319\n",
      "Iteration: 21 \t--- Loss: 0.301\n",
      "Iteration: 22 \t--- Loss: 0.314\n",
      "Iteration: 23 \t--- Loss: 0.317\n",
      "Iteration: 24 \t--- Loss: 0.302\n",
      "Iteration: 25 \t--- Loss: 0.284\n",
      "Iteration: 26 \t--- Loss: 0.272\n",
      "Iteration: 27 \t--- Loss: 0.264\n",
      "Iteration: 28 \t--- Loss: 0.257\n",
      "Iteration: 29 \t--- Loss: 0.245\n",
      "Iteration: 30 \t--- Loss: 0.249\n",
      "Iteration: 31 \t--- Loss: 0.230\n",
      "Iteration: 32 \t--- Loss: 0.248\n",
      "Iteration: 33 \t--- Loss: 0.361\n",
      "Iteration: 34 \t--- Loss: 0.719\n",
      "Iteration: 35 \t--- Loss: 0.701\n",
      "Iteration: 36 \t--- Loss: 0.682\n",
      "Iteration: 37 \t--- Loss: 0.663\n",
      "Iteration: 38 \t--- Loss: 0.643\n",
      "Iteration: 39 \t--- Loss: 0.621\n",
      "Iteration: 40 \t--- Loss: 0.596\n",
      "Iteration: 41 \t--- Loss: 0.569\n",
      "Iteration: 42 \t--- Loss: 0.545\n",
      "Iteration: 43 \t--- Loss: 0.524\n",
      "Iteration: 44 \t--- Loss: 0.495\n",
      "Iteration: 45 \t--- Loss: 0.481\n",
      "Iteration: 46 \t--- Loss: 0.459\n",
      "Iteration: 47 \t--- Loss: 0.440\n",
      "Iteration: 48 \t--- Loss: 0.445\n",
      "Iteration: 49 \t--- Loss: 0.421\n",
      "Iteration: 50 \t--- Loss: 0.406\n",
      "Iteration: 51 \t--- Loss: 0.384\n",
      "Iteration: 52 \t--- Loss: 0.370\n",
      "Iteration: 53 \t--- Loss: 0.354\n",
      "Iteration: 54 \t--- Loss: 0.368\n",
      "Iteration: 55 \t--- Loss: 0.356\n",
      "Iteration: 56 \t--- Loss: 0.355\n",
      "Iteration: 57 \t--- Loss: 0.326\n",
      "Iteration: 58 \t--- Loss: 0.322\n",
      "Iteration: 59 \t--- Loss: 0.303\n",
      "Iteration: 60 \t--- Loss: 0.316\n",
      "Iteration: 61 \t--- Loss: 0.297\n",
      "Iteration: 62 \t--- Loss: 0.297\n",
      "Iteration: 63 \t--- Loss: 0.292\n",
      "Iteration: 64 \t--- Loss: 0.304\n",
      "Iteration: 65 \t--- Loss: 0.275\n",
      "Iteration: 66 \t--- Loss: 0.242\n",
      "Iteration: 67 \t--- Loss: 0.266\n",
      "Iteration: 68 \t--- Loss: 0.235\n",
      "Iteration: 69 \t--- Loss: 0.238\n",
      "Iteration: 70 \t--- Loss: 0.238\n",
      "Iteration: 71 \t--- Loss: 0.245\n",
      "Iteration: 72 \t--- Loss: 0.229\n",
      "Iteration: 73 \t--- Loss: 0.231\n",
      "Iteration: 74 \t--- Loss: 0.212\n",
      "Iteration: 75 \t--- Loss: 0.201\n",
      "Iteration: 76 \t--- Loss: 0.199\n",
      "Iteration: 77 \t--- Loss: 0.193\n",
      "Iteration: 78 \t--- Loss: 0.186\n",
      "Iteration: 79 \t--- Loss: 0.186\n",
      "Iteration: 80 \t--- Loss: 0.174\n",
      "Iteration: 81 \t--- Loss: 0.177\n",
      "Iteration: 82 \t--- Loss: 0.185\n",
      "Iteration: 83 \t--- Loss: 0.157\n",
      "Iteration: 84 \t--- Loss: 0.153\n",
      "Iteration: 85 \t--- Loss: 0.165\n",
      "Iteration: 86 \t--- Loss: 0.160\n",
      "Iteration: 87 \t--- Loss: 0.160\n",
      "Iteration: 88 \t--- Loss: 0.161\n",
      "Iteration: 89 \t--- Loss: 0.159\n",
      "Iteration: 90 \t--- Loss: 0.157\n",
      "Iteration: 91 \t--- Loss: 0.149\n",
      "Iteration: 92 \t--- Loss: 0.153\n",
      "Iteration: 93 \t--- Loss: 0.167\n",
      "Iteration: 94 \t--- Loss: 0.155\n",
      "Iteration: 95 \t--- Loss: 0.145\n",
      "Iteration: 96 \t--- Loss: 0.148\n",
      "Iteration: 97 \t--- Loss: 0.132\n",
      "Iteration: 98 \t--- Loss: 0.152\n",
      "Iteration: 99 \t--- Loss: 0.137\n",
      "Iteration: 100 \t--- Loss: 0.132\n",
      "Iteration: 101 \t--- Loss: 0.147\n",
      "Iteration: 102 \t--- Loss: 0.135\n",
      "Iteration: 103 \t--- Loss: 0.153\n",
      "Iteration: 104 \t--- Loss: 0.136\n",
      "Iteration: 105 \t--- Loss: 0.140\n",
      "Iteration: 106 \t--- Loss: 0.147\n",
      "Iteration: 107 \t--- Loss: 0.142\n",
      "Iteration: 108 \t--- Loss: 0.131\n",
      "Iteration: 109 \t--- Loss: 0.123\n",
      "Iteration: 110 \t--- Loss: 0.142\n",
      "Iteration: 111 \t--- Loss: 0.132\n",
      "Iteration: 112 \t--- Loss: 0.129\n",
      "Iteration: 113 \t--- Loss: 0.131\n",
      "Iteration: 114 \t--- Loss: 0.136\n",
      "Iteration: 115 \t--- Loss: 0.146\n",
      "Iteration: 116 \t--- Loss: 0.233\n",
      "Iteration: 117 \t--- Loss: 0.294\n",
      "Iteration: 118 \t--- Loss: 0.300\n",
      "Iteration: 119 \t--- Loss: 0.304\n",
      "Iteration: 120 \t--- Loss: 0.294\n",
      "Iteration: 121 \t--- Loss: 0.255\n",
      "Iteration: 122 \t--- Loss: 0.262\n",
      "Iteration: 123 \t--- Loss: 0.243\n",
      "Iteration: 124 \t--- Loss: 0.254\n",
      "Iteration: 125 \t--- Loss: 0.233\n",
      "Iteration: 126 \t--- Loss: 0.214\n",
      "Iteration: 127 \t--- Loss: 0.235\n",
      "Iteration: 128 \t--- Loss: 0.214\n",
      "Iteration: 129 \t--- Loss: 0.196\n",
      "Iteration: 130 \t--- Loss: 0.178\n",
      "Iteration: 131 \t--- Loss: 0.181\n",
      "Iteration: 132 \t--- Loss: 0.181\n",
      "Iteration: 133 \t--- Loss: 0.151\n",
      "Iteration: 134 \t--- Loss: 0.154\n",
      "Iteration: 135 \t--- Loss: 0.121\n",
      "Iteration: 136 \t--- Loss: 0.124\n",
      "Iteration: 137 \t--- Loss: 0.112\n",
      "Iteration: 138 \t--- Loss: 0.115\n",
      "Iteration: 139 \t--- Loss: 0.099\n",
      "Iteration: 140 \t--- Loss: 0.100\n",
      "Iteration: 141 \t--- Loss: 0.112\n",
      "Iteration: 142 \t--- Loss: 0.101\n",
      "Iteration: 143 \t--- Loss: 0.098\n",
      "Iteration: 144 \t--- Loss: 0.101\n",
      "Iteration: 145 \t--- Loss: 0.093\n",
      "Iteration: 146 \t--- Loss: 0.101\n",
      "Iteration: 147 \t--- Loss: 0.099\n",
      "Iteration: 148 \t--- Loss: 0.097\n",
      "Iteration: 149 \t--- Loss: 0.090\n",
      "Iteration: 150 \t--- Loss: 0.112\n",
      "Iteration: 151 \t--- Loss: 0.088\n",
      "Iteration: 152 \t--- Loss: 0.101\n",
      "Iteration: 153 \t--- Loss: 0.097\n",
      "Iteration: 154 \t--- Loss: 0.098\n",
      "Iteration: 155 \t--- Loss: 0.101\n",
      "Iteration: 156 \t--- Loss: 0.100\n",
      "Iteration: 157 \t--- Loss: 0.094\n",
      "Iteration: 158 \t--- Loss: 0.098\n",
      "Iteration: 159 \t--- Loss: 0.097\n",
      "Iteration: 160 \t--- Loss: 0.092\n",
      "Iteration: 161 \t--- Loss: 0.094\n",
      "Iteration: 162 \t--- Loss: 0.100\n",
      "Iteration: 163 \t--- Loss: 0.091\n",
      "Iteration: 164 \t--- Loss: 0.096\n",
      "Iteration: 165 \t--- Loss: 0.098\n",
      "Iteration: 166 \t--- Loss: 0.092\n",
      "Iteration: 167 \t--- Loss: 0.093\n",
      "Iteration: 168 \t--- Loss: 0.086\n",
      "Iteration: 169 \t--- Loss: 0.095\n",
      "Iteration: 170 \t--- Loss: 0.095\n",
      "Iteration: 171 \t--- Loss: 0.088\n",
      "Iteration: 172 \t--- Loss: 0.089\n",
      "Iteration: 173 \t--- Loss: 0.090\n",
      "Iteration: 174 \t--- Loss: 0.077\n",
      "Iteration: 175 \t--- Loss: 0.099\n",
      "Iteration: 176 \t--- Loss: 0.086\n",
      "Iteration: 177 \t--- Loss: 0.083\n",
      "Iteration: 178 \t--- Loss: 0.088\n",
      "Iteration: 179 \t--- Loss: 0.083\n",
      "Iteration: 180 \t--- Loss: 0.095\n",
      "Iteration: 181 \t--- Loss: 0.084\n",
      "Iteration: 182 \t--- Loss: 0.091\n",
      "Iteration: 183 \t--- Loss: 0.086\n",
      "Iteration: 184 \t--- Loss: 0.080\n",
      "Iteration: 185 \t--- Loss: 0.087\n",
      "Iteration: 186 \t--- Loss: 0.083\n",
      "Iteration: 187 \t--- Loss: 0.080\n",
      "Iteration: 188 \t--- Loss: 0.089\n",
      "Iteration: 189 \t--- Loss: 0.085\n",
      "Iteration: 190 \t--- Loss: 0.078\n",
      "Iteration: 191 \t--- Loss: 0.079\n",
      "Iteration: 192 \t--- Loss: 0.089\n",
      "Iteration: 193 \t--- Loss: 0.087\n",
      "Iteration: 194 \t--- Loss: 0.080\n",
      "Iteration: 195 \t--- Loss: 0.089\n",
      "Iteration: 196 \t--- Loss: 0.086\n",
      "Iteration: 197 \t--- Loss: 0.084\n",
      "Iteration: 198 \t--- Loss: 0.083\n",
      "Iteration: 199 \t--- Loss: 0.091\n",
      "Iteration: 200 \t--- Loss: 0.084\n",
      "Iteration: 201 \t--- Loss: 0.083\n",
      "Iteration: 202 \t--- Loss: 0.086\n",
      "Iteration: 203 \t--- Loss: 0.085\n",
      "Iteration: 204 \t--- Loss: 0.086\n",
      "Iteration: 205 \t--- Loss: 0.091\n",
      "Iteration: 206 \t--- Loss: 0.083\n",
      "Iteration: 207 \t--- Loss: 0.093\n",
      "Iteration: 208 \t--- Loss: 0.152\n",
      "Iteration: 209 \t--- Loss: 0.210\n",
      "Iteration: 210 \t--- Loss: 0.163\n",
      "Iteration: 211 \t--- Loss: 0.183\n",
      "Iteration: 212 \t--- Loss: 0.166\n",
      "Iteration: 213 \t--- Loss: 0.144\n",
      "Iteration: 214 \t--- Loss: 0.117\n",
      "Iteration: 215 \t--- Loss: 0.099\n",
      "Iteration: 216 \t--- Loss: 0.085\n",
      "Iteration: 217 \t--- Loss: 0.076\n",
      "Iteration: 218 \t--- Loss: 0.068\n",
      "Iteration: 219 \t--- Loss: 0.073\n",
      "Iteration: 220 \t--- Loss: 0.073\n",
      "Iteration: 221 \t--- Loss: 0.074\n",
      "Iteration: 222 \t--- Loss: 0.069\n",
      "Iteration: 223 \t--- Loss: 0.075\n",
      "Iteration: 224 \t--- Loss: 0.068\n",
      "Iteration: 225 \t--- Loss: 0.072\n",
      "Iteration: 226 \t--- Loss: 0.074\n",
      "Iteration: 227 \t--- Loss: 0.067\n",
      "Iteration: 228 \t--- Loss: 0.076\n",
      "Iteration: 229 \t--- Loss: 0.074\n",
      "Iteration: 230 \t--- Loss: 0.077\n",
      "Iteration: 231 \t--- Loss: 0.083\n",
      "Iteration: 232 \t--- Loss: 0.086\n",
      "Iteration: 233 \t--- Loss: 0.090\n",
      "Iteration: 234 \t--- Loss: 0.158\n",
      "Iteration: 235 \t--- Loss: 0.244\n",
      "Iteration: 236 \t--- Loss: 0.249\n",
      "Iteration: 237 \t--- Loss: 0.240\n",
      "Iteration: 238 \t--- Loss: 0.235\n",
      "Iteration: 239 \t--- Loss: 0.236\n",
      "Iteration: 240 \t--- Loss: 0.236\n",
      "Iteration: 241 \t--- Loss: 0.217\n",
      "Iteration: 242 \t--- Loss: 0.205\n",
      "Iteration: 243 \t--- Loss: 0.202\n",
      "Iteration: 244 \t--- Loss: 0.198\n",
      "Iteration: 245 \t--- Loss: 0.201\n",
      "Iteration: 246 \t--- Loss: 0.197\n",
      "Iteration: 247 \t--- Loss: 0.201\n",
      "Iteration: 248 \t--- Loss: 0.196\n",
      "Iteration: 249 \t--- Loss: 0.191\n",
      "Iteration: 250 \t--- Loss: 0.185\n",
      "Iteration: 251 \t--- Loss: 0.178\n",
      "Iteration: 252 \t--- Loss: 0.175\n",
      "Iteration: 253 \t--- Loss: 0.170\n",
      "Iteration: 254 \t--- Loss: 0.161\n",
      "Iteration: 255 \t--- Loss: 0.158\n",
      "Iteration: 256 \t--- Loss: 0.153\n",
      "Iteration: 257 \t--- Loss: 0.150\n",
      "Iteration: 258 \t--- Loss: 0.146\n",
      "Iteration: 259 \t--- Loss: 0.150"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.50s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:22<00:00, 82.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.758\n",
      "Iteration: 261 \t--- Loss: 0.721\n",
      "Iteration: 262 \t--- Loss: 0.740\n",
      "Iteration: 263 \t--- Loss: 0.738\n",
      "Iteration: 264 \t--- Loss: 0.691\n",
      "Iteration: 265 \t--- Loss: 0.737\n",
      "Iteration: 266 \t--- Loss: 0.773\n",
      "Iteration: 267 \t--- Loss: 0.792\n",
      "Iteration: 268 \t--- Loss: 0.734\n",
      "Iteration: 269 \t--- Loss: 0.745\n",
      "Iteration: 270 \t--- Loss: 0.775\n",
      "Iteration: 271 \t--- Loss: 0.700\n",
      "Iteration: 272 \t--- Loss: 0.737\n",
      "Iteration: 273 \t--- Loss: 0.768\n",
      "Iteration: 274 \t--- Loss: 0.705\n",
      "Iteration: 275 \t--- Loss: 0.706\n",
      "Iteration: 276 \t--- Loss: 0.734\n",
      "Iteration: 277 \t--- Loss: 0.716\n",
      "Iteration: 278 \t--- Loss: 0.738\n",
      "Iteration: 279 \t--- Loss: 0.714\n",
      "Iteration: 280 \t--- Loss: 0.740\n",
      "Iteration: 281 \t--- Loss: 0.742\n",
      "Iteration: 282 \t--- Loss: 0.758\n",
      "Iteration: 283 \t--- Loss: 0.693\n",
      "Iteration: 284 \t--- Loss: 0.669\n",
      "Iteration: 285 \t--- Loss: 0.745\n",
      "Iteration: 286 \t--- Loss: 0.755\n",
      "Iteration: 287 \t--- Loss: 0.738\n",
      "Iteration: 288 \t--- Loss: 0.768\n",
      "Iteration: 289 \t--- Loss: 0.683\n",
      "Iteration: 290 \t--- Loss: 0.763\n",
      "Iteration: 291 \t--- Loss: 0.784\n",
      "Iteration: 292 \t--- Loss: 0.703\n",
      "Iteration: 293 \t--- Loss: 0.728\n",
      "Iteration: 294 \t--- Loss: 0.741\n",
      "Iteration: 295 \t--- Loss: 0.722\n",
      "Iteration: 296 \t--- Loss: 0.707\n",
      "Iteration: 297 \t--- Loss: 0.746\n",
      "Iteration: 298 \t--- Loss: 0.731\n",
      "Iteration: 299 \t--- Loss: 0.784\n",
      "Iteration: 300 \t--- Loss: 0.803\n",
      "Iteration: 301 \t--- Loss: 0.817\n",
      "Iteration: 302 \t--- Loss: 0.756\n",
      "Iteration: 303 \t--- Loss: 0.719\n",
      "Iteration: 304 \t--- Loss: 0.763\n",
      "Iteration: 305 \t--- Loss: 0.675\n",
      "Iteration: 306 \t--- Loss: 0.792\n",
      "Iteration: 307 \t--- Loss: 0.771\n",
      "Iteration: 308 \t--- Loss: 0.731\n",
      "Iteration: 309 \t--- Loss: 0.707\n",
      "Iteration: 310 \t--- Loss: 0.774\n",
      "Iteration: 311 \t--- Loss: 0.762\n",
      "Iteration: 312 \t--- Loss: 0.771\n",
      "Iteration: 313 \t--- Loss: 0.730\n",
      "Iteration: 314 \t--- Loss: 0.734\n",
      "Iteration: 315 \t--- Loss: 0.738\n",
      "Iteration: 316 \t--- Loss: 0.733\n",
      "Iteration: 317 \t--- Loss: 0.764\n",
      "Iteration: 318 \t--- Loss: 0.698\n",
      "Iteration: 319 \t--- Loss: 0.743\n",
      "Iteration: 320 \t--- Loss: 0.717\n",
      "Iteration: 321 \t--- Loss: 0.717\n",
      "Iteration: 322 \t--- Loss: 0.768\n",
      "Iteration: 323 \t--- Loss: 0.730\n",
      "Iteration: 324 \t--- Loss: 0.697\n",
      "Iteration: 325 \t--- Loss: 0.666\n",
      "Iteration: 326 \t--- Loss: 0.845\n",
      "Iteration: 327 \t--- Loss: 0.747\n",
      "Iteration: 328 \t--- Loss: 0.707\n",
      "Iteration: 329 \t--- Loss: 0.771\n",
      "Iteration: 330 \t--- Loss: 0.742\n",
      "Iteration: 331 \t--- Loss: 0.729\n",
      "Iteration: 332 \t--- Loss: 0.767\n",
      "Iteration: 333 \t--- Loss: 0.750\n",
      "Iteration: 334 \t--- Loss: 0.721\n",
      "Iteration: 335 \t--- Loss: 0.772\n",
      "Iteration: 336 \t--- Loss: 0.720\n",
      "Iteration: 337 \t--- Loss: 0.759\n",
      "Iteration: 338 \t--- Loss: 0.742\n",
      "Iteration: 339 \t--- Loss: 0.718\n",
      "Iteration: 340 \t--- Loss: 0.696\n",
      "Iteration: 341 \t--- Loss: 0.794\n",
      "Iteration: 342 \t--- Loss: 0.786\n",
      "Iteration: 343 \t--- Loss: 0.722\n",
      "Iteration: 344 \t--- Loss: 0.765\n",
      "Iteration: 345 \t--- Loss: 0.717\n",
      "Iteration: 346 \t--- Loss: 0.772\n",
      "Iteration: 347 \t--- Loss: 0.769\n",
      "Iteration: 348 \t--- Loss: 0.672\n",
      "Iteration: 349 \t--- Loss: 0.712\n",
      "Iteration: 350 \t--- Loss: 0.709\n",
      "Iteration: 351 \t--- Loss: 0.692\n",
      "Iteration: 352 \t--- Loss: 0.750\n",
      "Iteration: 353 \t--- Loss: 0.714\n",
      "Iteration: 354 \t--- Loss: 0.746\n",
      "Iteration: 355 \t--- Loss: 0.776\n",
      "Iteration: 356 \t--- Loss: 0.731\n",
      "Iteration: 357 \t--- Loss: 0.732\n",
      "Iteration: 358 \t--- Loss: 0.666\n",
      "Iteration: 359 \t--- Loss: 0.766\n",
      "Iteration: 360 \t--- Loss: 0.709\n",
      "Iteration: 361 \t--- Loss: 0.797\n",
      "Iteration: 362 \t--- Loss: 0.757\n",
      "Iteration: 363 \t--- Loss: 0.715\n",
      "Iteration: 364 \t--- Loss: 0.747\n",
      "Iteration: 365 \t--- Loss: 0.729\n",
      "Iteration: 366 \t--- Loss: 0.721\n",
      "Iteration: 367 \t--- Loss: 0.800\n",
      "Iteration: 368 \t--- Loss: 0.759\n",
      "Iteration: 369 \t--- Loss: 0.702\n",
      "Iteration: 370 \t--- Loss: 0.728\n",
      "Iteration: 371 \t--- Loss: 0.704\n",
      "Iteration: 372 \t--- Loss: 0.778\n",
      "Iteration: 373 \t--- Loss: 0.775\n",
      "Iteration: 374 \t--- Loss: 0.765\n",
      "Iteration: 375 \t--- Loss: 0.664\n",
      "Iteration: 376 \t--- Loss: 0.715\n",
      "Iteration: 377 \t--- Loss: 0.757\n",
      "Iteration: 378 \t--- Loss: 0.760\n",
      "Iteration: 379 \t--- Loss: 0.774\n",
      "Iteration: 380 \t--- Loss: 0.752\n",
      "Iteration: 381 \t--- Loss: 0.733\n",
      "Iteration: 382 \t--- Loss: 0.735\n",
      "Iteration: 383 \t--- Loss: 0.724\n",
      "Iteration: 384 \t--- Loss: 0.770\n",
      "Iteration: 385 \t--- Loss: 0.789\n",
      "Iteration: 386 \t--- Loss: 0.758\n",
      "Iteration: 387 \t--- Loss: 0.795\n",
      "Iteration: 388 \t--- Loss: 0.690\n",
      "Iteration: 389 \t--- Loss: 0.759\n",
      "Iteration: 390 \t--- Loss: 0.791\n",
      "Iteration: 391 \t--- Loss: 0.716\n",
      "Iteration: 392 \t--- Loss: 0.771\n",
      "Iteration: 393 \t--- Loss: 0.717\n",
      "Iteration: 394 \t--- Loss: 0.739\n",
      "Iteration: 395 \t--- Loss: 0.736\n",
      "Iteration: 396 \t--- Loss: 0.719\n",
      "Iteration: 397 \t--- Loss: 0.683\n",
      "Iteration: 398 \t--- Loss: 0.774\n",
      "Iteration: 399 \t--- Loss: 0.749\n",
      "Iteration: 400 \t--- Loss: 0.694\n",
      "Iteration: 401 \t--- Loss: 0.744\n",
      "Iteration: 402 \t--- Loss: 0.690\n",
      "Iteration: 403 \t--- Loss: 0.744\n",
      "Iteration: 404 \t--- Loss: 0.725\n",
      "Iteration: 405 \t--- Loss: 0.734\n",
      "Iteration: 406 \t--- Loss: 0.752\n",
      "Iteration: 407 \t--- Loss: 0.732\n",
      "Iteration: 408 \t--- Loss: 0.794\n",
      "Iteration: 409 \t--- Loss: 0.775\n",
      "Iteration: 410 \t--- Loss: 0.748\n",
      "Iteration: 411 \t--- Loss: 0.727\n",
      "Iteration: 412 \t--- Loss: 0.707\n",
      "Iteration: 413 \t--- Loss: 0.683\n",
      "Iteration: 414 \t--- Loss: 0.791\n",
      "Iteration: 415 \t--- Loss: 0.758\n",
      "Iteration: 416 \t--- Loss: 0.706\n",
      "Iteration: 417 \t--- Loss: 0.762\n",
      "Iteration: 418 \t--- Loss: 0.743\n",
      "Iteration: 419 \t--- Loss: 0.805\n",
      "Iteration: 420 \t--- Loss: 0.731\n",
      "Iteration: 421 \t--- Loss: 0.728\n",
      "Iteration: 422 \t--- Loss: 0.753\n",
      "Iteration: 423 \t--- Loss: 0.734\n",
      "Iteration: 424 \t--- Loss: 0.736\n",
      "Iteration: 425 \t--- Loss: 0.755\n",
      "Iteration: 426 \t--- Loss: 0.713\n",
      "Iteration: 427 \t--- Loss: 0.757\n",
      "Iteration: 428 \t--- Loss: 0.773\n",
      "Iteration: 429 \t--- Loss: 0.753\n",
      "Iteration: 430 \t--- Loss: 0.684\n",
      "Iteration: 431 \t--- Loss: 0.791\n",
      "Iteration: 432 \t--- Loss: 0.728\n",
      "Iteration: 433 \t--- Loss: 0.792\n",
      "Iteration: 434 \t--- Loss: 0.681\n",
      "Iteration: 435 \t--- Loss: 0.708\n",
      "Iteration: 436 \t--- Loss: 0.747\n",
      "Iteration: 437 \t--- Loss: 0.747\n",
      "Iteration: 438 \t--- Loss: 0.719\n",
      "Iteration: 439 \t--- Loss: 0.740\n",
      "Iteration: 440 \t--- Loss: 0.745\n",
      "Iteration: 441 \t--- Loss: 0.762\n",
      "Iteration: 442 \t--- Loss: 0.693\n",
      "Iteration: 443 \t--- Loss: 0.772\n",
      "Iteration: 444 \t--- Loss: 0.802\n",
      "Iteration: 445 \t--- Loss: 0.756\n",
      "Iteration: 446 \t--- Loss: 0.700\n",
      "Iteration: 447 \t--- Loss: 0.759\n",
      "Iteration: 448 \t--- Loss: 0.712\n",
      "Iteration: 449 \t--- Loss: 0.698\n",
      "Iteration: 450 \t--- Loss: 0.775\n",
      "Iteration: 451 \t--- Loss: 0.729\n",
      "Iteration: 452 \t--- Loss: 0.683\n",
      "Iteration: 453 \t--- Loss: 0.763\n",
      "Iteration: 454 \t--- Loss: 0.739\n",
      "Iteration: 455 \t--- Loss: 0.826\n",
      "Iteration: 456 \t--- Loss: 0.796\n",
      "Iteration: 457 \t--- Loss: 0.751\n",
      "Iteration: 458 \t--- Loss: 0.735\n",
      "Iteration: 459 \t--- Loss: 0.695\n",
      "Iteration: 460 \t--- Loss: 0.758\n",
      "Iteration: 461 \t--- Loss: 0.724\n",
      "Iteration: 462 \t--- Loss: 0.727\n",
      "Iteration: 463 \t--- Loss: 0.804\n",
      "Iteration: 464 \t--- Loss: 0.759\n",
      "Iteration: 465 \t--- Loss: 0.709\n",
      "Iteration: 466 \t--- Loss: 0.730\n",
      "Iteration: 467 \t--- Loss: 0.707\n",
      "Iteration: 468 \t--- Loss: 0.814\n",
      "Iteration: 469 \t--- Loss: 0.736\n",
      "Iteration: 470 \t--- Loss: 0.736\n",
      "Iteration: 471 \t--- Loss: 0.739\n",
      "Iteration: 472 \t--- Loss: 0.717\n",
      "Iteration: 473 \t--- Loss: 0.787\n",
      "Iteration: 474 \t--- Loss: 0.705\n",
      "Iteration: 475 \t--- Loss: 0.760\n",
      "Iteration: 476 \t--- Loss: 0.786\n",
      "Iteration: 477 \t--- Loss: 0.695\n",
      "Iteration: 478 \t--- Loss: 0.723\n",
      "Iteration: 479 \t--- Loss: 0.813\n",
      "Iteration: 480 \t--- Loss: 0.682\n",
      "Iteration: 481 \t--- Loss: 0.757\n",
      "Iteration: 482 \t--- Loss: 0.734\n",
      "Iteration: 483 \t--- Loss: 0.775\n",
      "Iteration: 484 \t--- Loss: 0.728\n",
      "Iteration: 485 \t--- Loss: 0.741\n",
      "Iteration: 486 \t--- Loss: 0.762\n",
      "Iteration: 487 \t--- Loss: 0.758\n",
      "Iteration: 488 \t--- Loss: 0.764\n",
      "Iteration: 489 \t--- Loss: 0.773\n",
      "Iteration: 490 \t--- Loss: 0.805\n",
      "Iteration: 491 \t--- Loss: 0.728\n",
      "Iteration: 492 \t--- Loss: 0.769\n",
      "Iteration: 493 \t--- Loss: 0.668\n",
      "Iteration: 494 \t--- Loss: 0.720\n",
      "Iteration: 495 \t--- Loss: 0.763\n",
      "Iteration: 496 \t--- Loss: 0.738\n",
      "Iteration: 497 \t--- Loss: 0.682\n",
      "Iteration: 498 \t--- Loss: 0.796\n",
      "Iteration: 499 \t--- Loss: 0.765\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.49s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:36<00:00, 156.96s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.036\n",
      "Iteration: 261 \t--- Loss: 0.034\n",
      "Iteration: 262 \t--- Loss: 0.035\n",
      "Iteration: 263 \t--- Loss: 0.033\n",
      "Iteration: 264 \t--- Loss: 0.034\n",
      "Iteration: 265 \t--- Loss: 0.032\n",
      "Iteration: 266 \t--- Loss: 0.030\n",
      "Iteration: 267 \t--- Loss: 0.031\n",
      "Iteration: 268 \t--- Loss: 0.028\n",
      "Iteration: 269 \t--- Loss: 0.029\n",
      "Iteration: 270 \t--- Loss: 0.029\n",
      "Iteration: 271 \t--- Loss: 0.029\n",
      "Iteration: 272 \t--- Loss: 0.027\n",
      "Iteration: 273 \t--- Loss: 0.027\n",
      "Iteration: 274 \t--- Loss: 0.026\n",
      "Iteration: 275 \t--- Loss: 0.026\n",
      "Iteration: 276 \t--- Loss: 0.023\n",
      "Iteration: 277 \t--- Loss: 0.027\n",
      "Iteration: 278 \t--- Loss: 0.024\n",
      "Iteration: 279 \t--- Loss: 0.024\n",
      "Iteration: 280 \t--- Loss: 0.023\n",
      "Iteration: 281 \t--- Loss: 0.024\n",
      "Iteration: 282 \t--- Loss: 0.021\n",
      "Iteration: 283 \t--- Loss: 0.022\n",
      "Iteration: 284 \t--- Loss: 0.022\n",
      "Iteration: 285 \t--- Loss: 0.020\n",
      "Iteration: 286 \t--- Loss: 0.023\n",
      "Iteration: 287 \t--- Loss: 0.022\n",
      "Iteration: 288 \t--- Loss: 0.019\n",
      "Iteration: 289 \t--- Loss: 0.021\n",
      "Iteration: 290 \t--- Loss: 0.018\n",
      "Iteration: 291 \t--- Loss: 0.019\n",
      "Iteration: 292 \t--- Loss: 0.019\n",
      "Iteration: 293 \t--- Loss: 0.019\n",
      "Iteration: 294 \t--- Loss: 0.020\n",
      "Iteration: 295 \t--- Loss: 0.017\n",
      "Iteration: 296 \t--- Loss: 0.018\n",
      "Iteration: 297 \t--- Loss: 0.019\n",
      "Iteration: 298 \t--- Loss: 0.018\n",
      "Iteration: 299 \t--- Loss: 0.018\n",
      "Iteration: 300 \t--- Loss: 0.018\n",
      "Iteration: 301 \t--- Loss: 0.017\n",
      "Iteration: 302 \t--- Loss: 0.015\n",
      "Iteration: 303 \t--- Loss: 0.015\n",
      "Iteration: 304 \t--- Loss: 0.016\n",
      "Iteration: 305 \t--- Loss: 0.016\n",
      "Iteration: 306 \t--- Loss: 0.016\n",
      "Iteration: 307 \t--- Loss: 0.013\n",
      "Iteration: 308 \t--- Loss: 0.015\n",
      "Iteration: 309 \t--- Loss: 0.016\n",
      "Iteration: 310 \t--- Loss: 0.015\n",
      "Iteration: 311 \t--- Loss: 0.015\n",
      "Iteration: 312 \t--- Loss: 0.014\n",
      "Iteration: 313 \t--- Loss: 0.013\n",
      "Iteration: 314 \t--- Loss: 0.013\n",
      "Iteration: 315 \t--- Loss: 0.012\n",
      "Iteration: 316 \t--- Loss: 0.014\n",
      "Iteration: 317 \t--- Loss: 0.013\n",
      "Iteration: 318 \t--- Loss: 0.014\n",
      "Iteration: 319 \t--- Loss: 0.013\n",
      "Iteration: 320 \t--- Loss: 0.013\n",
      "Iteration: 321 \t--- Loss: 0.012\n",
      "Iteration: 322 \t--- Loss: 0.013\n",
      "Iteration: 323 \t--- Loss: 0.012\n",
      "Iteration: 324 \t--- Loss: 0.012\n",
      "Iteration: 325 \t--- Loss: 0.013\n",
      "Iteration: 326 \t--- Loss: 0.013\n",
      "Iteration: 327 \t--- Loss: 0.012\n",
      "Iteration: 328 \t--- Loss: 0.011\n",
      "Iteration: 329 \t--- Loss: 0.011\n",
      "Iteration: 330 \t--- Loss: 0.011\n",
      "Iteration: 331 \t--- Loss: 0.012\n",
      "Iteration: 332 \t--- Loss: 0.012\n",
      "Iteration: 333 \t--- Loss: 0.011\n",
      "Iteration: 334 \t--- Loss: 0.011\n",
      "Iteration: 335 \t--- Loss: 0.011\n",
      "Iteration: 336 \t--- Loss: 0.010\n",
      "Iteration: 337 \t--- Loss: 0.011\n",
      "Iteration: 338 \t--- Loss: 0.011\n",
      "Iteration: 339 \t--- Loss: 0.011\n",
      "Iteration: 340 \t--- Loss: 0.011\n",
      "Iteration: 341 \t--- Loss: 0.010\n",
      "Iteration: 342 \t--- Loss: 0.010\n",
      "Iteration: 343 \t--- Loss: 0.010\n",
      "Iteration: 344 \t--- Loss: 0.009\n",
      "Iteration: 345 \t--- Loss: 0.009\n",
      "Iteration: 346 \t--- Loss: 0.010\n",
      "Iteration: 347 \t--- Loss: 0.010\n",
      "Iteration: 348 \t--- Loss: 0.009\n",
      "Iteration: 349 \t--- Loss: 0.010\n",
      "Iteration: 350 \t--- Loss: 0.010\n",
      "Iteration: 351 \t--- Loss: 0.010\n",
      "Iteration: 352 \t--- Loss: 0.009\n",
      "Iteration: 353 \t--- Loss: 0.009\n",
      "Iteration: 354 \t--- Loss: 0.009\n",
      "Iteration: 355 \t--- Loss: 0.008\n",
      "Iteration: 356 \t--- Loss: 0.009\n",
      "Iteration: 357 \t--- Loss: 0.008\n",
      "Iteration: 358 \t--- Loss: 0.008\n",
      "Iteration: 359 \t--- Loss: 0.009\n",
      "Iteration: 360 \t--- Loss: 0.009\n",
      "Iteration: 361 \t--- Loss: 0.008\n",
      "Iteration: 362 \t--- Loss: 0.008\n",
      "Iteration: 363 \t--- Loss: 0.008\n",
      "Iteration: 364 \t--- Loss: 0.008\n",
      "Iteration: 365 \t--- Loss: 0.008\n",
      "Iteration: 366 \t--- Loss: 0.008\n",
      "Iteration: 367 \t--- Loss: 0.007\n",
      "Iteration: 368 \t--- Loss: 0.008\n",
      "Iteration: 369 \t--- Loss: 0.007\n",
      "Iteration: 370 \t--- Loss: 0.007\n",
      "Iteration: 371 \t--- Loss: 0.008\n",
      "Iteration: 372 \t--- Loss: 0.007\n",
      "Iteration: 373 \t--- Loss: 0.007\n",
      "Iteration: 374 \t--- Loss: 0.007\n",
      "Iteration: 375 \t--- Loss: 0.007\n",
      "Iteration: 376 \t--- Loss: 0.007\n",
      "Iteration: 377 \t--- Loss: 0.007\n",
      "Iteration: 378 \t--- Loss: 0.007\n",
      "Iteration: 379 \t--- Loss: 0.007\n",
      "Iteration: 380 \t--- Loss: 0.007\n",
      "Iteration: 381 \t--- Loss: 0.007\n",
      "Iteration: 382 \t--- Loss: 0.007\n",
      "Iteration: 383 \t--- Loss: 0.007\n",
      "Iteration: 384 \t--- Loss: 0.007\n",
      "Iteration: 385 \t--- Loss: 0.007\n",
      "Iteration: 386 \t--- Loss: 0.006\n",
      "Iteration: 387 \t--- Loss: 0.006\n",
      "Iteration: 388 \t--- Loss: 0.007\n",
      "Iteration: 389 \t--- Loss: 0.006\n",
      "Iteration: 390 \t--- Loss: 0.006\n",
      "Iteration: 391 \t--- Loss: 0.007\n",
      "Iteration: 392 \t--- Loss: 0.006\n",
      "Iteration: 393 \t--- Loss: 0.007\n",
      "Iteration: 394 \t--- Loss: 0.006\n",
      "Iteration: 395 \t--- Loss: 0.006\n",
      "Iteration: 396 \t--- Loss: 0.006\n",
      "Iteration: 397 \t--- Loss: 0.006\n",
      "Iteration: 398 \t--- Loss: 0.006\n",
      "Iteration: 399 \t--- Loss: 0.006\n",
      "Iteration: 400 \t--- Loss: 0.006\n",
      "Iteration: 401 \t--- Loss: 0.006\n",
      "Iteration: 402 \t--- Loss: 0.006\n",
      "Iteration: 403 \t--- Loss: 0.006\n",
      "Iteration: 404 \t--- Loss: 0.006\n",
      "Iteration: 405 \t--- Loss: 0.005\n",
      "Iteration: 406 \t--- Loss: 0.006\n",
      "Iteration: 407 \t--- Loss: 0.005\n",
      "Iteration: 408 \t--- Loss: 0.005\n",
      "Iteration: 409 \t--- Loss: 0.006\n",
      "Iteration: 410 \t--- Loss: 0.006\n",
      "Iteration: 411 \t--- Loss: 0.005\n",
      "Iteration: 412 \t--- Loss: 0.005\n",
      "Iteration: 413 \t--- Loss: 0.006\n",
      "Iteration: 414 \t--- Loss: 0.006\n",
      "Iteration: 415 \t--- Loss: 0.006\n",
      "Iteration: 416 \t--- Loss: 0.005\n",
      "Iteration: 417 \t--- Loss: 0.005\n",
      "Iteration: 418 \t--- Loss: 0.005\n",
      "Iteration: 419 \t--- Loss: 0.005\n",
      "Iteration: 420 \t--- Loss: 0.005\n",
      "Iteration: 421 \t--- Loss: 0.005\n",
      "Iteration: 422 \t--- Loss: 0.005\n",
      "Iteration: 423 \t--- Loss: 0.005\n",
      "Iteration: 424 \t--- Loss: 0.005\n",
      "Iteration: 425 \t--- Loss: 0.005\n",
      "Iteration: 426 \t--- Loss: 0.005\n",
      "Iteration: 427 \t--- Loss: 0.005\n",
      "Iteration: 428 \t--- Loss: 0.005\n",
      "Iteration: 429 \t--- Loss: 0.005\n",
      "Iteration: 430 \t--- Loss: 0.005\n",
      "Iteration: 431 \t--- Loss: 0.005\n",
      "Iteration: 432 \t--- Loss: 0.005\n",
      "Iteration: 433 \t--- Loss: 0.005\n",
      "Iteration: 434 \t--- Loss: 0.005\n",
      "Iteration: 435 \t--- Loss: 0.005\n",
      "Iteration: 436 \t--- Loss: 0.005\n",
      "Iteration: 437 \t--- Loss: 0.005\n",
      "Iteration: 438 \t--- Loss: 0.005\n",
      "Iteration: 439 \t--- Loss: 0.005\n",
      "Iteration: 440 \t--- Loss: 0.004\n",
      "Iteration: 441 \t--- Loss: 0.005\n",
      "Iteration: 442 \t--- Loss: 0.005\n",
      "Iteration: 443 \t--- Loss: 0.004\n",
      "Iteration: 444 \t--- Loss: 0.004\n",
      "Iteration: 445 \t--- Loss: 0.004\n",
      "Iteration: 446 \t--- Loss: 0.005\n",
      "Iteration: 447 \t--- Loss: 0.004\n",
      "Iteration: 448 \t--- Loss: 0.004\n",
      "Iteration: 449 \t--- Loss: 0.005\n",
      "Iteration: 450 \t--- Loss: 0.004\n",
      "Iteration: 451 \t--- Loss: 0.004\n",
      "Iteration: 452 \t--- Loss: 0.004\n",
      "Iteration: 453 \t--- Loss: 0.004\n",
      "Iteration: 454 \t--- Loss: 0.004\n",
      "Iteration: 455 \t--- Loss: 0.004\n",
      "Iteration: 456 \t--- Loss: 0.004\n",
      "Iteration: 457 \t--- Loss: 0.004\n",
      "Iteration: 458 \t--- Loss: 0.004\n",
      "Iteration: 459 \t--- Loss: 0.004\n",
      "Iteration: 460 \t--- Loss: 0.004\n",
      "Iteration: 461 \t--- Loss: 0.004\n",
      "Iteration: 462 \t--- Loss: 0.004\n",
      "Iteration: 463 \t--- Loss: 0.004\n",
      "Iteration: 464 \t--- Loss: 0.004\n",
      "Iteration: 465 \t--- Loss: 0.004\n",
      "Iteration: 466 \t--- Loss: 0.004\n",
      "Iteration: 467 \t--- Loss: 0.004\n",
      "Iteration: 468 \t--- Loss: 0.004\n",
      "Iteration: 469 \t--- Loss: 0.004\n",
      "Iteration: 470 \t--- Loss: 0.004\n",
      "Iteration: 471 \t--- Loss: 0.004\n",
      "Iteration: 472 \t--- Loss: 0.004\n",
      "Iteration: 473 \t--- Loss: 0.004\n",
      "Iteration: 474 \t--- Loss: 0.004\n",
      "Iteration: 475 \t--- Loss: 0.004\n",
      "Iteration: 476 \t--- Loss: 0.004\n",
      "Iteration: 477 \t--- Loss: 0.004\n",
      "Iteration: 478 \t--- Loss: 0.004\n",
      "Iteration: 479 \t--- Loss: 0.004\n",
      "Iteration: 480 \t--- Loss: 0.004\n",
      "Iteration: 481 \t--- Loss: 0.004\n",
      "Iteration: 482 \t--- Loss: 0.003\n",
      "Iteration: 483 \t--- Loss: 0.004\n",
      "Iteration: 484 \t--- Loss: 0.003\n",
      "Iteration: 485 \t--- Loss: 0.004\n",
      "Iteration: 486 \t--- Loss: 0.004\n",
      "Iteration: 487 \t--- Loss: 0.004\n",
      "Iteration: 488 \t--- Loss: 0.003\n",
      "Iteration: 489 \t--- Loss: 0.004\n",
      "Iteration: 490 \t--- Loss: 0.003\n",
      "Iteration: 491 \t--- Loss: 0.004\n",
      "Iteration: 492 \t--- Loss: 0.004\n",
      "Iteration: 493 \t--- Loss: 0.004\n",
      "Iteration: 494 \t--- Loss: 0.004\n",
      "Iteration: 495 \t--- Loss: 0.004\n",
      "Iteration: 496 \t--- Loss: 0.003\n",
      "Iteration: 497 \t--- Loss: 0.003\n",
      "Iteration: 498 \t--- Loss: 0.003\n",
      "Iteration: 499 \t--- Loss: 0.004\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:34<00:00, 214.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.145\n",
      "Iteration: 261 \t--- Loss: 0.147\n",
      "Iteration: 262 \t--- Loss: 0.144\n",
      "Iteration: 263 \t--- Loss: 0.142\n",
      "Iteration: 264 \t--- Loss: 0.132\n",
      "Iteration: 265 \t--- Loss: 0.141\n",
      "Iteration: 266 \t--- Loss: 0.130\n",
      "Iteration: 267 \t--- Loss: 0.135\n",
      "Iteration: 268 \t--- Loss: 0.132\n",
      "Iteration: 269 \t--- Loss: 0.134\n",
      "Iteration: 270 \t--- Loss: 0.125\n",
      "Iteration: 271 \t--- Loss: 0.129\n",
      "Iteration: 272 \t--- Loss: 0.128\n",
      "Iteration: 273 \t--- Loss: 0.139\n",
      "Iteration: 274 \t--- Loss: 0.124\n",
      "Iteration: 275 \t--- Loss: 0.120\n",
      "Iteration: 276 \t--- Loss: 0.128\n",
      "Iteration: 277 \t--- Loss: 0.126\n",
      "Iteration: 278 \t--- Loss: 0.133\n",
      "Iteration: 279 \t--- Loss: 0.127\n",
      "Iteration: 280 \t--- Loss: 0.124\n",
      "Iteration: 281 \t--- Loss: 0.124\n",
      "Iteration: 282 \t--- Loss: 0.127\n",
      "Iteration: 283 \t--- Loss: 0.119\n",
      "Iteration: 284 \t--- Loss: 0.124\n",
      "Iteration: 285 \t--- Loss: 0.121\n",
      "Iteration: 286 \t--- Loss: 0.125\n",
      "Iteration: 287 \t--- Loss: 0.113\n",
      "Iteration: 288 \t--- Loss: 0.113\n",
      "Iteration: 289 \t--- Loss: 0.114\n",
      "Iteration: 290 \t--- Loss: 0.113\n",
      "Iteration: 291 \t--- Loss: 0.112\n",
      "Iteration: 292 \t--- Loss: 0.108\n",
      "Iteration: 293 \t--- Loss: 0.109\n",
      "Iteration: 294 \t--- Loss: 0.107\n",
      "Iteration: 295 \t--- Loss: 0.105\n",
      "Iteration: 296 \t--- Loss: 0.103\n",
      "Iteration: 297 \t--- Loss: 0.098\n",
      "Iteration: 298 \t--- Loss: 0.098\n",
      "Iteration: 299 \t--- Loss: 0.094\n",
      "Iteration: 300 \t--- Loss: 0.088\n",
      "Iteration: 301 \t--- Loss: 0.088\n",
      "Iteration: 302 \t--- Loss: 0.085\n",
      "Iteration: 303 \t--- Loss: 0.083\n",
      "Iteration: 304 \t--- Loss: 0.078\n",
      "Iteration: 305 \t--- Loss: 0.077\n",
      "Iteration: 306 \t--- Loss: 0.074\n",
      "Iteration: 307 \t--- Loss: 0.074\n",
      "Iteration: 308 \t--- Loss: 0.068\n",
      "Iteration: 309 \t--- Loss: 0.065\n",
      "Iteration: 310 \t--- Loss: 0.061\n",
      "Iteration: 311 \t--- Loss: 0.059\n",
      "Iteration: 312 \t--- Loss: 0.058\n",
      "Iteration: 313 \t--- Loss: 0.055\n",
      "Iteration: 314 \t--- Loss: 0.057\n",
      "Iteration: 315 \t--- Loss: 0.047\n",
      "Iteration: 316 \t--- Loss: 0.052\n",
      "Iteration: 317 \t--- Loss: 0.048\n",
      "Iteration: 318 \t--- Loss: 0.048\n",
      "Iteration: 319 \t--- Loss: 0.045\n",
      "Iteration: 320 \t--- Loss: 0.043\n",
      "Iteration: 321 \t--- Loss: 0.042\n",
      "Iteration: 322 \t--- Loss: 0.039\n",
      "Iteration: 323 \t--- Loss: 0.040\n",
      "Iteration: 324 \t--- Loss: 0.038\n",
      "Iteration: 325 \t--- Loss: 0.037\n",
      "Iteration: 326 \t--- Loss: 0.035\n",
      "Iteration: 327 \t--- Loss: 0.037\n",
      "Iteration: 328 \t--- Loss: 0.035\n",
      "Iteration: 329 \t--- Loss: 0.029\n",
      "Iteration: 330 \t--- Loss: 0.033\n",
      "Iteration: 331 \t--- Loss: 0.034\n",
      "Iteration: 332 \t--- Loss: 0.034\n",
      "Iteration: 333 \t--- Loss: 0.029\n",
      "Iteration: 334 \t--- Loss: 0.030\n",
      "Iteration: 335 \t--- Loss: 0.030\n",
      "Iteration: 336 \t--- Loss: 0.032\n",
      "Iteration: 337 \t--- Loss: 0.029\n",
      "Iteration: 338 \t--- Loss: 0.031\n",
      "Iteration: 339 \t--- Loss: 0.025\n",
      "Iteration: 340 \t--- Loss: 0.030\n",
      "Iteration: 341 \t--- Loss: 0.028\n",
      "Iteration: 342 \t--- Loss: 0.027\n",
      "Iteration: 343 \t--- Loss: 0.024\n",
      "Iteration: 344 \t--- Loss: 0.024\n",
      "Iteration: 345 \t--- Loss: 0.026\n",
      "Iteration: 346 \t--- Loss: 0.028\n",
      "Iteration: 347 \t--- Loss: 0.026\n",
      "Iteration: 348 \t--- Loss: 0.027\n",
      "Iteration: 349 \t--- Loss: 0.025\n",
      "Iteration: 350 \t--- Loss: 0.023\n",
      "Iteration: 351 \t--- Loss: 0.026\n",
      "Iteration: 352 \t--- Loss: 0.024\n",
      "Iteration: 353 \t--- Loss: 0.024\n",
      "Iteration: 354 \t--- Loss: 0.024\n",
      "Iteration: 355 \t--- Loss: 0.023\n",
      "Iteration: 356 \t--- Loss: 0.024\n",
      "Iteration: 357 \t--- Loss: 0.022\n",
      "Iteration: 358 \t--- Loss: 0.023\n",
      "Iteration: 359 \t--- Loss: 0.021\n",
      "Iteration: 360 \t--- Loss: 0.020\n",
      "Iteration: 361 \t--- Loss: 0.021\n",
      "Iteration: 362 \t--- Loss: 0.022\n",
      "Iteration: 363 \t--- Loss: 0.021\n",
      "Iteration: 364 \t--- Loss: 0.024\n",
      "Iteration: 365 \t--- Loss: 0.021\n",
      "Iteration: 366 \t--- Loss: 0.018\n",
      "Iteration: 367 \t--- Loss: 0.018\n",
      "Iteration: 368 \t--- Loss: 0.017\n",
      "Iteration: 369 \t--- Loss: 0.019\n",
      "Iteration: 370 \t--- Loss: 0.021\n",
      "Iteration: 371 \t--- Loss: 0.019\n",
      "Iteration: 372 \t--- Loss: 0.019\n",
      "Iteration: 373 \t--- Loss: 0.021\n",
      "Iteration: 374 \t--- Loss: 0.017\n",
      "Iteration: 375 \t--- Loss: 0.017\n",
      "Iteration: 376 \t--- Loss: 0.017\n",
      "Iteration: 377 \t--- Loss: 0.017\n",
      "Iteration: 378 \t--- Loss: 0.017\n",
      "Iteration: 379 \t--- Loss: 0.018\n",
      "Iteration: 380 \t--- Loss: 0.018\n",
      "Iteration: 381 \t--- Loss: 0.016\n",
      "Iteration: 382 \t--- Loss: 0.016\n",
      "Iteration: 383 \t--- Loss: 0.017\n",
      "Iteration: 384 \t--- Loss: 0.017\n",
      "Iteration: 385 \t--- Loss: 0.018\n",
      "Iteration: 386 \t--- Loss: 0.018\n",
      "Iteration: 387 \t--- Loss: 0.016\n",
      "Iteration: 388 \t--- Loss: 0.017\n",
      "Iteration: 389 \t--- Loss: 0.016\n",
      "Iteration: 390 \t--- Loss: 0.015\n",
      "Iteration: 391 \t--- Loss: 0.016\n",
      "Iteration: 392 \t--- Loss: 0.017\n",
      "Iteration: 393 \t--- Loss: 0.018\n",
      "Iteration: 394 \t--- Loss: 0.014\n",
      "Iteration: 395 \t--- Loss: 0.014\n",
      "Iteration: 396 \t--- Loss: 0.017\n",
      "Iteration: 397 \t--- Loss: 0.015\n",
      "Iteration: 398 \t--- Loss: 0.014\n",
      "Iteration: 399 \t--- Loss: 0.014\n",
      "Iteration: 400 \t--- Loss: 0.014\n",
      "Iteration: 401 \t--- Loss: 0.015\n",
      "Iteration: 402 \t--- Loss: 0.015\n",
      "Iteration: 403 \t--- Loss: 0.015\n",
      "Iteration: 404 \t--- Loss: 0.015\n",
      "Iteration: 405 \t--- Loss: 0.015\n",
      "Iteration: 406 \t--- Loss: 0.014\n",
      "Iteration: 407 \t--- Loss: 0.014\n",
      "Iteration: 408 \t--- Loss: 0.013\n",
      "Iteration: 409 \t--- Loss: 0.013\n",
      "Iteration: 410 \t--- Loss: 0.012\n",
      "Iteration: 411 \t--- Loss: 0.015\n",
      "Iteration: 412 \t--- Loss: 0.014\n",
      "Iteration: 413 \t--- Loss: 0.013\n",
      "Iteration: 414 \t--- Loss: 0.012\n",
      "Iteration: 415 \t--- Loss: 0.014\n",
      "Iteration: 416 \t--- Loss: 0.014\n",
      "Iteration: 417 \t--- Loss: 0.012\n",
      "Iteration: 418 \t--- Loss: 0.013\n",
      "Iteration: 419 \t--- Loss: 0.013\n",
      "Iteration: 420 \t--- Loss: 0.013\n",
      "Iteration: 421 \t--- Loss: 0.012\n",
      "Iteration: 422 \t--- Loss: 0.013\n",
      "Iteration: 423 \t--- Loss: 0.012\n",
      "Iteration: 424 \t--- Loss: 0.012\n",
      "Iteration: 425 \t--- Loss: 0.012\n",
      "Iteration: 426 \t--- Loss: 0.013\n",
      "Iteration: 427 \t--- Loss: 0.013\n",
      "Iteration: 428 \t--- Loss: 0.012\n",
      "Iteration: 429 \t--- Loss: 0.013\n",
      "Iteration: 430 \t--- Loss: 0.011\n",
      "Iteration: 431 \t--- Loss: 0.012\n",
      "Iteration: 432 \t--- Loss: 0.012\n",
      "Iteration: 433 \t--- Loss: 0.011\n",
      "Iteration: 434 \t--- Loss: 0.011\n",
      "Iteration: 435 \t--- Loss: 0.012\n",
      "Iteration: 436 \t--- Loss: 0.012\n",
      "Iteration: 437 \t--- Loss: 0.012\n",
      "Iteration: 438 \t--- Loss: 0.012\n",
      "Iteration: 439 \t--- Loss: 0.012\n",
      "Iteration: 440 \t--- Loss: 0.012\n",
      "Iteration: 441 \t--- Loss: 0.011\n",
      "Iteration: 442 \t--- Loss: 0.010\n",
      "Iteration: 443 \t--- Loss: 0.011\n",
      "Iteration: 444 \t--- Loss: 0.010\n",
      "Iteration: 445 \t--- Loss: 0.011\n",
      "Iteration: 446 \t--- Loss: 0.011\n",
      "Iteration: 447 \t--- Loss: 0.011\n",
      "Iteration: 448 \t--- Loss: 0.012\n",
      "Iteration: 449 \t--- Loss: 0.011\n",
      "Iteration: 450 \t--- Loss: 0.010\n",
      "Iteration: 451 \t--- Loss: 0.011\n",
      "Iteration: 452 \t--- Loss: 0.011\n",
      "Iteration: 453 \t--- Loss: 0.011\n",
      "Iteration: 454 \t--- Loss: 0.010\n",
      "Iteration: 455 \t--- Loss: 0.010\n",
      "Iteration: 456 \t--- Loss: 0.010\n",
      "Iteration: 457 \t--- Loss: 0.010\n",
      "Iteration: 458 \t--- Loss: 0.010\n",
      "Iteration: 459 \t--- Loss: 0.010\n",
      "Iteration: 460 \t--- Loss: 0.010\n",
      "Iteration: 461 \t--- Loss: 0.010\n",
      "Iteration: 462 \t--- Loss: 0.011\n",
      "Iteration: 463 \t--- Loss: 0.010\n",
      "Iteration: 464 \t--- Loss: 0.010\n",
      "Iteration: 465 \t--- Loss: 0.011\n",
      "Iteration: 466 \t--- Loss: 0.010\n",
      "Iteration: 467 \t--- Loss: 0.010\n",
      "Iteration: 468 \t--- Loss: 0.010\n",
      "Iteration: 469 \t--- Loss: 0.009\n",
      "Iteration: 470 \t--- Loss: 0.010\n",
      "Iteration: 471 \t--- Loss: 0.010\n",
      "Iteration: 472 \t--- Loss: 0.010\n",
      "Iteration: 473 \t--- Loss: 0.009\n",
      "Iteration: 474 \t--- Loss: 0.009\n",
      "Iteration: 475 \t--- Loss: 0.010\n",
      "Iteration: 476 \t--- Loss: 0.010\n",
      "Iteration: 477 \t--- Loss: 0.009\n",
      "Iteration: 478 \t--- Loss: 0.010\n",
      "Iteration: 479 \t--- Loss: 0.009\n",
      "Iteration: 480 \t--- Loss: 0.010\n",
      "Iteration: 481 \t--- Loss: 0.009\n",
      "Iteration: 482 \t--- Loss: 0.009\n",
      "Iteration: 483 \t--- Loss: 0.009\n",
      "Iteration: 484 \t--- Loss: 0.009\n",
      "Iteration: 485 \t--- Loss: 0.010\n",
      "Iteration: 486 \t--- Loss: 0.010\n",
      "Iteration: 487 \t--- Loss: 0.010\n",
      "Iteration: 488 \t--- Loss: 0.010\n",
      "Iteration: 489 \t--- Loss: 0.009\n",
      "Iteration: 490 \t--- Loss: 0.009\n",
      "Iteration: 491 \t--- Loss: 0.009\n",
      "Iteration: 492 \t--- Loss: 0.009\n",
      "Iteration: 493 \t--- Loss: 0.009\n",
      "Iteration: 494 \t--- Loss: 0.009\n",
      "Iteration: 495 \t--- Loss: 0.010\n",
      "Iteration: 496 \t--- Loss: 0.008\n",
      "Iteration: 497 \t--- Loss: 0.009\n",
      "Iteration: 498 \t--- Loss: 0.009\n",
      "Iteration: 499 \t--- Loss: 0.009\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tuning the basis functions ----\n",
      "Iteration: 0 \t--- Loss: 0.061\n",
      "Iteration: 1 \t--- Loss: 0.057\n",
      "Iteration: 2 \t--- Loss: 0.056\n",
      "Iteration: 3 \t--- Loss: 0.054\n",
      "Iteration: 4 \t--- Loss: 0.053\n",
      "Iteration: 5 \t--- Loss: 0.055\n",
      "Iteration: 6 \t--- Loss: 0.056\n",
      "Iteration: 7 \t--- Loss: 0.052\n",
      "Iteration: 8 \t--- Loss: 0.050\n",
      "Iteration: 9 \t--- Loss: 0.052\n",
      "Iteration: 10 \t--- Loss: 0.052\n",
      "Iteration: 11 \t--- Loss: 0.054\n",
      "Iteration: 12 \t--- Loss: 0.054\n",
      "Iteration: 13 \t--- Loss: 0.046\n",
      "Iteration: 14 \t--- Loss: 0.050\n",
      "Iteration: 15 \t--- Loss: 0.054\n",
      "Iteration: 16 \t--- Loss: 0.051\n",
      "Iteration: 17 \t--- Loss: 0.046\n",
      "Iteration: 18 \t--- Loss: 0.047\n",
      "Iteration: 19 \t--- Loss: 0.049\n",
      "Iteration: 20 \t--- Loss: 0.049\n",
      "Iteration: 21 \t--- Loss: 0.051\n",
      "Iteration: 22 \t--- Loss: 0.051\n",
      "Iteration: 23 \t--- Loss: 0.045\n",
      "Iteration: 24 \t--- Loss: 0.044\n",
      "Iteration: 25 \t--- Loss: 0.054\n",
      "Iteration: 26 \t--- Loss: 0.049\n",
      "Iteration: 27 \t--- Loss: 0.041\n",
      "Iteration: 28 \t--- Loss: 0.049\n",
      "Iteration: 29 \t--- Loss: 0.050\n",
      "Iteration: 30 \t--- Loss: 0.049\n",
      "Iteration: 31 \t--- Loss: 0.048\n",
      "Iteration: 32 \t--- Loss: 0.046\n",
      "Iteration: 33 \t--- Loss: 0.047\n",
      "Iteration: 34 \t--- Loss: 0.049\n",
      "Iteration: 35 \t--- Loss: 0.048\n",
      "Iteration: 36 \t--- Loss: 0.046\n",
      "Iteration: 37 \t--- Loss: 0.046\n",
      "Iteration: 38 \t--- Loss: 0.049\n",
      "Iteration: 39 \t--- Loss: 0.047\n",
      "Iteration: 40 \t--- Loss: 0.045\n",
      "Iteration: 41 \t--- Loss: 0.045\n",
      "Iteration: 42 \t--- Loss: 0.049\n",
      "Iteration: 43 \t--- Loss: 0.044\n",
      "Iteration: 44 \t--- Loss: 0.047\n",
      "Iteration: 45 \t--- Loss: 0.047\n",
      "Iteration: 46 \t--- Loss: 0.046\n",
      "Iteration: 47 \t--- Loss: 0.047\n",
      "Iteration: 48 \t--- Loss: 0.043\n",
      "Iteration: 49 \t--- Loss: 0.047\n",
      "Iteration: 50 \t--- Loss: 0.045\n",
      "Iteration: 51 \t--- Loss: 0.048\n",
      "Iteration: 52 \t--- Loss: 0.043\n",
      "Iteration: 53 \t--- Loss: 0.049\n",
      "Iteration: 54 \t--- Loss: 0.044\n",
      "Iteration: 55 \t--- Loss: 0.046\n",
      "Iteration: 56 \t--- Loss: 0.045\n",
      "Iteration: 57 \t--- Loss: 0.043\n",
      "Iteration: 58 \t--- Loss: 0.043\n",
      "Iteration: 59 \t--- Loss: 0.048\n",
      "Iteration: 60 \t--- Loss: 0.043\n",
      "Iteration: 61 \t--- Loss: 0.047\n",
      "Iteration: 62 \t--- Loss: 0.043\n",
      "Iteration: 63 \t--- Loss: 0.043\n",
      "Iteration: 64 \t--- Loss: 0.039\n",
      "Iteration: 65 \t--- Loss: 0.046\n",
      "Iteration: 66 \t--- Loss: 0.049\n",
      "Iteration: 67 \t--- Loss: 0.042\n",
      "Iteration: 68 \t--- Loss: 0.044\n",
      "Iteration: 69 \t--- Loss: 0.045\n",
      "Iteration: 70 \t--- Loss: 0.047\n",
      "Iteration: 71 \t--- Loss: 0.045\n",
      "Iteration: 72 \t--- Loss: 0.046\n",
      "Iteration: 73 \t--- Loss: 0.042\n",
      "Iteration: 74 \t--- Loss: 0.045\n",
      "Iteration: 75 \t--- Loss: 0.043\n",
      "Iteration: 76 \t--- Loss: 0.046\n",
      "Iteration: 77 \t--- Loss: 0.044\n",
      "Iteration: 78 \t--- Loss: 0.046\n",
      "Iteration: 79 \t--- Loss: 0.046\n",
      "Iteration: 80 \t--- Loss: 0.047\n",
      "Iteration: 81 \t--- Loss: 0.047\n",
      "Iteration: 82 \t--- Loss: 0.047\n",
      "Iteration: 83 \t--- Loss: 0.051\n",
      "Iteration: 84 \t--- Loss: 0.047\n",
      "Iteration: 85 \t--- Loss: 0.045\n",
      "Iteration: 86 \t--- Loss: 0.042\n",
      "Iteration: 87 \t--- Loss: 0.047\n",
      "Iteration: 88 \t--- Loss: 0.043\n",
      "Iteration: 89 \t--- Loss: 0.050\n",
      "Iteration: 90 \t--- Loss: 0.044\n",
      "Iteration: 91 \t--- Loss: 0.051\n",
      "Iteration: 92 \t--- Loss: 0.047\n",
      "Iteration: 93 \t--- Loss: 0.046\n",
      "Iteration: 94 \t--- Loss: 0.043\n",
      "Iteration: 95 \t--- Loss: 0.049\n",
      "Iteration: 96 \t--- Loss: 0.046\n",
      "Iteration: 97 \t--- Loss: 0.048\n",
      "Iteration: 98 \t--- Loss: 0.046\n",
      "Iteration: 99 \t--- Loss: 0.047\n",
      "Iteration: 100 \t--- Loss: 0.044\n",
      "Iteration: 101 \t--- Loss: 0.042\n",
      "Iteration: 102 \t--- Loss: 0.042\n",
      "Iteration: 103 \t--- Loss: 0.042\n",
      "Iteration: 104 \t--- Loss: 0.042\n",
      "Iteration: 105 \t--- Loss: 0.044\n",
      "Iteration: 106 \t--- Loss: 0.046\n",
      "Iteration: 107 \t--- Loss: 0.042\n",
      "Iteration: 108 \t--- Loss: 0.043\n",
      "Iteration: 109 \t--- Loss: 0.045\n",
      "Iteration: 110 \t--- Loss: 0.040\n",
      "Iteration: 111 \t--- Loss: 0.043\n",
      "Iteration: 112 \t--- Loss: 0.043\n",
      "Iteration: 113 \t--- Loss: 0.041\n",
      "Iteration: 114 \t--- Loss: 0.044\n",
      "Iteration: 115 \t--- Loss: 0.043\n",
      "Iteration: 116 \t--- Loss: 0.041\n",
      "Iteration: 117 \t--- Loss: 0.046\n",
      "Iteration: 118 \t--- Loss: 0.044\n",
      "Iteration: 119 \t--- Loss: 0.046\n",
      "Iteration: 120 \t--- Loss: 0.043\n",
      "Iteration: 121 \t--- Loss: 0.043\n",
      "Iteration: 122 \t--- Loss: 0.049\n",
      "Iteration: 123 \t--- Loss: 0.043\n",
      "Iteration: 124 \t--- Loss: 0.041\n",
      "Iteration: 125 \t--- Loss: 0.047\n",
      "Iteration: 126 \t--- Loss: 0.045\n",
      "Iteration: 127 \t--- Loss: 0.043\n",
      "Iteration: 128 \t--- Loss: 0.047\n",
      "Iteration: 129 \t--- Loss: 0.049\n",
      "Iteration: 130 \t--- Loss: 0.047\n",
      "Iteration: 131 \t--- Loss: 0.044\n",
      "Iteration: 132 \t--- Loss: 0.045\n",
      "Iteration: 133 \t--- Loss: 0.041\n",
      "Iteration: 134 \t--- Loss: 0.046\n",
      "Iteration: 135 \t--- Loss: 0.051\n",
      "Iteration: 136 \t--- Loss: 0.045\n",
      "Iteration: 137 \t--- Loss: 0.043\n",
      "Iteration: 138 \t--- Loss: 0.044\n",
      "Iteration: 139 \t--- Loss: 0.041\n",
      "Iteration: 140 \t--- Loss: 0.045\n",
      "Iteration: 141 \t--- Loss: 0.042\n",
      "Iteration: 142 \t--- Loss: 0.046\n",
      "Iteration: 143 \t--- Loss: 0.042\n",
      "Iteration: 144 \t--- Loss: 0.044\n",
      "Iteration: 145 \t--- Loss: 0.046\n",
      "Iteration: 146 \t--- Loss: 0.044\n",
      "Iteration: 147 \t--- Loss: 0.043\n",
      "Iteration: 148 \t--- Loss: 0.045\n",
      "Iteration: 149 \t--- Loss: 0.041\n",
      "Iteration: 150 \t--- Loss: 0.041\n",
      "Iteration: 151 \t--- Loss: 0.045\n",
      "Iteration: 152 \t--- Loss: 0.041\n",
      "Iteration: 153 \t--- Loss: 0.043\n",
      "Iteration: 154 \t--- Loss: 0.044\n",
      "Iteration: 155 \t--- Loss: 0.045\n",
      "Iteration: 156 \t--- Loss: 0.042\n",
      "Iteration: 157 \t--- Loss: 0.045\n",
      "Iteration: 158 \t--- Loss: 0.043\n",
      "Iteration: 159 \t--- Loss: 0.043\n",
      "Iteration: 160 \t--- Loss: 0.042\n",
      "Iteration: 161 \t--- Loss: 0.043\n",
      "Iteration: 162 \t--- Loss: 0.039\n",
      "Iteration: 163 \t--- Loss: 0.047\n",
      "Iteration: 164 \t--- Loss: 0.041\n",
      "Iteration: 165 \t--- Loss: 0.043\n",
      "Iteration: 166 \t--- Loss: 0.045\n",
      "Iteration: 167 \t--- Loss: 0.043\n",
      "Iteration: 168 \t--- Loss: 0.044\n",
      "Iteration: 169 \t--- Loss: 0.043\n",
      "Iteration: 170 \t--- Loss: 0.047\n",
      "Iteration: 171 \t--- Loss: 0.043\n",
      "Iteration: 172 \t--- Loss: 0.042\n",
      "Iteration: 173 \t--- Loss: 0.041\n",
      "Iteration: 174 \t--- Loss: 0.044\n",
      "Iteration: 175 \t--- Loss: 0.046\n",
      "Iteration: 176 \t--- Loss: 0.047\n",
      "Iteration: 177 \t--- Loss: 0.042\n",
      "Iteration: 178 \t--- Loss: 0.045\n",
      "Iteration: 179 \t--- Loss: 0.044\n",
      "Iteration: 180 \t--- Loss: 0.045\n",
      "Iteration: 181 \t--- Loss: 0.043\n",
      "Iteration: 182 \t--- Loss: 0.044\n",
      "Iteration: 183 \t--- Loss: 0.042\n",
      "Iteration: 184 \t--- Loss: 0.045\n",
      "Iteration: 185 \t--- Loss: 0.045\n",
      "Iteration: 186 \t--- Loss: 0.043\n",
      "Iteration: 187 \t--- Loss: 0.044\n",
      "Iteration: 188 \t--- Loss: 0.046\n",
      "Iteration: 189 \t--- Loss: 0.047\n",
      "Iteration: 190 \t--- Loss: 0.046\n",
      "Iteration: 191 \t--- Loss: 0.047\n",
      "Iteration: 192 \t--- Loss: 0.047\n",
      "Iteration: 193 \t--- Loss: 0.045\n",
      "Iteration: 194 \t--- Loss: 0.042\n",
      "Iteration: 195 \t--- Loss: 0.045\n",
      "Iteration: 196 \t--- Loss: 0.045\n",
      "Iteration: 197 \t--- Loss: 0.043\n",
      "Iteration: 198 \t--- Loss: 0.042\n",
      "Iteration: 199 \t--- Loss: 0.045\n",
      "Iteration: 200 \t--- Loss: 0.041\n",
      "Iteration: 201 \t--- Loss: 0.045\n",
      "Iteration: 202 \t--- Loss: 0.042\n",
      "Iteration: 203 \t--- Loss: 0.044\n",
      "Iteration: 204 \t--- Loss: 0.042\n",
      "Iteration: 205 \t--- Loss: 0.041\n",
      "Iteration: 206 \t--- Loss: 0.045\n",
      "Iteration: 207 \t--- Loss: 0.048\n",
      "Iteration: 208 \t--- Loss: 0.042\n",
      "Iteration: 209 \t--- Loss: 0.044\n",
      "Iteration: 210 \t--- Loss: 0.042\n",
      "Iteration: 211 \t--- Loss: 0.043\n",
      "Iteration: 212 \t--- Loss: 0.047\n",
      "Iteration: 213 \t--- Loss: 0.041\n",
      "Iteration: 214 \t--- Loss: 0.049\n",
      "Iteration: 215 \t--- Loss: 0.043\n",
      "Iteration: 216 \t--- Loss: 0.043\n",
      "Iteration: 217 \t--- Loss: 0.040\n",
      "Iteration: 218 \t--- Loss: 0.042\n",
      "Iteration: 219 \t--- Loss: 0.045\n",
      "Iteration: 220 \t--- Loss: 0.042\n",
      "Iteration: 221 \t--- Loss: 0.041\n",
      "Iteration: 222 \t--- Loss: 0.042\n",
      "Iteration: 223 \t--- Loss: 0.041\n",
      "Iteration: 224 \t--- Loss: 0.046\n",
      "Iteration: 225 \t--- Loss: 0.045\n",
      "Iteration: 226 \t--- Loss: 0.036\n",
      "Iteration: 227 \t--- Loss: 0.043\n",
      "Iteration: 228 \t--- Loss: 0.039\n",
      "Iteration: 229 \t--- Loss: 0.044\n",
      "Iteration: 230 \t--- Loss: 0.046\n",
      "Iteration: 231 \t--- Loss: 0.042\n",
      "Iteration: 232 \t--- Loss: 0.038\n",
      "Iteration: 233 \t--- Loss: 0.045\n",
      "Iteration: 234 \t--- Loss: 0.040\n",
      "Iteration: 235 \t--- Loss: 0.044\n",
      "Iteration: 236 \t--- Loss: 0.041\n",
      "Iteration: 237 \t--- Loss: 0.037\n",
      "Iteration: 238 \t--- Loss: 0.043\n",
      "Iteration: 239 \t--- Loss: 0.047\n",
      "Iteration: 240 \t--- Loss: 0.047\n",
      "Iteration: 241 \t--- Loss: 0.045\n",
      "Iteration: 242 \t--- Loss: 0.043\n",
      "Iteration: 243 \t--- Loss: 0.042\n",
      "Iteration: 244 \t--- Loss: 0.040\n",
      "Iteration: 245 \t--- Loss: 0.042\n",
      "Iteration: 246 \t--- Loss: 0.043\n",
      "Iteration: 247 \t--- Loss: 0.038\n",
      "Iteration: 248 \t--- Loss: 0.041\n",
      "Iteration: 249 \t--- Loss: 0.042\n",
      "Iteration: 250 \t--- Loss: 0.041\n",
      "Iteration: 251 \t--- Loss: 0.044\n",
      "Iteration: 252 \t--- Loss: 0.039\n",
      "Iteration: 253 \t--- Loss: 0.044\n",
      "Iteration: 254 \t--- Loss: 0.046\n",
      "Iteration: 255 \t--- Loss: 0.039\n",
      "Iteration: 256 \t--- Loss: 0.044\n",
      "Iteration: 257 \t--- Loss: 0.038\n",
      "Iteration: 258 \t--- Loss: 0.042\n",
      "Iteration: 259 \t--- Loss: 0.042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [09:59<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.061\n",
      "Iteration: 1 \t--- Loss: 0.058\n",
      "Iteration: 2 \t--- Loss: 0.062\n",
      "Iteration: 3 \t--- Loss: 0.064\n",
      "Iteration: 4 \t--- Loss: 0.060\n",
      "Iteration: 5 \t--- Loss: 0.067\n",
      "Iteration: 6 \t--- Loss: 0.068\n",
      "Iteration: 7 \t--- Loss: 0.066\n",
      "Iteration: 8 \t--- Loss: 0.060\n",
      "Iteration: 9 \t--- Loss: 0.065\n",
      "Iteration: 10 \t--- Loss: 0.062\n",
      "Iteration: 11 \t--- Loss: 0.066\n",
      "Iteration: 12 \t--- Loss: 0.064\n",
      "Iteration: 13 \t--- Loss: 0.063\n",
      "Iteration: 14 \t--- Loss: 0.065\n",
      "Iteration: 15 \t--- Loss: 0.065\n",
      "Iteration: 16 \t--- Loss: 0.068\n",
      "Iteration: 17 \t--- Loss: 0.066\n",
      "Iteration: 18 \t--- Loss: 0.063\n",
      "Iteration: 19 \t--- Loss: 0.071\n",
      "Iteration: 20 \t--- Loss: 0.063\n",
      "Iteration: 21 \t--- Loss: 0.068\n",
      "Iteration: 22 \t--- Loss: 0.062\n",
      "Iteration: 23 \t--- Loss: 0.060\n",
      "Iteration: 24 \t--- Loss: 0.061\n",
      "Iteration: 25 \t--- Loss: 0.060\n",
      "Iteration: 26 \t--- Loss: 0.065\n",
      "Iteration: 27 \t--- Loss: 0.065\n",
      "Iteration: 28 \t--- Loss: 0.058\n",
      "Iteration: 29 \t--- Loss: 0.063\n",
      "Iteration: 30 \t--- Loss: 0.063\n",
      "Iteration: 31 \t--- Loss: 0.065\n",
      "Iteration: 32 \t--- Loss: 0.064\n",
      "Iteration: 33 \t--- Loss: 0.062\n",
      "Iteration: 34 \t--- Loss: 0.062\n",
      "Iteration: 35 \t--- Loss: 0.061\n",
      "Iteration: 36 \t--- Loss: 0.062\n",
      "Iteration: 37 \t--- Loss: 0.065\n",
      "Iteration: 38 \t--- Loss: 0.062\n",
      "Iteration: 39 \t--- Loss: 0.064\n",
      "Iteration: 40 \t--- Loss: 0.064\n",
      "Iteration: 41 \t--- Loss: 0.061\n",
      "Iteration: 42 \t--- Loss: 0.064\n",
      "Iteration: 43 \t--- Loss: 0.062\n",
      "Iteration: 44 \t--- Loss: 0.061\n",
      "Iteration: 45 \t--- Loss: 0.062\n",
      "Iteration: 46 \t--- Loss: 0.062\n",
      "Iteration: 47 \t--- Loss: 0.061\n",
      "Iteration: 48 \t--- Loss: 0.064\n",
      "Iteration: 49 \t--- Loss: 0.063\n",
      "Iteration: 50 \t--- Loss: 0.061\n",
      "Iteration: 51 \t--- Loss: 0.064\n",
      "Iteration: 52 \t--- Loss: 0.064\n",
      "Iteration: 53 \t--- Loss: 0.068\n",
      "Iteration: 54 \t--- Loss: 0.060\n",
      "Iteration: 55 \t--- Loss: 0.062\n",
      "Iteration: 56 \t--- Loss: 0.065\n",
      "Iteration: 57 \t--- Loss: 0.062\n",
      "Iteration: 58 \t--- Loss: 0.059\n",
      "Iteration: 59 \t--- Loss: 0.061\n",
      "Iteration: 60 \t--- Loss: 0.059\n",
      "Iteration: 61 \t--- Loss: 0.063\n",
      "Iteration: 62 \t--- Loss: 0.064\n",
      "Iteration: 63 \t--- Loss: 0.062\n",
      "Iteration: 64 \t--- Loss: 0.060\n",
      "Iteration: 65 \t--- Loss: 0.064\n",
      "Iteration: 66 \t--- Loss: 0.057\n",
      "Iteration: 67 \t--- Loss: 0.059\n",
      "Iteration: 68 \t--- Loss: 0.065\n",
      "Iteration: 69 \t--- Loss: 0.058\n",
      "Iteration: 70 \t--- Loss: 0.063\n",
      "Iteration: 71 \t--- Loss: 0.055\n",
      "Iteration: 72 \t--- Loss: 0.063\n",
      "Iteration: 73 \t--- Loss: 0.058\n",
      "Iteration: 74 \t--- Loss: 0.065\n",
      "Iteration: 75 \t--- Loss: 0.061\n",
      "Iteration: 76 \t--- Loss: 0.063\n",
      "Iteration: 77 \t--- Loss: 0.061\n",
      "Iteration: 78 \t--- Loss: 0.057\n",
      "Iteration: 79 \t--- Loss: 0.061\n",
      "Iteration: 80 \t--- Loss: 0.060\n",
      "Iteration: 81 \t--- Loss: 0.059\n",
      "Iteration: 82 \t--- Loss: 0.061\n",
      "Iteration: 83 \t--- Loss: 0.059\n",
      "Iteration: 84 \t--- Loss: 0.056\n",
      "Iteration: 85 \t--- Loss: 0.057\n",
      "Iteration: 86 \t--- Loss: 0.057\n",
      "Iteration: 87 \t--- Loss: 0.058\n",
      "Iteration: 88 \t--- Loss: 0.062\n",
      "Iteration: 89 \t--- Loss: 0.061\n",
      "Iteration: 90 \t--- Loss: 0.055\n",
      "Iteration: 91 \t--- Loss: 0.057\n",
      "Iteration: 92 \t--- Loss: 0.061\n",
      "Iteration: 93 \t--- Loss: 0.057\n",
      "Iteration: 94 \t--- Loss: 0.064\n",
      "Iteration: 95 \t--- Loss: 0.060\n",
      "Iteration: 96 \t--- Loss: 0.060\n",
      "Iteration: 97 \t--- Loss: 0.067\n",
      "Iteration: 98 \t--- Loss: 0.062\n",
      "Iteration: 99 \t--- Loss: 0.056\n",
      "Iteration: 100 \t--- Loss: 0.057\n",
      "Iteration: 101 \t--- Loss: 0.063\n",
      "Iteration: 102 \t--- Loss: 0.056\n",
      "Iteration: 103 \t--- Loss: 0.058\n",
      "Iteration: 104 \t--- Loss: 0.055\n",
      "Iteration: 105 \t--- Loss: 0.057\n",
      "Iteration: 106 \t--- Loss: 0.060\n",
      "Iteration: 107 \t--- Loss: 0.060\n",
      "Iteration: 108 \t--- Loss: 0.060\n",
      "Iteration: 109 \t--- Loss: 0.057\n",
      "Iteration: 110 \t--- Loss: 0.059\n",
      "Iteration: 111 \t--- Loss: 0.058\n",
      "Iteration: 112 \t--- Loss: 0.057\n",
      "Iteration: 113 \t--- Loss: 0.057\n",
      "Iteration: 114 \t--- Loss: 0.060\n",
      "Iteration: 115 \t--- Loss: 0.057\n",
      "Iteration: 116 \t--- Loss: 0.061\n",
      "Iteration: 117 \t--- Loss: 0.063\n",
      "Iteration: 118 \t--- Loss: 0.059\n",
      "Iteration: 119 \t--- Loss: 0.054\n",
      "Iteration: 120 \t--- Loss: 0.058\n",
      "Iteration: 121 \t--- Loss: 0.059\n",
      "Iteration: 122 \t--- Loss: 0.061\n",
      "Iteration: 123 \t--- Loss: 0.059\n",
      "Iteration: 124 \t--- Loss: 0.060\n",
      "Iteration: 125 \t--- Loss: 0.056\n",
      "Iteration: 126 \t--- Loss: 0.059\n",
      "Iteration: 127 \t--- Loss: 0.054\n",
      "Iteration: 128 \t--- Loss: 0.054\n",
      "Iteration: 129 \t--- Loss: 0.052\n",
      "Iteration: 130 \t--- Loss: 0.052\n",
      "Iteration: 131 \t--- Loss: 0.056\n",
      "Iteration: 132 \t--- Loss: 0.047\n",
      "Iteration: 133 \t--- Loss: 0.049\n",
      "Iteration: 134 \t--- Loss: 0.046\n",
      "Iteration: 135 \t--- Loss: 0.044\n",
      "Iteration: 136 \t--- Loss: 0.047\n",
      "Iteration: 137 \t--- Loss: 0.042\n",
      "Iteration: 138 \t--- Loss: 0.037\n",
      "Iteration: 139 \t--- Loss: 0.033\n",
      "Iteration: 140 \t--- Loss: 0.039\n",
      "Iteration: 141 \t--- Loss: 0.028\n",
      "Iteration: 142 \t--- Loss: 0.030\n",
      "Iteration: 143 \t--- Loss: 0.032\n",
      "Iteration: 144 \t--- Loss: 0.033\n",
      "Iteration: 145 \t--- Loss: 0.037\n",
      "Iteration: 146 \t--- Loss: 0.032\n",
      "Iteration: 147 \t--- Loss: 0.034\n",
      "Iteration: 148 \t--- Loss: 0.037\n",
      "Iteration: 149 \t--- Loss: 0.030\n",
      "Iteration: 150 \t--- Loss: 0.029\n",
      "Iteration: 151 \t--- Loss: 0.029\n",
      "Iteration: 152 \t--- Loss: 0.028\n",
      "Iteration: 153 \t--- Loss: 0.033\n",
      "Iteration: 154 \t--- Loss: 0.034\n",
      "Iteration: 155 \t--- Loss: 0.031\n",
      "Iteration: 156 \t--- Loss: 0.031\n",
      "Iteration: 157 \t--- Loss: 0.030\n",
      "Iteration: 158 \t--- Loss: 0.029\n",
      "Iteration: 159 \t--- Loss: 0.029\n",
      "Iteration: 160 \t--- Loss: 0.033\n",
      "Iteration: 161 \t--- Loss: 0.032\n",
      "Iteration: 162 \t--- Loss: 0.026\n",
      "Iteration: 163 \t--- Loss: 0.028\n",
      "Iteration: 164 \t--- Loss: 0.026\n",
      "Iteration: 165 \t--- Loss: 0.031\n",
      "Iteration: 166 \t--- Loss: 0.027\n",
      "Iteration: 167 \t--- Loss: 0.028\n",
      "Iteration: 168 \t--- Loss: 0.026\n",
      "Iteration: 169 \t--- Loss: 0.027\n",
      "Iteration: 170 \t--- Loss: 0.029\n",
      "Iteration: 171 \t--- Loss: 0.026\n",
      "Iteration: 172 \t--- Loss: 0.028\n",
      "Iteration: 173 \t--- Loss: 0.030\n",
      "Iteration: 174 \t--- Loss: 0.027\n",
      "Iteration: 175 \t--- Loss: 0.031\n",
      "Iteration: 176 \t--- Loss: 0.028\n",
      "Iteration: 177 \t--- Loss: 0.028\n",
      "Iteration: 178 \t--- Loss: 0.025\n",
      "Iteration: 179 \t--- Loss: 0.031\n",
      "Iteration: 180 \t--- Loss: 0.027\n",
      "Iteration: 181 \t--- Loss: 0.025\n",
      "Iteration: 182 \t--- Loss: 0.030\n",
      "Iteration: 183 \t--- Loss: 0.029\n",
      "Iteration: 184 \t--- Loss: 0.031\n",
      "Iteration: 185 \t--- Loss: 0.032\n",
      "Iteration: 186 \t--- Loss: 0.034\n",
      "Iteration: 187 \t--- Loss: 0.051\n",
      "Iteration: 188 \t--- Loss: 0.111\n",
      "Iteration: 189 \t--- Loss: 0.040\n",
      "Iteration: 190 \t--- Loss: 0.029\n",
      "Iteration: 191 \t--- Loss: 0.034\n",
      "Iteration: 192 \t--- Loss: 0.041\n",
      "Iteration: 193 \t--- Loss: 0.072\n",
      "Iteration: 194 \t--- Loss: 0.031\n",
      "Iteration: 195 \t--- Loss: 0.028\n",
      "Iteration: 196 \t--- Loss: 0.038\n",
      "Iteration: 197 \t--- Loss: 0.046\n",
      "Iteration: 198 \t--- Loss: 0.045\n",
      "Iteration: 199 \t--- Loss: 0.086\n",
      "Iteration: 200 \t--- Loss: 0.026\n",
      "Iteration: 201 \t--- Loss: 0.026\n",
      "Iteration: 202 \t--- Loss: 0.025\n",
      "Iteration: 203 \t--- Loss: 0.026\n",
      "Iteration: 204 \t--- Loss: 0.029\n",
      "Iteration: 205 \t--- Loss: 0.039\n",
      "Iteration: 206 \t--- Loss: 0.059\n",
      "Iteration: 207 \t--- Loss: 0.028\n",
      "Iteration: 208 \t--- Loss: 0.030\n",
      "Iteration: 209 \t--- Loss: 0.030\n",
      "Iteration: 210 \t--- Loss: 0.037\n",
      "Iteration: 211 \t--- Loss: 0.044\n",
      "Iteration: 212 \t--- Loss: 0.069\n",
      "Iteration: 213 \t--- Loss: 0.020\n",
      "Iteration: 214 \t--- Loss: 0.020\n",
      "Iteration: 215 \t--- Loss: 0.019\n",
      "Iteration: 216 \t--- Loss: 0.018\n",
      "Iteration: 217 \t--- Loss: 0.026\n",
      "Iteration: 218 \t--- Loss: 0.020\n",
      "Iteration: 219 \t--- Loss: 0.028\n",
      "Iteration: 220 \t--- Loss: 0.042\n",
      "Iteration: 221 \t--- Loss: 0.031\n",
      "Iteration: 222 \t--- Loss: 0.053\n",
      "Iteration: 223 \t--- Loss: 0.043\n",
      "Iteration: 224 \t--- Loss: 0.084\n",
      "Iteration: 225 \t--- Loss: 0.025\n",
      "Iteration: 226 \t--- Loss: 0.027\n",
      "Iteration: 227 \t--- Loss: 0.026\n",
      "Iteration: 228 \t--- Loss: 0.025\n",
      "Iteration: 229 \t--- Loss: 0.025\n",
      "Iteration: 230 \t--- Loss: 0.029\n",
      "Iteration: 231 \t--- Loss: 0.039\n",
      "Iteration: 232 \t--- Loss: 0.020\n",
      "Iteration: 233 \t--- Loss: 0.017\n",
      "Iteration: 234 \t--- Loss: 0.030\n",
      "Iteration: 235 \t--- Loss: 0.033\n",
      "Iteration: 236 \t--- Loss: 0.022\n",
      "Iteration: 237 \t--- Loss: 0.026\n",
      "Iteration: 238 \t--- Loss: 0.029\n",
      "Iteration: 239 \t--- Loss: 0.038\n",
      "Iteration: 240 \t--- Loss: 0.022\n",
      "Iteration: 241 \t--- Loss: 0.021\n",
      "Iteration: 242 \t--- Loss: 0.023\n",
      "Iteration: 243 \t--- Loss: 0.033\n",
      "Iteration: 244 \t--- Loss: 0.026\n",
      "Iteration: 245 \t--- Loss: 0.030\n",
      "Iteration: 246 \t--- Loss: 0.022\n",
      "Iteration: 247 \t--- Loss: 0.025\n",
      "Iteration: 248 \t--- Loss: 0.022\n",
      "Iteration: 249 \t--- Loss: 0.030\n",
      "Iteration: 250 \t--- Loss: 0.024\n",
      "Iteration: 251 \t--- Loss: 0.026\n",
      "Iteration: 252 \t--- Loss: 0.018\n",
      "Iteration: 253 \t--- Loss: 0.017\n",
      "Iteration: 254 \t--- Loss: 0.016\n",
      "Iteration: 255 \t--- Loss: 0.014\n",
      "Iteration: 256 \t--- Loss: 0.019\n",
      "Iteration: 257 \t--- Loss: 0.017\n",
      "Iteration: 258 \t--- Loss: 0.019\n",
      "Iteration: 259 \t--- Loss: 0.021Iteration: 0 \t--- Loss: 0.851\n",
      "Iteration: 1 \t--- Loss: 0.797\n",
      "Iteration: 2 \t--- Loss: 0.767\n",
      "Iteration: 3 \t--- Loss: 0.783\n",
      "Iteration: 4 \t--- Loss: 0.656\n",
      "Iteration: 5 \t--- Loss: 0.644\n",
      "Iteration: 6 \t--- Loss: 0.607\n",
      "Iteration: 7 \t--- Loss: 0.591\n",
      "Iteration: 8 \t--- Loss: 0.581\n",
      "Iteration: 9 \t--- Loss: 0.556\n",
      "Iteration: 10 \t--- Loss: 0.605\n",
      "Iteration: 11 \t--- Loss: 0.511\n",
      "Iteration: 12 \t--- Loss: 0.559\n",
      "Iteration: 13 \t--- Loss: 0.565\n",
      "Iteration: 14 \t--- Loss: 0.476\n",
      "Iteration: 15 \t--- Loss: 0.507\n",
      "Iteration: 16 \t--- Loss: 0.517\n",
      "Iteration: 17 \t--- Loss: 0.479\n",
      "Iteration: 18 \t--- Loss: 0.502\n",
      "Iteration: 19 \t--- Loss: 0.491\n",
      "Iteration: 20 \t--- Loss: 0.486\n",
      "Iteration: 21 \t--- Loss: 0.509\n",
      "Iteration: 22 \t--- Loss: 0.510\n",
      "Iteration: 23 \t--- Loss: 0.495\n",
      "Iteration: 24 \t--- Loss: 0.557\n",
      "Iteration: 25 \t--- Loss: 0.507\n",
      "Iteration: 26 \t--- Loss: 0.499\n",
      "Iteration: 27 \t--- Loss: 0.511\n",
      "Iteration: 28 \t--- Loss: 0.517\n",
      "Iteration: 29 \t--- Loss: 0.524\n",
      "Iteration: 30 \t--- Loss: 0.498\n",
      "Iteration: 31 \t--- Loss: 0.578\n",
      "Iteration: 32 \t--- Loss: 0.527\n",
      "Iteration: 33 \t--- Loss: 0.520\n",
      "Iteration: 34 \t--- Loss: 0.538\n",
      "Iteration: 35 \t--- Loss: 0.484\n",
      "Iteration: 36 \t--- Loss: 0.499\n",
      "Iteration: 37 \t--- Loss: 0.498\n",
      "Iteration: 38 \t--- Loss: 0.468\n",
      "Iteration: 39 \t--- Loss: 0.452\n",
      "Iteration: 40 \t--- Loss: 0.478\n",
      "Iteration: 41 \t--- Loss: 0.512\n",
      "Iteration: 42 \t--- Loss: 0.491\n",
      "Iteration: 43 \t--- Loss: 0.525\n",
      "Iteration: 44 \t--- Loss: 0.477\n",
      "Iteration: 45 \t--- Loss: 0.512\n",
      "Iteration: 46 \t--- Loss: 0.525\n",
      "Iteration: 47 \t--- Loss: 0.516\n",
      "Iteration: 48 \t--- Loss: 0.512\n",
      "Iteration: 49 \t--- Loss: 0.489\n",
      "Iteration: 50 \t--- Loss: 0.496\n",
      "Iteration: 51 \t--- Loss: 0.490\n",
      "Iteration: 52 \t--- Loss: 0.532\n",
      "Iteration: 53 \t--- Loss: 0.475\n",
      "Iteration: 54 \t--- Loss: 0.517\n",
      "Iteration: 55 \t--- Loss: 0.497\n",
      "Iteration: 56 \t--- Loss: 0.507\n",
      "Iteration: 57 \t--- Loss: 0.481\n",
      "Iteration: 58 \t--- Loss: 0.475\n",
      "Iteration: 59 \t--- Loss: 0.441\n",
      "Iteration: 60 \t--- Loss: 0.526\n",
      "Iteration: 61 \t--- Loss: 0.498\n",
      "Iteration: 62 \t--- Loss: 0.484\n",
      "Iteration: 63 \t--- Loss: 0.478\n",
      "Iteration: 64 \t--- Loss: 0.521\n",
      "Iteration: 65 \t--- Loss: 0.517\n",
      "Iteration: 66 \t--- Loss: 0.482\n",
      "Iteration: 67 \t--- Loss: 0.507\n",
      "Iteration: 68 \t--- Loss: 0.479\n",
      "Iteration: 69 \t--- Loss: 0.557\n",
      "Iteration: 70 \t--- Loss: 0.460\n",
      "Iteration: 71 \t--- Loss: 0.491\n",
      "Iteration: 72 \t--- Loss: 0.523\n",
      "Iteration: 73 \t--- Loss: 0.490\n",
      "Iteration: 74 \t--- Loss: 0.518\n",
      "Iteration: 75 \t--- Loss: 0.490\n",
      "Iteration: 76 \t--- Loss: 0.470\n",
      "Iteration: 77 \t--- Loss: 0.525\n",
      "Iteration: 78 \t--- Loss: 0.493\n",
      "Iteration: 79 \t--- Loss: 0.485\n",
      "Iteration: 80 \t--- Loss: 0.503\n",
      "Iteration: 81 \t--- Loss: 0.569\n",
      "Iteration: 82 \t--- Loss: 0.474\n",
      "Iteration: 83 \t--- Loss: 0.520\n",
      "Iteration: 84 \t--- Loss: 0.494\n",
      "Iteration: 85 \t--- Loss: 0.551\n",
      "Iteration: 86 \t--- Loss: 0.485\n",
      "Iteration: 87 \t--- Loss: 0.497\n",
      "Iteration: 88 \t--- Loss: 0.515\n",
      "Iteration: 89 \t--- Loss: 0.456\n",
      "Iteration: 90 \t--- Loss: 0.490\n",
      "Iteration: 91 \t--- Loss: 0.511\n",
      "Iteration: 92 \t--- Loss: 0.493\n",
      "Iteration: 93 \t--- Loss: 0.476\n",
      "Iteration: 94 \t--- Loss: 0.542\n",
      "Iteration: 95 \t--- Loss: 0.516\n",
      "Iteration: 96 \t--- Loss: 0.442\n",
      "Iteration: 97 \t--- Loss: 0.498\n",
      "Iteration: 98 \t--- Loss: 0.490\n",
      "Iteration: 99 \t--- Loss: 0.507\n",
      "Iteration: 100 \t--- Loss: 0.459\n",
      "Iteration: 101 \t--- Loss: 0.469\n",
      "Iteration: 102 \t--- Loss: 0.497\n",
      "Iteration: 103 \t--- Loss: 0.504\n",
      "Iteration: 104 \t--- Loss: 0.511\n",
      "Iteration: 105 \t--- Loss: 0.470\n",
      "Iteration: 106 \t--- Loss: 0.549\n",
      "Iteration: 107 \t--- Loss: 0.543\n",
      "Iteration: 108 \t--- Loss: 0.450\n",
      "Iteration: 109 \t--- Loss: 0.511\n",
      "Iteration: 110 \t--- Loss: 0.523\n",
      "Iteration: 111 \t--- Loss: 0.537\n",
      "Iteration: 112 \t--- Loss: 0.485\n",
      "Iteration: 113 \t--- Loss: 0.512\n",
      "Iteration: 114 \t--- Loss: 0.522\n",
      "Iteration: 115 \t--- Loss: 0.572\n",
      "Iteration: 116 \t--- Loss: 0.499\n",
      "Iteration: 117 \t--- Loss: 0.547\n",
      "Iteration: 118 \t--- Loss: 0.539\n",
      "Iteration: 119 \t--- Loss: 0.507\n",
      "Iteration: 120 \t--- Loss: 0.514\n",
      "Iteration: 121 \t--- Loss: 0.477\n",
      "Iteration: 122 \t--- Loss: 0.491\n",
      "Iteration: 123 \t--- Loss: 0.523\n",
      "Iteration: 124 \t--- Loss: 0.450\n",
      "Iteration: 125 \t--- Loss: 0.513\n",
      "Iteration: 126 \t--- Loss: 0.525\n",
      "Iteration: 127 \t--- Loss: 0.508\n",
      "Iteration: 128 \t--- Loss: 0.499\n",
      "Iteration: 129 \t--- Loss: 0.456\n",
      "Iteration: 130 \t--- Loss: 0.563\n",
      "Iteration: 131 \t--- Loss: 0.522\n",
      "Iteration: 132 \t--- Loss: 0.541\n",
      "Iteration: 133 \t--- Loss: 0.475\n",
      "Iteration: 134 \t--- Loss: 0.505\n",
      "Iteration: 135 \t--- Loss: 0.471\n",
      "Iteration: 136 \t--- Loss: 0.446\n",
      "Iteration: 137 \t--- Loss: 0.508\n",
      "Iteration: 138 \t--- Loss: 0.485\n",
      "Iteration: 139 \t--- Loss: 0.408\n",
      "Iteration: 140 \t--- Loss: 0.554\n",
      "Iteration: 141 \t--- Loss: 0.471\n",
      "Iteration: 142 \t--- Loss: 0.499\n",
      "Iteration: 143 \t--- Loss: 0.492\n",
      "Iteration: 144 \t--- Loss: 0.517\n",
      "Iteration: 145 \t--- Loss: 0.486\n",
      "Iteration: 146 \t--- Loss: 0.545\n",
      "Iteration: 147 \t--- Loss: 0.444\n",
      "Iteration: 148 \t--- Loss: 0.513\n",
      "Iteration: 149 \t--- Loss: 0.510\n",
      "Iteration: 150 \t--- Loss: 0.491\n",
      "Iteration: 151 \t--- Loss: 0.516\n",
      "Iteration: 152 \t--- Loss: 0.522\n",
      "Iteration: 153 \t--- Loss: 0.465\n",
      "Iteration: 154 \t--- Loss: 0.496\n",
      "Iteration: 155 \t--- Loss: 0.504\n",
      "Iteration: 156 \t--- Loss: 0.469\n",
      "Iteration: 157 \t--- Loss: 0.547\n",
      "Iteration: 158 \t--- Loss: 0.488\n",
      "Iteration: 159 \t--- Loss: 0.503\n",
      "Iteration: 160 \t--- Loss: 0.441\n",
      "Iteration: 161 \t--- Loss: 0.499\n",
      "Iteration: 162 \t--- Loss: 0.491\n",
      "Iteration: 163 \t--- Loss: 0.449\n",
      "Iteration: 164 \t--- Loss: 0.474\n",
      "Iteration: 165 \t--- Loss: 0.514\n",
      "Iteration: 166 \t--- Loss: 0.523\n",
      "Iteration: 167 \t--- Loss: 0.495\n",
      "Iteration: 168 \t--- Loss: 0.572\n",
      "Iteration: 169 \t--- Loss: 0.439\n",
      "Iteration: 170 \t--- Loss: 0.491\n",
      "Iteration: 171 \t--- Loss: 0.483\n",
      "Iteration: 172 \t--- Loss: 0.520\n",
      "Iteration: 173 \t--- Loss: 0.516\n",
      "Iteration: 174 \t--- Loss: 0.467\n",
      "Iteration: 175 \t--- Loss: 0.518\n",
      "Iteration: 176 \t--- Loss: 0.532\n",
      "Iteration: 177 \t--- Loss: 0.489\n",
      "Iteration: 178 \t--- Loss: 0.468\n",
      "Iteration: 179 \t--- Loss: 0.490\n",
      "Iteration: 180 \t--- Loss: 0.432\n",
      "Iteration: 181 \t--- Loss: 0.487\n",
      "Iteration: 182 \t--- Loss: 0.468\n",
      "Iteration: 183 \t--- Loss: 0.504\n",
      "Iteration: 184 \t--- Loss: 0.524\n",
      "Iteration: 185 \t--- Loss: 0.475\n",
      "Iteration: 186 \t--- Loss: 0.563\n",
      "Iteration: 187 \t--- Loss: 0.510\n",
      "Iteration: 188 \t--- Loss: 0.481\n",
      "Iteration: 189 \t--- Loss: 0.474\n",
      "Iteration: 190 \t--- Loss: 0.496\n",
      "Iteration: 191 \t--- Loss: 0.477\n",
      "Iteration: 192 \t--- Loss: 0.470\n",
      "Iteration: 193 \t--- Loss: 0.523\n",
      "Iteration: 194 \t--- Loss: 0.467\n",
      "Iteration: 195 \t--- Loss: 0.492\n",
      "Iteration: 196 \t--- Loss: 0.491\n",
      "Iteration: 197 \t--- Loss: 0.507\n",
      "Iteration: 198 \t--- Loss: 0.528\n",
      "Iteration: 199 \t--- Loss: 0.531\n",
      "Iteration: 200 \t--- Loss: 0.462\n",
      "Iteration: 201 \t--- Loss: 0.486\n",
      "Iteration: 202 \t--- Loss: 0.485\n",
      "Iteration: 203 \t--- Loss: 0.496\n",
      "Iteration: 204 \t--- Loss: 0.501\n",
      "Iteration: 205 \t--- Loss: 0.500\n",
      "Iteration: 206 \t--- Loss: 0.513\n",
      "Iteration: 207 \t--- Loss: 0.508\n",
      "Iteration: 208 \t--- Loss: 0.463\n",
      "Iteration: 209 \t--- Loss: 0.506\n",
      "Iteration: 210 \t--- Loss: 0.511\n",
      "Iteration: 211 \t--- Loss: 0.516\n",
      "Iteration: 212 \t--- Loss: 0.497\n",
      "Iteration: 213 \t--- Loss: 0.524\n",
      "Iteration: 214 \t--- Loss: 0.511\n",
      "Iteration: 215 \t--- Loss: 0.427\n",
      "Iteration: 216 \t--- Loss: 0.531\n",
      "Iteration: 217 \t--- Loss: 0.501\n",
      "Iteration: 218 \t--- Loss: 0.470\n",
      "Iteration: 219 \t--- Loss: 0.466\n",
      "Iteration: 220 \t--- Loss: 0.486\n",
      "Iteration: 221 \t--- Loss: 0.505\n",
      "Iteration: 222 \t--- Loss: 0.545\n",
      "Iteration: 223 \t--- Loss: 0.475\n",
      "Iteration: 224 \t--- Loss: 0.508\n",
      "Iteration: 225 \t--- Loss: 0.498\n",
      "Iteration: 226 \t--- Loss: 0.521\n",
      "Iteration: 227 \t--- Loss: 0.453\n",
      "Iteration: 228 \t--- Loss: 0.484\n",
      "Iteration: 229 \t--- Loss: 0.466\n",
      "Iteration: 230 \t--- Loss: 0.523\n",
      "Iteration: 231 \t--- Loss: 0.481\n",
      "Iteration: 232 \t--- Loss: 0.494\n",
      "Iteration: 233 \t--- Loss: 0.471\n",
      "Iteration: 234 \t--- Loss: 0.478\n",
      "Iteration: 235 \t--- Loss: 0.485\n",
      "Iteration: 236 \t--- Loss: 0.527\n",
      "Iteration: 237 \t--- Loss: 0.519\n",
      "Iteration: 238 \t--- Loss: 0.478\n",
      "Iteration: 239 \t--- Loss: 0.512\n",
      "Iteration: 240 \t--- Loss: 0.493\n",
      "Iteration: 241 \t--- Loss: 0.467\n",
      "Iteration: 242 \t--- Loss: 0.540\n",
      "Iteration: 243 \t--- Loss: 0.548\n",
      "Iteration: 244 \t--- Loss: 0.497\n",
      "Iteration: 245 \t--- Loss: 0.478\n",
      "Iteration: 246 \t--- Loss: 0.480\n",
      "Iteration: 247 \t--- Loss: 0.434\n",
      "Iteration: 248 \t--- Loss: 0.503\n",
      "Iteration: 249 \t--- Loss: 0.582\n",
      "Iteration: 250 \t--- Loss: 0.525\n",
      "Iteration: 251 \t--- Loss: 0.469\n",
      "Iteration: 252 \t--- Loss: 0.490\n",
      "Iteration: 253 \t--- Loss: 0.527\n",
      "Iteration: 254 \t--- Loss: 0.496\n",
      "Iteration: 255 \t--- Loss: 0.463\n",
      "Iteration: 256 \t--- Loss: 0.439\n",
      "Iteration: 257 \t--- Loss: 0.508\n",
      "Iteration: 258 \t--- Loss: 0.496\n",
      "Iteration: 259 \t--- Loss: 0.508"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:37<00:00, 97.34s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.044\n",
      "Iteration: 261 \t--- Loss: 0.044\n",
      "Iteration: 262 \t--- Loss: 0.044\n",
      "Iteration: 263 \t--- Loss: 0.042\n",
      "Iteration: 264 \t--- Loss: 0.043\n",
      "Iteration: 265 \t--- Loss: 0.044\n",
      "Iteration: 266 \t--- Loss: 0.044\n",
      "Iteration: 267 \t--- Loss: 0.043\n",
      "Iteration: 268 \t--- Loss: 0.045\n",
      "Iteration: 269 \t--- Loss: 0.044\n",
      "Iteration: 270 \t--- Loss: 0.046\n",
      "Iteration: 271 \t--- Loss: 0.044\n",
      "Iteration: 272 \t--- Loss: 0.043\n",
      "Iteration: 273 \t--- Loss: 0.043\n",
      "Iteration: 274 \t--- Loss: 0.039\n",
      "Iteration: 275 \t--- Loss: 0.041\n",
      "Iteration: 276 \t--- Loss: 0.043\n",
      "Iteration: 277 \t--- Loss: 0.043\n",
      "Iteration: 278 \t--- Loss: 0.043\n",
      "Iteration: 279 \t--- Loss: 0.041\n",
      "Iteration: 280 \t--- Loss: 0.041\n",
      "Iteration: 281 \t--- Loss: 0.041\n",
      "Iteration: 282 \t--- Loss: 0.041\n",
      "Iteration: 283 \t--- Loss: 0.040\n",
      "Iteration: 284 \t--- Loss: 0.041\n",
      "Iteration: 285 \t--- Loss: 0.044\n",
      "Iteration: 286 \t--- Loss: 0.044\n",
      "Iteration: 287 \t--- Loss: 0.041\n",
      "Iteration: 288 \t--- Loss: 0.041\n",
      "Iteration: 289 \t--- Loss: 0.039\n",
      "Iteration: 290 \t--- Loss: 0.045\n",
      "Iteration: 291 \t--- Loss: 0.040\n",
      "Iteration: 292 \t--- Loss: 0.043\n",
      "Iteration: 293 \t--- Loss: 0.044\n",
      "Iteration: 294 \t--- Loss: 0.046\n",
      "Iteration: 295 \t--- Loss: 0.044\n",
      "Iteration: 296 \t--- Loss: 0.042\n",
      "Iteration: 297 \t--- Loss: 0.042\n",
      "Iteration: 298 \t--- Loss: 0.041\n",
      "Iteration: 299 \t--- Loss: 0.037\n",
      "Iteration: 300 \t--- Loss: 0.038\n",
      "Iteration: 301 \t--- Loss: 0.039\n",
      "Iteration: 302 \t--- Loss: 0.047\n",
      "Iteration: 303 \t--- Loss: 0.038\n",
      "Iteration: 304 \t--- Loss: 0.044\n",
      "Iteration: 305 \t--- Loss: 0.040\n",
      "Iteration: 306 \t--- Loss: 0.043\n",
      "Iteration: 307 \t--- Loss: 0.039\n",
      "Iteration: 308 \t--- Loss: 0.039\n",
      "Iteration: 309 \t--- Loss: 0.046\n",
      "Iteration: 310 \t--- Loss: 0.043\n",
      "Iteration: 311 \t--- Loss: 0.045\n",
      "Iteration: 312 \t--- Loss: 0.042\n",
      "Iteration: 313 \t--- Loss: 0.037\n",
      "Iteration: 314 \t--- Loss: 0.043\n",
      "Iteration: 315 \t--- Loss: 0.039\n",
      "Iteration: 316 \t--- Loss: 0.039\n",
      "Iteration: 317 \t--- Loss: 0.042\n",
      "Iteration: 318 \t--- Loss: 0.041\n",
      "Iteration: 319 \t--- Loss: 0.042\n",
      "Iteration: 320 \t--- Loss: 0.040\n",
      "Iteration: 321 \t--- Loss: 0.040\n",
      "Iteration: 322 \t--- Loss: 0.043\n",
      "Iteration: 323 \t--- Loss: 0.039\n",
      "Iteration: 324 \t--- Loss: 0.039\n",
      "Iteration: 325 \t--- Loss: 0.042\n",
      "Iteration: 326 \t--- Loss: 0.038\n",
      "Iteration: 327 \t--- Loss: 0.044\n",
      "Iteration: 328 \t--- Loss: 0.042\n",
      "Iteration: 329 \t--- Loss: 0.037\n",
      "Iteration: 330 \t--- Loss: 0.042\n",
      "Iteration: 331 \t--- Loss: 0.039\n",
      "Iteration: 332 \t--- Loss: 0.039\n",
      "Iteration: 333 \t--- Loss: 0.038\n",
      "Iteration: 334 \t--- Loss: 0.039\n",
      "Iteration: 335 \t--- Loss: 0.039\n",
      "Iteration: 336 \t--- Loss: 0.035\n",
      "Iteration: 337 \t--- Loss: 0.042\n",
      "Iteration: 338 \t--- Loss: 0.043\n",
      "Iteration: 339 \t--- Loss: 0.039\n",
      "Iteration: 340 \t--- Loss: 0.043\n",
      "Iteration: 341 \t--- Loss: 0.041\n",
      "Iteration: 342 \t--- Loss: 0.038\n",
      "Iteration: 343 \t--- Loss: 0.039\n",
      "Iteration: 344 \t--- Loss: 0.043\n",
      "Iteration: 345 \t--- Loss: 0.043\n",
      "Iteration: 346 \t--- Loss: 0.037\n",
      "Iteration: 347 \t--- Loss: 0.038\n",
      "Iteration: 348 \t--- Loss: 0.040\n",
      "Iteration: 349 \t--- Loss: 0.043\n",
      "Iteration: 350 \t--- Loss: 0.044\n",
      "Iteration: 351 \t--- Loss: 0.038\n",
      "Iteration: 352 \t--- Loss: 0.041\n",
      "Iteration: 353 \t--- Loss: 0.040\n",
      "Iteration: 354 \t--- Loss: 0.042\n",
      "Iteration: 355 \t--- Loss: 0.043\n",
      "Iteration: 356 \t--- Loss: 0.039\n",
      "Iteration: 357 \t--- Loss: 0.042\n",
      "Iteration: 358 \t--- Loss: 0.043\n",
      "Iteration: 359 \t--- Loss: 0.037\n",
      "Iteration: 360 \t--- Loss: 0.040\n",
      "Iteration: 361 \t--- Loss: 0.037\n",
      "Iteration: 362 \t--- Loss: 0.038\n",
      "Iteration: 363 \t--- Loss: 0.038\n",
      "Iteration: 364 \t--- Loss: 0.039\n",
      "Iteration: 365 \t--- Loss: 0.043\n",
      "Iteration: 366 \t--- Loss: 0.041\n",
      "Iteration: 367 \t--- Loss: 0.037\n",
      "Iteration: 368 \t--- Loss: 0.041\n",
      "Iteration: 369 \t--- Loss: 0.036\n",
      "Iteration: 370 \t--- Loss: 0.037\n",
      "Iteration: 371 \t--- Loss: 0.043\n",
      "Iteration: 372 \t--- Loss: 0.042\n",
      "Iteration: 373 \t--- Loss: 0.040\n",
      "Iteration: 374 \t--- Loss: 0.039\n",
      "Iteration: 375 \t--- Loss: 0.037\n",
      "Iteration: 376 \t--- Loss: 0.044\n",
      "Iteration: 377 \t--- Loss: 0.043\n",
      "Iteration: 378 \t--- Loss: 0.040\n",
      "Iteration: 379 \t--- Loss: 0.039\n",
      "Iteration: 380 \t--- Loss: 0.038\n",
      "Iteration: 381 \t--- Loss: 0.037\n",
      "Iteration: 382 \t--- Loss: 0.042\n",
      "Iteration: 383 \t--- Loss: 0.038\n",
      "Iteration: 384 \t--- Loss: 0.040\n",
      "Iteration: 385 \t--- Loss: 0.038\n",
      "Iteration: 386 \t--- Loss: 0.040\n",
      "Iteration: 387 \t--- Loss: 0.038\n",
      "Iteration: 388 \t--- Loss: 0.039\n",
      "Iteration: 389 \t--- Loss: 0.038\n",
      "Iteration: 390 \t--- Loss: 0.041\n",
      "Iteration: 391 \t--- Loss: 0.039\n",
      "Iteration: 392 \t--- Loss: 0.036\n",
      "Iteration: 393 \t--- Loss: 0.039\n",
      "Iteration: 394 \t--- Loss: 0.040\n",
      "Iteration: 395 \t--- Loss: 0.037\n",
      "Iteration: 396 \t--- Loss: 0.037\n",
      "Iteration: 397 \t--- Loss: 0.036\n",
      "Iteration: 398 \t--- Loss: 0.037\n",
      "Iteration: 399 \t--- Loss: 0.037\n",
      "Iteration: 400 \t--- Loss: 0.039\n",
      "Iteration: 401 \t--- Loss: 0.036\n",
      "Iteration: 402 \t--- Loss: 0.037\n",
      "Iteration: 403 \t--- Loss: 0.042\n",
      "Iteration: 404 \t--- Loss: 0.038\n",
      "Iteration: 405 \t--- Loss: 0.039\n",
      "Iteration: 406 \t--- Loss: 0.039\n",
      "Iteration: 407 \t--- Loss: 0.037\n",
      "Iteration: 408 \t--- Loss: 0.040\n",
      "Iteration: 409 \t--- Loss: 0.035\n",
      "Iteration: 410 \t--- Loss: 0.037\n",
      "Iteration: 411 \t--- Loss: 0.039\n",
      "Iteration: 412 \t--- Loss: 0.035\n",
      "Iteration: 413 \t--- Loss: 0.036\n",
      "Iteration: 414 \t--- Loss: 0.037\n",
      "Iteration: 415 \t--- Loss: 0.037\n",
      "Iteration: 416 \t--- Loss: 0.037\n",
      "Iteration: 417 \t--- Loss: 0.041\n",
      "Iteration: 418 \t--- Loss: 0.037\n",
      "Iteration: 419 \t--- Loss: 0.035\n",
      "Iteration: 420 \t--- Loss: 0.036\n",
      "Iteration: 421 \t--- Loss: 0.040\n",
      "Iteration: 422 \t--- Loss: 0.037\n",
      "Iteration: 423 \t--- Loss: 0.038\n",
      "Iteration: 424 \t--- Loss: 0.038\n",
      "Iteration: 425 \t--- Loss: 0.037\n",
      "Iteration: 426 \t--- Loss: 0.035\n",
      "Iteration: 427 \t--- Loss: 0.040\n",
      "Iteration: 428 \t--- Loss: 0.038\n",
      "Iteration: 429 \t--- Loss: 0.035\n",
      "Iteration: 430 \t--- Loss: 0.038\n",
      "Iteration: 431 \t--- Loss: 0.041\n",
      "Iteration: 432 \t--- Loss: 0.039\n",
      "Iteration: 433 \t--- Loss: 0.036\n",
      "Iteration: 434 \t--- Loss: 0.037\n",
      "Iteration: 435 \t--- Loss: 0.040\n",
      "Iteration: 436 \t--- Loss: 0.035\n",
      "Iteration: 437 \t--- Loss: 0.039\n",
      "Iteration: 438 \t--- Loss: 0.039\n",
      "Iteration: 439 \t--- Loss: 0.039\n",
      "Iteration: 440 \t--- Loss: 0.035\n",
      "Iteration: 441 \t--- Loss: 0.039\n",
      "Iteration: 442 \t--- Loss: 0.039\n",
      "Iteration: 443 \t--- Loss: 0.038\n",
      "Iteration: 444 \t--- Loss: 0.037\n",
      "Iteration: 445 \t--- Loss: 0.035\n",
      "Iteration: 446 \t--- Loss: 0.034\n",
      "Iteration: 447 \t--- Loss: 0.038\n",
      "Iteration: 448 \t--- Loss: 0.040\n",
      "Iteration: 449 \t--- Loss: 0.037\n",
      "Iteration: 450 \t--- Loss: 0.037\n",
      "Iteration: 451 \t--- Loss: 0.037\n",
      "Iteration: 452 \t--- Loss: 0.036\n",
      "Iteration: 453 \t--- Loss: 0.038\n",
      "Iteration: 454 \t--- Loss: 0.037\n",
      "Iteration: 455 \t--- Loss: 0.040\n",
      "Iteration: 456 \t--- Loss: 0.036\n",
      "Iteration: 457 \t--- Loss: 0.037\n",
      "Iteration: 458 \t--- Loss: 0.033\n",
      "Iteration: 459 \t--- Loss: 0.036\n",
      "Iteration: 460 \t--- Loss: 0.039\n",
      "Iteration: 461 \t--- Loss: 0.036\n",
      "Iteration: 462 \t--- Loss: 0.034\n",
      "Iteration: 463 \t--- Loss: 0.037\n",
      "Iteration: 464 \t--- Loss: 0.037\n",
      "Iteration: 465 \t--- Loss: 0.036\n",
      "Iteration: 466 \t--- Loss: 0.034\n",
      "Iteration: 467 \t--- Loss: 0.033\n",
      "Iteration: 468 \t--- Loss: 0.033\n",
      "Iteration: 469 \t--- Loss: 0.037\n",
      "Iteration: 470 \t--- Loss: 0.032\n",
      "Iteration: 471 \t--- Loss: 0.036\n",
      "Iteration: 472 \t--- Loss: 0.035\n",
      "Iteration: 473 \t--- Loss: 0.038\n",
      "Iteration: 474 \t--- Loss: 0.037\n",
      "Iteration: 475 \t--- Loss: 0.040\n",
      "Iteration: 476 \t--- Loss: 0.034\n",
      "Iteration: 477 \t--- Loss: 0.038\n",
      "Iteration: 478 \t--- Loss: 0.034\n",
      "Iteration: 479 \t--- Loss: 0.037\n",
      "Iteration: 480 \t--- Loss: 0.034\n",
      "Iteration: 481 \t--- Loss: 0.035\n",
      "Iteration: 482 \t--- Loss: 0.035\n",
      "Iteration: 483 \t--- Loss: 0.035\n",
      "Iteration: 484 \t--- Loss: 0.035\n",
      "Iteration: 485 \t--- Loss: 0.034\n",
      "Iteration: 486 \t--- Loss: 0.031\n",
      "Iteration: 487 \t--- Loss: 0.033\n",
      "Iteration: 488 \t--- Loss: 0.033\n",
      "Iteration: 489 \t--- Loss: 0.033\n",
      "Iteration: 490 \t--- Loss: 0.035\n",
      "Iteration: 491 \t--- Loss: 0.033\n",
      "Iteration: 492 \t--- Loss: 0.033\n",
      "Iteration: 493 \t--- Loss: 0.034\n",
      "Iteration: 494 \t--- Loss: 0.031\n",
      "Iteration: 495 \t--- Loss: 0.033\n",
      "Iteration: 496 \t--- Loss: 0.033\n",
      "Iteration: 497 \t--- Loss: 0.034\n",
      "Iteration: 498 \t--- Loss: 0.035\n",
      "Iteration: 499 \t--- Loss: 0.031\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:07,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.306\n",
      "Iteration: 1 \t--- Loss: 0.310\n",
      "Iteration: 2 \t--- Loss: 0.301\n",
      "Iteration: 3 \t--- Loss: 0.298\n",
      "Iteration: 4 \t--- Loss: 0.314\n",
      "Iteration: 5 \t--- Loss: 0.300\n",
      "Iteration: 6 \t--- Loss: 0.299\n",
      "Iteration: 7 \t--- Loss: 0.330\n",
      "Iteration: 8 \t--- Loss: 0.301\n",
      "Iteration: 9 \t--- Loss: 0.299\n",
      "Iteration: 10 \t--- Loss: 0.302\n",
      "Iteration: 11 \t--- Loss: 0.300\n",
      "Iteration: 12 \t--- Loss: 0.329\n",
      "Iteration: 13 \t--- Loss: 0.298\n",
      "Iteration: 14 \t--- Loss: 0.308\n",
      "Iteration: 15 \t--- Loss: 0.265\n",
      "Iteration: 16 \t--- Loss: 0.300\n",
      "Iteration: 17 \t--- Loss: 0.269\n",
      "Iteration: 18 \t--- Loss: 0.322\n",
      "Iteration: 19 \t--- Loss: 0.288\n",
      "Iteration: 20 \t--- Loss: 0.295\n",
      "Iteration: 21 \t--- Loss: 0.274\n",
      "Iteration: 22 \t--- Loss: 0.285\n",
      "Iteration: 23 \t--- Loss: 0.294\n",
      "Iteration: 24 \t--- Loss: 0.300\n",
      "Iteration: 25 \t--- Loss: 0.291\n",
      "Iteration: 26 \t--- Loss: 0.317\n",
      "Iteration: 27 \t--- Loss: 0.297\n",
      "Iteration: 28 \t--- Loss: 0.310\n",
      "Iteration: 29 \t--- Loss: 0.266\n",
      "Iteration: 30 \t--- Loss: 0.301\n",
      "Iteration: 31 \t--- Loss: 0.296\n",
      "Iteration: 32 \t--- Loss: 0.279\n",
      "Iteration: 33 \t--- Loss: 0.299\n",
      "Iteration: 34 \t--- Loss: 0.269\n",
      "Iteration: 35 \t--- Loss: 0.288\n",
      "Iteration: 36 \t--- Loss: 0.272\n",
      "Iteration: 37 \t--- Loss: 0.298\n",
      "Iteration: 38 \t--- Loss: 0.293\n",
      "Iteration: 39 \t--- Loss: 0.277\n",
      "Iteration: 40 \t--- Loss: 0.285\n",
      "Iteration: 41 \t--- Loss: 0.268\n",
      "Iteration: 42 \t--- Loss: 0.299\n",
      "Iteration: 43 \t--- Loss: 0.292\n",
      "Iteration: 44 \t--- Loss: 0.289\n",
      "Iteration: 45 \t--- Loss: 0.290\n",
      "Iteration: 46 \t--- Loss: 0.312\n",
      "Iteration: 47 \t--- Loss: 0.304\n",
      "Iteration: 48 \t--- Loss: 0.302\n",
      "Iteration: 49 \t--- Loss: 0.301\n",
      "Iteration: 50 \t--- Loss: 0.280\n",
      "Iteration: 51 \t--- Loss: 0.272\n",
      "Iteration: 52 \t--- Loss: 0.303\n",
      "Iteration: 53 \t--- Loss: 0.288\n",
      "Iteration: 54 \t--- Loss: 0.312\n",
      "Iteration: 55 \t--- Loss: 0.295\n",
      "Iteration: 56 \t--- Loss: 0.282\n",
      "Iteration: 57 \t--- Loss: 0.297\n",
      "Iteration: 58 \t--- Loss: 0.328\n",
      "Iteration: 59 \t--- Loss: 0.275\n",
      "Iteration: 60 \t--- Loss: 0.288\n",
      "Iteration: 61 \t--- Loss: 0.312\n",
      "Iteration: 62 \t--- Loss: 0.307\n",
      "Iteration: 63 \t--- Loss: 0.265\n",
      "Iteration: 64 \t--- Loss: 0.293\n",
      "Iteration: 65 \t--- Loss: 0.287\n",
      "Iteration: 66 \t--- Loss: 0.287\n",
      "Iteration: 67 \t--- Loss: 0.298\n",
      "Iteration: 68 \t--- Loss: 0.278\n",
      "Iteration: 69 \t--- Loss: 0.298\n",
      "Iteration: 70 \t--- Loss: 0.292\n",
      "Iteration: 71 \t--- Loss: 0.300\n",
      "Iteration: 72 \t--- Loss: 0.286\n",
      "Iteration: 73 \t--- Loss: 0.280\n",
      "Iteration: 74 \t--- Loss: 0.297\n",
      "Iteration: 75 \t--- Loss: 0.290\n",
      "Iteration: 76 \t--- Loss: 0.305\n",
      "Iteration: 77 \t--- Loss: 0.260\n",
      "Iteration: 78 \t--- Loss: 0.286\n",
      "Iteration: 79 \t--- Loss: 0.294\n",
      "Iteration: 80 \t--- Loss: 0.288\n",
      "Iteration: 81 \t--- Loss: 0.281\n",
      "Iteration: 82 \t--- Loss: 0.322\n",
      "Iteration: 83 \t--- Loss: 0.292\n",
      "Iteration: 84 \t--- Loss: 0.296\n",
      "Iteration: 85 \t--- Loss: 0.282\n",
      "Iteration: 86 \t--- Loss: 0.290\n",
      "Iteration: 87 \t--- Loss: 0.286\n",
      "Iteration: 88 \t--- Loss: 0.297\n",
      "Iteration: 89 \t--- Loss: 0.271\n",
      "Iteration: 90 \t--- Loss: 0.310\n",
      "Iteration: 91 \t--- Loss: 0.262\n",
      "Iteration: 92 \t--- Loss: 0.251\n",
      "Iteration: 93 \t--- Loss: 0.283\n",
      "Iteration: 94 \t--- Loss: 0.269\n",
      "Iteration: 95 \t--- Loss: 0.288\n",
      "Iteration: 96 \t--- Loss: 0.307\n",
      "Iteration: 97 \t--- Loss: 0.287\n",
      "Iteration: 98 \t--- Loss: 0.276\n",
      "Iteration: 99 \t--- Loss: 0.316\n",
      "Iteration: 100 \t--- Loss: 0.284\n",
      "Iteration: 101 \t--- Loss: 0.291\n",
      "Iteration: 102 \t--- Loss: 0.282\n",
      "Iteration: 103 \t--- Loss: 0.295\n",
      "Iteration: 104 \t--- Loss: 0.278\n",
      "Iteration: 105 \t--- Loss: 0.276\n",
      "Iteration: 106 \t--- Loss: 0.300\n",
      "Iteration: 107 \t--- Loss: 0.301\n",
      "Iteration: 108 \t--- Loss: 0.303\n",
      "Iteration: 109 \t--- Loss: 0.277\n",
      "Iteration: 110 \t--- Loss: 0.295\n",
      "Iteration: 111 \t--- Loss: 0.303\n",
      "Iteration: 112 \t--- Loss: 0.280\n",
      "Iteration: 113 \t--- Loss: 0.276\n",
      "Iteration: 114 \t--- Loss: 0.299\n",
      "Iteration: 115 \t--- Loss: 0.293\n",
      "Iteration: 116 \t--- Loss: 0.287\n",
      "Iteration: 117 \t--- Loss: 0.260\n",
      "Iteration: 118 \t--- Loss: 0.318\n",
      "Iteration: 119 \t--- Loss: 0.323\n",
      "Iteration: 120 \t--- Loss: 0.290\n",
      "Iteration: 121 \t--- Loss: 0.294\n",
      "Iteration: 122 \t--- Loss: 0.280\n",
      "Iteration: 123 \t--- Loss: 0.281\n",
      "Iteration: 124 \t--- Loss: 0.300\n",
      "Iteration: 125 \t--- Loss: 0.287\n",
      "Iteration: 126 \t--- Loss: 0.298\n",
      "Iteration: 127 \t--- Loss: 0.273\n",
      "Iteration: 128 \t--- Loss: 0.261\n",
      "Iteration: 129 \t--- Loss: 0.307\n",
      "Iteration: 130 \t--- Loss: 0.302\n",
      "Iteration: 131 \t--- Loss: 0.287\n",
      "Iteration: 132 \t--- Loss: 0.268\n",
      "Iteration: 133 \t--- Loss: 0.297\n",
      "Iteration: 134 \t--- Loss: 0.270\n",
      "Iteration: 135 \t--- Loss: 0.303\n",
      "Iteration: 136 \t--- Loss: 0.285\n",
      "Iteration: 137 \t--- Loss: 0.287\n",
      "Iteration: 138 \t--- Loss: 0.296\n",
      "Iteration: 139 \t--- Loss: 0.270\n",
      "Iteration: 140 \t--- Loss: 0.283\n",
      "Iteration: 141 \t--- Loss: 0.309\n",
      "Iteration: 142 \t--- Loss: 0.316\n",
      "Iteration: 143 \t--- Loss: 0.254\n",
      "Iteration: 144 \t--- Loss: 0.269\n",
      "Iteration: 145 \t--- Loss: 0.291\n",
      "Iteration: 146 \t--- Loss: 0.280\n",
      "Iteration: 147 \t--- Loss: 0.268\n",
      "Iteration: 148 \t--- Loss: 0.283\n",
      "Iteration: 149 \t--- Loss: 0.297\n",
      "Iteration: 150 \t--- Loss: 0.269\n",
      "Iteration: 151 \t--- Loss: 0.296\n",
      "Iteration: 152 \t--- Loss: 0.298\n",
      "Iteration: 153 \t--- Loss: 0.275\n",
      "Iteration: 154 \t--- Loss: 0.304\n",
      "Iteration: 155 \t--- Loss: 0.281\n",
      "Iteration: 156 \t--- Loss: 0.271\n",
      "Iteration: 157 \t--- Loss: 0.260\n",
      "Iteration: 158 \t--- Loss: 0.306\n",
      "Iteration: 159 \t--- Loss: 0.277\n",
      "Iteration: 160 \t--- Loss: 0.292\n",
      "Iteration: 161 \t--- Loss: 0.301\n",
      "Iteration: 162 \t--- Loss: 0.272\n",
      "Iteration: 163 \t--- Loss: 0.272\n",
      "Iteration: 164 \t--- Loss: 0.313\n",
      "Iteration: 165 \t--- Loss: 0.279\n",
      "Iteration: 166 \t--- Loss: 0.283\n",
      "Iteration: 167 \t--- Loss: 0.284\n",
      "Iteration: 168 \t--- Loss: 0.277\n",
      "Iteration: 169 \t--- Loss: 0.263\n",
      "Iteration: 170 \t--- Loss: 0.294\n",
      "Iteration: 171 \t--- Loss: 0.318\n",
      "Iteration: 172 \t--- Loss: 0.283\n",
      "Iteration: 173 \t--- Loss: 0.300\n",
      "Iteration: 174 \t--- Loss: 0.302\n",
      "Iteration: 175 \t--- Loss: 0.312\n",
      "Iteration: 176 \t--- Loss: 0.269\n",
      "Iteration: 177 \t--- Loss: 0.263\n",
      "Iteration: 178 \t--- Loss: 0.287\n",
      "Iteration: 179 \t--- Loss: 0.287\n",
      "Iteration: 180 \t--- Loss: 0.287\n",
      "Iteration: 181 \t--- Loss: 0.280\n",
      "Iteration: 182 \t--- Loss: 0.300\n",
      "Iteration: 183 \t--- Loss: 0.283\n",
      "Iteration: 184 \t--- Loss: 0.295\n",
      "Iteration: 185 \t--- Loss: 0.290\n",
      "Iteration: 186 \t--- Loss: 0.311\n",
      "Iteration: 187 \t--- Loss: 0.307\n",
      "Iteration: 188 \t--- Loss: 0.283\n",
      "Iteration: 189 \t--- Loss: 0.318\n",
      "Iteration: 190 \t--- Loss: 0.297\n",
      "Iteration: 191 \t--- Loss: 0.295\n",
      "Iteration: 192 \t--- Loss: 0.310\n",
      "Iteration: 193 \t--- Loss: 0.303\n",
      "Iteration: 194 \t--- Loss: 0.300\n",
      "Iteration: 195 \t--- Loss: 0.290\n",
      "Iteration: 196 \t--- Loss: 0.280\n",
      "Iteration: 197 \t--- Loss: 0.292\n",
      "Iteration: 198 \t--- Loss: 0.292\n",
      "Iteration: 199 \t--- Loss: 0.297\n",
      "Iteration: 200 \t--- Loss: 0.292\n",
      "Iteration: 201 \t--- Loss: 0.290\n",
      "Iteration: 202 \t--- Loss: 0.300\n",
      "Iteration: 203 \t--- Loss: 0.290\n",
      "Iteration: 204 \t--- Loss: 0.312\n",
      "Iteration: 205 \t--- Loss: 0.277\n",
      "Iteration: 206 \t--- Loss: 0.288\n",
      "Iteration: 207 \t--- Loss: 0.287\n",
      "Iteration: 208 \t--- Loss: 0.274\n",
      "Iteration: 209 \t--- Loss: 0.285\n",
      "Iteration: 210 \t--- Loss: 0.282\n",
      "Iteration: 211 \t--- Loss: 0.279\n",
      "Iteration: 212 \t--- Loss: 0.301\n",
      "Iteration: 213 \t--- Loss: 0.270\n",
      "Iteration: 214 \t--- Loss: 0.297\n",
      "Iteration: 215 \t--- Loss: 0.292\n",
      "Iteration: 216 \t--- Loss: 0.281\n",
      "Iteration: 217 \t--- Loss: 0.316\n",
      "Iteration: 218 \t--- Loss: 0.278\n",
      "Iteration: 219 \t--- Loss: 0.286\n",
      "Iteration: 220 \t--- Loss: 0.253\n",
      "Iteration: 221 \t--- Loss: 0.267\n",
      "Iteration: 222 \t--- Loss: 0.285\n",
      "Iteration: 223 \t--- Loss: 0.317\n",
      "Iteration: 224 \t--- Loss: 0.269\n",
      "Iteration: 225 \t--- Loss: 0.301\n",
      "Iteration: 226 \t--- Loss: 0.311\n",
      "Iteration: 227 \t--- Loss: 0.282\n",
      "Iteration: 228 \t--- Loss: 0.295\n",
      "Iteration: 229 \t--- Loss: 0.302\n",
      "Iteration: 230 \t--- Loss: 0.302\n",
      "Iteration: 231 \t--- Loss: 0.288\n",
      "Iteration: 232 \t--- Loss: 0.281\n",
      "Iteration: 233 \t--- Loss: 0.271\n",
      "Iteration: 234 \t--- Loss: 0.270\n",
      "Iteration: 235 \t--- Loss: 0.292\n",
      "Iteration: 236 \t--- Loss: 0.277\n",
      "Iteration: 237 \t--- Loss: 0.287\n",
      "Iteration: 238 \t--- Loss: 0.270\n",
      "Iteration: 239 \t--- Loss: 0.295\n",
      "Iteration: 240 \t--- Loss: 0.299\n",
      "Iteration: 241 \t--- Loss: 0.266\n",
      "Iteration: 242 \t--- Loss: 0.305\n",
      "Iteration: 243 \t--- Loss: 0.302\n",
      "Iteration: 244 \t--- Loss: 0.295\n",
      "Iteration: 245 \t--- Loss: 0.289\n",
      "Iteration: 246 \t--- Loss: 0.295\n",
      "Iteration: 247 \t--- Loss: 0.286\n",
      "Iteration: 248 \t--- Loss: 0.295\n",
      "Iteration: 249 \t--- Loss: 0.275\n",
      "Iteration: 250 \t--- Loss: 0.282\n",
      "Iteration: 251 \t--- Loss: 0.315\n",
      "Iteration: 252 \t--- Loss: 0.296\n",
      "Iteration: 253 \t--- Loss: 0.290\n",
      "Iteration: 254 \t--- Loss: 0.278\n",
      "Iteration: 255 \t--- Loss: 0.306\n",
      "Iteration: 256 \t--- Loss: 0.319\n",
      "Iteration: 257 \t--- Loss: 0.293\n",
      "Iteration: 258 \t--- Loss: 0.294\n",
      "Iteration: 259 \t--- Loss: 0.295"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:33<00:00, 93.67s/it]t]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.028\n",
      "Iteration: 261 \t--- Loss: 0.040\n",
      "Iteration: 262 \t--- Loss: 0.020\n",
      "Iteration: 263 \t--- Loss: 0.028\n",
      "Iteration: 264 \t--- Loss: 0.021\n",
      "Iteration: 265 \t--- Loss: 0.024\n",
      "Iteration: 266 \t--- Loss: 0.022\n",
      "Iteration: 267 \t--- Loss: 0.029\n",
      "Iteration: 268 \t--- Loss: 0.018\n",
      "Iteration: 269 \t--- Loss: 0.019\n",
      "Iteration: 270 \t--- Loss: 0.018\n",
      "Iteration: 271 \t--- Loss: 0.017\n",
      "Iteration: 272 \t--- Loss: 0.015\n",
      "Iteration: 273 \t--- Loss: 0.015\n",
      "Iteration: 274 \t--- Loss: 0.018\n",
      "Iteration: 275 \t--- Loss: 0.021\n",
      "Iteration: 276 \t--- Loss: 0.018\n",
      "Iteration: 277 \t--- Loss: 0.025\n",
      "Iteration: 278 \t--- Loss: 0.023\n",
      "Iteration: 279 \t--- Loss: 0.033\n",
      "Iteration: 280 \t--- Loss: 0.018\n",
      "Iteration: 281 \t--- Loss: 0.022\n",
      "Iteration: 282 \t--- Loss: 0.012\n",
      "Iteration: 283 \t--- Loss: 0.012\n",
      "Iteration: 284 \t--- Loss: 0.013\n",
      "Iteration: 285 \t--- Loss: 0.014\n",
      "Iteration: 286 \t--- Loss: 0.016\n",
      "Iteration: 287 \t--- Loss: 0.015\n",
      "Iteration: 288 \t--- Loss: 0.014\n",
      "Iteration: 289 \t--- Loss: 0.017\n",
      "Iteration: 290 \t--- Loss: 0.016\n",
      "Iteration: 291 \t--- Loss: 0.017\n",
      "Iteration: 292 \t--- Loss: 0.012\n",
      "Iteration: 293 \t--- Loss: 0.012\n",
      "Iteration: 294 \t--- Loss: 0.013\n",
      "Iteration: 295 \t--- Loss: 0.014\n",
      "Iteration: 296 \t--- Loss: 0.012\n",
      "Iteration: 297 \t--- Loss: 0.011\n",
      "Iteration: 298 \t--- Loss: 0.012\n",
      "Iteration: 299 \t--- Loss: 0.011\n",
      "Iteration: 300 \t--- Loss: 0.012\n",
      "Iteration: 301 \t--- Loss: 0.014\n",
      "Iteration: 302 \t--- Loss: 0.014\n",
      "Iteration: 303 \t--- Loss: 0.017\n",
      "Iteration: 304 \t--- Loss: 0.012\n",
      "Iteration: 305 \t--- Loss: 0.012\n",
      "Iteration: 306 \t--- Loss: 0.011\n",
      "Iteration: 307 \t--- Loss: 0.011\n",
      "Iteration: 308 \t--- Loss: 0.010\n",
      "Iteration: 309 \t--- Loss: 0.011\n",
      "Iteration: 310 \t--- Loss: 0.009\n",
      "Iteration: 311 \t--- Loss: 0.011\n",
      "Iteration: 312 \t--- Loss: 0.009\n",
      "Iteration: 313 \t--- Loss: 0.009\n",
      "Iteration: 314 \t--- Loss: 0.010\n",
      "Iteration: 315 \t--- Loss: 0.009\n",
      "Iteration: 316 \t--- Loss: 0.011\n",
      "Iteration: 317 \t--- Loss: 0.011\n",
      "Iteration: 318 \t--- Loss: 0.012\n",
      "Iteration: 319 \t--- Loss: 0.013\n",
      "Iteration: 320 \t--- Loss: 0.014\n",
      "Iteration: 321 \t--- Loss: 0.014\n",
      "Iteration: 322 \t--- Loss: 0.014\n",
      "Iteration: 323 \t--- Loss: 0.014\n",
      "Iteration: 324 \t--- Loss: 0.012\n",
      "Iteration: 325 \t--- Loss: 0.013\n",
      "Iteration: 326 \t--- Loss: 0.010\n",
      "Iteration: 327 \t--- Loss: 0.011\n",
      "Iteration: 328 \t--- Loss: 0.008\n",
      "Iteration: 329 \t--- Loss: 0.008\n",
      "Iteration: 330 \t--- Loss: 0.008\n",
      "Iteration: 331 \t--- Loss: 0.007\n",
      "Iteration: 332 \t--- Loss: 0.008\n",
      "Iteration: 333 \t--- Loss: 0.007\n",
      "Iteration: 334 \t--- Loss: 0.008\n",
      "Iteration: 335 \t--- Loss: 0.008\n",
      "Iteration: 336 \t--- Loss: 0.009\n",
      "Iteration: 337 \t--- Loss: 0.008\n",
      "Iteration: 338 \t--- Loss: 0.009\n",
      "Iteration: 339 \t--- Loss: 0.008\n",
      "Iteration: 340 \t--- Loss: 0.009\n",
      "Iteration: 341 \t--- Loss: 0.008\n",
      "Iteration: 342 \t--- Loss: 0.009\n",
      "Iteration: 343 \t--- Loss: 0.009\n",
      "Iteration: 344 \t--- Loss: 0.010\n",
      "Iteration: 345 \t--- Loss: 0.009\n",
      "Iteration: 346 \t--- Loss: 0.010\n",
      "Iteration: 347 \t--- Loss: 0.011\n",
      "Iteration: 348 \t--- Loss: 0.010\n",
      "Iteration: 349 \t--- Loss: 0.010\n",
      "Iteration: 350 \t--- Loss: 0.011\n",
      "Iteration: 351 \t--- Loss: 0.009\n",
      "Iteration: 352 \t--- Loss: 0.008\n",
      "Iteration: 353 \t--- Loss: 0.008\n",
      "Iteration: 354 \t--- Loss: 0.009\n",
      "Iteration: 355 \t--- Loss: 0.008\n",
      "Iteration: 356 \t--- Loss: 0.009\n",
      "Iteration: 357 \t--- Loss: 0.008\n",
      "Iteration: 358 \t--- Loss: 0.009\n",
      "Iteration: 359 \t--- Loss: 0.009\n",
      "Iteration: 360 \t--- Loss: 0.010\n",
      "Iteration: 361 \t--- Loss: 0.009\n",
      "Iteration: 362 \t--- Loss: 0.009\n",
      "Iteration: 363 \t--- Loss: 0.011\n",
      "Iteration: 364 \t--- Loss: 0.013\n",
      "Iteration: 365 \t--- Loss: 0.015\n",
      "Iteration: 366 \t--- Loss: 0.012\n",
      "Iteration: 367 \t--- Loss: 0.012\n",
      "Iteration: 368 \t--- Loss: 0.007\n",
      "Iteration: 369 \t--- Loss: 0.007\n",
      "Iteration: 370 \t--- Loss: 0.008\n",
      "Iteration: 371 \t--- Loss: 0.007\n",
      "Iteration: 372 \t--- Loss: 0.007\n",
      "Iteration: 373 \t--- Loss: 0.008\n",
      "Iteration: 374 \t--- Loss: 0.009\n",
      "Iteration: 375 \t--- Loss: 0.009\n",
      "Iteration: 376 \t--- Loss: 0.008\n",
      "Iteration: 377 \t--- Loss: 0.008\n",
      "Iteration: 378 \t--- Loss: 0.007\n",
      "Iteration: 379 \t--- Loss: 0.008\n",
      "Iteration: 380 \t--- Loss: 0.008\n",
      "Iteration: 381 \t--- Loss: 0.008\n",
      "Iteration: 382 \t--- Loss: 0.007\n",
      "Iteration: 383 \t--- Loss: 0.007\n",
      "Iteration: 384 \t--- Loss: 0.007\n",
      "Iteration: 385 \t--- Loss: 0.008\n",
      "Iteration: 386 \t--- Loss: 0.007\n",
      "Iteration: 387 \t--- Loss: 0.007\n",
      "Iteration: 388 \t--- Loss: 0.007\n",
      "Iteration: 389 \t--- Loss: 0.008\n",
      "Iteration: 390 \t--- Loss: 0.007\n",
      "Iteration: 391 \t--- Loss: 0.007\n",
      "Iteration: 392 \t--- Loss: 0.007\n",
      "Iteration: 393 \t--- Loss: 0.006\n",
      "Iteration: 394 \t--- Loss: 0.006\n",
      "Iteration: 395 \t--- Loss: 0.006\n",
      "Iteration: 396 \t--- Loss: 0.007\n",
      "Iteration: 397 \t--- Loss: 0.007\n",
      "Iteration: 398 \t--- Loss: 0.006\n",
      "Iteration: 399 \t--- Loss: 0.006\n",
      "Iteration: 400 \t--- Loss: 0.006\n",
      "Iteration: 401 \t--- Loss: 0.007\n",
      "Iteration: 402 \t--- Loss: 0.006\n",
      "Iteration: 403 \t--- Loss: 0.006\n",
      "Iteration: 404 \t--- Loss: 0.006\n",
      "Iteration: 405 \t--- Loss: 0.006\n",
      "Iteration: 406 \t--- Loss: 0.006\n",
      "Iteration: 407 \t--- Loss: 0.006\n",
      "Iteration: 408 \t--- Loss: 0.006\n",
      "Iteration: 409 \t--- Loss: 0.006\n",
      "Iteration: 410 \t--- Loss: 0.006\n",
      "Iteration: 411 \t--- Loss: 0.006\n",
      "Iteration: 412 \t--- Loss: 0.006\n",
      "Iteration: 413 \t--- Loss: 0.006\n",
      "Iteration: 414 \t--- Loss: 0.006\n",
      "Iteration: 415 \t--- Loss: 0.006\n",
      "Iteration: 416 \t--- Loss: 0.006\n",
      "Iteration: 417 \t--- Loss: 0.006\n",
      "Iteration: 418 \t--- Loss: 0.006\n",
      "Iteration: 419 \t--- Loss: 0.006\n",
      "Iteration: 420 \t--- Loss: 0.006\n",
      "Iteration: 421 \t--- Loss: 0.007\n",
      "Iteration: 422 \t--- Loss: 0.006\n",
      "Iteration: 423 \t--- Loss: 0.006\n",
      "Iteration: 424 \t--- Loss: 0.006\n",
      "Iteration: 425 \t--- Loss: 0.006\n",
      "Iteration: 426 \t--- Loss: 0.006\n",
      "Iteration: 427 \t--- Loss: 0.006\n",
      "Iteration: 428 \t--- Loss: 0.006\n",
      "Iteration: 429 \t--- Loss: 0.005\n",
      "Iteration: 430 \t--- Loss: 0.006\n",
      "Iteration: 431 \t--- Loss: 0.007\n",
      "Iteration: 432 \t--- Loss: 0.006\n",
      "Iteration: 433 \t--- Loss: 0.006\n",
      "Iteration: 434 \t--- Loss: 0.006\n",
      "Iteration: 435 \t--- Loss: 0.006\n",
      "Iteration: 436 \t--- Loss: 0.006\n",
      "Iteration: 437 \t--- Loss: 0.006\n",
      "Iteration: 438 \t--- Loss: 0.006\n",
      "Iteration: 439 \t--- Loss: 0.006\n",
      "Iteration: 440 \t--- Loss: 0.006\n",
      "Iteration: 441 \t--- Loss: 0.006\n",
      "Iteration: 442 \t--- Loss: 0.006\n",
      "Iteration: 443 \t--- Loss: 0.006\n",
      "Iteration: 444 \t--- Loss: 0.006\n",
      "Iteration: 445 \t--- Loss: 0.006\n",
      "Iteration: 446 \t--- Loss: 0.005\n",
      "Iteration: 447 \t--- Loss: 0.005\n",
      "Iteration: 448 \t--- Loss: 0.006\n",
      "Iteration: 449 \t--- Loss: 0.006\n",
      "Iteration: 450 \t--- Loss: 0.006\n",
      "Iteration: 451 \t--- Loss: 0.005\n",
      "Iteration: 452 \t--- Loss: 0.006\n",
      "Iteration: 453 \t--- Loss: 0.005\n",
      "Iteration: 454 \t--- Loss: 0.005\n",
      "Iteration: 455 \t--- Loss: 0.006\n",
      "Iteration: 456 \t--- Loss: 0.006\n",
      "Iteration: 457 \t--- Loss: 0.006\n",
      "Iteration: 458 \t--- Loss: 0.006\n",
      "Iteration: 459 \t--- Loss: 0.005\n",
      "Iteration: 460 \t--- Loss: 0.006\n",
      "Iteration: 461 \t--- Loss: 0.005\n",
      "Iteration: 462 \t--- Loss: 0.006\n",
      "Iteration: 463 \t--- Loss: 0.006\n",
      "Iteration: 464 \t--- Loss: 0.005\n",
      "Iteration: 465 \t--- Loss: 0.005\n",
      "Iteration: 466 \t--- Loss: 0.005\n",
      "Iteration: 467 \t--- Loss: 0.006\n",
      "Iteration: 468 \t--- Loss: 0.005\n",
      "Iteration: 469 \t--- Loss: 0.006\n",
      "Iteration: 470 \t--- Loss: 0.006\n",
      "Iteration: 471 \t--- Loss: 0.006\n",
      "Iteration: 472 \t--- Loss: 0.005\n",
      "Iteration: 473 \t--- Loss: 0.005\n",
      "Iteration: 474 \t--- Loss: 0.006\n",
      "Iteration: 475 \t--- Loss: 0.006\n",
      "Iteration: 476 \t--- Loss: 0.005\n",
      "Iteration: 477 \t--- Loss: 0.006\n",
      "Iteration: 478 \t--- Loss: 0.006\n",
      "Iteration: 479 \t--- Loss: 0.005\n",
      "Iteration: 480 \t--- Loss: 0.006\n",
      "Iteration: 481 \t--- Loss: 0.005\n",
      "Iteration: 482 \t--- Loss: 0.006\n",
      "Iteration: 483 \t--- Loss: 0.005\n",
      "Iteration: 484 \t--- Loss: 0.005\n",
      "Iteration: 485 \t--- Loss: 0.005\n",
      "Iteration: 486 \t--- Loss: 0.005\n",
      "Iteration: 487 \t--- Loss: 0.005\n",
      "Iteration: 488 \t--- Loss: 0.005\n",
      "Iteration: 489 \t--- Loss: 0.005\n",
      "Iteration: 490 \t--- Loss: 0.005\n",
      "Iteration: 491 \t--- Loss: 0.005\n",
      "Iteration: 492 \t--- Loss: 0.005\n",
      "Iteration: 493 \t--- Loss: 0.005\n",
      "Iteration: 494 \t--- Loss: 0.005\n",
      "Iteration: 495 \t--- Loss: 0.006\n",
      "Iteration: 496 \t--- Loss: 0.006\n",
      "Iteration: 497 \t--- Loss: 0.006\n",
      "Iteration: 498 \t--- Loss: 0.006\n",
      "Iteration: 499 \t--- Loss: 0.005\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:24<00:00, 84.39s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.478\n",
      "Iteration: 261 \t--- Loss: 0.510\n",
      "Iteration: 262 \t--- Loss: 0.522\n",
      "Iteration: 263 \t--- Loss: 0.587\n",
      "Iteration: 264 \t--- Loss: 0.512\n",
      "Iteration: 265 \t--- Loss: 0.559\n",
      "Iteration: 266 \t--- Loss: 0.493\n",
      "Iteration: 267 \t--- Loss: 0.458\n",
      "Iteration: 268 \t--- Loss: 0.461\n",
      "Iteration: 269 \t--- Loss: 0.534\n",
      "Iteration: 270 \t--- Loss: 0.476\n",
      "Iteration: 271 \t--- Loss: 0.464\n",
      "Iteration: 272 \t--- Loss: 0.522\n",
      "Iteration: 273 \t--- Loss: 0.532\n",
      "Iteration: 274 \t--- Loss: 0.503\n",
      "Iteration: 275 \t--- Loss: 0.483\n",
      "Iteration: 276 \t--- Loss: 0.465\n",
      "Iteration: 277 \t--- Loss: 0.503\n",
      "Iteration: 278 \t--- Loss: 0.523\n",
      "Iteration: 279 \t--- Loss: 0.547\n",
      "Iteration: 280 \t--- Loss: 0.503\n",
      "Iteration: 281 \t--- Loss: 0.551\n",
      "Iteration: 282 \t--- Loss: 0.566\n",
      "Iteration: 283 \t--- Loss: 0.511\n",
      "Iteration: 284 \t--- Loss: 0.466\n",
      "Iteration: 285 \t--- Loss: 0.535\n",
      "Iteration: 286 \t--- Loss: 0.523\n",
      "Iteration: 287 \t--- Loss: 0.545\n",
      "Iteration: 288 \t--- Loss: 0.496\n",
      "Iteration: 289 \t--- Loss: 0.519\n",
      "Iteration: 290 \t--- Loss: 0.507\n",
      "Iteration: 291 \t--- Loss: 0.510\n",
      "Iteration: 292 \t--- Loss: 0.517\n",
      "Iteration: 293 \t--- Loss: 0.542\n",
      "Iteration: 294 \t--- Loss: 0.559\n",
      "Iteration: 295 \t--- Loss: 0.498\n",
      "Iteration: 296 \t--- Loss: 0.459\n",
      "Iteration: 297 \t--- Loss: 0.470\n",
      "Iteration: 298 \t--- Loss: 0.491\n",
      "Iteration: 299 \t--- Loss: 0.531\n",
      "Iteration: 300 \t--- Loss: 0.541\n",
      "Iteration: 301 \t--- Loss: 0.473\n",
      "Iteration: 302 \t--- Loss: 0.493\n",
      "Iteration: 303 \t--- Loss: 0.506\n",
      "Iteration: 304 \t--- Loss: 0.541\n",
      "Iteration: 305 \t--- Loss: 0.486\n",
      "Iteration: 306 \t--- Loss: 0.503\n",
      "Iteration: 307 \t--- Loss: 0.529\n",
      "Iteration: 308 \t--- Loss: 0.437\n",
      "Iteration: 309 \t--- Loss: 0.471\n",
      "Iteration: 310 \t--- Loss: 0.511\n",
      "Iteration: 311 \t--- Loss: 0.464\n",
      "Iteration: 312 \t--- Loss: 0.544\n",
      "Iteration: 313 \t--- Loss: 0.483\n",
      "Iteration: 314 \t--- Loss: 0.517\n",
      "Iteration: 315 \t--- Loss: 0.524\n",
      "Iteration: 316 \t--- Loss: 0.467\n",
      "Iteration: 317 \t--- Loss: 0.500\n",
      "Iteration: 318 \t--- Loss: 0.493\n",
      "Iteration: 319 \t--- Loss: 0.460\n",
      "Iteration: 320 \t--- Loss: 0.525\n",
      "Iteration: 321 \t--- Loss: 0.487\n",
      "Iteration: 322 \t--- Loss: 0.434\n",
      "Iteration: 323 \t--- Loss: 0.562\n",
      "Iteration: 324 \t--- Loss: 0.505\n",
      "Iteration: 325 \t--- Loss: 0.487\n",
      "Iteration: 326 \t--- Loss: 0.524\n",
      "Iteration: 327 \t--- Loss: 0.502\n",
      "Iteration: 328 \t--- Loss: 0.434\n",
      "Iteration: 329 \t--- Loss: 0.504\n",
      "Iteration: 330 \t--- Loss: 0.559\n",
      "Iteration: 331 \t--- Loss: 0.464\n",
      "Iteration: 332 \t--- Loss: 0.453\n",
      "Iteration: 333 \t--- Loss: 0.503\n",
      "Iteration: 334 \t--- Loss: 0.440\n",
      "Iteration: 335 \t--- Loss: 0.526\n",
      "Iteration: 336 \t--- Loss: 0.496\n",
      "Iteration: 337 \t--- Loss: 0.551\n",
      "Iteration: 338 \t--- Loss: 0.531\n",
      "Iteration: 339 \t--- Loss: 0.516\n",
      "Iteration: 340 \t--- Loss: 0.533\n",
      "Iteration: 341 \t--- Loss: 0.515\n",
      "Iteration: 342 \t--- Loss: 0.522\n",
      "Iteration: 343 \t--- Loss: 0.523\n",
      "Iteration: 344 \t--- Loss: 0.471\n",
      "Iteration: 345 \t--- Loss: 0.467\n",
      "Iteration: 346 \t--- Loss: 0.493\n",
      "Iteration: 347 \t--- Loss: 0.470\n",
      "Iteration: 348 \t--- Loss: 0.494\n",
      "Iteration: 349 \t--- Loss: 0.470\n",
      "Iteration: 350 \t--- Loss: 0.474\n",
      "Iteration: 351 \t--- Loss: 0.474\n",
      "Iteration: 352 \t--- Loss: 0.505\n",
      "Iteration: 353 \t--- Loss: 0.484\n",
      "Iteration: 354 \t--- Loss: 0.505\n",
      "Iteration: 355 \t--- Loss: 0.505\n",
      "Iteration: 356 \t--- Loss: 0.471\n",
      "Iteration: 357 \t--- Loss: 0.524\n",
      "Iteration: 358 \t--- Loss: 0.521\n",
      "Iteration: 359 \t--- Loss: 0.498\n",
      "Iteration: 360 \t--- Loss: 0.493\n",
      "Iteration: 361 \t--- Loss: 0.513\n",
      "Iteration: 362 \t--- Loss: 0.455\n",
      "Iteration: 363 \t--- Loss: 0.531\n",
      "Iteration: 364 \t--- Loss: 0.544\n",
      "Iteration: 365 \t--- Loss: 0.539\n",
      "Iteration: 366 \t--- Loss: 0.481\n",
      "Iteration: 367 \t--- Loss: 0.540\n",
      "Iteration: 368 \t--- Loss: 0.476\n",
      "Iteration: 369 \t--- Loss: 0.497\n",
      "Iteration: 370 \t--- Loss: 0.471\n",
      "Iteration: 371 \t--- Loss: 0.463\n",
      "Iteration: 372 \t--- Loss: 0.463\n",
      "Iteration: 373 \t--- Loss: 0.469\n",
      "Iteration: 374 \t--- Loss: 0.495\n",
      "Iteration: 375 \t--- Loss: 0.494\n",
      "Iteration: 376 \t--- Loss: 0.543\n",
      "Iteration: 377 \t--- Loss: 0.510\n",
      "Iteration: 378 \t--- Loss: 0.522\n",
      "Iteration: 379 \t--- Loss: 0.506\n",
      "Iteration: 380 \t--- Loss: 0.525\n",
      "Iteration: 381 \t--- Loss: 0.483\n",
      "Iteration: 382 \t--- Loss: 0.534\n",
      "Iteration: 383 \t--- Loss: 0.471\n",
      "Iteration: 384 \t--- Loss: 0.532\n",
      "Iteration: 385 \t--- Loss: 0.469\n",
      "Iteration: 386 \t--- Loss: 0.494\n",
      "Iteration: 387 \t--- Loss: 0.473\n",
      "Iteration: 388 \t--- Loss: 0.469\n",
      "Iteration: 389 \t--- Loss: 0.457\n",
      "Iteration: 390 \t--- Loss: 0.502\n",
      "Iteration: 391 \t--- Loss: 0.477\n",
      "Iteration: 392 \t--- Loss: 0.456\n",
      "Iteration: 393 \t--- Loss: 0.507\n",
      "Iteration: 394 \t--- Loss: 0.454\n",
      "Iteration: 395 \t--- Loss: 0.532\n",
      "Iteration: 396 \t--- Loss: 0.454\n",
      "Iteration: 397 \t--- Loss: 0.491\n",
      "Iteration: 398 \t--- Loss: 0.479\n",
      "Iteration: 399 \t--- Loss: 0.460\n",
      "Iteration: 400 \t--- Loss: 0.495\n",
      "Iteration: 401 \t--- Loss: 0.471\n",
      "Iteration: 402 \t--- Loss: 0.548\n",
      "Iteration: 403 \t--- Loss: 0.499\n",
      "Iteration: 404 \t--- Loss: 0.530\n",
      "Iteration: 405 \t--- Loss: 0.541\n",
      "Iteration: 406 \t--- Loss: 0.491\n",
      "Iteration: 407 \t--- Loss: 0.475\n",
      "Iteration: 408 \t--- Loss: 0.502\n",
      "Iteration: 409 \t--- Loss: 0.452\n",
      "Iteration: 410 \t--- Loss: 0.486\n",
      "Iteration: 411 \t--- Loss: 0.497\n",
      "Iteration: 412 \t--- Loss: 0.477\n",
      "Iteration: 413 \t--- Loss: 0.499\n",
      "Iteration: 414 \t--- Loss: 0.512\n",
      "Iteration: 415 \t--- Loss: 0.473\n",
      "Iteration: 416 \t--- Loss: 0.494\n",
      "Iteration: 417 \t--- Loss: 0.519\n",
      "Iteration: 418 \t--- Loss: 0.491\n",
      "Iteration: 419 \t--- Loss: 0.523\n",
      "Iteration: 420 \t--- Loss: 0.499\n",
      "Iteration: 421 \t--- Loss: 0.435\n",
      "Iteration: 422 \t--- Loss: 0.484\n",
      "Iteration: 423 \t--- Loss: 0.542\n",
      "Iteration: 424 \t--- Loss: 0.482\n",
      "Iteration: 425 \t--- Loss: 0.449\n",
      "Iteration: 426 \t--- Loss: 0.511\n",
      "Iteration: 427 \t--- Loss: 0.556\n",
      "Iteration: 428 \t--- Loss: 0.551\n",
      "Iteration: 429 \t--- Loss: 0.527\n",
      "Iteration: 430 \t--- Loss: 0.512\n",
      "Iteration: 431 \t--- Loss: 0.432\n",
      "Iteration: 432 \t--- Loss: 0.523\n",
      "Iteration: 433 \t--- Loss: 0.540\n",
      "Iteration: 434 \t--- Loss: 0.497\n",
      "Iteration: 435 \t--- Loss: 0.469\n",
      "Iteration: 436 \t--- Loss: 0.536\n",
      "Iteration: 437 \t--- Loss: 0.453\n",
      "Iteration: 438 \t--- Loss: 0.475\n",
      "Iteration: 439 \t--- Loss: 0.466\n",
      "Iteration: 440 \t--- Loss: 0.521\n",
      "Iteration: 441 \t--- Loss: 0.528\n",
      "Iteration: 442 \t--- Loss: 0.481\n",
      "Iteration: 443 \t--- Loss: 0.478\n",
      "Iteration: 444 \t--- Loss: 0.489\n",
      "Iteration: 445 \t--- Loss: 0.488\n",
      "Iteration: 446 \t--- Loss: 0.500\n",
      "Iteration: 447 \t--- Loss: 0.551\n",
      "Iteration: 448 \t--- Loss: 0.474\n",
      "Iteration: 449 \t--- Loss: 0.524\n",
      "Iteration: 450 \t--- Loss: 0.530\n",
      "Iteration: 451 \t--- Loss: 0.520\n",
      "Iteration: 452 \t--- Loss: 0.475\n",
      "Iteration: 453 \t--- Loss: 0.536\n",
      "Iteration: 454 \t--- Loss: 0.469\n",
      "Iteration: 455 \t--- Loss: 0.553\n",
      "Iteration: 456 \t--- Loss: 0.533\n",
      "Iteration: 457 \t--- Loss: 0.471\n",
      "Iteration: 458 \t--- Loss: 0.475\n",
      "Iteration: 459 \t--- Loss: 0.475\n",
      "Iteration: 460 \t--- Loss: 0.494\n",
      "Iteration: 461 \t--- Loss: 0.502\n",
      "Iteration: 462 \t--- Loss: 0.519\n",
      "Iteration: 463 \t--- Loss: 0.487\n",
      "Iteration: 464 \t--- Loss: 0.569\n",
      "Iteration: 465 \t--- Loss: 0.524\n",
      "Iteration: 466 \t--- Loss: 0.478\n",
      "Iteration: 467 \t--- Loss: 0.492\n",
      "Iteration: 468 \t--- Loss: 0.502\n",
      "Iteration: 469 \t--- Loss: 0.500\n",
      "Iteration: 470 \t--- Loss: 0.520\n",
      "Iteration: 471 \t--- Loss: 0.476\n",
      "Iteration: 472 \t--- Loss: 0.499\n",
      "Iteration: 473 \t--- Loss: 0.489\n",
      "Iteration: 474 \t--- Loss: 0.542\n",
      "Iteration: 475 \t--- Loss: 0.452\n",
      "Iteration: 476 \t--- Loss: 0.515\n",
      "Iteration: 477 \t--- Loss: 0.473\n",
      "Iteration: 478 \t--- Loss: 0.519\n",
      "Iteration: 479 \t--- Loss: 0.493\n",
      "Iteration: 480 \t--- Loss: 0.495\n",
      "Iteration: 481 \t--- Loss: 0.498\n",
      "Iteration: 482 \t--- Loss: 0.468\n",
      "Iteration: 483 \t--- Loss: 0.486\n",
      "Iteration: 484 \t--- Loss: 0.523\n",
      "Iteration: 485 \t--- Loss: 0.479\n",
      "Iteration: 486 \t--- Loss: 0.458\n",
      "Iteration: 487 \t--- Loss: 0.450\n",
      "Iteration: 488 \t--- Loss: 0.522\n",
      "Iteration: 489 \t--- Loss: 0.522\n",
      "Iteration: 490 \t--- Loss: 0.466\n",
      "Iteration: 491 \t--- Loss: 0.511\n",
      "Iteration: 492 \t--- Loss: 0.513\n",
      "Iteration: 493 \t--- Loss: 0.474\n",
      "Iteration: 494 \t--- Loss: 0.488\n",
      "Iteration: 495 \t--- Loss: 0.501\n",
      "Iteration: 496 \t--- Loss: 0.568\n",
      "Iteration: 497 \t--- Loss: 0.553\n",
      "Iteration: 498 \t--- Loss: 0.517\n",
      "Iteration: 499 \t--- Loss: 0.488\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:11<00:02,  1.50s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 260 \t--- Loss: 0.283\n",
      "Iteration: 261 \t--- Loss: 0.277\n",
      "Iteration: 262 \t--- Loss: 0.263\n",
      "Iteration: 263 \t--- Loss: 0.283\n",
      "Iteration: 264 \t--- Loss: 0.290\n",
      "Iteration: 265 \t--- Loss: 0.284\n",
      "Iteration: 266 \t--- Loss: 0.279\n",
      "Iteration: 267 \t--- Loss: 0.309\n",
      "Iteration: 268 \t--- Loss: 0.273\n",
      "Iteration: 269 \t--- Loss: 0.268\n",
      "Iteration: 270 \t--- Loss: 0.283\n",
      "Iteration: 271 \t--- Loss: 0.284\n",
      "Iteration: 272 \t--- Loss: 0.295\n",
      "Iteration: 273 \t--- Loss: 0.290\n",
      "Iteration: 274 \t--- Loss: 0.298\n",
      "Iteration: 275 \t--- Loss: 0.302\n",
      "Iteration: 276 \t--- Loss: 0.248\n",
      "Iteration: 277 \t--- Loss: 0.275\n",
      "Iteration: 278 \t--- Loss: 0.274\n",
      "Iteration: 279 \t--- Loss: 0.284\n",
      "Iteration: 280 \t--- Loss: 0.327\n",
      "Iteration: 281 \t--- Loss: 0.289\n",
      "Iteration: 282 \t--- Loss: 0.310\n",
      "Iteration: 283 \t--- Loss: 0.287\n",
      "Iteration: 284 \t--- Loss: 0.273\n",
      "Iteration: 285 \t--- Loss: 0.299\n",
      "Iteration: 286 \t--- Loss: 0.295\n",
      "Iteration: 287 \t--- Loss: 0.295\n",
      "Iteration: 288 \t--- Loss: 0.281\n",
      "Iteration: 289 \t--- Loss: 0.287\n",
      "Iteration: 290 \t--- Loss: 0.277\n",
      "Iteration: 291 \t--- Loss: 0.283\n",
      "Iteration: 292 \t--- Loss: 0.282\n",
      "Iteration: 293 \t--- Loss: 0.286\n",
      "Iteration: 294 \t--- Loss: 0.304\n",
      "Iteration: 295 \t--- Loss: 0.272\n",
      "Iteration: 296 \t--- Loss: 0.300\n",
      "Iteration: 297 \t--- Loss: 0.274\n",
      "Iteration: 298 \t--- Loss: 0.291\n",
      "Iteration: 299 \t--- Loss: 0.289\n",
      "Iteration: 300 \t--- Loss: 0.271\n",
      "Iteration: 301 \t--- Loss: 0.300\n",
      "Iteration: 302 \t--- Loss: 0.254\n",
      "Iteration: 303 \t--- Loss: 0.286\n",
      "Iteration: 304 \t--- Loss: 0.297\n",
      "Iteration: 305 \t--- Loss: 0.286\n",
      "Iteration: 306 \t--- Loss: 0.298\n",
      "Iteration: 307 \t--- Loss: 0.311\n",
      "Iteration: 308 \t--- Loss: 0.290\n",
      "Iteration: 309 \t--- Loss: 0.296\n",
      "Iteration: 310 \t--- Loss: 0.306\n",
      "Iteration: 311 \t--- Loss: 0.297\n",
      "Iteration: 312 \t--- Loss: 0.277\n",
      "Iteration: 313 \t--- Loss: 0.295\n",
      "Iteration: 314 \t--- Loss: 0.285\n",
      "Iteration: 315 \t--- Loss: 0.294\n",
      "Iteration: 316 \t--- Loss: 0.289\n",
      "Iteration: 317 \t--- Loss: 0.310\n",
      "Iteration: 318 \t--- Loss: 0.285\n",
      "Iteration: 319 \t--- Loss: 0.301\n",
      "Iteration: 320 \t--- Loss: 0.300\n",
      "Iteration: 321 \t--- Loss: 0.308\n",
      "Iteration: 322 \t--- Loss: 0.298\n",
      "Iteration: 323 \t--- Loss: 0.274\n",
      "Iteration: 324 \t--- Loss: 0.262\n",
      "Iteration: 325 \t--- Loss: 0.299\n",
      "Iteration: 326 \t--- Loss: 0.264\n",
      "Iteration: 327 \t--- Loss: 0.290\n",
      "Iteration: 328 \t--- Loss: 0.290\n",
      "Iteration: 329 \t--- Loss: 0.295\n",
      "Iteration: 330 \t--- Loss: 0.291\n",
      "Iteration: 331 \t--- Loss: 0.278\n",
      "Iteration: 332 \t--- Loss: 0.278\n",
      "Iteration: 333 \t--- Loss: 0.285\n",
      "Iteration: 334 \t--- Loss: 0.310\n",
      "Iteration: 335 \t--- Loss: 0.284\n",
      "Iteration: 336 \t--- Loss: 0.301\n",
      "Iteration: 337 \t--- Loss: 0.301\n",
      "Iteration: 338 \t--- Loss: 0.277\n",
      "Iteration: 339 \t--- Loss: 0.297\n",
      "Iteration: 340 \t--- Loss: 0.289\n",
      "Iteration: 341 \t--- Loss: 0.295\n",
      "Iteration: 342 \t--- Loss: 0.289\n",
      "Iteration: 343 \t--- Loss: 0.286\n",
      "Iteration: 344 \t--- Loss: 0.288\n",
      "Iteration: 345 \t--- Loss: 0.305\n",
      "Iteration: 346 \t--- Loss: 0.310\n",
      "Iteration: 347 \t--- Loss: 0.323\n",
      "Iteration: 348 \t--- Loss: 0.299\n",
      "Iteration: 349 \t--- Loss: 0.305\n",
      "Iteration: 350 \t--- Loss: 0.295\n",
      "Iteration: 351 \t--- Loss: 0.259\n",
      "Iteration: 352 \t--- Loss: 0.293\n",
      "Iteration: 353 \t--- Loss: 0.297\n",
      "Iteration: 354 \t--- Loss: 0.270\n",
      "Iteration: 355 \t--- Loss: 0.297\n",
      "Iteration: 356 \t--- Loss: 0.287\n",
      "Iteration: 357 \t--- Loss: 0.299\n",
      "Iteration: 358 \t--- Loss: 0.282\n",
      "Iteration: 359 \t--- Loss: 0.251\n",
      "Iteration: 360 \t--- Loss: 0.308\n",
      "Iteration: 361 \t--- Loss: 0.295\n",
      "Iteration: 362 \t--- Loss: 0.285\n",
      "Iteration: 363 \t--- Loss: 0.269\n",
      "Iteration: 364 \t--- Loss: 0.319\n",
      "Iteration: 365 \t--- Loss: 0.283\n",
      "Iteration: 366 \t--- Loss: 0.316\n",
      "Iteration: 367 \t--- Loss: 0.293\n",
      "Iteration: 368 \t--- Loss: 0.287\n",
      "Iteration: 369 \t--- Loss: 0.272\n",
      "Iteration: 370 \t--- Loss: 0.302\n",
      "Iteration: 371 \t--- Loss: 0.279\n",
      "Iteration: 372 \t--- Loss: 0.269\n",
      "Iteration: 373 \t--- Loss: 0.267\n",
      "Iteration: 374 \t--- Loss: 0.292\n",
      "Iteration: 375 \t--- Loss: 0.293\n",
      "Iteration: 376 \t--- Loss: 0.297\n",
      "Iteration: 377 \t--- Loss: 0.283\n",
      "Iteration: 378 \t--- Loss: 0.297\n",
      "Iteration: 379 \t--- Loss: 0.280\n",
      "Iteration: 380 \t--- Loss: 0.277\n",
      "Iteration: 381 \t--- Loss: 0.303\n",
      "Iteration: 382 \t--- Loss: 0.286\n",
      "Iteration: 383 \t--- Loss: 0.288\n",
      "Iteration: 384 \t--- Loss: 0.291\n",
      "Iteration: 385 \t--- Loss: 0.316\n",
      "Iteration: 386 \t--- Loss: 0.291\n",
      "Iteration: 387 \t--- Loss: 0.270\n",
      "Iteration: 388 \t--- Loss: 0.281\n",
      "Iteration: 389 \t--- Loss: 0.285\n",
      "Iteration: 390 \t--- Loss: 0.302\n",
      "Iteration: 391 \t--- Loss: 0.294\n",
      "Iteration: 392 \t--- Loss: 0.263\n",
      "Iteration: 393 \t--- Loss: 0.297\n",
      "Iteration: 394 \t--- Loss: 0.279\n",
      "Iteration: 395 \t--- Loss: 0.262\n",
      "Iteration: 396 \t--- Loss: 0.269\n",
      "Iteration: 397 \t--- Loss: 0.291\n",
      "Iteration: 398 \t--- Loss: 0.276\n",
      "Iteration: 399 \t--- Loss: 0.281\n",
      "Iteration: 400 \t--- Loss: 0.272\n",
      "Iteration: 401 \t--- Loss: 0.296\n",
      "Iteration: 402 \t--- Loss: 0.280\n",
      "Iteration: 403 \t--- Loss: 0.261\n",
      "Iteration: 404 \t--- Loss: 0.311\n",
      "Iteration: 405 \t--- Loss: 0.273\n",
      "Iteration: 406 \t--- Loss: 0.297\n",
      "Iteration: 407 \t--- Loss: 0.298\n",
      "Iteration: 408 \t--- Loss: 0.285\n",
      "Iteration: 409 \t--- Loss: 0.301\n",
      "Iteration: 410 \t--- Loss: 0.294\n",
      "Iteration: 411 \t--- Loss: 0.296\n",
      "Iteration: 412 \t--- Loss: 0.264\n",
      "Iteration: 413 \t--- Loss: 0.279\n",
      "Iteration: 414 \t--- Loss: 0.300\n",
      "Iteration: 415 \t--- Loss: 0.287\n",
      "Iteration: 416 \t--- Loss: 0.294\n",
      "Iteration: 417 \t--- Loss: 0.281\n",
      "Iteration: 418 \t--- Loss: 0.284\n",
      "Iteration: 419 \t--- Loss: 0.289\n",
      "Iteration: 420 \t--- Loss: 0.317\n",
      "Iteration: 421 \t--- Loss: 0.276\n",
      "Iteration: 422 \t--- Loss: 0.295\n",
      "Iteration: 423 \t--- Loss: 0.295\n",
      "Iteration: 424 \t--- Loss: 0.281\n",
      "Iteration: 425 \t--- Loss: 0.304\n",
      "Iteration: 426 \t--- Loss: 0.280\n",
      "Iteration: 427 \t--- Loss: 0.256\n",
      "Iteration: 428 \t--- Loss: 0.295\n",
      "Iteration: 429 \t--- Loss: 0.272\n",
      "Iteration: 430 \t--- Loss: 0.284\n",
      "Iteration: 431 \t--- Loss: 0.297\n",
      "Iteration: 432 \t--- Loss: 0.287\n",
      "Iteration: 433 \t--- Loss: 0.306\n",
      "Iteration: 434 \t--- Loss: 0.295\n",
      "Iteration: 435 \t--- Loss: 0.271\n",
      "Iteration: 436 \t--- Loss: 0.312\n",
      "Iteration: 437 \t--- Loss: 0.266\n",
      "Iteration: 438 \t--- Loss: 0.289\n",
      "Iteration: 439 \t--- Loss: 0.279\n",
      "Iteration: 440 \t--- Loss: 0.289\n",
      "Iteration: 441 \t--- Loss: 0.290\n",
      "Iteration: 442 \t--- Loss: 0.291\n",
      "Iteration: 443 \t--- Loss: 0.294\n",
      "Iteration: 444 \t--- Loss: 0.285\n",
      "Iteration: 445 \t--- Loss: 0.279\n",
      "Iteration: 446 \t--- Loss: 0.282\n",
      "Iteration: 447 \t--- Loss: 0.289\n",
      "Iteration: 448 \t--- Loss: 0.295\n",
      "Iteration: 449 \t--- Loss: 0.250\n",
      "Iteration: 450 \t--- Loss: 0.265\n",
      "Iteration: 451 \t--- Loss: 0.274\n",
      "Iteration: 452 \t--- Loss: 0.288\n",
      "Iteration: 453 \t--- Loss: 0.293\n",
      "Iteration: 454 \t--- Loss: 0.278\n",
      "Iteration: 455 \t--- Loss: 0.272\n",
      "Iteration: 456 \t--- Loss: 0.290\n",
      "Iteration: 457 \t--- Loss: 0.304\n",
      "Iteration: 458 \t--- Loss: 0.278\n",
      "Iteration: 459 \t--- Loss: 0.287\n",
      "Iteration: 460 \t--- Loss: 0.274\n",
      "Iteration: 461 \t--- Loss: 0.284\n",
      "Iteration: 462 \t--- Loss: 0.302\n",
      "Iteration: 463 \t--- Loss: 0.284\n",
      "Iteration: 464 \t--- Loss: 0.261\n",
      "Iteration: 465 \t--- Loss: 0.271\n",
      "Iteration: 466 \t--- Loss: 0.301\n",
      "Iteration: 467 \t--- Loss: 0.298\n",
      "Iteration: 468 \t--- Loss: 0.313\n",
      "Iteration: 469 \t--- Loss: 0.278\n",
      "Iteration: 470 \t--- Loss: 0.277\n",
      "Iteration: 471 \t--- Loss: 0.292\n",
      "Iteration: 472 \t--- Loss: 0.296\n",
      "Iteration: 473 \t--- Loss: 0.287\n",
      "Iteration: 474 \t--- Loss: 0.303\n",
      "Iteration: 475 \t--- Loss: 0.290\n",
      "Iteration: 476 \t--- Loss: 0.272\n",
      "Iteration: 477 \t--- Loss: 0.303\n",
      "Iteration: 478 \t--- Loss: 0.286\n",
      "Iteration: 479 \t--- Loss: 0.290\n",
      "Iteration: 480 \t--- Loss: 0.293\n",
      "Iteration: 481 \t--- Loss: 0.265\n",
      "Iteration: 482 \t--- Loss: 0.280\n",
      "Iteration: 483 \t--- Loss: 0.283\n",
      "Iteration: 484 \t--- Loss: 0.276\n",
      "Iteration: 485 \t--- Loss: 0.280\n",
      "Iteration: 486 \t--- Loss: 0.278\n",
      "Iteration: 487 \t--- Loss: 0.282\n",
      "Iteration: 488 \t--- Loss: 0.300\n",
      "Iteration: 489 \t--- Loss: 0.279\n",
      "Iteration: 490 \t--- Loss: 0.273\n",
      "Iteration: 491 \t--- Loss: 0.288\n",
      "Iteration: 492 \t--- Loss: 0.290\n",
      "Iteration: 493 \t--- Loss: 0.310\n",
      "Iteration: 494 \t--- Loss: 0.276\n",
      "Iteration: 495 \t--- Loss: 0.260\n",
      "Iteration: 496 \t--- Loss: 0.256\n",
      "Iteration: 497 \t--- Loss: 0.296\n",
      "Iteration: 498 \t--- Loss: 0.300\n",
      "Iteration: 499 \t--- Loss: 0.271\n",
      "----  Optimizing the metamodel  ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.53s/it][Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed: 59.2min finished\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/(0.865660599140134*exp(-0.429615784309434*X0**3 + 0.743209306323792*X0**2 - 0.0749518272307708*X0) + 1), 1/(0.596587574711138*exp(-0.361071995878036*X0**3 + 0.660568464064468*X0**2 - 0.279715372400982*X0) + 1), 1/(1.00000000002653*exp(7.01626854889467e-10*X0**3 + 2.39382601939568e-9*X0**2 - 2.80978378192068e-10*X0) + 1), 1/(0.996685394262472*exp(-0.024484152101091*X0**3 + 0.061128253197913*X0**2 + 0.0538916796269331*X0) + 1), 1/(1.0*exp(4.69267683340293e-18*X0**3 + 1.51245555906694e-17*X0**2 - 1.82290168082546e-18*X0) + 1), 1/(1.00000000013896*exp(3.65534525661227e-9*X0**3 + 1.25889670297096e-8*X0**2 - 1.47215495221614e-9*X0) + 1), 1/(1.00000000015586*exp(4.08826364818537e-9*X0**3 + 1.41276497735523e-8*X0**2 - 1.65124263865496e-9*X0) + 1), 1/(1.0*exp(3.8276545518784e-15*X0**3 + 1.24857956987681e-14*X0**2 - 1.4957502705547e-15*X0) + 1), 1/(1.0000000000019*exp(5.0557602473565e-11*X0**3 + 1.70627862594763e-10*X0**2 - 2.01108178344622e-11*X0) + 1), None, 1/(1.00000000000178*exp(4.7549254427216e-11*X0**3 + 1.5910868345868e-10*X0**2 - 1.88387758579471e-11*X0) + 1), 1/(0.572486784963154*exp(-0.768804195967495*X0**3 + 1.60814480753709*X0**2 - 1.19878842828324*X0) + 1), 1/(1.0*exp(4.35814402414924e-16*X0**3 + 1.40621942419078e-15*X0**2 - 1.69401375340952e-16*X0) + 1), 1/(0.57185191012477*exp(-0.615489750920325*X0**3 + 1.24265095912308*X0**2 - 0.82552935803625*X0) + 1), 1/(1.0*exp(7.32755232002936e-14*X0**3 + 2.42663910059911e-13*X0**2 - 2.8848250639714e-14*X0) + 1), 1/(0.310661436960512*exp(-0.0178032991203745*X0**3 + 0.0242148013835732*X0**2 + 0.10554356808612*X0) + 1), 1/(1.0*exp(6.70577429132785e-15*X0**3 + 2.15463381764626e-14*X0**2 - 2.60158389959194e-15*X0) + 1), 1/(0.999414920383153*exp(-0.0111213415760196*X0**3 + 0.0487831798327393*X0**2 + 0.00663017865706754*X0) + 1), 1/(1.00000000000001*exp(2.29909753100741e-13*X0**3 + 7.62671944913323e-13*X0**2 - 9.06054476441094e-14*X0) + 1), 1/(1.0*exp(5.55214255822696e-15*X0**3 + 1.79437083565704e-14*X0**2 - 2.16003828127808e-15*X0) + 1), 1/(1.00000210183809*exp(5.02953542629562e-5*X0**3 + 0.000192182181326734*X0**2 - 2.22524717771868e-5*X0) + 1), 1/(0.99814154465207*exp(-0.0285427699739209*X0**3 - 0.182705678546245*X0**2 + 0.019827084503526*X0) + 1), 1/(1.0*exp(3.06666494271574e-18*X0**3 + 9.75802631590425e-18*X0**2 - 1.1838987555071e-18*X0) + 1), 1/(1.00000000000008*exp(2.19285582742002e-12*X0**3 + 7.25036316931584e-12*X0**2 - 8.63168998060245e-13*X0) + 1), 1/(1.0*exp(8.71394699004322e-14*X0**3 + 2.88883592658879e-13*X0**2 - 3.4324075159908e-14*X0) + 1), 1/(1.0*exp(8.33009441432686e-14*X0**3 + 2.74054800215267e-13*X0**2 - 3.26925805717356e-14*X0) + 1), None, 1/(1.0*exp(1.01097669933586e-14*X0**3 + 3.28696319843655e-14*X0**2 - 3.94470098394214e-15*X0) + 1), 1/(0.319456636719059*exp(0.0517639428598233*X0**3 - 0.128269295281267*X0**2 + 0.228563054251366*X0) + 1), 1/(1.00000000000003*exp(9.15301945747103e-13*X0**3 + 2.9939668243527e-12*X0**2 - 3.58394156450301e-13*X0) + 1), 1/(0.993160974244088*exp(-0.0140509307536695*X0**3 - 0.969855397618078*X0**2 + 0.0775614801286144*X0) + 1), None, 1/(0.499976190052032*exp(-0.34882018659445*X0**3 + 0.694802529667828*X0**2 - 0.373997315747479*X0) + 1), 1/(1.0*exp(1.80975940366861e-14*X0**3 + 5.9613231035013e-14*X0**2 - 7.10593968824077e-15*X0) + 1), None, 1/(0.558900790089937*exp(-0.366697256385095*X0**3 + 0.659911650982166*X0**2 - 0.259126458308761*X0) + 1), 1/(1.00000000000001*exp(2.36155442286741e-13*X0**3 + 7.70594465029911e-13*X0**2 - 9.23394477965993e-14*X0) + 1), 1/(1.0*exp(5.28773177621412e-16*X0**3 + 1.68606424767883e-15*X0**2 - 2.04362660225813e-16*X0) + 1), 1/(0.729932484263076*exp(-0.456257945709793*X0**3 + 0.891068603443536*X0**2 - 0.517785551828388*X0) + 1), 1/(0.964001272756455*exp(-0.338226263568622*X0**3 + 0.827372431642774*X0**2 - 0.76604584936515*X0) + 1), 1/(0.799355774243301*exp(-0.592565541823557*X0**3 + 1.24621963168581*X0**2 - 0.89517475490567*X0) + 1), 1/(1.00002730100614*exp(0.000623946846743751*X0**3 + 0.00249296822147789*X0**2 - 0.000288791111475745*X0) + 1), 1/(1.00000000000076*exp(2.01711115862204e-11*X0**3 + 6.78454997960886e-11*X0**2 - 8.00944769091348e-12*X0) + 1), 1/(0.558514143248098*exp(-0.518102612667529*X0**3 + 1.01049987552605*X0**2 - 0.591945593405521*X0) + 1), 1/(1.0*exp(5.65983360562926e-17*X0**3 + 1.82400193760955e-16*X0**2 - 2.1985833888902e-17*X0) + 1), 1/(0.671782678217687*exp(-0.382816746281842*X0**3 + 0.703679599096009*X0**2 - 0.319563659993805*X0) + 1), 1/(1.00000000000024*exp(6.51386558296178e-12*X0**3 + 2.17765189700984e-11*X0**2 - 2.57873023002705e-12*X0) + 1), 1/(0.655052603339784*exp(-0.362578381145778*X0**3 + 0.65688638220852*X0**2 - 0.275691711332986*X0) + 1), 1/(0.999665982489588*exp(0.092635073960654*X0**3 - 0.824760725371353*X0**2 + 0.0106790775963414*X0) + 1), 1/(1.00000000000032*exp(8.45389486538907e-12*X0**3 + 2.85214249402104e-11*X0**2 - 3.36322790085732e-12*X0) + 1), None, 1/(0.999623845704406*exp(-0.0063857989257934*X0**3 - 0.035945490362278*X0**2 + 0.00399573354369374*X0) + 1), 1/(1.0*exp(3.16389056663997e-15*X0**3 + 1.03969377226286e-14*X0**2 - 1.24075267537734e-15*X0) + 1), 1/(1.00000013839153*exp(3.49074247923255e-6*X0**3 + 1.26440887730297e-5*X0**2 - 1.46643956177589e-6*X0) + 1), 1/(0.997857861361659*exp(0.0201889480972416*X0**3 - 0.463500969063624*X0**2 + 0.026113880431896*X0) + 1), 1/(0.653511686734009*exp(-0.385655150495044*X0**3 + 0.70879906067598*X0**2 - 0.322214243796962*X0) + 1), 1/(0.339618231525546*exp(0.0664819116932683*X0**3 - 0.234949685727779*X0**2 + 0.489869340991213*X0) + 1), 1/(1.0*exp(3.08784295925064e-14*X0**3 + 1.02246076903916e-13*X0**2 - 1.21553509212467e-14*X0) + 1), None, 1/(1.0*exp(2.13167324789926e-16*X0**3 + 6.94646261314779e-16*X0**2 - 8.32512271280394e-17*X0) + 1), 1/(1.00000000001724*exp(4.60117934656337e-10*X0**3 + 1.53442612606311e-9*X0**2 - 1.82303150795874e-10*X0) + 1), 1/(0.595947466734802*exp(-0.345071049797186*X0**3 + 0.621046876831783*X0**2 - 0.241995405307944*X0) + 1), 1/(1.0086705017578*exp(0.0913700671480992*X0**3 - 0.28352181951583*X0**2 - 0.117863087428209*X0) + 1), None, None, 1/(0.492719476587501*exp(-0.333404990428365*X0**3 + 0.659843674796119*X0**2 - 0.34181006736477*X0) + 1), 1/(1.00000000000001*exp(2.83153428342742e-13*X0**3 + 9.35626312211356e-13*X0**2 - 1.11385156655086e-13*X0) + 1), 1/(1.0*exp(4.7036738704362e-19*X0**3 + 1.51037976221762e-18*X0**2 - 1.82386618323483e-19*X0) + 1), 1/(1.00000000000012*exp(3.08066542541071e-12*X0**3 + 1.03087894567185e-11*X0**2 - 1.21955479187409e-12*X0) + 1), None, 1/(1.0*exp(4.4947920783736e-16*X0**3 + 1.46527535518573e-15*X0**2 - 1.75579042907712e-16*X0) + 1), 1/(0.999999046350153*exp(-2.32409713849164e-5*X0**3 - 8.71027694386376e-5*X0**2 + 1.00981967847689e-5*X0) + 1), 1/(1.0*exp(8.39046647783639e-17*X0**3 + 2.73093931702146e-16*X0**2 - 3.27491093116686e-17*X0) + 1), 1/(1.0*exp(5.47796566157592e-17*X0**3 + 1.79681679755813e-16*X0**2 - 2.146031629342e-17*X0) + 1), 1/(1.00099944581558*exp(-0.0109898517340026*X0**3 + 0.230824074702768*X0**2 - 0.0122792024001734*X0) + 1), 1/(1.0*exp(1.5369809655227e-15*X0**3 + 4.97870083880113e-15*X0**2 - 5.98579132551791e-16*X0) + 1), 1/(1.00000000000509*exp(1.35312741177584e-10*X0**3 + 4.57272543871774e-10*X0**2 - 5.38870106500163e-11*X0) + 1), 1/(1.0*exp(3.87114418852186e-15*X0**3 + 1.25540125704088e-14*X0**2 - 1.50853675899227e-15*X0) + 1), None, 1/(1.00000000000001*exp(3.80969434510584e-13*X0**3 + 1.24703640498007e-12*X0**2 - 1.49189361309977e-13*X0) + 1), 1/(1.0*exp(2.9732502564887e-17*X0**3 + 9.64800028355288e-17*X0**2 - 1.15878095304747e-17*X0) + 1), 1/(1.00000000574758*exp(1.47593663381426e-7*X0**3 + 5.27335351773072e-7*X0**2 - 6.09591059506463e-8*X0) + 1), 1/(1.00000000000001*exp(2.25291702061527e-13*X0**3 + 7.42788963862812e-13*X0**2 - 8.85214952443623e-14*X0) + 1), 1/(0.976844570671813*exp(-0.459987408042624*X0**3 + 1.22195898129765*X0**2 - 1.45764129544313*X0) + 1), 1/(0.999997845399925*exp(-4.8899322135886e-5*X0**3 - 0.000199145976677045*X0**2 + 2.28242222250284e-5*X0) + 1), 1/(1.00000000000049*exp(1.30526009117423e-11*X0**3 + 4.35234379934108e-11*X0**2 - 5.16198818036804e-12*X0) + 1), 1/(1.0*exp(3.9756536179147e-15*X0**3 + 1.28422350646612e-14*X0**2 - 1.54632120154703e-15*X0) + 1), 1/(1.0*exp(7.47208500118904e-15*X0**3 + 2.45127460176383e-14*X0**2 - 2.92797778144897e-15*X0) + 1), 1/(1.0*exp(6.62243436038098e-14*X0**3 + 2.16753932994259e-13*X0**2 - 2.59261429609127e-14*X0) + 1), 1/(0.685480768048494*exp(-0.375770363866119*X0**3 + 0.701214619701719*X0**2 - 0.336650787935981*X0) + 1), 1/(0.263962169173215*exp(-0.164642580185294*X0**3 + 0.345975791040808*X0**2 - 0.146079038144279*X0) + 1), 1/(0.357812843594577*exp(-0.0461025563970621*X0**3 + 0.055513393593072*X0**2 + 0.156858429123241*X0) + 1), 1/(0.664236342043963*exp(-0.385716779180525*X0**3 + 0.714455720792065*X0**2 - 0.33494126258142*X0) + 1), 1/(0.361242347367381*exp(-0.056479003068047*X0**3 + 0.0966350229689101*X0**2 + 0.0745792455407349*X0) + 1), 1/(0.999757440105161*exp(0.00982423369975654*X0**3 - 0.113720001641385*X0**2 + 0.00352281542772817*X0) + 1), 1/(1.0*exp(1.7764824295557e-14*X0**3 + 5.83495533693844e-14*X0**2 - 6.96558482765954e-15*X0) + 1), 1/(1.01175587520687*exp(-0.0050983370342441*X0**3 + 0.0979235804238155*X0**2 - 0.405302995614372*X0) + 1), 1/(0.741409417381346*exp(-0.459202288266245*X0**3 + 0.897449017307858*X0**2 - 0.52299011989641*X0) + 1), 1/(1.03762328723554*exp(0.284431021465453*X0**3 - 0.72901678556773*X0**2 - 0.577915168550522*X0) + 1), 1/(1.00000000000009*exp(2.48665498564724e-12*X0**3 + 8.26275870865589e-12*X0**2 - 9.81015866836295e-13*X0) + 1)]\n",
      "Metamodel Optimization Time: 0:59:10\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "if symbolic_metamodeling_evaluation:\n",
    "    print('---------------------------------------------------- CALCULATE METAMODEL --------------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=False)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T08:42:48.666339Z",
     "iopub.status.busy": "2021-10-21T08:42:48.666083Z",
     "iopub.status.idle": "2021-10-21T08:42:50.662457Z",
     "shell.execute_reply": "2021-10-21T08:42:50.661721Z",
     "shell.execute_reply.started": "2021-10-21T08:42:48.666295Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_function_evaluation:\n",
    "    print('----------------------------------------------- CALCULATE METAMODEL FUNCTION ----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions_no_GD'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Function Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-10-21T08:42:50.663996Z",
     "iopub.status.busy": "2021-10-21T08:42:50.663763Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   1 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=5)]: Done   2 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=5)]: Done   4 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t--- Loss: 0.393\n",
      "Iteration: 1 \t--- Loss: 0.382\n",
      "Iteration: 2 \t--- Loss: 0.370\n",
      "Iteration: 3 \t--- Loss: 0.375\n",
      "Iteration: 4 \t--- Loss: 0.356\n",
      "Iteration: 5 \t--- Loss: 0.351\n",
      "Iteration: 6 \t--- Loss: 0.343\n",
      "Iteration: 7 \t--- Loss: 0.371\n",
      "Iteration: 8 \t--- Loss: 0.345\n",
      "Iteration: 9 \t--- Loss: 0.352\n",
      "Iteration: 10 \t--- Loss: 0.354\n",
      "Iteration: 11 \t--- Loss: 0.326\n",
      "Iteration: 12 \t--- Loss: 0.349\n",
      "Iteration: 13 \t--- Loss: 0.313\n",
      "Iteration: 14 \t--- Loss: 0.325\n",
      "Iteration: 15 \t--- Loss: 0.330\n",
      "Iteration: 16 \t--- Loss: 0.324\n",
      "Iteration: 17 \t--- Loss: 0.324\n",
      "Iteration: 18 \t--- Loss: 0.307\n",
      "Iteration: 19 \t--- Loss: 0.319\n",
      "Iteration: 20 \t--- Loss: 0.311\n",
      "Iteration: 21 \t--- Loss: 0.322\n",
      "Iteration: 22 \t--- Loss: 0.298\n",
      "Iteration: 23 \t--- Loss: 0.297\n",
      "Iteration: 24 \t--- Loss: 0.307\n",
      "Iteration: 25 \t--- Loss: 0.304\n",
      "Iteration: 26 \t--- Loss: 0.292\n",
      "Iteration: 27 \t--- Loss: 0.291\n",
      "Iteration: 28 \t--- Loss: 0.284\n",
      "Iteration: 29 \t--- Loss: 0.270\n",
      "Iteration: 30 \t--- Loss: 0.271\n",
      "Iteration: 31 \t--- Loss: 0.266\n",
      "Iteration: 32 \t--- Loss: 0.263\n",
      "Iteration: 33 \t--- Loss: 0.262\n",
      "Iteration: 34 \t--- Loss: 0.261\n",
      "Iteration: 35 \t--- Loss: 0.273\n",
      "Iteration: 36 \t--- Loss: 0.280\n",
      "Iteration: 37 \t--- Loss: 0.287\n",
      "Iteration: 38 \t--- Loss: 0.331\n",
      "Iteration: 39 \t--- Loss: 0.299\n",
      "Iteration: 40 \t--- Loss: 0.335\n",
      "Iteration: 41 \t--- Loss: 0.264\n",
      "Iteration: 42 \t--- Loss: 0.260\n",
      "Iteration: 43 \t--- Loss: 0.239\n",
      "Iteration: 44 \t--- Loss: 0.237\n",
      "Iteration: 45 \t--- Loss: 0.259\n",
      "Iteration: 46 \t--- Loss: 0.284\n",
      "Iteration: 47 \t--- Loss: 0.686\n",
      "Iteration: 48 \t--- Loss: 0.623\n",
      "\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.83       31       0.00705949              N/A      0.00s\n",
      "best_fitness_generation 0.0070594934030241046\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.07204       31       0.00507533              N/A      0.00s\n",
      "best_fitness_generation 0.00507533015399523\n",
      "best_fitness 0.0070594934030241046\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.45          1.26353       39       0.00452069              N/A      0.00s\n",
      "best_fitness_generation 0.004520687490936986\n",
      "best_fitness 0.00507533015399523\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    30.80          1.14019       37       0.00427853              N/A      0.00s\n",
      "best_fitness_generation 0.004278525489407342\n",
      "best_fitness 0.004520687490936986\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    30.33         0.994309       31       0.00422485              N/A      0.00s\n",
      "best_fitness_generation 0.0042248502957952586\n",
      "best_fitness 0.004278525489407342\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    27.30         0.958382       31       0.00422485              N/A      0.00s\n",
      "best_fitness_generation 0.0042248502957952586\n",
      "best_fitness 0.0042248502957952586\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    26.91          1.07639       27       0.00504042              N/A      0.00s\n",
      "best_fitness_generation 0.00504042017330501\n",
      "best_fitness 0.0042248502957952586\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    27.10           1.1528       27       0.00504042              N/A      0.00s\n",
      "best_fitness_generation 0.00504042017330501\n",
      "best_fitness 0.0042248502957952586\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    26.91           1.1916       27       0.00504042              N/A      0.00s\n",
      "best_fitness_generation 0.00504042017330501\n",
      "best_fitness 0.0042248502957952586\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    27.32         0.949646       27       0.00504042              N/A      0.00s\n",
      "best_fitness_generation 0.00504042017330501\n",
      "best_fitness 0.0042248502957952586\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    26.94          1.11943       27       0.00504042              N/A      0.00s\n",
      "best_fitness_generation 0.00504042017330501\n",
      "best_fitness 0.0042248502957952586\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    27.19          1.02838       27       0.00504042              N/A      0.00s\n",
      "best_fitness_generation 0.00504042017330501\n",
      "best_fitness 0.0042248502957952586\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    26.91          1.36236       27       0.00504042              N/A      0.00s\n",
      "best_fitness_generation 0.00504042017330501\n",
      "best_fitness 0.0042248502957952586\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    26.95          1.24769       27       0.00504042              N/A      0.00s\n",
      "best_fitness_generation 0.00504042017330501\n",
      "best_fitness 0.0042248502957952586\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    27.01         0.939222       27       0.00504042              N/A      0.00s\n",
      "best_fitness_generation 0.00504042017330501\n",
      "best_fitness 0.0042248502957952586\n",
      "0.586106852497096 - 0.547*X0\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00           2646.9       31        0.0708448              N/A      0.00s\n",
      "best_fitness_generation 0.0708448324608351\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.91       31        0.0455387              N/A      0.00s\n",
      "best_fitness_generation 0.045538745092917554\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          1.60083       27         0.021586              N/A      0.00s\n",
      "best_fitness_generation 0.02158604225520981\n",
      "best_fitness 0.045538745092917554\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.19          1.55949       31       0.00936611              N/A      0.00s\n",
      "best_fitness_generation 0.009366106832830658\n",
      "best_fitness 0.02158604225520981\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    26.55          5.96842       31       0.00936611              N/A      0.00s\n",
      "best_fitness_generation 0.009366106832830658\n",
      "best_fitness 0.009366106832830658\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    24.02          1.07663       35       0.00899438              N/A      0.00s\n",
      "best_fitness_generation 0.008994376084935545\n",
      "best_fitness 0.009366106832830658\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    20.95         0.521679       33       0.00880776              N/A      0.00s\n",
      "best_fitness_generation 0.00880776120460147\n",
      "best_fitness 0.008994376084935545\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    17.06         0.557656       27       0.00921018              N/A      0.00s\n",
      "best_fitness_generation 0.009210183410416479\n",
      "best_fitness 0.00880776120460147\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    15.98         0.621313       23       0.00936611              N/A      0.00s\n",
      "best_fitness_generation 0.009366106832830658\n",
      "best_fitness 0.00880776120460147\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    11.10         0.617039       23       0.00936611              N/A      0.00s\n",
      "best_fitness_generation 0.009366106832830658\n",
      "best_fitness 0.00880776120460147\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    11.07          0.50545       11        0.0200093              N/A      0.00s\n",
      "best_fitness_generation 0.020009319109551057\n",
      "best_fitness 0.00880776120460147\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    10.97         0.636762       11        0.0200093              N/A      0.00s\n",
      "best_fitness_generation 0.020009319109551057\n",
      "best_fitness 0.00880776120460147\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    11.05         0.597206       11        0.0200093              N/A      0.00s\n",
      "best_fitness_generation 0.020009319109551057\n",
      "best_fitness 0.00880776120460147\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    10.96         0.840224       11        0.0200093              N/A      0.00s\n",
      "best_fitness_generation 0.020009319109551057\n",
      "best_fitness 0.00880776120460147\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    10.98         0.747082       11        0.0200093              N/A      0.00s\n",
      "best_fitness_generation 0.020009319109551057\n",
      "best_fitness 0.00880776120460147\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    10.99         0.508727       11        0.0200093              N/A      0.00s\n",
      "best_fitness_generation 0.020009319109551057\n",
      "best_fitness 0.00880776120460147\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    11.07         0.489257       11        0.0200093              N/A      0.00s\n",
      "best_fitness_generation 0.020009319109551057\n",
      "best_fitness 0.00880776120460147\n",
      "1.56678431372549*X0*(X0 + 0.041)\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.89       31        0.0890327              N/A      0.00s\n",
      "best_fitness_generation 0.08903274417758678\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          16.7017       27        0.0143788              N/A      0.00s\n",
      "best_fitness_generation 0.014378833507015628\n",
      "best_fitness 0.08903274417758678\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.82         0.781893       27       0.00969004              N/A      0.00s\n",
      "best_fitness_generation 0.009690036352103026\n",
      "best_fitness 0.014378833507015628\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    27.49         0.744608       27       0.00888205              N/A      0.00s\n",
      "best_fitness_generation 0.00888204557542146\n",
      "best_fitness 0.009690036352103026\n",
      "    |   Population Average    |             Best Individual              |    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.88       31        0.0977628              N/A      0.00s\n",
      "best_fitness_generation 0.09776280236805146\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          1.49856       35        0.0375334              N/A      0.00s\n",
      "best_fitness_generation 0.03753335670900666\n",
      "best_fitness 0.09776280236805146\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    31.67          1.50675       29        0.0274682              N/A      0.00s\n",
      "best_fitness_generation 0.02746823419903628\n",
      "best_fitness 0.03753335670900666\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    31.43          1.01714       29        0.0124198              N/A      0.00s\n",
      "best_fitness_generation 0.012419792833731414\n",
      "best_fitness 0.02746823419903628\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    27.42         0.621932       33        0.0114259              N/A      0.00s\n",
      "best_fitness_generation 0.011425901480589477\n",
      "best_fitness 0.012419792833731414\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    29.67         0.525846       33       0.00528348              N/A      0.00s\n",
      "best_fitness_generation 0.0052834768490027375\n",
      "best_fitness 0.011425901480589477\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    29.78         0.409604       43       0.00339637              N/A      0.00s\n",
      "best_fitness_generation 0.003396365320575407\n",
      "best_fitness 0.0052834768490027375\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    29.16          0.28967       35       0.00352005              N/A      0.00s\n",
      "best_fitness_generation 0.003520050704644367\n",
      "best_fitness 0.003396365320575407\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    27.41         0.394611       33       0.00372879              N/A      0.00s\n",
      "best_fitness_generation 0.0037287882342370944\n",
      "best_fitness 0.003396365320575407\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    30.41         0.273447       33       0.00372879              N/A      0.00s\n",
      "best_fitness_generation 0.0037287882342370944\n",
      "best_fitness 0.003396365320575407\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    32.81         0.300452       35       0.00336682              N/A      0.00s\n",
      "best_fitness_generation 0.0033668214928319977\n",
      "best_fitness 0.003396365320575407\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    32.37         0.310325       35       0.00336682              N/A      0.00s\n",
      "best_fitness_generation 0.0033668214928319977\n",
      "best_fitness 0.0033668214928319977\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    30.96         0.685948       33       0.00372879              N/A      0.00s\n",
      "best_fitness_generation 0.0037287882342370944\n",
      "best_fitness 0.0033668214928319977\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    30.97         0.551332       31       0.00476723              N/A      0.00s\n",
      "best_fitness_generation 0.004767231691282051\n",
      "best_fitness 0.0033668214928319977\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    30.90          0.26624       31       0.00476723              N/A      0.00s\n",
      "best_fitness_generation 0.004767231691282051\n",
      "best_fitness 0.0033668214928319977\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    31.05         0.289304       31       0.00476723              N/A      0.00s\n",
      "best_fitness_generation 0.004767231691282051\n",
      "best_fitness 0.0033668214928319977\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16    31.17         0.266813       31       0.00476723              N/A      0.00s\n",
      "best_fitness_generation 0.004767231691282051\n",
      "best_fitness 0.0033668214928319977\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  17    30.81         0.267952       31       0.00476723              N/A      0.00s\n",
      "best_fitness_generation 0.004767231691282051\n",
      "best_fitness 0.0033668214928319977\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  18    30.91         0.381863       31       0.00476723              N/A      0.00s\n",
      "best_fitness_generation 0.004767231691282051\n",
      "best_fitness 0.0033668214928319977\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  19    30.88         0.286551       31       0.00476723              N/A      0.00s\n",
      "best_fitness_generation 0.004767231691282051\n",
      "best_fitness 0.0033668214928319977\n",
      "    |   Population Average    |             Best Individual              |    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.85       31        0.0248527              N/A      0.00s\n",
      "best_fitness_generation 0.02485272697230875\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          1.47437       59        0.0129141              N/A      0.00s\n",
      "best_fitness_generation 0.012914080898433475\n",
      "best_fitness 0.02485272697230875\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.45         0.851427       43        0.0111481              N/A      0.00s\n",
      "best_fitness_generation 0.011148146359949608\n",
      "best_fitness 0.012914080898433475\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    26.97         0.869585       31        0.0106977              N/A      0.00s\n",
      "best_fitness_generation 0.01069771544674952\n",
      "best_fitness 0.011148146359949608\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    21.88         0.496566       25       0.00929222              N/A      0.00s\n",
      "best_fitness_generation 0.009292215551434778\n",
      "best_fitness 0.01069771544674952\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    19.11         0.495098       25       0.00929222              N/A      0.00s\n",
      "best_fitness_generation 0.009292215551434778\n",
      "best_fitness 0.009292215551434778\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    16.38         0.469629       25       0.00929222              N/A      0.00s\n",
      "best_fitness_generation 0.009292215551434778\n",
      "best_fitness 0.009292215551434778\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    15.05         0.462376       15        0.0130619              N/A      0.00s\n",
      "best_fitness_generation 0.013061890942374083\n",
      "best_fitness 0.009292215551434778\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    15.04         0.528141       15        0.0130619              N/A      0.00s\n",
      "best_fitness_generation 0.013061890942374083\n",
      "best_fitness 0.009292215551434778\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    15.13         0.444792       15        0.0130619              N/A      0.00s\n",
      "best_fitness_generation 0.013061890942374083\n",
      "best_fitness 0.009292215551434778\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    14.92         0.510951       15        0.0130619              N/A      0.00s\n",
      "best_fitness_generation 0.013061890942374083\n",
      "best_fitness 0.009292215551434778\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    15.03         0.500487       15        0.0130619              N/A      0.00s\n",
      "best_fitness_generation 0.013061890942374083\n",
      "best_fitness 0.009292215551434778\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    15.00         0.791693       15        0.0130619              N/A      0.00s\n",
      "best_fitness_generation 0.013061890942374083\n",
      "best_fitness 0.009292215551434778\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    14.98         0.712269       15        0.0130619              N/A      0.00s\n",
      "best_fitness_generation 0.013061890942374083\n",
      "best_fitness 0.009292215551434778\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    14.97         0.456489       15        0.0130619              N/A      0.00s\n",
      "best_fitness_generation 0.013061890942374083\n",
      "best_fitness 0.009292215551434778\n",
      "-X0**2*(1.05405177502319*X0 + 0.147018644067797)\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.82       31        0.0341218              N/A      0.00s\n",
      "best_fitness_generation 0.03412182889403293\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          3.01999       31        0.0144159              N/A      0.00s\n",
      "best_fitness_generation 0.014415884394572098\n",
      "best_fitness 0.03412182889403293\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.25          26.0523       43       0.00855586              N/A      0.00s\n",
      "best_fitness_generation 0.008555861900844982\n",
      "best_fitness 0.014415884394572098\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    16.49          1.54912       43       0.00855586              N/A      0.00s\n",
      "best_fitness_generation 0.008555861900844982\n",
      "best_fitness 0.008555861900844982\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    10.45         0.579799       25       0.00845851              N/A      0.00s\n",
      "best_fitness_generation 0.00845850838518133\n",
      "best_fitness 0.008555861900844982\n",
      "    |   Population Average    |             Best Individual              |    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2647.35       31         0.103935              N/A      0.00s\n",
      "best_fitness_generation 0.10393540123093853\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.56029       31        0.0334172              N/A      0.00s\n",
      "best_fitness_generation 0.033417197417526925\n",
      "best_fitness 0.10393540123093853\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.39          1.26271       37        0.0173911              N/A      0.00s\n",
      "best_fitness_generation 0.017391134592764597\n",
      "best_fitness 0.033417197417526925\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    30.37          1.82112       27         0.016285              N/A      0.00s\n",
      "best_fitness_generation 0.016284999345833467\n",
      "best_fitness 0.017391134592764597\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    24.20          1.05982       33        0.0131883              N/A      0.00s\n",
      "best_fitness_generation 0.01318832020337826\n",
      "best_fitness 0.016284999345833467\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    17.13         0.890736       27         0.016285              N/A      0.00s\n",
      "best_fitness_generation 0.016284999345833467\n",
      "best_fitness 0.01318832020337826\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    18.92         0.883992       21        0.0149119              N/A      0.00s\n",
      "best_fitness_generation 0.014911852928656901\n",
      "best_fitness 0.01318832020337826\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    18.12          0.88554       21        0.0149119              N/A      0.00s\n",
      "best_fitness_generation 0.014911852928656901\n",
      "best_fitness 0.01318832020337826\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    20.01          1.03456       21        0.0149119              N/A      0.00s\n",
      "best_fitness_generation 0.014911852928656901\n",
      "best_fitness 0.01318832020337826\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    21.10         0.672566       21        0.0149119              N/A      0.00s\n",
      "best_fitness_generation 0.014911852928656901\n",
      "best_fitness 0.01318832020337826\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    20.90          0.82841       21        0.0149119              N/A      0.00s\n",
      "best_fitness_generation 0.014911852928656901\n",
      "best_fitness 0.01318832020337826\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    20.97         0.786339       21        0.0149119              N/A      0.00s\n",
      "best_fitness_generation 0.014911852928656901\n",
      "best_fitness 0.01318832020337826\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    20.88         0.974621       21        0.0149119              N/A      0.00s\n",
      "best_fitness_generation 0.014911852928656901\n",
      "best_fitness 0.01318832020337826\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    20.99         0.894329       21        0.0149119              N/A      0.00s\n",
      "best_fitness_generation 0.014911852928656901\n",
      "best_fitness 0.01318832020337826\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    20.91         0.690085       21        0.0149119              N/A      0.00s\n",
      "best_fitness_generation 0.014911852928656901\n",
      "best_fitness 0.01318832020337826\n",
      "-(X0**2*(X0 + 0.204) + (X0 + 1.047)*(X0**2 + 1))/(X0 + 1.047)\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2647.04       31        0.0437969              N/A      0.00s\n",
      "best_fitness_generation 0.043796936179675026\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          60.4855       31        0.0245479              N/A      0.00s\n",
      "best_fitness_generation 0.024547850208070123\n",
      "best_fitness 0.043796936179675026\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.75          1.04517       43        0.0229513              N/A      0.00s\n",
      "best_fitness_generation 0.02295133203523438\n",
      "best_fitness 0.024547850208070123\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    29.87          1.53495       43        0.0228764              N/A      0.00s\n",
      "best_fitness_generation 0.02287635338437572\n",
      "best_fitness 0.02295133203523438\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    29.46          1.73683       19        0.0155701              N/A      0.00s\n",
      "best_fitness_generation 0.01557009670596492\n",
      "best_fitness 0.02287635338437572\n",
      "    |   Population Average    |             Best Individual              |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done   5 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=5)]: Done   6 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=5)]: Done   7 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=5)]: Done   9 tasks      | elapsed:  4.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93           1.9818       31        0.0334271              N/A      0.00s\n",
      "best_fitness_generation 0.03342709646140863\n",
      "best_fitness 0.0708448324608351\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.56         0.963072       47        0.0272404              N/A      0.00s\n",
      "best_fitness_generation 0.02724041808076455\n",
      "best_fitness 0.03342709646140863\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    28.30         0.843249       27        0.0136268              N/A      0.00s\n",
      "best_fitness_generation 0.013626758724121236\n",
      "best_fitness 0.02724041808076455\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    27.73         0.843194       35        0.0076489              N/A      0.00s\n",
      "best_fitness_generation 0.007648904936830382\n",
      "best_fitness 0.013626758724121236\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    23.44         0.894254       35        0.0076489              N/A      0.00s\n",
      "best_fitness_generation 0.007648904936830382\n",
      "best_fitness 0.007648904936830382\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    15.25          1.12725       35        0.0076489              N/A      0.00s\n",
      "best_fitness_generation 0.007648904936830382\n",
      "best_fitness 0.007648904936830382\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    11.40          1.15905       13        0.0204801              N/A      0.00s\n",
      "best_fitness_generation 0.020480092393287266\n",
      "best_fitness 0.007648904936830382\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    12.92          1.19316       13        0.0204801              N/A      0.00s\n",
      "best_fitness_generation 0.020480092393287266\n",
      "best_fitness 0.007648904936830382\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    13.10          1.00882       13        0.0204801              N/A      0.00s\n",
      "best_fitness_generation 0.020480092393287266\n",
      "best_fitness 0.007648904936830382\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    12.95          1.10503       13        0.0204801              N/A      0.00s\n",
      "best_fitness_generation 0.020480092393287266\n",
      "best_fitness 0.007648904936830382\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    13.05          1.12077       13        0.0204801              N/A      0.00s\n",
      "best_fitness_generation 0.020480092393287266\n",
      "best_fitness 0.007648904936830382\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    12.94          1.54975       13        0.0204801              N/A      0.00s\n",
      "best_fitness_generation 0.020480092393287266\n",
      "best_fitness 0.007648904936830382\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    12.97          1.48377       13        0.0204801              N/A      0.00s\n",
      "best_fitness_generation 0.020480092393287266\n",
      "best_fitness 0.007648904936830382\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    12.99          1.03453       13        0.0204801              N/A      0.00s\n",
      "best_fitness_generation 0.020480092393287266\n",
      "best_fitness 0.007648904936830382\n",
      "-(X0 - 0.295)*(X0 + 0.957)\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.96       31        0.0604194              N/A      0.00s\n",
      "best_fitness_generation 0.06041935799442642\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          5.92703       35        0.0244593              N/A      0.00s\n",
      "best_fitness_generation 0.02445929788022203\n",
      "best_fitness 0.06041935799442642\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.21          3.86853       35       0.00893924              N/A      0.00s\n",
      "best_fitness_generation 0.008939242472846973\n",
      "best_fitness 0.02445929788022203\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    27.72          4.92092       35        0.0059418              N/A      0.00s\n",
      "best_fitness_generation 0.005941796087563879\n",
      "best_fitness 0.008939242472846973\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    24.64          2.76964       31       0.00365784              N/A      0.00s\n",
      "best_fitness_generation 0.003657835840510755\n",
      "best_fitness 0.005941796087563879\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    17.78         0.473342       25       0.00353523              N/A      0.00s\n",
      "best_fitness_generation 0.0035352297487558693\n",
      "best_fitness 0.003657835840510755\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    15.90          0.35256       27       0.00888205              N/A      0.00s\n",
      "best_fitness_generation 0.00888204557542146\n",
      "best_fitness 0.00888204557542146\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    10.97         0.331316       11        0.0143788              N/A      0.00s\n",
      "best_fitness_generation 0.014378833507015628\n",
      "best_fitness 0.00888204557542146\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    11.00         0.377447       11        0.0143788              N/A      0.00s\n",
      "best_fitness_generation 0.014378833507015628\n",
      "best_fitness 0.00888204557542146\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    11.05         0.513382       11        0.0143788              N/A      0.00s\n",
      "best_fitness_generation 0.014378833507015628\n",
      "best_fitness 0.00888204557542146\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    11.06         0.562412       15        0.0113007              N/A      0.00s\n",
      "best_fitness_generation 0.01130067750637\n",
      "best_fitness 0.00888204557542146\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    11.07         0.329288       11        0.0143788              N/A      0.00s\n",
      "best_fitness_generation 0.014378833507015628\n",
      "best_fitness 0.00888204557542146\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    10.97         0.474471       11        0.0143788              N/A      0.00s\n",
      "best_fitness_generation 0.014378833507015628\n",
      "best_fitness 0.00888204557542146\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    11.05         0.399988       11        0.0143788              N/A      0.00s\n",
      "best_fitness_generation 0.014378833507015628\n",
      "best_fitness 0.00888204557542146\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    10.96         0.670124       11        0.0143788              N/A      0.00s\n",
      "best_fitness_generation 0.014378833507015628\n",
      "best_fitness 0.00888204557542146\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    10.98         0.554219       11        0.0143788              N/A      0.00s\n",
      "best_fitness_generation 0.014378833507015628\n",
      "best_fitness 0.00888204557542146\n",
      "-0.878*X0**3 - X0 + 0.453\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.88       31        0.0352017              N/A      0.00s\n",
      "best_fitness_generation 0.03520167070734715\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.10564       15        0.0137741              N/A      0.00s\n",
      "best_fitness_generation 0.013774086325499386\n",
      "best_fitness 0.03520167070734715\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.79          1.99047       43        0.0110863              N/A      0.00s\n",
      "best_fitness_generation 0.011086309559474528\n",
      "best_fitness 0.013774086325499386\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    28.23           1.7809       43       0.00930397              N/A      0.00s\n",
      "best_fitness_generation 0.009303966126133057\n",
      "best_fitness 0.011086309559474528\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    18.40          1.07132       23       0.00953881              N/A      0.00s\n",
      "best_fitness_generation 0.00953880856796533\n",
      "best_fitness 0.009303966126133057\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    14.89          1.20019       15        0.0130874              N/A      0.00s\n",
      "best_fitness_generation 0.013087393086865046\n",
      "best_fitness 0.009303966126133057\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    15.05          1.75345       15        0.0130874              N/A      0.00s\n",
      "best_fitness_generation 0.013087393086865046\n",
      "best_fitness 0.009303966126133057\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    15.05          1.35934       15        0.0130874              N/A      0.00s\n",
      "best_fitness_generation 0.013087393086865046\n",
      "best_fitness 0.009303966126133057\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    15.04          1.25455       15       0.00903186              N/A      0.00s\n",
      "best_fitness_generation 0.009031863772013869\n",
      "best_fitness 0.009303966126133057\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    15.13          1.13366       15       0.00837435              N/A      0.00s\n",
      "best_fitness_generation 0.008374350934136953\n",
      "best_fitness 0.009031863772013869\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  20    30.80         0.290205       31       0.00476723              N/A      0.00s\n",
      "best_fitness_generation 0.004767231691282051\n",
      "best_fitness 0.0033668214928319977\n",
      "-0.226*X0**3 - 1.176076*X0**2 - 0.125*X0 + 0.76907\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2647.31       31        0.0753257              N/A      0.00s\n",
      "best_fitness_generation 0.07532572700128873\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          6.67534       45        0.0459706              N/A      0.00s\n",
      "best_fitness_generation 0.045970580344624194\n",
      "best_fitness 0.07532572700128873\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    31.24          1.51332       49        0.0384523              N/A      0.00s\n",
      "best_fitness_generation 0.03845230116650437\n",
      "best_fitness 0.045970580344624194\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    31.85          1.26693       41        0.0201564              N/A      0.00s\n",
      "best_fitness_generation 0.020156416257394647\n",
      "best_fitness 0.03845230116650437\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    28.19         0.651928       41        0.0201564              N/A      0.00s\n",
      "best_fitness_generation 0.020156416257394647\n",
      "best_fitness 0.020156416257394647\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    28.59         0.697247       41        0.0201564              N/A      0.00s\n",
      "best_fitness_generation 0.020156416257394647\n",
      "best_fitness 0.020156416257394647\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    32.52         0.775759       33        0.0189029              N/A      0.00s\n",
      "best_fitness_generation 0.01890294986719649\n",
      "best_fitness 0.020156416257394647\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    29.58         0.773444       29        0.0181738              N/A      0.00s\n",
      "best_fitness_generation 0.01817382757652547\n",
      "best_fitness 0.01890294986719649\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    30.56         0.778311       29        0.0181738              N/A      0.00s\n",
      "best_fitness_generation 0.01817382757652547\n",
      "best_fitness 0.01817382757652547\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    29.59         0.610236       29        0.0181738              N/A      0.00s\n",
      "best_fitness_generation 0.01817382757652547\n",
      "best_fitness 0.01817382757652547\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    28.84         0.660913       29        0.0181738              N/A      0.00s\n",
      "best_fitness_generation 0.01817382757652547\n",
      "best_fitness 0.01817382757652547\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    29.10         0.659421       29        0.0181738              N/A      0.00s\n",
      "best_fitness_generation 0.01817382757652547\n",
      "best_fitness 0.01817382757652547\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    28.95          1.06474       29        0.0181738              N/A      0.00s\n",
      "best_fitness_generation 0.01817382757652547\n",
      "best_fitness 0.01817382757652547\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    29.07         0.963015       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01817382757652547\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    28.88         0.612007       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01730642764359665\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    29.03         0.844944       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01730642764359665\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16    28.96         0.617785       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01730642764359665\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  17    28.84         0.641009       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01730642764359665\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  18    28.84         0.725715       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01730642764359665\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    20.33         0.840409       43       0.00436903              N/A      0.00s\n",
      "best_fitness_generation 0.00436903213353219\n",
      "best_fitness 0.00845850838518133\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    22.42          9.80167       41       0.00451602              N/A      0.00s\n",
      "best_fitness_generation 0.004516023316580675\n",
      "best_fitness 0.00436903213353219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    21.51          0.97498       39       0.00552651              N/A      0.00s\n",
      "best_fitness_generation 0.0055265074951025735\n",
      "best_fitness 0.00436903213353219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    20.99         0.822505       21        0.0090049              N/A      0.00s\n",
      "best_fitness_generation 0.009004902917173797\n",
      "best_fitness 0.00436903213353219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    21.16         0.731165       21        0.0090049              N/A      0.00s\n",
      "best_fitness_generation 0.009004902917173797\n",
      "best_fitness 0.00436903213353219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    20.97          0.73999       21        0.0090049              N/A      0.00s\n",
      "best_fitness_generation 0.009004902917173797\n",
      "best_fitness 0.00436903213353219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    20.95         0.814634       21        0.0090049              N/A      0.00s\n",
      "best_fitness_generation 0.009004902917173797\n",
      "best_fitness 0.00436903213353219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    20.85           1.0273       21        0.0090049              N/A      0.00s\n",
      "best_fitness_generation 0.009004902917173797\n",
      "best_fitness 0.00436903213353219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    20.92         0.817463       21        0.0090049              N/A      0.00s\n",
      "best_fitness_generation 0.009004902917173797\n",
      "best_fitness 0.00436903213353219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    20.96         0.722518       21        0.0090049              N/A      0.00s\n",
      "best_fitness_generation 0.009004902917173797\n",
      "best_fitness 0.00436903213353219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    20.97         0.735835       21        0.0090049              N/A      0.00s\n",
      "best_fitness_generation 0.009004902917173797\n",
      "best_fitness 0.00436903213353219\n",
      "-(1.367*X0 + 0.313)/(3.73267326732673*X0 + 1.92946058091286)\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.93       31        0.0229321              N/A      0.00s\n",
      "best_fitness_generation 0.022932124292443984\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          3.19838       31        0.0190338              N/A      0.00s\n",
      "best_fitness_generation 0.01903376151659562\n",
      "best_fitness 0.022932124292443984\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    27.96         0.769259       49       0.00951072              N/A      0.00s\n",
      "best_fitness_generation 0.009510715636567633\n",
      "best_fitness 0.01903376151659562\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    28.32          0.86169       27       0.00967115              N/A      0.00s\n",
      "best_fitness_generation 0.009671145509578916\n",
      "best_fitness 0.009510715636567633\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    19.66          1.19478       27       0.00967115              N/A      0.00s\n",
      "best_fitness_generation 0.009671145509578916\n",
      "best_fitness 0.009510715636567633\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    17.39          1.43012       21         0.010503              N/A      0.00s\n",
      "best_fitness_generation 0.01050304037934892\n",
      "best_fitness 0.009510715636567633\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    13.11         0.895849       13         0.011933              N/A      0.00s\n",
      "best_fitness_generation 0.011933005564351618\n",
      "best_fitness 0.009510715636567633\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    13.01         0.812217       13        0.0098714              N/A      0.00s\n",
      "best_fitness_generation 0.009871404832375945\n",
      "best_fitness 0.009510715636567633\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    13.01         0.947363       13        0.0098714              N/A      0.00s\n",
      "best_fitness_generation 0.009871404832375943\n",
      "best_fitness 0.009510715636567633\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    28.39          1.63425       35       0.00968141              N/A      0.00s\n",
      "best_fitness_generation 0.009681406282428844\n",
      "best_fitness 0.01557009670596492\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    25.33            1.508       35       0.00968141              N/A      0.00s\n",
      "best_fitness_generation 0.009681406282428844\n",
      "best_fitness 0.009681406282428844\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    20.87          0.99724       35       0.00968141              N/A      0.00s\n",
      "best_fitness_generation 0.009681406282428844\n",
      "best_fitness 0.009681406282428844\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    19.05          0.91618       19        0.0155701              N/A      0.00s\n",
      "best_fitness_generation 0.01557009670596492\n",
      "best_fitness 0.009681406282428844\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    19.10         0.769337       19        0.0155701              N/A      0.00s\n",
      "best_fitness_generation 0.01557009670596492\n",
      "best_fitness 0.009681406282428844\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    18.92         0.904287       19         0.015273              N/A      0.00s\n",
      "best_fitness_generation 0.015272962067720176\n",
      "best_fitness 0.009681406282428844\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    18.98         0.831277       19         0.015273              N/A      0.00s\n",
      "best_fitness_generation 0.015272962067720176\n",
      "best_fitness 0.009681406282428844\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    18.90          1.17018       19         0.015273              N/A      0.00s\n",
      "best_fitness_generation 0.015272962067720176\n",
      "best_fitness 0.009681406282428844\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    18.97          1.05985       19         0.015273              N/A      0.00s\n",
      "best_fitness_generation 0.015272962067720176\n",
      "best_fitness 0.009681406282428844\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    18.91         0.757524       19         0.015273              N/A      0.00s\n",
      "best_fitness_generation 0.015272962067720176\n",
      "best_fitness 0.009681406282428844\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    19.10         0.931999       19         0.015273              N/A      0.00s\n",
      "best_fitness_generation 0.015272962067720176\n",
      "best_fitness 0.009681406282428844\n",
      "-0.230244655581948*X0**2 - 0.961629919239905*X0 - 0.316785451\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2647.13       31        0.0704154              N/A      0.00s\n",
      "best_fitness_generation 0.07041538049391206\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.27814       29        0.0451051              N/A      0.00s\n",
      "best_fitness_generation 0.04510512280901309\n",
      "best_fitness 0.07041538049391206\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.41          1.75991       35        0.0158757              N/A      0.00s\n",
      "best_fitness_generation 0.015875653345400042\n",
      "best_fitness 0.04510512280901309\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    27.02          2.33099       35        0.0158757              N/A      0.00s\n",
      "best_fitness_generation 0.015875653345400042\n",
      "best_fitness 0.015875653345400042\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    23.49          1.04449       33        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.015875653345400042\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    26.17         0.856778       33        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.010153868209584764\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    21.66         0.816351       33        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.010153868209584764\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    29.54         0.848747       39        0.0095533              N/A      0.00s\n",
      "best_fitness_generation 0.009553295712468952\n",
      "best_fitness 0.010153868209584764\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    30.01         0.885371       29        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.009553295712468952\n",
      "    |   Population Average    |             Best Individual              |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  10 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=5)]: Done  11 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=5)]: Done  12 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=5)]: Done  13 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=5)]: Done  14 tasks      | elapsed:  8.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   6    15.03         0.554642       21       0.00428476              N/A      0.00s\n",
      "best_fitness_generation 0.0042847583501416965\n",
      "best_fitness 0.0035352297487558693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    11.73         0.605616       15       0.00514038              N/A      0.00s\n",
      "best_fitness_generation 0.005140375786134484\n",
      "best_fitness 0.0035352297487558693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    11.02         0.568894       11       0.00528243              N/A      0.00s\n",
      "best_fitness_generation 0.005282429999309197\n",
      "best_fitness 0.0035352297487558693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    11.07         0.451879       11       0.00514038              N/A      0.00s\n",
      "best_fitness_generation 0.005140375786134484\n",
      "best_fitness 0.0035352297487558693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    10.90         0.498748       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.0035352297487558693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    11.09         0.506193       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.002802039351688242\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    11.63         0.878096       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.002802039351688242\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    13.09         0.664218       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.002802039351688242\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    13.01         0.408502       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.002802039351688242\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    13.02         0.409108       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.002802039351688242\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16    13.03         0.373262       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.002802039351688242\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  17    12.95         0.457161       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.002802039351688242\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  18    12.94         0.467277       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.002802039351688242\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  19    13.00         0.439661       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.002802039351688242\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  20    12.89         0.459171       13       0.00280204              N/A      0.00s\n",
      "best_fitness_generation 0.002802039351688242\n",
      "best_fitness 0.002802039351688242\n",
      "0.905964846560847 - 0.292412698412698*X0\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2647.03       31        0.0535738              N/A      0.00s\n",
      "best_fitness_generation 0.053573846430070536\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.97468       31        0.0283648              N/A      0.00s\n",
      "best_fitness_generation 0.028364819260569323\n",
      "best_fitness 0.053573846430070536\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.90          1.30545       27       0.00714599              N/A      0.00s\n",
      "best_fitness_generation 0.007145987549600939\n",
      "best_fitness 0.028364819260569323\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    31.09          1.75829       27       0.00673836              N/A      0.00s\n",
      "best_fitness_generation 0.006738359424222649\n",
      "best_fitness 0.007145987549600939\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    30.20          1.78693       29       0.00635085              N/A      0.00s\n",
      "best_fitness_generation 0.0063508498926702385\n",
      "best_fitness 0.006738359424222649\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    14.92          1.35923       15       0.00837435              N/A      0.00s\n",
      "best_fitness_generation 0.008374350934136953\n",
      "best_fitness 0.008374350934136953\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    15.03          1.16152       15       0.00837435              N/A      0.00s\n",
      "best_fitness_generation 0.008374350934136953\n",
      "best_fitness 0.008374350934136953\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    15.00          1.56832       15       0.00837435              N/A      0.00s\n",
      "best_fitness_generation 0.008374350934136953\n",
      "best_fitness 0.008374350934136953\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    14.98          1.31884       15       0.00835059              N/A      0.00s\n",
      "best_fitness_generation 0.008350592154999295\n",
      "best_fitness 0.008374350934136953\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    14.97          1.10747       15       0.00835059              N/A      0.00s\n",
      "best_fitness_generation 0.008350592154999295\n",
      "best_fitness 0.008350592154999295\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    15.06          1.09186       15       0.00835059              N/A      0.00s\n",
      "best_fitness_generation 0.008350592154999295\n",
      "best_fitness 0.008350592154999295\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16    15.09          1.06838       15       0.00835059              N/A      0.00s\n",
      "best_fitness_generation 0.008350592154999295\n",
      "best_fitness 0.008350592154999295\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  17    14.92          1.08508       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008350592154999295\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  18    14.86          1.75451       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008336228506784503\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  19    15.02          4.48217       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008336228506784503\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  20    14.77          1.14688       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008336228506784503\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  21    14.99          1.11334       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008336228506784503\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  22    15.04          1.07211       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008336228506784503\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  23    15.09          1.10391       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008336228506784503\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  24    15.04          1.13829       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008336228506784503\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  25    15.03          1.09981       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008336228506784503\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  26    14.92          1.10873       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008336228506784503\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  27    15.08          1.08247       15       0.00833623              N/A      0.00s\n",
      "best_fitness_generation 0.008336228506784503\n",
      "best_fitness 0.008336228506784503\n",
      "-3.44851369059935*X0**2/(1.23456790123457*X0 + 1.892)\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.93       31        0.0586204              N/A      0.00s\n",
      "best_fitness_generation 0.058620384899895774\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          16.6119       59        0.0270972              N/A      0.00s\n",
      "best_fitness_generation 0.027097219248552164\n",
      "best_fitness 0.058620384899895774\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  19    28.89         0.614779       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01730642764359665\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  20    28.82           0.7171       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01730642764359665\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  21    28.96         0.597109       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01730642764359665\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  22    29.08          3.72486       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01730642764359665\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  23    29.02           0.6436       29        0.0173064              N/A      0.00s\n",
      "best_fitness_generation 0.01730642764359665\n",
      "best_fitness 0.01730642764359665\n",
      "2.0*X0**2 + 0.260875912408759*X0 + 0.686460262773723\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2647.15       31         0.107702              N/A      0.00s\n",
      "best_fitness_generation 0.10770196369028634\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93           2.7937       27        0.0293499              N/A      0.00s\n",
      "best_fitness_generation 0.029349947852650632\n",
      "best_fitness 0.10770196369028634\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.82         0.828789       35        0.0133293              N/A      0.00s\n",
      "best_fitness_generation 0.013329268880119219\n",
      "best_fitness 0.029349947852650632\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    31.83          1.04395       35        0.0133293              N/A      0.00s\n",
      "best_fitness_generation 0.013329268880119219\n",
      "best_fitness 0.013329268880119219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    27.84         0.539338       33        0.0128697              N/A      0.00s\n",
      "best_fitness_generation 0.012869732182961852\n",
      "best_fitness 0.013329268880119219\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    29.63         0.713248       35        0.0112984              N/A      0.00s\n",
      "best_fitness_generation 0.0112984109116114\n",
      "best_fitness 0.012869732182961852\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    31.61         0.658264       35        0.0112984              N/A      0.00s\n",
      "best_fitness_generation 0.0112984109116114\n",
      "best_fitness 0.0112984109116114\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    30.64         0.569346       41        0.0133145              N/A      0.00s\n",
      "best_fitness_generation 0.013314541584546561\n",
      "best_fitness 0.0112984109116114\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    27.71         0.751797       31        0.0133293              N/A      0.00s\n",
      "best_fitness_generation 0.013329268880119219\n",
      "best_fitness 0.0112984109116114\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    27.13         0.530036       29        0.0138855              N/A      0.00s\n",
      "best_fitness_generation 0.013885545369791905\n",
      "best_fitness 0.0112984109116114\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    27.17         0.677422       29        0.0134079              N/A      0.00s\n",
      "best_fitness_generation 0.013407881089170206\n",
      "best_fitness 0.0112984109116114\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    28.73         0.553299       31        0.0119275              N/A      0.00s\n",
      "best_fitness_generation 0.011927543920814708\n",
      "best_fitness 0.0112984109116114\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    27.75         0.900211       31        0.0117114              N/A      0.00s\n",
      "best_fitness_generation 0.011711381905110839\n",
      "best_fitness 0.0112984109116114\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    25.14         0.751704       29        0.0134079              N/A      0.00s\n",
      "best_fitness_generation 0.013407881089170206\n",
      "best_fitness 0.0112984109116114\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    24.90         0.463541       25        0.0173384              N/A      0.00s\n",
      "best_fitness_generation 0.017338356227910315\n",
      "best_fitness 0.0112984109116114\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    28.73         0.587338       29        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.009553295712468952\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    28.90         0.741914       29        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.009553295712468952\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    28.96          0.64045       29        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.009553295712468952\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    28.97         0.830813       29        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.009553295712468952\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    29.05         0.673428       29        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.009553295712468952\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    28.83         0.571979       29        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.009553295712468952\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    29.19         0.741183       29        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.009553295712468952\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16    29.04         0.553789       29        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.009553295712468952\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  17    28.86         0.754839       29        0.0101539              N/A      0.00s\n",
      "best_fitness_generation 0.010153868209584764\n",
      "best_fitness 0.009553295712468952\n",
      "-X0**2 - 0.4025*X0 - 0.5380995\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.83       31        0.0549776              N/A      0.00s\n",
      "best_fitness_generation 0.054977647175143626\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93           1.8326       31        0.0390237              N/A      0.00s\n",
      "best_fitness_generation 0.039023734540437394\n",
      "best_fitness 0.054977647175143626\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    28.90          9.47209       31         0.014303              N/A      0.00s\n",
      "best_fitness_generation 0.01430303930250349\n",
      "best_fitness 0.039023734540437394\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    22.78          1.18632       37        0.0106131              N/A      0.00s\n",
      "best_fitness_generation 0.01061305942687421\n",
      "best_fitness 0.01430303930250349\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    17.50         0.693185       37        0.0106131              N/A      0.00s\n",
      "best_fitness_generation 0.01061305942687421\n",
      "best_fitness 0.01061305942687421\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    15.70         0.455077       21         0.010422              N/A      0.00s\n",
      "best_fitness_generation 0.010422042337561123\n",
      "best_fitness 0.01061305942687421\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6     9.46         0.504207       13          0.01053              N/A      0.00s\n",
      "best_fitness_generation 0.010530028150260825\n",
      "best_fitness 0.010422042337561123\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    10.48         0.526631       13          0.01053              N/A      0.00s\n",
      "best_fitness_generation 0.010530028150260825\n",
      "best_fitness 0.010422042337561123\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    13.02         0.704201       13          0.01053              N/A      0.00s\n",
      "best_fitness_generation 0.010530028150260825\n",
      "best_fitness 0.010422042337561123\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    13.08         0.627413       13          0.01053              N/A      0.00s\n",
      "best_fitness_generation 0.010530028150260825\n",
      "best_fitness 0.010422042337561123\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    12.86         0.640398       13          0.01053              N/A      0.00s\n",
      "best_fitness_generation 0.010530028150260825\n",
      "best_fitness 0.010422042337561123\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  8.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    13.07         0.719328       13        0.0098714              N/A      0.00s\n",
      "best_fitness_generation 0.009871404832375943\n",
      "best_fitness 0.009510715636567633\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    12.87         0.734438       13        0.0098714              N/A      0.00s\n",
      "best_fitness_generation 0.009871404832375943\n",
      "best_fitness 0.009510715636567633\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    13.10         0.763685       13        0.0098714              N/A      0.00s\n",
      "best_fitness_generation 0.009871404832375943\n",
      "best_fitness 0.009510715636567633\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    12.93         0.900656       13        0.0098714              N/A      0.00s\n",
      "best_fitness_generation 0.009871404832375943\n",
      "best_fitness 0.009510715636567633\n",
      "(X0 + 0.582)/(X0 + 1.05)\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.95       31        0.0222509              N/A      0.00s\n",
      "best_fitness_generation 0.02225085944448037\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93           119.92       35        0.0210973              N/A      0.00s\n",
      "best_fitness_generation 0.021097315554862547\n",
      "best_fitness 0.02225085944448037\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    31.07          17.9136       33       0.00748415              N/A      0.00s\n",
      "best_fitness_generation 0.007484146392552274\n",
      "best_fitness 0.021097315554862547\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    26.59         0.863819       37       0.00558395              N/A      0.00s\n",
      "best_fitness_generation 0.00558395078945532\n",
      "best_fitness 0.007484146392552274\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    24.51         0.676138       37       0.00558395              N/A      0.00s\n",
      "best_fitness_generation 0.00558395078945532\n",
      "best_fitness 0.00558395078945532\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    24.59          0.46425       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.00558395078945532\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    24.94         0.497652       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.005005413935569812\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    25.35         0.471641       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.005005413935569812\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    26.88         0.488927       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.005005413935569812\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    27.26         0.399904       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.005005413935569812\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    27.01         0.441799       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.005005413935569812\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    27.07         0.445632       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.005005413935569812\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    26.86         0.800842       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.005005413935569812\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    27.10         0.782638       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.005005413935569812\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    27.02         0.405898       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.005005413935569812\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    27.08         0.382261       27       0.00500541              N/A      0.00s\n",
      "best_fitness_generation 0.005005413935569812\n",
      "best_fitness 0.005005413935569812\n",
      "X0**3*(0.485*X0 - 0.362295) - 0.468*X0 - 0.447456\n",
      "    |   Population Average    |             Best Individual              |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  16 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=5)]: Done  17 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=5)]: Done  18 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=5)]: Done  19 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=5)]: Done  20 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=5)]: Done  21 tasks      | elapsed: 10.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   5    26.17          3.60888       21       0.00634088              N/A      0.00s\n",
      "best_fitness_generation 0.00634087706475365\n",
      "best_fitness 0.0063508498926702385\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    21.82          6.54274       21       0.00634088              N/A      0.00s\n",
      "best_fitness_generation 0.00634087706475365\n",
      "best_fitness 0.00634087706475365\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    19.01          2.47475       21       0.00634088              N/A      0.00s\n",
      "best_fitness_generation 0.00634087706475365\n",
      "best_fitness 0.00634087706475365\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    18.45          2.63926       19       0.00671311              N/A      0.00s\n",
      "best_fitness_generation 0.006713114759616681\n",
      "best_fitness 0.00634087706475365\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    16.96          2.75373       19       0.00671311              N/A      0.00s\n",
      "best_fitness_generation 0.006713114759616681\n",
      "best_fitness 0.00634087706475365\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    16.19          3.27307       19       0.00747386              N/A      0.00s\n",
      "best_fitness_generation 0.007473862200530984\n",
      "best_fitness 0.00634087706475365\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    15.08          2.37793       15       0.00747386              N/A      0.00s\n",
      "best_fitness_generation 0.007473862200530989\n",
      "best_fitness 0.00634087706475365\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    14.99          2.33597       15       0.00747386              N/A      0.00s\n",
      "best_fitness_generation 0.007473862200530989\n",
      "best_fitness 0.00634087706475365\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    14.99          2.28261       15       0.00747386              N/A      0.00s\n",
      "best_fitness_generation 0.007473862200530989\n",
      "best_fitness 0.00634087706475365\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    14.94          2.31528       15       0.00747386              N/A      0.00s\n",
      "best_fitness_generation 0.007473862200530989\n",
      "best_fitness 0.00634087706475365\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    15.03          2.09183       15       0.00747386              N/A      0.00s\n",
      "best_fitness_generation 0.007473862200530989\n",
      "best_fitness 0.00634087706475365\n",
      "-(2*X0 + 0.995)/(X0 + 1.749)\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.81       31        0.0476363              N/A      0.00s\n",
      "best_fitness_generation 0.047636332611431555\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93           1.4804       27        0.0202793              N/A      0.00s\n",
      "best_fitness_generation 0.020279260550197062\n",
      "best_fitness 0.047636332611431555\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    27.94         0.952756       31        0.0126575              N/A      0.00s\n",
      "best_fitness_generation 0.012657523088945936\n",
      "best_fitness 0.020279260550197062\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    16.37         0.892393       31       0.00509518              N/A      0.00s\n",
      "best_fitness_generation 0.0050951790348433695\n",
      "best_fitness 0.012657523088945936\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    13.63         0.569339       31       0.00509518              N/A      0.00s\n",
      "best_fitness_generation 0.0050951790348433695\n",
      "best_fitness 0.0050951790348433695\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5     5.05         0.273995        5        0.0272104              N/A      0.00s\n",
      "best_fitness_generation 0.027210398611088395\n",
      "best_fitness 0.0050951790348433695\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6     5.01         0.363697        5        0.0272104              N/A      0.00s\n",
      "best_fitness_generation 0.027210398611088395\n",
      "best_fitness 0.0050951790348433695\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7     5.06         0.429315        5        0.0272104              N/A      0.00s\n",
      "best_fitness_generation 0.027210398611088395\n",
      "best_fitness 0.0050951790348433695\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8     5.06         0.456148        5        0.0272104              N/A      0.00s\n",
      "best_fitness_generation 0.027210398611088395\n",
      "best_fitness 0.0050951790348433695\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9     5.08          0.25229        5        0.0272104              N/A      0.00s\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    31.23          2.34886       59        0.0217747              N/A      0.00s\n",
      "best_fitness_generation 0.021774739310781135\n",
      "best_fitness 0.027097219248552164\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    28.27          2.01243       53        0.0198192              N/A      0.00s\n",
      "best_fitness_generation 0.019819161132486416\n",
      "best_fitness 0.021774739310781135\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    25.00          1.36277       35        0.0237551              N/A      0.00s\n",
      "best_fitness_generation 0.02375505491944374\n",
      "best_fitness 0.019819161132486416\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    22.18          1.78071       23        0.0192523              N/A      0.00s\n",
      "best_fitness_generation 0.01925232586745313\n",
      "best_fitness 0.019819161132486416\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    17.22          2.27104       23        0.0192523              N/A      0.00s\n",
      "best_fitness_generation 0.01925232586745313\n",
      "best_fitness 0.01925232586745313\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    17.59          2.18088       23        0.0192523              N/A      0.00s\n",
      "best_fitness_generation 0.01925232586745313\n",
      "best_fitness 0.01925232586745313\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    20.37          2.01514       37        0.0151628              N/A      0.00s\n",
      "best_fitness_generation 0.015162751929044243\n",
      "best_fitness 0.01925232586745313\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    23.21          1.74879       37        0.0151628              N/A      0.00s\n",
      "best_fitness_generation 0.015162751929044243\n",
      "best_fitness 0.015162751929044243\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    22.42          1.99687       37        0.0151628              N/A      0.00s\n",
      "best_fitness_generation 0.015162751929044243\n",
      "best_fitness 0.015162751929044243\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    17.25          2.05175       37        0.0151628              N/A      0.00s\n",
      "best_fitness_generation 0.015162751929044243\n",
      "best_fitness 0.015162751929044243\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12     8.95          2.52637       23        0.0192523              N/A      0.00s\n",
      "best_fitness_generation 0.01925232586745313\n",
      "best_fitness 0.015162751929044243\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13     9.05          2.41401        9        0.0280309              N/A      0.00s\n",
      "best_fitness_generation 0.028030874779149555\n",
      "best_fitness 0.015162751929044243\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14     9.02          2.22205        9        0.0280309              N/A      0.00s\n",
      "best_fitness_generation 0.028030874779149555\n",
      "best_fitness 0.015162751929044243\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15     9.06          2.08493        9        0.0280309              N/A      0.00s\n",
      "best_fitness_generation 0.028030874779149555\n",
      "best_fitness 0.015162751929044243\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16     8.97          2.17501        9        0.0280309              N/A      0.00s\n",
      "best_fitness_generation 0.028030874779149555\n",
      "best_fitness 0.015162751929044243\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  17     8.97          2.12773        9        0.0280309              N/A      0.00s\n",
      "best_fitness_generation 0.028030874779149555\n",
      "best_fitness 0.015162751929044243\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  18     8.94          2.27167        9        0.0280309              N/A      0.00s\n",
      "best_fitness_generation 0.028030874779149555\n",
      "best_fitness 0.015162751929044243\n",
      "0.431726907630522*X0**2 - 0.714\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.95       31        0.0508713              N/A      0.00s\n",
      "best_fitness_generation 0.050871288537483336\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          122.189       43          0.02042              N/A      0.00s\n",
      "best_fitness_generation 0.020420032842545944\n",
      "best_fitness 0.050871288537483336\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.90          116.286       39       0.00381852              N/A      0.00s\n",
      "best_fitness_generation 0.0038185198554258094\n",
      "best_fitness 0.020420032842545944\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    25.07         0.653778       25        0.0173384              N/A      0.00s\n",
      "best_fitness_generation 0.017338356227910315\n",
      "best_fitness 0.0112984109116114\n",
      "X0**2*(X0 + 0.254) + 0.742*X0 + 0.474336225596529\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.79       31        0.0182842              N/A      0.00s\n",
      "best_fitness_generation 0.018284191361219608\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          1.40566       31       0.00898059              N/A      0.00s\n",
      "best_fitness_generation 0.00898058569630862\n",
      "best_fitness 0.018284191361219608\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    28.20          1.41223       35       0.00810012              N/A      0.00s\n",
      "best_fitness_generation 0.008100122160266432\n",
      "best_fitness 0.00898058569630862\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    19.91         0.999239       35       0.00810012              N/A      0.00s\n",
      "best_fitness_generation 0.008100122160266432\n",
      "best_fitness 0.008100122160266432\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4     7.07         0.337897       31        0.0150741              N/A      0.00s\n",
      "best_fitness_generation 0.015074118960525101\n",
      "best_fitness 0.008100122160266432\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5     7.03         0.318011        7        0.0263324              N/A      0.00s\n",
      "best_fitness_generation 0.026332386047471984\n",
      "best_fitness 0.008100122160266432\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6     7.00         0.353791        7        0.0263324              N/A      0.00s\n",
      "best_fitness_generation 0.026332386047471984\n",
      "best_fitness 0.008100122160266432\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7     7.07         0.385879        7        0.0263324              N/A      0.00s\n",
      "best_fitness_generation 0.026332386047471984\n",
      "best_fitness 0.008100122160266432\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8     7.06          0.43185        7        0.0263324              N/A      0.00s\n",
      "best_fitness_generation 0.026332386047471984\n",
      "best_fitness 0.008100122160266432\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9     7.06         0.303396        7        0.0263324              N/A      0.00s\n",
      "best_fitness_generation 0.026332386047471984\n",
      "best_fitness 0.008100122160266432\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10     7.00         0.477678        7        0.0263324              N/A      0.00s\n",
      "best_fitness_generation 0.026332386047471984\n",
      "best_fitness 0.008100122160266432\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11     7.07         0.383207        7        0.0263324              N/A      0.00s\n",
      "best_fitness_generation 0.026332386047471984\n",
      "best_fitness 0.008100122160266432\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12     6.99         0.569384        7        0.0263324              N/A      0.00s\n",
      "best_fitness_generation 0.026332386047471984\n",
      "best_fitness 0.008100122160266432\n",
      "-0.344*X0**2\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00           2646.9       31         0.016629              N/A      0.00s\n",
      "best_fitness_generation 0.016629004441723577\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          3.86196       27       0.00893491              N/A      0.00s\n",
      "best_fitness_generation 0.00893491303356231\n",
      "best_fitness 0.016629004441723577\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.75          4.48356       31       0.00590065              N/A      0.00s\n",
      "best_fitness_generation 0.00590065155479693\n",
      "best_fitness 0.00893491303356231\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    16.59          22.2908       31       0.00590065              N/A      0.00s\n",
      "best_fitness_generation 0.00590065155479693\n",
      "best_fitness 0.00590065155479693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4     6.46         0.713617       11        0.0141132              N/A      0.00s\n",
      "best_fitness_generation 0.01411319292638654\n",
      "best_fitness 0.00590065155479693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5     1.90        0.0886372        5        0.0142521              N/A      0.00s\n",
      "best_fitness_generation 0.014252126189735424\n",
      "best_fitness 0.00590065155479693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed: 11.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    13.10         0.701395       13          0.01053              N/A      0.00s\n",
      "best_fitness_generation 0.010530028150260825\n",
      "best_fitness 0.010422042337561123\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    12.94         0.922027       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010422042337561123\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    13.08         0.818338       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010193276353727517\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    13.04         0.660155       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010193276353727517\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    13.04         0.636053       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010193276353727517\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16    13.08         0.628531       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010193276353727517\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  17    12.91         0.624671       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010193276353727517\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  18    13.00          0.64355       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010193276353727517\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  19    12.97         0.591696       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010193276353727517\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  20    12.87         0.656503       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010193276353727517\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  21    12.95         0.598788       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010193276353727517\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  22    13.14         0.624542       13        0.0101933              N/A      0.00s\n",
      "best_fitness_generation 0.010193276353727517\n",
      "best_fitness 0.010193276353727517\n",
      "-(X0 - 0.74)*(X0*(0.645*X0 + 0.661) + 0.485)/(0.645*X0 + 0.661)\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2647.41       31        0.0664464              N/A      0.00s\n",
      "best_fitness_generation 0.06644639712463629\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          4.25921       33        0.0296935              N/A      0.00s\n",
      "best_fitness_generation 0.02969345900011426\n",
      "best_fitness 0.06644639712463629\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.10          1.96404       33        0.0256901              N/A      0.00s\n",
      "best_fitness_generation 0.025690110175433142\n",
      "best_fitness 0.02969345900011426\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    29.94          1.95482       35        0.0175376              N/A      0.00s\n",
      "best_fitness_generation 0.017537606106911352\n",
      "best_fitness 0.025690110175433142\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    31.11          3.45218       33        0.0158203              N/A      0.00s\n",
      "best_fitness_generation 0.01582029419816072\n",
      "best_fitness 0.017537606106911352\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    30.64          3.00637       35        0.0138787              N/A      0.00s\n",
      "best_fitness_generation 0.013878747175886736\n",
      "best_fitness 0.01582029419816072\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    33.43          3.36864       35        0.0138787              N/A      0.00s\n",
      "best_fitness_generation 0.013878747175886736\n",
      "best_fitness 0.013878747175886736\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    32.66          3.65947       35        0.0138787              N/A      0.00s\n",
      "best_fitness_generation 0.013878747175886736\n",
      "best_fitness 0.013878747175886736\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2647.03       31        0.0522373              N/A      0.00s\n",
      "best_fitness_generation 0.0522372812145947\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          60.3753       29        0.0149163              N/A      0.00s\n",
      "best_fitness_generation 0.01491632072864557\n",
      "best_fitness 0.0522372812145947\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.70          84.2612       37        0.0049783              N/A      0.00s\n",
      "best_fitness_generation 0.004978299405593083\n",
      "best_fitness 0.01491632072864557\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    29.87          1.34541       39       0.00304932              N/A      0.00s\n",
      "best_fitness_generation 0.0030493221984207075\n",
      "best_fitness 0.004978299405593083\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    28.20          1.06213       39       0.00304932              N/A      0.00s\n",
      "best_fitness_generation 0.0030493221984207075\n",
      "best_fitness 0.0030493221984207075\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    28.15         0.839977       29       0.00611336              N/A      0.00s\n",
      "best_fitness_generation 0.006113358855840634\n",
      "best_fitness 0.0030493221984207075\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    29.13         0.741281       31       0.00535332              N/A      0.00s\n",
      "best_fitness_generation 0.005353320153314041\n",
      "best_fitness 0.0030493221984207075\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    26.56         0.751898       31       0.00556581              N/A      0.00s\n",
      "best_fitness_generation 0.00556581494381856\n",
      "best_fitness 0.0030493221984207075\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    23.15         0.769771       29       0.00611336              N/A      0.00s\n",
      "best_fitness_generation 0.006113358855840634\n",
      "best_fitness 0.0030493221984207075\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    21.14          0.53462       23       0.00720192              N/A      0.00s\n",
      "best_fitness_generation 0.007201922839475486\n",
      "best_fitness 0.0030493221984207075\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    20.96         0.691998       21        0.0079882              N/A      0.00s\n",
      "best_fitness_generation 0.007988195751786659\n",
      "best_fitness 0.0030493221984207075\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    20.95         0.623944       21        0.0079882              N/A      0.00s\n",
      "best_fitness_generation 0.007988195751786659\n",
      "best_fitness 0.0030493221984207075\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    20.86         0.799531       21        0.0079882              N/A      0.00s\n",
      "best_fitness_generation 0.007988195751786659\n",
      "best_fitness 0.0030493221984207075\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    20.95         0.674996       21        0.0079882              N/A      0.00s\n",
      "best_fitness_generation 0.007988195751786659\n",
      "best_fitness 0.0030493221984207075\n",
      "-0.788*X0 - 0.45365\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.96       31        0.0549259              N/A      0.00s\n",
      "best_fitness_generation 0.05492585926430482\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93           122.08       31        0.0101886              N/A      0.00s\n",
      "best_fitness_generation 0.010188615008535481\n",
      "best_fitness 0.05492585926430482\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.73          4.09237       31       0.00958862              N/A      0.00s\n",
      "best_fitness_generation 0.009588615008535481\n",
      "best_fitness 0.010188615008535481\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    30.02          1.42715       33        0.0092808              N/A      0.00s\n",
      "best_fitness_generation 0.009280802004669887\n",
      "best_fitness 0.009588615008535481\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    26.89          0.66784       27        0.0093951              N/A      0.00s\n",
      "best_fitness_generation 0.00939509835404697\n",
      "best_fitness 0.009280802004669887\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    19.63         0.840268       23       0.00958862              N/A      0.00s\n",
      "best_fitness_generation 0.009588615008535481\n",
      "best_fitness 0.009280802004669887\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  23 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=5)]: Done  24 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=5)]: Done  25 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=5)]: Done  26 tasks      | elapsed: 13.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best_fitness_generation 0.027210398611088395\n",
      "best_fitness 0.0050951790348433695\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10     5.04         0.375138        5        0.0272104              N/A      0.00s\n",
      "best_fitness_generation 0.027210398611088395\n",
      "best_fitness 0.0050951790348433695\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11     5.06         0.332314        5        0.0272104              N/A      0.00s\n",
      "best_fitness_generation 0.027210398611088395\n",
      "best_fitness 0.0050951790348433695\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12     4.99         0.679294        5        0.0272104              N/A      0.00s\n",
      "best_fitness_generation 0.027210398611088395\n",
      "best_fitness 0.0050951790348433695\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13     5.02         0.549653        5        0.0272104              N/A      0.00s\n",
      "best_fitness_generation 0.027210398611088395\n",
      "best_fitness 0.0050951790348433695\n",
      "0.726*X0 - 0.397848\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.92       31        0.0681978              N/A      0.00s\n",
      "best_fitness_generation 0.06819776315047733\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.35528       43        0.0164966              N/A      0.00s\n",
      "best_fitness_generation 0.0164966481303494\n",
      "best_fitness 0.06819776315047733\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.33          1.89401       29        0.0151937              N/A      0.00s\n",
      "best_fitness_generation 0.015193710373465303\n",
      "best_fitness 0.0164966481303494\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    31.97          20.2598       37       0.00954221              N/A      0.00s\n",
      "best_fitness_generation 0.009542205684168656\n",
      "best_fitness 0.015193710373465303\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    20.86         0.738845       35       0.00862882              N/A      0.00s\n",
      "best_fitness_generation 0.008628824308787891\n",
      "best_fitness 0.009542205684168656\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    16.62         0.769368       15        0.0176833              N/A      0.00s\n",
      "best_fitness_generation 0.017683284449591087\n",
      "best_fitness 0.008628824308787891\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    15.05         0.754685        9        0.0176202              N/A      0.00s\n",
      "best_fitness_generation 0.017620193370524815\n",
      "best_fitness 0.008628824308787891\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    11.79         0.691353        9        0.0176202              N/A      0.00s\n",
      "best_fitness_generation 0.017620193370524815\n",
      "best_fitness 0.008628824308787891\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8     9.51         0.610174        9        0.0176202              N/A      0.00s\n",
      "best_fitness_generation 0.017620193370524815\n",
      "best_fitness 0.008628824308787891\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9     9.03         0.367933        9        0.0176202              N/A      0.00s\n",
      "best_fitness_generation 0.017620193370524815\n",
      "best_fitness 0.008628824308787891\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10     9.00         0.531675        9        0.0176202              N/A      0.00s\n",
      "best_fitness_generation 0.017620193370524815\n",
      "best_fitness 0.008628824308787891\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11     9.04         0.437042        9        0.0176202              N/A      0.00s\n",
      "best_fitness_generation 0.017620193370524815\n",
      "best_fitness 0.008628824308787891\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12     8.94         0.711855        9        0.0176202              N/A      0.00s\n",
      "best_fitness_generation 0.017620193370524815\n",
      "best_fitness 0.008628824308787891\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13     9.01         0.593427        9        0.0176202              N/A      0.00s\n",
      "best_fitness_generation 0.017620193370524815\n",
      "best_fitness 0.008628824308787891\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14     9.00         0.376916        9        0.0176202              N/A      0.00s\n",
      "best_fitness_generation 0.017620193370524815\n",
      "best_fitness 0.008628824308787891\n",
      "0.627*X0 + 0.342342\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.87       31        0.0790035              N/A      0.00s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  27 tasks      | elapsed: 13.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   6     1.06         0.162872        1        0.0143523              N/A      0.00s\n",
      "best_fitness_generation 0.014352340131853032\n",
      "best_fitness 0.00590065155479693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7     1.05          0.21572        1        0.0143523              N/A      0.00s\n",
      "best_fitness_generation 0.014352340131853032\n",
      "best_fitness 0.00590065155479693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8     1.07         0.268472        1        0.0143523              N/A      0.00s\n",
      "best_fitness_generation 0.014352340131853032\n",
      "best_fitness 0.00590065155479693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9     1.06        0.0280177        1        0.0143523              N/A      0.00s\n",
      "best_fitness_generation 0.014352340131853032\n",
      "best_fitness 0.00590065155479693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10     1.07         0.184325        1        0.0143523              N/A      0.00s\n",
      "best_fitness_generation 0.014352340131853032\n",
      "best_fitness 0.00590065155479693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11     1.08        0.0993978        1        0.0143523              N/A      0.00s\n",
      "best_fitness_generation 0.014352340131853032\n",
      "best_fitness 0.00590065155479693\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12     1.06          0.45269        1        0.0143523              N/A      0.00s\n",
      "best_fitness_generation 0.014352340131853032\n",
      "best_fitness 0.00590065155479693\n",
      "-0.511000000000000\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00             2647       31        0.0304404              N/A      0.00s\n",
      "best_fitness_generation 0.030440427539292363\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          4.56269       31        0.0123095              N/A      0.00s\n",
      "best_fitness_generation 0.012309531881901006\n",
      "best_fitness 0.030440427539292363\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.04          1.46402       35       0.00534122              N/A      0.00s\n",
      "best_fitness_generation 0.005341220019433536\n",
      "best_fitness 0.012309531881901006\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    18.75          1.24723       35       0.00534122              N/A      0.00s\n",
      "best_fitness_generation 0.005341220019433536\n",
      "best_fitness 0.005341220019433536\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4     9.53         0.571641       31       0.00731398              N/A      0.00s\n",
      "best_fitness_generation 0.007313976900562481\n",
      "best_fitness 0.005341220019433536\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5     3.11         0.229265       29       0.00808873              N/A      0.00s\n",
      "best_fitness_generation 0.008088725603774912\n",
      "best_fitness 0.005341220019433536\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6     3.01         0.276222        3         0.022536              N/A      0.00s\n",
      "best_fitness_generation 0.022535963174159197\n",
      "best_fitness 0.005341220019433536\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7     3.06          0.32829        3         0.022536              N/A      0.00s\n",
      "best_fitness_generation 0.022535963174159197\n",
      "best_fitness 0.005341220019433536\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8     3.06         0.382944        3         0.022536              N/A      0.00s\n",
      "best_fitness_generation 0.022535963174159197\n",
      "best_fitness 0.005341220019433536\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9     3.06         0.143298        3         0.022536              N/A      0.00s\n",
      "best_fitness_generation 0.022535963174159197\n",
      "best_fitness 0.005341220019433536\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10     3.03         0.301737        3         0.022536              N/A      0.00s\n",
      "best_fitness_generation 0.022535963174159197\n",
      "best_fitness 0.005341220019433536\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11     3.07          0.21759        3         0.022536              N/A      0.00s\n",
      "best_fitness_generation 0.022535963174159197\n",
      "best_fitness 0.005341220019433536\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12     3.04         0.568017        3         0.022536              N/A      0.00s\n",
      "best_fitness_generation 0.022535963174159197\n",
      "best_fitness 0.005341220019433536\n",
      "-0.805000000000000\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  28 tasks      | elapsed: 13.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   3    29.90          1.87156       33       0.00251121              N/A      0.00s\n",
      "best_fitness_generation 0.002511214269383092\n",
      "best_fitness 0.0038185198554258094\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    22.62          1.20563       33       0.00188943              N/A      0.00s\n",
      "best_fitness_generation 0.0018894274755553355\n",
      "best_fitness 0.002511214269383092\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    17.14         0.905571       29       0.00290765              N/A      0.00s\n",
      "best_fitness_generation 0.0029076511707074224\n",
      "best_fitness 0.0018894274755553355\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6     9.87         0.683655        7       0.00691991              N/A      0.00s\n",
      "best_fitness_generation 0.006919913805811709\n",
      "best_fitness 0.0018894274755553355\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7     7.07         0.633356        7       0.00691991              N/A      0.00s\n",
      "best_fitness_generation 0.006919913805811709\n",
      "best_fitness 0.0018894274755553355\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8     7.06         0.687867        7       0.00691991              N/A      0.00s\n",
      "best_fitness_generation 0.006919913805811709\n",
      "best_fitness 0.0018894274755553355\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9     7.06          0.44308        7       0.00691991              N/A      0.00s\n",
      "best_fitness_generation 0.006919913805811709\n",
      "best_fitness 0.0018894274755553355\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10     7.00         0.612584        7       0.00691991              N/A      0.00s\n",
      "best_fitness_generation 0.006919913805811709\n",
      "best_fitness 0.0018894274755553355\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11     7.07         0.531621        7       0.00691991              N/A      0.00s\n",
      "best_fitness_generation 0.006919913805811709\n",
      "best_fitness 0.0018894274755553355\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12     6.99         0.879382        7       0.00691991              N/A      0.00s\n",
      "best_fitness_generation 0.006919913805811709\n",
      "best_fitness 0.0018894274755553355\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13     6.99         0.736558        7       0.00691991              N/A      0.00s\n",
      "best_fitness_generation 0.006919913805811709\n",
      "best_fitness 0.0018894274755553355\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14     7.00         0.458946        7       0.00691991              N/A      0.00s\n",
      "best_fitness_generation 0.006919913805811709\n",
      "best_fitness 0.0018894274755553355\n",
      "-0.693*X0 - 0.317\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2647.09       31        0.0949368              N/A      0.00s\n",
      "best_fitness_generation 0.09493678469704198\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93           23.137       31        0.0376306              N/A      0.00s\n",
      "best_fitness_generation 0.03763056869001925\n",
      "best_fitness 0.09493678469704198\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.10          5.25375       41        0.0179024              N/A      0.00s\n",
      "best_fitness_generation 0.017902387423305244\n",
      "best_fitness 0.03763056869001925\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    28.51         0.757496       39        0.0139529              N/A      0.00s\n",
      "best_fitness_generation 0.013952916345308326\n",
      "best_fitness 0.017902387423305244\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    22.92         0.746335       39        0.0139529              N/A      0.00s\n",
      "best_fitness_generation 0.013952916345308326\n",
      "best_fitness 0.013952916345308326\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    17.79         0.629142       19        0.0150751              N/A      0.00s\n",
      "best_fitness_generation 0.015075076705534417\n",
      "best_fitness 0.013952916345308326\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    20.13          0.69652       19        0.0150751              N/A      0.00s\n",
      "best_fitness_generation 0.015075076705534417\n",
      "best_fitness 0.013952916345308326\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    18.61         0.773678       19        0.0150751              N/A      0.00s\n",
      "best_fitness_generation 0.015075076705534417\n",
      "best_fitness 0.013952916345308326\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    17.20           0.8012       19        0.0150751              N/A      0.00s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  29 tasks      | elapsed: 14.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    20.08          0.69854       21        0.0093951              N/A      0.00s\n",
      "best_fitness_generation 0.00939509835404697\n",
      "best_fitness 0.009280802004669887\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    17.32         0.891339       23       0.00958862              N/A      0.00s\n",
      "best_fitness_generation 0.009588615008535481\n",
      "best_fitness 0.009280802004669887\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    17.05         0.980207       17        0.0112009              N/A      0.00s\n",
      "best_fitness_generation 0.01120087264971078\n",
      "best_fitness 0.009280802004669887\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    17.06         0.738951       17        0.0112009              N/A      0.00s\n",
      "best_fitness_generation 0.01120087264971078\n",
      "best_fitness 0.009280802004669887\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    16.93         0.882905       17        0.0112009              N/A      0.00s\n",
      "best_fitness_generation 0.01120087264971078\n",
      "best_fitness 0.009280802004669887\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    17.03         0.811445       17        0.0112009              N/A      0.00s\n",
      "best_fitness_generation 0.01120087264971078\n",
      "best_fitness 0.009280802004669887\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    16.89          1.16123       17        0.0112009              N/A      0.00s\n",
      "best_fitness_generation 0.01120087264971078\n",
      "best_fitness 0.009280802004669887\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    16.99          1.02394       17        0.0112009              N/A      0.00s\n",
      "best_fitness_generation 0.01120087264971078\n",
      "best_fitness 0.009280802004669887\n",
      "-0.801*X0 - 0.279\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.91       31        0.0528899              N/A      0.00s\n",
      "best_fitness_generation 0.05288994823421149\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          17.2441       43        0.0137873              N/A      0.00s\n",
      "best_fitness_generation 0.013787323518456582\n",
      "best_fitness 0.05288994823421149\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    31.17          1.91319       43        0.0137873              N/A      0.00s\n",
      "best_fitness_generation 0.013787323518456582\n",
      "best_fitness 0.013787323518456582\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    28.94          1.79136       35       0.00663322              N/A      0.00s\n",
      "best_fitness_generation 0.006633223537629874\n",
      "best_fitness 0.013787323518456582\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    22.81           1.9169       23       0.00348353              N/A      0.00s\n",
      "best_fitness_generation 0.0034835262292839987\n",
      "best_fitness 0.006633223537629874\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    21.45          1.25678       23       0.00348353              N/A      0.00s\n",
      "best_fitness_generation 0.0034835262292839987\n",
      "best_fitness 0.0034835262292839987\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    20.69         0.815407       23       0.00348353              N/A      0.00s\n",
      "best_fitness_generation 0.0034835262292839987\n",
      "best_fitness 0.0034835262292839987\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    13.83         0.717189       23       0.00348353              N/A      0.00s\n",
      "best_fitness_generation 0.0034835262292839987\n",
      "best_fitness 0.0034835262292839987\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    13.01         0.736456       13       0.00483051              N/A      0.00s\n",
      "best_fitness_generation 0.0048305053291068135\n",
      "best_fitness 0.0034835262292839987\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    13.07         0.550033       13       0.00483051              N/A      0.00s\n",
      "best_fitness_generation 0.0048305053291068135\n",
      "best_fitness 0.0034835262292839987\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    12.87         0.668017       13       0.00483051              N/A      0.00s\n",
      "best_fitness_generation 0.0048305053291068135\n",
      "best_fitness 0.0034835262292839987\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    13.10         0.634473       13       0.00483051              N/A      0.00s\n",
      "best_fitness_generation 0.0048305053291068135\n",
      "best_fitness 0.0034835262292839987\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    30.10          3.23058       33        0.0142929              N/A      0.00s\n",
      "best_fitness_generation 0.014292922076062616\n",
      "best_fitness 0.013878747175886736\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    22.62          1.31752       23        0.0136953              N/A      0.00s\n",
      "best_fitness_generation 0.013695279693320924\n",
      "best_fitness 0.013878747175886736\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    21.03          1.38689       21        0.0153537              N/A      0.00s\n",
      "best_fitness_generation 0.01535372649553247\n",
      "best_fitness 0.013695279693320924\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    20.99          1.37105       21        0.0153537              N/A      0.00s\n",
      "best_fitness_generation 0.01535372649553247\n",
      "best_fitness 0.013695279693320924\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    20.97          1.63342       21        0.0153537              N/A      0.00s\n",
      "best_fitness_generation 0.01535372649553247\n",
      "best_fitness 0.013695279693320924\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    20.95          1.95404       21        0.0153537              N/A      0.00s\n",
      "best_fitness_generation 0.01535372649553247\n",
      "best_fitness 0.013695279693320924\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    21.06          1.28435       21        0.0153537              N/A      0.00s\n",
      "best_fitness_generation 0.01535372649553247\n",
      "best_fitness 0.013695279693320924\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    20.95          1.37563       21        0.0153537              N/A      0.00s\n",
      "best_fitness_generation 0.01535372649553247\n",
      "best_fitness 0.013695279693320924\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16    20.95          1.28755       21        0.0153537              N/A      0.00s\n",
      "best_fitness_generation 0.01535372649553247\n",
      "best_fitness 0.013695279693320924\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  17    20.88          1.40828       21        0.0153537              N/A      0.00s\n",
      "best_fitness_generation 0.01535372649553247\n",
      "best_fitness 0.013695279693320924\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  18    20.82          1.34352       21        0.0153537              N/A      0.00s\n",
      "best_fitness_generation 0.01535372649553247\n",
      "best_fitness 0.013695279693320924\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  19    20.92          1.26417       21        0.0153537              N/A      0.00s\n",
      "best_fitness_generation 0.01535372649553247\n",
      "best_fitness 0.013695279693320924\n",
      "-1.42484008528785*X0**2 - 0.407504264392324*X0 - 0.93608919031621\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.95       31       0.00835014              N/A      0.00s\n",
      "best_fitness_generation 0.008350141904853064\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          3.90448       31       0.00761139              N/A      0.00s\n",
      "best_fitness_generation 0.007611387957502104\n",
      "best_fitness 0.008350141904853064\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.84          1.84615       33       0.00405036              N/A      0.00s\n",
      "best_fitness_generation 0.004050361633538309\n",
      "best_fitness 0.007611387957502104\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    30.84          1.56018       33       0.00405036              N/A      0.00s\n",
      "best_fitness_generation 0.004050361633538309\n",
      "best_fitness 0.004050361633538309\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    31.61         0.847623       33       0.00405036              N/A      0.00s\n",
      "best_fitness_generation 0.004050361633538309\n",
      "best_fitness 0.004050361633538309\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    32.73          1.09343       39       0.00360508              N/A      0.00s\n",
      "best_fitness_generation 0.003605084295554356\n",
      "best_fitness 0.004050361633538309\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    32.95          1.18459       39       0.00360508              N/A      0.00s\n",
      "best_fitness_generation 0.003605084295554356\n",
      "best_fitness 0.003605084295554356\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    33.13          1.14496       33       0.00405036              N/A      0.00s\n",
      "best_fitness_generation 0.004050361633538309\n",
      "best_fitness 0.003605084295554356\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  30 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=5)]: Done  32 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=5)]: Done  33 tasks      | elapsed: 16.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   0    31.00          2646.91       31        0.0356868              N/A      0.00s\n",
      "best_fitness_generation 0.0356868211236831\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93           2.1646       31        0.0155801              N/A      0.00s\n",
      "best_fitness_generation 0.015580120875902125\n",
      "best_fitness 0.0356868211236831\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    27.47         0.995635       29       0.00957883              N/A      0.00s\n",
      "best_fitness_generation 0.009578828120668516\n",
      "best_fitness 0.015580120875902125\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    18.81         0.884107       59       0.00349011              N/A      0.00s\n",
      "best_fitness_generation 0.003490114391676953\n",
      "best_fitness 0.009578828120668516\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    29.05         0.594134       61       0.00404234              N/A      0.00s\n",
      "best_fitness_generation 0.00404234438988243\n",
      "best_fitness 0.003490114391676953\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    25.04         0.490776       31       0.00685436              N/A      0.00s\n",
      "best_fitness_generation 0.006854364523547243\n",
      "best_fitness 0.003490114391676953\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    21.52          0.50536       31       0.00685436              N/A      0.00s\n",
      "best_fitness_generation 0.006854364523547243\n",
      "best_fitness 0.003490114391676953\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    22.47         0.630839       47       0.00331174              N/A      0.00s\n",
      "best_fitness_generation 0.003311741100094487\n",
      "best_fitness 0.003490114391676953\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    23.09         0.637793       27       0.00685436              N/A      0.00s\n",
      "best_fitness_generation 0.006854364523547243\n",
      "best_fitness 0.003311741100094487\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    23.15         0.385734       23       0.00962211              N/A      0.00s\n",
      "best_fitness_generation 0.009622110270574939\n",
      "best_fitness 0.003311741100094487\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    22.91         0.563279       23       0.00962211              N/A      0.00s\n",
      "best_fitness_generation 0.009622110270574939\n",
      "best_fitness 0.003311741100094487\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    22.94          0.46495       23       0.00962211              N/A      0.00s\n",
      "best_fitness_generation 0.009622110270574939\n",
      "best_fitness 0.003311741100094487\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    22.92          0.66344       23       0.00962211              N/A      0.00s\n",
      "best_fitness_generation 0.009622110270574939\n",
      "best_fitness 0.003311741100094487\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    22.91         0.528315       23       0.00962211              N/A      0.00s\n",
      "best_fitness_generation 0.009622110270574939\n",
      "best_fitness 0.003311741100094487\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    22.93         0.421592       23       0.00962211              N/A      0.00s\n",
      "best_fitness_generation 0.009622110270574939\n",
      "best_fitness 0.003311741100094487\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    23.01          0.54702       29       0.00668775              N/A      0.00s\n",
      "best_fitness_generation 0.006687747301549248\n",
      "best_fitness 0.003311741100094487\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16    23.03         0.394925       23       0.00962211              N/A      0.00s\n",
      "best_fitness_generation 0.009622110270574939\n",
      "best_fitness 0.003311741100094487\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  17    22.95         0.404263       23       0.00962211              N/A      0.00s\n",
      "best_fitness_generation 0.009622110270574939\n",
      "best_fitness 0.003311741100094487\n",
      "-0.69208211143695*X0**2 + 0.933*X0 + 0.393975\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.94       31        0.0931909              N/A      0.00s\n",
      "best_fitness_generation 0.09319088224519186\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          16.2946       31        0.0475462              N/A      0.00s\n",
      "best_fitness_generation 0.04754621116791601\n",
      "best_fitness 0.09319088224519186\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.88          2.78602       31        0.0280535              N/A      0.00s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  34 tasks      | elapsed: 16.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best_fitness_generation 0.07900353192695707\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.10628       27        0.0520702              N/A      0.00s\n",
      "best_fitness_generation 0.05207022480607086\n",
      "best_fitness 0.07900353192695707\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.39          47.7462       27        0.0339923              N/A      0.00s\n",
      "best_fitness_generation 0.03399228769814983\n",
      "best_fitness 0.05207022480607086\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    27.33           1.5707       35        0.0173612              N/A      0.00s\n",
      "best_fitness_generation 0.017361186008573597\n",
      "best_fitness 0.03399228769814983\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    25.62          1.70174       35        0.0173612              N/A      0.00s\n",
      "best_fitness_generation 0.017361186008573597\n",
      "best_fitness 0.017361186008573597\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    27.24         0.915303       33        0.0169129              N/A      0.00s\n",
      "best_fitness_generation 0.01691289853946069\n",
      "best_fitness 0.017361186008573597\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    21.80          2.91891       23        0.0177345              N/A      0.00s\n",
      "best_fitness_generation 0.01773446408870146\n",
      "best_fitness 0.01691289853946069\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    15.30          3.36977       23        0.0177345              N/A      0.00s\n",
      "best_fitness_generation 0.01773446408870146\n",
      "best_fitness 0.01691289853946069\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    15.03          3.33311       15        0.0215485              N/A      0.00s\n",
      "best_fitness_generation 0.021548484441300603\n",
      "best_fitness 0.01691289853946069\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    15.09          3.13343       15        0.0215485              N/A      0.00s\n",
      "best_fitness_generation 0.021548484441300603\n",
      "best_fitness 0.01691289853946069\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    14.95          3.28076       15        0.0215485              N/A      0.00s\n",
      "best_fitness_generation 0.021548484441300603\n",
      "best_fitness 0.01691289853946069\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    15.06          3.47068       15        0.0215485              N/A      0.00s\n",
      "best_fitness_generation 0.021548484441300603\n",
      "best_fitness 0.01691289853946069\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    15.00          3.64985       15        0.0215485              N/A      0.00s\n",
      "best_fitness_generation 0.021548484441300603\n",
      "best_fitness 0.01691289853946069\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    15.01          3.51748       15        0.0215485              N/A      0.00s\n",
      "best_fitness_generation 0.021548484441300603\n",
      "best_fitness 0.01691289853946069\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    15.02          3.29896       15        0.0215485              N/A      0.00s\n",
      "best_fitness_generation 0.021548484441300603\n",
      "best_fitness 0.01691289853946069\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    15.01          3.17336       15        0.0215485              N/A      0.00s\n",
      "best_fitness_generation 0.021548484441300603\n",
      "best_fitness 0.01691289853946069\n",
      "1.70224719101124*X0**2 - 0.00351123595505609*X0 - 0.512755617977528\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.91       31         0.045365              N/A      0.00s\n",
      "best_fitness_generation 0.04536497628752733\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.16715       31         0.014204              N/A      0.00s\n",
      "best_fitness_generation 0.01420403058927135\n",
      "best_fitness 0.04536497628752733\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    28.60           1.1123       31       0.00451384              N/A      0.00s\n",
      "best_fitness_generation 0.004513836645713\n",
      "best_fitness 0.01420403058927135\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    24.85          1.01737       61       0.00321435              N/A      0.00s\n",
      "best_fitness_generation 0.0032143532855811218\n",
      "best_fitness 0.004513836645713\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    23.26         0.670451       29       0.00203862              N/A      0.00s\n",
      "best_fitness_generation\n",
      "best_fitness_generation 0.015075076705534417\n",
      "best_fitness 0.013952916345308326\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    16.98         0.561475       17        0.0154382              N/A      0.00s\n",
      "best_fitness_generation 0.015438184044237403\n",
      "best_fitness 0.013952916345308326\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    16.95         0.726477       17        0.0154382              N/A      0.00s\n",
      "best_fitness_generation 0.015438184044237403\n",
      "best_fitness 0.013952916345308326\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    17.08         0.640497       17        0.0154382              N/A      0.00s\n",
      "best_fitness_generation 0.015438184044237403\n",
      "best_fitness 0.013952916345308326\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    16.99         0.970173       17        0.0154382              N/A      0.00s\n",
      "best_fitness_generation 0.015438184044237403\n",
      "best_fitness 0.013952916345308326\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    17.03         0.850172       17        0.0154382              N/A      0.00s\n",
      "best_fitness_generation 0.015438184044237403\n",
      "best_fitness 0.013952916345308326\n",
      "X0**2 + 0.73192\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.98       31        0.0218994              N/A      0.00s\n",
      "best_fitness_generation 0.02189937349702432\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          4.83152       43       0.00547089              N/A      0.00s\n",
      "best_fitness_generation 0.005470893029651078\n",
      "best_fitness 0.02189937349702432\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.46          3.21587       31       0.00441532              N/A      0.00s\n",
      "best_fitness_generation 0.004415323680287276\n",
      "best_fitness 0.005470893029651078\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    27.29          66.8188       45       0.00424983              N/A      0.00s\n",
      "best_fitness_generation 0.004249826943504862\n",
      "best_fitness 0.004415323680287276\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    18.51          1.11684       53       0.00447089              N/A      0.00s\n",
      "best_fitness_generation 0.004470893029651078\n",
      "best_fitness 0.004249826943504862\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    16.17         0.932472       17        0.0125142              N/A      0.00s\n",
      "best_fitness_generation 0.012514231200787717\n",
      "best_fitness 0.004249826943504862\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    15.01         0.968926       15        0.0125142              N/A      0.00s\n",
      "best_fitness_generation 0.012514231200787717\n",
      "best_fitness 0.004249826943504862\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    15.10         0.998264       15        0.0125142              N/A      0.00s\n",
      "best_fitness_generation 0.012514231200787717\n",
      "best_fitness 0.004249826943504862\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    15.04         0.879095       15        0.0125142              N/A      0.00s\n",
      "best_fitness_generation 0.012514231200787717\n",
      "best_fitness 0.004249826943504862\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    15.07         0.818114       15        0.0125142              N/A      0.00s\n",
      "best_fitness_generation 0.012514231200787717\n",
      "best_fitness 0.004249826943504862\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    14.95         0.986228       15        0.0125142              N/A      0.00s\n",
      "best_fitness_generation 0.012514231200787717\n",
      "best_fitness 0.004249826943504862\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    15.07         0.893876       15        0.0125142              N/A      0.00s\n",
      "best_fitness_generation 0.012514231200787717\n",
      "best_fitness 0.004249826943504862\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    14.91          1.21347       15        0.0125142              N/A      0.00s\n",
      "best_fitness_generation 0.012514231200787717\n",
      "best_fitness 0.004249826943504862\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    14.97          1.08455       15        0.0125142              N/A      0.00s\n",
      "best_fitness_generation 0.012514231200787717\n",
      "best_fitness 0.004249826943504862\n",
      "-0.229*X0 - 0.641319\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.84       31        0.0338163              N/A      0.00s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  35 tasks      | elapsed: 16.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  12    12.93         0.970396       13       0.00483051              N/A      0.00s\n",
      "best_fitness_generation 0.0048305053291068135\n",
      "best_fitness 0.0034835262292839987\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    13.06         0.837813       13       0.00483051              N/A      0.00s\n",
      "best_fitness_generation 0.0048305053291068135\n",
      "best_fitness 0.0034835262292839987\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    13.05         0.552142       13       0.00483051              N/A      0.00s\n",
      "best_fitness_generation 0.0048305053291068135\n",
      "best_fitness 0.0034835262292839987\n",
      "0.501952*X0 - 0.801\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.91       31        0.0533092              N/A      0.00s\n",
      "best_fitness_generation 0.053309191805895786\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.09829       43        0.0242655              N/A      0.00s\n",
      "best_fitness_generation 0.024265471369746945\n",
      "best_fitness 0.053309191805895786\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    27.35          1.02861       43        0.0160967              N/A      0.00s\n",
      "best_fitness_generation 0.016096737156825476\n",
      "best_fitness 0.024265471369746945\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    12.86         0.611268       43        0.0142345              N/A      0.00s\n",
      "best_fitness_generation 0.014234544850144595\n",
      "best_fitness 0.016096737156825476\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4     9.06         0.631376       33         0.014512              N/A      0.00s\n",
      "best_fitness_generation 0.014511979166862136\n",
      "best_fitness 0.014234544850144595\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5     6.14         0.612963        9         0.026173              N/A      0.00s\n",
      "best_fitness_generation 0.026173046255445427\n",
      "best_fitness 0.014234544850144595\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6     8.95         0.574013        9         0.026173              N/A      0.00s\n",
      "best_fitness_generation 0.026173046255445427\n",
      "best_fitness 0.014234544850144595\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7     9.06         0.535018        9         0.026173              N/A      0.00s\n",
      "best_fitness_generation 0.026173046255445427\n",
      "best_fitness 0.014234544850144595\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8     9.09          0.51359        9        0.0250421              N/A      0.00s\n",
      "best_fitness_generation 0.025042131118322812\n",
      "best_fitness 0.014234544850144595\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9     8.98         0.407113        9        0.0250421              N/A      0.00s\n",
      "best_fitness_generation 0.025042131118322812\n",
      "best_fitness 0.014234544850144595\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10     8.99         0.450162        9        0.0250421              N/A      0.00s\n",
      "best_fitness_generation 0.025042131118322812\n",
      "best_fitness 0.014234544850144595\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11     9.04         0.514873        9        0.0250421              N/A      0.00s\n",
      "best_fitness_generation 0.025042131118322812\n",
      "best_fitness 0.014234544850144595\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12     8.93         0.854972        9        0.0250421              N/A      0.00s\n",
      "best_fitness_generation 0.025042131118322812\n",
      "best_fitness 0.014234544850144595\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13     9.04         0.718787        9        0.0250421              N/A      0.00s\n",
      "best_fitness_generation 0.025042131118322812\n",
      "best_fitness 0.014234544850144595\n",
      "0.78997867803838*X0 + 0.212876332622601\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.82       31         0.029347              N/A      0.00s\n",
      "best_fitness_generation 0.02934695869545164\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          1.73651       33       0.00991765              N/A      0.00s\n",
      "best_fitness_generation 0.00991764674631912\n",
      "best_fitness 0.02934695869545164\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.20          1.20139       33       0.00991765              N/A      0.00s\n",
      "best_fitness_generation 0.00991764674631912\n",
      "best_fitness 0.00991764674631912\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  36 tasks      | elapsed: 17.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    32.91          1.26191       39       0.00360508              N/A      0.00s\n",
      "best_fitness_generation 0.003605084295554356\n",
      "best_fitness 0.003605084295554356\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    33.24          1.07723       39       0.00360508              N/A      0.00s\n",
      "best_fitness_generation 0.003605084295554356\n",
      "best_fitness 0.003605084295554356\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    32.95          1.10135       33       0.00405036              N/A      0.00s\n",
      "best_fitness_generation 0.004050361633538309\n",
      "best_fitness 0.003605084295554356\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    33.02          1.14857       39       0.00360508              N/A      0.00s\n",
      "best_fitness_generation 0.003605084295554356\n",
      "best_fitness 0.003605084295554356\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    32.91          1.43576       39       0.00360508              N/A      0.00s\n",
      "best_fitness_generation 0.003605084295554356\n",
      "best_fitness 0.003605084295554356\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    33.02          1.39548       39       0.00360508              N/A      0.00s\n",
      "best_fitness_generation 0.003605084295554356\n",
      "best_fitness 0.003605084295554356\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    32.93          1.07848       33       0.00405036              N/A      0.00s\n",
      "best_fitness_generation 0.004050361633538309\n",
      "best_fitness 0.003605084295554356\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    32.97          1.07856       39       0.00360508              N/A      0.00s\n",
      "best_fitness_generation 0.003605084295554356\n",
      "best_fitness 0.003605084295554356\n",
      "0.665415313396089*X0 + 0.398061674008811\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2647.12       31        0.0440306              N/A      0.00s\n",
      "best_fitness_generation 0.04403063219985013\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.75192       31        0.0169583              N/A      0.00s\n",
      "best_fitness_generation 0.016958331053157878\n",
      "best_fitness 0.04403063219985013\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.69          2.31448       27         0.010415              N/A      0.00s\n",
      "best_fitness_generation 0.010414950304794897\n",
      "best_fitness 0.016958331053157878\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    26.99          2.71909       31       0.00103959              N/A      0.00s\n",
      "best_fitness_generation 0.0010395908418685791\n",
      "best_fitness 0.010414950304794897\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    21.65         0.770996       31       0.00103959              N/A      0.00s\n",
      "best_fitness_generation 0.0010395908418685791\n",
      "best_fitness 0.0010395908418685791\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    17.00         0.865584       27       0.00137857              N/A      0.00s\n",
      "best_fitness_generation 0.00137857274893145\n",
      "best_fitness 0.0010395908418685791\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    19.42          1.01058       37      0.000687965              N/A      0.00s\n",
      "best_fitness_generation 0.0006879647757898747\n",
      "best_fitness 0.0010395908418685791\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    21.86          1.73964       37      0.000687965              N/A      0.00s\n",
      "best_fitness_generation 0.0006879647757898747\n",
      "best_fitness 0.0006879647757898747\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    21.00          2.49874       19       0.00137857              N/A      0.00s\n",
      "best_fitness_generation 0.00137857274893145\n",
      "best_fitness 0.0006879647757898747\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    18.08           1.6259       19       0.00137857              N/A      0.00s\n",
      "best_fitness_generation 0.00137857274893145\n",
      "best_fitness 0.0006879647757898747\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    13.16         0.669652       19       0.00137857              N/A      0.00s\n",
      "best_fitness_generation 0.00137857274893145\n",
      "best_fitness 0.0006879647757898747\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    13.06         0.586521       13       0.00527154              N/A      0.00s\n",
      "best_fitness_generation 0.005271541268779532\n",
      "best_fitness 0.0006879647757898747\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  37 tasks      | elapsed: 18.2min\n",
      "[Parallel(n_jobs=5)]: Done  38 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=5)]: Done  39 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed: 19.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0020386159802181516\n",
      "best_fitness 0.0032143532855811218\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    18.83         0.471763       19       0.00472051              N/A      0.00s\n",
      "best_fitness_generation 0.004720512272853739\n",
      "best_fitness 0.0020386159802181516\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    17.03         0.589879       19       0.00472051              N/A      0.00s\n",
      "best_fitness_generation 0.004720512272853739\n",
      "best_fitness 0.0020386159802181516\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7     7.79         0.459219       19       0.00472051              N/A      0.00s\n",
      "best_fitness_generation 0.004720512272853739\n",
      "best_fitness 0.0020386159802181516\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8     7.06         0.486098        7        0.0127642              N/A      0.00s\n",
      "best_fitness_generation 0.012764230014266662\n",
      "best_fitness 0.0020386159802181516\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9     7.06         0.242914        7        0.0127642              N/A      0.00s\n",
      "best_fitness_generation 0.012764230014266662\n",
      "best_fitness 0.0020386159802181516\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10     7.00         0.409692        7        0.0127642              N/A      0.00s\n",
      "best_fitness_generation 0.012764230014266662\n",
      "best_fitness 0.0020386159802181516\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11     7.07          0.32293        7        0.0127642              N/A      0.00s\n",
      "best_fitness_generation 0.012764230014266662\n",
      "best_fitness 0.0020386159802181516\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12     6.99         0.675563        7        0.0127642              N/A      0.00s\n",
      "best_fitness_generation 0.012764230014266662\n",
      "best_fitness 0.0020386159802181516\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13     6.99         0.532285        7        0.0127642              N/A      0.00s\n",
      "best_fitness_generation 0.012764230014266662\n",
      "best_fitness 0.0020386159802181516\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14     7.00         0.256798        7        0.0127642              N/A      0.00s\n",
      "best_fitness_generation 0.012764230014266662\n",
      "best_fitness 0.0020386159802181516\n",
      "0.958 - 0.777*X0\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.83       31        0.0266737              N/A      0.00s\n",
      "best_fitness_generation 0.02667371158064964\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.02885       43        0.0190512              N/A      0.00s\n",
      "best_fitness_generation 0.019051175384083456\n",
      "best_fitness 0.02667371158064964\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    31.25          1.81797       27         0.013008              N/A      0.00s\n",
      "best_fitness_generation 0.013007972753474786\n",
      "best_fitness 0.019051175384083456\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    26.85          1.24887       39         0.013008              N/A      0.00s\n",
      "best_fitness_generation 0.013007972753474786\n",
      "best_fitness 0.013007972753474786\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    19.66          1.15755       27         0.013008              N/A      0.00s\n",
      "best_fitness_generation 0.013007972753474786\n",
      "best_fitness 0.013007972753474786\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    21.65          1.57924       25        0.0106089              N/A      0.00s\n",
      "best_fitness_generation 0.010608851262365266\n",
      "best_fitness 0.013007972753474786\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    17.46         0.762724       23        0.0106089              N/A      0.00s\n",
      "best_fitness_generation 0.010608851262365266\n",
      "best_fitness 0.010608851262365266\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    15.10          1.13692       15         0.013008              N/A      0.00s\n",
      "best_fitness_generation 0.013007972753474786\n",
      "best_fitness 0.010608851262365266\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    15.04         0.717077       15         0.013008              N/A      0.00s\n",
      "best_fitness_generation 0.013007972753474786\n",
      "best_fitness 0.010608851262365266\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    15.07         0.610499       15         0.013008              N/A      0.00s\n",
      "best_fitness_generation 0.013007972753474786\n",
      "best_fitness_generation 0.028053482386838084\n",
      "best_fitness 0.04754621116791601\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    30.63          1.50402       35        0.0249137              N/A      0.00s\n",
      "best_fitness_generation 0.024913717657382012\n",
      "best_fitness 0.028053482386838084\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    26.82          1.16338       23        0.0149909              N/A      0.00s\n",
      "best_fitness_generation 0.01499093479174856\n",
      "best_fitness 0.024913717657382012\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    26.02         0.897495       21        0.0145099              N/A      0.00s\n",
      "best_fitness_generation 0.014509923208523467\n",
      "best_fitness 0.01499093479174856\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    26.68         0.928961       21        0.0145099              N/A      0.00s\n",
      "best_fitness_generation 0.014509923208523467\n",
      "best_fitness 0.014509923208523467\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    22.63         0.982034       21        0.0145099              N/A      0.00s\n",
      "best_fitness_generation 0.014509923208523467\n",
      "best_fitness 0.014509923208523467\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    21.03          1.14464       21        0.0145099              N/A      0.00s\n",
      "best_fitness_generation 0.014509923208523467\n",
      "best_fitness 0.014509923208523467\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    21.17         0.860599       21        0.0145099              N/A      0.00s\n",
      "best_fitness_generation 0.014509923208523467\n",
      "best_fitness 0.014509923208523467\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    20.95          1.08731       21        0.0145099              N/A      0.00s\n",
      "best_fitness_generation 0.014509923208523467\n",
      "best_fitness 0.014509923208523467\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    20.94         0.975468       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.014509923208523467\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    20.86          1.19528       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.012546629626798079\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    20.91         0.990863       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.012546629626798079\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    20.94         0.885834       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.012546629626798079\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    21.01          1.16721       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.012546629626798079\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16    21.13         0.869613       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.012546629626798079\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  17    20.86          1.15528       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.012546629626798079\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  18    20.90         0.916793       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.012546629626798079\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  19    20.99           0.8835       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.012546629626798079\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  20    20.79         0.921313       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.012546629626798079\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  21    21.05         0.882881       21        0.0125466              N/A      0.00s\n",
      "best_fitness_generation 0.012546629626798079\n",
      "best_fitness 0.012546629626798079\n",
      "1.51491324200913*X0**2 - 0.963484821917808*X0 - 0.634748648401826\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00           2647.2       31        0.0428218              N/A      0.00s\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    12.99         0.756171       15        0.0025531              N/A      0.00s\n",
      "best_fitness_generation 0.0025531001671235446\n",
      "best_fitness 0.0006879647757898747\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    13.05         0.642528       15       0.00241214              N/A      0.00s\n",
      "best_fitness_generation 0.0024121430287655427\n",
      "best_fitness 0.0006879647757898747\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    13.71         0.487962       15       0.00241214              N/A      0.00s\n",
      "best_fitness_generation 0.0024121430287655427\n",
      "best_fitness 0.0006879647757898747\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  15    15.02          0.63177       15       0.00241214              N/A      0.00s\n",
      "best_fitness_generation 0.0024121430287655427\n",
      "best_fitness 0.0006879647757898747\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  16    15.00         0.433192       15       0.00241214              N/A      0.00s\n",
      "best_fitness_generation 0.0024121430287655427\n",
      "best_fitness 0.0006879647757898747\n",
      "-0.799869*X0 - 0.659131\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.84       31        0.0636844              N/A      0.00s\n",
      "best_fitness_generation 0.06368440270588051\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          1.46345       31        0.0396101              N/A      0.00s\n",
      "best_fitness_generation 0.03961007655318107\n",
      "best_fitness 0.06368440270588051\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.92         0.871263       33        0.0296389              N/A      0.00s\n",
      "best_fitness_generation 0.02963893532594028\n",
      "best_fitness 0.03961007655318107\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    30.42          1.06212       33        0.0101038              N/A      0.00s\n",
      "best_fitness_generation 0.010103822119468068\n",
      "best_fitness 0.02963893532594028\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    31.61         0.510256       39       0.00948263              N/A      0.00s\n",
      "best_fitness_generation 0.009482629897828195\n",
      "best_fitness 0.010103822119468068\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    29.89         0.609003       35       0.00392849              N/A      0.00s\n",
      "best_fitness_generation 0.003928490103917975\n",
      "best_fitness 0.009482629897828195\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    27.22         0.716419       35       0.00392849              N/A      0.00s\n",
      "best_fitness_generation 0.003928490103917975\n",
      "best_fitness 0.003928490103917975\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    21.87         0.730489       29        0.0119813              N/A      0.00s\n",
      "best_fitness_generation 0.011981277559577128\n",
      "best_fitness 0.003928490103917975\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    21.07          0.89626       21         0.014318              N/A      0.00s\n",
      "best_fitness_generation 0.014318042725653339\n",
      "best_fitness 0.003928490103917975\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    21.14         0.711322       21         0.014318              N/A      0.00s\n",
      "best_fitness_generation 0.014318042725653339\n",
      "best_fitness 0.003928490103917975\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    20.94         0.780381       21        0.0142502              N/A      0.00s\n",
      "best_fitness_generation 0.01425021953530586\n",
      "best_fitness 0.003928490103917975\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    20.96         0.709424       21        0.0142502              N/A      0.00s\n",
      "best_fitness_generation 0.01425021953530586\n",
      "best_fitness 0.003928490103917975\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    20.87         0.955683       21        0.0142502              N/A      0.00s\n",
      "best_fitness_generation 0.01425021953530586\n",
      "best_fitness 0.003928490103917975\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    20.98         0.863747       21        0.0142502              N/A      0.00s\n",
      "best_fitness_generation 0.01425021953530586\n",
      "best_fitness 0.003928490103917975\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    20.86         0.667639       21        0.0142502              N/A      0.00s\n",
      "best_fitness_generation 0.01425021953530586\n",
      "best_fitness 0.003928490103917975\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "best_fitness_generation 0.03381632199696979\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          1.81604       31        0.0191483              N/A      0.00s\n",
      "best_fitness_generation 0.019148310276638904\n",
      "best_fitness 0.03381632199696979\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.64          1.03733       19        0.0190369              N/A      0.00s\n",
      "best_fitness_generation 0.019036903089202417\n",
      "best_fitness 0.019148310276638904\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    20.37          0.70514       23        0.0138797              N/A      0.00s\n",
      "best_fitness_generation 0.013879689601269939\n",
      "best_fitness 0.019036903089202417\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    10.01         0.372161       23        0.0138546              N/A      0.00s\n",
      "best_fitness_generation 0.013854614390274494\n",
      "best_fitness 0.013879689601269939\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    10.80         0.343949       23        0.0138546              N/A      0.00s\n",
      "best_fitness_generation 0.013854614390274494\n",
      "best_fitness 0.013854614390274494\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6     9.25         0.426078       15        0.0138797              N/A      0.00s\n",
      "best_fitness_generation 0.013879689601269939\n",
      "best_fitness 0.013854614390274494\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    14.91         0.468187       15        0.0138797              N/A      0.00s\n",
      "best_fitness_generation 0.013879689601269939\n",
      "best_fitness 0.013854614390274494\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    15.04          0.49388       15        0.0138797              N/A      0.00s\n",
      "best_fitness_generation 0.013879689601269939\n",
      "best_fitness 0.013854614390274494\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    15.13         0.431487       15        0.0138797              N/A      0.00s\n",
      "best_fitness_generation 0.013879689601269939\n",
      "best_fitness 0.013854614390274494\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    14.92         0.566762       15        0.0138797              N/A      0.00s\n",
      "best_fitness_generation 0.013879689601269939\n",
      "best_fitness 0.013854614390274494\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    15.03         0.500782       15        0.0138797              N/A      0.00s\n",
      "best_fitness_generation 0.013879689601269939\n",
      "best_fitness 0.013854614390274494\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    15.00         0.646121       15        0.0138797              N/A      0.00s\n",
      "best_fitness_generation 0.013879689601269939\n",
      "best_fitness 0.013854614390274494\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    14.98         0.539433       15        0.0138797              N/A      0.00s\n",
      "best_fitness_generation 0.013879689601269939\n",
      "best_fitness 0.013854614390274494\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    14.97         0.434126       15        0.0138797              N/A      0.00s\n",
      "best_fitness_generation 0.013879689601269939\n",
      "best_fitness 0.013854614390274494\n",
      "0.456064511713849*X0 + 0.147\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.87       31        0.0272244              N/A      0.00s\n",
      "best_fitness_generation 0.02722438151521167\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.70573       39       0.00466317              N/A      0.00s\n",
      "best_fitness_generation 0.004663167019202756\n",
      "best_fitness 0.02722438151521167\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    30.58          21.8945       39       0.00466317              N/A      0.00s\n",
      "best_fitness_generation 0.004663167019202756\n",
      "best_fitness 0.004663167019202756\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    31.61          3.85838       39       0.00357865              N/A      0.00s\n",
      "best_fitness_generation 0.0035786501346269602\n",
      "best_fitness 0.004663167019202756\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    25.76          1.18355       21       0.00204238              N/A      0.00s\n",
      "best_fitness_generation 0.0020423831854948452\n",
      "best_fitness 0.0035786501346269602\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    20.85         0.954854       21       0.00204238              N/A      0.00s\n",
      "best_fitness_generation"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  41 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=5)]: Done  42 tasks      | elapsed: 20.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   3    29.02         0.989537       47       0.00511148              N/A      0.00s\n",
      "best_fitness_generation 0.005111482958760529\n",
      "best_fitness 0.00991764674631912\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    26.76         0.631167       31       0.00315142              N/A      0.00s\n",
      "best_fitness_generation 0.0031514232363886244\n",
      "best_fitness 0.005111482958760529\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    18.14         0.636773       31       0.00315142              N/A      0.00s\n",
      "best_fitness_generation 0.0031514232363886244\n",
      "best_fitness 0.0031514232363886244\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    17.35           0.6933       23       0.00526662              N/A      0.00s\n",
      "best_fitness_generation 0.00526661734332826\n",
      "best_fitness 0.0031514232363886244\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    17.06         0.731222       17       0.00785245              N/A      0.00s\n",
      "best_fitness_generation 0.007852451494291892\n",
      "best_fitness 0.0031514232363886244\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    17.03         0.693346       17       0.00785245              N/A      0.00s\n",
      "best_fitness_generation 0.007852451494291892\n",
      "best_fitness 0.0031514232363886244\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   9    17.08          0.57833       17       0.00606689              N/A      0.00s\n",
      "best_fitness_generation 0.006066893020953298\n",
      "best_fitness 0.0031514232363886244\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  10    16.90         0.707174       17       0.00606689              N/A      0.00s\n",
      "best_fitness_generation 0.006066893020953298\n",
      "best_fitness 0.0031514232363886244\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  11    17.02         0.629512       17       0.00606689              N/A      0.00s\n",
      "best_fitness_generation 0.006066893020953298\n",
      "best_fitness 0.0031514232363886244\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  12    16.91         0.974597       17       0.00606689              N/A      0.00s\n",
      "best_fitness_generation 0.006066893020953298\n",
      "best_fitness 0.0031514232363886244\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  13    16.98         0.834091       17       0.00606689              N/A      0.00s\n",
      "best_fitness_generation 0.006066893020953298\n",
      "best_fitness 0.0031514232363886244\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "  14    16.87         0.570771       17       0.00606689              N/A      0.00s\n",
      "best_fitness_generation 0.006066893020953298\n",
      "best_fitness 0.0031514232363886244\n",
      "0.49735 - 0.421*X0\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    31.00          2646.92       31        0.0558042              N/A      0.00s\n",
      "best_fitness_generation 0.05580418733662739\n",
      "best_fitness inf\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   1    30.93          2.61921       31        0.0177691              N/A      0.00s\n",
      "best_fitness_generation 0.017769119782365995\n",
      "best_fitness 0.05580418733662739\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   2    29.19          1.12723       33       0.00960657              N/A      0.00s\n",
      "best_fitness_generation 0.009606570006881785\n",
      "best_fitness 0.017769119782365995\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   3    30.52          1.13026       33       0.00425625              N/A      0.00s\n",
      "best_fitness_generation 0.004256250894018228\n",
      "best_fitness 0.009606570006881785\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   4    29.78         0.723333       31       0.00217658              N/A      0.00s\n",
      "best_fitness_generation 0.00217658160605277\n",
      "best_fitness 0.004256250894018228\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   5    27.50         0.831653       31       0.00217658              N/A      0.00s\n",
      "best_fitness_generation 0.00217658160605277\n",
      "best_fitness 0.00217658160605277\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   6    26.78          0.62578       31       0.00217658              N/A      0.00s\n",
      "best_fitness_generation 0.00217658160605277\n",
      "best_fitness 0.00217658160605277\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   7    17.36         0.465193       31       0.00217658              N/A      0.00s\n",
      "best_fitness_generation 0.00217658160605277\n",
      "best_fitness 0.00217658160605277\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   8    11.49         0.560062       15       0.00470218              N/A      0.00s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  43 tasks      | elapsed: 20.8min\n",
      "[Parallel(n_jobs=5)]: Done  44 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=5)]: Done  45 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=5)]: Done  46 tasks      | elapsed: 21.3min\n"
     ]
    }
   ],
   "source": [
    "if symbolic_regression_evaluation:\n",
    "    print('----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        symbolic_regression_functions_test = symbolic_regression_function_generation(lambda_net_test_dataset)\n",
    "        polynomial_dict_test_list[i]['symbolic_regression_functions'] = symbolic_regression_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Symbolic Regression Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if per_network_evaluation:\n",
    "    print('------------------------------------------------ CALCULATE PER NETWORK POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        per_network_poly_test = per_network_poly_generation(lambda_net_test_dataset, optimization_type='scipy')\n",
    "        polynomial_dict_test_list[i]['per_network_polynomials'] = per_network_poly_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Per Network Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:03.719173Z",
     "iopub.status.busy": "2021-10-21T09:30:03.718971Z",
     "iopub.status.idle": "2021-10-21T09:30:25.475423Z",
     "shell.execute_reply": "2021-10-21T09:30:25.474457Z",
     "shell.execute_reply.started": "2021-10-21T09:30:03.719142Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  31 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metamodel_poly\n",
      "Exit <class 'KeyError'>\n",
      "metamodel_functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=10)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metamodel_functions_no_GD\n",
      "Exit <class 'KeyError'>\n",
      "symbolic_regression_functions\n",
      "per_network_polynomials\n",
      "False\n",
      "FV Calculation Time: 0:00:21\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "print('------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "function_values_test_list = []\n",
    "for lambda_net_test_dataset, polynomial_dict_test in zip(lambda_net_test_dataset_list, polynomial_dict_test_list):\n",
    "    function_values_test = calculate_all_function_values(lambda_net_test_dataset, polynomial_dict_test)\n",
    "    function_values_test_list.append(function_values_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('FV Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:30.266828Z",
     "iopub.status.busy": "2021-10-21T09:30:30.266569Z",
     "iopub.status.idle": "2021-10-21T09:30:34.968056Z",
     "shell.execute_reply": "2021-10-21T09:30:34.967227Z",
     "shell.execute_reply.started": "2021-10-21T09:30:30.266793Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------\n",
      "lambda_preds_VS_target_polynomials\n",
      "lambda_preds_VS_lstsq_lambda_pred_polynomials\n",
      "lambda_preds_VS_lstsq_target_polynomials\n",
      "lambda_preds_VS_inet_polynomials\n",
      "lambda_preds_VS_metamodel_functions\n",
      "lambda_preds_VS_symbolic_regression_functions\n",
      "lambda_preds_VS_per_network_polynomials\n",
      "target_polynomials_VS_lstsq_lambda_pred_polynomials\n",
      "target_polynomials_VS_lstsq_target_polynomials\n",
      "target_polynomials_VS_inet_polynomials\n",
      "target_polynomials_VS_metamodel_functions\n",
      "target_polynomials_VS_symbolic_regression_functions\n",
      "target_polynomials_VS_per_network_polynomials\n",
      "lstsq_lambda_pred_polynomials_VS_lstsq_target_polynomials\n",
      "lstsq_lambda_pred_polynomials_VS_inet_polynomials\n",
      "lstsq_lambda_pred_polynomials_VS_metamodel_functions\n",
      "lstsq_lambda_pred_polynomials_VS_symbolic_regression_functions\n",
      "lstsq_lambda_pred_polynomials_VS_per_network_polynomials\n",
      "lstsq_target_polynomials_VS_inet_polynomials\n",
      "lstsq_target_polynomials_VS_metamodel_functions\n",
      "lstsq_target_polynomials_VS_symbolic_regression_functions\n",
      "lstsq_target_polynomials_VS_per_network_polynomials\n",
      "inet_polynomials_VS_metamodel_functions\n",
      "inet_polynomials_VS_symbolic_regression_functions\n",
      "inet_polynomials_VS_per_network_polynomials\n",
      "metamodel_functions_VS_symbolic_regression_functions\n",
      "metamodel_functions_VS_per_network_polynomials\n",
      "symbolic_regression_functions_VS_per_network_polynomials\n",
      "Score Calculation Time: 0:00:04\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "scores_test_list = []\n",
    "distrib_dict_test_list = []\n",
    "\n",
    "for function_values_test, polynomial_dict_test in zip(function_values_test_list, polynomial_dict_test_list):\n",
    "    scores_test, distrib_test = evaluate_all_predictions(function_values_test, polynomial_dict_test)\n",
    "    scores_test_list.append(scores_test)\n",
    "    distrib_dict_test_list.append(distrib_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Score Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('---------------------------------------------------------------------------------------------------------------------------')         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:35.084530Z",
     "iopub.status.busy": "2021-10-21T09:30:35.084312Z",
     "iopub.status.idle": "2021-10-21T09:30:35.149578Z",
     "shell.execute_reply": "2021-10-21T09:30:35.148747Z",
     "shell.execute_reply.started": "2021-10-21T09:30:35.084508Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier_type = 'epochs' if samples_list == None else 'samples'\n",
    "save_results(scores_list=scores_test_list, by=identifier_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:35.211432Z",
     "iopub.status.busy": "2021-10-21T09:30:35.211200Z",
     "iopub.status.idle": "2021-10-21T09:30:35.276347Z",
     "shell.execute_reply": "2021-10-21T09:30:35.275339Z",
     "shell.execute_reply.started": "2021-10-21T09:30:35.211402Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:35.348423Z",
     "iopub.status.busy": "2021-10-21T09:30:35.347811Z",
     "iopub.status.idle": "2021-10-21T09:30:35.418817Z",
     "shell.execute_reply": "2021-10-21T09:30:35.416713Z",
     "shell.execute_reply.started": "2021-10-21T09:30:35.348396Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 61)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden1_2048 (Dense)            (None, 2048)         126976      input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation1_relu (Activation)   (None, 2048)         0           hidden1_2048[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_coeff_2 (Dense)          (None, 2)            4098        activation1_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier1_4 (Dense)    (None, 4)            8196        activation1_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier2_4 (Dense)    (None, 4)            8196        activation1_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_combined (Concatenate)   (None, 10)           0           output_coeff_2[0][0]             \n",
      "                                                                 output_identifier1_4[0][0]       \n",
      "                                                                 output_identifier2_4[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 147,466\n",
      "Trainable params: 147,466\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "{'name': 'model', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 61), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input'}, 'name': 'input', 'inbound_nodes': []}, {'class_name': 'Dense', 'config': {'name': 'hidden1_2048', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'hidden1_2048', 'inbound_nodes': [[['input', 0, 0, {}]]]}, {'class_name': 'Activation', 'config': {'name': 'activation1_relu', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}, 'name': 'activation1_relu', 'inbound_nodes': [[['hidden1_2048', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'output_coeff_2', 'trainable': True, 'dtype': 'float32', 'units': 2, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'output_coeff_2', 'inbound_nodes': [[['activation1_relu', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'output_identifier1_4', 'trainable': True, 'dtype': 'float32', 'units': 4, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'output_identifier1_4', 'inbound_nodes': [[['activation1_relu', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'output_identifier2_4', 'trainable': True, 'dtype': 'float32', 'units': 4, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'output_identifier2_4', 'inbound_nodes': [[['activation1_relu', 0, 0, {}]]]}, {'class_name': 'Concatenate', 'config': {'name': 'output_combined', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'name': 'output_combined', 'inbound_nodes': [[['output_coeff_2', 0, 0, {}], ['output_identifier1_4', 0, 0, {}], ['output_identifier2_4', 0, 0, {}]]]}], 'input_layers': [['input', 0, 0]], 'output_layers': [['output_combined', 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:35.492410Z",
     "iopub.status.busy": "2021-10-21T09:30:35.489941Z",
     "iopub.status.idle": "2021-10-21T09:30:35.551309Z",
     "shell.execute_reply": "2021-10-21T09:30:35.550486Z",
     "shell.execute_reply.started": "2021-10-21T09:30:35.492370Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:35.616503Z",
     "iopub.status.busy": "2021-10-21T09:30:35.616031Z",
     "iopub.status.idle": "2021-10-21T09:30:35.678990Z",
     "shell.execute_reply": "2021-10-21T09:30:35.678179Z",
     "shell.execute_reply.started": "2021-10-21T09:30:35.616467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#0.183\t0.234\t3.604\t0.143\t0.687\t2.559\t0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:35.763092Z",
     "iopub.status.busy": "2021-10-21T09:30:35.762862Z",
     "iopub.status.idle": "2021-10-21T09:30:35.858147Z",
     "shell.execute_reply": "2021-10-21T09:30:35.856374Z",
     "shell.execute_reply.started": "2021-10-21T09:30:35.763061Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Multilabel</th>\n",
       "      <th>MAE FV</th>\n",
       "      <th>RMSE FV</th>\n",
       "      <th>MAPE FV</th>\n",
       "      <th>R2 FV</th>\n",
       "      <th>RAAE FV</th>\n",
       "      <th>RMAE FV</th>\n",
       "      <th>MEAN STD FV DIFF</th>\n",
       "      <th>MEAN FV1</th>\n",
       "      <th>MEAN FV2</th>\n",
       "      <th>STD FV1</th>\n",
       "      <th>STD FV2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_target_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_target_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_inet_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.548</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_metamodel_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.862</td>\n",
       "      <td>4.380</td>\n",
       "      <td>-143.519</td>\n",
       "      <td>6.484</td>\n",
       "      <td>-4.360</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.779</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_symbolic_regression_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_per_network_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>0.156</td>\n",
       "      <td>0.296</td>\n",
       "      <td>2.227</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.495</td>\n",
       "      <td>0.721</td>\n",
       "      <td>1.806</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_metamodel_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.867</td>\n",
       "      <td>4.069</td>\n",
       "      <td>-65.395</td>\n",
       "      <td>5.124</td>\n",
       "      <td>-3.053</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.785</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.156</td>\n",
       "      <td>0.296</td>\n",
       "      <td>15.917</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.474</td>\n",
       "      <td>0.712</td>\n",
       "      <td>12.897</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_metamodel_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.862</td>\n",
       "      <td>4.821</td>\n",
       "      <td>-143.838</td>\n",
       "      <td>6.486</td>\n",
       "      <td>-4.362</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.779</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.495</td>\n",
       "      <td>0.721</td>\n",
       "      <td>1.806</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_metamodel_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.867</td>\n",
       "      <td>4.069</td>\n",
       "      <td>-65.395</td>\n",
       "      <td>5.124</td>\n",
       "      <td>-3.053</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.785</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_metamodel_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.832</td>\n",
       "      <td>22.175</td>\n",
       "      <td>-191.638</td>\n",
       "      <td>7.200</td>\n",
       "      <td>-5.139</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.756</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.123</td>\n",
       "      <td>14.267</td>\n",
       "      <td>-1.277</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.107</td>\n",
       "      <td>16.042</td>\n",
       "      <td>-1.368</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metamodel_functions_VS_symbolic_regression_functions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.862</td>\n",
       "      <td>1.584</td>\n",
       "      <td>-1140853940435.496</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metamodel_functions_VS_per_network_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.861</td>\n",
       "      <td>1.586</td>\n",
       "      <td>-1075903442154.623</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbolic_regression_functions_VS_per_network_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-368273370.586</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     MAE  RMSE   MAPE  \\\n",
       "lambda_preds_VS_target_polynomials                   NaN   NaN    NaN   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials        NaN   NaN    NaN   \n",
       "lambda_preds_VS_lstsq_target_polynomials             NaN   NaN    NaN   \n",
       "lambda_preds_VS_inet_polynomials                     NaN   NaN    NaN   \n",
       "lambda_preds_VS_metamodel_functions                  NaN   NaN    NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions        NaN   NaN    NaN   \n",
       "lambda_preds_VS_per_network_polynomials              NaN   NaN    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.156 0.296  2.227   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000  0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.495 0.721  1.806   \n",
       "target_polynomials_VS_metamodel_functions            NaN   NaN    NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...   NaN   NaN    NaN   \n",
       "target_polynomials_VS_per_network_polynomials        NaN   NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.156 0.296 15.917   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.474 0.712 12.897   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...   NaN   NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...   NaN   NaN    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.495 0.721  1.806   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      NaN   NaN    NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...   NaN   NaN    NaN   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...   NaN   NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_functions              NaN   NaN    NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions    NaN   NaN    NaN   \n",
       "inet_polynomials_VS_per_network_polynomials          NaN   NaN    NaN   \n",
       "metamodel_functions_VS_symbolic_regression_func...   NaN   NaN    NaN   \n",
       "metamodel_functions_VS_per_network_polynomials       NaN   NaN    NaN   \n",
       "symbolic_regression_functions_VS_per_network_po...   NaN   NaN    NaN   \n",
       "\n",
       "                                                    Accuracy  \\\n",
       "lambda_preds_VS_target_polynomials                       NaN   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials            NaN   \n",
       "lambda_preds_VS_lstsq_target_polynomials                 NaN   \n",
       "lambda_preds_VS_inet_polynomials                         NaN   \n",
       "lambda_preds_VS_metamodel_functions                      NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions            NaN   \n",
       "lambda_preds_VS_per_network_polynomials                  NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...     0.510   \n",
       "target_polynomials_VS_lstsq_target_polynomials         1.000   \n",
       "target_polynomials_VS_inet_polynomials                 0.085   \n",
       "target_polynomials_VS_metamodel_functions                NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...       NaN   \n",
       "target_polynomials_VS_per_network_polynomials            NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...     0.510   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials      0.045   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...       NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...       NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...       NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials           0.085   \n",
       "lstsq_target_polynomials_VS_metamodel_functions          NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...       NaN   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...       NaN   \n",
       "inet_polynomials_VS_metamodel_functions                  NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions        NaN   \n",
       "inet_polynomials_VS_per_network_polynomials              NaN   \n",
       "metamodel_functions_VS_symbolic_regression_func...       NaN   \n",
       "metamodel_functions_VS_per_network_polynomials           NaN   \n",
       "symbolic_regression_functions_VS_per_network_po...       NaN   \n",
       "\n",
       "                                                    Accuracy Multilabel  \\\n",
       "lambda_preds_VS_target_polynomials                                  NaN   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                       NaN   \n",
       "lambda_preds_VS_lstsq_target_polynomials                            NaN   \n",
       "lambda_preds_VS_inet_polynomials                                    NaN   \n",
       "lambda_preds_VS_metamodel_functions                                 NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions                       NaN   \n",
       "lambda_preds_VS_per_network_polynomials                             NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...                0.290   \n",
       "target_polynomials_VS_lstsq_target_polynomials                    1.000   \n",
       "target_polynomials_VS_inet_polynomials                            0.010   \n",
       "target_polynomials_VS_metamodel_functions                           NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...                  NaN   \n",
       "target_polynomials_VS_per_network_polynomials                       NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...                0.290   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials                 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...                  NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...                  NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...                  NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                      0.010   \n",
       "lstsq_target_polynomials_VS_metamodel_functions                     NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...                  NaN   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...                  NaN   \n",
       "inet_polynomials_VS_metamodel_functions                             NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions                   NaN   \n",
       "inet_polynomials_VS_per_network_polynomials                         NaN   \n",
       "metamodel_functions_VS_symbolic_regression_func...                  NaN   \n",
       "metamodel_functions_VS_per_network_polynomials                      NaN   \n",
       "symbolic_regression_functions_VS_per_network_po...                  NaN   \n",
       "\n",
       "                                                    MAE FV  RMSE FV  MAPE FV  \\\n",
       "lambda_preds_VS_target_polynomials                   0.017    0.023    0.049   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials        0.002    0.003    0.011   \n",
       "lambda_preds_VS_lstsq_target_polynomials             0.017    0.023    0.049   \n",
       "lambda_preds_VS_inet_polynomials                     0.104    0.122    0.548   \n",
       "lambda_preds_VS_metamodel_functions                  0.824    0.862    4.380   \n",
       "lambda_preds_VS_symbolic_regression_functions        0.012    0.016    0.085   \n",
       "lambda_preds_VS_per_network_polynomials              0.036    0.047    0.222   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...   0.016    0.020    0.065   \n",
       "target_polynomials_VS_lstsq_target_polynomials       0.000    0.000    0.000   \n",
       "target_polynomials_VS_inet_polynomials               0.107    0.126    0.515   \n",
       "target_polynomials_VS_metamodel_functions            0.828    0.867    4.069   \n",
       "target_polynomials_VS_symbolic_regression_funct...   0.025    0.032    0.136   \n",
       "target_polynomials_VS_per_network_polynomials        0.038    0.050    0.249   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...   0.016    0.020    0.045   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials    0.104    0.122    0.508   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   0.824    0.862    4.821   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...   0.012    0.015    0.101   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...   0.035    0.046    0.202   \n",
       "lstsq_target_polynomials_VS_inet_polynomials         0.107    0.126    0.515   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      0.828    0.867    4.069   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...   0.025    0.032    0.136   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...   0.038    0.050    0.249   \n",
       "inet_polynomials_VS_metamodel_functions              0.786    0.832   22.175   \n",
       "inet_polynomials_VS_symbolic_regression_functions    0.105    0.123   14.267   \n",
       "inet_polynomials_VS_per_network_polynomials          0.091    0.107   16.042   \n",
       "metamodel_functions_VS_symbolic_regression_func...   0.825    0.862    1.584   \n",
       "metamodel_functions_VS_per_network_polynomials       0.825    0.861    1.586   \n",
       "symbolic_regression_functions_VS_per_network_po...   0.037    0.047    0.262   \n",
       "\n",
       "                                                                R2 FV  \\\n",
       "lambda_preds_VS_target_polynomials                              0.458   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                   1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                        0.458   \n",
       "lambda_preds_VS_inet_polynomials                               -0.474   \n",
       "lambda_preds_VS_metamodel_functions                          -143.519   \n",
       "lambda_preds_VS_symbolic_regression_functions                   0.950   \n",
       "lambda_preds_VS_per_network_polynomials                         0.888   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...              0.903   \n",
       "target_polynomials_VS_lstsq_target_polynomials                  1.000   \n",
       "target_polynomials_VS_inet_polynomials                         -0.523   \n",
       "target_polynomials_VS_metamodel_functions                     -65.395   \n",
       "target_polynomials_VS_symbolic_regression_funct...              0.876   \n",
       "target_polynomials_VS_per_network_polynomials                   0.834   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...              0.458   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials              -0.467   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           -143.838   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...              0.950   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...              0.888   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                   -0.523   \n",
       "lstsq_target_polynomials_VS_metamodel_functions               -65.395   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...              0.876   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...              0.834   \n",
       "inet_polynomials_VS_metamodel_functions                      -191.638   \n",
       "inet_polynomials_VS_symbolic_regression_functions              -1.277   \n",
       "inet_polynomials_VS_per_network_polynomials                    -1.368   \n",
       "metamodel_functions_VS_symbolic_regression_func... -1140853940435.496   \n",
       "metamodel_functions_VS_per_network_polynomials     -1075903442154.623   \n",
       "symbolic_regression_functions_VS_per_network_po...     -368273370.586   \n",
       "\n",
       "                                                    RAAE FV  RMAE FV  \\\n",
       "lambda_preds_VS_target_polynomials                    0.205    0.666   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         0.009    0.042   \n",
       "lambda_preds_VS_lstsq_target_polynomials              0.205    0.666   \n",
       "lambda_preds_VS_inet_polynomials                      0.557    0.773   \n",
       "lambda_preds_VS_metamodel_functions                   6.484   -4.360   \n",
       "lambda_preds_VS_symbolic_regression_functions         0.096    0.218   \n",
       "lambda_preds_VS_per_network_polynomials               0.210    0.504   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    0.117    0.265   \n",
       "target_polynomials_VS_lstsq_target_polynomials        0.000    0.000   \n",
       "target_polynomials_VS_inet_polynomials                0.585    0.795   \n",
       "target_polynomials_VS_metamodel_functions             5.124   -3.053   \n",
       "target_polynomials_VS_symbolic_regression_funct...    0.175    0.392   \n",
       "target_polynomials_VS_per_network_polynomials         0.236    0.531   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    0.200    0.639   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     0.557    0.760   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    6.486   -4.362   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...    0.094    0.211   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...    0.210    0.492   \n",
       "lstsq_target_polynomials_VS_inet_polynomials          0.585    0.795   \n",
       "lstsq_target_polynomials_VS_metamodel_functions       5.124   -3.053   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...    0.175    0.392   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...    0.236    0.531   \n",
       "inet_polynomials_VS_metamodel_functions               7.200   -5.139   \n",
       "inet_polynomials_VS_symbolic_regression_functions     0.615    0.978   \n",
       "inet_polynomials_VS_per_network_polynomials           0.498    0.637   \n",
       "metamodel_functions_VS_symbolic_regression_func...      inf      inf   \n",
       "metamodel_functions_VS_per_network_polynomials          inf      inf   \n",
       "symbolic_regression_functions_VS_per_network_po...      inf      inf   \n",
       "\n",
       "                                                    MEAN STD FV DIFF  \\\n",
       "lambda_preds_VS_target_polynomials                             0.022   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                  0.003   \n",
       "lambda_preds_VS_lstsq_target_polynomials                       0.022   \n",
       "lambda_preds_VS_inet_polynomials                               0.090   \n",
       "lambda_preds_VS_metamodel_functions                            0.224   \n",
       "lambda_preds_VS_symbolic_regression_functions                  0.014   \n",
       "lambda_preds_VS_per_network_polynomials                        0.044   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...             0.020   \n",
       "target_polynomials_VS_lstsq_target_polynomials                 0.000   \n",
       "target_polynomials_VS_inet_polynomials                         0.093   \n",
       "target_polynomials_VS_metamodel_functions                      0.231   \n",
       "target_polynomials_VS_symbolic_regression_funct...             0.030   \n",
       "target_polynomials_VS_per_network_polynomials                  0.048   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...             0.020   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials              0.090   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...             0.224   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...             0.014   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...             0.043   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                   0.093   \n",
       "lstsq_target_polynomials_VS_metamodel_functions                0.231   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...             0.030   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...             0.048   \n",
       "inet_polynomials_VS_metamodel_functions                        0.238   \n",
       "inet_polynomials_VS_symbolic_regression_functions              0.090   \n",
       "inet_polynomials_VS_per_network_polynomials                    0.073   \n",
       "metamodel_functions_VS_symbolic_regression_func...             0.221   \n",
       "metamodel_functions_VS_per_network_polynomials                 0.213   \n",
       "symbolic_regression_functions_VS_per_network_po...             0.044   \n",
       "\n",
       "                                                    MEAN FV1  MEAN FV2  \\\n",
       "lambda_preds_VS_target_polynomials                    -0.104    -0.106   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         -0.104    -0.104   \n",
       "lambda_preds_VS_lstsq_target_polynomials              -0.104    -0.106   \n",
       "lambda_preds_VS_inet_polynomials                      -0.104    -0.070   \n",
       "lambda_preds_VS_metamodel_functions                   -0.104       NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions         -0.104    -0.105   \n",
       "lambda_preds_VS_per_network_polynomials               -0.104    -0.104   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    -0.106    -0.104   \n",
       "target_polynomials_VS_lstsq_target_polynomials        -0.106    -0.106   \n",
       "target_polynomials_VS_inet_polynomials                -0.106    -0.070   \n",
       "target_polynomials_VS_metamodel_functions             -0.106       NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...    -0.106    -0.105   \n",
       "target_polynomials_VS_per_network_polynomials         -0.106    -0.104   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    -0.104    -0.106   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     -0.104    -0.070   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    -0.104       NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...    -0.104    -0.105   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...    -0.104    -0.104   \n",
       "lstsq_target_polynomials_VS_inet_polynomials          -0.106    -0.070   \n",
       "lstsq_target_polynomials_VS_metamodel_functions       -0.106       NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...    -0.106    -0.105   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...    -0.106    -0.104   \n",
       "inet_polynomials_VS_metamodel_functions               -0.070       NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions     -0.070    -0.105   \n",
       "inet_polynomials_VS_per_network_polynomials           -0.070    -0.104   \n",
       "metamodel_functions_VS_symbolic_regression_func...       NaN    -0.105   \n",
       "metamodel_functions_VS_per_network_polynomials           NaN    -0.104   \n",
       "symbolic_regression_functions_VS_per_network_po...    -0.105    -0.104   \n",
       "\n",
       "                                                    STD FV1  STD FV2  \n",
       "lambda_preds_VS_target_polynomials                    0.779    0.785  \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         0.779    0.779  \n",
       "lambda_preds_VS_lstsq_target_polynomials              0.779    0.785  \n",
       "lambda_preds_VS_inet_polynomials                      0.779    0.756  \n",
       "lambda_preds_VS_metamodel_functions                   0.779      NaN  \n",
       "lambda_preds_VS_symbolic_regression_functions         0.779    0.779  \n",
       "lambda_preds_VS_per_network_polynomials               0.779    0.786  \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    0.785    0.779  \n",
       "target_polynomials_VS_lstsq_target_polynomials        0.785    0.785  \n",
       "target_polynomials_VS_inet_polynomials                0.785    0.756  \n",
       "target_polynomials_VS_metamodel_functions             0.785      NaN  \n",
       "target_polynomials_VS_symbolic_regression_funct...    0.785    0.779  \n",
       "target_polynomials_VS_per_network_polynomials         0.785    0.786  \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    0.779    0.785  \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     0.779    0.756  \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    0.779      NaN  \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...    0.779    0.779  \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...    0.779    0.786  \n",
       "lstsq_target_polynomials_VS_inet_polynomials          0.785    0.756  \n",
       "lstsq_target_polynomials_VS_metamodel_functions       0.785      NaN  \n",
       "lstsq_target_polynomials_VS_symbolic_regression...    0.785    0.779  \n",
       "lstsq_target_polynomials_VS_per_network_polynom...    0.785    0.786  \n",
       "inet_polynomials_VS_metamodel_functions               0.756      NaN  \n",
       "inet_polynomials_VS_symbolic_regression_functions     0.756    0.779  \n",
       "inet_polynomials_VS_per_network_polynomials           0.756    0.786  \n",
       "metamodel_functions_VS_symbolic_regression_func...      NaN    0.779  \n",
       "metamodel_functions_VS_per_network_polynomials          NaN    0.786  \n",
       "symbolic_regression_functions_VS_per_network_po...    0.779    0.786  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:35.926941Z",
     "iopub.status.busy": "2021-10-21T09:30:35.926659Z",
     "iopub.status.idle": "2021-10-21T09:30:35.998676Z",
     "shell.execute_reply": "2021-10-21T09:30:35.997728Z",
     "shell.execute_reply.started": "2021-10-21T09:30:35.926906Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05215618, 0.08780678, 0.01657939, 0.06406506, 0.5456385 ,\n",
       "       0.13421601, 0.0138118 , 0.0890084 , 0.08351042, 0.10908501,\n",
       "       0.09776805, 0.0304245 , 0.34661967, 0.03225134, 0.04824347,\n",
       "       0.09242237, 0.03908215, 0.05432433, 0.47830635, 0.05432834,\n",
       "       0.04198886, 0.07903894, 0.5845059 , 0.0034149 , 0.06499712,\n",
       "       0.07695328, 0.34523746, 0.00529256, 0.03673235, 0.03004109,\n",
       "       0.05913755, 0.05498796, 0.06513682, 0.01644997, 0.05541001,\n",
       "       0.07678991, 0.06709049, 0.0485206 , 0.03155126, 0.03918766,\n",
       "       0.02959145, 0.1558649 , 0.04679106, 0.03184014, 0.07443127,\n",
       "       0.01328758, 0.02448245, 0.02398834, 0.05689285, 0.11348906,\n",
       "       0.09765755, 0.04980956, 0.59041756, 0.0991414 , 0.07743409,\n",
       "       0.00949333, 0.01849242, 0.53332156, 0.04369245, 0.07046679,\n",
       "       0.02005086, 0.07418898, 0.18798894, 0.05859619, 0.0701904 ,\n",
       "       0.02333119, 0.03537287, 0.54149723, 0.07604847, 0.0433985 ,\n",
       "       0.6441436 , 0.02847311, 0.04081523, 0.05501246, 0.01915803,\n",
       "       0.0832786 , 0.0287376 , 0.01040666, 0.0612655 , 0.00483835,\n",
       "       0.4560877 , 0.02565609, 0.03902853, 0.04384688, 0.03810994,\n",
       "       0.03690073, 0.00776641, 0.04719659, 0.05550422, 0.09062282,\n",
       "       0.08333829, 0.02771869, 0.26198775, 0.0313213 , 0.0932327 ,\n",
       "       0.234454  , 0.04060271, 0.02465622, 0.09311091, 0.16581267],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:36.073283Z",
     "iopub.status.busy": "2021-10-21T09:30:36.073026Z",
     "iopub.status.idle": "2021-10-21T09:30:36.159429Z",
     "shell.execute_reply": "2021-10-21T09:30:36.155638Z",
     "shell.execute_reply.started": "2021-10-21T09:30:36.073261Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.40372120e-01,  9.52988229e-01,  9.95845930e-01,  9.73307746e-01,\n",
       "       -7.76324242e-01,  8.80935871e-01,  8.38222373e-01,  8.66137782e-01,\n",
       "        9.57608862e-01,  9.58574677e-01,  8.90295050e-01,  8.33166562e-01,\n",
       "        5.66749530e-02,  5.43136912e-01,  7.77068411e-01,  9.57404103e-01,\n",
       "        8.70436411e-01,  9.35657162e-01, -1.09840701e+01,  8.75796613e-01,\n",
       "        8.36792630e-01,  8.39272889e-01, -5.35929810e-01,  9.36244073e-01,\n",
       "        8.30435835e-01,  8.30690337e-01, -3.01202799e+00,  9.20877179e-01,\n",
       "        9.74870428e-01,  9.32301827e-01,  9.81919481e-01,  1.27367940e-01,\n",
       "        8.22230570e-01,  9.26523606e-01,  9.03731740e-01,  8.29780873e-01,\n",
       "        8.03559994e-01,  9.26760822e-01,  9.06548050e-01,  8.44718899e-01,\n",
       "        9.29979455e-01,  8.02929273e-01,  8.40275140e-01,  8.31360775e-01,\n",
       "        9.08688681e-01,  9.32421537e-01,  9.69855068e-01,  8.22319674e-01,\n",
       "        9.51753126e-01,  8.50896544e-01,  8.82109055e-01,  6.37275099e-01,\n",
       "       -2.19612887e+01,  8.86572633e-01,  7.76177563e-01,  7.04855232e-01,\n",
       "        9.59914962e-01, -1.90657390e+01, -4.55385340e-02,  9.04598725e-01,\n",
       "        8.44902113e-01,  9.66079999e-01, -4.41405685e+00,  8.22278531e-01,\n",
       "        9.58460713e-01,  8.28164014e-01,  9.29569625e-01,  3.08455057e-01,\n",
       "        8.39612545e-01,  9.60531218e-01, -4.75845365e+01,  9.91613515e-01,\n",
       "        9.29599908e-01,  9.34479867e-01,  9.91242256e-01,  8.32863593e-01,\n",
       "        9.92609627e-01,  9.49942039e-01,  9.53038588e-01,  9.23882665e-01,\n",
       "       -3.33992343e-01,  9.53693232e-01,  7.70977675e-01,  9.14779495e-01,\n",
       "        8.27586087e-01,  9.01058156e-01,  9.29305734e-01,  8.78512317e-01,\n",
       "        8.76222930e-01,  9.08117120e-01,  9.02599094e-01,  8.20867091e-01,\n",
       "       -1.07565946e+01,  2.97212324e-01, -2.64947833e+00,  5.48291152e-01,\n",
       "        8.37977926e-01,  8.35248741e-01,  9.52313934e-01,  8.56177129e-01])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:36.350028Z",
     "iopub.status.busy": "2021-10-21T09:30:36.349536Z",
     "iopub.status.idle": "2021-10-21T09:30:36.535337Z",
     "shell.execute_reply": "2021-10-21T09:30:36.534625Z",
     "shell.execute_reply.started": "2021-10-21T09:30:36.349993Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L-0</th>\n",
       "      <th>L-1</th>\n",
       "      <th>L-2</th>\n",
       "      <th>L-3</th>\n",
       "      <th>L-4</th>\n",
       "      <th>L-5</th>\n",
       "      <th>L-6</th>\n",
       "      <th>L-7</th>\n",
       "      <th>L-8</th>\n",
       "      <th>L-9</th>\n",
       "      <th>L-10</th>\n",
       "      <th>L-11</th>\n",
       "      <th>L-12</th>\n",
       "      <th>L-13</th>\n",
       "      <th>L-14</th>\n",
       "      <th>L-15</th>\n",
       "      <th>L-16</th>\n",
       "      <th>L-17</th>\n",
       "      <th>L-18</th>\n",
       "      <th>L-19</th>\n",
       "      <th>L-20</th>\n",
       "      <th>L-21</th>\n",
       "      <th>L-22</th>\n",
       "      <th>L-23</th>\n",
       "      <th>L-24</th>\n",
       "      <th>L-25</th>\n",
       "      <th>L-26</th>\n",
       "      <th>L-27</th>\n",
       "      <th>L-28</th>\n",
       "      <th>L-29</th>\n",
       "      <th>L-30</th>\n",
       "      <th>L-31</th>\n",
       "      <th>L-32</th>\n",
       "      <th>L-33</th>\n",
       "      <th>L-34</th>\n",
       "      <th>L-35</th>\n",
       "      <th>L-36</th>\n",
       "      <th>L-37</th>\n",
       "      <th>L-38</th>\n",
       "      <th>L-39</th>\n",
       "      <th>L-40</th>\n",
       "      <th>L-41</th>\n",
       "      <th>L-42</th>\n",
       "      <th>L-43</th>\n",
       "      <th>L-44</th>\n",
       "      <th>L-45</th>\n",
       "      <th>L-46</th>\n",
       "      <th>L-47</th>\n",
       "      <th>L-48</th>\n",
       "      <th>L-49</th>\n",
       "      <th>L-50</th>\n",
       "      <th>L-51</th>\n",
       "      <th>L-52</th>\n",
       "      <th>L-53</th>\n",
       "      <th>L-54</th>\n",
       "      <th>L-55</th>\n",
       "      <th>L-56</th>\n",
       "      <th>L-57</th>\n",
       "      <th>L-58</th>\n",
       "      <th>L-59</th>\n",
       "      <th>L-60</th>\n",
       "      <th>L-61</th>\n",
       "      <th>L-62</th>\n",
       "      <th>L-63</th>\n",
       "      <th>L-64</th>\n",
       "      <th>L-65</th>\n",
       "      <th>L-66</th>\n",
       "      <th>L-67</th>\n",
       "      <th>L-68</th>\n",
       "      <th>L-69</th>\n",
       "      <th>L-70</th>\n",
       "      <th>L-71</th>\n",
       "      <th>L-72</th>\n",
       "      <th>L-73</th>\n",
       "      <th>L-74</th>\n",
       "      <th>L-75</th>\n",
       "      <th>L-76</th>\n",
       "      <th>L-77</th>\n",
       "      <th>L-78</th>\n",
       "      <th>L-79</th>\n",
       "      <th>L-80</th>\n",
       "      <th>L-81</th>\n",
       "      <th>L-82</th>\n",
       "      <th>L-83</th>\n",
       "      <th>L-84</th>\n",
       "      <th>L-85</th>\n",
       "      <th>L-86</th>\n",
       "      <th>L-87</th>\n",
       "      <th>L-88</th>\n",
       "      <th>L-89</th>\n",
       "      <th>L-90</th>\n",
       "      <th>L-91</th>\n",
       "      <th>L-92</th>\n",
       "      <th>L-93</th>\n",
       "      <th>L-94</th>\n",
       "      <th>L-95</th>\n",
       "      <th>L-96</th>\n",
       "      <th>L-97</th>\n",
       "      <th>L-98</th>\n",
       "      <th>L-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_target_polynomials</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_inet_polynomials</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_metamodel_functions</th>\n",
       "      <td>0.203</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.379</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.754</td>\n",
       "      <td>1.384</td>\n",
       "      <td>0.887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.102</td>\n",
       "      <td>1.579</td>\n",
       "      <td>0.045</td>\n",
       "      <td>1.175</td>\n",
       "      <td>0.513</td>\n",
       "      <td>1.366</td>\n",
       "      <td>0.295</td>\n",
       "      <td>1.087</td>\n",
       "      <td>1.351</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.529</td>\n",
       "      <td>2.095</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.298</td>\n",
       "      <td>0.322</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197</td>\n",
       "      <td>1.114</td>\n",
       "      <td>1.555</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.082</td>\n",
       "      <td>1.715</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.527</td>\n",
       "      <td>1.341</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.177</td>\n",
       "      <td>1.193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.601</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072</td>\n",
       "      <td>1.072</td>\n",
       "      <td>2.270</td>\n",
       "      <td>1.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.506</td>\n",
       "      <td>0.621</td>\n",
       "      <td>1.644</td>\n",
       "      <td>1.706</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1.424</td>\n",
       "      <td>0.894</td>\n",
       "      <td>1.358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.091</td>\n",
       "      <td>1.822</td>\n",
       "      <td>0.686</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.316</td>\n",
       "      <td>1.212</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.407</td>\n",
       "      <td>1.260</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.492</td>\n",
       "      <td>1.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_symbolic_regression_functions</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_per_network_polynomials</th>\n",
       "      <td>0.031</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.049</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_metamodel_functions</th>\n",
       "      <td>0.206</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.379</td>\n",
       "      <td>2.004</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.761</td>\n",
       "      <td>1.388</td>\n",
       "      <td>0.887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.092</td>\n",
       "      <td>1.581</td>\n",
       "      <td>0.045</td>\n",
       "      <td>1.175</td>\n",
       "      <td>0.515</td>\n",
       "      <td>1.366</td>\n",
       "      <td>0.297</td>\n",
       "      <td>1.087</td>\n",
       "      <td>1.351</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.531</td>\n",
       "      <td>2.097</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.195</td>\n",
       "      <td>1.204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.305</td>\n",
       "      <td>0.323</td>\n",
       "      <td>1.041</td>\n",
       "      <td>0.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.182</td>\n",
       "      <td>1.272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197</td>\n",
       "      <td>1.113</td>\n",
       "      <td>1.581</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.083</td>\n",
       "      <td>1.744</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.526</td>\n",
       "      <td>1.341</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.177</td>\n",
       "      <td>1.193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.617</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086</td>\n",
       "      <td>1.069</td>\n",
       "      <td>2.277</td>\n",
       "      <td>1.019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.505</td>\n",
       "      <td>0.621</td>\n",
       "      <td>1.647</td>\n",
       "      <td>1.718</td>\n",
       "      <td>0.414</td>\n",
       "      <td>1.430</td>\n",
       "      <td>0.894</td>\n",
       "      <td>1.359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.115</td>\n",
       "      <td>1.823</td>\n",
       "      <td>0.687</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.316</td>\n",
       "      <td>1.212</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.408</td>\n",
       "      <td>1.270</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.491</td>\n",
       "      <td>1.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>0.028</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_metamodel_functions</th>\n",
       "      <td>0.203</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.379</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.754</td>\n",
       "      <td>1.385</td>\n",
       "      <td>0.887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.102</td>\n",
       "      <td>1.580</td>\n",
       "      <td>0.045</td>\n",
       "      <td>1.175</td>\n",
       "      <td>0.514</td>\n",
       "      <td>1.366</td>\n",
       "      <td>0.295</td>\n",
       "      <td>1.088</td>\n",
       "      <td>1.352</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.529</td>\n",
       "      <td>2.095</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.298</td>\n",
       "      <td>0.322</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197</td>\n",
       "      <td>1.113</td>\n",
       "      <td>1.555</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.082</td>\n",
       "      <td>1.715</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.527</td>\n",
       "      <td>1.341</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.177</td>\n",
       "      <td>1.193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.601</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072</td>\n",
       "      <td>1.072</td>\n",
       "      <td>2.271</td>\n",
       "      <td>1.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.506</td>\n",
       "      <td>0.621</td>\n",
       "      <td>1.644</td>\n",
       "      <td>1.706</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1.424</td>\n",
       "      <td>0.894</td>\n",
       "      <td>1.358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.091</td>\n",
       "      <td>1.822</td>\n",
       "      <td>0.687</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.316</td>\n",
       "      <td>1.212</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.407</td>\n",
       "      <td>1.261</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.492</td>\n",
       "      <td>1.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>0.031</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.049</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_metamodel_functions</th>\n",
       "      <td>0.206</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.379</td>\n",
       "      <td>2.004</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.761</td>\n",
       "      <td>1.388</td>\n",
       "      <td>0.887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.092</td>\n",
       "      <td>1.581</td>\n",
       "      <td>0.045</td>\n",
       "      <td>1.175</td>\n",
       "      <td>0.515</td>\n",
       "      <td>1.366</td>\n",
       "      <td>0.297</td>\n",
       "      <td>1.087</td>\n",
       "      <td>1.351</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.531</td>\n",
       "      <td>2.097</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.195</td>\n",
       "      <td>1.204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.305</td>\n",
       "      <td>0.323</td>\n",
       "      <td>1.041</td>\n",
       "      <td>0.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.182</td>\n",
       "      <td>1.272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197</td>\n",
       "      <td>1.113</td>\n",
       "      <td>1.581</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.083</td>\n",
       "      <td>1.744</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.526</td>\n",
       "      <td>1.341</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.177</td>\n",
       "      <td>1.193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.617</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086</td>\n",
       "      <td>1.069</td>\n",
       "      <td>2.277</td>\n",
       "      <td>1.019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.505</td>\n",
       "      <td>0.621</td>\n",
       "      <td>1.647</td>\n",
       "      <td>1.718</td>\n",
       "      <td>0.414</td>\n",
       "      <td>1.430</td>\n",
       "      <td>0.894</td>\n",
       "      <td>1.359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.115</td>\n",
       "      <td>1.823</td>\n",
       "      <td>0.687</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.316</td>\n",
       "      <td>1.212</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.408</td>\n",
       "      <td>1.270</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.491</td>\n",
       "      <td>1.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>0.028</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_metamodel_functions</th>\n",
       "      <td>0.208</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.339</td>\n",
       "      <td>1.521</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.753</td>\n",
       "      <td>1.295</td>\n",
       "      <td>0.895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.103</td>\n",
       "      <td>1.304</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.464</td>\n",
       "      <td>1.371</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.616</td>\n",
       "      <td>1.398</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.518</td>\n",
       "      <td>1.606</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.164</td>\n",
       "      <td>1.186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.300</td>\n",
       "      <td>0.338</td>\n",
       "      <td>1.031</td>\n",
       "      <td>0.619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.146</td>\n",
       "      <td>1.141</td>\n",
       "      <td>1.577</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.066</td>\n",
       "      <td>1.770</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.658</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.764</td>\n",
       "      <td>1.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.622</td>\n",
       "      <td>1.655</td>\n",
       "      <td>1.725</td>\n",
       "      <td>0.414</td>\n",
       "      <td>1.412</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.093</td>\n",
       "      <td>1.462</td>\n",
       "      <td>0.661</td>\n",
       "      <td>1.140</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.382</td>\n",
       "      <td>1.348</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.493</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.487</td>\n",
       "      <td>1.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>0.053</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>0.022</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metamodel_functions_VS_symbolic_regression_functions</th>\n",
       "      <td>0.201</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.375</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.752</td>\n",
       "      <td>1.387</td>\n",
       "      <td>0.880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.581</td>\n",
       "      <td>0.047</td>\n",
       "      <td>1.176</td>\n",
       "      <td>0.514</td>\n",
       "      <td>1.366</td>\n",
       "      <td>0.287</td>\n",
       "      <td>1.069</td>\n",
       "      <td>1.344</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.552</td>\n",
       "      <td>2.105</td>\n",
       "      <td>1.011</td>\n",
       "      <td>1.155</td>\n",
       "      <td>1.176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.305</td>\n",
       "      <td>0.316</td>\n",
       "      <td>1.048</td>\n",
       "      <td>0.591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.168</td>\n",
       "      <td>1.255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.196</td>\n",
       "      <td>1.116</td>\n",
       "      <td>1.557</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.078</td>\n",
       "      <td>1.708</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.536</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.193</td>\n",
       "      <td>1.191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.602</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.078</td>\n",
       "      <td>2.282</td>\n",
       "      <td>1.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.508</td>\n",
       "      <td>0.624</td>\n",
       "      <td>1.644</td>\n",
       "      <td>1.702</td>\n",
       "      <td>0.419</td>\n",
       "      <td>1.424</td>\n",
       "      <td>0.894</td>\n",
       "      <td>1.359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.092</td>\n",
       "      <td>1.824</td>\n",
       "      <td>0.682</td>\n",
       "      <td>1.120</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.929</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.316</td>\n",
       "      <td>1.201</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.411</td>\n",
       "      <td>1.267</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.488</td>\n",
       "      <td>1.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metamodel_functions_VS_per_network_polynomials</th>\n",
       "      <td>0.212</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.379</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.759</td>\n",
       "      <td>1.407</td>\n",
       "      <td>0.897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.096</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.028</td>\n",
       "      <td>1.191</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.385</td>\n",
       "      <td>0.296</td>\n",
       "      <td>1.084</td>\n",
       "      <td>1.375</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.517</td>\n",
       "      <td>2.114</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.177</td>\n",
       "      <td>1.201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.301</td>\n",
       "      <td>0.326</td>\n",
       "      <td>1.031</td>\n",
       "      <td>0.593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.147</td>\n",
       "      <td>1.260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.173</td>\n",
       "      <td>1.134</td>\n",
       "      <td>1.583</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.754</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.534</td>\n",
       "      <td>1.344</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.186</td>\n",
       "      <td>1.192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.638</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071</td>\n",
       "      <td>1.061</td>\n",
       "      <td>2.282</td>\n",
       "      <td>0.982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.529</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1.666</td>\n",
       "      <td>1.734</td>\n",
       "      <td>0.409</td>\n",
       "      <td>1.448</td>\n",
       "      <td>0.868</td>\n",
       "      <td>1.358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.093</td>\n",
       "      <td>1.848</td>\n",
       "      <td>0.656</td>\n",
       "      <td>1.151</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.923</td>\n",
       "      <td>1.382</td>\n",
       "      <td>1.341</td>\n",
       "      <td>1.237</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.402</td>\n",
       "      <td>1.271</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.385</td>\n",
       "      <td>1.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbolic_regression_functions_VS_per_network_polynomials</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     L-0   L-1   L-2   L-3  \\\n",
       "lambda_preds_VS_target_polynomials                 0.007 0.002 0.002 0.002   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.002 0.002 0.002   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.007 0.002 0.002 0.002   \n",
       "lambda_preds_VS_inet_polynomials                   0.052 0.088 0.017 0.064   \n",
       "lambda_preds_VS_metamodel_functions                0.203 0.431 0.837 0.379   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.005 0.020 0.013 0.005   \n",
       "lambda_preds_VS_per_network_polynomials            0.031 0.013 0.022 0.006   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.007 0.000 0.001 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.049 0.088 0.017 0.064   \n",
       "target_polynomials_VS_metamodel_functions          0.206 0.431 0.837 0.379   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.009 0.020 0.014 0.005   \n",
       "target_polynomials_VS_per_network_polynomials      0.028 0.013 0.022 0.006   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.007 0.000 0.001 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.052 0.088 0.016 0.064   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.203 0.431 0.837 0.379   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.005 0.020 0.013 0.005   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.031 0.013 0.021 0.006   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.049 0.088 0.017 0.064   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.206 0.431 0.837 0.379   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.009 0.020 0.014 0.005   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.028 0.013 0.022 0.006   \n",
       "inet_polynomials_VS_metamodel_functions            0.208 0.413 0.821 0.339   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.053 0.076 0.004 0.062   \n",
       "inet_polynomials_VS_per_network_polynomials        0.022 0.078 0.006 0.065   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.201 0.434 0.824 0.375   \n",
       "metamodel_functions_VS_per_network_polynomials     0.212 0.422 0.822 0.379   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.033 0.012 0.010 0.004   \n",
       "\n",
       "                                                     L-4   L-5   L-6   L-7  \\\n",
       "lambda_preds_VS_target_polynomials                 0.012 0.002 0.015 0.008   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.012 0.002 0.000 0.007   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.012 0.002 0.015 0.008   \n",
       "lambda_preds_VS_inet_polynomials                   0.546 0.134 0.014 0.089   \n",
       "lambda_preds_VS_metamodel_functions                2.000 0.776 0.754 1.384   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.015 0.015 0.009 0.016   \n",
       "lambda_preds_VS_per_network_polynomials            0.061 0.034 0.013 0.045   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.004 0.001 0.015 0.004   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.549 0.134 0.008 0.093   \n",
       "target_polynomials_VS_metamodel_functions          2.004 0.777 0.761 1.388   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.008 0.014 0.023 0.018   \n",
       "target_polynomials_VS_per_network_polynomials      0.063 0.034 0.006 0.043   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.004 0.001 0.015 0.004   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.546 0.134 0.014 0.089   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 2.000 0.776 0.754 1.385   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.012 0.014 0.009 0.015   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.064 0.034 0.013 0.046   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.549 0.134 0.008 0.093   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    2.004 0.777 0.761 1.388   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.008 0.014 0.023 0.018   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.063 0.034 0.006 0.043   \n",
       "inet_polynomials_VS_metamodel_functions            1.521 0.873 0.753 1.295   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.557 0.125 0.020 0.092   \n",
       "inet_polynomials_VS_per_network_polynomials        0.523 0.112 0.006 0.119   \n",
       "metamodel_functions_VS_symbolic_regression_func... 2.010 0.768 0.752 1.387   \n",
       "metamodel_functions_VS_per_network_polynomials     2.010 0.789 0.759 1.407   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.061 0.028 0.019 0.058   \n",
       "\n",
       "                                                     L-8   L-9  L-10  L-11  \\\n",
       "lambda_preds_VS_target_polynomials                 0.002 0.013 0.001 0.022   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.002 0.013 0.001 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.002 0.013 0.001 0.022   \n",
       "lambda_preds_VS_inet_polynomials                   0.084 0.109 0.098 0.030   \n",
       "lambda_preds_VS_metamodel_functions                0.887   NaN 0.914 0.102   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.021 0.017 0.008 0.003   \n",
       "lambda_preds_VS_per_network_polynomials            0.019 0.022 0.029 0.030   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.004 0.000 0.022   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.083 0.113 0.098 0.050   \n",
       "target_polynomials_VS_metamodel_functions          0.887   NaN 0.914 0.092   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.021 0.012 0.008 0.022   \n",
       "target_polynomials_VS_per_network_polynomials      0.018 0.019 0.029 0.049   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.004 0.000 0.022   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.083 0.113 0.098 0.030   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.887   NaN 0.914 0.102   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.021 0.014 0.008 0.003   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.018 0.021 0.029 0.030   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.083 0.113 0.098 0.050   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.887   NaN 0.914 0.092   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.021 0.012 0.008 0.022   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.018 0.019 0.029 0.049   \n",
       "inet_polynomials_VS_metamodel_functions            0.895   NaN 0.825 0.103   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.102 0.112 0.098 0.029   \n",
       "inet_polynomials_VS_per_network_polynomials        0.067 0.097 0.077 0.010   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.880   NaN 0.915 0.100   \n",
       "metamodel_functions_VS_per_network_polynomials     0.897   NaN 0.902 0.096   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.039 0.015 0.030 0.029   \n",
       "\n",
       "                                                    L-12  L-13  L-14  L-15  \\\n",
       "lambda_preds_VS_target_polynomials                 0.007 0.001 0.002 0.006   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.007 0.000 0.001 0.006   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.007 0.001 0.002 0.006   \n",
       "lambda_preds_VS_inet_polynomials                   0.347 0.032 0.048 0.092   \n",
       "lambda_preds_VS_metamodel_functions                1.579 0.045 1.175 0.513   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.010 0.010 0.005 0.017   \n",
       "lambda_preds_VS_per_network_polynomials            0.076 0.024 0.063 0.017   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.002 0.000 0.001 0.002   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.348 0.032 0.048 0.093   \n",
       "target_polynomials_VS_metamodel_functions          1.581 0.045 1.175 0.515   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.009 0.010 0.005 0.018   \n",
       "target_polynomials_VS_per_network_polynomials      0.077 0.024 0.063 0.018   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.002 0.000 0.001 0.002   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.348 0.032 0.049 0.092   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.580 0.045 1.175 0.514   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.010 0.010 0.005 0.017   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.077 0.024 0.063 0.018   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.348 0.032 0.048 0.093   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    1.581 0.045 1.175 0.515   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.009 0.010 0.005 0.018   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.077 0.024 0.063 0.018   \n",
       "inet_polynomials_VS_metamodel_functions            1.304 0.029 1.183 0.464   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.350 0.027 0.044 0.082   \n",
       "inet_polynomials_VS_per_network_polynomials        0.338 0.013 0.017 0.078   \n",
       "metamodel_functions_VS_symbolic_regression_func... 1.581 0.047 1.176 0.514   \n",
       "metamodel_functions_VS_per_network_polynomials     1.599 0.028 1.191 0.500   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.082 0.021 0.059 0.021   \n",
       "\n",
       "                                                    L-16  L-17  L-18  L-19  \\\n",
       "lambda_preds_VS_target_polynomials                 0.000 0.002 0.006 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.002 0.005 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.000 0.002 0.006 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.039 0.054 0.478 0.054   \n",
       "lambda_preds_VS_metamodel_functions                1.366 0.295 1.087 1.351   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.007 0.010 0.028 0.008   \n",
       "lambda_preds_VS_per_network_polynomials            0.055 0.020 0.014 0.078   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.001 0.003 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.039 0.054 0.480 0.055   \n",
       "target_polynomials_VS_metamodel_functions          1.366 0.297 1.087 1.351   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.007 0.011 0.028 0.008   \n",
       "target_polynomials_VS_per_network_polynomials      0.055 0.020 0.015 0.079   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.001 0.003 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.039 0.054 0.479 0.054   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.366 0.295 1.088 1.352   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.007 0.010 0.027 0.008   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.055 0.019 0.013 0.078   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.039 0.054 0.480 0.055   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    1.366 0.297 1.087 1.351   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.007 0.011 0.028 0.008   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.055 0.020 0.015 0.079   \n",
       "inet_polynomials_VS_metamodel_functions            1.371 0.292 0.616 1.398   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.044 0.053 0.455 0.056   \n",
       "inet_polynomials_VS_per_network_polynomials        0.020 0.037 0.475 0.035   \n",
       "metamodel_functions_VS_symbolic_regression_func... 1.366 0.287 1.069 1.344   \n",
       "metamodel_functions_VS_per_network_polynomials     1.385 0.296 1.084 1.375   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.061 0.017 0.025 0.077   \n",
       "\n",
       "                                                    L-20  L-21  L-22  L-23  \\\n",
       "lambda_preds_VS_target_polynomials                 0.024 0.006 0.010 0.062   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.002 0.010 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.024 0.006 0.010 0.062   \n",
       "lambda_preds_VS_inet_polynomials                   0.042 0.079 0.585 0.003   \n",
       "lambda_preds_VS_metamodel_functions                0.599 0.529 2.095 1.007   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.026 0.027 0.015 0.015   \n",
       "lambda_preds_VS_per_network_polynomials            0.000 0.045 0.088 0.003   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.024 0.006 0.003 0.062   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.062 0.084 0.587 0.066   \n",
       "target_polynomials_VS_metamodel_functions          0.589 0.531 2.097 1.032   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.048 0.026 0.011 0.063   \n",
       "target_polynomials_VS_per_network_polynomials      0.024 0.050 0.091 0.066   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.024 0.006 0.003 0.062   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.042 0.079 0.586 0.003   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.599 0.529 2.095 1.007   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.026 0.026 0.013 0.015   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.000 0.044 0.091 0.003   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.062 0.084 0.587 0.066   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.589 0.531 2.097 1.032   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.048 0.026 0.011 0.063   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.024 0.050 0.091 0.066   \n",
       "inet_polynomials_VS_metamodel_functions            0.602 0.518 1.606 1.006   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.019 0.071 0.596 0.014   \n",
       "inet_polynomials_VS_per_network_polynomials        0.042 0.041 0.568 0.001   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.617 0.552 2.105 1.011   \n",
       "metamodel_functions_VS_per_network_polynomials     0.599 0.517 2.114 1.006   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.026 0.049 0.100 0.013   \n",
       "\n",
       "                                                    L-24  L-25  L-26  L-27  \\\n",
       "lambda_preds_VS_target_polynomials                 0.103 0.100 0.003 0.018   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.002 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.103 0.100 0.003 0.018   \n",
       "lambda_preds_VS_inet_polynomials                   0.065 0.077 0.345 0.005   \n",
       "lambda_preds_VS_metamodel_functions                1.158 1.174   NaN 1.298   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.007 0.011 0.018 0.023   \n",
       "lambda_preds_VS_per_network_polynomials            0.064 0.076 0.056 0.009   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.103 0.100 0.004 0.018   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.059 0.052 0.346 0.013   \n",
       "target_polynomials_VS_metamodel_functions          1.195 1.204   NaN 1.305   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.104 0.100 0.017 0.023   \n",
       "target_polynomials_VS_per_network_polynomials      0.058 0.055 0.056 0.012   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.103 0.100 0.004 0.018   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.065 0.077 0.345 0.005   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.158 1.174   NaN 1.298   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.007 0.011 0.019 0.023   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.064 0.076 0.057 0.009   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.059 0.052 0.346 0.013   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    1.195 1.204   NaN 1.305   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.104 0.100 0.017 0.023   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.058 0.055 0.056 0.012   \n",
       "inet_polynomials_VS_metamodel_functions            1.164 1.186   NaN 1.300   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.068 0.082 0.335 0.021   \n",
       "inet_polynomials_VS_per_network_polynomials        0.012 0.015 0.347 0.003   \n",
       "metamodel_functions_VS_symbolic_regression_func... 1.155 1.176   NaN 1.305   \n",
       "metamodel_functions_VS_per_network_polynomials     1.177 1.201   NaN 1.301   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.067 0.081 0.039 0.020   \n",
       "\n",
       "                                                    L-28  L-29  L-30  L-31  \\\n",
       "lambda_preds_VS_target_polynomials                 0.008 0.010 0.002 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.008 0.000 0.002 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.008 0.010 0.002 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.037 0.030 0.059 0.055   \n",
       "lambda_preds_VS_metamodel_functions                0.322 1.043 0.604   NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.014 0.005 0.018 0.010   \n",
       "lambda_preds_VS_per_network_polynomials            0.012 0.030 0.017 0.040   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.003 0.010 0.001 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.037 0.024 0.059 0.056   \n",
       "target_polynomials_VS_metamodel_functions          0.323 1.041 0.603   NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.011 0.010 0.018 0.009   \n",
       "target_polynomials_VS_per_network_polynomials      0.011 0.023 0.017 0.041   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.003 0.010 0.001 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.036 0.030 0.059 0.055   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.322 1.043 0.604   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.011 0.005 0.018 0.010   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.012 0.030 0.017 0.040   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.037 0.024 0.059 0.056   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.323 1.041 0.603   NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.011 0.010 0.018 0.009   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.011 0.023 0.017 0.041   \n",
       "inet_polynomials_VS_metamodel_functions            0.338 1.031 0.619   NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.046 0.030 0.076 0.062   \n",
       "inet_polynomials_VS_per_network_polynomials        0.047 0.001 0.074 0.021   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.316 1.048 0.591   NaN   \n",
       "metamodel_functions_VS_per_network_polynomials     0.326 1.031 0.593   NaN   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.013 0.030 0.008 0.045   \n",
       "\n",
       "                                                    L-32  L-33  L-34  L-35  \\\n",
       "lambda_preds_VS_target_polynomials                 0.109 0.066 0.004 0.003   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.004 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.109 0.066 0.004 0.003   \n",
       "lambda_preds_VS_inet_polynomials                   0.065 0.016 0.055 0.077   \n",
       "lambda_preds_VS_metamodel_functions                0.170 1.251   NaN 0.197   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.004 0.012 0.024 0.013   \n",
       "lambda_preds_VS_per_network_polynomials            0.039 0.027 0.024 0.046   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.109 0.066 0.001 0.003   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.070 0.051 0.055 0.080   \n",
       "target_polynomials_VS_metamodel_functions          0.182 1.272   NaN 0.197   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.109 0.066 0.024 0.014   \n",
       "target_polynomials_VS_per_network_polynomials      0.074 0.044 0.023 0.049   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.109 0.066 0.001 0.003   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.065 0.016 0.055 0.077   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.170 1.251   NaN 0.197   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.004 0.012 0.024 0.013   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.039 0.027 0.024 0.046   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.070 0.051 0.055 0.080   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.182 1.272   NaN 0.197   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.109 0.066 0.024 0.014   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.074 0.044 0.023 0.049   \n",
       "inet_polynomials_VS_metamodel_functions            0.123 1.258   NaN 0.146   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.065 0.016 0.079 0.076   \n",
       "inet_polynomials_VS_per_network_polynomials        0.028 0.011 0.033 0.034   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.168 1.255   NaN 0.196   \n",
       "metamodel_functions_VS_per_network_polynomials     0.147 1.260   NaN 0.173   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.039 0.024 0.047 0.046   \n",
       "\n",
       "                                                    L-36  L-37  L-38  L-39  \\\n",
       "lambda_preds_VS_target_polynomials                 0.002 0.070 0.003 0.009   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.002 0.000 0.003 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.002 0.070 0.003 0.009   \n",
       "lambda_preds_VS_inet_polynomials                   0.067 0.049 0.032 0.039   \n",
       "lambda_preds_VS_metamodel_functions                1.114 1.555 0.222 0.266   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.012 0.003 0.013 0.006   \n",
       "lambda_preds_VS_per_network_polynomials            0.039 0.079 0.013 0.023   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.070 0.001 0.009   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.068 0.034 0.032 0.048   \n",
       "target_polynomials_VS_metamodel_functions          1.113 1.581 0.222 0.265   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.012 0.070 0.013 0.011   \n",
       "target_polynomials_VS_per_network_polynomials      0.040 0.048 0.014 0.032   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.070 0.001 0.009   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.067 0.049 0.032 0.039   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.113 1.555 0.223 0.266   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.012 0.003 0.013 0.007   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.040 0.079 0.014 0.023   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.068 0.034 0.032 0.048   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    1.113 1.581 0.222 0.265   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.012 0.070 0.013 0.011   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.040 0.048 0.014 0.032   \n",
       "inet_polynomials_VS_metamodel_functions            1.141 1.577 0.222 0.267   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.060 0.048 0.044 0.042   \n",
       "inet_polynomials_VS_per_network_polynomials        0.029 0.032 0.019 0.017   \n",
       "metamodel_functions_VS_symbolic_regression_func... 1.116 1.557 0.224 0.270   \n",
       "metamodel_functions_VS_per_network_polynomials     1.134 1.583 0.217 0.274   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.031 0.079 0.026 0.025   \n",
       "\n",
       "                                                    L-40  L-41  L-42  L-43  \\\n",
       "lambda_preds_VS_target_polynomials                 0.003 0.002 0.082 0.041   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.003 0.002 0.000 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.003 0.002 0.082 0.041   \n",
       "lambda_preds_VS_inet_polynomials                   0.030 0.156 0.047 0.032   \n",
       "lambda_preds_VS_metamodel_functions                0.271 0.581 0.925 0.082   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.013 0.014 0.002 0.005   \n",
       "lambda_preds_VS_per_network_polynomials            0.014 0.015 0.046 0.020   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.001 0.082 0.041   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.030 0.156 0.048 0.021   \n",
       "target_polynomials_VS_metamodel_functions          0.272 0.581 0.943 0.083   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.012 0.014 0.081 0.041   \n",
       "target_polynomials_VS_per_network_polynomials      0.014 0.015 0.050 0.023   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.001 0.082 0.041   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.030 0.156 0.047 0.032   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.273 0.581 0.925 0.082   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.012 0.014 0.002 0.005   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.014 0.015 0.046 0.020   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.030 0.156 0.048 0.021   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.272 0.581 0.943 0.083   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.012 0.014 0.081 0.041   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.014 0.015 0.050 0.023   \n",
       "inet_polynomials_VS_metamodel_functions            0.272 0.704 0.929 0.066   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.038 0.161 0.047 0.031   \n",
       "inet_polynomials_VS_per_network_polynomials        0.021 0.164 0.009 0.013   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.274 0.573 0.926 0.078   \n",
       "metamodel_functions_VS_per_network_polynomials     0.267 0.572 0.937 0.075   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.017 0.006 0.047 0.019   \n",
       "\n",
       "                                                    L-44  L-45  L-46  L-47  \\\n",
       "lambda_preds_VS_target_polynomials                 0.097 0.001 0.002 0.023   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.001 0.002 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.097 0.001 0.002 0.023   \n",
       "lambda_preds_VS_inet_polynomials                   0.074 0.013 0.024 0.024   \n",
       "lambda_preds_VS_metamodel_functions                1.715 0.175 0.953 0.160   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.008 0.011 0.018 0.001   \n",
       "lambda_preds_VS_per_network_polynomials            0.114 0.002 0.021 0.015   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.097 0.000 0.000 0.023   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.050 0.013 0.024 0.023   \n",
       "target_polynomials_VS_metamodel_functions          1.744 0.175 0.953 0.163   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.097 0.011 0.017 0.023   \n",
       "target_polynomials_VS_per_network_polynomials      0.071 0.001 0.021 0.018   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.097 0.000 0.000 0.023   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.074 0.013 0.024 0.024   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.715 0.175 0.953 0.160   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.008 0.011 0.017 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.114 0.001 0.021 0.015   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.050 0.013 0.024 0.023   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    1.744 0.175 0.953 0.163   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.097 0.011 0.017 0.023   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.071 0.001 0.021 0.018   \n",
       "inet_polynomials_VS_metamodel_functions            1.770 0.166 0.939 0.153   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.077 0.023 0.041 0.024   \n",
       "inet_polynomials_VS_per_network_polynomials        0.057 0.012 0.014 0.010   \n",
       "metamodel_functions_VS_symbolic_regression_func... 1.708 0.175 0.959 0.160   \n",
       "metamodel_functions_VS_per_network_polynomials     1.754 0.176 0.945 0.155   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.116 0.012 0.038 0.014   \n",
       "\n",
       "                                                    L-48  L-49  L-50  L-51  \\\n",
       "lambda_preds_VS_target_polynomials                 0.002 0.002 0.002 0.002   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.002 0.002 0.002 0.002   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.002 0.002 0.002 0.002   \n",
       "lambda_preds_VS_inet_polynomials                   0.057 0.113 0.098 0.050   \n",
       "lambda_preds_VS_metamodel_functions                0.471 0.879   NaN 0.527   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.015 0.006 0.014 0.020   \n",
       "lambda_preds_VS_per_network_polynomials            0.057 0.044 0.051 0.011   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.001 0.001 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.057 0.113 0.097 0.050   \n",
       "target_polynomials_VS_metamodel_functions          0.471 0.877   NaN 0.526   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.015 0.006 0.014 0.020   \n",
       "target_polynomials_VS_per_network_polynomials      0.057 0.045 0.052 0.011   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.001 0.001 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.057 0.113 0.097 0.050   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.471 0.878   NaN 0.527   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.015 0.005 0.013 0.019   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.057 0.045 0.052 0.010   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.057 0.113 0.097 0.050   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.471 0.877   NaN 0.526   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.015 0.006 0.014 0.020   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.057 0.045 0.052 0.011   \n",
       "inet_polynomials_VS_metamodel_functions            0.464 0.948   NaN 0.544   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.054 0.116 0.085 0.049   \n",
       "inet_polynomials_VS_per_network_polynomials        0.007 0.090 0.052 0.041   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.462 0.881   NaN 0.536   \n",
       "metamodel_functions_VS_per_network_polynomials     0.461 0.858   NaN 0.534   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.055 0.050 0.042 0.025   \n",
       "\n",
       "                                                    L-52  L-53  L-54  L-55  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.002 0.004 0.010   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.002 0.004 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.002 0.004 0.010   \n",
       "lambda_preds_VS_inet_polynomials                   0.590 0.099 0.077 0.009   \n",
       "lambda_preds_VS_metamodel_functions                1.341 0.642 0.454 0.150   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.009 0.005 0.015 0.004   \n",
       "lambda_preds_VS_per_network_polynomials            0.016 0.043 0.064 0.005   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.000 0.001 0.010   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.590 0.099 0.078 0.013   \n",
       "target_polynomials_VS_metamodel_functions          1.341 0.643 0.455 0.147   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.009 0.005 0.016 0.011   \n",
       "target_polynomials_VS_per_network_polynomials      0.016 0.043 0.066 0.009   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.000 0.001 0.010   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.590 0.099 0.076 0.009   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.341 0.642 0.454 0.150   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.009 0.005 0.015 0.004   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.016 0.043 0.066 0.005   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.590 0.099 0.078 0.013   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    1.341 0.643 0.455 0.147   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.009 0.005 0.016 0.011   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.016 0.043 0.066 0.009   \n",
       "inet_polynomials_VS_metamodel_functions            0.848 0.737 0.459 0.142   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.600 0.097 0.066 0.012   \n",
       "inet_polynomials_VS_per_network_polynomials        0.586 0.103 0.079 0.006   \n",
       "metamodel_functions_VS_symbolic_regression_func... 1.347 0.645 0.460 0.153   \n",
       "metamodel_functions_VS_per_network_polynomials     1.344 0.634 0.393 0.148   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.019 0.044 0.073 0.006   \n",
       "\n",
       "                                                    L-56  L-57  L-58  L-59  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.002 0.001 0.053   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.002 0.001 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.002 0.001 0.053   \n",
       "lambda_preds_VS_inet_polynomials                   0.018 0.533 0.044 0.070   \n",
       "lambda_preds_VS_metamodel_functions                0.177 1.193   NaN 1.601   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.017 0.025 0.025 0.011   \n",
       "lambda_preds_VS_per_network_polynomials            0.016 0.008 0.035 0.106   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.001 0.000 0.053   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.019 0.533 0.043 0.044   \n",
       "target_polynomials_VS_metamodel_functions          0.177 1.193   NaN 1.617   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.018 0.025 0.025 0.054   \n",
       "target_polynomials_VS_per_network_polynomials      0.016 0.007 0.035 0.067   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.001 0.000 0.053   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.018 0.534 0.044 0.070   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.177 1.193   NaN 1.601   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.017 0.025 0.025 0.011   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.016 0.007 0.035 0.106   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.019 0.533 0.043 0.044   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.177 1.193   NaN 1.617   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.018 0.025 0.025 0.054   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.016 0.007 0.035 0.067   \n",
       "inet_polynomials_VS_metamodel_functions            0.177 0.666   NaN 1.658   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.017 0.529 0.028 0.069   \n",
       "inet_polynomials_VS_per_network_polynomials        0.012 0.530 0.027 0.048   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.193 1.191   NaN 1.602   \n",
       "metamodel_functions_VS_per_network_polynomials     0.186 1.192   NaN 1.638   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.006 0.021 0.013 0.103   \n",
       "\n",
       "                                                    L-60  L-61  L-62  L-63  \\\n",
       "lambda_preds_VS_target_polynomials                 0.075 0.002 0.001 0.016   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.002 0.001 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.075 0.002 0.001 0.016   \n",
       "lambda_preds_VS_inet_polynomials                   0.020 0.074 0.188 0.059   \n",
       "lambda_preds_VS_metamodel_functions                0.839 0.443 0.299   NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.001 0.013 0.012 0.004   \n",
       "lambda_preds_VS_per_network_polynomials            0.020 0.013 0.048 0.036   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.075 0.000 0.000 0.016   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.058 0.074 0.188 0.047   \n",
       "target_polynomials_VS_metamodel_functions          0.861 0.443 0.299   NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.075 0.013 0.012 0.016   \n",
       "target_polynomials_VS_per_network_polynomials      0.059 0.013 0.048 0.025   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.075 0.000 0.000 0.016   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.020 0.074 0.188 0.059   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.839 0.443 0.299   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.001 0.013 0.012 0.004   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.020 0.013 0.048 0.036   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.058 0.074 0.188 0.047   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.861 0.443 0.299   NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.075 0.013 0.012 0.016   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.059 0.013 0.048 0.025   \n",
       "inet_polynomials_VS_metamodel_functions            0.843 0.430 0.472   NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.020 0.075 0.191 0.059   \n",
       "inet_polynomials_VS_per_network_polynomials        0.003 0.075 0.200 0.025   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.839 0.441 0.295   NaN   \n",
       "metamodel_functions_VS_per_network_polynomials     0.845 0.440 0.278   NaN   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.020 0.002 0.039 0.036   \n",
       "\n",
       "                                                    L-64  L-65  L-66  L-67  \\\n",
       "lambda_preds_VS_target_polynomials                 0.022 0.060 0.009 0.016   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.022 0.000 0.002 0.016   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.022 0.060 0.009 0.016   \n",
       "lambda_preds_VS_inet_polynomials                   0.070 0.023 0.035 0.541   \n",
       "lambda_preds_VS_metamodel_functions                  NaN 0.072 1.072 2.270   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.026 0.005 0.005 0.021   \n",
       "lambda_preds_VS_per_network_polynomials            0.026 0.021 0.034 0.024   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.008 0.060 0.008 0.006   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.069 0.081 0.028 0.543   \n",
       "target_polynomials_VS_metamodel_functions            NaN 0.086 1.069 2.277   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.017 0.060 0.011 0.017   \n",
       "target_polynomials_VS_per_network_polynomials      0.014 0.079 0.027 0.019   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.008 0.060 0.008 0.006   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.072 0.023 0.035 0.540   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN 0.072 1.072 2.271   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.013 0.005 0.005 0.018   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.018 0.021 0.034 0.021   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.069 0.081 0.028 0.543   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      NaN 0.086 1.069 2.277   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.017 0.060 0.011 0.017   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.014 0.079 0.027 0.019   \n",
       "inet_polynomials_VS_metamodel_functions              NaN 0.082 1.051 1.764   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.085 0.023 0.038 0.556   \n",
       "inet_polynomials_VS_per_network_polynomials        0.070 0.014 0.010 0.551   \n",
       "metamodel_functions_VS_symbolic_regression_func...   NaN 0.067 1.078 2.282   \n",
       "metamodel_functions_VS_per_network_polynomials       NaN 0.071 1.061 2.282   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.023 0.020 0.037 0.031   \n",
       "\n",
       "                                                    L-68  L-69  L-70  L-71  \\\n",
       "lambda_preds_VS_target_polynomials                 0.043 0.006 0.002 0.002   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.005 0.002 0.002   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.043 0.006 0.002 0.002   \n",
       "lambda_preds_VS_inet_polynomials                   0.076 0.043 0.644 0.028   \n",
       "lambda_preds_VS_metamodel_functions                1.001   NaN 1.506 0.621   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.005 0.016 0.014 0.004   \n",
       "lambda_preds_VS_per_network_polynomials            0.024 0.014 0.074 0.023   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.043 0.001 0.001 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.056 0.044 0.644 0.028   \n",
       "target_polynomials_VS_metamodel_functions          1.019   NaN 1.505 0.621   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.043 0.015 0.015 0.004   \n",
       "target_polynomials_VS_per_network_polynomials      0.060 0.013 0.075 0.024   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.043 0.001 0.001 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.076 0.044 0.644 0.028   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.001   NaN 1.506 0.621   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.005 0.016 0.014 0.004   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.024 0.015 0.075 0.024   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.056 0.044 0.644 0.028   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    1.019   NaN 1.505 0.621   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.043 0.015 0.015 0.004   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.060 0.013 0.075 0.024   \n",
       "inet_polynomials_VS_metamodel_functions            1.007   NaN 0.987 0.622   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.075 0.044 0.635 0.025   \n",
       "inet_polynomials_VS_per_network_polynomials        0.097 0.044 0.646 0.051   \n",
       "metamodel_functions_VS_symbolic_regression_func... 1.005   NaN 1.508 0.624   \n",
       "metamodel_functions_VS_per_network_polynomials     0.982   NaN 1.529 0.610   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.028 0.005 0.069 0.027   \n",
       "\n",
       "                                                    L-72  L-73  L-74  L-75  \\\n",
       "lambda_preds_VS_target_polynomials                 0.014 0.040 0.001 0.030   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.001 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.014 0.040 0.001 0.030   \n",
       "lambda_preds_VS_inet_polynomials                   0.041 0.055 0.019 0.083   \n",
       "lambda_preds_VS_metamodel_functions                1.644 1.706 0.413 1.424   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.006 0.012 0.007 0.005   \n",
       "lambda_preds_VS_per_network_polynomials            0.067 0.091 0.015 0.079   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.014 0.040 0.001 0.030   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.028 0.027 0.019 0.066   \n",
       "target_polynomials_VS_metamodel_functions          1.647 1.718 0.414 1.430   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.015 0.041 0.007 0.030   \n",
       "target_polynomials_VS_per_network_polynomials      0.055 0.060 0.016 0.056   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.014 0.040 0.001 0.030   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.041 0.055 0.019 0.083   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.644 1.706 0.413 1.424   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.006 0.012 0.007 0.005   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.067 0.091 0.015 0.079   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.028 0.027 0.019 0.066   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    1.647 1.718 0.414 1.430   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.015 0.041 0.007 0.030   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.055 0.060 0.016 0.056   \n",
       "inet_polynomials_VS_metamodel_functions            1.655 1.725 0.414 1.412   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.043 0.058 0.023 0.086   \n",
       "inet_polynomials_VS_per_network_polynomials        0.028 0.038 0.034 0.037   \n",
       "metamodel_functions_VS_symbolic_regression_func... 1.644 1.702 0.419 1.424   \n",
       "metamodel_functions_VS_per_network_polynomials     1.666 1.734 0.409 1.448   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.069 0.095 0.016 0.080   \n",
       "\n",
       "                                                    L-76  L-77  L-78  L-79  \\\n",
       "lambda_preds_VS_target_polynomials                 0.002 0.002 0.004 0.073   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.002 0.002 0.004 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.002 0.002 0.004 0.073   \n",
       "lambda_preds_VS_inet_polynomials                   0.029 0.010 0.061 0.005   \n",
       "lambda_preds_VS_metamodel_functions                0.894 1.358   NaN 1.091   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.008 0.007 0.014 0.008   \n",
       "lambda_preds_VS_per_network_polynomials            0.043 0.007 0.015 0.008   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.001 0.002 0.073   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.028 0.012 0.063 0.069   \n",
       "target_polynomials_VS_metamodel_functions          0.894 1.359   NaN 1.115   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.008 0.008 0.014 0.073   \n",
       "target_polynomials_VS_per_network_polynomials      0.042 0.008 0.015 0.066   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.001 0.002 0.073   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.029 0.011 0.063 0.005   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.894 1.358   NaN 1.091   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.008 0.007 0.015 0.008   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.043 0.007 0.016 0.008   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.028 0.012 0.063 0.069   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.894 1.359   NaN 1.115   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.008 0.008 0.014 0.073   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.042 0.008 0.015 0.066   \n",
       "inet_polynomials_VS_metamodel_functions            0.867 1.349   NaN 1.093   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.027 0.010 0.069 0.007   \n",
       "inet_polynomials_VS_per_network_polynomials        0.018 0.009 0.055 0.003   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.894 1.359   NaN 1.092   \n",
       "metamodel_functions_VS_per_network_polynomials     0.868 1.358   NaN 1.093   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.037 0.001 0.013 0.007   \n",
       "\n",
       "                                                    L-80  L-81  L-82  L-83  \\\n",
       "lambda_preds_VS_target_polynomials                 0.008 0.001 0.001 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.008 0.001 0.001 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.008 0.001 0.001 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.456 0.026 0.039 0.044   \n",
       "lambda_preds_VS_metamodel_functions                1.822 0.686 1.130 0.308   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.009 0.010 0.023 0.020   \n",
       "lambda_preds_VS_per_network_polynomials            0.080 0.034 0.048 0.020   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.002 0.001 0.001 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.459 0.026 0.039 0.044   \n",
       "target_polynomials_VS_metamodel_functions          1.823 0.687 1.130 0.308   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.006 0.010 0.023 0.021   \n",
       "target_polynomials_VS_per_network_polynomials      0.081 0.033 0.048 0.020   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.002 0.001 0.001 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.458 0.026 0.039 0.044   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.822 0.687 1.130 0.308   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.005 0.010 0.023 0.020   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.081 0.034 0.048 0.020   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.459 0.026 0.039 0.044   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    1.823 0.687 1.130 0.308   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.006 0.010 0.023 0.021   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.081 0.033 0.048 0.020   \n",
       "inet_polynomials_VS_metamodel_functions            1.462 0.661 1.140 0.301   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.460 0.023 0.022 0.061   \n",
       "inet_polynomials_VS_per_network_polynomials        0.453 0.010 0.022 0.062   \n",
       "metamodel_functions_VS_symbolic_regression_func... 1.824 0.682 1.120 0.317   \n",
       "metamodel_functions_VS_per_network_polynomials     1.848 0.656 1.151 0.313   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.082 0.033 0.037 0.007   \n",
       "\n",
       "                                                    L-84  L-85  L-86  L-87  \\\n",
       "lambda_preds_VS_target_polynomials                 0.030 0.001 0.060 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.001 0.000 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.030 0.001 0.060 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.038 0.037 0.008 0.047   \n",
       "lambda_preds_VS_metamodel_functions                0.573 0.925 1.378 1.316   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.002 0.012 0.008 0.008   \n",
       "lambda_preds_VS_per_network_polynomials            0.023 0.016 0.013 0.069   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.030 0.001 0.060 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.065 0.037 0.053 0.047   \n",
       "target_polynomials_VS_metamodel_functions          0.581 0.925 1.392 1.316   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.030 0.012 0.061 0.008   \n",
       "target_polynomials_VS_per_network_polynomials      0.053 0.016 0.049 0.070   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.030 0.001 0.060 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.038 0.037 0.008 0.047   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.573 0.925 1.378 1.316   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.002 0.013 0.008 0.008   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.023 0.016 0.013 0.069   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.065 0.037 0.053 0.047   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.581 0.925 1.392 1.316   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.030 0.012 0.061 0.008   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.053 0.016 0.049 0.070   \n",
       "inet_polynomials_VS_metamodel_functions            0.567 0.928 1.382 1.348   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.038 0.042 0.012 0.047   \n",
       "inet_polynomials_VS_per_network_polynomials        0.018 0.027 0.006 0.026   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.574 0.929 1.380 1.316   \n",
       "metamodel_functions_VS_per_network_polynomials     0.566 0.923 1.382 1.341   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.023 0.026 0.017 0.068   \n",
       "\n",
       "                                                    L-88  L-89  L-90  L-91  \\\n",
       "lambda_preds_VS_target_polynomials                 0.000 0.001 0.008 0.015   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.001 0.007 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.000 0.001 0.008 0.015   \n",
       "lambda_preds_VS_inet_polynomials                   0.056 0.091 0.083 0.028   \n",
       "lambda_preds_VS_metamodel_functions                1.212 0.360 0.527 0.216   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.011 0.006 0.037 0.002   \n",
       "lambda_preds_VS_per_network_polynomials            0.079 0.024 0.038 0.017   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.000 0.003 0.015   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.055 0.090 0.082 0.016   \n",
       "target_polynomials_VS_metamodel_functions          1.212 0.360 0.530 0.220   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.011 0.006 0.038 0.015   \n",
       "target_polynomials_VS_per_network_polynomials      0.079 0.024 0.037 0.006   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.000 0.003 0.015   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.055 0.091 0.083 0.028   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.212 0.360 0.527 0.216   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.011 0.006 0.036 0.002   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.079 0.024 0.039 0.017   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.055 0.090 0.082 0.016   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    1.212 0.360 0.530 0.220   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.011 0.006 0.038 0.015   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.079 0.024 0.037 0.006   \n",
       "inet_polynomials_VS_metamodel_functions            1.255 0.350 0.554 0.228   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.066 0.088 0.106 0.027   \n",
       "inet_polynomials_VS_per_network_polynomials        0.036 0.069 0.047 0.012   \n",
       "metamodel_functions_VS_symbolic_regression_func... 1.201 0.359 0.509 0.214   \n",
       "metamodel_functions_VS_per_network_polynomials     1.237 0.339 0.554 0.222   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.089 0.025 0.070 0.017   \n",
       "\n",
       "                                                    L-92  L-93  L-94  L-95  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.001 0.001 0.027   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.001 0.001 0.026   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.001 0.001 0.027   \n",
       "lambda_preds_VS_inet_polynomials                   0.262 0.031 0.093 0.234   \n",
       "lambda_preds_VS_metamodel_functions                0.163 0.230 0.407 1.260   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.007 0.032 0.005 0.027   \n",
       "lambda_preds_VS_per_network_polynomials            0.039 0.025 0.020 0.037   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.001 0.000 0.010   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.262 0.031 0.093 0.244   \n",
       "target_polynomials_VS_metamodel_functions          0.163 0.230 0.408 1.270   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.007 0.033 0.005 0.023   \n",
       "target_polynomials_VS_per_network_polynomials      0.039 0.024 0.020 0.025   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.001 0.000 0.010   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.262 0.032 0.093 0.235   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.163 0.230 0.407 1.261   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.007 0.033 0.005 0.016   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.039 0.025 0.020 0.033   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.262 0.031 0.093 0.244   \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.163 0.230 0.408 1.270   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.007 0.033 0.005 0.023   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.039 0.024 0.020 0.025   \n",
       "inet_polynomials_VS_metamodel_functions            0.413 0.247 0.493 1.026   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.263 0.031 0.091 0.241   \n",
       "inet_polynomials_VS_per_network_polynomials        0.259 0.014 0.094 0.245   \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.162 0.257 0.411 1.267   \n",
       "metamodel_functions_VS_per_network_polynomials     0.154 0.246 0.402 1.271   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.033 0.018 0.017 0.048   \n",
       "\n",
       "                                                    L-96  L-97  L-98  L-99  \n",
       "lambda_preds_VS_target_polynomials                 0.025 0.069 0.002 0.002  \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.002 0.002  \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.025 0.069 0.002 0.002  \n",
       "lambda_preds_VS_inet_polynomials                   0.041 0.025 0.093 0.166  \n",
       "lambda_preds_VS_metamodel_functions                0.255 0.217 0.492 1.053  \n",
       "lambda_preds_VS_symbolic_regression_functions      0.009 0.004 0.011 0.011  \n",
       "lambda_preds_VS_per_network_polynomials            0.024 0.014 0.126 0.011  \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.025 0.069 0.001 0.001  \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000  \n",
       "target_polynomials_VS_inet_polynomials             0.063 0.090 0.093 0.166  \n",
       "target_polynomials_VS_metamodel_functions          0.249 0.189 0.491 1.053  \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.026 0.069 0.010 0.011  \n",
       "target_polynomials_VS_per_network_polynomials      0.049 0.082 0.126 0.011  \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.025 0.069 0.001 0.001  \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.041 0.025 0.093 0.166  \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.255 0.217 0.492 1.053  \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.009 0.004 0.010 0.011  \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.024 0.014 0.127 0.011  \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.063 0.090 0.093 0.166  \n",
       "lstsq_target_polynomials_VS_metamodel_functions    0.249 0.189 0.491 1.053  \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.026 0.069 0.010 0.011  \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.049 0.082 0.126 0.011  \n",
       "inet_polynomials_VS_metamodel_functions            0.257 0.214 0.487 1.212  \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.039 0.023 0.096 0.158  \n",
       "inet_polynomials_VS_per_network_polynomials        0.018 0.012 0.104 0.158  \n",
       "metamodel_functions_VS_symbolic_regression_func... 0.261 0.217 0.488 1.059  \n",
       "metamodel_functions_VS_per_network_polynomials     0.264 0.222 0.385 1.060  \n",
       "symbolic_regression_functions_VS_per_network_po... 0.023 0.013 0.131 0.003  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:36.725004Z",
     "iopub.status.busy": "2021-10-21T09:30:36.724760Z",
     "iopub.status.idle": "2021-10-21T09:30:36.913513Z",
     "shell.execute_reply": "2021-10-21T09:30:36.912760Z",
     "shell.execute_reply.started": "2021-10-21T09:30:36.724975Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L-0</th>\n",
       "      <th>L-1</th>\n",
       "      <th>L-2</th>\n",
       "      <th>L-3</th>\n",
       "      <th>L-4</th>\n",
       "      <th>L-5</th>\n",
       "      <th>L-6</th>\n",
       "      <th>L-7</th>\n",
       "      <th>L-8</th>\n",
       "      <th>L-9</th>\n",
       "      <th>L-10</th>\n",
       "      <th>L-11</th>\n",
       "      <th>L-12</th>\n",
       "      <th>L-13</th>\n",
       "      <th>L-14</th>\n",
       "      <th>L-15</th>\n",
       "      <th>L-16</th>\n",
       "      <th>L-17</th>\n",
       "      <th>L-18</th>\n",
       "      <th>L-19</th>\n",
       "      <th>L-20</th>\n",
       "      <th>L-21</th>\n",
       "      <th>L-22</th>\n",
       "      <th>L-23</th>\n",
       "      <th>L-24</th>\n",
       "      <th>L-25</th>\n",
       "      <th>L-26</th>\n",
       "      <th>L-27</th>\n",
       "      <th>L-28</th>\n",
       "      <th>L-29</th>\n",
       "      <th>L-30</th>\n",
       "      <th>L-31</th>\n",
       "      <th>L-32</th>\n",
       "      <th>L-33</th>\n",
       "      <th>L-34</th>\n",
       "      <th>L-35</th>\n",
       "      <th>L-36</th>\n",
       "      <th>L-37</th>\n",
       "      <th>L-38</th>\n",
       "      <th>L-39</th>\n",
       "      <th>L-40</th>\n",
       "      <th>L-41</th>\n",
       "      <th>L-42</th>\n",
       "      <th>L-43</th>\n",
       "      <th>L-44</th>\n",
       "      <th>L-45</th>\n",
       "      <th>L-46</th>\n",
       "      <th>L-47</th>\n",
       "      <th>L-48</th>\n",
       "      <th>L-49</th>\n",
       "      <th>L-50</th>\n",
       "      <th>L-51</th>\n",
       "      <th>L-52</th>\n",
       "      <th>L-53</th>\n",
       "      <th>L-54</th>\n",
       "      <th>L-55</th>\n",
       "      <th>L-56</th>\n",
       "      <th>L-57</th>\n",
       "      <th>L-58</th>\n",
       "      <th>L-59</th>\n",
       "      <th>L-60</th>\n",
       "      <th>L-61</th>\n",
       "      <th>L-62</th>\n",
       "      <th>L-63</th>\n",
       "      <th>L-64</th>\n",
       "      <th>L-65</th>\n",
       "      <th>L-66</th>\n",
       "      <th>L-67</th>\n",
       "      <th>L-68</th>\n",
       "      <th>L-69</th>\n",
       "      <th>L-70</th>\n",
       "      <th>L-71</th>\n",
       "      <th>L-72</th>\n",
       "      <th>L-73</th>\n",
       "      <th>L-74</th>\n",
       "      <th>L-75</th>\n",
       "      <th>L-76</th>\n",
       "      <th>L-77</th>\n",
       "      <th>L-78</th>\n",
       "      <th>L-79</th>\n",
       "      <th>L-80</th>\n",
       "      <th>L-81</th>\n",
       "      <th>L-82</th>\n",
       "      <th>L-83</th>\n",
       "      <th>L-84</th>\n",
       "      <th>L-85</th>\n",
       "      <th>L-86</th>\n",
       "      <th>L-87</th>\n",
       "      <th>L-88</th>\n",
       "      <th>L-89</th>\n",
       "      <th>L-90</th>\n",
       "      <th>L-91</th>\n",
       "      <th>L-92</th>\n",
       "      <th>L-93</th>\n",
       "      <th>L-94</th>\n",
       "      <th>L-95</th>\n",
       "      <th>L-96</th>\n",
       "      <th>L-97</th>\n",
       "      <th>L-98</th>\n",
       "      <th>L-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_target_polynomials</th>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-21.872</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.776</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.583</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.957</td>\n",
       "      <td>-1.382</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.997</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-14.178</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.855</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-3.099</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.929</td>\n",
       "      <td>-0.809</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-21.872</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.776</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.583</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.957</td>\n",
       "      <td>-1.382</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.997</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-14.178</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.855</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-3.099</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.929</td>\n",
       "      <td>-0.809</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_inet_polynomials</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.973</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.936</td>\n",
       "      <td>-10.984</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.839</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.831</td>\n",
       "      <td>-3.012</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.637</td>\n",
       "      <td>-21.961</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-19.066</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-4.414</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.961</td>\n",
       "      <td>-47.585</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.924</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.821</td>\n",
       "      <td>-10.757</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-2.649</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_metamodel_functions</th>\n",
       "      <td>-1.452</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-6.117</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-17.772</td>\n",
       "      <td>-2.120</td>\n",
       "      <td>-371.411</td>\n",
       "      <td>-15.307</td>\n",
       "      <td>-3.360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.232</td>\n",
       "      <td>-1.125</td>\n",
       "      <td>-15.140</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-94.017</td>\n",
       "      <td>-0.633</td>\n",
       "      <td>-96.920</td>\n",
       "      <td>-1.277</td>\n",
       "      <td>-49.091</td>\n",
       "      <td>-36.664</td>\n",
       "      <td>-23.775</td>\n",
       "      <td>-5.030</td>\n",
       "      <td>-15.195</td>\n",
       "      <td>-3749.886</td>\n",
       "      <td>-38.148</td>\n",
       "      <td>-27.995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2537.487</td>\n",
       "      <td>-1.140</td>\n",
       "      <td>-49.684</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-244.912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-34.832</td>\n",
       "      <td>-43.477</td>\n",
       "      <td>-2.889</td>\n",
       "      <td>-5.689</td>\n",
       "      <td>-3.328</td>\n",
       "      <td>-2.226</td>\n",
       "      <td>-43.776</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-26.505</td>\n",
       "      <td>-9.257</td>\n",
       "      <td>-26.583</td>\n",
       "      <td>-5.325</td>\n",
       "      <td>-1.528</td>\n",
       "      <td>-7.024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22.570</td>\n",
       "      <td>-94.644</td>\n",
       "      <td>-3.445</td>\n",
       "      <td>-5.342</td>\n",
       "      <td>-36.536</td>\n",
       "      <td>-1.721</td>\n",
       "      <td>-80.429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-25.370</td>\n",
       "      <td>-183.519</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-11.981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-38.154</td>\n",
       "      <td>-9.111</td>\n",
       "      <td>-19.370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-220.238</td>\n",
       "      <td>-2.671</td>\n",
       "      <td>-70.746</td>\n",
       "      <td>-38.121</td>\n",
       "      <td>-2.890</td>\n",
       "      <td>-36.782</td>\n",
       "      <td>-5.858</td>\n",
       "      <td>-516.157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2112.601</td>\n",
       "      <td>-16.361</td>\n",
       "      <td>-20.752</td>\n",
       "      <td>-117.307</td>\n",
       "      <td>-2.300</td>\n",
       "      <td>-25.963</td>\n",
       "      <td>-49.998</td>\n",
       "      <td>-1312.758</td>\n",
       "      <td>-48.593</td>\n",
       "      <td>-30.687</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-2.536</td>\n",
       "      <td>-7.087</td>\n",
       "      <td>-3.983</td>\n",
       "      <td>-21.009</td>\n",
       "      <td>-60.223</td>\n",
       "      <td>-11.184</td>\n",
       "      <td>-5.134</td>\n",
       "      <td>-9.867</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-4.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_symbolic_regression_functions</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.986</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.981</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_per_network_polynomials</th>\n",
       "      <td>0.932</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.743</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.871</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.531</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.884</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.892</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.859</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.782</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.379</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.970</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.976</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.070</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.312</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.098</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.856</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-10.595</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.818</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.925</td>\n",
       "      <td>-3.085</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.635</td>\n",
       "      <td>-21.724</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.959</td>\n",
       "      <td>-18.894</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.967</td>\n",
       "      <td>-4.396</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.968</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.963</td>\n",
       "      <td>-47.079</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.939</td>\n",
       "      <td>-10.971</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-2.623</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_metamodel_functions</th>\n",
       "      <td>-1.419</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-6.133</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-17.836</td>\n",
       "      <td>-2.116</td>\n",
       "      <td>-396.493</td>\n",
       "      <td>-15.350</td>\n",
       "      <td>-3.367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.242</td>\n",
       "      <td>-1.432</td>\n",
       "      <td>-15.091</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-95.113</td>\n",
       "      <td>-0.641</td>\n",
       "      <td>-96.788</td>\n",
       "      <td>-1.276</td>\n",
       "      <td>-47.285</td>\n",
       "      <td>-36.744</td>\n",
       "      <td>-30.381</td>\n",
       "      <td>-5.219</td>\n",
       "      <td>-15.157</td>\n",
       "      <td>-192.221</td>\n",
       "      <td>-26.475</td>\n",
       "      <td>-21.314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2431.926</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>-46.067</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-107.265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-34.654</td>\n",
       "      <td>-38.431</td>\n",
       "      <td>-2.827</td>\n",
       "      <td>-6.179</td>\n",
       "      <td>-3.224</td>\n",
       "      <td>-2.229</td>\n",
       "      <td>-24.697</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-22.719</td>\n",
       "      <td>-9.249</td>\n",
       "      <td>-26.542</td>\n",
       "      <td>-3.096</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>-6.996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22.306</td>\n",
       "      <td>-93.578</td>\n",
       "      <td>-3.455</td>\n",
       "      <td>-5.330</td>\n",
       "      <td>-18.660</td>\n",
       "      <td>-1.696</td>\n",
       "      <td>-79.743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-23.478</td>\n",
       "      <td>-50.383</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-11.980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-36.006</td>\n",
       "      <td>-9.102</td>\n",
       "      <td>-18.095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-217.897</td>\n",
       "      <td>-2.672</td>\n",
       "      <td>-73.388</td>\n",
       "      <td>-36.130</td>\n",
       "      <td>-2.897</td>\n",
       "      <td>-34.698</td>\n",
       "      <td>-5.842</td>\n",
       "      <td>-533.555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-135.321</td>\n",
       "      <td>-16.399</td>\n",
       "      <td>-20.494</td>\n",
       "      <td>-116.416</td>\n",
       "      <td>-2.282</td>\n",
       "      <td>-32.508</td>\n",
       "      <td>-50.223</td>\n",
       "      <td>-224.861</td>\n",
       "      <td>-48.494</td>\n",
       "      <td>-30.695</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-2.546</td>\n",
       "      <td>-8.747</td>\n",
       "      <td>-4.036</td>\n",
       "      <td>-20.875</td>\n",
       "      <td>-60.108</td>\n",
       "      <td>-11.154</td>\n",
       "      <td>-4.640</td>\n",
       "      <td>-3.600</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-4.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.956</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.961</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.998</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.744</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.755</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-21.872</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.869</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.776</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.588</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.957</td>\n",
       "      <td>-1.382</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-14.209</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.855</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-3.102</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.930</td>\n",
       "      <td>-0.809</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.936</td>\n",
       "      <td>-10.933</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.839</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.831</td>\n",
       "      <td>-2.998</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.636</td>\n",
       "      <td>-21.838</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-18.983</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-4.410</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.963</td>\n",
       "      <td>-47.228</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.925</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.821</td>\n",
       "      <td>-10.767</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-2.640</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_metamodel_functions</th>\n",
       "      <td>-1.452</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-6.111</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-17.714</td>\n",
       "      <td>-2.115</td>\n",
       "      <td>-371.626</td>\n",
       "      <td>-15.277</td>\n",
       "      <td>-3.358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.232</td>\n",
       "      <td>-1.125</td>\n",
       "      <td>-15.111</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-94.136</td>\n",
       "      <td>-0.634</td>\n",
       "      <td>-96.914</td>\n",
       "      <td>-1.274</td>\n",
       "      <td>-48.837</td>\n",
       "      <td>-36.679</td>\n",
       "      <td>-23.775</td>\n",
       "      <td>-5.038</td>\n",
       "      <td>-15.185</td>\n",
       "      <td>-3749.886</td>\n",
       "      <td>-38.148</td>\n",
       "      <td>-28.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2558.876</td>\n",
       "      <td>-1.139</td>\n",
       "      <td>-49.668</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-244.991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-34.719</td>\n",
       "      <td>-43.478</td>\n",
       "      <td>-2.891</td>\n",
       "      <td>-5.691</td>\n",
       "      <td>-3.324</td>\n",
       "      <td>-2.227</td>\n",
       "      <td>-43.779</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-26.505</td>\n",
       "      <td>-9.245</td>\n",
       "      <td>-26.601</td>\n",
       "      <td>-5.326</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>-7.012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22.547</td>\n",
       "      <td>-94.084</td>\n",
       "      <td>-3.444</td>\n",
       "      <td>-5.360</td>\n",
       "      <td>-36.692</td>\n",
       "      <td>-1.713</td>\n",
       "      <td>-80.072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-25.369</td>\n",
       "      <td>-183.496</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-11.997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-38.212</td>\n",
       "      <td>-9.091</td>\n",
       "      <td>-19.368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-218.461</td>\n",
       "      <td>-2.673</td>\n",
       "      <td>-70.759</td>\n",
       "      <td>-38.121</td>\n",
       "      <td>-2.886</td>\n",
       "      <td>-36.782</td>\n",
       "      <td>-5.856</td>\n",
       "      <td>-519.616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2118.040</td>\n",
       "      <td>-16.334</td>\n",
       "      <td>-20.660</td>\n",
       "      <td>-117.154</td>\n",
       "      <td>-2.291</td>\n",
       "      <td>-25.944</td>\n",
       "      <td>-50.026</td>\n",
       "      <td>-1314.247</td>\n",
       "      <td>-48.605</td>\n",
       "      <td>-30.699</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-2.525</td>\n",
       "      <td>-7.094</td>\n",
       "      <td>-3.987</td>\n",
       "      <td>-21.014</td>\n",
       "      <td>-60.108</td>\n",
       "      <td>-11.078</td>\n",
       "      <td>-5.132</td>\n",
       "      <td>-9.867</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-4.843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.986</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.957</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.840</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.981</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>0.932</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.856</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-10.595</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.818</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.925</td>\n",
       "      <td>-3.085</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.635</td>\n",
       "      <td>-21.724</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.959</td>\n",
       "      <td>-18.894</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.967</td>\n",
       "      <td>-4.396</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.968</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.963</td>\n",
       "      <td>-47.079</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.939</td>\n",
       "      <td>-10.971</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-2.623</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_metamodel_functions</th>\n",
       "      <td>-1.419</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-6.133</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-17.836</td>\n",
       "      <td>-2.116</td>\n",
       "      <td>-396.493</td>\n",
       "      <td>-15.350</td>\n",
       "      <td>-3.367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.242</td>\n",
       "      <td>-1.432</td>\n",
       "      <td>-15.091</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-95.113</td>\n",
       "      <td>-0.641</td>\n",
       "      <td>-96.788</td>\n",
       "      <td>-1.276</td>\n",
       "      <td>-47.285</td>\n",
       "      <td>-36.744</td>\n",
       "      <td>-30.381</td>\n",
       "      <td>-5.219</td>\n",
       "      <td>-15.157</td>\n",
       "      <td>-192.221</td>\n",
       "      <td>-26.475</td>\n",
       "      <td>-21.314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2431.926</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>-46.067</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-107.265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-34.654</td>\n",
       "      <td>-38.431</td>\n",
       "      <td>-2.827</td>\n",
       "      <td>-6.179</td>\n",
       "      <td>-3.224</td>\n",
       "      <td>-2.229</td>\n",
       "      <td>-24.697</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-22.719</td>\n",
       "      <td>-9.249</td>\n",
       "      <td>-26.542</td>\n",
       "      <td>-3.096</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>-6.996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22.306</td>\n",
       "      <td>-93.578</td>\n",
       "      <td>-3.455</td>\n",
       "      <td>-5.330</td>\n",
       "      <td>-18.660</td>\n",
       "      <td>-1.696</td>\n",
       "      <td>-79.743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-23.478</td>\n",
       "      <td>-50.383</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-11.980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-36.006</td>\n",
       "      <td>-9.102</td>\n",
       "      <td>-18.095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-217.897</td>\n",
       "      <td>-2.672</td>\n",
       "      <td>-73.388</td>\n",
       "      <td>-36.130</td>\n",
       "      <td>-2.897</td>\n",
       "      <td>-34.698</td>\n",
       "      <td>-5.842</td>\n",
       "      <td>-533.555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-135.321</td>\n",
       "      <td>-16.399</td>\n",
       "      <td>-20.494</td>\n",
       "      <td>-116.416</td>\n",
       "      <td>-2.282</td>\n",
       "      <td>-32.508</td>\n",
       "      <td>-50.223</td>\n",
       "      <td>-224.861</td>\n",
       "      <td>-48.494</td>\n",
       "      <td>-30.695</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-2.546</td>\n",
       "      <td>-8.747</td>\n",
       "      <td>-4.036</td>\n",
       "      <td>-20.875</td>\n",
       "      <td>-60.108</td>\n",
       "      <td>-11.154</td>\n",
       "      <td>-4.640</td>\n",
       "      <td>-3.600</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-4.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.956</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.961</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.998</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.744</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_metamodel_functions</th>\n",
       "      <td>-1.929</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-5.547</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-2.945</td>\n",
       "      <td>-3.839</td>\n",
       "      <td>-434.146</td>\n",
       "      <td>-8.933</td>\n",
       "      <td>-4.192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.681</td>\n",
       "      <td>-1.269</td>\n",
       "      <td>-3.613</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-123.817</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>-133.069</td>\n",
       "      <td>-1.328</td>\n",
       "      <td>-35.670</td>\n",
       "      <td>-51.681</td>\n",
       "      <td>-32.411</td>\n",
       "      <td>-7.124</td>\n",
       "      <td>-2.638</td>\n",
       "      <td>-3764.201</td>\n",
       "      <td>-52.285</td>\n",
       "      <td>-37.908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3115.632</td>\n",
       "      <td>-1.239</td>\n",
       "      <td>-55.159</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-300.337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-42.008</td>\n",
       "      <td>-53.788</td>\n",
       "      <td>-3.161</td>\n",
       "      <td>-6.969</td>\n",
       "      <td>-3.136</td>\n",
       "      <td>-6.864</td>\n",
       "      <td>-53.867</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-29.721</td>\n",
       "      <td>-9.974</td>\n",
       "      <td>-23.768</td>\n",
       "      <td>-6.463</td>\n",
       "      <td>-1.523</td>\n",
       "      <td>-9.371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.782</td>\n",
       "      <td>-7.665</td>\n",
       "      <td>-6.865</td>\n",
       "      <td>-7.615</td>\n",
       "      <td>-48.619</td>\n",
       "      <td>-1.981</td>\n",
       "      <td>-21.165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-30.934</td>\n",
       "      <td>-229.001</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-146.488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.365</td>\n",
       "      <td>-40.623</td>\n",
       "      <td>-2.493</td>\n",
       "      <td>-26.114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.116</td>\n",
       "      <td>-2.611</td>\n",
       "      <td>-84.783</td>\n",
       "      <td>-44.568</td>\n",
       "      <td>-2.825</td>\n",
       "      <td>-49.946</td>\n",
       "      <td>-5.116</td>\n",
       "      <td>-589.610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2400.105</td>\n",
       "      <td>-3.147</td>\n",
       "      <td>-15.488</td>\n",
       "      <td>-166.108</td>\n",
       "      <td>-2.330</td>\n",
       "      <td>-36.434</td>\n",
       "      <td>-66.631</td>\n",
       "      <td>-1473.756</td>\n",
       "      <td>-64.519</td>\n",
       "      <td>-37.459</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-3.309</td>\n",
       "      <td>-9.757</td>\n",
       "      <td>-4.725</td>\n",
       "      <td>-45.646</td>\n",
       "      <td>-3062.261</td>\n",
       "      <td>-5.318</td>\n",
       "      <td>-6.564</td>\n",
       "      <td>-11.792</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-7.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_symbolic_regression_functions</th>\n",
       "      <td>0.793</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.937</td>\n",
       "      <td>-23.314</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.741</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-3.184</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-3.788</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.972</td>\n",
       "      <td>-16.154</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-27.997</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.582</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-1.229</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-122.114</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_per_network_polynomials</th>\n",
       "      <td>0.970</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.969</td>\n",
       "      <td>-25.757</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.994</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.967</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.748</td>\n",
       "      <td>-3.675</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.987</td>\n",
       "      <td>-15.828</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-32.589</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.968</td>\n",
       "      <td>-1.829</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.965</td>\n",
       "      <td>-1.213</td>\n",
       "      <td>0.769</td>\n",
       "      <td>-138.603</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metamodel_functions_VS_symbolic_regression_functions</th>\n",
       "      <td>-134.678</td>\n",
       "      <td>-11906.911</td>\n",
       "      <td>-9993044432.594</td>\n",
       "      <td>-5728.176</td>\n",
       "      <td>-53246513670.875</td>\n",
       "      <td>-10861677245.094</td>\n",
       "      <td>-7091281737.281</td>\n",
       "      <td>-25530187987.281</td>\n",
       "      <td>-12627280272.438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11867427977.516</td>\n",
       "      <td>-39.308</td>\n",
       "      <td>-33347268065.406</td>\n",
       "      <td>-26.052</td>\n",
       "      <td>-17474050291.969</td>\n",
       "      <td>-13320.404</td>\n",
       "      <td>-23583671874.000</td>\n",
       "      <td>-13409.999</td>\n",
       "      <td>-14483209227.516</td>\n",
       "      <td>-23218022459.938</td>\n",
       "      <td>-1116811071.567</td>\n",
       "      <td>-1736.157</td>\n",
       "      <td>-58990312499.000</td>\n",
       "      <td>-12776508788.062</td>\n",
       "      <td>-17155263670.875</td>\n",
       "      <td>-17980512694.312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-21287812499.000</td>\n",
       "      <td>-2843.734</td>\n",
       "      <td>-13988186034.156</td>\n",
       "      <td>-111.403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3745.277</td>\n",
       "      <td>-19741022948.219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1880.932</td>\n",
       "      <td>-16037998045.875</td>\n",
       "      <td>-30983378905.250</td>\n",
       "      <td>-2825.194</td>\n",
       "      <td>-274.387</td>\n",
       "      <td>-437.917</td>\n",
       "      <td>-10666024.863</td>\n",
       "      <td>-10961640624.000</td>\n",
       "      <td>-274.853</td>\n",
       "      <td>-37865258788.062</td>\n",
       "      <td>-2130.429</td>\n",
       "      <td>-11909537352.516</td>\n",
       "      <td>-1503.761</td>\n",
       "      <td>-101.437</td>\n",
       "      <td>-11064311522.438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37201.739</td>\n",
       "      <td>-22953947752.906</td>\n",
       "      <td>-6578885437.761</td>\n",
       "      <td>-256.320</td>\n",
       "      <td>-1513.371</td>\n",
       "      <td>-124.101</td>\n",
       "      <td>-17984249266.578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-33232739256.812</td>\n",
       "      <td>-8845091551.734</td>\n",
       "      <td>-8981.770</td>\n",
       "      <td>-188.541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-672.811</td>\n",
       "      <td>-14911336668.922</td>\n",
       "      <td>-72002036131.812</td>\n",
       "      <td>-13227362059.547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28547055663.062</td>\n",
       "      <td>-3575455637.868</td>\n",
       "      <td>-34289541014.625</td>\n",
       "      <td>-37238095702.125</td>\n",
       "      <td>-872.478</td>\n",
       "      <td>-26050805663.062</td>\n",
       "      <td>-11759652098.609</td>\n",
       "      <td>-23136508788.062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-14920097655.250</td>\n",
       "      <td>-44108388670.875</td>\n",
       "      <td>-6101031010.661</td>\n",
       "      <td>-15783909911.109</td>\n",
       "      <td>-54.982</td>\n",
       "      <td>-975058111.284</td>\n",
       "      <td>-10973363036.109</td>\n",
       "      <td>-23833110350.562</td>\n",
       "      <td>-22066728514.625</td>\n",
       "      <td>-18698500975.562</td>\n",
       "      <td>-13783.799</td>\n",
       "      <td>-43974.504</td>\n",
       "      <td>-532.612</td>\n",
       "      <td>-2254.792</td>\n",
       "      <td>-1309.845</td>\n",
       "      <td>-3129.347</td>\n",
       "      <td>-21841235350.562</td>\n",
       "      <td>-166.838</td>\n",
       "      <td>-1988.993</td>\n",
       "      <td>-59.312</td>\n",
       "      <td>-16732579344.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metamodel_functions_VS_per_network_polynomials</th>\n",
       "      <td>-140.994</td>\n",
       "      <td>-11274.456</td>\n",
       "      <td>-10029376830.055</td>\n",
       "      <td>-5777.053</td>\n",
       "      <td>-53512539061.500</td>\n",
       "      <td>-11157797850.562</td>\n",
       "      <td>-7212138670.875</td>\n",
       "      <td>-26235996092.750</td>\n",
       "      <td>-12840728758.766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11742531737.281</td>\n",
       "      <td>-29.431</td>\n",
       "      <td>-34004213866.188</td>\n",
       "      <td>-15.097</td>\n",
       "      <td>-17840874022.438</td>\n",
       "      <td>-12875.439</td>\n",
       "      <td>-24141923827.125</td>\n",
       "      <td>-13956.384</td>\n",
       "      <td>-14986557616.188</td>\n",
       "      <td>-24084736327.125</td>\n",
       "      <td>-1067602373.894</td>\n",
       "      <td>-1571.957</td>\n",
       "      <td>-59544277342.750</td>\n",
       "      <td>-12648275145.484</td>\n",
       "      <td>-17638322752.906</td>\n",
       "      <td>-18490842284.156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-21165781249.000</td>\n",
       "      <td>-3059.328</td>\n",
       "      <td>-13534619139.625</td>\n",
       "      <td>-112.203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3456.513</td>\n",
       "      <td>-19893813475.562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1714.415</td>\n",
       "      <td>-16571998290.016</td>\n",
       "      <td>-31853037108.375</td>\n",
       "      <td>-2546.179</td>\n",
       "      <td>-269.382</td>\n",
       "      <td>-406.711</td>\n",
       "      <td>-10695550.716</td>\n",
       "      <td>-11182025145.484</td>\n",
       "      <td>-276.872</td>\n",
       "      <td>-39502905272.438</td>\n",
       "      <td>-2131.594</td>\n",
       "      <td>-11544748534.156</td>\n",
       "      <td>-1387.070</td>\n",
       "      <td>-93.738</td>\n",
       "      <td>-10456147459.938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37585.166</td>\n",
       "      <td>-22796923827.125</td>\n",
       "      <td>-6321916047.741</td>\n",
       "      <td>-174.214</td>\n",
       "      <td>-1404.359</td>\n",
       "      <td>-115.846</td>\n",
       "      <td>-17977197264.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-34531142577.125</td>\n",
       "      <td>-8962338866.188</td>\n",
       "      <td>-8923.337</td>\n",
       "      <td>-168.203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-596.043</td>\n",
       "      <td>-14399436034.156</td>\n",
       "      <td>-72009580077.125</td>\n",
       "      <td>-12852061766.578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-29249199217.750</td>\n",
       "      <td>-3506512815.875</td>\n",
       "      <td>-35045786131.812</td>\n",
       "      <td>-38360844725.562</td>\n",
       "      <td>-825.946</td>\n",
       "      <td>-26727719725.562</td>\n",
       "      <td>-11468615721.656</td>\n",
       "      <td>-23088339842.750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-14946610106.422</td>\n",
       "      <td>-45126489256.812</td>\n",
       "      <td>-5753166037.179</td>\n",
       "      <td>-16630971678.688</td>\n",
       "      <td>-54.638</td>\n",
       "      <td>-942792489.729</td>\n",
       "      <td>-10860996092.750</td>\n",
       "      <td>-23891921385.719</td>\n",
       "      <td>-22790156249.000</td>\n",
       "      <td>-19571962889.625</td>\n",
       "      <td>-11865.495</td>\n",
       "      <td>-51721.150</td>\n",
       "      <td>-561.594</td>\n",
       "      <td>-2075.604</td>\n",
       "      <td>-1208.143</td>\n",
       "      <td>-2991.997</td>\n",
       "      <td>-22045458983.375</td>\n",
       "      <td>-169.581</td>\n",
       "      <td>-2062.254</td>\n",
       "      <td>-32.927</td>\n",
       "      <td>-16792751463.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbolic_regression_functions_VS_per_network_polynomials</th>\n",
       "      <td>0.926</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.955</td>\n",
       "      <td>-3284207.179</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.926</td>\n",
       "      <td>-6540686.515</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.836</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-28.027</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        L-0        L-1  \\\n",
       "lambda_preds_VS_target_polynomials                    0.997      1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         1.000      1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials              0.997      1.000   \n",
       "lambda_preds_VS_inet_polynomials                      0.840      0.953   \n",
       "lambda_preds_VS_metamodel_functions                  -1.452     -0.050   \n",
       "lambda_preds_VS_symbolic_regression_functions         0.998      0.997   \n",
       "lambda_preds_VS_per_network_polynomials               0.932      0.998   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    0.997      1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials        1.000      1.000   \n",
       "target_polynomials_VS_inet_polynomials                0.856      0.953   \n",
       "target_polynomials_VS_metamodel_functions            -1.419     -0.050   \n",
       "target_polynomials_VS_symbolic_regression_funct...    0.996      0.997   \n",
       "target_polynomials_VS_per_network_polynomials         0.938      0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    0.997      1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     0.840      0.953   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   -1.452     -0.050   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...    0.998      0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...    0.932      0.998   \n",
       "lstsq_target_polynomials_VS_inet_polynomials          0.856      0.953   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      -1.419     -0.050   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...    0.996      0.997   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...    0.938      0.998   \n",
       "inet_polynomials_VS_metamodel_functions              -1.929     -0.039   \n",
       "inet_polynomials_VS_symbolic_regression_functions     0.793      0.965   \n",
       "inet_polynomials_VS_per_network_polynomials           0.970      0.964   \n",
       "metamodel_functions_VS_symbolic_regression_func... -134.678 -11906.911   \n",
       "metamodel_functions_VS_per_network_polynomials     -140.994 -11274.456   \n",
       "symbolic_regression_functions_VS_per_network_po...    0.926      0.999   \n",
       "\n",
       "                                                                L-2       L-3  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000     1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000     1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000     1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.996     0.973   \n",
       "lambda_preds_VS_metamodel_functions                          -6.117    -0.257   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.996     1.000   \n",
       "lambda_preds_VS_per_network_polynomials                       0.994     1.000   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000     1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000     1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.995     0.974   \n",
       "target_polynomials_VS_metamodel_functions                    -6.133    -0.258   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.996     1.000   \n",
       "target_polynomials_VS_per_network_polynomials                 0.994     1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000     1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.996     0.974   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           -6.111    -0.257   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.996     1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.994     1.000   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.995     0.974   \n",
       "lstsq_target_polynomials_VS_metamodel_functions              -6.133    -0.258   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.996     1.000   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.994     1.000   \n",
       "inet_polynomials_VS_metamodel_functions                      -5.547    -0.277   \n",
       "inet_polynomials_VS_symbolic_regression_functions             1.000     0.974   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.999     0.972   \n",
       "metamodel_functions_VS_symbolic_regression_func...  -9993044432.594 -5728.176   \n",
       "metamodel_functions_VS_per_network_polynomials     -10029376830.055 -5777.053   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.999     1.000   \n",
       "\n",
       "                                                                L-4  \\\n",
       "lambda_preds_VS_target_polynomials                            0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.999   \n",
       "lambda_preds_VS_inet_polynomials                             -0.776   \n",
       "lambda_preds_VS_metamodel_functions                         -17.772   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.998   \n",
       "lambda_preds_VS_per_network_polynomials                       0.973   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                       -0.791   \n",
       "target_polynomials_VS_metamodel_functions                   -17.836   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.999   \n",
       "target_polynomials_VS_per_network_polynomials                 0.975   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            -0.767   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -17.714   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.975   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 -0.791   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -17.836   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.999   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.975   \n",
       "inet_polynomials_VS_metamodel_functions                      -2.945   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.472   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.477   \n",
       "metamodel_functions_VS_symbolic_regression_func... -53246513670.875   \n",
       "metamodel_functions_VS_per_network_polynomials     -53512539061.500   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.976   \n",
       "\n",
       "                                                                L-5  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.881   \n",
       "lambda_preds_VS_metamodel_functions                          -2.120   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999   \n",
       "lambda_preds_VS_per_network_polynomials                       0.992   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.882   \n",
       "target_polynomials_VS_metamodel_functions                    -2.116   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.999   \n",
       "target_polynomials_VS_per_network_polynomials                 0.992   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.881   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           -2.115   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.992   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.882   \n",
       "lstsq_target_polynomials_VS_metamodel_functions              -2.116   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.999   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.992   \n",
       "inet_polynomials_VS_metamodel_functions                      -3.839   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.840   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.894   \n",
       "metamodel_functions_VS_symbolic_regression_func... -10861677245.094   \n",
       "metamodel_functions_VS_per_network_polynomials     -11157797850.562   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.993   \n",
       "\n",
       "                                                               L-6  \\\n",
       "lambda_preds_VS_target_polynomials                           0.755   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     0.755   \n",
       "lambda_preds_VS_inet_polynomials                             0.838   \n",
       "lambda_preds_VS_metamodel_functions                       -371.411   \n",
       "lambda_preds_VS_symbolic_regression_functions                0.923   \n",
       "lambda_preds_VS_per_network_polynomials                      0.825   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           0.743   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.928   \n",
       "target_polynomials_VS_metamodel_functions                 -396.493   \n",
       "target_polynomials_VS_symbolic_regression_funct...           0.441   \n",
       "target_polynomials_VS_per_network_polynomials                0.964   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           0.755   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.838   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...        -371.626   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...           0.922   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...           0.825   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.928   \n",
       "lstsq_target_polynomials_VS_metamodel_functions           -396.493   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...           0.441   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...           0.964   \n",
       "inet_polynomials_VS_metamodel_functions                   -434.146   \n",
       "inet_polynomials_VS_symbolic_regression_functions            0.566   \n",
       "inet_polynomials_VS_per_network_polynomials                  0.970   \n",
       "metamodel_functions_VS_symbolic_regression_func... -7091281737.281   \n",
       "metamodel_functions_VS_per_network_polynomials     -7212138670.875   \n",
       "symbolic_regression_functions_VS_per_network_po...           0.551   \n",
       "\n",
       "                                                                L-7  \\\n",
       "lambda_preds_VS_target_polynomials                            0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.999   \n",
       "lambda_preds_VS_inet_polynomials                              0.866   \n",
       "lambda_preds_VS_metamodel_functions                         -15.307   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.997   \n",
       "lambda_preds_VS_per_network_polynomials                       0.969   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.858   \n",
       "target_polynomials_VS_metamodel_functions                   -15.350   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.996   \n",
       "target_polynomials_VS_per_network_polynomials                 0.975   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.867   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -15.277   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.970   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.858   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -15.350   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.996   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.975   \n",
       "inet_polynomials_VS_metamodel_functions                      -8.933   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.907   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.826   \n",
       "metamodel_functions_VS_symbolic_regression_func... -25530187987.281   \n",
       "metamodel_functions_VS_per_network_polynomials     -26235996092.750   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.956   \n",
       "\n",
       "                                                                L-8   L-9  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000 0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000 0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000 0.999   \n",
       "lambda_preds_VS_inet_polynomials                              0.958 0.959   \n",
       "lambda_preds_VS_metamodel_functions                          -3.360   NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.997 0.999   \n",
       "lambda_preds_VS_per_network_polynomials                       0.996 0.997   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.958 0.960   \n",
       "target_polynomials_VS_metamodel_functions                    -3.367   NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.997 0.999   \n",
       "target_polynomials_VS_per_network_polynomials                 0.996 0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.958 0.959   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           -3.358   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.997 0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.996 0.997   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.958 0.960   \n",
       "lstsq_target_polynomials_VS_metamodel_functions              -3.367   NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.997 0.999   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.996 0.998   \n",
       "inet_polynomials_VS_metamodel_functions                      -4.192   NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.924 0.962   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.970 0.972   \n",
       "metamodel_functions_VS_symbolic_regression_func... -12627280272.438   NaN   \n",
       "metamodel_functions_VS_per_network_polynomials     -12840728758.766   NaN   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.989 0.999   \n",
       "\n",
       "                                                               L-10    L-11  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   0.880   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   0.880   \n",
       "lambda_preds_VS_inet_polynomials                              0.890   0.833   \n",
       "lambda_preds_VS_metamodel_functions                          -7.232  -1.125   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999   0.999   \n",
       "lambda_preds_VS_per_network_polynomials                       0.991   0.819   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   0.871   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.891   0.486   \n",
       "target_polynomials_VS_metamodel_functions                    -7.242  -1.432   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.999   0.880   \n",
       "target_polynomials_VS_per_network_polynomials                 0.991   0.452   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   0.880   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.890   0.833   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           -7.232  -1.125   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999   0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.991   0.819   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.891   0.486   \n",
       "lstsq_target_polynomials_VS_metamodel_functions              -7.242  -1.432   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.999   0.880   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.991   0.452   \n",
       "inet_polynomials_VS_metamodel_functions                      -5.681  -1.269   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.892   0.801   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.933   0.984   \n",
       "metamodel_functions_VS_symbolic_regression_func... -11867427977.516 -39.308   \n",
       "metamodel_functions_VS_per_network_polynomials     -11742531737.281 -29.431   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.989   0.817   \n",
       "\n",
       "                                                               L-12    L-13  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.057   0.543   \n",
       "lambda_preds_VS_metamodel_functions                         -15.140   0.230   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999   0.964   \n",
       "lambda_preds_VS_per_network_polynomials                       0.946   0.615   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.061   0.547   \n",
       "target_polynomials_VS_metamodel_functions                   -15.091   0.232   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.999   0.965   \n",
       "target_polynomials_VS_per_network_polynomials                 0.948   0.616   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.060   0.545   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -15.111   0.230   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999   0.964   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.947   0.616   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.061   0.547   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -15.091   0.232   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.999   0.965   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.948   0.616   \n",
       "inet_polynomials_VS_metamodel_functions                      -3.613   0.064   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.671   0.444   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.625   0.903   \n",
       "metamodel_functions_VS_symbolic_regression_func... -33347268065.406 -26.052   \n",
       "metamodel_functions_VS_per_network_polynomials     -34004213866.188 -15.097   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.944   0.725   \n",
       "\n",
       "                                                               L-14  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.777   \n",
       "lambda_preds_VS_metamodel_functions                         -94.017   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.997   \n",
       "lambda_preds_VS_per_network_polynomials                       0.615   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.775   \n",
       "target_polynomials_VS_metamodel_functions                   -95.113   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.998   \n",
       "target_polynomials_VS_per_network_polynomials                 0.612   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.775   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -94.136   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.613   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.775   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -95.113   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.998   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.612   \n",
       "inet_polynomials_VS_metamodel_functions                    -123.817   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.758   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.961   \n",
       "metamodel_functions_VS_symbolic_regression_func... -17474050291.969   \n",
       "metamodel_functions_VS_per_network_polynomials     -17840874022.438   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.651   \n",
       "\n",
       "                                                         L-15  \\\n",
       "lambda_preds_VS_target_polynomials                      1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials           1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                1.000   \n",
       "lambda_preds_VS_inet_polynomials                        0.957   \n",
       "lambda_preds_VS_metamodel_functions                    -0.633   \n",
       "lambda_preds_VS_symbolic_regression_functions           0.998   \n",
       "lambda_preds_VS_per_network_polynomials                 0.997   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...      1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials          1.000   \n",
       "target_polynomials_VS_inet_polynomials                  0.958   \n",
       "target_polynomials_VS_metamodel_functions              -0.641   \n",
       "target_polynomials_VS_symbolic_regression_funct...      0.998   \n",
       "target_polynomials_VS_per_network_polynomials           0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...      1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials       0.958   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...     -0.634   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...      0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...      0.997   \n",
       "lstsq_target_polynomials_VS_inet_polynomials            0.958   \n",
       "lstsq_target_polynomials_VS_metamodel_functions        -0.641   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...      0.998   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...      0.997   \n",
       "inet_polynomials_VS_metamodel_functions                -0.677   \n",
       "inet_polynomials_VS_symbolic_regression_functions       0.966   \n",
       "inet_polynomials_VS_per_network_polynomials             0.971   \n",
       "metamodel_functions_VS_symbolic_regression_func... -13320.404   \n",
       "metamodel_functions_VS_per_network_polynomials     -12875.439   \n",
       "symbolic_regression_functions_VS_per_network_po...      0.996   \n",
       "\n",
       "                                                               L-16  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.870   \n",
       "lambda_preds_VS_metamodel_functions                         -96.920   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.996   \n",
       "lambda_preds_VS_per_network_polynomials                       0.733   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.870   \n",
       "target_polynomials_VS_metamodel_functions                   -96.788   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.996   \n",
       "target_polynomials_VS_per_network_polynomials                 0.733   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.870   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -96.914   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.996   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.733   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.870   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -96.788   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.996   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.733   \n",
       "inet_polynomials_VS_metamodel_functions                    -133.069   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.779   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.953   \n",
       "metamodel_functions_VS_symbolic_regression_func... -23583671874.000   \n",
       "metamodel_functions_VS_per_network_polynomials     -24141923827.125   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.717   \n",
       "\n",
       "                                                         L-17  \\\n",
       "lambda_preds_VS_target_polynomials                      1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials           1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                1.000   \n",
       "lambda_preds_VS_inet_polynomials                        0.936   \n",
       "lambda_preds_VS_metamodel_functions                    -1.277   \n",
       "lambda_preds_VS_symbolic_regression_functions           0.997   \n",
       "lambda_preds_VS_per_network_polynomials                 0.985   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...      1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials          1.000   \n",
       "target_polynomials_VS_inet_polynomials                  0.935   \n",
       "target_polynomials_VS_metamodel_functions              -1.276   \n",
       "target_polynomials_VS_symbolic_regression_funct...      0.997   \n",
       "target_polynomials_VS_per_network_polynomials           0.984   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...      1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials       0.936   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...     -1.274   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...      0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...      0.985   \n",
       "lstsq_target_polynomials_VS_inet_polynomials            0.935   \n",
       "lstsq_target_polynomials_VS_metamodel_functions        -1.276   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...      0.997   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...      0.984   \n",
       "inet_polynomials_VS_metamodel_functions                -1.328   \n",
       "inet_polynomials_VS_symbolic_regression_functions       0.937   \n",
       "inet_polynomials_VS_per_network_polynomials             0.969   \n",
       "metamodel_functions_VS_symbolic_regression_func... -13409.999   \n",
       "metamodel_functions_VS_per_network_polynomials     -13956.384   \n",
       "symbolic_regression_functions_VS_per_network_po...      0.989   \n",
       "\n",
       "                                                               L-18  \\\n",
       "lambda_preds_VS_target_polynomials                            0.998   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 0.998   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.998   \n",
       "lambda_preds_VS_inet_polynomials                            -10.984   \n",
       "lambda_preds_VS_metamodel_functions                         -49.091   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.946   \n",
       "lambda_preds_VS_per_network_polynomials                       0.986   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.999   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                      -10.595   \n",
       "target_polynomials_VS_metamodel_functions                   -47.285   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.939   \n",
       "target_polynomials_VS_per_network_polynomials                 0.982   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           -10.933   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -48.837   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.946   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.987   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                -10.595   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -47.285   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.939   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.982   \n",
       "inet_polynomials_VS_metamodel_functions                     -35.670   \n",
       "inet_polynomials_VS_symbolic_regression_functions           -23.314   \n",
       "inet_polynomials_VS_per_network_polynomials                 -25.757   \n",
       "metamodel_functions_VS_symbolic_regression_func... -14483209227.516   \n",
       "metamodel_functions_VS_per_network_polynomials     -14986557616.188   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.951   \n",
       "\n",
       "                                                               L-19  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.876   \n",
       "lambda_preds_VS_metamodel_functions                         -36.664   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.998   \n",
       "lambda_preds_VS_per_network_polynomials                       0.810   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.874   \n",
       "target_polynomials_VS_metamodel_functions                   -36.744   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.998   \n",
       "target_polynomials_VS_per_network_polynomials                 0.808   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.876   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -36.679   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.810   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.874   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -36.744   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.998   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.808   \n",
       "inet_polynomials_VS_metamodel_functions                     -51.681   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.819   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.957   \n",
       "metamodel_functions_VS_symbolic_regression_func... -23218022459.938   \n",
       "metamodel_functions_VS_per_network_polynomials     -24084736327.125   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.812   \n",
       "\n",
       "                                                              L-20      L-21  \\\n",
       "lambda_preds_VS_target_polynomials                           0.893     0.998   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000     1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     0.893     0.998   \n",
       "lambda_preds_VS_inet_polynomials                             0.837     0.839   \n",
       "lambda_preds_VS_metamodel_functions                        -23.775    -5.030   \n",
       "lambda_preds_VS_symbolic_regression_functions                0.901     0.978   \n",
       "lambda_preds_VS_per_network_polynomials                      1.000     0.945   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           0.859     0.998   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000     1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.507     0.818   \n",
       "target_polynomials_VS_metamodel_functions                  -30.381    -5.219   \n",
       "target_polynomials_VS_symbolic_regression_funct...           0.652     0.980   \n",
       "target_polynomials_VS_per_network_polynomials                0.859     0.930   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           0.893     0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.837     0.839   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...         -23.775    -5.038   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...           0.901     0.978   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...           1.000     0.945   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.507     0.818   \n",
       "lstsq_target_polynomials_VS_metamodel_functions            -30.381    -5.219   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...           0.652     0.980   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...           0.859     0.930   \n",
       "inet_polynomials_VS_metamodel_functions                    -32.411    -7.124   \n",
       "inet_polynomials_VS_symbolic_regression_functions            0.952     0.777   \n",
       "inet_polynomials_VS_per_network_polynomials                  0.778     0.942   \n",
       "metamodel_functions_VS_symbolic_regression_func... -1116811071.567 -1736.157   \n",
       "metamodel_functions_VS_per_network_polynomials     -1067602373.894 -1571.957   \n",
       "symbolic_regression_functions_VS_per_network_po...           0.859     0.898   \n",
       "\n",
       "                                                               L-22  \\\n",
       "lambda_preds_VS_target_polynomials                            0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.999   \n",
       "lambda_preds_VS_inet_polynomials                             -0.536   \n",
       "lambda_preds_VS_metamodel_functions                         -15.195   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999   \n",
       "lambda_preds_VS_per_network_polynomials                       0.960   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                       -0.531   \n",
       "target_polynomials_VS_metamodel_functions                   -15.157   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.999   \n",
       "target_polynomials_VS_per_network_polynomials                 0.961   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            -0.534   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -15.185   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.961   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 -0.531   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -15.157   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.999   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.961   \n",
       "inet_polynomials_VS_metamodel_functions                      -2.638   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.537   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.514   \n",
       "metamodel_functions_VS_symbolic_regression_func... -58990312499.000   \n",
       "metamodel_functions_VS_per_network_polynomials     -59544277342.750   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.955   \n",
       "\n",
       "                                                               L-23  \\\n",
       "lambda_preds_VS_target_polynomials                          -21.872   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    -21.872   \n",
       "lambda_preds_VS_inet_polynomials                              0.936   \n",
       "lambda_preds_VS_metamodel_functions                       -3749.886   \n",
       "lambda_preds_VS_symbolic_regression_functions                -0.059   \n",
       "lambda_preds_VS_per_network_polynomials                       0.933   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           -0.117   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                       -0.233   \n",
       "target_polynomials_VS_metamodel_functions                  -192.221   \n",
       "target_polynomials_VS_symbolic_regression_funct...           -0.076   \n",
       "target_polynomials_VS_per_network_polynomials                -0.227   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          -21.872   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.936   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...        -3749.886   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...           -0.059   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.933   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 -0.233   \n",
       "lstsq_target_polynomials_VS_metamodel_functions            -192.221   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...           -0.076   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...           -0.227   \n",
       "inet_polynomials_VS_metamodel_functions                   -3764.201   \n",
       "inet_polynomials_VS_symbolic_regression_functions            -0.086   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.995   \n",
       "metamodel_functions_VS_symbolic_regression_func... -12776508788.062   \n",
       "metamodel_functions_VS_per_network_polynomials     -12648275145.484   \n",
       "symbolic_regression_functions_VS_per_network_po...     -3284207.179   \n",
       "\n",
       "                                                               L-24  \\\n",
       "lambda_preds_VS_target_polynomials                            0.510   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.510   \n",
       "lambda_preds_VS_inet_polynomials                              0.830   \n",
       "lambda_preds_VS_metamodel_functions                         -38.148   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.998   \n",
       "lambda_preds_VS_per_network_polynomials                       0.823   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.681   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.871   \n",
       "target_polynomials_VS_metamodel_functions                   -26.475   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.678   \n",
       "target_polynomials_VS_per_network_polynomials                 0.887   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.510   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.830   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -38.148   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.823   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.871   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -26.475   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.678   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.887   \n",
       "inet_polynomials_VS_metamodel_functions                     -52.285   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.743   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.994   \n",
       "metamodel_functions_VS_symbolic_regression_func... -17155263670.875   \n",
       "metamodel_functions_VS_per_network_polynomials     -17638322752.906   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.817   \n",
       "\n",
       "                                                               L-25   L-26  \\\n",
       "lambda_preds_VS_target_polynomials                            0.697  0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.697  0.999   \n",
       "lambda_preds_VS_inet_polynomials                              0.831 -3.012   \n",
       "lambda_preds_VS_metamodel_functions                         -27.995    NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.996  0.986   \n",
       "lambda_preds_VS_per_network_polynomials                       0.820  0.864   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.781  0.999   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.925 -3.085   \n",
       "target_polynomials_VS_metamodel_functions                   -21.314    NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.785  0.985   \n",
       "target_polynomials_VS_per_network_polynomials                 0.931  0.870   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.697  0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.831 -2.998   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -28.004    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.996  0.986   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.820  0.864   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.925 -3.085   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -21.314    NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.785  0.985   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.931  0.870   \n",
       "inet_polynomials_VS_metamodel_functions                     -37.908    NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.741 -0.203   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.994 -0.302   \n",
       "metamodel_functions_VS_symbolic_regression_func... -17980512694.312    NaN   \n",
       "metamodel_functions_VS_per_network_polynomials     -18490842284.156    NaN   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.817  0.926   \n",
       "\n",
       "                                                               L-27      L-28  \\\n",
       "lambda_preds_VS_target_polynomials                           -0.224     0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 0.999     0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     -0.224     0.999   \n",
       "lambda_preds_VS_inet_polynomials                              0.921     0.975   \n",
       "lambda_preds_VS_metamodel_functions                       -2537.487    -1.140   \n",
       "lambda_preds_VS_symbolic_regression_functions                -0.072     0.996   \n",
       "lambda_preds_VS_per_network_polynomials                       0.821     0.998   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           -0.146     1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000     1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.328     0.977   \n",
       "target_polynomials_VS_metamodel_functions                 -2431.926    -1.153   \n",
       "target_polynomials_VS_symbolic_regression_funct...           -0.000     0.996   \n",
       "target_polynomials_VS_per_network_polynomials                 0.474     0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           -0.219     1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.925     0.977   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...        -2558.876    -1.139   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...           -0.071     0.996   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.825     0.998   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.328     0.977   \n",
       "lstsq_target_polynomials_VS_metamodel_functions           -2431.926    -1.153   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...           -0.000     0.996   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.474     0.999   \n",
       "inet_polynomials_VS_metamodel_functions                   -3115.632    -1.239   \n",
       "inet_polynomials_VS_symbolic_regression_functions            -0.043     0.961   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.971     0.967   \n",
       "metamodel_functions_VS_symbolic_regression_func... -21287812499.000 -2843.734   \n",
       "metamodel_functions_VS_per_network_polynomials     -21165781249.000 -3059.328   \n",
       "symbolic_regression_functions_VS_per_network_po...     -6540686.515     0.997   \n",
       "\n",
       "                                                               L-29     L-30  \\\n",
       "lambda_preds_VS_target_polynomials                            0.992    1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000    1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.992    1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.932    0.982   \n",
       "lambda_preds_VS_metamodel_functions                         -49.684   -0.855   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999    0.998   \n",
       "lambda_preds_VS_per_network_polynomials                       0.932    0.998   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.992    1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000    1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.944    0.982   \n",
       "target_polynomials_VS_metamodel_functions                   -46.067   -0.855   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.990    0.998   \n",
       "target_polynomials_VS_per_network_polynomials                 0.943    0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.992    1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.932    0.982   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -49.668   -0.855   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999    0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.932    0.998   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.944    0.982   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -46.067   -0.855   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.990    0.998   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.943    0.998   \n",
       "inet_polynomials_VS_metamodel_functions                     -55.159   -0.896   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.917    0.970   \n",
       "inet_polynomials_VS_per_network_polynomials                   1.000    0.971   \n",
       "metamodel_functions_VS_symbolic_regression_func... -13988186034.156 -111.403   \n",
       "metamodel_functions_VS_per_network_polynomials     -13534619139.625 -112.203   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.926    1.000   \n",
       "\n",
       "                                                     L-31      L-32  \\\n",
       "lambda_preds_VS_target_polynomials                  0.998     0.482   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       0.999     1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            0.998     0.482   \n",
       "lambda_preds_VS_inet_polynomials                    0.127     0.822   \n",
       "lambda_preds_VS_metamodel_functions                   NaN    -0.075   \n",
       "lambda_preds_VS_symbolic_regression_functions       0.979     0.999   \n",
       "lambda_preds_VS_per_network_polynomials             0.600     0.929   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  0.999     0.692   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000     1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.120     0.859   \n",
       "target_polynomials_VS_metamodel_functions             NaN    -0.115   \n",
       "target_polynomials_VS_symbolic_regression_funct...  0.983     0.687   \n",
       "target_polynomials_VS_per_network_polynomials       0.589     0.841   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  0.999     0.482   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.125     0.822   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN    -0.075   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...  0.980     0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...  0.598     0.929   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.120     0.859   \n",
       "lstsq_target_polynomials_VS_metamodel_functions       NaN    -0.115   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...  0.983     0.687   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...  0.589     0.841   \n",
       "inet_polynomials_VS_metamodel_functions               NaN    -0.204   \n",
       "inet_polynomials_VS_symbolic_regression_functions  -3.184     0.747   \n",
       "inet_polynomials_VS_per_network_polynomials         0.425     0.959   \n",
       "metamodel_functions_VS_symbolic_regression_func...    NaN -3745.277   \n",
       "metamodel_functions_VS_per_network_polynomials        NaN -3456.513   \n",
       "symbolic_regression_functions_VS_per_network_po...  0.604     0.926   \n",
       "\n",
       "                                                               L-33  L-34  \\\n",
       "lambda_preds_VS_target_polynomials                           -0.107 0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000 0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     -0.107 0.999   \n",
       "lambda_preds_VS_inet_polynomials                              0.927 0.904   \n",
       "lambda_preds_VS_metamodel_functions                        -244.912   NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.967 0.981   \n",
       "lambda_preds_VS_per_network_polynomials                       0.827 0.973   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.531 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.691 0.908   \n",
       "target_polynomials_VS_metamodel_functions                  -107.265   NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.504 0.980   \n",
       "target_polynomials_VS_per_network_polynomials                 0.751 0.974   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           -0.108 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.927 0.907   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...         -244.991   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.967 0.981   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.827 0.974   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.691 0.908   \n",
       "lstsq_target_polynomials_VS_metamodel_functions            -107.265   NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.504 0.980   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.751 0.974   \n",
       "inet_polynomials_VS_metamodel_functions                    -300.337   NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.933 0.808   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.971 0.969   \n",
       "metamodel_functions_VS_symbolic_regression_func... -19741022948.219   NaN   \n",
       "metamodel_functions_VS_per_network_polynomials     -19893813475.562   NaN   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.816 0.927   \n",
       "\n",
       "                                                        L-35             L-36  \\\n",
       "lambda_preds_VS_target_polynomials                     1.000            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials          1.000            1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials               1.000            1.000   \n",
       "lambda_preds_VS_inet_polynomials                       0.830            0.804   \n",
       "lambda_preds_VS_metamodel_functions                   -0.030          -34.832   \n",
       "lambda_preds_VS_symbolic_regression_functions          0.996            0.995   \n",
       "lambda_preds_VS_per_network_polynomials                0.933            0.907   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...     1.000            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials         1.000            1.000   \n",
       "target_polynomials_VS_inet_polynomials                 0.818            0.797   \n",
       "target_polynomials_VS_metamodel_functions             -0.029          -34.654   \n",
       "target_polynomials_VS_symbolic_regression_funct...     0.996            0.995   \n",
       "target_polynomials_VS_per_network_polynomials          0.925            0.902   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...     1.000            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials      0.830            0.802   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    -0.030          -34.719   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...     0.996            0.995   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...     0.933            0.906   \n",
       "lstsq_target_polynomials_VS_inet_polynomials           0.818            0.797   \n",
       "lstsq_target_polynomials_VS_metamodel_functions       -0.029          -34.654   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...     0.996            0.995   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...     0.925            0.902   \n",
       "inet_polynomials_VS_metamodel_functions               -0.106          -42.008   \n",
       "inet_polynomials_VS_symbolic_regression_functions      0.788            0.811   \n",
       "inet_polynomials_VS_per_network_polynomials            0.962            0.949   \n",
       "metamodel_functions_VS_symbolic_regression_func... -1880.932 -16037998045.875   \n",
       "metamodel_functions_VS_per_network_polynomials     -1714.415 -16571998290.016   \n",
       "symbolic_regression_functions_VS_per_network_po...     0.938            0.928   \n",
       "\n",
       "                                                               L-37      L-38  \\\n",
       "lambda_preds_VS_target_polynomials                            0.864     0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000     0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.864     0.999   \n",
       "lambda_preds_VS_inet_polynomials                              0.927     0.907   \n",
       "lambda_preds_VS_metamodel_functions                         -43.477    -2.889   \n",
       "lambda_preds_VS_symbolic_regression_functions                 1.000     0.984   \n",
       "lambda_preds_VS_per_network_polynomials                       0.827     0.973   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.884     1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000     1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.970     0.909   \n",
       "target_polynomials_VS_metamodel_functions                   -38.431    -2.827   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.885     0.984   \n",
       "target_polynomials_VS_per_network_polynomials                 0.957     0.975   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.864     1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.927     0.907   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -43.478    -2.891   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            1.000     0.985   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.827     0.974   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.970     0.909   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -38.431    -2.827   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.885     0.984   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.957     0.975   \n",
       "inet_polynomials_VS_metamodel_functions                     -53.788    -3.161   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.916     0.798   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.971     0.969   \n",
       "metamodel_functions_VS_symbolic_regression_func... -30983378905.250 -2825.194   \n",
       "metamodel_functions_VS_per_network_polynomials     -31853037108.375 -2546.179   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.829     0.928   \n",
       "\n",
       "                                                       L-39     L-40  \\\n",
       "lambda_preds_VS_target_polynomials                    0.984    0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         1.000    0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials              0.984    0.999   \n",
       "lambda_preds_VS_inet_polynomials                      0.845    0.930   \n",
       "lambda_preds_VS_metamodel_functions                  -5.689   -3.328   \n",
       "lambda_preds_VS_symbolic_regression_functions         0.995    0.990   \n",
       "lambda_preds_VS_per_network_polynomials               0.936    0.978   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    0.982    1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials        1.000    1.000   \n",
       "target_polynomials_VS_inet_polynomials                0.750    0.935   \n",
       "target_polynomials_VS_metamodel_functions            -6.179   -3.224   \n",
       "target_polynomials_VS_symbolic_regression_funct...    0.970    0.990   \n",
       "target_polynomials_VS_per_network_polynomials         0.873    0.979   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    0.984    1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     0.845    0.933   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   -5.691   -3.324   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...    0.995    0.990   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...    0.936    0.979   \n",
       "lstsq_target_polynomials_VS_inet_polynomials          0.750    0.935   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      -6.179   -3.224   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...    0.970    0.990   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...    0.873    0.979   \n",
       "inet_polynomials_VS_metamodel_functions              -6.969   -3.136   \n",
       "inet_polynomials_VS_symbolic_regression_functions     0.773    0.896   \n",
       "inet_polynomials_VS_per_network_polynomials           0.964    0.971   \n",
       "metamodel_functions_VS_symbolic_regression_func... -274.387 -437.917   \n",
       "metamodel_functions_VS_per_network_polynomials     -269.382 -406.711   \n",
       "symbolic_regression_functions_VS_per_network_po...    0.931    0.969   \n",
       "\n",
       "                                                            L-41  \\\n",
       "lambda_preds_VS_target_polynomials                         1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials              1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                   1.000   \n",
       "lambda_preds_VS_inet_polynomials                           0.803   \n",
       "lambda_preds_VS_metamodel_functions                       -2.226   \n",
       "lambda_preds_VS_symbolic_regression_functions              0.997   \n",
       "lambda_preds_VS_per_network_polynomials                    0.996   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...         1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials             1.000   \n",
       "target_polynomials_VS_inet_polynomials                     0.804   \n",
       "target_polynomials_VS_metamodel_functions                 -2.229   \n",
       "target_polynomials_VS_symbolic_regression_funct...         0.997   \n",
       "target_polynomials_VS_per_network_polynomials              0.995   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...         1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials          0.803   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...        -2.227   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...         0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...         0.996   \n",
       "lstsq_target_polynomials_VS_inet_polynomials               0.804   \n",
       "lstsq_target_polynomials_VS_metamodel_functions           -2.229   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...         0.997   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...         0.995   \n",
       "inet_polynomials_VS_metamodel_functions                   -6.864   \n",
       "inet_polynomials_VS_symbolic_regression_functions          0.539   \n",
       "inet_polynomials_VS_per_network_polynomials                0.522   \n",
       "metamodel_functions_VS_symbolic_regression_func... -10666024.863   \n",
       "metamodel_functions_VS_per_network_polynomials     -10695550.716   \n",
       "symbolic_regression_functions_VS_per_network_po...         0.999   \n",
       "\n",
       "                                                               L-42     L-43  \\\n",
       "lambda_preds_VS_target_polynomials                            0.464    0.715   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000    1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.464    0.715   \n",
       "lambda_preds_VS_inet_polynomials                              0.840    0.831   \n",
       "lambda_preds_VS_metamodel_functions                         -43.776    0.052   \n",
       "lambda_preds_VS_symbolic_regression_functions                 1.000    0.996   \n",
       "lambda_preds_VS_per_network_polynomials                       0.833    0.932   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.709    0.753   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000    1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.871    0.949   \n",
       "target_polynomials_VS_metamodel_functions                   -24.697   -0.034   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.713    0.765   \n",
       "target_polynomials_VS_per_network_polynomials                 0.876    0.927   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.464    0.715   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.840    0.832   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -43.779    0.052   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            1.000    0.996   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.833    0.932   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.871    0.949   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -24.697   -0.034   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.713    0.765   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.876    0.927   \n",
       "inet_polynomials_VS_metamodel_functions                     -53.867   -0.017   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.800    0.812   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.995    0.966   \n",
       "metamodel_functions_VS_symbolic_regression_func... -10961640624.000 -274.853   \n",
       "metamodel_functions_VS_per_network_polynomials     -11182025145.484 -276.872   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.835    0.937   \n",
       "\n",
       "                                                               L-44      L-45  \\\n",
       "lambda_preds_VS_target_polynomials                            0.869     0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000     0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.869     0.999   \n",
       "lambda_preds_VS_inet_polynomials                              0.909     0.932   \n",
       "lambda_preds_VS_metamodel_functions                         -26.505    -9.257   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999     0.943   \n",
       "lambda_preds_VS_per_network_polynomials                       0.822     0.998   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.892     1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000     1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.978     0.935   \n",
       "target_polynomials_VS_metamodel_functions                   -22.719    -9.249   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.889     0.942   \n",
       "target_polynomials_VS_per_network_polynomials                 0.951     0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.869     1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.909     0.934   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -26.505    -9.245   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999     0.943   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.822     0.999   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.978     0.935   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -22.719    -9.249   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.889     0.942   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.951     0.999   \n",
       "inet_polynomials_VS_metamodel_functions                     -29.721    -9.974   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.894     0.754   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.962     0.925   \n",
       "metamodel_functions_VS_symbolic_regression_func... -37865258788.062 -2130.429   \n",
       "metamodel_functions_VS_per_network_polynomials     -39502905272.438 -2131.594   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.815     0.937   \n",
       "\n",
       "                                                               L-46      L-47  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000     0.776   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000     1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000     0.776   \n",
       "lambda_preds_VS_inet_polynomials                              0.970     0.822   \n",
       "lambda_preds_VS_metamodel_functions                         -26.583    -5.325   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.987     1.000   \n",
       "lambda_preds_VS_per_network_polynomials                       0.975     0.929   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000     0.859   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000     1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.970     0.893   \n",
       "target_polynomials_VS_metamodel_functions                   -26.542    -3.096   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.986     0.857   \n",
       "target_polynomials_VS_per_network_polynomials                 0.975     0.913   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000     0.776   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.970     0.822   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -26.601    -5.326   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.987     1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.975     0.929   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.970     0.893   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -26.542    -3.096   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.986     0.857   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.975     0.913   \n",
       "inet_polynomials_VS_metamodel_functions                     -23.768    -6.463   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.925     0.766   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.992     0.963   \n",
       "metamodel_functions_VS_symbolic_regression_func... -11909537352.516 -1503.761   \n",
       "metamodel_functions_VS_per_network_polynomials     -11544748534.156 -1387.070   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.930     0.929   \n",
       "\n",
       "                                                       L-48             L-49  \\\n",
       "lambda_preds_VS_target_polynomials                    1.000            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         1.000            1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials              1.000            1.000   \n",
       "lambda_preds_VS_inet_polynomials                      0.952            0.851   \n",
       "lambda_preds_VS_metamodel_functions                  -1.528           -7.024   \n",
       "lambda_preds_VS_symbolic_regression_functions         0.995            1.000   \n",
       "lambda_preds_VS_per_network_polynomials               0.950            0.965   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    1.000            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials        1.000            1.000   \n",
       "target_polynomials_VS_inet_polynomials                0.952            0.851   \n",
       "target_polynomials_VS_metamodel_functions            -1.526           -6.996   \n",
       "target_polynomials_VS_symbolic_regression_funct...    0.995            0.999   \n",
       "target_polynomials_VS_per_network_polynomials         0.950            0.966   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    1.000            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     0.952            0.852   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   -1.526           -7.012   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...    0.995            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...    0.950            0.966   \n",
       "lstsq_target_polynomials_VS_inet_polynomials          0.952            0.851   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      -1.526           -6.996   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...    0.995            0.999   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...    0.950            0.966   \n",
       "inet_polynomials_VS_metamodel_functions              -1.523           -9.371   \n",
       "inet_polynomials_VS_symbolic_regression_functions     0.961            0.821   \n",
       "inet_polynomials_VS_per_network_polynomials           0.999            0.887   \n",
       "metamodel_functions_VS_symbolic_regression_func... -101.437 -11064311522.438   \n",
       "metamodel_functions_VS_per_network_polynomials      -93.738 -10456147459.938   \n",
       "symbolic_regression_functions_VS_per_network_po...    0.962            0.959   \n",
       "\n",
       "                                                    L-50       L-51  \\\n",
       "lambda_preds_VS_target_polynomials                 1.000      1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      1.000      1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           1.000      1.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.882      0.637   \n",
       "lambda_preds_VS_metamodel_functions                  NaN    -22.570   \n",
       "lambda_preds_VS_symbolic_regression_functions      0.996      0.923   \n",
       "lambda_preds_VS_per_network_polynomials            0.962      0.970   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 1.000      1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     1.000      1.000   \n",
       "target_polynomials_VS_inet_polynomials             0.883      0.635   \n",
       "target_polynomials_VS_metamodel_functions            NaN    -22.306   \n",
       "target_polynomials_VS_symbolic_regression_funct... 0.996      0.921   \n",
       "target_polynomials_VS_per_network_polynomials      0.962      0.969   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 1.000      1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.883      0.636   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN    -22.547   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre... 0.996      0.923   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po... 0.962      0.970   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.883      0.635   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      NaN    -22.306   \n",
       "lstsq_target_polynomials_VS_symbolic_regression... 0.996      0.921   \n",
       "lstsq_target_polynomials_VS_per_network_polynom... 0.962      0.969   \n",
       "inet_polynomials_VS_metamodel_functions              NaN    -28.782   \n",
       "inet_polynomials_VS_symbolic_regression_functions  0.893      0.601   \n",
       "inet_polynomials_VS_per_network_polynomials        0.963      0.748   \n",
       "metamodel_functions_VS_symbolic_regression_func...   NaN -37201.739   \n",
       "metamodel_functions_VS_per_network_polynomials       NaN -37585.166   \n",
       "symbolic_regression_functions_VS_per_network_po... 0.977      0.817   \n",
       "\n",
       "                                                               L-52  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   \n",
       "lambda_preds_VS_inet_polynomials                            -21.961   \n",
       "lambda_preds_VS_metamodel_functions                         -94.644   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.994   \n",
       "lambda_preds_VS_per_network_polynomials                       0.982   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                      -21.724   \n",
       "target_polynomials_VS_metamodel_functions                   -93.578   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.995   \n",
       "target_polynomials_VS_per_network_polynomials                 0.981   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           -21.838   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -94.084   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.995   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.981   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                -21.724   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -93.578   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.995   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.981   \n",
       "inet_polynomials_VS_metamodel_functions                      -7.665   \n",
       "inet_polynomials_VS_symbolic_regression_functions            -3.788   \n",
       "inet_polynomials_VS_per_network_polynomials                  -3.675   \n",
       "metamodel_functions_VS_symbolic_regression_func... -22953947752.906   \n",
       "metamodel_functions_VS_per_network_polynomials     -22796923827.125   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.976   \n",
       "\n",
       "                                                              L-53     L-54  \\\n",
       "lambda_preds_VS_target_polynomials                           1.000    0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000    1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     1.000    0.999   \n",
       "lambda_preds_VS_inet_polynomials                             0.887    0.776   \n",
       "lambda_preds_VS_metamodel_functions                         -3.445   -5.342   \n",
       "lambda_preds_VS_symbolic_regression_functions                1.000    0.991   \n",
       "lambda_preds_VS_per_network_polynomials                      0.978    0.701   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           1.000    1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000    1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.888    0.772   \n",
       "target_polynomials_VS_metamodel_functions                   -3.455   -5.330   \n",
       "target_polynomials_VS_symbolic_regression_funct...           1.000    0.991   \n",
       "target_polynomials_VS_per_network_polynomials                0.977    0.695   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           1.000    1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.887    0.778   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -3.444   -5.360   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...           1.000    0.992   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...           0.978    0.702   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.888    0.772   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -3.455   -5.330   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...           1.000    0.991   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...           0.977    0.695   \n",
       "inet_polynomials_VS_metamodel_functions                     -6.865   -7.615   \n",
       "inet_polynomials_VS_symbolic_regression_functions            0.841    0.765   \n",
       "inet_polynomials_VS_per_network_polynomials                  0.783    0.738   \n",
       "metamodel_functions_VS_symbolic_regression_func... -6578885437.761 -256.320   \n",
       "metamodel_functions_VS_per_network_polynomials     -6321916047.741 -174.214   \n",
       "symbolic_regression_functions_VS_per_network_po...           0.976    0.707   \n",
       "\n",
       "                                                        L-55     L-56  \\\n",
       "lambda_preds_VS_target_polynomials                     0.583    1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials          0.997    1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials               0.583    1.000   \n",
       "lambda_preds_VS_inet_polynomials                       0.705    0.960   \n",
       "lambda_preds_VS_metamodel_functions                  -36.536   -1.721   \n",
       "lambda_preds_VS_symbolic_regression_functions          0.975    0.933   \n",
       "lambda_preds_VS_per_network_polynomials                0.912    0.949   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...     0.782    1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials         1.000    1.000   \n",
       "target_polynomials_VS_inet_polynomials                 0.785    0.959   \n",
       "target_polynomials_VS_metamodel_functions            -18.660   -1.696   \n",
       "target_polynomials_VS_symbolic_regression_funct...     0.744    0.931   \n",
       "target_polynomials_VS_per_network_polynomials          0.828    0.948   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...     0.588    1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials      0.709    0.960   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   -36.692   -1.713   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...     0.976    0.933   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...     0.915    0.950   \n",
       "lstsq_target_polynomials_VS_inet_polynomials           0.785    0.959   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      -18.660   -1.696   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...     0.744    0.931   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...     0.828    0.948   \n",
       "inet_polynomials_VS_metamodel_functions              -48.619   -1.981   \n",
       "inet_polynomials_VS_symbolic_regression_functions      0.444    0.972   \n",
       "inet_polynomials_VS_per_network_polynomials            0.866    0.987   \n",
       "metamodel_functions_VS_symbolic_regression_func... -1513.371 -124.101   \n",
       "metamodel_functions_VS_per_network_polynomials     -1404.359 -115.846   \n",
       "symbolic_regression_functions_VS_per_network_po...     0.884    0.997   \n",
       "\n",
       "                                                               L-57   L-58  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                            -19.066 -0.046   \n",
       "lambda_preds_VS_metamodel_functions                         -80.429    NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.952  0.681   \n",
       "lambda_preds_VS_per_network_polynomials                       0.995  0.443   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                      -18.894 -0.047   \n",
       "target_polynomials_VS_metamodel_functions                   -79.743    NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.953  0.677   \n",
       "target_polynomials_VS_per_network_polynomials                 0.995  0.439   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           -18.983 -0.051   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -80.072    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.953  0.678   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.995  0.439   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                -18.894 -0.047   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -79.743    NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.953  0.677   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.995  0.439   \n",
       "inet_polynomials_VS_metamodel_functions                     -21.165    NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions           -16.154 -0.158   \n",
       "inet_polynomials_VS_per_network_polynomials                 -15.828  0.347   \n",
       "metamodel_functions_VS_symbolic_regression_func... -17984249266.578    NaN   \n",
       "metamodel_functions_VS_per_network_polynomials     -17977197264.625    NaN   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.967  0.909   \n",
       "\n",
       "                                                               L-59  \\\n",
       "lambda_preds_VS_target_polynomials                            0.957   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.957   \n",
       "lambda_preds_VS_inet_polynomials                              0.905   \n",
       "lambda_preds_VS_metamodel_functions                         -25.370   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.998   \n",
       "lambda_preds_VS_per_network_polynomials                       0.829   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.961   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.969   \n",
       "target_polynomials_VS_metamodel_functions                   -23.478   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.957   \n",
       "target_polynomials_VS_per_network_polynomials                 0.934   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.957   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.905   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -25.369   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.829   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.969   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -23.478   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.957   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.934   \n",
       "inet_polynomials_VS_metamodel_functions                     -30.934   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.900   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.967   \n",
       "metamodel_functions_VS_symbolic_regression_func... -33232739256.812   \n",
       "metamodel_functions_VS_per_network_polynomials     -34531142577.125   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.829   \n",
       "\n",
       "                                                              L-60      L-61  \\\n",
       "lambda_preds_VS_target_polynomials                          -1.382     1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000     1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    -1.382     1.000   \n",
       "lambda_preds_VS_inet_polynomials                             0.845     0.966   \n",
       "lambda_preds_VS_metamodel_functions                       -183.519    -0.072   \n",
       "lambda_preds_VS_symbolic_regression_functions                1.000     0.999   \n",
       "lambda_preds_VS_per_network_polynomials                      0.837     0.999   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           0.379     1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000     1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.591     0.967   \n",
       "target_polynomials_VS_metamodel_functions                  -50.383    -0.071   \n",
       "target_polynomials_VS_symbolic_regression_funct...           0.378     0.999   \n",
       "target_polynomials_VS_per_network_polynomials                0.584     0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          -1.382     1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.845     0.966   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...        -183.496    -0.071   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...           1.000     0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...           0.837     0.999   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.591     0.967   \n",
       "lstsq_target_polynomials_VS_metamodel_functions            -50.383    -0.071   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...           0.378     0.999   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...           0.584     0.999   \n",
       "inet_polynomials_VS_metamodel_functions                   -229.001    -0.052   \n",
       "inet_polynomials_VS_symbolic_regression_functions            0.806     0.966   \n",
       "inet_polynomials_VS_per_network_polynomials                  0.997     0.966   \n",
       "metamodel_functions_VS_symbolic_regression_func... -8845091551.734 -8981.770   \n",
       "metamodel_functions_VS_per_network_polynomials     -8962338866.188 -8923.337   \n",
       "symbolic_regression_functions_VS_per_network_po...           0.836     1.000   \n",
       "\n",
       "                                                       L-62  L-63  L-64  \\\n",
       "lambda_preds_VS_target_polynomials                    1.000 0.982 0.997   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         1.000 1.000 0.997   \n",
       "lambda_preds_VS_lstsq_target_polynomials              1.000 0.982 0.997   \n",
       "lambda_preds_VS_inet_polynomials                     -4.414 0.822 0.958   \n",
       "lambda_preds_VS_metamodel_functions                 -11.981   NaN   NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions         0.974 0.999 0.996   \n",
       "lambda_preds_VS_per_network_polynomials               0.470 0.929 0.995   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    1.000 0.984 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials        1.000 1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials               -4.396 0.888 0.968   \n",
       "target_polynomials_VS_metamodel_functions           -11.980   NaN   NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...    0.974 0.986 0.998   \n",
       "target_polynomials_VS_per_network_polynomials         0.468 0.962 0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    1.000 0.982 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials    -4.410 0.822 0.962   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...  -11.997   NaN   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...    0.974 0.999 0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...    0.470 0.929 0.997   \n",
       "lstsq_target_polynomials_VS_inet_polynomials         -4.396 0.888 0.968   \n",
       "lstsq_target_polynomials_VS_metamodel_functions     -11.980   NaN   NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...    0.974 0.986 0.998   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...    0.468 0.962 0.998   \n",
       "inet_polynomials_VS_metamodel_functions            -146.488   NaN   NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions   -27.997 0.774 0.947   \n",
       "inet_polynomials_VS_per_network_polynomials         -32.589 0.965 0.969   \n",
       "metamodel_functions_VS_symbolic_regression_func... -188.541   NaN   NaN   \n",
       "metamodel_functions_VS_per_network_polynomials     -168.203   NaN   NaN   \n",
       "symbolic_regression_functions_VS_per_network_po...    0.637 0.932 0.994   \n",
       "\n",
       "                                                       L-65             L-66  \\\n",
       "lambda_preds_VS_target_polynomials                   -0.457            0.995   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         1.000            1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials             -0.457            0.995   \n",
       "lambda_preds_VS_inet_polynomials                      0.828            0.930   \n",
       "lambda_preds_VS_metamodel_functions                  -0.947          -38.154   \n",
       "lambda_preds_VS_symbolic_regression_functions         0.992            0.997   \n",
       "lambda_preds_VS_per_network_polynomials               0.827            0.941   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    0.222            0.995   \n",
       "target_polynomials_VS_lstsq_target_polynomials        1.000            1.000   \n",
       "target_polynomials_VS_inet_polynomials               -0.250            0.947   \n",
       "target_polynomials_VS_metamodel_functions            -0.845          -36.006   \n",
       "target_polynomials_VS_symbolic_regression_funct...    0.227            0.993   \n",
       "target_polynomials_VS_per_network_polynomials        -0.289            0.956   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...   -0.456            0.995   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     0.828            0.930   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   -0.947          -38.212   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...    0.992            0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...    0.827            0.941   \n",
       "lstsq_target_polynomials_VS_inet_polynomials         -0.250            0.947   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      -0.845          -36.006   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...    0.227            0.993   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...   -0.289            0.956   \n",
       "inet_polynomials_VS_metamodel_functions              -1.365          -40.623   \n",
       "inet_polynomials_VS_symbolic_regression_functions     0.804            0.897   \n",
       "inet_polynomials_VS_per_network_polynomials           0.941            0.996   \n",
       "metamodel_functions_VS_symbolic_regression_func... -672.811 -14911336668.922   \n",
       "metamodel_functions_VS_per_network_polynomials     -596.043 -14399436034.156   \n",
       "symbolic_regression_functions_VS_per_network_po...    0.832            0.925   \n",
       "\n",
       "                                                               L-67  \\\n",
       "lambda_preds_VS_target_polynomials                            0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.999   \n",
       "lambda_preds_VS_inet_polynomials                              0.308   \n",
       "lambda_preds_VS_metamodel_functions                          -9.111   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999   \n",
       "lambda_preds_VS_per_network_polynomials                       0.998   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.306   \n",
       "target_polynomials_VS_metamodel_functions                    -9.102   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.999   \n",
       "target_polynomials_VS_per_network_polynomials                 0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.311   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           -9.091   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.998   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.306   \n",
       "lstsq_target_polynomials_VS_metamodel_functions              -9.102   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.999   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.999   \n",
       "inet_polynomials_VS_metamodel_functions                      -2.493   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.672   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.674   \n",
       "metamodel_functions_VS_symbolic_regression_func... -72002036131.812   \n",
       "metamodel_functions_VS_per_network_polynomials     -72009580077.125   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.997   \n",
       "\n",
       "                                                               L-68  L-69  \\\n",
       "lambda_preds_VS_target_polynomials                            0.943 0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000 0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.943 0.999   \n",
       "lambda_preds_VS_inet_polynomials                              0.840 0.961   \n",
       "lambda_preds_VS_metamodel_functions                         -19.370   NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999 0.995   \n",
       "lambda_preds_VS_per_network_polynomials                       0.976 0.996   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.949 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.934 0.963   \n",
       "target_polynomials_VS_metamodel_functions                   -18.095   NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.949 0.996   \n",
       "target_polynomials_VS_per_network_polynomials                 0.890 0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.943 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.840 0.963   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -19.368   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999 0.996   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.976 0.996   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.934 0.963   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -18.095   NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.949 0.996   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.890 0.997   \n",
       "inet_polynomials_VS_metamodel_functions                     -26.114   NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.800 0.970   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.619 0.968   \n",
       "metamodel_functions_VS_symbolic_regression_func... -13227362059.547   NaN   \n",
       "metamodel_functions_VS_per_network_polynomials     -12852061766.578   NaN   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.965 1.000   \n",
       "\n",
       "                                                               L-70  \\\n",
       "lambda_preds_VS_target_polynomials                            0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.999   \n",
       "lambda_preds_VS_inet_polynomials                            -47.585   \n",
       "lambda_preds_VS_metamodel_functions                        -220.238   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.958   \n",
       "lambda_preds_VS_per_network_polynomials                       0.181   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                      -47.079   \n",
       "target_polynomials_VS_metamodel_functions                  -217.897   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.956   \n",
       "target_polynomials_VS_per_network_polynomials                 0.181   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           -47.228   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...         -218.461   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.957   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.184   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                -47.079   \n",
       "lstsq_target_polynomials_VS_metamodel_functions            -217.897   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.956   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.181   \n",
       "inet_polynomials_VS_metamodel_functions                      -5.116   \n",
       "inet_polynomials_VS_symbolic_regression_functions            -1.582   \n",
       "inet_polynomials_VS_per_network_polynomials                  -1.829   \n",
       "metamodel_functions_VS_symbolic_regression_func... -28547055663.062   \n",
       "metamodel_functions_VS_per_network_polynomials     -29249199217.750   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.048   \n",
       "\n",
       "                                                              L-71  \\\n",
       "lambda_preds_VS_target_polynomials                           1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     1.000   \n",
       "lambda_preds_VS_inet_polynomials                             0.992   \n",
       "lambda_preds_VS_metamodel_functions                         -2.671   \n",
       "lambda_preds_VS_symbolic_regression_functions                1.000   \n",
       "lambda_preds_VS_per_network_polynomials                      0.993   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.992   \n",
       "target_polynomials_VS_metamodel_functions                   -2.672   \n",
       "target_polynomials_VS_symbolic_regression_funct...           1.000   \n",
       "target_polynomials_VS_per_network_polynomials                0.993   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.992   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -2.673   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...           1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...           0.993   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.992   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -2.672   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...           1.000   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...           0.993   \n",
       "inet_polynomials_VS_metamodel_functions                     -2.611   \n",
       "inet_polynomials_VS_symbolic_regression_functions            0.993   \n",
       "inet_polynomials_VS_per_network_polynomials                  0.972   \n",
       "metamodel_functions_VS_symbolic_regression_func... -3575455637.868   \n",
       "metamodel_functions_VS_per_network_polynomials     -3506512815.875   \n",
       "symbolic_regression_functions_VS_per_network_po...           0.990   \n",
       "\n",
       "                                                               L-72  \\\n",
       "lambda_preds_VS_target_polynomials                            0.992   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.992   \n",
       "lambda_preds_VS_inet_polynomials                              0.930   \n",
       "lambda_preds_VS_metamodel_functions                         -70.746   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999   \n",
       "lambda_preds_VS_per_network_polynomials                       0.821   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.992   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.966   \n",
       "target_polynomials_VS_metamodel_functions                   -73.388   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.989   \n",
       "target_polynomials_VS_per_network_polynomials                 0.878   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.992   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.930   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -70.759   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.822   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.966   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -73.388   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.989   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.878   \n",
       "inet_polynomials_VS_metamodel_functions                     -84.783   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.907   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.967   \n",
       "metamodel_functions_VS_symbolic_regression_func... -34289541014.625   \n",
       "metamodel_functions_VS_per_network_polynomials     -35045786131.812   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.819   \n",
       "\n",
       "                                                               L-73     L-74  \\\n",
       "lambda_preds_VS_target_polynomials                            0.968    1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000    1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.968    1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.934    0.991   \n",
       "lambda_preds_VS_metamodel_functions                         -38.121   -2.890   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.998    0.999   \n",
       "lambda_preds_VS_per_network_polynomials                       0.834    0.994   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.970    1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000    1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.984    0.992   \n",
       "target_polynomials_VS_metamodel_functions                   -36.130   -2.897   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.968    0.999   \n",
       "target_polynomials_VS_per_network_polynomials                 0.929    0.994   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.968    1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.934    0.991   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -38.121   -2.886   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.998    0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.834    0.994   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.984    0.992   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -36.130   -2.897   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.968    0.999   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.929    0.994   \n",
       "inet_polynomials_VS_metamodel_functions                     -44.568   -2.825   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.910    0.989   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.970    0.973   \n",
       "metamodel_functions_VS_symbolic_regression_func... -37238095702.125 -872.478   \n",
       "metamodel_functions_VS_per_network_polynomials     -38360844725.562 -825.946   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.828    0.992   \n",
       "\n",
       "                                                               L-75  \\\n",
       "lambda_preds_VS_target_polynomials                            0.975   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.975   \n",
       "lambda_preds_VS_inet_polynomials                              0.833   \n",
       "lambda_preds_VS_metamodel_functions                         -36.782   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999   \n",
       "lambda_preds_VS_per_network_polynomials                       0.826   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.976   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.907   \n",
       "target_polynomials_VS_metamodel_functions                   -34.698   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.977   \n",
       "target_polynomials_VS_per_network_polynomials                 0.910   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.975   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.833   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -36.782   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.826   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.907   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -34.698   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.977   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.910   \n",
       "inet_polynomials_VS_metamodel_functions                     -49.946   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.755   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.966   \n",
       "metamodel_functions_VS_symbolic_regression_func... -26050805663.062   \n",
       "metamodel_functions_VS_per_network_polynomials     -26727719725.562   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.824   \n",
       "\n",
       "                                                               L-76  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.993   \n",
       "lambda_preds_VS_metamodel_functions                          -5.858   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999   \n",
       "lambda_preds_VS_per_network_polynomials                       0.985   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.993   \n",
       "target_polynomials_VS_metamodel_functions                    -5.842   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.999   \n",
       "target_polynomials_VS_per_network_polynomials                 0.985   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.993   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           -5.856   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.985   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.993   \n",
       "lstsq_target_polynomials_VS_metamodel_functions              -5.842   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.999   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.985   \n",
       "inet_polynomials_VS_metamodel_functions                      -5.116   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.995   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.996   \n",
       "metamodel_functions_VS_symbolic_regression_func... -11759652098.609   \n",
       "metamodel_functions_VS_per_network_polynomials     -11468615721.656   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.989   \n",
       "\n",
       "                                                               L-77  L-78  \\\n",
       "lambda_preds_VS_target_polynomials                            0.998 1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 0.999 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.998 1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.950 0.953   \n",
       "lambda_preds_VS_metamodel_functions                        -516.157   NaN   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.978 0.998   \n",
       "lambda_preds_VS_per_network_polynomials                       0.978 0.996   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.999 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.941 0.954   \n",
       "target_polynomials_VS_metamodel_functions                  -533.555   NaN   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.972 0.998   \n",
       "target_polynomials_VS_per_network_polynomials                 0.971 0.996   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.999 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.951 0.954   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...         -519.616   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.978 0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.978 0.996   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.941 0.954   \n",
       "lstsq_target_polynomials_VS_metamodel_functions            -533.555   NaN   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.972 0.998   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.971 0.996   \n",
       "inet_polynomials_VS_metamodel_functions                    -589.610   NaN   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.961 0.950   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.968 0.966   \n",
       "metamodel_functions_VS_symbolic_regression_func... -23136508788.062   NaN   \n",
       "metamodel_functions_VS_per_network_polynomials     -23088339842.750   NaN   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.999 0.998   \n",
       "\n",
       "                                                               L-79  \\\n",
       "lambda_preds_VS_target_polynomials                          -14.178   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    -14.178   \n",
       "lambda_preds_VS_inet_polynomials                              0.924   \n",
       "lambda_preds_VS_metamodel_functions                       -2112.601   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.839   \n",
       "lambda_preds_VS_per_network_polynomials                       0.829   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.070   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.178   \n",
       "target_polynomials_VS_metamodel_functions                  -135.321   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.040   \n",
       "target_polynomials_VS_per_network_polynomials                 0.218   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          -14.209   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.925   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...        -2118.040   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.840   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.831   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.178   \n",
       "lstsq_target_polynomials_VS_metamodel_functions            -135.321   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.040   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.218   \n",
       "inet_polynomials_VS_metamodel_functions                   -2400.105   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.829   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.971   \n",
       "metamodel_functions_VS_symbolic_regression_func... -14920097655.250   \n",
       "metamodel_functions_VS_per_network_polynomials     -14946610106.422   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.544   \n",
       "\n",
       "                                                               L-80  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   \n",
       "lambda_preds_VS_inet_polynomials                             -0.334   \n",
       "lambda_preds_VS_metamodel_functions                         -16.361   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.999   \n",
       "lambda_preds_VS_per_network_polynomials                       0.949   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                       -0.340   \n",
       "target_polynomials_VS_metamodel_functions                   -16.399   \n",
       "target_polynomials_VS_symbolic_regression_funct...            1.000   \n",
       "target_polynomials_VS_per_network_polynomials                 0.951   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            -0.330   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -16.334   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.951   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 -0.340   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -16.399   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            1.000   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.951   \n",
       "inet_polynomials_VS_metamodel_functions                      -3.147   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.597   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.549   \n",
       "metamodel_functions_VS_symbolic_regression_func... -44108388670.875   \n",
       "metamodel_functions_VS_per_network_polynomials     -45126489256.812   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.950   \n",
       "\n",
       "                                                              L-81  \\\n",
       "lambda_preds_VS_target_polynomials                           1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     1.000   \n",
       "lambda_preds_VS_inet_polynomials                             0.954   \n",
       "lambda_preds_VS_metamodel_functions                        -20.752   \n",
       "lambda_preds_VS_symbolic_regression_functions                0.994   \n",
       "lambda_preds_VS_per_network_polynomials                      0.931   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.954   \n",
       "target_polynomials_VS_metamodel_functions                  -20.494   \n",
       "target_polynomials_VS_symbolic_regression_funct...           0.994   \n",
       "target_polynomials_VS_per_network_polynomials                0.932   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.954   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...         -20.660   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...           0.994   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...           0.932   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.954   \n",
       "lstsq_target_polynomials_VS_metamodel_functions            -20.494   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...           0.994   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...           0.932   \n",
       "inet_polynomials_VS_metamodel_functions                    -15.488   \n",
       "inet_polynomials_VS_symbolic_regression_functions            0.976   \n",
       "inet_polynomials_VS_per_network_polynomials                  0.995   \n",
       "metamodel_functions_VS_symbolic_regression_func... -6101031010.661   \n",
       "metamodel_functions_VS_per_network_polynomials     -5753166037.179   \n",
       "symbolic_regression_functions_VS_per_network_po...           0.946   \n",
       "\n",
       "                                                               L-82    L-83  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.771   0.915   \n",
       "lambda_preds_VS_metamodel_functions                        -117.307  -2.300   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.932   0.975   \n",
       "lambda_preds_VS_per_network_polynomials                       0.581   0.981   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.767   0.916   \n",
       "target_polynomials_VS_metamodel_functions                  -116.416  -2.282   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.930   0.974   \n",
       "target_polynomials_VS_per_network_polynomials                 0.576   0.980   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.771   0.915   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...         -117.154  -2.291   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.932   0.975   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.581   0.981   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.767   0.916   \n",
       "lstsq_target_polynomials_VS_metamodel_functions            -116.416  -2.282   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.930   0.974   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.576   0.980   \n",
       "inet_polynomials_VS_metamodel_functions                    -166.108  -2.330   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.874   0.818   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.907   0.819   \n",
       "metamodel_functions_VS_symbolic_regression_func... -15783909911.109 -54.982   \n",
       "metamodel_functions_VS_per_network_polynomials     -16630971678.688 -54.638   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.685   0.998   \n",
       "\n",
       "                                                             L-84  \\\n",
       "lambda_preds_VS_target_polynomials                          0.855   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials               1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    0.855   \n",
       "lambda_preds_VS_inet_polynomials                            0.828   \n",
       "lambda_preds_VS_metamodel_functions                       -25.963   \n",
       "lambda_preds_VS_symbolic_regression_functions               1.000   \n",
       "lambda_preds_VS_per_network_polynomials                     0.931   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...          0.824   \n",
       "target_polynomials_VS_lstsq_target_polynomials              1.000   \n",
       "target_polynomials_VS_inet_polynomials                      0.423   \n",
       "target_polynomials_VS_metamodel_functions                 -32.508   \n",
       "target_polynomials_VS_symbolic_regression_funct...          0.822   \n",
       "target_polynomials_VS_per_network_polynomials               0.580   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          0.855   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           0.827   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...        -25.944   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...          1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...          0.931   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                0.423   \n",
       "lstsq_target_polynomials_VS_metamodel_functions           -32.508   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...          0.822   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...          0.580   \n",
       "inet_polynomials_VS_metamodel_functions                   -36.434   \n",
       "inet_polynomials_VS_symbolic_regression_functions           0.743   \n",
       "inet_polynomials_VS_per_network_polynomials                 0.953   \n",
       "metamodel_functions_VS_symbolic_regression_func... -975058111.284   \n",
       "metamodel_functions_VS_per_network_polynomials     -942792489.729   \n",
       "symbolic_regression_functions_VS_per_network_po...          0.928   \n",
       "\n",
       "                                                               L-85  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.901   \n",
       "lambda_preds_VS_metamodel_functions                         -49.998   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.981   \n",
       "lambda_preds_VS_per_network_polynomials                       0.980   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.898   \n",
       "target_polynomials_VS_metamodel_functions                   -50.223   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.982   \n",
       "target_polynomials_VS_per_network_polynomials                 0.978   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.900   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -50.026   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.982   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.980   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.898   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -50.223   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.982   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.978   \n",
       "inet_polynomials_VS_metamodel_functions                     -66.631   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.830   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.934   \n",
       "metamodel_functions_VS_symbolic_regression_func... -10973363036.109   \n",
       "metamodel_functions_VS_per_network_polynomials     -10860996092.750   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.920   \n",
       "\n",
       "                                                               L-86  \\\n",
       "lambda_preds_VS_target_polynomials                           -3.099   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     -3.099   \n",
       "lambda_preds_VS_inet_polynomials                              0.929   \n",
       "lambda_preds_VS_metamodel_functions                       -1312.758   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.938   \n",
       "lambda_preds_VS_per_network_polynomials                       0.828   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.312   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.446   \n",
       "target_polynomials_VS_metamodel_functions                  -224.861   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.347   \n",
       "target_polynomials_VS_per_network_polynomials                 0.489   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           -3.102   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.930   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...        -1314.247   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.938   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.829   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.446   \n",
       "lstsq_target_polynomials_VS_metamodel_functions            -224.861   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.347   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.489   \n",
       "inet_polynomials_VS_metamodel_functions                   -1473.756   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.815   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.969   \n",
       "metamodel_functions_VS_symbolic_regression_func... -23833110350.562   \n",
       "metamodel_functions_VS_per_network_polynomials     -23891921385.719   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.793   \n",
       "\n",
       "                                                               L-87  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.879   \n",
       "lambda_preds_VS_metamodel_functions                         -48.593   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.998   \n",
       "lambda_preds_VS_per_network_polynomials                       0.787   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.878   \n",
       "target_polynomials_VS_metamodel_functions                   -48.494   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.998   \n",
       "target_polynomials_VS_per_network_polynomials                 0.786   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.879   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -48.605   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.787   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.878   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -48.494   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.998   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.786   \n",
       "inet_polynomials_VS_metamodel_functions                     -64.519   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.853   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.970   \n",
       "metamodel_functions_VS_symbolic_regression_func... -22066728514.625   \n",
       "metamodel_functions_VS_per_network_polynomials     -22790156249.000   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.779   \n",
       "\n",
       "                                                               L-88  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.876   \n",
       "lambda_preds_VS_metamodel_functions                         -30.687   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.994   \n",
       "lambda_preds_VS_per_network_polynomials                       0.795   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.876   \n",
       "target_polynomials_VS_metamodel_functions                   -30.695   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.994   \n",
       "target_polynomials_VS_per_network_polynomials                 0.795   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.876   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -30.699   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.994   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.795   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.876   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -30.695   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.994   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.795   \n",
       "inet_polynomials_VS_metamodel_functions                     -37.459   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.793   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.960   \n",
       "metamodel_functions_VS_symbolic_regression_func... -18698500975.562   \n",
       "metamodel_functions_VS_per_network_polynomials     -19571962889.625   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.751   \n",
       "\n",
       "                                                         L-89       L-90  \\\n",
       "lambda_preds_VS_target_polynomials                      1.000      0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials           1.000      0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                1.000      0.999   \n",
       "lambda_preds_VS_inet_polynomials                        0.908      0.903   \n",
       "lambda_preds_VS_metamodel_functions                    -0.289     -2.536   \n",
       "lambda_preds_VS_symbolic_regression_functions           1.000      0.973   \n",
       "lambda_preds_VS_per_network_polynomials                 0.986      0.970   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...      1.000      1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials          1.000      1.000   \n",
       "target_polynomials_VS_inet_polynomials                  0.909      0.910   \n",
       "target_polynomials_VS_metamodel_functions              -0.289     -2.546   \n",
       "target_polynomials_VS_symbolic_regression_funct...      1.000      0.970   \n",
       "target_polynomials_VS_per_network_polynomials           0.986      0.974   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...      1.000      1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials       0.908      0.905   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...     -0.289     -2.525   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...      1.000      0.973   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...      0.986      0.971   \n",
       "lstsq_target_polynomials_VS_inet_polynomials            0.909      0.910   \n",
       "lstsq_target_polynomials_VS_metamodel_functions        -0.289     -2.546   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...      1.000      0.970   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...      0.986      0.974   \n",
       "inet_polynomials_VS_metamodel_functions                -0.260     -3.309   \n",
       "inet_polynomials_VS_symbolic_regression_functions       0.899      0.814   \n",
       "inet_polynomials_VS_per_network_polynomials             0.947      0.970   \n",
       "metamodel_functions_VS_symbolic_regression_func... -13783.799 -43974.504   \n",
       "metamodel_functions_VS_per_network_polynomials     -11865.495 -51721.150   \n",
       "symbolic_regression_functions_VS_per_network_po...      0.986      0.901   \n",
       "\n",
       "                                                       L-91      L-92  \\\n",
       "lambda_preds_VS_target_polynomials                    0.926     1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         1.000     1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials              0.926     1.000   \n",
       "lambda_preds_VS_inet_polynomials                      0.821   -10.757   \n",
       "lambda_preds_VS_metamodel_functions                  -7.087    -3.983   \n",
       "lambda_preds_VS_symbolic_regression_functions         0.999     0.981   \n",
       "lambda_preds_VS_per_network_polynomials               0.930     0.659   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    0.912     1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials        1.000     1.000   \n",
       "target_polynomials_VS_inet_polynomials                0.939   -10.971   \n",
       "target_polynomials_VS_metamodel_functions            -8.747    -4.036   \n",
       "target_polynomials_VS_symbolic_regression_funct...    0.916     0.984   \n",
       "target_polynomials_VS_per_network_polynomials         0.989     0.659   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    0.926     1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     0.821   -10.767   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   -7.094    -3.987   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...    0.999     0.981   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...    0.931     0.658   \n",
       "lstsq_target_polynomials_VS_inet_polynomials          0.939   -10.971   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      -8.747    -4.036   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...    0.916     0.984   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...    0.989     0.659   \n",
       "inet_polynomials_VS_metamodel_functions              -9.757    -4.725   \n",
       "inet_polynomials_VS_symbolic_regression_functions     0.781    -1.229   \n",
       "inet_polynomials_VS_per_network_polynomials           0.965    -1.213   \n",
       "metamodel_functions_VS_symbolic_regression_func... -532.612 -2254.792   \n",
       "metamodel_functions_VS_per_network_polynomials     -561.594 -2075.604   \n",
       "symbolic_regression_functions_VS_per_network_po...    0.928     0.687   \n",
       "\n",
       "                                                        L-93      L-94  \\\n",
       "lambda_preds_VS_target_polynomials                     0.999     1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials          1.000     1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials               0.999     1.000   \n",
       "lambda_preds_VS_inet_polynomials                       0.297    -2.649   \n",
       "lambda_preds_VS_metamodel_functions                  -21.009   -60.223   \n",
       "lambda_preds_VS_symbolic_regression_functions         -0.188     0.986   \n",
       "lambda_preds_VS_per_network_polynomials                0.329     0.774   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...     1.000     1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials         1.000     1.000   \n",
       "target_polynomials_VS_inet_polynomials                 0.317    -2.623   \n",
       "target_polynomials_VS_metamodel_functions            -20.875   -60.108   \n",
       "target_polynomials_VS_symbolic_regression_funct...    -0.181     0.987   \n",
       "target_polynomials_VS_per_network_polynomials          0.337     0.775   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...     1.000     1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials      0.299    -2.640   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   -21.014   -60.108   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...    -0.190     0.987   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...     0.329     0.774   \n",
       "lstsq_target_polynomials_VS_inet_polynomials           0.317    -2.623   \n",
       "lstsq_target_polynomials_VS_metamodel_functions      -20.875   -60.108   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...    -0.181     0.987   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...     0.337     0.775   \n",
       "inet_polynomials_VS_metamodel_functions              -45.646 -3062.261   \n",
       "inet_polynomials_VS_symbolic_regression_functions      0.110  -122.114   \n",
       "inet_polynomials_VS_per_network_polynomials            0.769  -138.603   \n",
       "metamodel_functions_VS_symbolic_regression_func... -1309.845 -3129.347   \n",
       "metamodel_functions_VS_per_network_polynomials     -1208.143 -2991.997   \n",
       "symbolic_regression_functions_VS_per_network_po...   -28.027     0.826   \n",
       "\n",
       "                                                               L-95     L-96  \\\n",
       "lambda_preds_VS_target_polynomials                            0.992    0.929   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 0.993    1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.992    0.929   \n",
       "lambda_preds_VS_inet_polynomials                              0.548    0.838   \n",
       "lambda_preds_VS_metamodel_functions                         -11.184   -5.134   \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.991    0.991   \n",
       "lambda_preds_VS_per_network_polynomials                       0.982    0.930   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.999    0.932   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000    1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.525    0.606   \n",
       "target_polynomials_VS_metamodel_functions                   -11.154   -4.640   \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.995    0.919   \n",
       "target_polynomials_VS_per_network_polynomials                 0.994    0.744   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.999    0.930   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.560    0.838   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          -11.078   -5.132   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.998    0.991   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.989    0.930   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.525    0.606   \n",
       "lstsq_target_polynomials_VS_metamodel_functions             -11.154   -4.640   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.995    0.919   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.994    0.744   \n",
       "inet_polynomials_VS_metamodel_functions                      -5.318   -6.564   \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.675    0.820   \n",
       "inet_polynomials_VS_per_network_polynomials                   0.628    0.963   \n",
       "metamodel_functions_VS_symbolic_regression_func... -21841235350.562 -166.838   \n",
       "metamodel_functions_VS_per_network_polynomials     -22045458983.375 -169.581   \n",
       "symbolic_regression_functions_VS_per_network_po...            0.979    0.935   \n",
       "\n",
       "                                                        L-97    L-98  \\\n",
       "lambda_preds_VS_target_polynomials                    -0.809   1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials          1.000   1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials              -0.809   1.000   \n",
       "lambda_preds_VS_inet_polynomials                       0.835   0.952   \n",
       "lambda_preds_VS_metamodel_functions                   -9.867  -0.202   \n",
       "lambda_preds_VS_symbolic_regression_functions          0.995   0.999   \n",
       "lambda_preds_VS_per_network_polynomials                0.930   0.874   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...     0.098   1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials         1.000   1.000   \n",
       "target_polynomials_VS_inet_polynomials                -0.356   0.952   \n",
       "target_polynomials_VS_metamodel_functions             -3.600  -0.201   \n",
       "target_polynomials_VS_symbolic_regression_funct...     0.115   0.999   \n",
       "target_polynomials_VS_per_network_polynomials         -0.216   0.875   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    -0.809   1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials      0.835   0.952   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    -9.867  -0.201   \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...     0.995   0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...     0.930   0.875   \n",
       "lstsq_target_polynomials_VS_inet_polynomials          -0.356   0.952   \n",
       "lstsq_target_polynomials_VS_metamodel_functions       -3.600  -0.201   \n",
       "lstsq_target_polynomials_VS_symbolic_regression...     0.115   0.999   \n",
       "lstsq_target_polynomials_VS_per_network_polynom...    -0.216   0.875   \n",
       "inet_polynomials_VS_metamodel_functions              -11.792  -0.191   \n",
       "inet_polynomials_VS_symbolic_regression_functions      0.821   0.948   \n",
       "inet_polynomials_VS_per_network_polynomials            0.953   0.944   \n",
       "metamodel_functions_VS_symbolic_regression_func... -1988.993 -59.312   \n",
       "metamodel_functions_VS_per_network_polynomials     -2062.254 -32.927   \n",
       "symbolic_regression_functions_VS_per_network_po...     0.931   0.875   \n",
       "\n",
       "                                                               L-99  \n",
       "lambda_preds_VS_target_polynomials                            1.000  \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000  \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000  \n",
       "lambda_preds_VS_inet_polynomials                              0.856  \n",
       "lambda_preds_VS_metamodel_functions                          -4.850  \n",
       "lambda_preds_VS_symbolic_regression_functions                 0.998  \n",
       "lambda_preds_VS_per_network_polynomials                       0.998  \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000  \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000  \n",
       "target_polynomials_VS_inet_polynomials                        0.857  \n",
       "target_polynomials_VS_metamodel_functions                    -4.840  \n",
       "target_polynomials_VS_symbolic_regression_funct...            0.998  \n",
       "target_polynomials_VS_per_network_polynomials                 0.998  \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000  \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.856  \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           -4.843  \n",
       "lstsq_lambda_pred_polynomials_VS_symbolic_regre...            0.998  \n",
       "lstsq_lambda_pred_polynomials_VS_per_network_po...            0.998  \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.857  \n",
       "lstsq_target_polynomials_VS_metamodel_functions              -4.840  \n",
       "lstsq_target_polynomials_VS_symbolic_regression...            0.998  \n",
       "lstsq_target_polynomials_VS_per_network_polynom...            0.998  \n",
       "inet_polynomials_VS_metamodel_functions                      -7.892  \n",
       "inet_polynomials_VS_symbolic_regression_functions             0.831  \n",
       "inet_polynomials_VS_per_network_polynomials                   0.830  \n",
       "metamodel_functions_VS_symbolic_regression_func... -16732579344.703  \n",
       "metamodel_functions_VS_per_network_polynomials     -16792751463.844  \n",
       "symbolic_regression_functions_VS_per_network_po...            1.000  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:37.805311Z",
     "iopub.status.busy": "2021-10-21T09:30:37.805046Z",
     "iopub.status.idle": "2021-10-21T09:30:39.625334Z",
     "shell.execute_reply": "2021-10-21T09:30:39.624575Z",
     "shell.execute_reply.started": "2021-10-21T09:30:37.805280Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9932328376406159\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.539110194823053 a^{3} - 0.785713704601377 a^{2} + 0.0248992101784917 a - 0.387545611168351$"
      ],
      "text/plain": [
       "-0.539110194823053*a**3 - 0.785713704601377*a**2 + 0.0248992101784917*a - 0.387545611168351"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:39.714656Z",
     "iopub.status.busy": "2021-10-21T09:30:39.714381Z",
     "iopub.status.idle": "2021-10-21T09:30:39.779466Z",
     "shell.execute_reply": "2021-10-21T09:30:39.777142Z",
     "shell.execute_reply.started": "2021-10-21T09:30:39.714620Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.4738107138505952 (100 Samples)\n",
      "Mean (only positive): 0.8493011988038852 (88 Samples)\n"
     ]
    }
   ],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:39.844882Z",
     "iopub.status.busy": "2021-10-21T09:30:39.844655Z",
     "iopub.status.idle": "2021-10-21T09:30:39.910344Z",
     "shell.execute_reply": "2021-10-21T09:30:39.907134Z",
     "shell.execute_reply.started": "2021-10-21T09:30:39.844849Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.9996610364106217 (100 Samples)\n",
      "Mean (only positive): 0.9996610364106217 (100 Samples)\n"
     ]
    }
   ],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:40.348335Z",
     "iopub.status.busy": "2021-10-21T09:30:40.348100Z",
     "iopub.status.idle": "2021-10-21T09:30:40.721559Z",
     "shell.execute_reply": "2021-10-21T09:30:40.720840Z",
     "shell.execute_reply.started": "2021-10-21T09:30:40.348302Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:41.093797Z",
     "iopub.status.busy": "2021-10-21T09:30:41.093571Z",
     "iopub.status.idle": "2021-10-21T09:30:41.460573Z",
     "shell.execute_reply": "2021-10-21T09:30:41.459714Z",
     "shell.execute_reply.started": "2021-10-21T09:30:41.093766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:42.536599Z",
     "iopub.status.busy": "2021-10-21T09:30:42.536323Z",
     "iopub.status.idle": "2021-10-21T09:30:42.933552Z",
     "shell.execute_reply": "2021-10-21T09:30:42.932751Z",
     "shell.execute_reply.started": "2021-10-21T09:30:42.536568Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:43.422794Z",
     "iopub.status.busy": "2021-10-21T09:30:43.422508Z",
     "iopub.status.idle": "2021-10-21T09:30:43.932545Z",
     "shell.execute_reply": "2021-10-21T09:30:43.931523Z",
     "shell.execute_reply.started": "2021-10-21T09:30:43.422760Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    try:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:44.418258Z",
     "iopub.status.busy": "2021-10-21T09:30:44.417945Z",
     "iopub.status.idle": "2021-10-21T09:30:44.914685Z",
     "shell.execute_reply": "2021-10-21T09:30:44.911508Z",
     "shell.execute_reply.started": "2021-10-21T09:30:44.418221Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:44.999317Z",
     "iopub.status.busy": "2021-10-21T09:30:44.999093Z",
     "iopub.status.idle": "2021-10-21T09:30:45.067588Z",
     "shell.execute_reply": "2021-10-21T09:30:45.066549Z",
     "shell.execute_reply.started": "2021-10-21T09:30:44.999287Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:45.136566Z",
     "iopub.status.busy": "2021-10-21T09:30:45.136347Z",
     "iopub.status.idle": "2021-10-21T09:30:45.203475Z",
     "shell.execute_reply": "2021-10-21T09:30:45.202644Z",
     "shell.execute_reply.started": "2021-10-21T09:30:45.136536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:45.265339Z",
     "iopub.status.busy": "2021-10-21T09:30:45.265108Z",
     "iopub.status.idle": "2021-10-21T09:30:45.325930Z",
     "shell.execute_reply": "2021-10-21T09:30:45.325107Z",
     "shell.execute_reply.started": "2021-10-21T09:30:45.265308Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:30:45.460677Z",
     "iopub.status.busy": "2021-10-21T09:30:45.460458Z",
     "iopub.status.idle": "2021-10-21T09:30:45.585782Z",
     "shell.execute_reply": "2021-10-21T09:30:45.584922Z",
     "shell.execute_reply.started": "2021-10-21T09:30:45.460649Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "lstsq_lambda_pred_polynomials\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.0016 a^{3} + 0.0026 a^{2} - 0.1355 a - 0.1872$"
      ],
      "text/plain": [
       "-0.0016*a**3 + 0.0026*a**2 - 0.1355*a - 0.1872"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00156717  0.00264205 -0.13553554 -0.18724118]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "lstsq_target_polynomials\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.1772 a^{3} - 0.4945 a^{2} + 0.2191 a - 0.2497$"
      ],
      "text/plain": [
       "0.1772*a**3 - 0.4945*a**2 + 0.2191*a - 0.2497"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17723667 -0.49447368  0.2191118  -0.2496587 ]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "target_polynomials\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.1772 a^{3} - 0.4945 a^{2} + 0.2191 a - 0.2497$"
      ],
      "text/plain": [
       "0.1772*a**3 - 0.4945*a**2 + 0.2191*a - 0.2497"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17723666 -0.49447366  0.2191118  -0.2496587 ]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "inet_polynomials\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.125 a^{3} - 0.221399992704391$"
      ],
      "text/plain": [
       "-0.125*a**3 - 0.221399992704391"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1250201  -0.22136208  0.27938354  0.26945674  0.24908274  0.20207691\n",
      "  0.24928103  0.25487125  0.22921409  0.2666337 ]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "metamodel_functions\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1.00000000015586 e^{4.08826364818537 \\cdot 10^{-9} X_{0}^{3} + 1.41276497735523 \\cdot 10^{-8} X_{0}^{2} - 1.65124263865496 \\cdot 10^{-9} X_{0}} + 1}$"
      ],
      "text/plain": [
       "1/(1.00000000015586*exp(4.08826364818537e-9*X0**3 + 1.41276497735523e-8*X0**2 - 1.65124263865496e-9*X0) + 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "symbolic_regression_functions\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{1.367 X_{0} + 0.313}{3.73267326732673 X_{0} + 1.92946058091286}$"
      ],
      "text/plain": [
       "-(1.367*X0 + 0.313)/(3.73267326732673*X0 + 1.92946058091286)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "per_network_polynomials\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.1199000030756 a^{3} - 0.228699997067451$"
      ],
      "text/plain": [
       "-0.1199000030756*a**3 - 0.228699997067451"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.22869956 -0.11990047  7.960468    7.530823    7.821873   12.244263\n",
      " 12.490771   11.978285   11.738708    7.5609183 ], shape=(10,), dtype=float32)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T15:03:00.984897Z",
     "iopub.status.busy": "2021-10-21T15:03:00.984535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1268.62x540 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    clear_output(wait=True)\n",
    "    plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                          function_values_test_list, \n",
    "                                                          polynomial_dict_test_list,\n",
    "                                                          rand_index=i, \n",
    "                                                          plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-21T15:01:08.843694Z",
     "iopub.status.busy": "2021-10-21T15:01:08.843394Z",
     "iopub.status.idle": "2021-10-21T15:01:11.967874Z",
     "shell.execute_reply": "2021-10-21T15:01:11.965110Z",
     "shell.execute_reply.started": "2021-10-21T15:01:08.843663Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1268.62x540 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-21T09:31:04.048446Z",
     "iopub.status.busy": "2021-10-21T09:31:04.046994Z",
     "iopub.status.idle": "2021-10-21T09:31:13.377810Z",
     "shell.execute_reply": "2021-10-21T09:31:13.376984Z",
     "shell.execute_reply.started": "2021-10-21T09:31:04.048405Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1634.75x1440 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-21T09:31:17.825647Z",
     "iopub.status.busy": "2021-10-21T09:31:17.825272Z",
     "iopub.status.idle": "2021-10-21T09:31:22.724290Z",
     "shell.execute_reply": "2021-10-21T09:31:22.721408Z",
     "shell.execute_reply.started": "2021-10-21T09:31:17.825619Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x1944 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-21T09:31:22.836260Z",
     "iopub.status.idle": "2021-10-21T09:31:22.836692Z",
     "shell.execute_reply": "2021-10-21T09:31:22.836492Z",
     "shell.execute_reply.started": "2021-10-21T09:31:22.836471Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    },
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.210781Z",
     "iopub.status.idle": "2021-10-20T17:14:43.213185Z",
     "shell.execute_reply": "2021-10-20T17:14:43.212994Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.212971Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    },
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.213925Z",
     "iopub.status.idle": "2021-10-20T17:14:43.214252Z",
     "shell.execute_reply": "2021-10-20T17:14:43.214084Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.214066Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    },
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.214914Z",
     "iopub.status.idle": "2021-10-20T17:14:43.215259Z",
     "shell.execute_reply": "2021-10-20T17:14:43.215087Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.215068Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    },
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.215881Z",
     "iopub.status.idle": "2021-10-20T17:14:43.216206Z",
     "shell.execute_reply": "2021-10-20T17:14:43.216042Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.216024Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    },
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.216855Z",
     "iopub.status.idle": "2021-10-20T17:14:43.217178Z",
     "shell.execute_reply": "2021-10-20T17:14:43.217010Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.216993Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ],
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.217893Z",
     "iopub.status.idle": "2021-10-20T17:14:43.218219Z",
     "shell.execute_reply": "2021-10-20T17:14:43.218053Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.218034Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.218913Z",
     "iopub.status.idle": "2021-10-20T17:14:43.219237Z",
     "shell.execute_reply": "2021-10-20T17:14:43.219072Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.219054Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if evaluate_with_real_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.219879Z",
     "iopub.status.idle": "2021-10-20T17:14:43.220136Z",
     "shell.execute_reply": "2021-10-20T17:14:43.220020Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.220006Z"
    }
   },
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.221537Z",
     "iopub.status.idle": "2021-10-20T17:14:43.221880Z",
     "shell.execute_reply": "2021-10-20T17:14:43.221710Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.221691Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.222578Z",
     "iopub.status.idle": "2021-10-20T17:14:43.222909Z",
     "shell.execute_reply": "2021-10-20T17:14:43.222741Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.222723Z"
    }
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.224549Z",
     "iopub.status.idle": "2021-10-20T17:14:43.224952Z",
     "shell.execute_reply": "2021-10-20T17:14:43.224748Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.224726Z"
    }
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.225742Z",
     "iopub.status.idle": "2021-10-20T17:14:43.226124Z",
     "shell.execute_reply": "2021-10-20T17:14:43.225926Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.225906Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.226932Z",
     "iopub.status.idle": "2021-10-20T17:14:43.227320Z",
     "shell.execute_reply": "2021-10-20T17:14:43.227121Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.227099Z"
    }
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.228095Z",
     "iopub.status.idle": "2021-10-20T17:14:43.228484Z",
     "shell.execute_reply": "2021-10-20T17:14:43.228277Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.228256Z"
    }
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.229283Z",
     "iopub.status.idle": "2021-10-20T17:14:43.229701Z",
     "shell.execute_reply": "2021-10-20T17:14:43.229476Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.229454Z"
    }
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.230511Z",
     "iopub.status.idle": "2021-10-20T17:14:43.230902Z",
     "shell.execute_reply": "2021-10-20T17:14:43.230698Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.230677Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.231824Z",
     "iopub.status.idle": "2021-10-20T17:14:43.232232Z",
     "shell.execute_reply": "2021-10-20T17:14:43.232020Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.232000Z"
    }
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.236678Z",
     "iopub.status.idle": "2021-10-20T17:14:43.237146Z",
     "shell.execute_reply": "2021-10-20T17:14:43.236933Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.236911Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.238451Z",
     "iopub.status.idle": "2021-10-20T17:14:43.238862Z",
     "shell.execute_reply": "2021-10-20T17:14:43.238651Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.238628Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.240050Z",
     "iopub.status.idle": "2021-10-20T17:14:43.241168Z",
     "shell.execute_reply": "2021-10-20T17:14:43.240988Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.240967Z"
    }
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.243732Z",
     "iopub.status.idle": "2021-10-20T17:14:43.244094Z",
     "shell.execute_reply": "2021-10-20T17:14:43.243918Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.243898Z"
    }
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.244837Z",
     "iopub.status.idle": "2021-10-20T17:14:43.245162Z",
     "shell.execute_reply": "2021-10-20T17:14:43.244996Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.244978Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.245845Z",
     "iopub.status.idle": "2021-10-20T17:14:43.246174Z",
     "shell.execute_reply": "2021-10-20T17:14:43.246003Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.245986Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.246838Z",
     "iopub.status.idle": "2021-10-20T17:14:43.247146Z",
     "shell.execute_reply": "2021-10-20T17:14:43.246996Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.246977Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.247715Z",
     "iopub.status.idle": "2021-10-20T17:14:43.248055Z",
     "shell.execute_reply": "2021-10-20T17:14:43.247885Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.247866Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.248750Z",
     "iopub.status.idle": "2021-10-20T17:14:43.249091Z",
     "shell.execute_reply": "2021-10-20T17:14:43.248921Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.248902Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.249778Z",
     "iopub.status.idle": "2021-10-20T17:14:43.250109Z",
     "shell.execute_reply": "2021-10-20T17:14:43.249936Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.249918Z"
    }
   },
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.251234Z",
     "iopub.status.idle": "2021-10-20T17:14:43.252100Z",
     "shell.execute_reply": "2021-10-20T17:14:43.251929Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.251900Z"
    }
   },
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.252823Z",
     "iopub.status.idle": "2021-10-20T17:14:43.254262Z",
     "shell.execute_reply": "2021-10-20T17:14:43.254055Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.254027Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.258045Z",
     "iopub.status.idle": "2021-10-20T17:14:43.258340Z",
     "shell.execute_reply": "2021-10-20T17:14:43.258220Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.258205Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.261536Z",
     "iopub.status.idle": "2021-10-20T17:14:43.264173Z",
     "shell.execute_reply": "2021-10-20T17:14:43.263943Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.263916Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.267107Z",
     "iopub.status.idle": "2021-10-20T17:14:43.267540Z",
     "shell.execute_reply": "2021-10-20T17:14:43.267317Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.267295Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T17:14:43.268912Z",
     "iopub.status.idle": "2021-10-20T17:14:43.269309Z",
     "shell.execute_reply": "2021-10-20T17:14:43.269107Z",
     "shell.execute_reply.started": "2021-10-20T17:14:43.269086Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
