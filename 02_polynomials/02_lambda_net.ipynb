{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:32.478396Z",
     "iopub.status.busy": "2021-09-24T16:23:32.478047Z",
     "iopub.status.idle": "2021-09-24T16:23:32.500583Z",
     "shell.execute_reply": "2021-09-24T16:23:32.497002Z",
     "shell.execute_reply.started": "2021-09-24T16:23:32.478306Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 3, #degree\n",
    "        'n': 2, #number of variables\n",
    "        'monomial_vars': None, #int or None\n",
    "        'laurent': False, #use Laurent polynomials (negative degree with up to -d)\n",
    "        'neg_d': 0,#int or None\n",
    "        'neg_d_prob': 0,\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': 2,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 1,\n",
    "        'a_min': -1,\n",
    "        'polynomial_data_size': 10000,  #number of generated polynomials (for loading)\n",
    "        'lambda_nets_total': 10000, #number of lambda-nets to train\n",
    "        'noise': 0,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'border_min': 0.2, #needs to be between 0 and (x_max-x_min)/2\n",
    "        'border_max': 0.4,\n",
    "        'lower_degree_prob': 0.5,\n",
    "        'a_zero_prob': 0.25,\n",
    "        'a_random_prob': 0.1,         \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sample_sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000, #lambda-net training dataset size\n",
    "    },    \n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': False,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "    \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "    },    \n",
    "    'computation':{\n",
    "        'n_jobs': 40,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:32.501847Z",
     "iopub.status.busy": "2021-09-24T16:23:32.501600Z",
     "iopub.status.idle": "2021-09-24T16:23:32.510683Z",
     "shell.execute_reply": "2021-09-24T16:23:32.510052Z",
     "shell.execute_reply.started": "2021-09-24T16:23:32.501813Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:32.513089Z",
     "iopub.status.busy": "2021-09-24T16:23:32.512727Z",
     "iopub.status.idle": "2021-09-24T16:23:35.689193Z",
     "shell.execute_reply": "2021-09-24T16:23:35.688065Z",
     "shell.execute_reply.started": "2021-09-24T16:23:32.513054Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-24 18:23:34.113394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "\n",
    "\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:35.690844Z",
     "iopub.status.busy": "2021-09-24T16:23:35.690641Z",
     "iopub.status.idle": "2021-09-24T16:23:35.699231Z",
     "shell.execute_reply": "2021-09-24T16:23:35.698511Z",
     "shell.execute_reply.started": "2021-09-24T16:23:35.690820Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n] \n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "    \n",
    "    \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:35.700458Z",
     "iopub.status.busy": "2021-09-24T16:23:35.700234Z",
     "iopub.status.idle": "2021-09-24T16:23:36.440593Z",
     "shell.execute_reply": "2021-09-24T16:23:36.439549Z",
     "shell.execute_reply.started": "2021-09-24T16:23:35.700437Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 10\n",
      "[[3, 0], [2, 1], [2, 0], [1, 2], [1, 1], [1, 0], [0, 3], [0, 2], [0, 1], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import flatten, rec_gen, gen_monomial_identifier_list\n",
    "\n",
    "list_of_monomial_identifiers_extended = []\n",
    "\n",
    "if laurent:\n",
    "    variable_sets = [list(flatten([[_d for _d in range(d+1)], [-_d for _d in range(1, neg_d+1)]])) for _ in range(n)]\n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)    \n",
    "        \n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity:' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)     \n",
    "        \n",
    "    list_of_monomial_identifiers = []\n",
    "    for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "        if np.sum(monomial_identifier) <= d:\n",
    "            if monomial_vars == None or len(list(filter(lambda x: x != 0, monomial_identifier))) <= monomial_vars:\n",
    "                list_of_monomial_identifiers.append(monomial_identifier)        \n",
    "else:\n",
    "    variable_list = ['x'+ str(i) for i in range(n)]\n",
    "    list_of_monomial_identifiers = gen_monomial_identifier_list(variable_list, d, n)\n",
    "            \n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "#print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "#print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:36.442082Z",
     "iopub.status.busy": "2021-09-24T16:23:36.441907Z",
     "iopub.status.idle": "2021-09-24T16:23:36.455752Z",
     "shell.execute_reply": "2021-09-24T16:23:36.454733Z",
     "shell.execute_reply.started": "2021-09-24T16:23:36.442059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'KeyError'>\n",
      "<class 'KeyError'>\n",
      "<class 'KeyError'>\n"
     ]
    }
   ],
   "source": [
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d']) if not laurent else len(list_of_monomial_identifiers)\n",
    "\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "    \n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sample_sparsity')*config['data']['sample_sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='lambda_net'))\n",
    "generate_directory_structure()\n",
    "generate_lambda_net_directory()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:36.456821Z",
     "iopub.status.busy": "2021-09-24T16:23:36.456659Z",
     "iopub.status.idle": "2021-09-24T16:23:36.461484Z",
     "shell.execute_reply": "2021-09-24T16:23:36.460734Z",
     "shell.execute_reply.started": "2021-09-24T16:23:36.456800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lnets_10000_10-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_2_d_3_negd_0_prob_0_spars_2_amin_-1_amax_1_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n",
      "poly_10000_train_5000_var_2_d_3_negd_0_prob_0_spars_2_amin_-1_amax_1_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1_diffX\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_lambda_net_data)\n",
    "\n",
    "print(path_identifier_polynomial_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:36.462632Z",
     "iopub.status.busy": "2021-09-24T16:23:36.462469Z",
     "iopub.status.idle": "2021-09-24T16:23:36.489971Z",
     "shell.execute_reply": "2021-09-24T16:23:36.488659Z",
     "shell.execute_reply.started": "2021-09-24T16:23:36.462610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-24 18:23:36.463755: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:36.491209Z",
     "iopub.status.busy": "2021-09-24T16:23:36.490958Z",
     "iopub.status.idle": "2021-09-24T16:23:36.498167Z",
     "shell.execute_reply": "2021-09-24T16:23:36.497467Z",
     "shell.execute_reply.started": "2021-09-24T16:23:36.491187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_network_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:11.182937Z",
     "start_time": "2021-01-17T09:44:31.797522Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:36.499475Z",
     "iopub.status.busy": "2021-09-24T16:23:36.499119Z",
     "iopub.status.idle": "2021-09-24T16:23:47.633178Z",
     "shell.execute_reply": "2021-09-24T16:23:47.632276Z",
     "shell.execute_reply.started": "2021-09-24T16:23:36.499452Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-24 18:23:36.464857: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-24 18:23:36.484140: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-24 18:23:36.484169: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: dws-07\n",
      "2021-09-24 18:23:36.484176: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: dws-07\n",
      "2021-09-24 18:23:36.484287: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.67.0\n",
      "2021-09-24 18:23:36.484315: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.67.0\n",
      "2021-09-24 18:23:36.484321: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.67.0\n"
     ]
    }
   ],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample_' + path_identifier_polynomial_data + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if lambda_nets_total < polynomial_data_size:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=lambda_nets_total, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, lambda_nets_total)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, lambda_nets_total)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:12.626401Z",
     "start_time": "2021-01-17T09:46:12.608200Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:47.634965Z",
     "iopub.status.busy": "2021-09-24T16:23:47.634659Z",
     "iopub.status.idle": "2021-09-24T16:23:47.644193Z",
     "shell.execute_reply": "2021-09-24T16:23:47.643731Z",
     "shell.execute_reply.started": "2021-09-24T16:23:47.634938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.603</td>\n",
       "      <td>0.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.424</td>\n",
       "      <td>0.646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.964</td>\n",
       "      <td>0.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.792</td>\n",
       "      <td>0.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.568</td>\n",
       "      <td>0.926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.071</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.778</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a     b\n",
       "0 0.549 0.715\n",
       "1 0.603 0.545\n",
       "2 0.424 0.646\n",
       "3 0.438 0.892\n",
       "4 0.964 0.383\n",
       "5 0.792 0.529\n",
       "6 0.568 0.926\n",
       "7 0.071 0.087\n",
       "8 0.020 0.833\n",
       "9 0.778 0.870"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:12.636995Z",
     "start_time": "2021-01-17T09:46:12.629349Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:47.646716Z",
     "iopub.status.busy": "2021-09-24T16:23:47.646523Z",
     "iopub.status.idle": "2021-09-24T16:23:47.652765Z",
     "shell.execute_reply": "2021-09-24T16:23:47.652294Z",
     "shell.execute_reply.started": "2021-09-24T16:23:47.646682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   result\n",
       "0   0.823\n",
       "1   0.681\n",
       "2   0.723\n",
       "3   0.955\n",
       "4   0.637\n",
       "5   0.721\n",
       "6   1.024\n",
       "7   0.102\n",
       "8   0.778\n",
       "9   1.034"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.853Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:47.653833Z",
     "iopub.status.busy": "2021-09-24T16:23:47.653653Z",
     "iopub.status.idle": "2021-09-24T16:23:47.658778Z",
     "shell.execute_reply": "2021-09-24T16:23:47.658297Z",
     "shell.execute_reply.started": "2021-09-24T16:23:47.653809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30   0.000\n",
       "21   0.000\n",
       "20   0.000\n",
       "12   0.000\n",
       "11   0.000\n",
       "10   0.292\n",
       "03   0.000\n",
       "02   0.000\n",
       "01   0.927\n",
       "00   0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.855Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:47.659828Z",
     "iopub.status.busy": "2021-09-24T16:23:47.659653Z",
     "iopub.status.idle": "2021-09-24T16:23:47.665750Z",
     "shell.execute_reply": "2021-09-24T16:23:47.664914Z",
     "shell.execute_reply.started": "2021-09-24T16:23:47.659804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30   0.000\n",
       "21   0.000\n",
       "20   0.000\n",
       "12   0.000\n",
       "11   0.000\n",
       "10   0.292\n",
       "03   0.000\n",
       "02   0.000\n",
       "01   0.927\n",
       "00   0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.863Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T16:23:47.666925Z",
     "iopub.status.busy": "2021-09-24T16:23:47.666647Z",
     "iopub.status.idle": "2021-09-24T17:23:23.208515Z",
     "shell.execute_reply": "2021-09-24T17:23:23.207424Z",
     "shell.execute_reply.started": "2021-09-24T16:23:47.666900Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed32a28fb9104417a4f67ebb29a3fdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "2021-09-24 18:23:49.660211: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.667357: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.674059: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.679144: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.684494: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.692969: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.693265: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.701167: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.703267: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.704490: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.712534: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.722505: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.724557: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.735263: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.740409: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.742622: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.749837: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.753194: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.774376: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.780671: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.787574: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.794031: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.794157: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.808099: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.810636: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.822333: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.827952: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.829292: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.836855: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.848493: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.849508: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.852594: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.858622: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.862464: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.870904: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.889342: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.894745: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.897624: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.903632: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.908166: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.909125: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.914172: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.914217: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.914307: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.937428: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.943497: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.974058: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.978354: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.980336: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.981361: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:49.984094: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:49.992091: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.000994: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.018309: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.021452: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.064941: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.068212: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.071775: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.079190: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.079483: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.079915: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.083416: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.106850: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.116626: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.145142: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.150128: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.150507: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.157694: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.160886: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.169050: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.190153: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.208392: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.209490: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.213931: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.215371: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:23:50.236391: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.263288: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.290940: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.313472: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.347791: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:23:50.370586: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.388010: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.388014: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.389225: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.389225: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.390906: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.434459: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.436291: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.449308: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.450631: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.453338: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.454377: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.457478: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.458448: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.483476: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.483920: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.484539: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.484683: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.487278: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.488152: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.510504: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.512141: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.536982: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.538178: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.556044: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.557224: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.558771: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.560179: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.594568: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.596712: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.596985: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.596990: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.597663: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.597836: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.605832: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.607013: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.625589: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.626841: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.637936: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.637936: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.638983: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.638983: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.642340: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.643360: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.673675: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.674777: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.685731: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.686861: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.691201: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.692132: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.702326: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.703411: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.706125: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.707026: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.732489: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.733757: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.764543: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.765649: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.797798: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.798373: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.799067: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.799170: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.799234: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.799860: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.815127: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.816221: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.816900: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.817772: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.818561: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.819417: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.876710: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.877823: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.893054: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.894182: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:50.924821: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:50.925927: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:51.026559: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:51.027866: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:51.039341: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:23:51.040631: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:23:58.446861: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850f6a220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:23:58.446918: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:23:58.560121: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:23:59.747817: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:23:59.747876: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:23:59.786689: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:23:59.786757: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:23:59.790516: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80feb81d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:23:59.790566: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:23:59.845458: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:23:59.935696: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:23:59.942291: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:00.249317: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85105c9b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:00.249397: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:00.345139: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:00.587507: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:00.587576: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:00.592503: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85104e710 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:00.592622: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:00.667985: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b9c7f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:00.668052: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:00.689313: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:00.691723: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:00.761039: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:00.944870: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80ffd2b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:00.944944: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:01.069925: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:01.803480: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b58000 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:01.803551: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:01.845340: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:01.845396: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:01.995213: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:02.018649: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:02.341835: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:02.341892: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:02.531741: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:02.666167: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:02.666280: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:02.863866: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:03.403039: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80ff7dbb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:03.403094: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:03.499930: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:05.198950: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859bb5ea0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:05.199013: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:05.262564: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510908f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:05.262628: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:05.298661: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:05.372456: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:05.636783: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850fa7430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:05.636848: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:05.759033: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:05.923127: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:05.923180: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:06.032759: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:06.237082: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8100c0e30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:06.238577: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:06.472243: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:06.483309: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:06.483369: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:06.586084: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:06.586131: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:06.633373: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:06.691817: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:06.922588: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:06.922778: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:07.014564: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:07.103408: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c369d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:07.103463: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:07.190402: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:07.228217: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80ff5e870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:07.228290: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:07.316995: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:07.333399: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80ff7ae00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:07.333460: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:07.468094: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:08.095195: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:08.095255: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:08.206773: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:08.468422: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:08.468477: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:08.572650: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:08.711095: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80cd72b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:08.711183: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:08.835786: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:08.974943: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859bd2d20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:08.974998: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:08.987118: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850fb01c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:08.987188: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:09.096823: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:09.106611: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:09.373760: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:09.373940: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:09.480567: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:09.834224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85109aba0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:09.834298: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:10.025592: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:11.173150: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b54ad0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:11.173198: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:11.247554: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:11.247621: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:11.264787: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:11.485552: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:12.529778: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:12.529869: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:12.628244: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:12.777916: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85108ec50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:12.778489: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:12.876448: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:16.125828: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859f62390 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:16.125888: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:16.273156: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:18.298245: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:18.298315: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:18.401492: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:   28.8s\n",
      "2021-09-24 18:24:19.806320: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:19.806367: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:19.898605: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:24:20.640569: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:24:20.640684: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:24:20.741394: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=40)]: Done 1000 out of 1000 | elapsed:  5.8min finished\n",
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "2021-09-24 18:29:36.982951: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:36.997334: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.003191: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.008533: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.017224: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.024079: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.031611: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.039614: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.046512: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.048757: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.062272: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.065119: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.071730: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.079663: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.087733: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.094972: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.105138: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.110903: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.120007: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.126338: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.136760: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.145953: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.152945: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.161495: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.172553: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.188400: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.193706: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.196005: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.211579: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.224786: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.225742: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.241381: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.244221: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.247762: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.267579: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.279846: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.281126: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.281125: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.297326: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.299351: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.313211: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.329027: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.329443: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.351066: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.361885: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.366135: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.373479: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.392141: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.399612: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.408256: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.411750: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.413868: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.438562: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.441479: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.445968: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.447802: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.477522: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.477522: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.479438: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.487144: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.487332: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.498954: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.504913: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.506835: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.508726: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.509278: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.509668: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.518828: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.546401: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.548463: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.563349: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.568948: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.570080: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.571535: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.575897: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.581042: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.594662: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.596313: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.603239: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.604679: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.606856: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.607108: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.610314: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.624384: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.633196: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.635017: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.645642: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.648686: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.651768: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.653878: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.657658: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.659419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.659420: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:29:37.679103: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.685740: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.685740: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:29:37.719567: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.721604: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.729740: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.729880: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.730806: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.730806: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.750990: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.753003: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.786306: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.786306: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.788381: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.788381: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.845623: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.847571: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.866184: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.868029: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.889158: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.890983: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.895468: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.896417: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.911561: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.913213: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.914182: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.915150: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.925765: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.927165: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.933454: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.934441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.975496: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.977640: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:37.994598: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:37.996864: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.013321: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.013318: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.015169: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.015169: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.022017: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.022759: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.025061: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.026898: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.029829: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.030698: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.038676: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.040491: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.064409: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.066348: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.147605: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.149272: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.213129: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.218971: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.223356: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.223356: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.224211: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.224211: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.225679: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.226472: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.227701: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.227734: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.228981: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.229018: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.233587: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.234931: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:38.313668: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:29:38.315248: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:29:48.144583: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c02690 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:48.144636: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:48.160593: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85104ece0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:48.160637: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:48.220478: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:48.240205: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:48.370158: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:48.370214: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:48.387609: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1078036530 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:48.387693: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:48.454216: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:48.478757: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:48.788717: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:48.788780: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:48.889150: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:49.509307: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851005d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:49.509366: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:49.525416: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b4df60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:49.525473: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:49.619484: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:49.691319: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:51.237493: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:51.237568: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:51.306091: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:51.978772: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:51.978843: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:52.033065: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80feb6bd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:52.033166: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:52.076036: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:52.089539: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:52.089611: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:52.108818: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:52.188167: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:52.676283: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510a52b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:52.676355: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:52.773956: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:53.011363: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:53.011419: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:53.080361: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:53.359643: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85102f5a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:53.359701: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:53.423002: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:53.720016: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8100b31e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:53.720075: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:53.812376: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:53.845270: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b3fd50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:53.845350: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:53.942455: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:53.978244: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:53.978302: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:54.082210: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:54.092500: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:54.092624: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:54.094596: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:54.094672: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:54.161022: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:54.169269: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851088ea0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:54.169334: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:54.183456: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:54.245720: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:54.517827: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850fa5370 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:54.517885: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:54.579864: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:54.922202: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510626e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:54.922440: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:55.034804: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:55.148246: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510a6710 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:55.148323: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:55.254381: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:55.356748: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510636b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:55.356819: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:55.443719: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:55.462301: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:55.462375: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:55.553381: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:55.574813: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c93440 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:55.574976: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:55.718786: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:55.718854: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:55.763051: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:55.842452: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:56.290931: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:56.291005: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:56.398131: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:56.649019: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850fb79b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:56.649088: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:56.765908: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:56.765426: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b17600 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:56.766215: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:56.909659: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:57.095632: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:57.095685: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:57.176295: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:57.224032: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:57.224108: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:57.358635: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:57.778520: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859bcd7d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:57.778575: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:57.866768: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:58.631666: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b4db90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:58.631757: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:58.762602: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:29:59.783067: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b2a9d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:29:59.783126: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:29:59.865217: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:30:00.216051: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510bae30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:30:00.216109: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:30:00.316276: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:30:02.075852: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859bd9170 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:30:02.075926: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:30:02.182284: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:30:02.402502: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:30:02.402557: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:30:02.497821: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:30:03.931656: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80cd65bd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:30:03.931708: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:30:04.008438: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:30:04.115057: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b3a130 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:30:04.115137: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:30:04.262801: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=40)]: Done 1000 out of 1000 | elapsed:  5.7min finished\n",
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "2021-09-24 18:35:21.032682: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.052746: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.052797: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.058918: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.061364: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.069531: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.074522: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.086542: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.099964: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.104069: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.108265: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.108265: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.108267: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.108265: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.109983: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.120788: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.124013: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.125122: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.135886: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.136916: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.138477: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.155764: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.166272: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.166427: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.166427: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.175550: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.177656: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.178277: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.204111: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.204196: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.217249: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.217414: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.226033: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.232150: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.232352: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.243447: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.261674: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.271387: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.275187: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.281114: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.285305: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.290946: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.298516: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.311297: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.316041: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.327120: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.343157: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.344525: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.358452: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.361527: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.370449: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.389186: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.399494: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.411133: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.413953: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.434609: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.451148: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.457852: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.470525: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.478776: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.483367: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.498870: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.499347: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.510818: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.512696: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.525452: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.525529: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.546008: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.551713: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.558279: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.569942: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.569941: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.594417: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.594517: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:35:21.624706: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.626280: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.653397: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.670817: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:21.690919: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:21.727700: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.741738: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.773090: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:35:21.881492: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:21.883102: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:21.891751: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:21.892752: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:21.951049: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:21.953060: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:21.964567: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:21.966313: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:21.978454: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:21.980335: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:21.984170: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:21.985038: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.000342: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.001316: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.006995: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.007569: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.007820: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.008311: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.015793: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.016861: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.019753: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.020576: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.020815: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.021585: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.021763: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.022411: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.027185: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.028100: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.028311: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.028808: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.029232: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.029954: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.030162: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.030952: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.032261: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.033064: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.034826: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.035604: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.037967: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.038983: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.042754: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.043042: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.043860: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.044087: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.044241: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.044754: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.045029: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.046119: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.046930: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.048407: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.057954: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.058940: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.061881: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.063257: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.081139: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.082315: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.082378: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.083156: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.153934: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.155372: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.239687: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.240708: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.244853: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.244853: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.245701: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.245702: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.267292: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.268322: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.268688: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.269451: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.315070: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.316159: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.349415: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.355785: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.377465: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.378569: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:22.462695: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:35:22.463740: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:35:36.352664: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1078036530 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:36.352768: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:36.462823: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:36.787026: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851060120 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:36.787093: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:36.906949: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:37.220369: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85105af20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:37.220477: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:37.488974: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:38.245089: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850fb8610 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:38.246728: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:38.334337: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:38.663569: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8100bb3b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:38.663654: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:38.755574: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:38.916913: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510b4680 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:38.916978: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:39.063999: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:39.857731: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c65060 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:39.857799: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:39.862818: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b963a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:39.863363: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:39.935277: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:39.935333: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:39.988966: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:40.024432: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:40.055649: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:40.814622: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85100a870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:40.814688: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:40.892652: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:40.892711: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:40.905461: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851095520 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:40.905531: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:40.965240: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:41.022940: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:41.098559: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:41.191124: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85108aef0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:41.191217: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:41.318405: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:43.095428: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c283f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:43.095486: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:43.183277: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:44.047917: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:44.047978: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:44.049166: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:44.049242: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:44.074254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b52f00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:44.074311: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:44.133093: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:44.143839: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:44.187626: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:44.193052: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:44.193103: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:44.297787: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:44.472971: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85108ee50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:44.473112: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:44.663207: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:44.814659: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:44.814727: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:44.983132: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:44.987027: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851084e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:44.987085: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:45.125253: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:45.905439: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:45.905496: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:46.026220: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:46.081995: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859bdc870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:46.082274: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:46.185717: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:46.216071: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c23b30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:46.216167: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:46.229166: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851042c80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:46.229318: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:46.350068: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:46.391784: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:46.492588: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510b4500 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:46.492659: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:46.622326: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:46.785047: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85104f150 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:46.785119: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:46.840530: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b309d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:46.840587: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:46.902984: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:46.963613: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:47.757551: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80cd72b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:47.757613: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:47.867275: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:48.078228: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:48.078295: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:48.183342: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:48.217221: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:48.217403: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:48.310119: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:48.989742: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510b8fe0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:48.989800: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:49.066544: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:49.206638: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510b8670 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:49.206723: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:49.246008: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859bc7800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:49.246067: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:49.303277: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:49.349573: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:49.453461: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b2eef0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:49.453510: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:49.530834: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:52.157790: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:52.157869: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:52.267060: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:35:52.344647: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:52.344709: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:52.454078: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:   36.6s\n",
      "2021-09-24 18:35:58.965112: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80cd65bd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:35:58.965197: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:35:59.081463: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:36:07.679281: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:36:07.679360: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:36:07.802288: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:36:16.655253: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:36:16.655331: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:36:16.769570: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=40)]: Done 1000 out of 1000 | elapsed:  6.1min finished\n",
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "2021-09-24 18:41:30.877886: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.878012: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.886656: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.891433: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.899135: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.910823: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:30.914457: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.922079: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:30.924187: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.925093: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:30.931139: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:30.931278: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:30.954217: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:30.954881: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.960756: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:30.963714: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.977707: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.984832: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:30.994516: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:30.997114: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.023464: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.028401: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.037942: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.040688: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.057562: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.060224: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.063450: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.075371: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.079369: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.083638: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.103360: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.121033: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.130688: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.139259: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.148692: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.150040: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.164829: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.166869: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.189308: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.195526: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.203661: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.209193: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.231784: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.242021: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.255365: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.256404: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.276005: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.288408: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.290799: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.299388: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.300752: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.314740: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.326323: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.329240: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.339424: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.347495: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.367706: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.367782: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.374744: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.384879: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.401633: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.410823: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.415544: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.430650: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.434182: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.464050: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.464050: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.472036: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.481252: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.495323: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.505427: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.547603: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.552018: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.552307: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.552514: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.555640: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:41:31.564071: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.570084: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.582909: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.596133: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.601704: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.609006: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:41:31.734540: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.734537: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.735750: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.735750: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.736105: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.736861: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.736885: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.737383: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.737473: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.737651: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.738154: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.738248: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.738253: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.738301: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.738826: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.739034: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.739120: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.739182: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.739580: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.739942: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.739988: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.740687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.740811: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.741671: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.742996: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.743825: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.744051: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.745008: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.753492: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.754552: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.758923: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.759888: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.771346: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.772408: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.781156: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.782068: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.798554: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.799732: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.844001: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.845104: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.849075: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.850255: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.874958: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.876033: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.906287: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.906298: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.907379: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.907443: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.960550: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.961686: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:31.985598: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:31.986691: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.003536: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.004627: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.023249: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.024261: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.047431: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.048614: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.070547: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.072012: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.074180: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.075180: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.087163: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.088295: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.089039: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.089881: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.126232: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.127506: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.130703: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.131650: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.132447: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.133292: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.141224: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.142316: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.150913: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.151807: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:32.160456: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:41:32.161625: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:41:54.086274: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510a8a80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:54.086332: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:54.099686: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:54.099746: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:54.180682: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:41:54.198403: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:41:55.117463: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850e38390 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:55.117519: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:55.231826: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:41:55.324873: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b2db10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:55.324931: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:55.433368: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:41:55.758308: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:55.758434: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:55.882724: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:41:56.358297: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510445f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:56.358353: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:56.463068: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:41:57.261079: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85105f400 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:57.261536: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:57.479895: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:41:57.628651: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:57.628721: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:57.842200: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:41:58.378167: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:58.378305: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:58.492381: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:41:58.568931: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b77de0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:58.569059: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:58.731952: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:41:59.768250: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:41:59.768416: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:41:59.895720: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:00.327000: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:00.327057: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:00.401457: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:00.401602: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:00.434660: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:00.577209: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:00.694633: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b27510 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:00.694694: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:00.713841: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:00.713920: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:00.825371: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:00.842394: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:01.205180: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b16350 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:01.205233: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:01.299672: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:01.578202: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:01.578254: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:01.723721: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:01.725729: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:01.725767: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:01.820374: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:02.067293: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:02.067355: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:02.170764: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:02.256963: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:02.257037: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:02.352386: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85105b020 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:02.352460: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:02.360290: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c84190 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:02.360351: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:02.373822: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:02.452088: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:02.473013: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:02.507866: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85109f700 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:02.507925: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:02.614151: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:02.627255: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:02.627318: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:02.728204: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:02.830166: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85108d2d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:02.830215: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:02.937315: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:03.361243: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851083fb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:03.361314: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:03.489830: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:03.778242: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b32380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:03.778314: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:03.836147: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850cf0c60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:03.836212: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:03.870357: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:03.935805: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:04.152088: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b6c180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:04.152157: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:04.284014: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:04.698457: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:04.698529: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:04.713178: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b51100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:04.713708: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:04.824897: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:04.824897: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:05.155162: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:05.155285: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:05.258226: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:05.640810: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510a1b70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:05.640865: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:05.752759: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:07.021561: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8100b2910 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:07.021773: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:07.120896: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:07.536855: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850f9f8e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:07.536899: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:07.693644: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:07.757940: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b7a420 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:07.758008: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:07.895461: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:08.728090: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85107d610 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:08.728150: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:08.819020: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:10.133495: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:10.133548: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:10.253172: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:10.645323: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:10.646478: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:10.752806: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:42:10.960536: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:42:10.960610: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:42:11.119079: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:   42.5s\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=40)]: Done 1000 out of 1000 | elapsed:  6.0min finished\n",
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "2021-09-24 18:47:35.643385: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.654397: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.664874: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.670547: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.677844: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.685751: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.695076: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.696823: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.702548: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.715491: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.716043: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.730039: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.735340: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.738635: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.741116: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.758267: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.762444: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.768108: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.774556: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.782961: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.806267: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.806227: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.814953: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.819718: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.835900: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.844034: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.844872: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.850278: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.855896: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.865611: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.878916: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.878915: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.885782: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.894953: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.909315: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.929538: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.929787: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.930775: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.955770: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.959046: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.964824: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.965374: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.987408: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:35.987581: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:35.992549: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.020410: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.027212: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.033857: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.033918: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.038559: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.048563: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.064142: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.069257: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.076615: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.087484: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.097250: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.097862: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.099784: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.115456: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.115742: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.133020: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.142249: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.142987: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.154743: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.171221: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.176840: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.177760: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.187726: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.189632: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.199390: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.210788: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.228504: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.233600: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:47:36.253399: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.255244: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.260189: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.262669: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.268262: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.271462: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.271984: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:47:36.295408: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.318914: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.332874: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.334145: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.342453: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.343521: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.360544: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.361410: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.391903: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.393079: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.400715: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.401812: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.462503: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.464176: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.497295: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.498565: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.531183: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.532299: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.541009: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.541009: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.542161: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.542161: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.543435: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.544460: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.559618: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.559614: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.560816: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.560816: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.573444: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.574628: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.581419: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.582405: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.593360: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.594562: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.618123: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.618918: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.633221: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.634363: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.634861: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.635770: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.648959: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.650168: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.662407: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.663813: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.683962: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.684979: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.687109: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.688025: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.724935: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.726022: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.735396: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.736619: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.749345: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.750365: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.755791: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.756616: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.756737: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.757579: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.792152: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.793191: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.824109: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.825055: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.825201: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.825825: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.848983: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.850099: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.859519: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.860673: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.895213: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.896235: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.928542: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.929544: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.936945: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.937914: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.949966: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.951255: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:36.986587: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:36.988188: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:37.024539: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:47:37.025643: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:47:57.017710: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:47:57.017782: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:47:57.104667: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:47:57.309454: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85103cd40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:47:57.309591: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:47:57.412150: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:47:57.454998: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:47:57.455255: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:47:57.548300: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:47:57.563638: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:47:57.563697: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:47:57.676968: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:47:57.702754: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bd2cd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:47:57.710809: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:47:57.822641: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:47:58.424831: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85109c3e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:47:58.424897: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:47:58.588980: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:47:58.639166: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85106b4b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:47:58.639256: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:47:58.767972: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:47:59.106469: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859bcfa30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:47:59.106539: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:47:59.211545: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:00.882273: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85103c9f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:00.882365: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:00.899646: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851019800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:00.899834: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:00.992170: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:00.997347: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:01.036825: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851065070 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:01.036900: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:01.130004: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:01.155080: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8100d5160 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:01.155161: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:01.266649: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:01.266717: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:01.266788: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:01.294103: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:01.294204: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:01.360955: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:01.371500: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:01.584486: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:01.584540: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:01.651611: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:02.004999: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:02.005067: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:02.078340: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:02.313005: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:02.313086: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:02.381257: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:02.900042: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b96900 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:02.900116: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:03.007212: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:03.599504: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:03.599558: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:03.693297: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:03.773871: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85109b970 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:03.773938: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:03.884043: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:04.243665: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850d13520 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:04.243747: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:04.472919: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:04.971138: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851076530 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:04.971194: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:05.041264: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:06.010916: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:06.011109: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:06.029143: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850cf6800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:06.029217: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:06.117408: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:06.210689: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:06.432828: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:06.432888: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:06.500045: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:07.186949: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:07.187061: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:07.293225: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:08.309232: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:08.309364: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:08.311561: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:08.313312: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:08.339866: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:08.339926: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:08.367282: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b9b510 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:08.367390: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:08.411985: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:08.418615: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:08.461574: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:08.547436: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:08.846061: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85109a730 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:08.846123: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:08.948045: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:09.778645: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859b7e290 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:09.778726: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:09.858857: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:10.404169: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c859bd7f80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:10.404240: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:10.477079: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:10.749429: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850fd7400 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:10.749484: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:10.818724: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85108b9f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:10.818787: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:10.827383: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:10.893272: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:11.760755: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510b5c40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:11.760815: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:11.822336: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510b59c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:11.822399: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:11.840740: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:11.919452: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:12.366596: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c851075120 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:12.366661: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:12.462341: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:48:13.171921: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:13.171974: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:13.264390: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:   38.9s\n",
      "2021-09-24 18:48:15.671946: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850fcaef0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:48:15.672054: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:48:15.781093: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=40)]: Done 1000 out of 1000 | elapsed:  5.7min finished\n",
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "2021-09-24 18:53:23.588058: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.605086: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.608875: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.611227: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.613072: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.621968: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.632261: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.632704: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.635831: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.637227: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.644263: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.648567: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.652366: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.663214: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.664181: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.666072: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.669112: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.679037: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.682127: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.691712: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.696545: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.697565: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.708249: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.712685: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.713315: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.726688: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.741872: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.748896: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.764592: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.767337: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.781897: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.781898: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.809154: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.809158: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.809156: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.817866: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.822621: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.828070: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.828143: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.842279: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.842278: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.856659: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.856728: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.865813: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.873459: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.886538: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.894368: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.898687: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.898921: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.899047: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.911953: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.916482: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.941615: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.954844: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.965445: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.969236: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.973346: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.974439: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:23.977290: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.985201: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:23.997126: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:24.006638: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:24.010683: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.023441: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:24.025185: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.039876: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:24.045699: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.050450: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:24.059807: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:24.062502: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:53:24.075383: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.075472: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.076913: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.081707: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.109555: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.113034: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.117090: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.117099: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.117855: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.132027: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:53:24.341540: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.352870: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.355285: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.369904: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.369904: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.369898: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.370990: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.371136: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.381942: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.383405: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.383830: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.384695: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.390220: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.390389: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.391277: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.391375: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.399389: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.399389: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.400517: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.400517: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.440747: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.441969: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.478584: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.480312: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.485531: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.486545: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.507018: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.508102: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.516404: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.517334: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.518149: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.519082: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.555830: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.557119: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.565333: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.566497: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.572203: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.579023: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.579280: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.580536: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.580536: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.580535: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.604072: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.605131: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.656674: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.656674: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.657951: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.657951: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.660190: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.661181: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.669449: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.670659: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.670687: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.671637: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.673287: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.674215: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.676803: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.677758: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.694102: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.694433: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.695900: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.695900: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.713900: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.715536: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.721920: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.722893: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.736051: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.736341: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.737132: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.737132: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.740736: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.741659: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.741976: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.742783: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.749500: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.750593: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.764597: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.765629: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:24.799885: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:53:24.800985: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:53:42.290974: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b10500 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:42.291040: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:42.369859: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:43.817563: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bd98b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:43.817634: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:43.987564: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:44.008951: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:44.009018: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:44.105164: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:44.294493: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:44.294538: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:44.377708: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:44.716165: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bc3800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:44.716269: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:44.832464: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:45.152880: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:45.152938: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:45.265168: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:45.462737: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b4c010 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:45.462790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:45.579211: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:45.859237: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b650d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:45.859446: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:45.986483: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:46.082141: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:46.082218: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:46.088081: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:46.088236: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:46.255034: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:46.259819: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:47.235467: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850befc80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:47.235537: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:47.362581: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:47.642247: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:47.642315: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:47.733503: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:47.895498: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:47.895557: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:48.001010: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:48.549933: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:48.550011: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:48.652579: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:48.823008: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:48.823081: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:48.903785: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:49.505774: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:49.505834: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:49.579701: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:49.772192: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:49.772247: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:49.817226: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c854c39240 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:49.817310: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:49.859960: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:49.881488: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:50.196856: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:50.196921: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:50.303372: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:50.528197: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850ac9c60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:50.528248: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:50.756961: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:50.816011: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c1e050 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:50.816071: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:50.888408: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:51.243052: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:51.246798: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:51.354721: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:51.878618: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c854d83260 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:51.878679: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:51.992833: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:52.076333: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:52.076405: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:52.201426: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:52.524766: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b792b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:52.524854: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:52.635320: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:52.686099: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b5fb40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:52.686601: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:52.755308: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:53.160207: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85100e2e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:53.160273: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:53.236692: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:53.579983: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b32570 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:53.580038: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:53.804859: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:53.857099: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c7c520 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:53.857152: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:53.931262: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:54.077679: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:54.077742: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:54.169780: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:55.348864: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:55.348924: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:55.447365: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:55.489718: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:55.489791: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:55.595993: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:55.621110: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c24f00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:55.621175: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:55.720527: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:56.511432: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:56.511491: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:56.622408: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:57.174299: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b99230 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:57.174361: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:57.292002: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:57.349281: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:57.349475: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:57.470223: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:58.259375: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:58.259857: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:58.420067: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:53:59.203800: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b7c370 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:53:59.203879: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:53:59.328288: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:   38.2s\n",
      "2021-09-24 18:54:02.435471: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:54:02.435515: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:54:02.529602: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:54:03.565689: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:54:03.565756: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:54:03.654372: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=40)]: Done 1000 out of 1000 | elapsed:  5.7min finished\n",
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "2021-09-24 18:59:07.627014: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.638069: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.645515: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.650503: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.659026: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.662226: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.669908: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.672814: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.681851: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.686768: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.697277: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.697502: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.718937: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.720212: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.725963: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.739750: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.744907: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.745315: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.752638: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.767365: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.774335: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.774408: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.792676: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.797110: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.798504: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.799210: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.813904: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.822006: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.825908: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.828520: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.829367: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.831945: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.836437: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.840090: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.863699: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.866601: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.872894: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.879225: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.879225: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.885746: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.886692: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.892266: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.896022: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.899888: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.912266: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.913632: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.930933: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.935737: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.940203: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:07.940475: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.972228: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.975780: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:07.987500: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.001012: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.009411: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.011253: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.023809: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.036015: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.036818: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.046467: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.047670: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.055228: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.077930: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.078485: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.082174: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.086329: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.091515: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.117743: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.126925: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.126997: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.139075: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.158787: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.160158: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.161067: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.161391: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.161688: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 18:59:08.186252: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.186252: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.196269: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.196269: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 18:59:08.235504: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.251958: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.254356: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.254716: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.254882: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.255538: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.265677: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.266498: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.267572: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.267574: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.272642: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.273577: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.276564: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.278327: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.282415: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.283326: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.307870: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.309822: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.316988: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.318010: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.327539: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.329602: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.341266: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.343158: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.357862: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.359855: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.412418: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.414428: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.453814: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.454894: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.455822: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.455824: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.461324: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.462388: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.469871: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.471794: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.493725: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.495630: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.505603: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.507059: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.511912: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.512914: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.529153: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.531304: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.532050: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.532173: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.532852: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.532916: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.577144: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.579045: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.584553: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.585253: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.597770: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.597834: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.599196: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.600570: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.633787: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.634216: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.635614: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.635614: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.639103: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.639704: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.640133: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.640464: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.664294: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.666303: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.679088: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.681069: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.696995: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.698434: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.698915: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.699218: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.707607: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.709069: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.713603: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.714862: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.729007: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.729032: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 18:59:08.730895: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:08.730895: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 18:59:15.417302: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:15.417361: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:15.421195: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c854ce9670 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:15.421248: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:15.475447: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:15.475504: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:15.498374: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:15.512809: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:15.560466: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:15.720260: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:15.720328: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:15.816005: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:16.411702: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850e2d9c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:16.411750: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:16.525874: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:16.576130: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b7e270 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:16.576190: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:16.727858: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:16.731157: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:16.731236: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:16.831286: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:17.857173: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b81610 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:17.857232: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:17.944242: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:18.322851: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b48700 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:18.322913: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:18.421686: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:18.469867: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:18.469939: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:18.595124: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:18.595189: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:18.636094: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:18.708995: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:18.825874: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:18.830791: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:18.897785: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:18.900781: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:19.049179: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:19.050902: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:20.166972: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850ab60a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:20.167029: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:20.404268: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:20.462871: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c44430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:20.462946: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:20.550389: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:20.550538: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:20.579283: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:20.720616: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:21.419165: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c6f590 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:21.419268: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:21.426398: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:21.426458: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:21.507141: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:21.524423: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:21.559773: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850ba0da0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:21.559842: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:21.653434: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:21.812944: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:21.813007: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:21.926854: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:22.424725: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:22.424784: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:22.527343: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:22.712885: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:22.722914: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:22.834379: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:24.149786: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8100e0790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:24.149838: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:24.222818: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850eb5c80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:24.222883: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:24.259368: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:24.324990: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:24.427702: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850badd60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:24.427793: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:24.524457: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:24.971070: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bd12a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:24.971130: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:25.099237: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:25.150772: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:25.150839: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:25.438838: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:25.553530: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8100eaa60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:25.553588: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:25.618111: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:25.913711: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850ad2e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:25.913883: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:26.069591: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:26.627834: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bb57e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:26.627949: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:26.747545: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:26.781165: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:26.781410: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:26.884815: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:27.815942: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80cd72b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:27.816016: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:27.917999: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:27.927748: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:27.927848: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:28.027832: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:28.354874: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:28.354935: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:28.459162: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:28.613380: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:28.613436: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:28.716658: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:28.781547: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:28.781612: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:28.874902: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:29.336982: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c854cedb00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:29.337039: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:29.438700: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:29.635283: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:29.635340: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:29.723527: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:30.305275: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bc5a10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:30.305332: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:30.418415: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 18:59:32.900237: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bffd80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 18:59:32.900313: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 18:59:32.995385: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=40)]: Done 1000 out of 1000 | elapsed:  6.0min finished\n",
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "2021-09-24 19:05:11.704718: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.738002: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.738210: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.738068: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.739207: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.740925: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.747621: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.750634: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.761068: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.773134: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.780001: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.789946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.810099: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.814950: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.830466: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.836154: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.836154: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.836154: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.836154: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.836154: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.836154: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.837735: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.837735: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.843622: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.851657: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.855176: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.862422: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.883710: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.892583: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.901432: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.901425: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.917720: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.921269: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.926104: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.932182: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.933501: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.948744: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.959921: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.961962: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.987327: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:11.993257: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.993312: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:11.997250: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.010655: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.027629: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.031233: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.039939: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.045357: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.049147: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.064281: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.066873: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.077081: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.081090: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.110982: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.123551: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.132851: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.139642: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.141568: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.146299: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.163122: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.177463: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.177879: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.196029: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.206800: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.214344: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.222942: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.236881: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.250575: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.257668: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.258860: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.277373: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.280798: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.285900: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:05:12.292172: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.309147: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.316518: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.332978: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.334585: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.346401: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.358468: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:05:12.450506: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.471030: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.572054: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.573172: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.573337: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.574136: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.582169: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.583489: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.590859: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.590914: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.591752: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.591752: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.594503: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.594992: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.595471: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.595905: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.603799: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.604700: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.607568: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.608623: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.610696: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.611545: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.615304: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.615308: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.616146: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.616149: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.617013: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.617858: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.626597: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.627984: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.643222: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.644355: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.688349: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.689487: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.699906: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.701124: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.703059: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.704135: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.731373: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.732487: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.740604: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.741646: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.758113: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.759441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.785577: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.787156: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.794203: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.795213: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.795634: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.796503: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.796664: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.797608: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.797710: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.798436: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.802566: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.803979: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.804058: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.804940: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.827290: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.828354: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.843348: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.844443: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.855096: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.856086: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.905294: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.906342: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.923999: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.925011: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.931665: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.932534: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.944974: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.945957: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:12.977518: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:12.978572: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:13.018844: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:13.019839: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:13.057665: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:13.058653: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:13.068100: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:05:13.069065: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:05:36.158622: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:36.158791: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:36.220671: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850ca22d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:36.220724: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:36.260061: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:36.316052: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:37.061879: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b727e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:37.061946: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:37.164681: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:37.265272: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bcc6d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:37.265434: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:37.336441: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850a0b550 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:37.336589: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:37.439852: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c440b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:37.442185: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:37.521053: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:37.549733: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:37.558956: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:37.629269: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:37.631613: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:37.733938: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:38.983839: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85108bee0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:38.983946: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:39.085352: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:39.916692: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c81010ee50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:39.916749: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:40.031591: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:40.083736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:40.089673: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:40.252124: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:40.773684: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bbb570 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:40.773755: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:40.987923: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:41.023407: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510823e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:41.023467: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:41.179257: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:42.024370: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c317d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:42.025293: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:42.421634: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:42.754779: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:42.758044: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:42.863309: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:42.943419: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:42.943495: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:43.057441: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:43.316784: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b7ac20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:43.316925: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:43.570346: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:43.660839: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c74ff0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:43.660924: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:43.706563: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:43.706726: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:43.811759: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:43.828642: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:43.981264: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c81000ef60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:43.981354: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:43.990415: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:43.991015: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:44.096372: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:44.107310: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:44.482469: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510a37f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:44.482637: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:44.599763: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:44.726185: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:44.727232: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:44.753067: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:44.754098: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:44.852799: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:44.860313: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:45.095868: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c0bfe0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:45.095917: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:45.219909: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:45.417584: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bcccc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:45.417640: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:45.527430: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:46.700833: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8597722b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:46.702792: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:46.818838: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:47.295050: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c854dac170 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:47.295107: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:47.381300: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:47.518449: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c8d6a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:47.518527: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:47.643908: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:48.363644: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bf7760 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:48.363736: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:48.462652: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:48.795901: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80feb81d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:48.795980: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:48.832712: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8100e8270 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:48.832758: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:48.886873: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:48.901900: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:51.103459: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850be17a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:51.103520: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:51.217722: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:51.334361: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b0af90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:51.335638: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:51.427964: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:51.430049: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:51.439068: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:51.504819: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80feb6bd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:51.504864: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:51.557941: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:51.603434: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:53.240942: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:53.241003: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:53.468626: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:54.034518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bc1850 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:54.034580: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:54.189215: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:55.445304: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850cd9ab0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:55.445410: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:55.522669: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:05:56.275630: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bc2f80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:05:56.276427: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:05:56.376106: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:   46.2s\n",
      "2021-09-24 19:06:02.834670: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:06:02.837584: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:06:02.933751: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=40)]: Done 1000 out of 1000 | elapsed:  6.0min finished\n",
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "2021-09-24 19:11:13.463231: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.480890: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.488753: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.492431: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.500918: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.514838: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.517746: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.523319: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.531277: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.532703: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.545462: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.557904: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.559220: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.567730: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.576337: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.580228: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.590750: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.595724: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.610504: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.611843: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.626352: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.627136: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.646973: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.657701: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.667210: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.680031: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.682602: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.689355: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.697782: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.711320: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.714952: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.732593: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.745256: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.745282: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.751764: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.770200: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.780424: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.791715: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.792942: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.805066: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.817665: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.823343: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.830927: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.857968: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.860370: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.865495: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.882859: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.883433: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.907966: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.914282: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.926859: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.932029: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.939383: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.964229: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.966421: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:13.978145: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:13.983802: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:13.990915: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:13.993996: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.003405: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.004885: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.006344: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.024092: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.025791: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.031910: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.044359: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.047327: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.060274: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.063352: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.074020: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.076232: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.094577: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.107912: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.110791: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.118756: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.139506: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.140560: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.141597: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.142431: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.143003: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.143380: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.156116: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.160976: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.161102: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.162736: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.172034: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.176553: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.180734: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.182251: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.192120: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.206244: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:11:14.217661: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.219696: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.221056: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.234249: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.235197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.242979: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.262910: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.268533: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.269489: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.270984: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.276759: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:11:14.285947: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.288195: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.293901: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.295027: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.349666: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.351277: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.355649: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.356648: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.377918: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.378413: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.379481: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.379481: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.426793: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.427982: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.435244: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.436256: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.445180: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.445344: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.446357: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.446357: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.473287: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.474338: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.514104: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.515680: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.549303: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.550377: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.573897: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.574060: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.574955: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.575405: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.592712: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.593794: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.594265: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.595108: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.644395: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.645486: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.674178: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.675477: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.679466: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.680190: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.698805: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.699821: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.699878: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.700591: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.725404: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.726504: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.740710: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.741910: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.745853: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.747400: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.758919: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.760854: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.812244: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.813352: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.821754: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.822698: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:14.835151: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:11:14.836215: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:11:30.311971: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:30.312026: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:30.385108: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:30.385203: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:30.398718: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:30.464323: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:32.649087: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850befee0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:32.649164: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:32.720423: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:33.120686: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510619c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:33.120751: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:33.207216: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:33.402936: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510646c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:33.403288: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:33.449319: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b27810 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:33.449438: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:33.569062: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:33.569196: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:33.672214: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:33.681958: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:33.789690: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b89b00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:33.789748: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:33.832515: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:33.893737: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:34.008006: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8101055b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:34.008062: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:34.011002: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:34.011049: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:34.011090: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:34.011135: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:34.065753: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:34.065826: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:34.098085: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:34.105645: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:34.132548: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:34.154822: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85103daf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:34.154887: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:34.193850: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80feb6bd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:34.193924: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:34.254123: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:34.294575: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:34.316645: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:34.579476: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:34.579548: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:34.706034: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:34.828875: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c60210 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:34.828948: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:34.944978: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:35.101078: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b3b380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:35.101147: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:35.168717: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b45aa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:35.168804: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:35.214925: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:35.248527: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:35.255704: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:35.255750: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:35.345053: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:35.449867: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510884d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:35.449933: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:35.561055: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:35.680314: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c854cde2f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:35.680439: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:35.796933: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:35.948234: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:35.948319: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:36.060974: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:36.552590: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c854dd06b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:36.553266: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:36.627352: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:36.731074: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:36.731143: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:36.753241: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:36.753295: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:36.817154: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:36.818553: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:38.537033: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850baa0f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:38.537083: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:38.642345: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:38.916177: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:38.916238: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:39.062124: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:39.618143: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:39.618210: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:39.722240: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:40.766738: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b87920 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:40.766803: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:40.858003: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:41.378948: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b91410 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:41.379017: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:41.519637: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:41.745947: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b837c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:41.746005: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:41.803507: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b14da0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:41.803565: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:41.818742: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:41.990749: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:42.037645: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b938c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:42.037723: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:42.123610: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:42.159805: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c3f160 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:42.159871: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:42.236167: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:42.748063: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:42.748125: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:42.826090: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:43.406976: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510aac40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:43.407065: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:43.536731: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:43.666622: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850cb3700 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:43.666743: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:43.771756: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:44.305933: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c66050 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:44.305985: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:44.383177: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:45.843668: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b887a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:45.843716: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:45.932687: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:11:46.783112: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:11:46.783184: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:11:46.873868: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:   34.8s\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=40)]: Done 1000 out of 1000 | elapsed:  5.8min finished\n",
      "[Parallel(n_jobs=40)]: Using backend MultiprocessingBackend with 40 concurrent workers.\n",
      "2021-09-24 19:17:05.303817: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.323826: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.335034: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.339783: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.344405: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.361458: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.361459: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.364473: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.376479: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.378064: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.381065: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.390904: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.396821: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.396821: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.411582: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.413161: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.429708: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.433418: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.454076: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.456214: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.462944: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.472983: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.474113: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.499651: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.499653: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.504780: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.510212: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.523484: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.535928: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.538110: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.551679: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.551679: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.553818: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.572011: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.593422: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.598510: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.601832: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.605593: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.607337: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.626175: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.629144: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.644043: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.656336: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.660625: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.664049: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.687109: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.688656: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.707148: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.707152: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.716832: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.724713: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.748508: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.750728: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.764394: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.769208: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.784648: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.788418: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.803858: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.809981: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.821873: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.826337: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.839904: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.846874: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.849684: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.869511: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.872227: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.885206: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.921476: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.924309: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.936177: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.942970: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.944876: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.954359: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.954359: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-24 19:17:05.956109: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:05.971458: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.971458: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:05.974967: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.000925: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:06.002058: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:06.024405: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:06.025395: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.026650: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.027641: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.028768: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.029784: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.030747: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.038267: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.039540: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.051147: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-24 19:17:06.065901: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.066700: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.067067: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.067656: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.068576: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.069470: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.111471: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.112603: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.117084: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.117846: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.118064: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.118663: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.159399: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.160522: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.160973: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.161885: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.164348: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.165271: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.181361: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.182499: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.203472: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.204538: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.225488: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.226569: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.250080: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.251484: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.258181: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.259625: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.263869: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.265057: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.310171: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.311527: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.341264: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.341317: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.342316: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.342316: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.354856: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.355947: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.357997: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.358941: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.410742: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.412664: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.416009: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.416945: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.441973: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.443565: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.445358: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.446254: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.518295: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.519680: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.520827: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.521825: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.524258: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.524788: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.525154: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.525544: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.539405: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.540496: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.548818: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.549755: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.588967: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.590039: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.609533: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.610622: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.622004: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.623273: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.624165: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.624939: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:06.715397: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-24 19:17:06.716501: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2197530000 Hz\n",
      "2021-09-24 19:17:26.645837: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:26.645903: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:26.655036: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80de190a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:26.655106: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:26.660606: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c854c6d1f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:26.660656: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:26.733135: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:26.750028: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:26.761364: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:27.010338: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b57330 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:27.010417: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:27.016992: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bf6ed0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:27.017039: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:27.102263: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:27.104613: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:29.102703: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c6d910 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:29.102774: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:29.236529: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:29.517746: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:29.517809: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:29.617784: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:30.048416: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bc5a60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:30.048636: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:30.236807: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:30.256627: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850cb2f20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:30.256788: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:30.358948: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c08ee0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:30.359002: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:30.401072: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:30.462482: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:30.621594: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850baa4c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:30.621662: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:30.719342: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:30.765573: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c00ec0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:30.765681: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:30.827003: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bef950 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:30.827267: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:30.908736: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:30.981336: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:31.081506: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:31.081571: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:31.193387: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:31.245838: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c8510a4720 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:31.245913: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:31.359576: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b8c480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:31.359701: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:31.361111: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:31.462882: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:32.838160: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f105800f5a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:32.838219: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:32.871690: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b92320 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:32.871759: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:32.889350: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bc3460 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:32.889413: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:32.955358: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:32.974255: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b77770 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:32.974323: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:32.975872: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:33.024240: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:33.084967: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:33.085027: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:33.085196: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:33.193660: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:34.328688: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f105800f5a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:34.328745: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:34.425237: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:34.474126: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:34.474188: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:34.584128: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:34.660144: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:34.661047: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:34.760881: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:34.866883: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:34.866944: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:34.982787: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:35.338372: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850be7a30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:35.338439: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:35.375570: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:35.375631: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:35.449265: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:35.473467: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:35.487705: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c5ad70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:35.487861: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:35.587845: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:35.613266: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c0db90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:35.613795: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:35.723546: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:35.785785: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80b89ea60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:35.787410: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:35.893344: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:36.525330: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c854d95ee0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:36.525385: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:36.634752: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:36.900324: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:36.900389: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:37.045696: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:37.449049: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c221f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:37.449133: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:37.555563: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:37.735113: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:37.735181: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:37.832986: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:41.391089: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850b9df60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:41.391147: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:41.479385: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:41.676474: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:41.676545: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:41.779866: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:42.822490: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80f67af00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:42.822559: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:42.925166: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:45.295504: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850bee8e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:45.295569: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:45.387583: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:45.864149: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c80dcf7eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:45.864199: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:45.934121: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2021-09-24 19:17:46.726781: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c850c423e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:17:46.726852: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:17:46.836305: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=40)]: Done 1000 out of 1000 | elapsed:  6.3min finished\n"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "clf_list = []\n",
    "chunksize = 5000 if lambda_nets_total > 50000 else max(lambda_nets_total//10, min(50, lambda_nets_total))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "max_seed = 2147483646\n",
    "seed_list = random.sample(range(0, max_seed), number_different_lambda_trainings)\n",
    "chunk_multiplier = 0\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    clf_sublist = parallel(delayed(train_nn)(chunksize*chunk_multiplier+index, X_data[1].values, y_data[1].values, X_data[0], seed_list, return_history=True, each_epochs_save=each_epochs_save_lambda, printing=True) for index, (X_data, y_data) in enumerate(zip(X_data_list_split, y_data_list_split)))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "    chunk_multiplier +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T17:23:23.210722Z",
     "iopub.status.busy": "2021-09-24T17:23:23.210404Z",
     "iopub.status.idle": "2021-09-24T17:23:23.215941Z",
     "shell.execute_reply": "2021-09-24T17:23:23.215207Z",
     "shell.execute_reply.started": "2021-09-24T17:23:23.210674Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T17:23:23.217297Z",
     "iopub.status.busy": "2021-09-24T17:23:23.217040Z",
     "iopub.status.idle": "2021-09-24T17:24:15.845001Z",
     "shell.execute_reply": "2021-09-24T17:24:15.844251Z",
     "shell.execute_reply.started": "2021-09-24T17:23:23.217262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    0.001, max:    0.225, cur:    0.002)\n",
      "\tvalidation       \t (min:    0.000, max:    0.191, cur:    0.003)\n",
      "mae\n",
      "\ttraining         \t (min:    0.001, max:    0.225, cur:    0.002)\n",
      "\tvalidation       \t (min:    0.000, max:    0.191, cur:    0.003)\n",
      "r2_keras_loss\n",
      "\ttraining         \t (min:   -0.971, max:  108.068, cur:   -0.971)\n",
      "\tvalidation       \t (min:   -0.993, max:   75.060, cur:   -0.984)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    0.004, max:    0.272, cur:    0.004)\n",
      "\tvalidation       \t (min:    0.002, max:    0.224, cur:    0.003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-24 19:24:14.471604: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c85b843a30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-09-24 19:24:14.471661: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-09-24 19:24:14.562905: I tensorflow/compiler/jit/xla_compilation_cache.cc:333] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "rand_index = np.random.randint(lambda_nets_total)\n",
    "\n",
    "random_network = train_nn(rand_index, X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], seed_list, callbacks=[PlotLossesKerasTF()], return_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Plot Lambda-Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.874Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T17:24:15.847333Z",
     "iopub.status.busy": "2021-09-24T17:24:15.846754Z",
     "iopub.status.idle": "2021-09-24T17:24:18.611993Z",
     "shell.execute_reply": "2021-09-24T17:24:18.611392Z",
     "shell.execute_reply.started": "2021-09-24T17:24:15.847291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9808bf6742274d17a3df4d781aba19b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10a9793d0b94cc5bf25bc58ed60f07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "index_list = []\n",
    "\n",
    "\n",
    "max_training_epochs = 0\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    history = entry[3]\n",
    "    \n",
    "    current_training_epochs = len(history[list(history.keys())[0]])\n",
    "    max_training_epochs = max(max_training_epochs, current_training_epochs)\n",
    "\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    history = entry[3]\n",
    "    index = entry[0][0]\n",
    "    \n",
    "    current_training_epochs = len(history[list(history.keys())[0]])\n",
    "    \n",
    "    loss_list = np.full(max_training_epochs, np.nan)\n",
    "    metric_list = np.full(max_training_epochs, np.nan)\n",
    "    val_loss_list = np.full(max_training_epochs, np.nan)\n",
    "    val_metric_list = np.full(max_training_epochs, np.nan) \n",
    "\n",
    "    for i in range(current_training_epochs):  \n",
    "        loss_list[i] = history[list(history.keys())[0]][i]\n",
    "        metric_list[i] = history[list(history.keys())[1]][i]\n",
    "        val_loss_list[i] = history[list(history.keys())[len(history.keys())//2]][i]\n",
    "        val_metric_list[i] = history[list(history.keys())[len(history.keys())//2+1]][i]\n",
    "    \n",
    "    index_list.append([index])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=np.hstack([index_list, loss_list_total]), columns=list(flatten(['index', [list(history.keys())[0] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]])))\n",
    "#loss_df['index'] = loss_df['index'].astype(int)\n",
    "metric_df = pd.DataFrame(data=np.hstack([index_list, metric_list_total]), columns=list(flatten(['index', [list(history.keys())[1] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]]))) \n",
    "#metric_df['index'] = metric_df['index'].astype(int)\n",
    "val_loss_df = pd.DataFrame(data=np.hstack([index_list, val_loss_list_total]), columns=list(flatten(['index', [list(history.keys())[len(history.keys())//2] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]])))\n",
    "#val_loss_df['index'] = val_loss_df['index'].astype(int)\n",
    "val_metric_df = pd.DataFrame(data=np.hstack([index_list, val_metric_list_total]), columns=list(flatten(['index', [list(history.keys())[len(history.keys())//2+1] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]]))) \n",
    "#val_metric_df['index'] = val_metric_df['index'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.875Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T17:24:18.613054Z",
     "iopub.status.busy": "2021-09-24T17:24:18.612880Z",
     "iopub.status.idle": "2021-09-24T17:24:26.969439Z",
     "shell.execute_reply": "2021-09-24T17:24:26.968390Z",
     "shell.execute_reply.started": "2021-09-24T17:24:18.613031Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[0] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "path_metric = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[1] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[len(history.keys())//2] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[len(history.keys())//2+1] + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.876Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T17:24:26.971466Z",
     "iopub.status.busy": "2021-09-24T17:24:26.971124Z",
     "iopub.status.idle": "2021-09-24T17:24:27.707993Z",
     "shell.execute_reply": "2021-09-24T17:24:27.707238Z",
     "shell.execute_reply.started": "2021-09-24T17:24:26.971434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>loss_epoch_11</th>\n",
       "      <th>loss_epoch_12</th>\n",
       "      <th>loss_epoch_13</th>\n",
       "      <th>loss_epoch_14</th>\n",
       "      <th>loss_epoch_15</th>\n",
       "      <th>loss_epoch_16</th>\n",
       "      <th>loss_epoch_17</th>\n",
       "      <th>loss_epoch_18</th>\n",
       "      <th>loss_epoch_19</th>\n",
       "      <th>loss_epoch_20</th>\n",
       "      <th>loss_epoch_21</th>\n",
       "      <th>loss_epoch_22</th>\n",
       "      <th>loss_epoch_23</th>\n",
       "      <th>loss_epoch_24</th>\n",
       "      <th>loss_epoch_25</th>\n",
       "      <th>loss_epoch_26</th>\n",
       "      <th>loss_epoch_27</th>\n",
       "      <th>loss_epoch_28</th>\n",
       "      <th>loss_epoch_29</th>\n",
       "      <th>loss_epoch_30</th>\n",
       "      <th>loss_epoch_31</th>\n",
       "      <th>loss_epoch_32</th>\n",
       "      <th>loss_epoch_33</th>\n",
       "      <th>loss_epoch_34</th>\n",
       "      <th>loss_epoch_35</th>\n",
       "      <th>loss_epoch_36</th>\n",
       "      <th>loss_epoch_37</th>\n",
       "      <th>loss_epoch_38</th>\n",
       "      <th>loss_epoch_39</th>\n",
       "      <th>loss_epoch_40</th>\n",
       "      <th>loss_epoch_41</th>\n",
       "      <th>loss_epoch_42</th>\n",
       "      <th>loss_epoch_43</th>\n",
       "      <th>loss_epoch_44</th>\n",
       "      <th>loss_epoch_45</th>\n",
       "      <th>loss_epoch_46</th>\n",
       "      <th>loss_epoch_47</th>\n",
       "      <th>loss_epoch_48</th>\n",
       "      <th>loss_epoch_49</th>\n",
       "      <th>loss_epoch_50</th>\n",
       "      <th>loss_epoch_51</th>\n",
       "      <th>loss_epoch_52</th>\n",
       "      <th>loss_epoch_53</th>\n",
       "      <th>loss_epoch_54</th>\n",
       "      <th>loss_epoch_55</th>\n",
       "      <th>loss_epoch_56</th>\n",
       "      <th>loss_epoch_57</th>\n",
       "      <th>loss_epoch_58</th>\n",
       "      <th>loss_epoch_59</th>\n",
       "      <th>loss_epoch_60</th>\n",
       "      <th>loss_epoch_61</th>\n",
       "      <th>loss_epoch_62</th>\n",
       "      <th>loss_epoch_63</th>\n",
       "      <th>loss_epoch_64</th>\n",
       "      <th>loss_epoch_65</th>\n",
       "      <th>loss_epoch_66</th>\n",
       "      <th>loss_epoch_67</th>\n",
       "      <th>loss_epoch_68</th>\n",
       "      <th>loss_epoch_69</th>\n",
       "      <th>loss_epoch_70</th>\n",
       "      <th>loss_epoch_71</th>\n",
       "      <th>loss_epoch_72</th>\n",
       "      <th>loss_epoch_73</th>\n",
       "      <th>loss_epoch_74</th>\n",
       "      <th>loss_epoch_75</th>\n",
       "      <th>loss_epoch_76</th>\n",
       "      <th>loss_epoch_77</th>\n",
       "      <th>loss_epoch_78</th>\n",
       "      <th>loss_epoch_79</th>\n",
       "      <th>loss_epoch_80</th>\n",
       "      <th>loss_epoch_81</th>\n",
       "      <th>loss_epoch_82</th>\n",
       "      <th>loss_epoch_83</th>\n",
       "      <th>loss_epoch_84</th>\n",
       "      <th>loss_epoch_85</th>\n",
       "      <th>loss_epoch_86</th>\n",
       "      <th>loss_epoch_87</th>\n",
       "      <th>loss_epoch_88</th>\n",
       "      <th>loss_epoch_89</th>\n",
       "      <th>loss_epoch_90</th>\n",
       "      <th>loss_epoch_91</th>\n",
       "      <th>loss_epoch_92</th>\n",
       "      <th>loss_epoch_93</th>\n",
       "      <th>loss_epoch_94</th>\n",
       "      <th>loss_epoch_95</th>\n",
       "      <th>loss_epoch_96</th>\n",
       "      <th>loss_epoch_97</th>\n",
       "      <th>loss_epoch_98</th>\n",
       "      <th>loss_epoch_99</th>\n",
       "      <th>loss_epoch_100</th>\n",
       "      <th>loss_epoch_101</th>\n",
       "      <th>loss_epoch_102</th>\n",
       "      <th>loss_epoch_103</th>\n",
       "      <th>loss_epoch_104</th>\n",
       "      <th>loss_epoch_105</th>\n",
       "      <th>loss_epoch_106</th>\n",
       "      <th>loss_epoch_107</th>\n",
       "      <th>loss_epoch_108</th>\n",
       "      <th>loss_epoch_109</th>\n",
       "      <th>loss_epoch_110</th>\n",
       "      <th>loss_epoch_111</th>\n",
       "      <th>loss_epoch_112</th>\n",
       "      <th>loss_epoch_113</th>\n",
       "      <th>loss_epoch_114</th>\n",
       "      <th>loss_epoch_115</th>\n",
       "      <th>loss_epoch_116</th>\n",
       "      <th>loss_epoch_117</th>\n",
       "      <th>loss_epoch_118</th>\n",
       "      <th>loss_epoch_119</th>\n",
       "      <th>loss_epoch_120</th>\n",
       "      <th>loss_epoch_121</th>\n",
       "      <th>loss_epoch_122</th>\n",
       "      <th>loss_epoch_123</th>\n",
       "      <th>loss_epoch_124</th>\n",
       "      <th>loss_epoch_125</th>\n",
       "      <th>loss_epoch_126</th>\n",
       "      <th>loss_epoch_127</th>\n",
       "      <th>loss_epoch_128</th>\n",
       "      <th>loss_epoch_129</th>\n",
       "      <th>loss_epoch_130</th>\n",
       "      <th>loss_epoch_131</th>\n",
       "      <th>loss_epoch_132</th>\n",
       "      <th>loss_epoch_133</th>\n",
       "      <th>loss_epoch_134</th>\n",
       "      <th>loss_epoch_135</th>\n",
       "      <th>loss_epoch_136</th>\n",
       "      <th>loss_epoch_137</th>\n",
       "      <th>loss_epoch_138</th>\n",
       "      <th>loss_epoch_139</th>\n",
       "      <th>loss_epoch_140</th>\n",
       "      <th>loss_epoch_141</th>\n",
       "      <th>loss_epoch_142</th>\n",
       "      <th>loss_epoch_143</th>\n",
       "      <th>loss_epoch_144</th>\n",
       "      <th>loss_epoch_145</th>\n",
       "      <th>loss_epoch_146</th>\n",
       "      <th>loss_epoch_147</th>\n",
       "      <th>loss_epoch_148</th>\n",
       "      <th>loss_epoch_149</th>\n",
       "      <th>loss_epoch_150</th>\n",
       "      <th>loss_epoch_151</th>\n",
       "      <th>loss_epoch_152</th>\n",
       "      <th>loss_epoch_153</th>\n",
       "      <th>loss_epoch_154</th>\n",
       "      <th>loss_epoch_155</th>\n",
       "      <th>loss_epoch_156</th>\n",
       "      <th>loss_epoch_157</th>\n",
       "      <th>loss_epoch_158</th>\n",
       "      <th>loss_epoch_159</th>\n",
       "      <th>loss_epoch_160</th>\n",
       "      <th>loss_epoch_161</th>\n",
       "      <th>loss_epoch_162</th>\n",
       "      <th>loss_epoch_163</th>\n",
       "      <th>loss_epoch_164</th>\n",
       "      <th>loss_epoch_165</th>\n",
       "      <th>loss_epoch_166</th>\n",
       "      <th>loss_epoch_167</th>\n",
       "      <th>loss_epoch_168</th>\n",
       "      <th>loss_epoch_169</th>\n",
       "      <th>loss_epoch_170</th>\n",
       "      <th>loss_epoch_171</th>\n",
       "      <th>loss_epoch_172</th>\n",
       "      <th>loss_epoch_173</th>\n",
       "      <th>loss_epoch_174</th>\n",
       "      <th>loss_epoch_175</th>\n",
       "      <th>loss_epoch_176</th>\n",
       "      <th>loss_epoch_177</th>\n",
       "      <th>loss_epoch_178</th>\n",
       "      <th>loss_epoch_179</th>\n",
       "      <th>loss_epoch_180</th>\n",
       "      <th>loss_epoch_181</th>\n",
       "      <th>loss_epoch_182</th>\n",
       "      <th>loss_epoch_183</th>\n",
       "      <th>loss_epoch_184</th>\n",
       "      <th>loss_epoch_185</th>\n",
       "      <th>loss_epoch_186</th>\n",
       "      <th>loss_epoch_187</th>\n",
       "      <th>loss_epoch_188</th>\n",
       "      <th>loss_epoch_189</th>\n",
       "      <th>loss_epoch_190</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "      <th>loss_epoch_201</th>\n",
       "      <th>loss_epoch_202</th>\n",
       "      <th>loss_epoch_203</th>\n",
       "      <th>loss_epoch_204</th>\n",
       "      <th>loss_epoch_205</th>\n",
       "      <th>loss_epoch_206</th>\n",
       "      <th>loss_epoch_207</th>\n",
       "      <th>loss_epoch_208</th>\n",
       "      <th>loss_epoch_209</th>\n",
       "      <th>loss_epoch_210</th>\n",
       "      <th>loss_epoch_211</th>\n",
       "      <th>loss_epoch_212</th>\n",
       "      <th>loss_epoch_213</th>\n",
       "      <th>loss_epoch_214</th>\n",
       "      <th>loss_epoch_215</th>\n",
       "      <th>loss_epoch_216</th>\n",
       "      <th>loss_epoch_217</th>\n",
       "      <th>loss_epoch_218</th>\n",
       "      <th>loss_epoch_219</th>\n",
       "      <th>loss_epoch_220</th>\n",
       "      <th>loss_epoch_221</th>\n",
       "      <th>loss_epoch_222</th>\n",
       "      <th>loss_epoch_223</th>\n",
       "      <th>loss_epoch_224</th>\n",
       "      <th>loss_epoch_225</th>\n",
       "      <th>loss_epoch_226</th>\n",
       "      <th>loss_epoch_227</th>\n",
       "      <th>loss_epoch_228</th>\n",
       "      <th>loss_epoch_229</th>\n",
       "      <th>loss_epoch_230</th>\n",
       "      <th>loss_epoch_231</th>\n",
       "      <th>loss_epoch_232</th>\n",
       "      <th>loss_epoch_233</th>\n",
       "      <th>loss_epoch_234</th>\n",
       "      <th>loss_epoch_235</th>\n",
       "      <th>loss_epoch_236</th>\n",
       "      <th>loss_epoch_237</th>\n",
       "      <th>loss_epoch_238</th>\n",
       "      <th>loss_epoch_239</th>\n",
       "      <th>loss_epoch_240</th>\n",
       "      <th>loss_epoch_241</th>\n",
       "      <th>loss_epoch_242</th>\n",
       "      <th>loss_epoch_243</th>\n",
       "      <th>loss_epoch_244</th>\n",
       "      <th>loss_epoch_245</th>\n",
       "      <th>loss_epoch_246</th>\n",
       "      <th>loss_epoch_247</th>\n",
       "      <th>loss_epoch_248</th>\n",
       "      <th>loss_epoch_249</th>\n",
       "      <th>loss_epoch_250</th>\n",
       "      <th>loss_epoch_251</th>\n",
       "      <th>loss_epoch_252</th>\n",
       "      <th>loss_epoch_253</th>\n",
       "      <th>loss_epoch_254</th>\n",
       "      <th>loss_epoch_255</th>\n",
       "      <th>loss_epoch_256</th>\n",
       "      <th>loss_epoch_257</th>\n",
       "      <th>loss_epoch_258</th>\n",
       "      <th>loss_epoch_259</th>\n",
       "      <th>loss_epoch_260</th>\n",
       "      <th>loss_epoch_261</th>\n",
       "      <th>loss_epoch_262</th>\n",
       "      <th>loss_epoch_263</th>\n",
       "      <th>loss_epoch_264</th>\n",
       "      <th>loss_epoch_265</th>\n",
       "      <th>loss_epoch_266</th>\n",
       "      <th>loss_epoch_267</th>\n",
       "      <th>loss_epoch_268</th>\n",
       "      <th>loss_epoch_269</th>\n",
       "      <th>loss_epoch_270</th>\n",
       "      <th>loss_epoch_271</th>\n",
       "      <th>loss_epoch_272</th>\n",
       "      <th>loss_epoch_273</th>\n",
       "      <th>loss_epoch_274</th>\n",
       "      <th>loss_epoch_275</th>\n",
       "      <th>loss_epoch_276</th>\n",
       "      <th>loss_epoch_277</th>\n",
       "      <th>loss_epoch_278</th>\n",
       "      <th>loss_epoch_279</th>\n",
       "      <th>loss_epoch_280</th>\n",
       "      <th>loss_epoch_281</th>\n",
       "      <th>loss_epoch_282</th>\n",
       "      <th>loss_epoch_283</th>\n",
       "      <th>loss_epoch_284</th>\n",
       "      <th>loss_epoch_285</th>\n",
       "      <th>loss_epoch_286</th>\n",
       "      <th>loss_epoch_287</th>\n",
       "      <th>loss_epoch_288</th>\n",
       "      <th>loss_epoch_289</th>\n",
       "      <th>loss_epoch_290</th>\n",
       "      <th>loss_epoch_291</th>\n",
       "      <th>loss_epoch_292</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>9997.000</td>\n",
       "      <td>9996.000</td>\n",
       "      <td>9992.000</td>\n",
       "      <td>9982.000</td>\n",
       "      <td>9962.000</td>\n",
       "      <td>9933.000</td>\n",
       "      <td>9879.000</td>\n",
       "      <td>9803.000</td>\n",
       "      <td>9713.000</td>\n",
       "      <td>9623.000</td>\n",
       "      <td>9547.000</td>\n",
       "      <td>9451.000</td>\n",
       "      <td>9350.000</td>\n",
       "      <td>9253.000</td>\n",
       "      <td>9165.000</td>\n",
       "      <td>9085.000</td>\n",
       "      <td>9010.000</td>\n",
       "      <td>8945.000</td>\n",
       "      <td>8886.000</td>\n",
       "      <td>8830.000</td>\n",
       "      <td>8774.000</td>\n",
       "      <td>8701.000</td>\n",
       "      <td>8634.000</td>\n",
       "      <td>8582.000</td>\n",
       "      <td>8508.000</td>\n",
       "      <td>8436.000</td>\n",
       "      <td>8355.000</td>\n",
       "      <td>8265.000</td>\n",
       "      <td>8167.000</td>\n",
       "      <td>8053.000</td>\n",
       "      <td>7937.000</td>\n",
       "      <td>7797.000</td>\n",
       "      <td>7660.000</td>\n",
       "      <td>7517.000</td>\n",
       "      <td>7374.000</td>\n",
       "      <td>7205.000</td>\n",
       "      <td>7035.000</td>\n",
       "      <td>6871.000</td>\n",
       "      <td>6685.000</td>\n",
       "      <td>6493.000</td>\n",
       "      <td>6323.000</td>\n",
       "      <td>6142.000</td>\n",
       "      <td>5943.000</td>\n",
       "      <td>5764.000</td>\n",
       "      <td>5561.000</td>\n",
       "      <td>5382.000</td>\n",
       "      <td>5198.000</td>\n",
       "      <td>4997.000</td>\n",
       "      <td>4824.000</td>\n",
       "      <td>4667.000</td>\n",
       "      <td>4475.000</td>\n",
       "      <td>4298.000</td>\n",
       "      <td>4128.000</td>\n",
       "      <td>3963.000</td>\n",
       "      <td>3838.000</td>\n",
       "      <td>3685.000</td>\n",
       "      <td>3538.000</td>\n",
       "      <td>3398.000</td>\n",
       "      <td>3290.000</td>\n",
       "      <td>3154.000</td>\n",
       "      <td>3032.000</td>\n",
       "      <td>2917.000</td>\n",
       "      <td>2796.000</td>\n",
       "      <td>2671.000</td>\n",
       "      <td>2586.000</td>\n",
       "      <td>2508.000</td>\n",
       "      <td>2416.000</td>\n",
       "      <td>2323.000</td>\n",
       "      <td>2232.000</td>\n",
       "      <td>2137.000</td>\n",
       "      <td>2054.000</td>\n",
       "      <td>1991.000</td>\n",
       "      <td>1925.000</td>\n",
       "      <td>1861.000</td>\n",
       "      <td>1791.000</td>\n",
       "      <td>1723.000</td>\n",
       "      <td>1672.000</td>\n",
       "      <td>1629.000</td>\n",
       "      <td>1574.000</td>\n",
       "      <td>1522.000</td>\n",
       "      <td>1474.000</td>\n",
       "      <td>1421.000</td>\n",
       "      <td>1373.000</td>\n",
       "      <td>1333.000</td>\n",
       "      <td>1286.000</td>\n",
       "      <td>1253.000</td>\n",
       "      <td>1207.000</td>\n",
       "      <td>1169.000</td>\n",
       "      <td>1122.000</td>\n",
       "      <td>1081.000</td>\n",
       "      <td>1048.000</td>\n",
       "      <td>1016.000</td>\n",
       "      <td>986.000</td>\n",
       "      <td>950.000</td>\n",
       "      <td>921.000</td>\n",
       "      <td>887.000</td>\n",
       "      <td>858.000</td>\n",
       "      <td>823.000</td>\n",
       "      <td>794.000</td>\n",
       "      <td>764.000</td>\n",
       "      <td>728.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>677.000</td>\n",
       "      <td>666.000</td>\n",
       "      <td>641.000</td>\n",
       "      <td>609.000</td>\n",
       "      <td>582.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>499.000</td>\n",
       "      <td>484.000</td>\n",
       "      <td>467.000</td>\n",
       "      <td>445.000</td>\n",
       "      <td>432.000</td>\n",
       "      <td>412.000</td>\n",
       "      <td>393.000</td>\n",
       "      <td>381.000</td>\n",
       "      <td>372.000</td>\n",
       "      <td>358.000</td>\n",
       "      <td>344.000</td>\n",
       "      <td>334.000</td>\n",
       "      <td>321.000</td>\n",
       "      <td>311.000</td>\n",
       "      <td>290.000</td>\n",
       "      <td>280.000</td>\n",
       "      <td>273.000</td>\n",
       "      <td>260.000</td>\n",
       "      <td>256.000</td>\n",
       "      <td>241.000</td>\n",
       "      <td>235.000</td>\n",
       "      <td>227.000</td>\n",
       "      <td>218.000</td>\n",
       "      <td>207.000</td>\n",
       "      <td>202.000</td>\n",
       "      <td>192.000</td>\n",
       "      <td>185.000</td>\n",
       "      <td>176.000</td>\n",
       "      <td>173.000</td>\n",
       "      <td>164.000</td>\n",
       "      <td>159.000</td>\n",
       "      <td>150.000</td>\n",
       "      <td>147.000</td>\n",
       "      <td>143.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>131.000</td>\n",
       "      <td>124.000</td>\n",
       "      <td>119.000</td>\n",
       "      <td>115.000</td>\n",
       "      <td>112.000</td>\n",
       "      <td>106.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>101.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>91.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>77.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>70.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>68.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>63.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>52.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>44.000</td>\n",
       "      <td>43.000</td>\n",
       "      <td>41.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>39.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>36.000</td>\n",
       "      <td>35.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>32.000</td>\n",
       "      <td>31.000</td>\n",
       "      <td>29.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>23.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>19.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4999.500</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.896</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2499.750</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4999.500</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7499.250</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9999.000</td>\n",
       "      <td>1.596</td>\n",
       "      <td>1.193</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  \\\n",
       "count 10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean   4999.500         0.334         0.165         0.111         0.085   \n",
       "std    2886.896         0.235         0.142         0.082         0.058   \n",
       "min       0.000         0.032         0.019         0.010         0.006   \n",
       "25%    2499.750         0.163         0.076         0.055         0.043   \n",
       "50%    4999.500         0.263         0.120         0.087         0.070   \n",
       "75%    7499.250         0.439         0.204         0.145         0.115   \n",
       "max    9999.000         1.596         1.193         0.820         0.450   \n",
       "\n",
       "       loss_epoch_5  loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean          0.071         0.061         0.053         0.047         0.043   \n",
       "std           0.049         0.042         0.036         0.032         0.028   \n",
       "min           0.003         0.002         0.001         0.001         0.001   \n",
       "25%           0.035         0.030         0.027         0.024         0.022   \n",
       "50%           0.059         0.051         0.045         0.041         0.038   \n",
       "75%           0.095         0.080         0.069         0.062         0.058   \n",
       "max           0.367         0.335         0.300         0.262         0.221   \n",
       "\n",
       "       loss_epoch_10  loss_epoch_11  loss_epoch_12  loss_epoch_13  \\\n",
       "count      10000.000      10000.000      10000.000      10000.000   \n",
       "mean           0.040          0.038          0.036          0.035   \n",
       "std            0.026          0.025          0.024          0.023   \n",
       "min            0.001          0.001          0.001          0.001   \n",
       "25%            0.020          0.019          0.018          0.017   \n",
       "50%            0.036          0.034          0.032          0.031   \n",
       "75%            0.055          0.052          0.050          0.049   \n",
       "max            0.184          0.170          0.160          0.151   \n",
       "\n",
       "       loss_epoch_14  loss_epoch_15  loss_epoch_16  loss_epoch_17  \\\n",
       "count      10000.000      10000.000      10000.000      10000.000   \n",
       "mean           0.033          0.032          0.031          0.030   \n",
       "std            0.023          0.022          0.022          0.022   \n",
       "min            0.000          0.000          0.001          0.000   \n",
       "25%            0.016          0.016          0.015          0.014   \n",
       "50%            0.029          0.028          0.027          0.026   \n",
       "75%            0.047          0.045          0.044          0.043   \n",
       "max            0.146          0.144          0.143          0.141   \n",
       "\n",
       "       loss_epoch_18  loss_epoch_19  loss_epoch_20  loss_epoch_21  \\\n",
       "count       9997.000       9996.000       9992.000       9982.000   \n",
       "mean           0.029          0.028          0.027          0.027   \n",
       "std            0.021          0.021          0.021          0.021   \n",
       "min            0.001          0.000          0.000          0.000   \n",
       "25%            0.013          0.012          0.012          0.011   \n",
       "50%            0.024          0.023          0.022          0.021   \n",
       "75%            0.041          0.040          0.039          0.038   \n",
       "max            0.140          0.139          0.137          0.135   \n",
       "\n",
       "       loss_epoch_22  loss_epoch_23  loss_epoch_24  loss_epoch_25  \\\n",
       "count       9962.000       9933.000       9879.000       9803.000   \n",
       "mean           0.026          0.025          0.024          0.023   \n",
       "std            0.020          0.020          0.020          0.019   \n",
       "min            0.000          0.000          0.000          0.000   \n",
       "25%            0.010          0.010          0.009          0.009   \n",
       "50%            0.020          0.019          0.018          0.018   \n",
       "75%            0.037          0.036          0.034          0.033   \n",
       "max            0.133          0.131          0.127          0.124   \n",
       "\n",
       "       loss_epoch_26  loss_epoch_27  loss_epoch_28  loss_epoch_29  \\\n",
       "count       9713.000       9623.000       9547.000       9451.000   \n",
       "mean           0.023          0.022          0.021          0.020   \n",
       "std            0.019          0.018          0.018          0.018   \n",
       "min            0.000          0.000          0.000          0.000   \n",
       "25%            0.009          0.008          0.008          0.008   \n",
       "50%            0.017          0.016          0.015          0.015   \n",
       "75%            0.032          0.031          0.029          0.028   \n",
       "max            0.119          0.116          0.113          0.110   \n",
       "\n",
       "       loss_epoch_30  loss_epoch_31  loss_epoch_32  loss_epoch_33  \\\n",
       "count       9350.000       9253.000       9165.000       9085.000   \n",
       "mean           0.020          0.019          0.018          0.018   \n",
       "std            0.017          0.017          0.016          0.016   \n",
       "min            0.000          0.000          0.000          0.000   \n",
       "25%            0.007          0.007          0.007          0.007   \n",
       "50%            0.014          0.014          0.013          0.013   \n",
       "75%            0.027          0.026          0.025          0.024   \n",
       "max            0.106          0.103          0.099          0.098   \n",
       "\n",
       "       loss_epoch_34  loss_epoch_35  loss_epoch_36  loss_epoch_37  \\\n",
       "count       9010.000       8945.000       8886.000       8830.000   \n",
       "mean           0.017          0.017          0.016          0.016   \n",
       "std            0.015          0.015          0.015          0.014   \n",
       "min            0.000          0.000          0.000          0.000   \n",
       "25%            0.007          0.007          0.007          0.006   \n",
       "50%            0.012          0.012          0.012          0.011   \n",
       "75%            0.023          0.022          0.021          0.020   \n",
       "max            0.098          0.098          0.098          0.098   \n",
       "\n",
       "       loss_epoch_38  loss_epoch_39  loss_epoch_40  loss_epoch_41  \\\n",
       "count       8774.000       8701.000       8634.000       8582.000   \n",
       "mean           0.016          0.015          0.015          0.015   \n",
       "std            0.014          0.014          0.013          0.013   \n",
       "min            0.000          0.000          0.000          0.000   \n",
       "25%            0.006          0.006          0.006          0.006   \n",
       "50%            0.011          0.011          0.011          0.011   \n",
       "75%            0.019          0.019          0.018          0.018   \n",
       "max            0.098          0.098          0.098          0.098   \n",
       "\n",
       "       loss_epoch_42  loss_epoch_43  loss_epoch_44  loss_epoch_45  \\\n",
       "count       8508.000       8436.000       8355.000       8265.000   \n",
       "mean           0.014          0.014          0.014          0.014   \n",
       "std            0.013          0.013          0.012          0.012   \n",
       "min            0.001          0.001          0.001          0.000   \n",
       "25%            0.006          0.006          0.006          0.006   \n",
       "50%            0.011          0.010          0.010          0.010   \n",
       "75%            0.018          0.017          0.017          0.017   \n",
       "max            0.098          0.097          0.097          0.097   \n",
       "\n",
       "       loss_epoch_46  loss_epoch_47  loss_epoch_48  loss_epoch_49  \\\n",
       "count       8167.000       8053.000       7937.000       7797.000   \n",
       "mean           0.014          0.014          0.013          0.013   \n",
       "std            0.012          0.012          0.012          0.011   \n",
       "min            0.000          0.001          0.001          0.000   \n",
       "25%            0.006          0.006          0.006          0.006   \n",
       "50%            0.010          0.010          0.010          0.010   \n",
       "75%            0.017          0.017          0.016          0.016   \n",
       "max            0.097          0.097          0.097          0.097   \n",
       "\n",
       "       loss_epoch_50  loss_epoch_51  loss_epoch_52  loss_epoch_53  \\\n",
       "count       7660.000       7517.000       7374.000       7205.000   \n",
       "mean           0.013          0.013          0.013          0.013   \n",
       "std            0.011          0.011          0.011          0.010   \n",
       "min            0.001          0.001          0.001          0.001   \n",
       "25%            0.006          0.006          0.006          0.006   \n",
       "50%            0.010          0.010          0.010          0.010   \n",
       "75%            0.016          0.016          0.016          0.016   \n",
       "max            0.097          0.097          0.097          0.097   \n",
       "\n",
       "       loss_epoch_54  loss_epoch_55  loss_epoch_56  loss_epoch_57  \\\n",
       "count       7035.000       6871.000       6685.000       6493.000   \n",
       "mean           0.013          0.013          0.013          0.013   \n",
       "std            0.010          0.010          0.010          0.010   \n",
       "min            0.000          0.000          0.000          0.001   \n",
       "25%            0.006          0.006          0.007          0.007   \n",
       "50%            0.010          0.010          0.011          0.011   \n",
       "75%            0.016          0.016          0.016          0.016   \n",
       "max            0.097          0.097          0.097          0.097   \n",
       "\n",
       "       loss_epoch_58  loss_epoch_59  loss_epoch_60  loss_epoch_61  \\\n",
       "count       6323.000       6142.000       5943.000       5764.000   \n",
       "mean           0.013          0.013          0.013          0.013   \n",
       "std            0.010          0.010          0.010          0.010   \n",
       "min            0.000          0.001          0.001          0.001   \n",
       "25%            0.007          0.007          0.007          0.007   \n",
       "50%            0.011          0.011          0.011          0.011   \n",
       "75%            0.016          0.016          0.016          0.016   \n",
       "max            0.097          0.097          0.097          0.097   \n",
       "\n",
       "       loss_epoch_62  loss_epoch_63  loss_epoch_64  loss_epoch_65  \\\n",
       "count       5561.000       5382.000       5198.000       4997.000   \n",
       "mean           0.013          0.013          0.013          0.013   \n",
       "std            0.010          0.009          0.009          0.009   \n",
       "min            0.001          0.001          0.001          0.001   \n",
       "25%            0.007          0.007          0.007          0.007   \n",
       "50%            0.011          0.011          0.011          0.011   \n",
       "75%            0.016          0.016          0.016          0.016   \n",
       "max            0.096          0.096          0.096          0.096   \n",
       "\n",
       "       loss_epoch_66  loss_epoch_67  loss_epoch_68  loss_epoch_69  \\\n",
       "count       4824.000       4667.000       4475.000       4298.000   \n",
       "mean           0.013          0.013          0.013          0.013   \n",
       "std            0.009          0.009          0.009          0.009   \n",
       "min            0.001          0.001          0.001          0.001   \n",
       "25%            0.007          0.007          0.007          0.007   \n",
       "50%            0.011          0.011          0.011          0.010   \n",
       "75%            0.016          0.016          0.016          0.016   \n",
       "max            0.097          0.096          0.096          0.096   \n",
       "\n",
       "       loss_epoch_70  loss_epoch_71  loss_epoch_72  loss_epoch_73  \\\n",
       "count       4128.000       3963.000       3838.000       3685.000   \n",
       "mean           0.013          0.013          0.012          0.012   \n",
       "std            0.009          0.009          0.009          0.009   \n",
       "min            0.001          0.001          0.001          0.001   \n",
       "25%            0.007          0.007          0.007          0.007   \n",
       "50%            0.010          0.010          0.010          0.010   \n",
       "75%            0.016          0.016          0.016          0.016   \n",
       "max            0.096          0.095          0.095          0.094   \n",
       "\n",
       "       loss_epoch_74  loss_epoch_75  loss_epoch_76  loss_epoch_77  \\\n",
       "count       3538.000       3398.000       3290.000       3154.000   \n",
       "mean           0.012          0.012          0.012          0.012   \n",
       "std            0.009          0.009          0.009          0.008   \n",
       "min            0.001          0.001          0.001          0.001   \n",
       "25%            0.007          0.007          0.006          0.007   \n",
       "50%            0.010          0.010          0.010          0.010   \n",
       "75%            0.016          0.016          0.015          0.015   \n",
       "max            0.092          0.090          0.087          0.084   \n",
       "\n",
       "       loss_epoch_78  loss_epoch_79  loss_epoch_80  loss_epoch_81  \\\n",
       "count       3032.000       2917.000       2796.000       2671.000   \n",
       "mean           0.012          0.012          0.012          0.012   \n",
       "std            0.008          0.008          0.008          0.008   \n",
       "min            0.001          0.001          0.001          0.001   \n",
       "25%            0.007          0.006          0.006          0.006   \n",
       "50%            0.010          0.010          0.010          0.010   \n",
       "75%            0.015          0.015          0.015          0.015   \n",
       "max            0.080          0.076          0.072          0.072   \n",
       "\n",
       "       loss_epoch_82  loss_epoch_83  loss_epoch_84  loss_epoch_85  \\\n",
       "count       2586.000       2508.000       2416.000       2323.000   \n",
       "mean           0.012          0.012          0.012          0.012   \n",
       "std            0.008          0.008          0.008          0.008   \n",
       "min            0.001          0.001          0.001          0.001   \n",
       "25%            0.006          0.006          0.006          0.006   \n",
       "50%            0.010          0.010          0.010          0.010   \n",
       "75%            0.015          0.015          0.015          0.015   \n",
       "max            0.073          0.072          0.072          0.071   \n",
       "\n",
       "       loss_epoch_86  loss_epoch_87  loss_epoch_88  loss_epoch_89  \\\n",
       "count       2232.000       2137.000       2054.000       1991.000   \n",
       "mean           0.011          0.011          0.011          0.011   \n",
       "std            0.007          0.007          0.007          0.007   \n",
       "min            0.001          0.001          0.001          0.001   \n",
       "25%            0.006          0.006          0.006          0.006   \n",
       "50%            0.010          0.010          0.010          0.010   \n",
       "75%            0.015          0.014          0.014          0.014   \n",
       "max            0.070          0.069          0.067          0.067   \n",
       "\n",
       "       loss_epoch_90  loss_epoch_91  loss_epoch_92  loss_epoch_93  \\\n",
       "count       1925.000       1861.000       1791.000       1723.000   \n",
       "mean           0.011          0.011          0.011          0.011   \n",
       "std            0.007          0.007          0.007          0.007   \n",
       "min            0.001          0.001          0.001          0.001   \n",
       "25%            0.006          0.006          0.006          0.006   \n",
       "50%            0.009          0.009          0.009          0.009   \n",
       "75%            0.014          0.014          0.014          0.014   \n",
       "max            0.066          0.067          0.066          0.067   \n",
       "\n",
       "       loss_epoch_94  loss_epoch_95  loss_epoch_96  loss_epoch_97  \\\n",
       "count       1672.000       1629.000       1574.000       1522.000   \n",
       "mean           0.011          0.011          0.011          0.011   \n",
       "std            0.006          0.006          0.006          0.006   \n",
       "min            0.001          0.001          0.002          0.002   \n",
       "25%            0.006          0.006          0.006          0.006   \n",
       "50%            0.009          0.009          0.009          0.009   \n",
       "75%            0.014          0.014          0.014          0.014   \n",
       "max            0.067          0.066          0.067          0.066   \n",
       "\n",
       "       loss_epoch_98  loss_epoch_99  loss_epoch_100  loss_epoch_101  \\\n",
       "count       1474.000       1421.000        1373.000        1333.000   \n",
       "mean           0.011          0.011           0.010           0.010   \n",
       "std            0.006          0.006           0.006           0.006   \n",
       "min            0.002          0.002           0.002           0.002   \n",
       "25%            0.006          0.006           0.006           0.006   \n",
       "50%            0.009          0.009           0.009           0.009   \n",
       "75%            0.013          0.013           0.013           0.013   \n",
       "max            0.066          0.065           0.063           0.061   \n",
       "\n",
       "       loss_epoch_102  loss_epoch_103  loss_epoch_104  loss_epoch_105  \\\n",
       "count        1286.000        1253.000        1207.000        1169.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.006           0.006           0.006           0.006   \n",
       "min             0.002           0.001           0.001           0.001   \n",
       "25%             0.006           0.006           0.006           0.006   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.013           0.013           0.013           0.013   \n",
       "max             0.058           0.054           0.052           0.051   \n",
       "\n",
       "       loss_epoch_106  loss_epoch_107  loss_epoch_108  loss_epoch_109  \\\n",
       "count        1122.000        1081.000        1048.000        1016.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.002           0.002           0.001           0.002   \n",
       "25%             0.006           0.006           0.007           0.006   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.013           0.013           0.013           0.012   \n",
       "max             0.051           0.051           0.051           0.051   \n",
       "\n",
       "       loss_epoch_110  loss_epoch_111  loss_epoch_112  loss_epoch_113  \\\n",
       "count         986.000         950.000         921.000         887.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.001           0.001           0.001           0.001   \n",
       "25%             0.006           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.012           0.012           0.012           0.012   \n",
       "max             0.051           0.051           0.051           0.051   \n",
       "\n",
       "       loss_epoch_114  loss_epoch_115  loss_epoch_116  loss_epoch_117  \\\n",
       "count         858.000         823.000         794.000         764.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.001           0.001           0.001           0.002   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.012           0.012           0.012           0.012   \n",
       "max             0.051           0.051           0.050           0.050   \n",
       "\n",
       "       loss_epoch_118  loss_epoch_119  loss_epoch_120  loss_epoch_121  \\\n",
       "count         728.000         705.000         677.000         666.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.001           0.001           0.001           0.001   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.012           0.012           0.012           0.012   \n",
       "max             0.051           0.050           0.050           0.050   \n",
       "\n",
       "       loss_epoch_122  loss_epoch_123  loss_epoch_124  loss_epoch_125  \\\n",
       "count         641.000         609.000         582.000         549.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.001           0.001           0.001           0.001   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.012           0.012           0.012           0.012   \n",
       "max             0.050           0.050           0.050           0.050   \n",
       "\n",
       "       loss_epoch_126  loss_epoch_127  loss_epoch_128  loss_epoch_129  \\\n",
       "count         519.000         499.000         484.000         467.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.001           0.001           0.001           0.001   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.012           0.012           0.012           0.012   \n",
       "max             0.050           0.050           0.050           0.049   \n",
       "\n",
       "       loss_epoch_130  loss_epoch_131  loss_epoch_132  loss_epoch_133  \\\n",
       "count         445.000         432.000         412.000         393.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.004           0.004           0.004   \n",
       "min             0.001           0.001           0.001           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.012           0.012           0.012           0.012   \n",
       "max             0.050           0.050           0.049           0.049   \n",
       "\n",
       "       loss_epoch_134  loss_epoch_135  loss_epoch_136  loss_epoch_137  \\\n",
       "count         381.000         372.000         358.000         344.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.004           0.004           0.004           0.004   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.012           0.012           0.011           0.011   \n",
       "max             0.049           0.049           0.049           0.049   \n",
       "\n",
       "       loss_epoch_138  loss_epoch_139  loss_epoch_140  loss_epoch_141  \\\n",
       "count         334.000         321.000         311.000         290.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.004           0.004           0.005           0.004   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.049           0.049           0.049           0.049   \n",
       "\n",
       "       loss_epoch_142  loss_epoch_143  loss_epoch_144  loss_epoch_145  \\\n",
       "count         280.000         273.000         260.000         256.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.049           0.049           0.048           0.048   \n",
       "\n",
       "       loss_epoch_146  loss_epoch_147  loss_epoch_148  loss_epoch_149  \\\n",
       "count         241.000         235.000         227.000         218.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.048           0.048           0.047           0.047   \n",
       "\n",
       "       loss_epoch_150  loss_epoch_151  loss_epoch_152  loss_epoch_153  \\\n",
       "count         207.000         202.000         192.000         185.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.047           0.047           0.046           0.046   \n",
       "\n",
       "       loss_epoch_154  loss_epoch_155  loss_epoch_156  loss_epoch_157  \\\n",
       "count         176.000         173.000         164.000         159.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.008           0.008           0.008   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.046           0.045           0.045           0.045   \n",
       "\n",
       "       loss_epoch_158  loss_epoch_159  loss_epoch_160  loss_epoch_161  \\\n",
       "count         150.000         147.000         143.000         138.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.009   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.044           0.044           0.044           0.043   \n",
       "\n",
       "       loss_epoch_162  loss_epoch_163  loss_epoch_164  loss_epoch_165  \\\n",
       "count         131.000         124.000         119.000         115.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.009           0.009           0.009           0.008   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.043           0.042           0.042           0.041   \n",
       "\n",
       "       loss_epoch_166  loss_epoch_167  loss_epoch_168  loss_epoch_169  \\\n",
       "count         112.000         106.000         103.000         101.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.005           0.005           0.005           0.005   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.008           0.008           0.008           0.008   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.041           0.041           0.040           0.040   \n",
       "\n",
       "       loss_epoch_170  loss_epoch_171  loss_epoch_172  loss_epoch_173  \\\n",
       "count          94.000          91.000          90.000          86.000   \n",
       "mean            0.009           0.009           0.009           0.009   \n",
       "std             0.004           0.004           0.004           0.004   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.008           0.008           0.008           0.008   \n",
       "75%             0.010           0.011           0.011           0.011   \n",
       "max             0.038           0.036           0.034           0.031   \n",
       "\n",
       "       loss_epoch_174  loss_epoch_175  loss_epoch_176  loss_epoch_177  \\\n",
       "count          84.000          77.000          73.000          70.000   \n",
       "mean            0.009           0.009           0.009           0.009   \n",
       "std             0.004           0.004           0.004           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.008           0.008           0.008           0.008   \n",
       "75%             0.011           0.011           0.010           0.010   \n",
       "max             0.027           0.023           0.021           0.020   \n",
       "\n",
       "       loss_epoch_178  loss_epoch_179  loss_epoch_180  loss_epoch_181  \\\n",
       "count          69.000          68.000          66.000          63.000   \n",
       "mean            0.009           0.009           0.009           0.009   \n",
       "std             0.003           0.003           0.003           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.008           0.008           0.008           0.008   \n",
       "75%             0.010           0.010           0.010           0.011   \n",
       "max             0.020           0.020           0.020           0.020   \n",
       "\n",
       "       loss_epoch_182  loss_epoch_183  loss_epoch_184  loss_epoch_185  \\\n",
       "count          62.000          55.000          55.000          53.000   \n",
       "mean            0.009           0.009           0.009           0.009   \n",
       "std             0.003           0.003           0.003           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.008           0.008           0.008           0.008   \n",
       "75%             0.010           0.010           0.010           0.010   \n",
       "max             0.017           0.017           0.017           0.016   \n",
       "\n",
       "       loss_epoch_186  loss_epoch_187  loss_epoch_188  loss_epoch_189  \\\n",
       "count          52.000          50.000          48.000          47.000   \n",
       "mean            0.009           0.009           0.008           0.009   \n",
       "std             0.003           0.003           0.003           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.008           0.008           0.008           0.008   \n",
       "75%             0.010           0.010           0.010           0.010   \n",
       "max             0.016           0.016           0.016           0.016   \n",
       "\n",
       "       loss_epoch_190  loss_epoch_191  loss_epoch_192  loss_epoch_193  \\\n",
       "count          44.000          43.000          41.000          40.000   \n",
       "mean            0.009           0.009           0.009           0.009   \n",
       "std             0.003           0.003           0.003           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.008           0.008           0.008           0.008   \n",
       "75%             0.010           0.010           0.010           0.011   \n",
       "max             0.016           0.016           0.016           0.016   \n",
       "\n",
       "       loss_epoch_194  loss_epoch_195  loss_epoch_196  loss_epoch_197  \\\n",
       "count          39.000          38.000          36.000          35.000   \n",
       "mean            0.009           0.009           0.008           0.008   \n",
       "std             0.003           0.003           0.003           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.006           0.007   \n",
       "50%             0.008           0.008           0.008           0.008   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.016           0.016           0.016           0.016   \n",
       "\n",
       "       loss_epoch_198  loss_epoch_199  loss_epoch_200  loss_epoch_201  \\\n",
       "count          34.000          33.000          32.000          31.000   \n",
       "mean            0.008           0.009           0.009           0.008   \n",
       "std             0.003           0.003           0.003           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.008           0.008           0.008           0.008   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.016           0.016           0.016           0.016   \n",
       "\n",
       "       loss_epoch_202  loss_epoch_203  loss_epoch_204  loss_epoch_205  \\\n",
       "count          29.000          26.000          25.000          25.000   \n",
       "mean            0.009           0.009           0.008           0.008   \n",
       "std             0.003           0.003           0.003           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.007           0.007           0.007           0.006   \n",
       "50%             0.008           0.008           0.008           0.008   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.016           0.016           0.014           0.014   \n",
       "\n",
       "       loss_epoch_206  loss_epoch_207  loss_epoch_208  loss_epoch_209  \\\n",
       "count          24.000          24.000          24.000          23.000   \n",
       "mean            0.008           0.008           0.008           0.008   \n",
       "std             0.003           0.003           0.003           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.006           0.006           0.006           0.006   \n",
       "50%             0.008           0.008           0.008           0.007   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.014           0.014           0.014           0.014   \n",
       "\n",
       "       loss_epoch_210  loss_epoch_211  loss_epoch_212  loss_epoch_213  \\\n",
       "count          21.000          21.000          21.000          19.000   \n",
       "mean            0.008           0.008           0.008           0.008   \n",
       "std             0.003           0.003           0.003           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.006           0.006           0.006           0.007   \n",
       "50%             0.007           0.008           0.007           0.008   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.014           0.014           0.014           0.014   \n",
       "\n",
       "       loss_epoch_214  loss_epoch_215  loss_epoch_216  loss_epoch_217  \\\n",
       "count          18.000          16.000          14.000          14.000   \n",
       "mean            0.008           0.009           0.008           0.008   \n",
       "std             0.003           0.003           0.003           0.003   \n",
       "min             0.003           0.003           0.003           0.003   \n",
       "25%             0.006           0.007           0.006           0.006   \n",
       "50%             0.008           0.009           0.009           0.009   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.014           0.014           0.012           0.012   \n",
       "\n",
       "       loss_epoch_218  loss_epoch_219  loss_epoch_220  loss_epoch_221  \\\n",
       "count          13.000          13.000          13.000          12.000   \n",
       "mean            0.009           0.009           0.009           0.009   \n",
       "std             0.003           0.003           0.003           0.002   \n",
       "min             0.003           0.003           0.004           0.005   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.010           0.010           0.010           0.010   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.012           0.012           0.012           0.012   \n",
       "\n",
       "       loss_epoch_222  loss_epoch_223  loss_epoch_224  loss_epoch_225  \\\n",
       "count          11.000          10.000          10.000           9.000   \n",
       "mean            0.009           0.009           0.009           0.009   \n",
       "std             0.002           0.002           0.002           0.002   \n",
       "min             0.005           0.005           0.005           0.005   \n",
       "25%             0.007           0.007           0.007           0.007   \n",
       "50%             0.011           0.010           0.010           0.010   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.012           0.012           0.012           0.012   \n",
       "\n",
       "       loss_epoch_226  loss_epoch_227  loss_epoch_228  loss_epoch_229  \\\n",
       "count           8.000           7.000           7.000           7.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.002           0.002           0.002           0.002   \n",
       "min             0.005           0.006           0.006           0.006   \n",
       "25%             0.009           0.010           0.010           0.010   \n",
       "50%             0.011           0.011           0.011           0.011   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.012           0.012           0.012           0.012   \n",
       "\n",
       "       loss_epoch_230  loss_epoch_231  loss_epoch_232  loss_epoch_233  \\\n",
       "count           6.000           5.000           5.000           5.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.002           0.002           0.002           0.002   \n",
       "min             0.006           0.006           0.006           0.006   \n",
       "25%             0.010           0.011           0.010           0.010   \n",
       "50%             0.011           0.011           0.011           0.011   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.012           0.012           0.011           0.011   \n",
       "\n",
       "       loss_epoch_234  loss_epoch_235  loss_epoch_236  loss_epoch_237  \\\n",
       "count           5.000           5.000           5.000           4.000   \n",
       "mean            0.010           0.010           0.010           0.011   \n",
       "std             0.002           0.002           0.002           0.000   \n",
       "min             0.006           0.006           0.006           0.010   \n",
       "25%             0.010           0.010           0.010           0.011   \n",
       "50%             0.011           0.011           0.011           0.011   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.011           0.011           0.011           0.011   \n",
       "\n",
       "       loss_epoch_238  loss_epoch_239  loss_epoch_240  loss_epoch_241  \\\n",
       "count           4.000           4.000           4.000           4.000   \n",
       "mean            0.011           0.011           0.011           0.011   \n",
       "std             0.000           0.000           0.000           0.000   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.011           0.011           0.010           0.010   \n",
       "50%             0.011           0.011           0.011           0.011   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.011           0.011           0.011           0.011   \n",
       "\n",
       "       loss_epoch_242  loss_epoch_243  loss_epoch_244  loss_epoch_245  \\\n",
       "count           4.000           4.000           4.000           4.000   \n",
       "mean            0.011           0.011           0.011           0.011   \n",
       "std             0.000           0.000           0.000           0.000   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.011           0.011           0.011           0.011   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.011           0.011           0.011           0.011   \n",
       "\n",
       "       loss_epoch_246  loss_epoch_247  loss_epoch_248  loss_epoch_249  \\\n",
       "count           4.000           4.000           3.000           3.000   \n",
       "mean            0.011           0.011           0.011           0.011   \n",
       "std             0.000           0.000           0.000           0.000   \n",
       "min             0.011           0.010           0.010           0.010   \n",
       "25%             0.011           0.010           0.010           0.010   \n",
       "50%             0.011           0.011           0.011           0.011   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.011           0.011           0.011           0.011   \n",
       "\n",
       "       loss_epoch_250  loss_epoch_251  loss_epoch_252  loss_epoch_253  \\\n",
       "count           3.000           3.000           3.000           3.000   \n",
       "mean            0.011           0.011           0.010           0.010   \n",
       "std             0.000           0.001           0.000           0.000   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.011           0.011           0.011           0.011   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.011           0.011           0.011           0.011   \n",
       "\n",
       "       loss_epoch_254  loss_epoch_255  loss_epoch_256  loss_epoch_257  \\\n",
       "count           3.000           3.000           3.000           3.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.000           0.000           0.000           0.000   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.011           0.011           0.010           0.011   \n",
       "75%             0.011           0.011           0.011           0.011   \n",
       "max             0.011           0.011           0.011           0.011   \n",
       "\n",
       "       loss_epoch_258  loss_epoch_259  loss_epoch_260  loss_epoch_261  \\\n",
       "count           3.000           3.000           3.000           3.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.000           0.000           0.000           0.000   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.010           0.010           0.010           0.010   \n",
       "75%             0.010           0.011           0.010           0.010   \n",
       "max             0.011           0.011           0.010           0.010   \n",
       "\n",
       "       loss_epoch_262  loss_epoch_263  loss_epoch_264  loss_epoch_265  \\\n",
       "count           3.000           3.000           3.000           3.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.000           0.000           0.000           0.000   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.010           0.010           0.010           0.010   \n",
       "75%             0.010           0.010           0.010           0.010   \n",
       "max             0.010           0.010           0.010           0.010   \n",
       "\n",
       "       loss_epoch_266  loss_epoch_267  loss_epoch_268  loss_epoch_269  \\\n",
       "count           3.000           3.000           3.000           3.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.000           0.000           0.000           0.000   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.010           0.010           0.010           0.010   \n",
       "75%             0.010           0.010           0.010           0.010   \n",
       "max             0.010           0.010           0.010           0.010   \n",
       "\n",
       "       loss_epoch_270  loss_epoch_271  loss_epoch_272  loss_epoch_273  \\\n",
       "count           3.000           3.000           3.000           3.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.000           0.000           0.000           0.000   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.010           0.010           0.010           0.010   \n",
       "75%             0.010           0.010           0.010           0.010   \n",
       "max             0.010           0.010           0.010           0.010   \n",
       "\n",
       "       loss_epoch_274  loss_epoch_275  loss_epoch_276  loss_epoch_277  \\\n",
       "count           3.000           2.000           1.000           1.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std             0.000           0.000             NaN             NaN   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.010           0.010           0.010           0.010   \n",
       "75%             0.010           0.010           0.010           0.010   \n",
       "max             0.010           0.010           0.010           0.010   \n",
       "\n",
       "       loss_epoch_278  loss_epoch_279  loss_epoch_280  loss_epoch_281  \\\n",
       "count           1.000           1.000           1.000           1.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std               NaN             NaN             NaN             NaN   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.010           0.010           0.010           0.010   \n",
       "75%             0.010           0.010           0.010           0.010   \n",
       "max             0.010           0.010           0.010           0.010   \n",
       "\n",
       "       loss_epoch_282  loss_epoch_283  loss_epoch_284  loss_epoch_285  \\\n",
       "count           1.000           1.000           1.000           1.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std               NaN             NaN             NaN             NaN   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.010           0.010           0.010           0.010   \n",
       "75%             0.010           0.010           0.010           0.010   \n",
       "max             0.010           0.010           0.010           0.010   \n",
       "\n",
       "       loss_epoch_286  loss_epoch_287  loss_epoch_288  loss_epoch_289  \\\n",
       "count           1.000           1.000           1.000           1.000   \n",
       "mean            0.010           0.010           0.010           0.010   \n",
       "std               NaN             NaN             NaN             NaN   \n",
       "min             0.010           0.010           0.010           0.010   \n",
       "25%             0.010           0.010           0.010           0.010   \n",
       "50%             0.010           0.010           0.010           0.010   \n",
       "75%             0.010           0.010           0.010           0.010   \n",
       "max             0.010           0.010           0.010           0.010   \n",
       "\n",
       "       loss_epoch_290  loss_epoch_291  loss_epoch_292  \n",
       "count           1.000           1.000           1.000  \n",
       "mean            0.010           0.010           0.010  \n",
       "std               NaN             NaN             NaN  \n",
       "min             0.010           0.010           0.010  \n",
       "25%             0.010           0.010           0.010  \n",
       "50%             0.010           0.010           0.010  \n",
       "75%             0.010           0.010           0.010  \n",
       "max             0.010           0.010           0.010  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.877Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T17:24:27.709222Z",
     "iopub.status.busy": "2021-09-24T17:24:27.709057Z",
     "iopub.status.idle": "2021-09-24T17:24:28.431943Z",
     "shell.execute_reply": "2021-09-24T17:24:28.431225Z",
     "shell.execute_reply.started": "2021-09-24T17:24:27.709199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>val_loss_epoch_11</th>\n",
       "      <th>val_loss_epoch_12</th>\n",
       "      <th>val_loss_epoch_13</th>\n",
       "      <th>val_loss_epoch_14</th>\n",
       "      <th>val_loss_epoch_15</th>\n",
       "      <th>val_loss_epoch_16</th>\n",
       "      <th>val_loss_epoch_17</th>\n",
       "      <th>val_loss_epoch_18</th>\n",
       "      <th>val_loss_epoch_19</th>\n",
       "      <th>val_loss_epoch_20</th>\n",
       "      <th>val_loss_epoch_21</th>\n",
       "      <th>val_loss_epoch_22</th>\n",
       "      <th>val_loss_epoch_23</th>\n",
       "      <th>val_loss_epoch_24</th>\n",
       "      <th>val_loss_epoch_25</th>\n",
       "      <th>val_loss_epoch_26</th>\n",
       "      <th>val_loss_epoch_27</th>\n",
       "      <th>val_loss_epoch_28</th>\n",
       "      <th>val_loss_epoch_29</th>\n",
       "      <th>val_loss_epoch_30</th>\n",
       "      <th>val_loss_epoch_31</th>\n",
       "      <th>val_loss_epoch_32</th>\n",
       "      <th>val_loss_epoch_33</th>\n",
       "      <th>val_loss_epoch_34</th>\n",
       "      <th>val_loss_epoch_35</th>\n",
       "      <th>val_loss_epoch_36</th>\n",
       "      <th>val_loss_epoch_37</th>\n",
       "      <th>val_loss_epoch_38</th>\n",
       "      <th>val_loss_epoch_39</th>\n",
       "      <th>val_loss_epoch_40</th>\n",
       "      <th>val_loss_epoch_41</th>\n",
       "      <th>val_loss_epoch_42</th>\n",
       "      <th>val_loss_epoch_43</th>\n",
       "      <th>val_loss_epoch_44</th>\n",
       "      <th>val_loss_epoch_45</th>\n",
       "      <th>val_loss_epoch_46</th>\n",
       "      <th>val_loss_epoch_47</th>\n",
       "      <th>val_loss_epoch_48</th>\n",
       "      <th>val_loss_epoch_49</th>\n",
       "      <th>val_loss_epoch_50</th>\n",
       "      <th>val_loss_epoch_51</th>\n",
       "      <th>val_loss_epoch_52</th>\n",
       "      <th>val_loss_epoch_53</th>\n",
       "      <th>val_loss_epoch_54</th>\n",
       "      <th>val_loss_epoch_55</th>\n",
       "      <th>val_loss_epoch_56</th>\n",
       "      <th>val_loss_epoch_57</th>\n",
       "      <th>val_loss_epoch_58</th>\n",
       "      <th>val_loss_epoch_59</th>\n",
       "      <th>val_loss_epoch_60</th>\n",
       "      <th>val_loss_epoch_61</th>\n",
       "      <th>val_loss_epoch_62</th>\n",
       "      <th>val_loss_epoch_63</th>\n",
       "      <th>val_loss_epoch_64</th>\n",
       "      <th>val_loss_epoch_65</th>\n",
       "      <th>val_loss_epoch_66</th>\n",
       "      <th>val_loss_epoch_67</th>\n",
       "      <th>val_loss_epoch_68</th>\n",
       "      <th>val_loss_epoch_69</th>\n",
       "      <th>val_loss_epoch_70</th>\n",
       "      <th>val_loss_epoch_71</th>\n",
       "      <th>val_loss_epoch_72</th>\n",
       "      <th>val_loss_epoch_73</th>\n",
       "      <th>val_loss_epoch_74</th>\n",
       "      <th>val_loss_epoch_75</th>\n",
       "      <th>val_loss_epoch_76</th>\n",
       "      <th>val_loss_epoch_77</th>\n",
       "      <th>val_loss_epoch_78</th>\n",
       "      <th>val_loss_epoch_79</th>\n",
       "      <th>val_loss_epoch_80</th>\n",
       "      <th>val_loss_epoch_81</th>\n",
       "      <th>val_loss_epoch_82</th>\n",
       "      <th>val_loss_epoch_83</th>\n",
       "      <th>val_loss_epoch_84</th>\n",
       "      <th>val_loss_epoch_85</th>\n",
       "      <th>val_loss_epoch_86</th>\n",
       "      <th>val_loss_epoch_87</th>\n",
       "      <th>val_loss_epoch_88</th>\n",
       "      <th>val_loss_epoch_89</th>\n",
       "      <th>val_loss_epoch_90</th>\n",
       "      <th>val_loss_epoch_91</th>\n",
       "      <th>val_loss_epoch_92</th>\n",
       "      <th>val_loss_epoch_93</th>\n",
       "      <th>val_loss_epoch_94</th>\n",
       "      <th>val_loss_epoch_95</th>\n",
       "      <th>val_loss_epoch_96</th>\n",
       "      <th>val_loss_epoch_97</th>\n",
       "      <th>val_loss_epoch_98</th>\n",
       "      <th>val_loss_epoch_99</th>\n",
       "      <th>val_loss_epoch_100</th>\n",
       "      <th>val_loss_epoch_101</th>\n",
       "      <th>val_loss_epoch_102</th>\n",
       "      <th>val_loss_epoch_103</th>\n",
       "      <th>val_loss_epoch_104</th>\n",
       "      <th>val_loss_epoch_105</th>\n",
       "      <th>val_loss_epoch_106</th>\n",
       "      <th>val_loss_epoch_107</th>\n",
       "      <th>val_loss_epoch_108</th>\n",
       "      <th>val_loss_epoch_109</th>\n",
       "      <th>val_loss_epoch_110</th>\n",
       "      <th>val_loss_epoch_111</th>\n",
       "      <th>val_loss_epoch_112</th>\n",
       "      <th>val_loss_epoch_113</th>\n",
       "      <th>val_loss_epoch_114</th>\n",
       "      <th>val_loss_epoch_115</th>\n",
       "      <th>val_loss_epoch_116</th>\n",
       "      <th>val_loss_epoch_117</th>\n",
       "      <th>val_loss_epoch_118</th>\n",
       "      <th>val_loss_epoch_119</th>\n",
       "      <th>val_loss_epoch_120</th>\n",
       "      <th>val_loss_epoch_121</th>\n",
       "      <th>val_loss_epoch_122</th>\n",
       "      <th>val_loss_epoch_123</th>\n",
       "      <th>val_loss_epoch_124</th>\n",
       "      <th>val_loss_epoch_125</th>\n",
       "      <th>val_loss_epoch_126</th>\n",
       "      <th>val_loss_epoch_127</th>\n",
       "      <th>val_loss_epoch_128</th>\n",
       "      <th>val_loss_epoch_129</th>\n",
       "      <th>val_loss_epoch_130</th>\n",
       "      <th>val_loss_epoch_131</th>\n",
       "      <th>val_loss_epoch_132</th>\n",
       "      <th>val_loss_epoch_133</th>\n",
       "      <th>val_loss_epoch_134</th>\n",
       "      <th>val_loss_epoch_135</th>\n",
       "      <th>val_loss_epoch_136</th>\n",
       "      <th>val_loss_epoch_137</th>\n",
       "      <th>val_loss_epoch_138</th>\n",
       "      <th>val_loss_epoch_139</th>\n",
       "      <th>val_loss_epoch_140</th>\n",
       "      <th>val_loss_epoch_141</th>\n",
       "      <th>val_loss_epoch_142</th>\n",
       "      <th>val_loss_epoch_143</th>\n",
       "      <th>val_loss_epoch_144</th>\n",
       "      <th>val_loss_epoch_145</th>\n",
       "      <th>val_loss_epoch_146</th>\n",
       "      <th>val_loss_epoch_147</th>\n",
       "      <th>val_loss_epoch_148</th>\n",
       "      <th>val_loss_epoch_149</th>\n",
       "      <th>val_loss_epoch_150</th>\n",
       "      <th>val_loss_epoch_151</th>\n",
       "      <th>val_loss_epoch_152</th>\n",
       "      <th>val_loss_epoch_153</th>\n",
       "      <th>val_loss_epoch_154</th>\n",
       "      <th>val_loss_epoch_155</th>\n",
       "      <th>val_loss_epoch_156</th>\n",
       "      <th>val_loss_epoch_157</th>\n",
       "      <th>val_loss_epoch_158</th>\n",
       "      <th>val_loss_epoch_159</th>\n",
       "      <th>val_loss_epoch_160</th>\n",
       "      <th>val_loss_epoch_161</th>\n",
       "      <th>val_loss_epoch_162</th>\n",
       "      <th>val_loss_epoch_163</th>\n",
       "      <th>val_loss_epoch_164</th>\n",
       "      <th>val_loss_epoch_165</th>\n",
       "      <th>val_loss_epoch_166</th>\n",
       "      <th>val_loss_epoch_167</th>\n",
       "      <th>val_loss_epoch_168</th>\n",
       "      <th>val_loss_epoch_169</th>\n",
       "      <th>val_loss_epoch_170</th>\n",
       "      <th>val_loss_epoch_171</th>\n",
       "      <th>val_loss_epoch_172</th>\n",
       "      <th>val_loss_epoch_173</th>\n",
       "      <th>val_loss_epoch_174</th>\n",
       "      <th>val_loss_epoch_175</th>\n",
       "      <th>val_loss_epoch_176</th>\n",
       "      <th>val_loss_epoch_177</th>\n",
       "      <th>val_loss_epoch_178</th>\n",
       "      <th>val_loss_epoch_179</th>\n",
       "      <th>val_loss_epoch_180</th>\n",
       "      <th>val_loss_epoch_181</th>\n",
       "      <th>val_loss_epoch_182</th>\n",
       "      <th>val_loss_epoch_183</th>\n",
       "      <th>val_loss_epoch_184</th>\n",
       "      <th>val_loss_epoch_185</th>\n",
       "      <th>val_loss_epoch_186</th>\n",
       "      <th>val_loss_epoch_187</th>\n",
       "      <th>val_loss_epoch_188</th>\n",
       "      <th>val_loss_epoch_189</th>\n",
       "      <th>val_loss_epoch_190</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "      <th>val_loss_epoch_201</th>\n",
       "      <th>val_loss_epoch_202</th>\n",
       "      <th>val_loss_epoch_203</th>\n",
       "      <th>val_loss_epoch_204</th>\n",
       "      <th>val_loss_epoch_205</th>\n",
       "      <th>val_loss_epoch_206</th>\n",
       "      <th>val_loss_epoch_207</th>\n",
       "      <th>val_loss_epoch_208</th>\n",
       "      <th>val_loss_epoch_209</th>\n",
       "      <th>val_loss_epoch_210</th>\n",
       "      <th>val_loss_epoch_211</th>\n",
       "      <th>val_loss_epoch_212</th>\n",
       "      <th>val_loss_epoch_213</th>\n",
       "      <th>val_loss_epoch_214</th>\n",
       "      <th>val_loss_epoch_215</th>\n",
       "      <th>val_loss_epoch_216</th>\n",
       "      <th>val_loss_epoch_217</th>\n",
       "      <th>val_loss_epoch_218</th>\n",
       "      <th>val_loss_epoch_219</th>\n",
       "      <th>val_loss_epoch_220</th>\n",
       "      <th>val_loss_epoch_221</th>\n",
       "      <th>val_loss_epoch_222</th>\n",
       "      <th>val_loss_epoch_223</th>\n",
       "      <th>val_loss_epoch_224</th>\n",
       "      <th>val_loss_epoch_225</th>\n",
       "      <th>val_loss_epoch_226</th>\n",
       "      <th>val_loss_epoch_227</th>\n",
       "      <th>val_loss_epoch_228</th>\n",
       "      <th>val_loss_epoch_229</th>\n",
       "      <th>val_loss_epoch_230</th>\n",
       "      <th>val_loss_epoch_231</th>\n",
       "      <th>val_loss_epoch_232</th>\n",
       "      <th>val_loss_epoch_233</th>\n",
       "      <th>val_loss_epoch_234</th>\n",
       "      <th>val_loss_epoch_235</th>\n",
       "      <th>val_loss_epoch_236</th>\n",
       "      <th>val_loss_epoch_237</th>\n",
       "      <th>val_loss_epoch_238</th>\n",
       "      <th>val_loss_epoch_239</th>\n",
       "      <th>val_loss_epoch_240</th>\n",
       "      <th>val_loss_epoch_241</th>\n",
       "      <th>val_loss_epoch_242</th>\n",
       "      <th>val_loss_epoch_243</th>\n",
       "      <th>val_loss_epoch_244</th>\n",
       "      <th>val_loss_epoch_245</th>\n",
       "      <th>val_loss_epoch_246</th>\n",
       "      <th>val_loss_epoch_247</th>\n",
       "      <th>val_loss_epoch_248</th>\n",
       "      <th>val_loss_epoch_249</th>\n",
       "      <th>val_loss_epoch_250</th>\n",
       "      <th>val_loss_epoch_251</th>\n",
       "      <th>val_loss_epoch_252</th>\n",
       "      <th>val_loss_epoch_253</th>\n",
       "      <th>val_loss_epoch_254</th>\n",
       "      <th>val_loss_epoch_255</th>\n",
       "      <th>val_loss_epoch_256</th>\n",
       "      <th>val_loss_epoch_257</th>\n",
       "      <th>val_loss_epoch_258</th>\n",
       "      <th>val_loss_epoch_259</th>\n",
       "      <th>val_loss_epoch_260</th>\n",
       "      <th>val_loss_epoch_261</th>\n",
       "      <th>val_loss_epoch_262</th>\n",
       "      <th>val_loss_epoch_263</th>\n",
       "      <th>val_loss_epoch_264</th>\n",
       "      <th>val_loss_epoch_265</th>\n",
       "      <th>val_loss_epoch_266</th>\n",
       "      <th>val_loss_epoch_267</th>\n",
       "      <th>val_loss_epoch_268</th>\n",
       "      <th>val_loss_epoch_269</th>\n",
       "      <th>val_loss_epoch_270</th>\n",
       "      <th>val_loss_epoch_271</th>\n",
       "      <th>val_loss_epoch_272</th>\n",
       "      <th>val_loss_epoch_273</th>\n",
       "      <th>val_loss_epoch_274</th>\n",
       "      <th>val_loss_epoch_275</th>\n",
       "      <th>val_loss_epoch_276</th>\n",
       "      <th>val_loss_epoch_277</th>\n",
       "      <th>val_loss_epoch_278</th>\n",
       "      <th>val_loss_epoch_279</th>\n",
       "      <th>val_loss_epoch_280</th>\n",
       "      <th>val_loss_epoch_281</th>\n",
       "      <th>val_loss_epoch_282</th>\n",
       "      <th>val_loss_epoch_283</th>\n",
       "      <th>val_loss_epoch_284</th>\n",
       "      <th>val_loss_epoch_285</th>\n",
       "      <th>val_loss_epoch_286</th>\n",
       "      <th>val_loss_epoch_287</th>\n",
       "      <th>val_loss_epoch_288</th>\n",
       "      <th>val_loss_epoch_289</th>\n",
       "      <th>val_loss_epoch_290</th>\n",
       "      <th>val_loss_epoch_291</th>\n",
       "      <th>val_loss_epoch_292</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>9997.000</td>\n",
       "      <td>9996.000</td>\n",
       "      <td>9992.000</td>\n",
       "      <td>9982.000</td>\n",
       "      <td>9962.000</td>\n",
       "      <td>9933.000</td>\n",
       "      <td>9879.000</td>\n",
       "      <td>9803.000</td>\n",
       "      <td>9713.000</td>\n",
       "      <td>9623.000</td>\n",
       "      <td>9547.000</td>\n",
       "      <td>9451.000</td>\n",
       "      <td>9350.000</td>\n",
       "      <td>9253.000</td>\n",
       "      <td>9165.000</td>\n",
       "      <td>9085.000</td>\n",
       "      <td>9010.000</td>\n",
       "      <td>8945.000</td>\n",
       "      <td>8886.000</td>\n",
       "      <td>8830.000</td>\n",
       "      <td>8774.000</td>\n",
       "      <td>8701.000</td>\n",
       "      <td>8634.000</td>\n",
       "      <td>8582.000</td>\n",
       "      <td>8508.000</td>\n",
       "      <td>8436.000</td>\n",
       "      <td>8355.000</td>\n",
       "      <td>8265.000</td>\n",
       "      <td>8167.000</td>\n",
       "      <td>8053.000</td>\n",
       "      <td>7937.000</td>\n",
       "      <td>7797.000</td>\n",
       "      <td>7660.000</td>\n",
       "      <td>7517.000</td>\n",
       "      <td>7374.000</td>\n",
       "      <td>7205.000</td>\n",
       "      <td>7035.000</td>\n",
       "      <td>6871.000</td>\n",
       "      <td>6685.000</td>\n",
       "      <td>6493.000</td>\n",
       "      <td>6323.000</td>\n",
       "      <td>6142.000</td>\n",
       "      <td>5943.000</td>\n",
       "      <td>5764.000</td>\n",
       "      <td>5561.000</td>\n",
       "      <td>5382.000</td>\n",
       "      <td>5198.000</td>\n",
       "      <td>4997.000</td>\n",
       "      <td>4824.000</td>\n",
       "      <td>4667.000</td>\n",
       "      <td>4475.000</td>\n",
       "      <td>4298.000</td>\n",
       "      <td>4128.000</td>\n",
       "      <td>3963.000</td>\n",
       "      <td>3838.000</td>\n",
       "      <td>3685.000</td>\n",
       "      <td>3538.000</td>\n",
       "      <td>3398.000</td>\n",
       "      <td>3290.000</td>\n",
       "      <td>3154.000</td>\n",
       "      <td>3032.000</td>\n",
       "      <td>2917.000</td>\n",
       "      <td>2796.000</td>\n",
       "      <td>2671.000</td>\n",
       "      <td>2586.000</td>\n",
       "      <td>2508.000</td>\n",
       "      <td>2416.000</td>\n",
       "      <td>2323.000</td>\n",
       "      <td>2232.000</td>\n",
       "      <td>2137.000</td>\n",
       "      <td>2054.000</td>\n",
       "      <td>1991.000</td>\n",
       "      <td>1925.000</td>\n",
       "      <td>1861.000</td>\n",
       "      <td>1791.000</td>\n",
       "      <td>1723.000</td>\n",
       "      <td>1672.000</td>\n",
       "      <td>1629.000</td>\n",
       "      <td>1574.000</td>\n",
       "      <td>1522.000</td>\n",
       "      <td>1474.000</td>\n",
       "      <td>1421.000</td>\n",
       "      <td>1373.000</td>\n",
       "      <td>1333.000</td>\n",
       "      <td>1286.000</td>\n",
       "      <td>1253.000</td>\n",
       "      <td>1207.000</td>\n",
       "      <td>1169.000</td>\n",
       "      <td>1122.000</td>\n",
       "      <td>1081.000</td>\n",
       "      <td>1048.000</td>\n",
       "      <td>1016.000</td>\n",
       "      <td>986.000</td>\n",
       "      <td>950.000</td>\n",
       "      <td>921.000</td>\n",
       "      <td>887.000</td>\n",
       "      <td>858.000</td>\n",
       "      <td>823.000</td>\n",
       "      <td>794.000</td>\n",
       "      <td>764.000</td>\n",
       "      <td>728.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>677.000</td>\n",
       "      <td>666.000</td>\n",
       "      <td>641.000</td>\n",
       "      <td>609.000</td>\n",
       "      <td>582.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>499.000</td>\n",
       "      <td>484.000</td>\n",
       "      <td>467.000</td>\n",
       "      <td>445.000</td>\n",
       "      <td>432.000</td>\n",
       "      <td>412.000</td>\n",
       "      <td>393.000</td>\n",
       "      <td>381.000</td>\n",
       "      <td>372.000</td>\n",
       "      <td>358.000</td>\n",
       "      <td>344.000</td>\n",
       "      <td>334.000</td>\n",
       "      <td>321.000</td>\n",
       "      <td>311.000</td>\n",
       "      <td>290.000</td>\n",
       "      <td>280.000</td>\n",
       "      <td>273.000</td>\n",
       "      <td>260.000</td>\n",
       "      <td>256.000</td>\n",
       "      <td>241.000</td>\n",
       "      <td>235.000</td>\n",
       "      <td>227.000</td>\n",
       "      <td>218.000</td>\n",
       "      <td>207.000</td>\n",
       "      <td>202.000</td>\n",
       "      <td>192.000</td>\n",
       "      <td>185.000</td>\n",
       "      <td>176.000</td>\n",
       "      <td>173.000</td>\n",
       "      <td>164.000</td>\n",
       "      <td>159.000</td>\n",
       "      <td>150.000</td>\n",
       "      <td>147.000</td>\n",
       "      <td>143.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>131.000</td>\n",
       "      <td>124.000</td>\n",
       "      <td>119.000</td>\n",
       "      <td>115.000</td>\n",
       "      <td>112.000</td>\n",
       "      <td>106.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>101.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>91.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>77.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>70.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>68.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>63.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>52.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>44.000</td>\n",
       "      <td>43.000</td>\n",
       "      <td>41.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>39.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>36.000</td>\n",
       "      <td>35.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>32.000</td>\n",
       "      <td>31.000</td>\n",
       "      <td>29.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>23.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>19.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4999.500</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.896</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2499.750</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4999.500</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7499.250</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9999.000</td>\n",
       "      <td>1.372</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  \\\n",
       "count 10000.000         10000.000         10000.000         10000.000   \n",
       "mean   4999.500             0.214             0.130             0.095   \n",
       "std    2886.896             0.186             0.105             0.066   \n",
       "min       0.000             0.023             0.014             0.007   \n",
       "25%    2499.750             0.092             0.063             0.047   \n",
       "50%    4999.500             0.151             0.099             0.076   \n",
       "75%    7499.250             0.268             0.164             0.127   \n",
       "max    9999.000             1.372             0.992             0.624   \n",
       "\n",
       "       val_loss_epoch_4  val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              0.077             0.065             0.056             0.049   \n",
       "std               0.053             0.046             0.039             0.034   \n",
       "min               0.004             0.003             0.001             0.001   \n",
       "25%               0.038             0.032             0.028             0.025   \n",
       "50%               0.063             0.055             0.048             0.043   \n",
       "75%               0.104             0.086             0.073             0.065   \n",
       "max               0.390             0.357             0.322             0.285   \n",
       "\n",
       "       val_loss_epoch_8  val_loss_epoch_9  val_loss_epoch_10  \\\n",
       "count         10000.000         10000.000          10000.000   \n",
       "mean              0.045             0.041              0.039   \n",
       "std               0.030             0.027              0.025   \n",
       "min               0.001             0.000              0.000   \n",
       "25%               0.023             0.021              0.020   \n",
       "50%               0.039             0.037              0.035   \n",
       "75%               0.060             0.056              0.053   \n",
       "max               0.244             0.201              0.183   \n",
       "\n",
       "       val_loss_epoch_11  val_loss_epoch_12  val_loss_epoch_13  \\\n",
       "count          10000.000          10000.000          10000.000   \n",
       "mean               0.037              0.035              0.034   \n",
       "std                0.024              0.023              0.023   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.018              0.018              0.017   \n",
       "50%                0.033              0.032              0.030   \n",
       "75%                0.051              0.050              0.048   \n",
       "max                0.170              0.162              0.153   \n",
       "\n",
       "       val_loss_epoch_14  val_loss_epoch_15  val_loss_epoch_16  \\\n",
       "count          10000.000          10000.000          10000.000   \n",
       "mean               0.033              0.032              0.031   \n",
       "std                0.023              0.022              0.022   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.016              0.015              0.014   \n",
       "50%                0.029              0.028              0.026   \n",
       "75%                0.046              0.045              0.044   \n",
       "max                0.146              0.144              0.143   \n",
       "\n",
       "       val_loss_epoch_17  val_loss_epoch_18  val_loss_epoch_19  \\\n",
       "count          10000.000           9997.000           9996.000   \n",
       "mean               0.030              0.029              0.028   \n",
       "std                0.022              0.021              0.021   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.014              0.013              0.012   \n",
       "50%                0.025              0.024              0.023   \n",
       "75%                0.042              0.041              0.040   \n",
       "max                0.142              0.142              0.141   \n",
       "\n",
       "       val_loss_epoch_20  val_loss_epoch_21  val_loss_epoch_22  \\\n",
       "count           9992.000           9982.000           9962.000   \n",
       "mean               0.027              0.026              0.025   \n",
       "std                0.021              0.020              0.020   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.011              0.011              0.010   \n",
       "50%                0.022              0.021              0.020   \n",
       "75%                0.039              0.038              0.036   \n",
       "max                0.138              0.136              0.134   \n",
       "\n",
       "       val_loss_epoch_23  val_loss_epoch_24  val_loss_epoch_25  \\\n",
       "count           9933.000           9879.000           9803.000   \n",
       "mean               0.025              0.024              0.023   \n",
       "std                0.020              0.020              0.019   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.010              0.009              0.009   \n",
       "50%                0.019              0.018              0.017   \n",
       "75%                0.035              0.034              0.033   \n",
       "max                0.130              0.126              0.123   \n",
       "\n",
       "       val_loss_epoch_26  val_loss_epoch_27  val_loss_epoch_28  \\\n",
       "count           9713.000           9623.000           9547.000   \n",
       "mean               0.022              0.022              0.021   \n",
       "std                0.019              0.018              0.018   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.008              0.008              0.008   \n",
       "50%                0.016              0.016              0.015   \n",
       "75%                0.032              0.030              0.029   \n",
       "max                0.119              0.116              0.112   \n",
       "\n",
       "       val_loss_epoch_29  val_loss_epoch_30  val_loss_epoch_31  \\\n",
       "count           9451.000           9350.000           9253.000   \n",
       "mean               0.020              0.019              0.019   \n",
       "std                0.017              0.017              0.017   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.008              0.007              0.007   \n",
       "50%                0.014              0.014              0.013   \n",
       "75%                0.028              0.026              0.025   \n",
       "max                0.109              0.106              0.103   \n",
       "\n",
       "       val_loss_epoch_32  val_loss_epoch_33  val_loss_epoch_34  \\\n",
       "count           9165.000           9085.000           9010.000   \n",
       "mean               0.018              0.018              0.017   \n",
       "std                0.016              0.016              0.015   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.007              0.007              0.007   \n",
       "50%                0.013              0.013              0.012   \n",
       "75%                0.024              0.023              0.022   \n",
       "max                0.100              0.100              0.100   \n",
       "\n",
       "       val_loss_epoch_35  val_loss_epoch_36  val_loss_epoch_37  \\\n",
       "count           8945.000           8886.000           8830.000   \n",
       "mean               0.017              0.016              0.016   \n",
       "std                0.015              0.015              0.014   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.007              0.007              0.006   \n",
       "50%                0.012              0.012              0.011   \n",
       "75%                0.021              0.021              0.020   \n",
       "max                0.100              0.100              0.100   \n",
       "\n",
       "       val_loss_epoch_38  val_loss_epoch_39  val_loss_epoch_40  \\\n",
       "count           8774.000           8701.000           8634.000   \n",
       "mean               0.015              0.015              0.015   \n",
       "std                0.014              0.014              0.013   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.006              0.006              0.006   \n",
       "50%                0.011              0.011              0.011   \n",
       "75%                0.019              0.019              0.018   \n",
       "max                0.100              0.100              0.100   \n",
       "\n",
       "       val_loss_epoch_41  val_loss_epoch_42  val_loss_epoch_43  \\\n",
       "count           8582.000           8508.000           8436.000   \n",
       "mean               0.015              0.014              0.014   \n",
       "std                0.013              0.013              0.012   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.006              0.006              0.006   \n",
       "50%                0.011              0.011              0.010   \n",
       "75%                0.018              0.018              0.017   \n",
       "max                0.100              0.099              0.100   \n",
       "\n",
       "       val_loss_epoch_44  val_loss_epoch_45  val_loss_epoch_46  \\\n",
       "count           8355.000           8265.000           8167.000   \n",
       "mean               0.014              0.014              0.014   \n",
       "std                0.012              0.012              0.012   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.006              0.006              0.006   \n",
       "50%                0.010              0.010              0.010   \n",
       "75%                0.017              0.017              0.017   \n",
       "max                0.100              0.099              0.099   \n",
       "\n",
       "       val_loss_epoch_47  val_loss_epoch_48  val_loss_epoch_49  \\\n",
       "count           8053.000           7937.000           7797.000   \n",
       "mean               0.014              0.013              0.013   \n",
       "std                0.012              0.011              0.011   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.006              0.006              0.006   \n",
       "50%                0.010              0.010              0.010   \n",
       "75%                0.017              0.016              0.016   \n",
       "max                0.099              0.099              0.099   \n",
       "\n",
       "       val_loss_epoch_50  val_loss_epoch_51  val_loss_epoch_52  \\\n",
       "count           7660.000           7517.000           7374.000   \n",
       "mean               0.013              0.013              0.013   \n",
       "std                0.011              0.011              0.011   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.006              0.006              0.006   \n",
       "50%                0.010              0.010              0.010   \n",
       "75%                0.016              0.016              0.016   \n",
       "max                0.099              0.099              0.099   \n",
       "\n",
       "       val_loss_epoch_53  val_loss_epoch_54  val_loss_epoch_55  \\\n",
       "count           7205.000           7035.000           6871.000   \n",
       "mean               0.013              0.013              0.013   \n",
       "std                0.010              0.010              0.010   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.006              0.006              0.006   \n",
       "50%                0.010              0.010              0.010   \n",
       "75%                0.016              0.016              0.016   \n",
       "max                0.099              0.099              0.101   \n",
       "\n",
       "       val_loss_epoch_56  val_loss_epoch_57  val_loss_epoch_58  \\\n",
       "count           6685.000           6493.000           6323.000   \n",
       "mean               0.013              0.013              0.013   \n",
       "std                0.010              0.010              0.010   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.006              0.007              0.007   \n",
       "50%                0.011              0.011              0.011   \n",
       "75%                0.016              0.016              0.016   \n",
       "max                0.099              0.099              0.099   \n",
       "\n",
       "       val_loss_epoch_59  val_loss_epoch_60  val_loss_epoch_61  \\\n",
       "count           6142.000           5943.000           5764.000   \n",
       "mean               0.013              0.013              0.013   \n",
       "std                0.010              0.010              0.010   \n",
       "min                0.000              0.000              0.000   \n",
       "25%                0.007              0.007              0.007   \n",
       "50%                0.011              0.011              0.011   \n",
       "75%                0.016              0.016              0.016   \n",
       "max                0.099              0.099              0.099   \n",
       "\n",
       "       val_loss_epoch_62  val_loss_epoch_63  val_loss_epoch_64  \\\n",
       "count           5561.000           5382.000           5198.000   \n",
       "mean               0.013              0.013              0.013   \n",
       "std                0.010              0.009              0.009   \n",
       "min                0.001              0.000              0.000   \n",
       "25%                0.007              0.007              0.007   \n",
       "50%                0.011              0.011              0.011   \n",
       "75%                0.016              0.016              0.016   \n",
       "max                0.099              0.099              0.099   \n",
       "\n",
       "       val_loss_epoch_65  val_loss_epoch_66  val_loss_epoch_67  \\\n",
       "count           4997.000           4824.000           4667.000   \n",
       "mean               0.013              0.013              0.013   \n",
       "std                0.009              0.009              0.009   \n",
       "min                0.000              0.001              0.000   \n",
       "25%                0.007              0.007              0.007   \n",
       "50%                0.011              0.011              0.011   \n",
       "75%                0.016              0.016              0.016   \n",
       "max                0.099              0.099              0.098   \n",
       "\n",
       "       val_loss_epoch_68  val_loss_epoch_69  val_loss_epoch_70  \\\n",
       "count           4475.000           4298.000           4128.000   \n",
       "mean               0.013              0.013              0.013   \n",
       "std                0.009              0.009              0.009   \n",
       "min                0.001              0.001              0.001   \n",
       "25%                0.007              0.007              0.007   \n",
       "50%                0.011              0.010              0.010   \n",
       "75%                0.016              0.016              0.016   \n",
       "max                0.098              0.099              0.098   \n",
       "\n",
       "       val_loss_epoch_71  val_loss_epoch_72  val_loss_epoch_73  \\\n",
       "count           3963.000           3838.000           3685.000   \n",
       "mean               0.013              0.013              0.012   \n",
       "std                0.009              0.009              0.009   \n",
       "min                0.001              0.001              0.000   \n",
       "25%                0.007              0.007              0.007   \n",
       "50%                0.010              0.010              0.010   \n",
       "75%                0.016              0.016              0.016   \n",
       "max                0.097              0.096              0.095   \n",
       "\n",
       "       val_loss_epoch_74  val_loss_epoch_75  val_loss_epoch_76  \\\n",
       "count           3538.000           3398.000           3290.000   \n",
       "mean               0.012              0.012              0.012   \n",
       "std                0.009              0.009              0.009   \n",
       "min                0.001              0.001              0.001   \n",
       "25%                0.007              0.007              0.007   \n",
       "50%                0.010              0.010              0.010   \n",
       "75%                0.016              0.016              0.016   \n",
       "max                0.093              0.091              0.088   \n",
       "\n",
       "       val_loss_epoch_77  val_loss_epoch_78  val_loss_epoch_79  \\\n",
       "count           3154.000           3032.000           2917.000   \n",
       "mean               0.012              0.012              0.012   \n",
       "std                0.009              0.008              0.008   \n",
       "min                0.001              0.000              0.001   \n",
       "25%                0.007              0.007              0.007   \n",
       "50%                0.010              0.010              0.010   \n",
       "75%                0.015              0.015              0.015   \n",
       "max                0.084              0.081              0.076   \n",
       "\n",
       "       val_loss_epoch_80  val_loss_epoch_81  val_loss_epoch_82  \\\n",
       "count           2796.000           2671.000           2586.000   \n",
       "mean               0.012              0.012              0.012   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.000              0.001              0.001   \n",
       "25%                0.006              0.007              0.007   \n",
       "50%                0.010              0.010              0.010   \n",
       "75%                0.015              0.015              0.015   \n",
       "max                0.076              0.076              0.076   \n",
       "\n",
       "       val_loss_epoch_83  val_loss_epoch_84  val_loss_epoch_85  \\\n",
       "count           2508.000           2416.000           2323.000   \n",
       "mean               0.012              0.012              0.012   \n",
       "std                0.008              0.008              0.008   \n",
       "min                0.001              0.001              0.001   \n",
       "25%                0.006              0.006              0.006   \n",
       "50%                0.010              0.010              0.010   \n",
       "75%                0.015              0.015              0.015   \n",
       "max                0.076              0.075              0.076   \n",
       "\n",
       "       val_loss_epoch_86  val_loss_epoch_87  val_loss_epoch_88  \\\n",
       "count           2232.000           2137.000           2054.000   \n",
       "mean               0.012              0.011              0.011   \n",
       "std                0.008              0.007              0.007   \n",
       "min                0.001              0.001              0.001   \n",
       "25%                0.007              0.007              0.007   \n",
       "50%                0.010              0.010              0.010   \n",
       "75%                0.015              0.015              0.014   \n",
       "max                0.074              0.071              0.069   \n",
       "\n",
       "       val_loss_epoch_89  val_loss_epoch_90  val_loss_epoch_91  \\\n",
       "count           1991.000           1925.000           1861.000   \n",
       "mean               0.011              0.011              0.011   \n",
       "std                0.007              0.007              0.007   \n",
       "min                0.001              0.001              0.001   \n",
       "25%                0.007              0.006              0.007   \n",
       "50%                0.010              0.009              0.009   \n",
       "75%                0.014              0.014              0.014   \n",
       "max                0.067              0.064              0.065   \n",
       "\n",
       "       val_loss_epoch_92  val_loss_epoch_93  val_loss_epoch_94  \\\n",
       "count           1791.000           1723.000           1672.000   \n",
       "mean               0.011              0.011              0.011   \n",
       "std                0.007              0.007              0.006   \n",
       "min                0.001              0.001              0.001   \n",
       "25%                0.006              0.006              0.006   \n",
       "50%                0.009              0.009              0.009   \n",
       "75%                0.014              0.014              0.014   \n",
       "max                0.065              0.064              0.065   \n",
       "\n",
       "       val_loss_epoch_95  val_loss_epoch_96  val_loss_epoch_97  \\\n",
       "count           1629.000           1574.000           1522.000   \n",
       "mean               0.011              0.011              0.011   \n",
       "std                0.006              0.006              0.006   \n",
       "min                0.002              0.002              0.002   \n",
       "25%                0.006              0.006              0.006   \n",
       "50%                0.009              0.009              0.009   \n",
       "75%                0.014              0.014              0.014   \n",
       "max                0.064              0.064              0.064   \n",
       "\n",
       "       val_loss_epoch_98  val_loss_epoch_99  val_loss_epoch_100  \\\n",
       "count           1474.000           1421.000            1373.000   \n",
       "mean               0.011              0.011               0.010   \n",
       "std                0.006              0.006               0.006   \n",
       "min                0.002              0.002               0.002   \n",
       "25%                0.007              0.006               0.006   \n",
       "50%                0.009              0.009               0.009   \n",
       "75%                0.013              0.013               0.013   \n",
       "max                0.063              0.062               0.060   \n",
       "\n",
       "       val_loss_epoch_101  val_loss_epoch_102  val_loss_epoch_103  \\\n",
       "count            1333.000            1286.000            1253.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.006               0.006               0.006   \n",
       "min                 0.002               0.002               0.001   \n",
       "25%                 0.006               0.006               0.006   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.013               0.013               0.013   \n",
       "max                 0.057               0.055               0.052   \n",
       "\n",
       "       val_loss_epoch_104  val_loss_epoch_105  val_loss_epoch_106  \\\n",
       "count            1207.000            1169.000            1122.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.006               0.006               0.005   \n",
       "min                 0.001               0.001               0.002   \n",
       "25%                 0.006               0.006               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.013               0.013               0.013   \n",
       "max                 0.051               0.051               0.051   \n",
       "\n",
       "       val_loss_epoch_107  val_loss_epoch_108  val_loss_epoch_109  \\\n",
       "count            1081.000            1048.000            1016.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.001               0.002               0.002   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.013               0.013               0.013   \n",
       "max                 0.051               0.051               0.051   \n",
       "\n",
       "       val_loss_epoch_110  val_loss_epoch_111  val_loss_epoch_112  \\\n",
       "count             986.000             950.000             921.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.001               0.001               0.001   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.013               0.012               0.012   \n",
       "max                 0.051               0.050               0.050   \n",
       "\n",
       "       val_loss_epoch_113  val_loss_epoch_114  val_loss_epoch_115  \\\n",
       "count             887.000             858.000             823.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.001               0.001               0.001   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.050               0.050               0.050   \n",
       "\n",
       "       val_loss_epoch_116  val_loss_epoch_117  val_loss_epoch_118  \\\n",
       "count             794.000             764.000             728.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.002               0.001               0.001   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.050               0.050               0.051   \n",
       "\n",
       "       val_loss_epoch_119  val_loss_epoch_120  val_loss_epoch_121  \\\n",
       "count             705.000             677.000             666.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.001               0.001               0.001   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.050               0.050               0.050   \n",
       "\n",
       "       val_loss_epoch_122  val_loss_epoch_123  val_loss_epoch_124  \\\n",
       "count             641.000             609.000             582.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.001               0.001               0.001   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.050               0.050               0.050   \n",
       "\n",
       "       val_loss_epoch_125  val_loss_epoch_126  val_loss_epoch_127  \\\n",
       "count             549.000             519.000             499.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.001               0.001               0.001   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.050               0.049               0.050   \n",
       "\n",
       "       val_loss_epoch_128  val_loss_epoch_129  val_loss_epoch_130  \\\n",
       "count             484.000             467.000             445.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.001               0.001               0.001   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.050               0.049               0.049   \n",
       "\n",
       "       val_loss_epoch_131  val_loss_epoch_132  val_loss_epoch_133  \\\n",
       "count             432.000             412.000             393.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.004               0.004   \n",
       "min                 0.001               0.001               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.049               0.050               0.049   \n",
       "\n",
       "       val_loss_epoch_134  val_loss_epoch_135  val_loss_epoch_136  \\\n",
       "count             381.000             372.000             358.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.004               0.004               0.005   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.049               0.049               0.049   \n",
       "\n",
       "       val_loss_epoch_137  val_loss_epoch_138  val_loss_epoch_139  \\\n",
       "count             344.000             334.000             321.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.049               0.049               0.049   \n",
       "\n",
       "       val_loss_epoch_140  val_loss_epoch_141  val_loss_epoch_142  \\\n",
       "count             311.000             290.000             280.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.049               0.049               0.048   \n",
       "\n",
       "       val_loss_epoch_143  val_loss_epoch_144  val_loss_epoch_145  \\\n",
       "count             273.000             260.000             256.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.012               0.012               0.012   \n",
       "max                 0.048               0.048               0.048   \n",
       "\n",
       "       val_loss_epoch_146  val_loss_epoch_147  val_loss_epoch_148  \\\n",
       "count             241.000             235.000             227.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.003               0.002               0.002   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.048               0.048               0.048   \n",
       "\n",
       "       val_loss_epoch_149  val_loss_epoch_150  val_loss_epoch_151  \\\n",
       "count             218.000             207.000             202.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.048               0.048               0.047   \n",
       "\n",
       "       val_loss_epoch_152  val_loss_epoch_153  val_loss_epoch_154  \\\n",
       "count             192.000             185.000             176.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.004               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.047               0.047               0.046   \n",
       "\n",
       "       val_loss_epoch_155  val_loss_epoch_156  val_loss_epoch_157  \\\n",
       "count             173.000             164.000             159.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.003               0.004               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.046               0.046               0.045   \n",
       "\n",
       "       val_loss_epoch_158  val_loss_epoch_159  val_loss_epoch_160  \\\n",
       "count             150.000             147.000             143.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.004               0.003               0.004   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.045               0.045               0.044   \n",
       "\n",
       "       val_loss_epoch_161  val_loss_epoch_162  val_loss_epoch_163  \\\n",
       "count             138.000             131.000             124.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.003               0.003               0.004   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.044               0.044               0.043   \n",
       "\n",
       "       val_loss_epoch_164  val_loss_epoch_165  val_loss_epoch_166  \\\n",
       "count             119.000             115.000             112.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.004               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.043               0.042               0.042   \n",
       "\n",
       "       val_loss_epoch_167  val_loss_epoch_168  val_loss_epoch_169  \\\n",
       "count             106.000             103.000             101.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.005               0.005               0.005   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.041               0.040               0.039   \n",
       "\n",
       "       val_loss_epoch_170  val_loss_epoch_171  val_loss_epoch_172  \\\n",
       "count              94.000              91.000              90.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.004               0.004               0.004   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.009               0.008               0.008   \n",
       "75%                 0.010               0.011               0.011   \n",
       "max                 0.037               0.035               0.033   \n",
       "\n",
       "       val_loss_epoch_173  val_loss_epoch_174  val_loss_epoch_175  \\\n",
       "count              86.000              84.000              77.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.004               0.004               0.004   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.029               0.025               0.022   \n",
       "\n",
       "       val_loss_epoch_176  val_loss_epoch_177  val_loss_epoch_178  \\\n",
       "count              73.000              70.000              69.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.004               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.011               0.010               0.010   \n",
       "max                 0.019               0.019               0.019   \n",
       "\n",
       "       val_loss_epoch_179  val_loss_epoch_180  val_loss_epoch_181  \\\n",
       "count              68.000              66.000              63.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.019               0.020               0.019   \n",
       "\n",
       "       val_loss_epoch_182  val_loss_epoch_183  val_loss_epoch_184  \\\n",
       "count              62.000              55.000              55.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.004               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.011               0.011               0.010   \n",
       "max                 0.017               0.017               0.017   \n",
       "\n",
       "       val_loss_epoch_185  val_loss_epoch_186  val_loss_epoch_187  \\\n",
       "count              53.000              52.000              50.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.010               0.010               0.010   \n",
       "max                 0.016               0.016               0.016   \n",
       "\n",
       "       val_loss_epoch_188  val_loss_epoch_189  val_loss_epoch_190  \\\n",
       "count              48.000              47.000              44.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.004               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.010               0.010               0.010   \n",
       "max                 0.016               0.016               0.016   \n",
       "\n",
       "       val_loss_epoch_191  val_loss_epoch_192  val_loss_epoch_193  \\\n",
       "count              43.000              41.000              40.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.010               0.010               0.011   \n",
       "max                 0.016               0.017               0.016   \n",
       "\n",
       "       val_loss_epoch_194  val_loss_epoch_195  val_loss_epoch_196  \\\n",
       "count              39.000              38.000              36.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.016               0.016               0.016   \n",
       "\n",
       "       val_loss_epoch_197  val_loss_epoch_198  val_loss_epoch_199  \\\n",
       "count              35.000              34.000              33.000   \n",
       "mean                0.008               0.008               0.009   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.016               0.016               0.016   \n",
       "\n",
       "       val_loss_epoch_200  val_loss_epoch_201  val_loss_epoch_202  \\\n",
       "count              32.000              31.000              29.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.016               0.017               0.016   \n",
       "\n",
       "       val_loss_epoch_203  val_loss_epoch_204  val_loss_epoch_205  \\\n",
       "count              26.000              25.000              25.000   \n",
       "mean                0.009               0.008               0.008   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.006               0.006   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.012               0.011               0.011   \n",
       "max                 0.016               0.013               0.013   \n",
       "\n",
       "       val_loss_epoch_206  val_loss_epoch_207  val_loss_epoch_208  \\\n",
       "count              24.000              24.000              24.000   \n",
       "mean                0.008               0.008               0.008   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.006               0.006               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.013               0.013               0.013   \n",
       "\n",
       "       val_loss_epoch_209  val_loss_epoch_210  val_loss_epoch_211  \\\n",
       "count              23.000              21.000              21.000   \n",
       "mean                0.008               0.008               0.008   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.006               0.006               0.006   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.011               0.011               0.012   \n",
       "max                 0.013               0.013               0.013   \n",
       "\n",
       "       val_loss_epoch_212  val_loss_epoch_213  val_loss_epoch_214  \\\n",
       "count              21.000              19.000              18.000   \n",
       "mean                0.008               0.009               0.008   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.006               0.007               0.007   \n",
       "50%                 0.008               0.008               0.008   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.013               0.013               0.012   \n",
       "\n",
       "       val_loss_epoch_215  val_loss_epoch_216  val_loss_epoch_217  \\\n",
       "count              16.000              14.000              14.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.003   \n",
       "25%                 0.007               0.006               0.006   \n",
       "50%                 0.009               0.009               0.009   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.013               0.012               0.013   \n",
       "\n",
       "       val_loss_epoch_218  val_loss_epoch_219  val_loss_epoch_220  \\\n",
       "count              13.000              13.000              13.000   \n",
       "mean                0.009               0.009               0.009   \n",
       "std                 0.003               0.003               0.003   \n",
       "min                 0.003               0.003               0.004   \n",
       "25%                 0.007               0.007               0.007   \n",
       "50%                 0.010               0.010               0.010   \n",
       "75%                 0.011               0.012               0.011   \n",
       "max                 0.012               0.012               0.012   \n",
       "\n",
       "       val_loss_epoch_221  val_loss_epoch_222  val_loss_epoch_223  \\\n",
       "count              12.000              11.000              10.000   \n",
       "mean                0.009               0.010               0.009   \n",
       "std                 0.002               0.002               0.003   \n",
       "min                 0.005               0.005               0.005   \n",
       "25%                 0.007               0.008               0.007   \n",
       "50%                 0.010               0.011               0.010   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.012               0.012               0.012   \n",
       "\n",
       "       val_loss_epoch_224  val_loss_epoch_225  val_loss_epoch_226  \\\n",
       "count              10.000               9.000               8.000   \n",
       "mean                0.009               0.009               0.010   \n",
       "std                 0.002               0.003               0.003   \n",
       "min                 0.005               0.005               0.005   \n",
       "25%                 0.008               0.007               0.009   \n",
       "50%                 0.010               0.011               0.011   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.012               0.012               0.012   \n",
       "\n",
       "       val_loss_epoch_227  val_loss_epoch_228  val_loss_epoch_229  \\\n",
       "count               7.000               7.000               7.000   \n",
       "mean                0.010               0.011               0.010   \n",
       "std                 0.002               0.002               0.002   \n",
       "min                 0.006               0.006               0.006   \n",
       "25%                 0.010               0.011               0.010   \n",
       "50%                 0.011               0.011               0.011   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.012               0.012               0.012   \n",
       "\n",
       "       val_loss_epoch_230  val_loss_epoch_231  val_loss_epoch_232  \\\n",
       "count               6.000               5.000               5.000   \n",
       "mean                0.011               0.010               0.010   \n",
       "std                 0.002               0.002               0.002   \n",
       "min                 0.006               0.006               0.006   \n",
       "25%                 0.011               0.011               0.011   \n",
       "50%                 0.011               0.011               0.011   \n",
       "75%                 0.012               0.011               0.011   \n",
       "max                 0.012               0.012               0.012   \n",
       "\n",
       "       val_loss_epoch_233  val_loss_epoch_234  val_loss_epoch_235  \\\n",
       "count               5.000               5.000               5.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.002               0.002               0.002   \n",
       "min                 0.006               0.006               0.006   \n",
       "25%                 0.011               0.011               0.011   \n",
       "50%                 0.011               0.011               0.011   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.011               0.011               0.011   \n",
       "\n",
       "       val_loss_epoch_236  val_loss_epoch_237  val_loss_epoch_238  \\\n",
       "count               5.000               4.000               4.000   \n",
       "mean                0.010               0.011               0.011   \n",
       "std                 0.002               0.000               0.000   \n",
       "min                 0.007               0.011               0.011   \n",
       "25%                 0.011               0.011               0.011   \n",
       "50%                 0.011               0.011               0.011   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.011               0.011               0.011   \n",
       "\n",
       "       val_loss_epoch_239  val_loss_epoch_240  val_loss_epoch_241  \\\n",
       "count               4.000               4.000               4.000   \n",
       "mean                0.011               0.011               0.011   \n",
       "std                 0.001               0.001               0.000   \n",
       "min                 0.011               0.010               0.010   \n",
       "25%                 0.011               0.011               0.011   \n",
       "50%                 0.011               0.011               0.011   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.012               0.012               0.011   \n",
       "\n",
       "       val_loss_epoch_242  val_loss_epoch_243  val_loss_epoch_244  \\\n",
       "count               4.000               4.000               4.000   \n",
       "mean                0.011               0.011               0.011   \n",
       "std                 0.001               0.000               0.000   \n",
       "min                 0.011               0.010               0.010   \n",
       "25%                 0.011               0.011               0.010   \n",
       "50%                 0.011               0.011               0.010   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.012               0.011               0.011   \n",
       "\n",
       "       val_loss_epoch_245  val_loss_epoch_246  val_loss_epoch_247  \\\n",
       "count               4.000               4.000               4.000   \n",
       "mean                0.011               0.011               0.011   \n",
       "std                 0.001               0.000               0.000   \n",
       "min                 0.011               0.010               0.011   \n",
       "25%                 0.011               0.010               0.011   \n",
       "50%                 0.011               0.011               0.011   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.012               0.011               0.011   \n",
       "\n",
       "       val_loss_epoch_248  val_loss_epoch_249  val_loss_epoch_250  \\\n",
       "count               3.000               3.000               3.000   \n",
       "mean                0.011               0.011               0.011   \n",
       "std                 0.000               0.000               0.001   \n",
       "min                 0.011               0.010               0.010   \n",
       "25%                 0.011               0.010               0.011   \n",
       "50%                 0.011               0.011               0.011   \n",
       "75%                 0.011               0.011               0.011   \n",
       "max                 0.011               0.011               0.012   \n",
       "\n",
       "       val_loss_epoch_251  val_loss_epoch_252  val_loss_epoch_253  \\\n",
       "count               3.000               3.000               3.000   \n",
       "mean                0.010               0.011               0.010   \n",
       "std                 0.000               0.000               0.000   \n",
       "min                 0.010               0.011               0.010   \n",
       "25%                 0.010               0.011               0.010   \n",
       "50%                 0.010               0.011               0.010   \n",
       "75%                 0.011               0.011               0.010   \n",
       "max                 0.011               0.011               0.011   \n",
       "\n",
       "       val_loss_epoch_254  val_loss_epoch_255  val_loss_epoch_256  \\\n",
       "count               3.000               3.000               3.000   \n",
       "mean                0.011               0.010               0.010   \n",
       "std                 0.000               0.000               0.000   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.011               0.010               0.010   \n",
       "50%                 0.011               0.010               0.010   \n",
       "75%                 0.011               0.010               0.010   \n",
       "max                 0.011               0.011               0.010   \n",
       "\n",
       "       val_loss_epoch_257  val_loss_epoch_258  val_loss_epoch_259  \\\n",
       "count               3.000               3.000               3.000   \n",
       "mean                0.010               0.010               0.011   \n",
       "std                 0.000               0.000               0.000   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.010               0.010               0.010   \n",
       "50%                 0.010               0.010               0.011   \n",
       "75%                 0.010               0.010               0.011   \n",
       "max                 0.011               0.010               0.011   \n",
       "\n",
       "       val_loss_epoch_260  val_loss_epoch_261  val_loss_epoch_262  \\\n",
       "count               3.000               3.000               3.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.000               0.000               0.000   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.010               0.010               0.010   \n",
       "50%                 0.010               0.010               0.010   \n",
       "75%                 0.010               0.010               0.010   \n",
       "max                 0.010               0.010               0.010   \n",
       "\n",
       "       val_loss_epoch_263  val_loss_epoch_264  val_loss_epoch_265  \\\n",
       "count               3.000               3.000               3.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.000               0.001               0.000   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.010               0.010               0.010   \n",
       "50%                 0.010               0.010               0.010   \n",
       "75%                 0.010               0.011               0.010   \n",
       "max                 0.010               0.011               0.010   \n",
       "\n",
       "       val_loss_epoch_266  val_loss_epoch_267  val_loss_epoch_268  \\\n",
       "count               3.000               3.000               3.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.001               0.000               0.001   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.010               0.010               0.010   \n",
       "50%                 0.010               0.010               0.010   \n",
       "75%                 0.011               0.010               0.011   \n",
       "max                 0.011               0.010               0.011   \n",
       "\n",
       "       val_loss_epoch_269  val_loss_epoch_270  val_loss_epoch_271  \\\n",
       "count               3.000               3.000               3.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.000               0.000               0.000   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.010               0.010               0.010   \n",
       "50%                 0.010               0.010               0.010   \n",
       "75%                 0.010               0.010               0.010   \n",
       "max                 0.011               0.010               0.010   \n",
       "\n",
       "       val_loss_epoch_272  val_loss_epoch_273  val_loss_epoch_274  \\\n",
       "count               3.000               3.000               3.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                 0.000               0.000               0.000   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.010               0.010               0.010   \n",
       "50%                 0.010               0.010               0.010   \n",
       "75%                 0.010               0.010               0.010   \n",
       "max                 0.011               0.010               0.010   \n",
       "\n",
       "       val_loss_epoch_275  val_loss_epoch_276  val_loss_epoch_277  \\\n",
       "count               2.000               1.000               1.000   \n",
       "mean                0.011               0.010               0.010   \n",
       "std                 0.001                 NaN                 NaN   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.010               0.010               0.010   \n",
       "50%                 0.011               0.010               0.010   \n",
       "75%                 0.011               0.010               0.010   \n",
       "max                 0.011               0.010               0.010   \n",
       "\n",
       "       val_loss_epoch_278  val_loss_epoch_279  val_loss_epoch_280  \\\n",
       "count               1.000               1.000               1.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                   NaN                 NaN                 NaN   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.010               0.010               0.010   \n",
       "50%                 0.010               0.010               0.010   \n",
       "75%                 0.010               0.010               0.010   \n",
       "max                 0.010               0.010               0.010   \n",
       "\n",
       "       val_loss_epoch_281  val_loss_epoch_282  val_loss_epoch_283  \\\n",
       "count               1.000               1.000               1.000   \n",
       "mean                0.011               0.010               0.010   \n",
       "std                   NaN                 NaN                 NaN   \n",
       "min                 0.011               0.010               0.010   \n",
       "25%                 0.011               0.010               0.010   \n",
       "50%                 0.011               0.010               0.010   \n",
       "75%                 0.011               0.010               0.010   \n",
       "max                 0.011               0.010               0.010   \n",
       "\n",
       "       val_loss_epoch_284  val_loss_epoch_285  val_loss_epoch_286  \\\n",
       "count               1.000               1.000               1.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                   NaN                 NaN                 NaN   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.010               0.010               0.010   \n",
       "50%                 0.010               0.010               0.010   \n",
       "75%                 0.010               0.010               0.010   \n",
       "max                 0.010               0.010               0.010   \n",
       "\n",
       "       val_loss_epoch_287  val_loss_epoch_288  val_loss_epoch_289  \\\n",
       "count               1.000               1.000               1.000   \n",
       "mean                0.010               0.010               0.010   \n",
       "std                   NaN                 NaN                 NaN   \n",
       "min                 0.010               0.010               0.010   \n",
       "25%                 0.010               0.010               0.010   \n",
       "50%                 0.010               0.010               0.010   \n",
       "75%                 0.010               0.010               0.010   \n",
       "max                 0.010               0.010               0.010   \n",
       "\n",
       "       val_loss_epoch_290  val_loss_epoch_291  val_loss_epoch_292  \n",
       "count               1.000               1.000               1.000  \n",
       "mean                0.010               0.010               0.010  \n",
       "std                   NaN                 NaN                 NaN  \n",
       "min                 0.010               0.010               0.010  \n",
       "25%                 0.010               0.010               0.010  \n",
       "50%                 0.010               0.010               0.010  \n",
       "75%                 0.010               0.010               0.010  \n",
       "max                 0.010               0.010               0.010  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.879Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T17:24:28.433143Z",
     "iopub.status.busy": "2021-09-24T17:24:28.432959Z",
     "iopub.status.idle": "2021-09-24T17:24:29.212833Z",
     "shell.execute_reply": "2021-09-24T17:24:29.212260Z",
     "shell.execute_reply.started": "2021-09-24T17:24:28.433119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>r2_keras_loss_epoch_1</th>\n",
       "      <th>r2_keras_loss_epoch_2</th>\n",
       "      <th>r2_keras_loss_epoch_3</th>\n",
       "      <th>r2_keras_loss_epoch_4</th>\n",
       "      <th>r2_keras_loss_epoch_5</th>\n",
       "      <th>r2_keras_loss_epoch_6</th>\n",
       "      <th>r2_keras_loss_epoch_7</th>\n",
       "      <th>r2_keras_loss_epoch_8</th>\n",
       "      <th>r2_keras_loss_epoch_9</th>\n",
       "      <th>r2_keras_loss_epoch_10</th>\n",
       "      <th>r2_keras_loss_epoch_11</th>\n",
       "      <th>r2_keras_loss_epoch_12</th>\n",
       "      <th>r2_keras_loss_epoch_13</th>\n",
       "      <th>r2_keras_loss_epoch_14</th>\n",
       "      <th>r2_keras_loss_epoch_15</th>\n",
       "      <th>r2_keras_loss_epoch_16</th>\n",
       "      <th>r2_keras_loss_epoch_17</th>\n",
       "      <th>r2_keras_loss_epoch_18</th>\n",
       "      <th>r2_keras_loss_epoch_19</th>\n",
       "      <th>r2_keras_loss_epoch_20</th>\n",
       "      <th>r2_keras_loss_epoch_21</th>\n",
       "      <th>r2_keras_loss_epoch_22</th>\n",
       "      <th>r2_keras_loss_epoch_23</th>\n",
       "      <th>r2_keras_loss_epoch_24</th>\n",
       "      <th>r2_keras_loss_epoch_25</th>\n",
       "      <th>r2_keras_loss_epoch_26</th>\n",
       "      <th>r2_keras_loss_epoch_27</th>\n",
       "      <th>r2_keras_loss_epoch_28</th>\n",
       "      <th>r2_keras_loss_epoch_29</th>\n",
       "      <th>r2_keras_loss_epoch_30</th>\n",
       "      <th>r2_keras_loss_epoch_31</th>\n",
       "      <th>r2_keras_loss_epoch_32</th>\n",
       "      <th>r2_keras_loss_epoch_33</th>\n",
       "      <th>r2_keras_loss_epoch_34</th>\n",
       "      <th>r2_keras_loss_epoch_35</th>\n",
       "      <th>r2_keras_loss_epoch_36</th>\n",
       "      <th>r2_keras_loss_epoch_37</th>\n",
       "      <th>r2_keras_loss_epoch_38</th>\n",
       "      <th>r2_keras_loss_epoch_39</th>\n",
       "      <th>r2_keras_loss_epoch_40</th>\n",
       "      <th>r2_keras_loss_epoch_41</th>\n",
       "      <th>r2_keras_loss_epoch_42</th>\n",
       "      <th>r2_keras_loss_epoch_43</th>\n",
       "      <th>r2_keras_loss_epoch_44</th>\n",
       "      <th>r2_keras_loss_epoch_45</th>\n",
       "      <th>r2_keras_loss_epoch_46</th>\n",
       "      <th>r2_keras_loss_epoch_47</th>\n",
       "      <th>r2_keras_loss_epoch_48</th>\n",
       "      <th>r2_keras_loss_epoch_49</th>\n",
       "      <th>r2_keras_loss_epoch_50</th>\n",
       "      <th>r2_keras_loss_epoch_51</th>\n",
       "      <th>r2_keras_loss_epoch_52</th>\n",
       "      <th>r2_keras_loss_epoch_53</th>\n",
       "      <th>r2_keras_loss_epoch_54</th>\n",
       "      <th>r2_keras_loss_epoch_55</th>\n",
       "      <th>r2_keras_loss_epoch_56</th>\n",
       "      <th>r2_keras_loss_epoch_57</th>\n",
       "      <th>r2_keras_loss_epoch_58</th>\n",
       "      <th>r2_keras_loss_epoch_59</th>\n",
       "      <th>r2_keras_loss_epoch_60</th>\n",
       "      <th>r2_keras_loss_epoch_61</th>\n",
       "      <th>r2_keras_loss_epoch_62</th>\n",
       "      <th>r2_keras_loss_epoch_63</th>\n",
       "      <th>r2_keras_loss_epoch_64</th>\n",
       "      <th>r2_keras_loss_epoch_65</th>\n",
       "      <th>r2_keras_loss_epoch_66</th>\n",
       "      <th>r2_keras_loss_epoch_67</th>\n",
       "      <th>r2_keras_loss_epoch_68</th>\n",
       "      <th>r2_keras_loss_epoch_69</th>\n",
       "      <th>r2_keras_loss_epoch_70</th>\n",
       "      <th>r2_keras_loss_epoch_71</th>\n",
       "      <th>r2_keras_loss_epoch_72</th>\n",
       "      <th>r2_keras_loss_epoch_73</th>\n",
       "      <th>r2_keras_loss_epoch_74</th>\n",
       "      <th>r2_keras_loss_epoch_75</th>\n",
       "      <th>r2_keras_loss_epoch_76</th>\n",
       "      <th>r2_keras_loss_epoch_77</th>\n",
       "      <th>r2_keras_loss_epoch_78</th>\n",
       "      <th>r2_keras_loss_epoch_79</th>\n",
       "      <th>r2_keras_loss_epoch_80</th>\n",
       "      <th>r2_keras_loss_epoch_81</th>\n",
       "      <th>r2_keras_loss_epoch_82</th>\n",
       "      <th>r2_keras_loss_epoch_83</th>\n",
       "      <th>r2_keras_loss_epoch_84</th>\n",
       "      <th>r2_keras_loss_epoch_85</th>\n",
       "      <th>r2_keras_loss_epoch_86</th>\n",
       "      <th>r2_keras_loss_epoch_87</th>\n",
       "      <th>r2_keras_loss_epoch_88</th>\n",
       "      <th>r2_keras_loss_epoch_89</th>\n",
       "      <th>r2_keras_loss_epoch_90</th>\n",
       "      <th>r2_keras_loss_epoch_91</th>\n",
       "      <th>r2_keras_loss_epoch_92</th>\n",
       "      <th>r2_keras_loss_epoch_93</th>\n",
       "      <th>r2_keras_loss_epoch_94</th>\n",
       "      <th>r2_keras_loss_epoch_95</th>\n",
       "      <th>r2_keras_loss_epoch_96</th>\n",
       "      <th>r2_keras_loss_epoch_97</th>\n",
       "      <th>r2_keras_loss_epoch_98</th>\n",
       "      <th>r2_keras_loss_epoch_99</th>\n",
       "      <th>r2_keras_loss_epoch_100</th>\n",
       "      <th>r2_keras_loss_epoch_101</th>\n",
       "      <th>r2_keras_loss_epoch_102</th>\n",
       "      <th>r2_keras_loss_epoch_103</th>\n",
       "      <th>r2_keras_loss_epoch_104</th>\n",
       "      <th>r2_keras_loss_epoch_105</th>\n",
       "      <th>r2_keras_loss_epoch_106</th>\n",
       "      <th>r2_keras_loss_epoch_107</th>\n",
       "      <th>r2_keras_loss_epoch_108</th>\n",
       "      <th>r2_keras_loss_epoch_109</th>\n",
       "      <th>r2_keras_loss_epoch_110</th>\n",
       "      <th>r2_keras_loss_epoch_111</th>\n",
       "      <th>r2_keras_loss_epoch_112</th>\n",
       "      <th>r2_keras_loss_epoch_113</th>\n",
       "      <th>r2_keras_loss_epoch_114</th>\n",
       "      <th>r2_keras_loss_epoch_115</th>\n",
       "      <th>r2_keras_loss_epoch_116</th>\n",
       "      <th>r2_keras_loss_epoch_117</th>\n",
       "      <th>r2_keras_loss_epoch_118</th>\n",
       "      <th>r2_keras_loss_epoch_119</th>\n",
       "      <th>r2_keras_loss_epoch_120</th>\n",
       "      <th>r2_keras_loss_epoch_121</th>\n",
       "      <th>r2_keras_loss_epoch_122</th>\n",
       "      <th>r2_keras_loss_epoch_123</th>\n",
       "      <th>r2_keras_loss_epoch_124</th>\n",
       "      <th>r2_keras_loss_epoch_125</th>\n",
       "      <th>r2_keras_loss_epoch_126</th>\n",
       "      <th>r2_keras_loss_epoch_127</th>\n",
       "      <th>r2_keras_loss_epoch_128</th>\n",
       "      <th>r2_keras_loss_epoch_129</th>\n",
       "      <th>r2_keras_loss_epoch_130</th>\n",
       "      <th>r2_keras_loss_epoch_131</th>\n",
       "      <th>r2_keras_loss_epoch_132</th>\n",
       "      <th>r2_keras_loss_epoch_133</th>\n",
       "      <th>r2_keras_loss_epoch_134</th>\n",
       "      <th>r2_keras_loss_epoch_135</th>\n",
       "      <th>r2_keras_loss_epoch_136</th>\n",
       "      <th>r2_keras_loss_epoch_137</th>\n",
       "      <th>r2_keras_loss_epoch_138</th>\n",
       "      <th>r2_keras_loss_epoch_139</th>\n",
       "      <th>r2_keras_loss_epoch_140</th>\n",
       "      <th>r2_keras_loss_epoch_141</th>\n",
       "      <th>r2_keras_loss_epoch_142</th>\n",
       "      <th>r2_keras_loss_epoch_143</th>\n",
       "      <th>r2_keras_loss_epoch_144</th>\n",
       "      <th>r2_keras_loss_epoch_145</th>\n",
       "      <th>r2_keras_loss_epoch_146</th>\n",
       "      <th>r2_keras_loss_epoch_147</th>\n",
       "      <th>r2_keras_loss_epoch_148</th>\n",
       "      <th>r2_keras_loss_epoch_149</th>\n",
       "      <th>r2_keras_loss_epoch_150</th>\n",
       "      <th>r2_keras_loss_epoch_151</th>\n",
       "      <th>r2_keras_loss_epoch_152</th>\n",
       "      <th>r2_keras_loss_epoch_153</th>\n",
       "      <th>r2_keras_loss_epoch_154</th>\n",
       "      <th>r2_keras_loss_epoch_155</th>\n",
       "      <th>r2_keras_loss_epoch_156</th>\n",
       "      <th>r2_keras_loss_epoch_157</th>\n",
       "      <th>r2_keras_loss_epoch_158</th>\n",
       "      <th>r2_keras_loss_epoch_159</th>\n",
       "      <th>r2_keras_loss_epoch_160</th>\n",
       "      <th>r2_keras_loss_epoch_161</th>\n",
       "      <th>r2_keras_loss_epoch_162</th>\n",
       "      <th>r2_keras_loss_epoch_163</th>\n",
       "      <th>r2_keras_loss_epoch_164</th>\n",
       "      <th>r2_keras_loss_epoch_165</th>\n",
       "      <th>r2_keras_loss_epoch_166</th>\n",
       "      <th>r2_keras_loss_epoch_167</th>\n",
       "      <th>r2_keras_loss_epoch_168</th>\n",
       "      <th>r2_keras_loss_epoch_169</th>\n",
       "      <th>r2_keras_loss_epoch_170</th>\n",
       "      <th>r2_keras_loss_epoch_171</th>\n",
       "      <th>r2_keras_loss_epoch_172</th>\n",
       "      <th>r2_keras_loss_epoch_173</th>\n",
       "      <th>r2_keras_loss_epoch_174</th>\n",
       "      <th>r2_keras_loss_epoch_175</th>\n",
       "      <th>r2_keras_loss_epoch_176</th>\n",
       "      <th>r2_keras_loss_epoch_177</th>\n",
       "      <th>r2_keras_loss_epoch_178</th>\n",
       "      <th>r2_keras_loss_epoch_179</th>\n",
       "      <th>r2_keras_loss_epoch_180</th>\n",
       "      <th>r2_keras_loss_epoch_181</th>\n",
       "      <th>r2_keras_loss_epoch_182</th>\n",
       "      <th>r2_keras_loss_epoch_183</th>\n",
       "      <th>r2_keras_loss_epoch_184</th>\n",
       "      <th>r2_keras_loss_epoch_185</th>\n",
       "      <th>r2_keras_loss_epoch_186</th>\n",
       "      <th>r2_keras_loss_epoch_187</th>\n",
       "      <th>r2_keras_loss_epoch_188</th>\n",
       "      <th>r2_keras_loss_epoch_189</th>\n",
       "      <th>r2_keras_loss_epoch_190</th>\n",
       "      <th>r2_keras_loss_epoch_191</th>\n",
       "      <th>r2_keras_loss_epoch_192</th>\n",
       "      <th>r2_keras_loss_epoch_193</th>\n",
       "      <th>r2_keras_loss_epoch_194</th>\n",
       "      <th>r2_keras_loss_epoch_195</th>\n",
       "      <th>r2_keras_loss_epoch_196</th>\n",
       "      <th>r2_keras_loss_epoch_197</th>\n",
       "      <th>r2_keras_loss_epoch_198</th>\n",
       "      <th>r2_keras_loss_epoch_199</th>\n",
       "      <th>r2_keras_loss_epoch_200</th>\n",
       "      <th>r2_keras_loss_epoch_201</th>\n",
       "      <th>r2_keras_loss_epoch_202</th>\n",
       "      <th>r2_keras_loss_epoch_203</th>\n",
       "      <th>r2_keras_loss_epoch_204</th>\n",
       "      <th>r2_keras_loss_epoch_205</th>\n",
       "      <th>r2_keras_loss_epoch_206</th>\n",
       "      <th>r2_keras_loss_epoch_207</th>\n",
       "      <th>r2_keras_loss_epoch_208</th>\n",
       "      <th>r2_keras_loss_epoch_209</th>\n",
       "      <th>r2_keras_loss_epoch_210</th>\n",
       "      <th>r2_keras_loss_epoch_211</th>\n",
       "      <th>r2_keras_loss_epoch_212</th>\n",
       "      <th>r2_keras_loss_epoch_213</th>\n",
       "      <th>r2_keras_loss_epoch_214</th>\n",
       "      <th>r2_keras_loss_epoch_215</th>\n",
       "      <th>r2_keras_loss_epoch_216</th>\n",
       "      <th>r2_keras_loss_epoch_217</th>\n",
       "      <th>r2_keras_loss_epoch_218</th>\n",
       "      <th>r2_keras_loss_epoch_219</th>\n",
       "      <th>r2_keras_loss_epoch_220</th>\n",
       "      <th>r2_keras_loss_epoch_221</th>\n",
       "      <th>r2_keras_loss_epoch_222</th>\n",
       "      <th>r2_keras_loss_epoch_223</th>\n",
       "      <th>r2_keras_loss_epoch_224</th>\n",
       "      <th>r2_keras_loss_epoch_225</th>\n",
       "      <th>r2_keras_loss_epoch_226</th>\n",
       "      <th>r2_keras_loss_epoch_227</th>\n",
       "      <th>r2_keras_loss_epoch_228</th>\n",
       "      <th>r2_keras_loss_epoch_229</th>\n",
       "      <th>r2_keras_loss_epoch_230</th>\n",
       "      <th>r2_keras_loss_epoch_231</th>\n",
       "      <th>r2_keras_loss_epoch_232</th>\n",
       "      <th>r2_keras_loss_epoch_233</th>\n",
       "      <th>r2_keras_loss_epoch_234</th>\n",
       "      <th>r2_keras_loss_epoch_235</th>\n",
       "      <th>r2_keras_loss_epoch_236</th>\n",
       "      <th>r2_keras_loss_epoch_237</th>\n",
       "      <th>r2_keras_loss_epoch_238</th>\n",
       "      <th>r2_keras_loss_epoch_239</th>\n",
       "      <th>r2_keras_loss_epoch_240</th>\n",
       "      <th>r2_keras_loss_epoch_241</th>\n",
       "      <th>r2_keras_loss_epoch_242</th>\n",
       "      <th>r2_keras_loss_epoch_243</th>\n",
       "      <th>r2_keras_loss_epoch_244</th>\n",
       "      <th>r2_keras_loss_epoch_245</th>\n",
       "      <th>r2_keras_loss_epoch_246</th>\n",
       "      <th>r2_keras_loss_epoch_247</th>\n",
       "      <th>r2_keras_loss_epoch_248</th>\n",
       "      <th>r2_keras_loss_epoch_249</th>\n",
       "      <th>r2_keras_loss_epoch_250</th>\n",
       "      <th>r2_keras_loss_epoch_251</th>\n",
       "      <th>r2_keras_loss_epoch_252</th>\n",
       "      <th>r2_keras_loss_epoch_253</th>\n",
       "      <th>r2_keras_loss_epoch_254</th>\n",
       "      <th>r2_keras_loss_epoch_255</th>\n",
       "      <th>r2_keras_loss_epoch_256</th>\n",
       "      <th>r2_keras_loss_epoch_257</th>\n",
       "      <th>r2_keras_loss_epoch_258</th>\n",
       "      <th>r2_keras_loss_epoch_259</th>\n",
       "      <th>r2_keras_loss_epoch_260</th>\n",
       "      <th>r2_keras_loss_epoch_261</th>\n",
       "      <th>r2_keras_loss_epoch_262</th>\n",
       "      <th>r2_keras_loss_epoch_263</th>\n",
       "      <th>r2_keras_loss_epoch_264</th>\n",
       "      <th>r2_keras_loss_epoch_265</th>\n",
       "      <th>r2_keras_loss_epoch_266</th>\n",
       "      <th>r2_keras_loss_epoch_267</th>\n",
       "      <th>r2_keras_loss_epoch_268</th>\n",
       "      <th>r2_keras_loss_epoch_269</th>\n",
       "      <th>r2_keras_loss_epoch_270</th>\n",
       "      <th>r2_keras_loss_epoch_271</th>\n",
       "      <th>r2_keras_loss_epoch_272</th>\n",
       "      <th>r2_keras_loss_epoch_273</th>\n",
       "      <th>r2_keras_loss_epoch_274</th>\n",
       "      <th>r2_keras_loss_epoch_275</th>\n",
       "      <th>r2_keras_loss_epoch_276</th>\n",
       "      <th>r2_keras_loss_epoch_277</th>\n",
       "      <th>r2_keras_loss_epoch_278</th>\n",
       "      <th>r2_keras_loss_epoch_279</th>\n",
       "      <th>r2_keras_loss_epoch_280</th>\n",
       "      <th>r2_keras_loss_epoch_281</th>\n",
       "      <th>r2_keras_loss_epoch_282</th>\n",
       "      <th>r2_keras_loss_epoch_283</th>\n",
       "      <th>r2_keras_loss_epoch_284</th>\n",
       "      <th>r2_keras_loss_epoch_285</th>\n",
       "      <th>r2_keras_loss_epoch_286</th>\n",
       "      <th>r2_keras_loss_epoch_287</th>\n",
       "      <th>r2_keras_loss_epoch_288</th>\n",
       "      <th>r2_keras_loss_epoch_289</th>\n",
       "      <th>r2_keras_loss_epoch_290</th>\n",
       "      <th>r2_keras_loss_epoch_291</th>\n",
       "      <th>r2_keras_loss_epoch_292</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>9997.000</td>\n",
       "      <td>9996.000</td>\n",
       "      <td>9992.000</td>\n",
       "      <td>9982.000</td>\n",
       "      <td>9962.000</td>\n",
       "      <td>9933.000</td>\n",
       "      <td>9879.000</td>\n",
       "      <td>9803.000</td>\n",
       "      <td>9713.000</td>\n",
       "      <td>9623.000</td>\n",
       "      <td>9547.000</td>\n",
       "      <td>9451.000</td>\n",
       "      <td>9350.000</td>\n",
       "      <td>9253.000</td>\n",
       "      <td>9165.000</td>\n",
       "      <td>9085.000</td>\n",
       "      <td>9010.000</td>\n",
       "      <td>8945.000</td>\n",
       "      <td>8886.000</td>\n",
       "      <td>8830.000</td>\n",
       "      <td>8774.000</td>\n",
       "      <td>8701.000</td>\n",
       "      <td>8634.000</td>\n",
       "      <td>8582.000</td>\n",
       "      <td>8508.000</td>\n",
       "      <td>8436.000</td>\n",
       "      <td>8355.000</td>\n",
       "      <td>8265.000</td>\n",
       "      <td>8167.000</td>\n",
       "      <td>8053.000</td>\n",
       "      <td>7937.000</td>\n",
       "      <td>7797.000</td>\n",
       "      <td>7660.000</td>\n",
       "      <td>7517.000</td>\n",
       "      <td>7374.000</td>\n",
       "      <td>7205.000</td>\n",
       "      <td>7035.000</td>\n",
       "      <td>6871.000</td>\n",
       "      <td>6685.000</td>\n",
       "      <td>6493.000</td>\n",
       "      <td>6323.000</td>\n",
       "      <td>6142.000</td>\n",
       "      <td>5943.000</td>\n",
       "      <td>5764.000</td>\n",
       "      <td>5561.000</td>\n",
       "      <td>5382.000</td>\n",
       "      <td>5198.000</td>\n",
       "      <td>4997.000</td>\n",
       "      <td>4824.000</td>\n",
       "      <td>4667.000</td>\n",
       "      <td>4475.000</td>\n",
       "      <td>4298.000</td>\n",
       "      <td>4128.000</td>\n",
       "      <td>3963.000</td>\n",
       "      <td>3838.000</td>\n",
       "      <td>3685.000</td>\n",
       "      <td>3538.000</td>\n",
       "      <td>3398.000</td>\n",
       "      <td>3290.000</td>\n",
       "      <td>3154.000</td>\n",
       "      <td>3032.000</td>\n",
       "      <td>2917.000</td>\n",
       "      <td>2796.000</td>\n",
       "      <td>2671.000</td>\n",
       "      <td>2586.000</td>\n",
       "      <td>2508.000</td>\n",
       "      <td>2416.000</td>\n",
       "      <td>2323.000</td>\n",
       "      <td>2232.000</td>\n",
       "      <td>2137.000</td>\n",
       "      <td>2054.000</td>\n",
       "      <td>1991.000</td>\n",
       "      <td>1925.000</td>\n",
       "      <td>1861.000</td>\n",
       "      <td>1791.000</td>\n",
       "      <td>1723.000</td>\n",
       "      <td>1672.000</td>\n",
       "      <td>1629.000</td>\n",
       "      <td>1574.000</td>\n",
       "      <td>1522.000</td>\n",
       "      <td>1474.000</td>\n",
       "      <td>1421.000</td>\n",
       "      <td>1373.000</td>\n",
       "      <td>1333.000</td>\n",
       "      <td>1286.000</td>\n",
       "      <td>1253.000</td>\n",
       "      <td>1207.000</td>\n",
       "      <td>1169.000</td>\n",
       "      <td>1122.000</td>\n",
       "      <td>1081.000</td>\n",
       "      <td>1048.000</td>\n",
       "      <td>1016.000</td>\n",
       "      <td>986.000</td>\n",
       "      <td>950.000</td>\n",
       "      <td>921.000</td>\n",
       "      <td>887.000</td>\n",
       "      <td>858.000</td>\n",
       "      <td>823.000</td>\n",
       "      <td>794.000</td>\n",
       "      <td>764.000</td>\n",
       "      <td>728.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>677.000</td>\n",
       "      <td>666.000</td>\n",
       "      <td>641.000</td>\n",
       "      <td>609.000</td>\n",
       "      <td>582.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>499.000</td>\n",
       "      <td>484.000</td>\n",
       "      <td>467.000</td>\n",
       "      <td>445.000</td>\n",
       "      <td>432.000</td>\n",
       "      <td>412.000</td>\n",
       "      <td>393.000</td>\n",
       "      <td>381.000</td>\n",
       "      <td>372.000</td>\n",
       "      <td>358.000</td>\n",
       "      <td>344.000</td>\n",
       "      <td>334.000</td>\n",
       "      <td>321.000</td>\n",
       "      <td>311.000</td>\n",
       "      <td>290.000</td>\n",
       "      <td>280.000</td>\n",
       "      <td>273.000</td>\n",
       "      <td>260.000</td>\n",
       "      <td>256.000</td>\n",
       "      <td>241.000</td>\n",
       "      <td>235.000</td>\n",
       "      <td>227.000</td>\n",
       "      <td>218.000</td>\n",
       "      <td>207.000</td>\n",
       "      <td>202.000</td>\n",
       "      <td>192.000</td>\n",
       "      <td>185.000</td>\n",
       "      <td>176.000</td>\n",
       "      <td>173.000</td>\n",
       "      <td>164.000</td>\n",
       "      <td>159.000</td>\n",
       "      <td>150.000</td>\n",
       "      <td>147.000</td>\n",
       "      <td>143.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>131.000</td>\n",
       "      <td>124.000</td>\n",
       "      <td>119.000</td>\n",
       "      <td>115.000</td>\n",
       "      <td>112.000</td>\n",
       "      <td>106.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>101.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>91.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>77.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>70.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>68.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>63.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>52.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>44.000</td>\n",
       "      <td>43.000</td>\n",
       "      <td>41.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>39.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>36.000</td>\n",
       "      <td>35.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>32.000</td>\n",
       "      <td>31.000</td>\n",
       "      <td>29.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>23.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>19.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4999.500</td>\n",
       "      <td>3759.318</td>\n",
       "      <td>349.459</td>\n",
       "      <td>138.222</td>\n",
       "      <td>58.481</td>\n",
       "      <td>32.671</td>\n",
       "      <td>20.763</td>\n",
       "      <td>9.868</td>\n",
       "      <td>4.803</td>\n",
       "      <td>2.151</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>-0.577</td>\n",
       "      <td>-0.638</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>-0.685</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.857</td>\n",
       "      <td>-0.864</td>\n",
       "      <td>-0.873</td>\n",
       "      <td>-0.883</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.896</td>\n",
       "      <td>341961.022</td>\n",
       "      <td>22870.739</td>\n",
       "      <td>9930.678</td>\n",
       "      <td>4456.614</td>\n",
       "      <td>2479.748</td>\n",
       "      <td>1595.522</td>\n",
       "      <td>710.601</td>\n",
       "      <td>347.627</td>\n",
       "      <td>169.544</td>\n",
       "      <td>98.409</td>\n",
       "      <td>56.685</td>\n",
       "      <td>32.375</td>\n",
       "      <td>21.760</td>\n",
       "      <td>14.672</td>\n",
       "      <td>14.594</td>\n",
       "      <td>15.615</td>\n",
       "      <td>11.853</td>\n",
       "      <td>6.784</td>\n",
       "      <td>13.868</td>\n",
       "      <td>11.026</td>\n",
       "      <td>10.504</td>\n",
       "      <td>16.535</td>\n",
       "      <td>14.042</td>\n",
       "      <td>4.082</td>\n",
       "      <td>10.373</td>\n",
       "      <td>2.906</td>\n",
       "      <td>2.634</td>\n",
       "      <td>2.515</td>\n",
       "      <td>2.296</td>\n",
       "      <td>2.339</td>\n",
       "      <td>2.318</td>\n",
       "      <td>2.061</td>\n",
       "      <td>1.870</td>\n",
       "      <td>1.901</td>\n",
       "      <td>1.801</td>\n",
       "      <td>1.560</td>\n",
       "      <td>1.912</td>\n",
       "      <td>1.711</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2499.750</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>-0.864</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4999.500</td>\n",
       "      <td>4.945</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7499.250</td>\n",
       "      <td>11.915</td>\n",
       "      <td>1.284</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>-0.809</td>\n",
       "      <td>-0.818</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-0.865</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>-0.878</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9999.000</td>\n",
       "      <td>34183808.000</td>\n",
       "      <td>2252836.250</td>\n",
       "      <td>984050.062</td>\n",
       "      <td>443024.969</td>\n",
       "      <td>246564.703</td>\n",
       "      <td>158512.859</td>\n",
       "      <td>69902.438</td>\n",
       "      <td>33622.801</td>\n",
       "      <td>15944.676</td>\n",
       "      <td>9207.550</td>\n",
       "      <td>5335.981</td>\n",
       "      <td>3012.900</td>\n",
       "      <td>1953.132</td>\n",
       "      <td>1231.671</td>\n",
       "      <td>1304.040</td>\n",
       "      <td>1453.175</td>\n",
       "      <td>1071.406</td>\n",
       "      <td>468.119</td>\n",
       "      <td>1318.698</td>\n",
       "      <td>1022.440</td>\n",
       "      <td>990.051</td>\n",
       "      <td>1611.749</td>\n",
       "      <td>1360.012</td>\n",
       "      <td>279.210</td>\n",
       "      <td>985.846</td>\n",
       "      <td>203.641</td>\n",
       "      <td>179.501</td>\n",
       "      <td>172.040</td>\n",
       "      <td>151.731</td>\n",
       "      <td>175.916</td>\n",
       "      <td>180.404</td>\n",
       "      <td>156.143</td>\n",
       "      <td>131.722</td>\n",
       "      <td>142.975</td>\n",
       "      <td>133.548</td>\n",
       "      <td>107.552</td>\n",
       "      <td>151.978</td>\n",
       "      <td>131.759</td>\n",
       "      <td>71.696</td>\n",
       "      <td>66.473</td>\n",
       "      <td>67.581</td>\n",
       "      <td>68.487</td>\n",
       "      <td>62.249</td>\n",
       "      <td>59.285</td>\n",
       "      <td>54.952</td>\n",
       "      <td>54.338</td>\n",
       "      <td>51.883</td>\n",
       "      <td>59.731</td>\n",
       "      <td>12.143</td>\n",
       "      <td>11.372</td>\n",
       "      <td>11.382</td>\n",
       "      <td>9.935</td>\n",
       "      <td>9.997</td>\n",
       "      <td>8.975</td>\n",
       "      <td>7.925</td>\n",
       "      <td>7.109</td>\n",
       "      <td>7.227</td>\n",
       "      <td>7.588</td>\n",
       "      <td>7.472</td>\n",
       "      <td>6.402</td>\n",
       "      <td>5.828</td>\n",
       "      <td>5.916</td>\n",
       "      <td>5.084</td>\n",
       "      <td>4.790</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>-0.609</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.626</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.697</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>-0.756</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>-0.858</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.857</td>\n",
       "      <td>-0.864</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  r2_keras_loss_epoch_1  r2_keras_loss_epoch_2  \\\n",
       "count 10000.000              10000.000              10000.000   \n",
       "mean   4999.500               3759.318                349.459   \n",
       "std    2886.896             341961.022              22870.739   \n",
       "min       0.000                 -0.952                 -0.984   \n",
       "25%    2499.750                  0.510                 -0.503   \n",
       "50%    4999.500                  4.945                  0.393   \n",
       "75%    7499.250                 11.915                  1.284   \n",
       "max    9999.000           34183808.000            2252836.250   \n",
       "\n",
       "       r2_keras_loss_epoch_3  r2_keras_loss_epoch_4  r2_keras_loss_epoch_5  \\\n",
       "count              10000.000              10000.000              10000.000   \n",
       "mean                 138.222                 58.481                 32.671   \n",
       "std                 9930.678               4456.614               2479.748   \n",
       "min                   -0.991                 -0.995                 -0.998   \n",
       "25%                   -0.716                 -0.820                 -0.864   \n",
       "50%                   -0.276                 -0.506                 -0.671   \n",
       "75%                    0.125                 -0.244                 -0.465   \n",
       "max               984050.062             443024.969             246564.703   \n",
       "\n",
       "       r2_keras_loss_epoch_6  r2_keras_loss_epoch_7  r2_keras_loss_epoch_8  \\\n",
       "count              10000.000              10000.000              10000.000   \n",
       "mean                  20.763                  9.868                  4.803   \n",
       "std                 1595.522                710.601                347.627   \n",
       "min                   -0.999                 -1.000                 -1.000   \n",
       "25%                   -0.893                 -0.919                 -0.936   \n",
       "50%                   -0.765                 -0.825                 -0.856   \n",
       "75%                   -0.605                 -0.690                 -0.743   \n",
       "max               158512.859              69902.438              33622.801   \n",
       "\n",
       "       r2_keras_loss_epoch_9  r2_keras_loss_epoch_10  r2_keras_loss_epoch_11  \\\n",
       "count              10000.000               10000.000               10000.000   \n",
       "mean                   2.151                   0.904                   0.166   \n",
       "std                  169.544                  98.409                  56.685   \n",
       "min                   -1.000                  -1.000                  -1.000   \n",
       "25%                   -0.946                  -0.954                  -0.959   \n",
       "50%                   -0.877                  -0.892                  -0.902   \n",
       "75%                   -0.774                  -0.795                  -0.809   \n",
       "max                15944.676                9207.550                5335.981   \n",
       "\n",
       "       r2_keras_loss_epoch_12  r2_keras_loss_epoch_13  r2_keras_loss_epoch_14  \\\n",
       "count               10000.000               10000.000               10000.000   \n",
       "mean                   -0.228                  -0.404                  -0.527   \n",
       "std                    32.375                  21.760                  14.672   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.964                  -0.968                  -0.971   \n",
       "50%                    -0.911                  -0.918                  -0.924   \n",
       "75%                    -0.818                  -0.826                  -0.832   \n",
       "max                  3012.900                1953.132                1231.671   \n",
       "\n",
       "       r2_keras_loss_epoch_15  r2_keras_loss_epoch_16  r2_keras_loss_epoch_17  \\\n",
       "count               10000.000               10000.000               10000.000   \n",
       "mean                   -0.563                  -0.577                  -0.638   \n",
       "std                    14.594                  15.615                  11.853   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.974                  -0.976                  -0.979   \n",
       "50%                    -0.928                  -0.932                  -0.936   \n",
       "75%                    -0.838                  -0.843                  -0.849   \n",
       "max                  1304.040                1453.175                1071.406   \n",
       "\n",
       "       r2_keras_loss_epoch_18  r2_keras_loss_epoch_19  r2_keras_loss_epoch_20  \\\n",
       "count                9997.000                9996.000                9992.000   \n",
       "mean                   -0.711                  -0.645                  -0.686   \n",
       "std                     6.784                  13.868                  11.026   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.981                  -0.983                  -0.985   \n",
       "50%                    -0.939                  -0.943                  -0.947   \n",
       "75%                    -0.853                  -0.859                  -0.865   \n",
       "max                   468.119                1318.698                1022.440   \n",
       "\n",
       "       r2_keras_loss_epoch_21  r2_keras_loss_epoch_22  r2_keras_loss_epoch_23  \\\n",
       "count                9982.000                9962.000                9933.000   \n",
       "mean                   -0.707                  -0.650                  -0.685   \n",
       "std                    10.504                  16.535                  14.042   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.987                  -0.988                  -0.990   \n",
       "50%                    -0.950                  -0.954                  -0.957   \n",
       "75%                    -0.872                  -0.878                  -0.885   \n",
       "max                   990.051                1611.749                1360.012   \n",
       "\n",
       "       r2_keras_loss_epoch_24  r2_keras_loss_epoch_25  r2_keras_loss_epoch_26  \\\n",
       "count                9879.000                9803.000                9713.000   \n",
       "mean                   -0.805                  -0.741                  -0.846   \n",
       "std                     4.082                  10.373                   2.906   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.991                  -0.992                  -0.993   \n",
       "50%                    -0.961                  -0.964                  -0.967   \n",
       "75%                    -0.891                  -0.898                  -0.905   \n",
       "max                   279.210                 985.846                 203.641   \n",
       "\n",
       "       r2_keras_loss_epoch_27  r2_keras_loss_epoch_28  r2_keras_loss_epoch_29  \\\n",
       "count                9623.000                9547.000                9451.000   \n",
       "mean                   -0.857                  -0.864                  -0.873   \n",
       "std                     2.634                   2.515                   2.296   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.993                  -0.994                  -0.995   \n",
       "50%                    -0.970                  -0.973                  -0.976   \n",
       "75%                    -0.913                  -0.919                  -0.926   \n",
       "max                   179.501                 172.040                 151.731   \n",
       "\n",
       "       r2_keras_loss_epoch_30  r2_keras_loss_epoch_31  r2_keras_loss_epoch_32  \\\n",
       "count                9350.000                9253.000                9165.000   \n",
       "mean                   -0.883                  -0.889                  -0.897   \n",
       "std                     2.339                   2.318                   2.061   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.995                  -0.995                  -0.996   \n",
       "50%                    -0.978                  -0.980                  -0.982   \n",
       "75%                    -0.931                  -0.936                  -0.941   \n",
       "max                   175.916                 180.404                 156.143   \n",
       "\n",
       "       r2_keras_loss_epoch_33  r2_keras_loss_epoch_34  r2_keras_loss_epoch_35  \\\n",
       "count                9085.000                9010.000                8945.000   \n",
       "mean                   -0.903                  -0.907                  -0.912   \n",
       "std                     1.870                   1.901                   1.801   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.996                  -0.996                  -0.996   \n",
       "50%                    -0.983                  -0.985                  -0.986   \n",
       "75%                    -0.946                  -0.951                  -0.954   \n",
       "max                   131.722                 142.975                 133.548   \n",
       "\n",
       "       r2_keras_loss_epoch_36  r2_keras_loss_epoch_37  r2_keras_loss_epoch_38  \\\n",
       "count                8886.000                8830.000                8774.000   \n",
       "mean                   -0.918                  -0.917                  -0.923   \n",
       "std                     1.560                   1.912                   1.711   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.997                  -0.997                  -0.997   \n",
       "50%                    -0.987                  -0.988                  -0.988   \n",
       "75%                    -0.957                  -0.960                  -0.963   \n",
       "max                   107.552                 151.978                 131.759   \n",
       "\n",
       "       r2_keras_loss_epoch_39  r2_keras_loss_epoch_40  r2_keras_loss_epoch_41  \\\n",
       "count                8701.000                8634.000                8582.000   \n",
       "mean                   -0.942                  -0.944                  -0.947   \n",
       "std                     0.955                   0.899                   0.888   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.997                  -0.997                  -0.997   \n",
       "50%                    -0.989                  -0.990                  -0.990   \n",
       "75%                    -0.965                  -0.968                  -0.970   \n",
       "max                    71.696                  66.473                  67.581   \n",
       "\n",
       "       r2_keras_loss_epoch_42  r2_keras_loss_epoch_43  r2_keras_loss_epoch_44  \\\n",
       "count                8508.000                8436.000                8355.000   \n",
       "mean                   -0.953                  -0.955                  -0.956   \n",
       "std                     0.815                   0.751                   0.717   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.997                  -0.997                  -0.997   \n",
       "50%                    -0.990                  -0.991                  -0.991   \n",
       "75%                    -0.972                  -0.974                  -0.975   \n",
       "max                    68.487                  62.249                  59.285   \n",
       "\n",
       "       r2_keras_loss_epoch_45  r2_keras_loss_epoch_46  r2_keras_loss_epoch_47  \\\n",
       "count                8265.000                8167.000                8053.000   \n",
       "mean                   -0.959                  -0.960                  -0.961   \n",
       "std                     0.665                   0.660                   0.630   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.997                  -0.997                  -0.997   \n",
       "50%                    -0.991                  -0.991                  -0.992   \n",
       "75%                    -0.976                  -0.977                  -0.978   \n",
       "max                    54.952                  54.338                  51.883   \n",
       "\n",
       "       r2_keras_loss_epoch_48  r2_keras_loss_epoch_49  r2_keras_loss_epoch_50  \\\n",
       "count                7937.000                7797.000                7660.000   \n",
       "mean                   -0.961                  -0.970                  -0.971   \n",
       "std                     0.716                   0.198                   0.184   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.997                  -0.997                  -0.997   \n",
       "50%                    -0.992                  -0.992                  -0.992   \n",
       "75%                    -0.979                  -0.979                  -0.980   \n",
       "max                    59.731                  12.143                  11.372   \n",
       "\n",
       "       r2_keras_loss_epoch_51  r2_keras_loss_epoch_52  r2_keras_loss_epoch_53  \\\n",
       "count                7517.000                7374.000                7205.000   \n",
       "mean                   -0.971                  -0.972                  -0.972   \n",
       "std                     0.182                   0.169                   0.173   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.997                  -0.997                  -0.997   \n",
       "50%                    -0.992                  -0.992                  -0.992   \n",
       "75%                    -0.980                  -0.980                  -0.981   \n",
       "max                    11.382                   9.935                   9.997   \n",
       "\n",
       "       r2_keras_loss_epoch_54  r2_keras_loss_epoch_55  r2_keras_loss_epoch_56  \\\n",
       "count                7035.000                6871.000                6685.000   \n",
       "mean                   -0.973                  -0.973                  -0.974   \n",
       "std                     0.158                   0.147                   0.136   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.997                  -0.997                  -0.998   \n",
       "50%                    -0.992                  -0.992                  -0.992   \n",
       "75%                    -0.981                  -0.981                  -0.981   \n",
       "max                     8.975                   7.925                   7.109   \n",
       "\n",
       "       r2_keras_loss_epoch_57  r2_keras_loss_epoch_58  r2_keras_loss_epoch_59  \\\n",
       "count                6493.000                6323.000                6142.000   \n",
       "mean                   -0.974                  -0.974                  -0.974   \n",
       "std                     0.137                   0.143                   0.140   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.992                  -0.992                  -0.993   \n",
       "75%                    -0.981                  -0.981                  -0.981   \n",
       "max                     7.227                   7.588                   7.472   \n",
       "\n",
       "       r2_keras_loss_epoch_60  r2_keras_loss_epoch_61  r2_keras_loss_epoch_62  \\\n",
       "count                5943.000                5764.000                5561.000   \n",
       "mean                   -0.975                  -0.976                  -0.976   \n",
       "std                     0.126                   0.103                   0.104   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.993                  -0.993                  -0.993   \n",
       "75%                    -0.981                  -0.981                  -0.981   \n",
       "max                     6.402                   5.828                   5.916   \n",
       "\n",
       "       r2_keras_loss_epoch_63  r2_keras_loss_epoch_64  r2_keras_loss_epoch_65  \\\n",
       "count                5382.000                5198.000                4997.000   \n",
       "mean                   -0.977                  -0.977                  -0.978   \n",
       "std                     0.096                   0.093                   0.047   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.993                  -0.993                  -0.993   \n",
       "75%                    -0.981                  -0.981                  -0.982   \n",
       "max                     5.084                   4.790                  -0.257   \n",
       "\n",
       "       r2_keras_loss_epoch_66  r2_keras_loss_epoch_67  r2_keras_loss_epoch_68  \\\n",
       "count                4824.000                4667.000                4475.000   \n",
       "mean                   -0.978                  -0.979                  -0.979   \n",
       "std                     0.047                   0.047                   0.046   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.993                  -0.993                  -0.993   \n",
       "75%                    -0.982                  -0.982                  -0.982   \n",
       "max                    -0.191                  -0.287                  -0.348   \n",
       "\n",
       "       r2_keras_loss_epoch_69  r2_keras_loss_epoch_70  r2_keras_loss_epoch_71  \\\n",
       "count                4298.000                4128.000                3963.000   \n",
       "mean                   -0.979                  -0.979                  -0.979   \n",
       "std                     0.046                   0.046                   0.045   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.994                  -0.994                  -0.994   \n",
       "75%                    -0.982                  -0.982                  -0.983   \n",
       "max                    -0.336                  -0.367                  -0.394   \n",
       "\n",
       "       r2_keras_loss_epoch_72  r2_keras_loss_epoch_73  r2_keras_loss_epoch_74  \\\n",
       "count                3838.000                3685.000                3538.000   \n",
       "mean                   -0.980                  -0.980                  -0.980   \n",
       "std                     0.045                   0.044                   0.043   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.994                  -0.994                  -0.994   \n",
       "75%                    -0.983                  -0.983                  -0.984   \n",
       "max                    -0.400                  -0.398                  -0.500   \n",
       "\n",
       "       r2_keras_loss_epoch_75  r2_keras_loss_epoch_76  r2_keras_loss_epoch_77  \\\n",
       "count                3398.000                3290.000                3154.000   \n",
       "mean                   -0.981                  -0.981                  -0.981   \n",
       "std                     0.043                   0.043                   0.043   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.994                  -0.995                  -0.995   \n",
       "75%                    -0.984                  -0.984                  -0.984   \n",
       "max                    -0.451                  -0.473                  -0.351   \n",
       "\n",
       "       r2_keras_loss_epoch_78  r2_keras_loss_epoch_79  r2_keras_loss_epoch_80  \\\n",
       "count                3032.000                2917.000                2796.000   \n",
       "mean                   -0.982                  -0.982                  -0.982   \n",
       "std                     0.042                   0.040                   0.040   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.995                  -0.995                  -0.995   \n",
       "75%                    -0.985                  -0.985                  -0.986   \n",
       "max                    -0.378                  -0.595                  -0.592   \n",
       "\n",
       "       r2_keras_loss_epoch_81  r2_keras_loss_epoch_82  r2_keras_loss_epoch_83  \\\n",
       "count                2671.000                2586.000                2508.000   \n",
       "mean                   -0.983                  -0.983                  -0.984   \n",
       "std                     0.039                   0.038                   0.038   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.995                  -0.995                  -0.995   \n",
       "75%                    -0.986                  -0.986                  -0.987   \n",
       "max                    -0.598                  -0.600                  -0.601   \n",
       "\n",
       "       r2_keras_loss_epoch_84  r2_keras_loss_epoch_85  r2_keras_loss_epoch_86  \\\n",
       "count                2416.000                2323.000                2232.000   \n",
       "mean                   -0.984                  -0.984                  -0.985   \n",
       "std                     0.037                   0.036                   0.036   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.995                  -0.995                  -0.995   \n",
       "75%                    -0.987                  -0.987                  -0.987   \n",
       "max                    -0.599                  -0.606                  -0.604   \n",
       "\n",
       "       r2_keras_loss_epoch_87  r2_keras_loss_epoch_88  r2_keras_loss_epoch_89  \\\n",
       "count                2137.000                2054.000                1991.000   \n",
       "mean                   -0.985                  -0.986                  -0.986   \n",
       "std                     0.035                   0.034                   0.033   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.995                  -0.996                  -0.996   \n",
       "75%                    -0.988                  -0.988                  -0.988   \n",
       "max                    -0.608                  -0.609                  -0.608   \n",
       "\n",
       "       r2_keras_loss_epoch_90  r2_keras_loss_epoch_91  r2_keras_loss_epoch_92  \\\n",
       "count                1925.000                1861.000                1791.000   \n",
       "mean                   -0.987                  -0.987                  -0.987   \n",
       "std                     0.032                   0.031                   0.030   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.996                  -0.996                  -0.996   \n",
       "75%                    -0.988                  -0.989                  -0.989   \n",
       "max                    -0.611                  -0.614                  -0.621   \n",
       "\n",
       "       r2_keras_loss_epoch_93  r2_keras_loss_epoch_94  r2_keras_loss_epoch_95  \\\n",
       "count                1723.000                1672.000                1629.000   \n",
       "mean                   -0.988                  -0.988                  -0.988   \n",
       "std                     0.030                   0.029                   0.028   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.996                  -0.996                  -0.996   \n",
       "75%                    -0.989                  -0.990                  -0.990   \n",
       "max                    -0.612                  -0.611                  -0.613   \n",
       "\n",
       "       r2_keras_loss_epoch_96  r2_keras_loss_epoch_97  r2_keras_loss_epoch_98  \\\n",
       "count                1574.000                1522.000                1474.000   \n",
       "mean                   -0.988                  -0.989                  -0.989   \n",
       "std                     0.028                   0.027                   0.027   \n",
       "min                    -1.000                  -1.000                  -1.000   \n",
       "25%                    -0.998                  -0.998                  -0.998   \n",
       "50%                    -0.996                  -0.996                  -0.996   \n",
       "75%                    -0.990                  -0.990                  -0.991   \n",
       "max                    -0.620                  -0.626                  -0.618   \n",
       "\n",
       "       r2_keras_loss_epoch_99  r2_keras_loss_epoch_100  \\\n",
       "count                1421.000                 1373.000   \n",
       "mean                   -0.989                   -0.990   \n",
       "std                     0.026                    0.025   \n",
       "min                    -1.000                   -1.000   \n",
       "25%                    -0.998                   -0.998   \n",
       "50%                    -0.996                   -0.996   \n",
       "75%                    -0.991                   -0.991   \n",
       "max                    -0.621                   -0.619   \n",
       "\n",
       "       r2_keras_loss_epoch_101  r2_keras_loss_epoch_102  \\\n",
       "count                 1333.000                 1286.000   \n",
       "mean                    -0.990                   -0.990   \n",
       "std                      0.025                    0.024   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.996                   -0.996   \n",
       "75%                     -0.991                   -0.991   \n",
       "max                     -0.628                   -0.625   \n",
       "\n",
       "       r2_keras_loss_epoch_103  r2_keras_loss_epoch_104  \\\n",
       "count                 1253.000                 1207.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.022                    0.022   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.999   \n",
       "50%                     -0.996                   -0.997   \n",
       "75%                     -0.992                   -0.992   \n",
       "max                     -0.622                   -0.625   \n",
       "\n",
       "       r2_keras_loss_epoch_105  r2_keras_loss_epoch_106  \\\n",
       "count                 1169.000                 1122.000   \n",
       "mean                    -0.991                   -0.991   \n",
       "std                      0.021                    0.020   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.992                   -0.993   \n",
       "max                     -0.627                   -0.628   \n",
       "\n",
       "       r2_keras_loss_epoch_107  r2_keras_loss_epoch_108  \\\n",
       "count                 1081.000                 1048.000   \n",
       "mean                    -0.992                   -0.992   \n",
       "std                      0.019                    0.019   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.993                   -0.993   \n",
       "max                     -0.628                   -0.637   \n",
       "\n",
       "       r2_keras_loss_epoch_109  r2_keras_loss_epoch_110  \\\n",
       "count                 1016.000                  986.000   \n",
       "mean                    -0.992                   -0.992   \n",
       "std                      0.019                    0.019   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.993                   -0.993   \n",
       "max                     -0.635                   -0.652   \n",
       "\n",
       "       r2_keras_loss_epoch_111  r2_keras_loss_epoch_112  \\\n",
       "count                  950.000                  921.000   \n",
       "mean                    -0.992                   -0.992   \n",
       "std                      0.019                    0.017   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.993                   -0.993   \n",
       "max                     -0.673                   -0.697   \n",
       "\n",
       "       r2_keras_loss_epoch_113  r2_keras_loss_epoch_114  \\\n",
       "count                  887.000                  858.000   \n",
       "mean                    -0.992                   -0.992   \n",
       "std                      0.017                    0.016   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.994                   -0.993   \n",
       "max                     -0.725                   -0.756   \n",
       "\n",
       "       r2_keras_loss_epoch_115  r2_keras_loss_epoch_116  \\\n",
       "count                  823.000                  794.000   \n",
       "mean                    -0.993                   -0.993   \n",
       "std                      0.015                    0.015   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.994                   -0.993   \n",
       "max                     -0.791                   -0.825   \n",
       "\n",
       "       r2_keras_loss_epoch_117  r2_keras_loss_epoch_118  \\\n",
       "count                  764.000                  728.000   \n",
       "mean                    -0.993                   -0.993   \n",
       "std                      0.014                    0.014   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.994                   -0.994   \n",
       "max                     -0.826                   -0.829   \n",
       "\n",
       "       r2_keras_loss_epoch_119  r2_keras_loss_epoch_120  \\\n",
       "count                  705.000                  677.000   \n",
       "mean                    -0.993                   -0.993   \n",
       "std                      0.014                    0.012   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.994                   -0.994   \n",
       "max                     -0.837                   -0.846   \n",
       "\n",
       "       r2_keras_loss_epoch_121  r2_keras_loss_epoch_122  \\\n",
       "count                  666.000                  641.000   \n",
       "mean                    -0.993                   -0.993   \n",
       "std                      0.012                    0.012   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.994                   -0.994   \n",
       "max                     -0.844                   -0.846   \n",
       "\n",
       "       r2_keras_loss_epoch_123  r2_keras_loss_epoch_124  \\\n",
       "count                  609.000                  582.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.012                    0.012   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.994                   -0.994   \n",
       "max                     -0.846                   -0.846   \n",
       "\n",
       "       r2_keras_loss_epoch_125  r2_keras_loss_epoch_126  \\\n",
       "count                  549.000                  519.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.011                    0.011   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.994                   -0.994   \n",
       "max                     -0.851                   -0.843   \n",
       "\n",
       "       r2_keras_loss_epoch_127  r2_keras_loss_epoch_128  \\\n",
       "count                  499.000                  484.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.011                    0.011   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.994                   -0.994   \n",
       "max                     -0.848                   -0.844   \n",
       "\n",
       "       r2_keras_loss_epoch_129  r2_keras_loss_epoch_130  \\\n",
       "count                  467.000                  445.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.011                    0.011   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.849                   -0.855   \n",
       "\n",
       "       r2_keras_loss_epoch_131  r2_keras_loss_epoch_132  \\\n",
       "count                  432.000                  412.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.011                    0.011   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.849                   -0.854   \n",
       "\n",
       "       r2_keras_loss_epoch_133  r2_keras_loss_epoch_134  \\\n",
       "count                  393.000                  381.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.011                    0.011   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.854                   -0.856   \n",
       "\n",
       "       r2_keras_loss_epoch_135  r2_keras_loss_epoch_136  \\\n",
       "count                  372.000                  358.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.011                    0.011   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.854                   -0.851   \n",
       "\n",
       "       r2_keras_loss_epoch_137  r2_keras_loss_epoch_138  \\\n",
       "count                  344.000                  334.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.011                    0.011   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.851                   -0.861   \n",
       "\n",
       "       r2_keras_loss_epoch_139  r2_keras_loss_epoch_140  \\\n",
       "count                  321.000                  311.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.011                    0.011   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.854                   -0.856   \n",
       "\n",
       "       r2_keras_loss_epoch_141  r2_keras_loss_epoch_142  \\\n",
       "count                  290.000                  280.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.012                    0.012   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.849                   -0.858   \n",
       "\n",
       "       r2_keras_loss_epoch_143  r2_keras_loss_epoch_144  \\\n",
       "count                  273.000                  260.000   \n",
       "mean                    -0.994                   -0.994   \n",
       "std                      0.012                    0.012   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.856                   -0.857   \n",
       "\n",
       "       r2_keras_loss_epoch_145  r2_keras_loss_epoch_146  \\\n",
       "count                  256.000                  241.000   \n",
       "mean                    -0.994                   -0.995   \n",
       "std                      0.012                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.864                   -0.928   \n",
       "\n",
       "       r2_keras_loss_epoch_147  r2_keras_loss_epoch_148  \\\n",
       "count                  235.000                  227.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.009                    0.009   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.929                   -0.929   \n",
       "\n",
       "       r2_keras_loss_epoch_149  r2_keras_loss_epoch_150  \\\n",
       "count                  218.000                  207.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.995   \n",
       "max                     -0.930                   -0.932   \n",
       "\n",
       "       r2_keras_loss_epoch_151  r2_keras_loss_epoch_152  \\\n",
       "count                  202.000                  192.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.995   \n",
       "max                     -0.934                   -0.934   \n",
       "\n",
       "       r2_keras_loss_epoch_153  r2_keras_loss_epoch_154  \\\n",
       "count                  185.000                  176.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.996   \n",
       "max                     -0.937                   -0.939   \n",
       "\n",
       "       r2_keras_loss_epoch_155  r2_keras_loss_epoch_156  \\\n",
       "count                  173.000                  164.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.940                   -0.941   \n",
       "\n",
       "       r2_keras_loss_epoch_157  r2_keras_loss_epoch_158  \\\n",
       "count                  159.000                  150.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.942                   -0.943   \n",
       "\n",
       "       r2_keras_loss_epoch_159  r2_keras_loss_epoch_160  \\\n",
       "count                  147.000                  143.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.996   \n",
       "max                     -0.946                   -0.946   \n",
       "\n",
       "       r2_keras_loss_epoch_161  r2_keras_loss_epoch_162  \\\n",
       "count                  138.000                  131.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.998   \n",
       "75%                     -0.995                   -0.996   \n",
       "max                     -0.948                   -0.949   \n",
       "\n",
       "       r2_keras_loss_epoch_163  r2_keras_loss_epoch_164  \\\n",
       "count                  124.000                  119.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.998                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.949                   -0.951   \n",
       "\n",
       "       r2_keras_loss_epoch_165  r2_keras_loss_epoch_166  \\\n",
       "count                  115.000                  112.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.953                   -0.953   \n",
       "\n",
       "       r2_keras_loss_epoch_167  r2_keras_loss_epoch_168  \\\n",
       "count                  106.000                  103.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.998                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.955                   -0.955   \n",
       "\n",
       "       r2_keras_loss_epoch_169  r2_keras_loss_epoch_170  \\\n",
       "count                  101.000                   94.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.957                   -0.957   \n",
       "\n",
       "       r2_keras_loss_epoch_171  r2_keras_loss_epoch_172  \\\n",
       "count                   91.000                   90.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.998                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.955                   -0.955   \n",
       "\n",
       "       r2_keras_loss_epoch_173  r2_keras_loss_epoch_174  \\\n",
       "count                   86.000                   84.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.998   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.956                   -0.956   \n",
       "\n",
       "       r2_keras_loss_epoch_175  r2_keras_loss_epoch_176  \\\n",
       "count                   77.000                   73.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.956                   -0.956   \n",
       "\n",
       "       r2_keras_loss_epoch_177  r2_keras_loss_epoch_178  \\\n",
       "count                   70.000                   69.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.957                   -0.955   \n",
       "\n",
       "       r2_keras_loss_epoch_179  r2_keras_loss_epoch_180  \\\n",
       "count                   68.000                   66.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.008                    0.008   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.995                   -0.996   \n",
       "max                     -0.956                   -0.957   \n",
       "\n",
       "       r2_keras_loss_epoch_181  r2_keras_loss_epoch_182  \\\n",
       "count                   63.000                   62.000   \n",
       "mean                    -0.995                   -0.995   \n",
       "std                      0.009                    0.009   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.957                   -0.956   \n",
       "\n",
       "       r2_keras_loss_epoch_183  r2_keras_loss_epoch_184  \\\n",
       "count                   55.000                   55.000   \n",
       "mean                    -0.995                   -0.996   \n",
       "std                      0.008                    0.007   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.957                   -0.957   \n",
       "\n",
       "       r2_keras_loss_epoch_185  r2_keras_loss_epoch_186  \\\n",
       "count                   53.000                   52.000   \n",
       "mean                    -0.996                   -0.996   \n",
       "std                      0.006                    0.006   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.961                   -0.961   \n",
       "\n",
       "       r2_keras_loss_epoch_187  r2_keras_loss_epoch_188  \\\n",
       "count                   50.000                   48.000   \n",
       "mean                    -0.996                   -0.997   \n",
       "std                      0.006                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.998   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.961                   -0.990   \n",
       "\n",
       "       r2_keras_loss_epoch_189  r2_keras_loss_epoch_190  \\\n",
       "count                   47.000                   44.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.998                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.989                   -0.990   \n",
       "\n",
       "       r2_keras_loss_epoch_191  r2_keras_loss_epoch_192  \\\n",
       "count                   43.000                   41.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.989                   -0.989   \n",
       "\n",
       "       r2_keras_loss_epoch_193  r2_keras_loss_epoch_194  \\\n",
       "count                   40.000                   39.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.998   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.990                   -0.990   \n",
       "\n",
       "       r2_keras_loss_epoch_195  r2_keras_loss_epoch_196  \\\n",
       "count                   38.000                   36.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.998   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.990                   -0.990   \n",
       "\n",
       "       r2_keras_loss_epoch_197  r2_keras_loss_epoch_198  \\\n",
       "count                   35.000                   34.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.998   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.990                   -0.990   \n",
       "\n",
       "       r2_keras_loss_epoch_199  r2_keras_loss_epoch_200  \\\n",
       "count                   33.000                   32.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.998   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.990                   -0.990   \n",
       "\n",
       "       r2_keras_loss_epoch_201  r2_keras_loss_epoch_202  \\\n",
       "count                   31.000                   29.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.990   \n",
       "\n",
       "       r2_keras_loss_epoch_203  r2_keras_loss_epoch_204  \\\n",
       "count                   26.000                   25.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.991   \n",
       "\n",
       "       r2_keras_loss_epoch_205  r2_keras_loss_epoch_206  \\\n",
       "count                   25.000                   24.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.990   \n",
       "\n",
       "       r2_keras_loss_epoch_207  r2_keras_loss_epoch_208  \\\n",
       "count                   24.000                   24.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.002   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.991   \n",
       "\n",
       "       r2_keras_loss_epoch_209  r2_keras_loss_epoch_210  \\\n",
       "count                   23.000                   21.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.998                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.991   \n",
       "\n",
       "       r2_keras_loss_epoch_211  r2_keras_loss_epoch_212  \\\n",
       "count                   21.000                   21.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.003   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.991   \n",
       "\n",
       "       r2_keras_loss_epoch_213  r2_keras_loss_epoch_214  \\\n",
       "count                   19.000                   18.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.003                    0.002   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.999                   -0.999   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.991   \n",
       "\n",
       "       r2_keras_loss_epoch_215  r2_keras_loss_epoch_216  \\\n",
       "count                   16.000                   14.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.002                    0.002   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.991   \n",
       "\n",
       "       r2_keras_loss_epoch_217  r2_keras_loss_epoch_218  \\\n",
       "count                   14.000                   13.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.002                    0.002   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.992   \n",
       "\n",
       "       r2_keras_loss_epoch_219  r2_keras_loss_epoch_220  \\\n",
       "count                   13.000                   13.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.002                    0.002   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.991   \n",
       "\n",
       "       r2_keras_loss_epoch_221  r2_keras_loss_epoch_222  \\\n",
       "count                   12.000                   11.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.002                    0.002   \n",
       "min                     -1.000                   -0.999   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.992                   -0.991   \n",
       "\n",
       "       r2_keras_loss_epoch_223  r2_keras_loss_epoch_224  \\\n",
       "count                   10.000                   10.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.002                    0.002   \n",
       "min                     -1.000                   -1.000   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.996                   -0.996   \n",
       "max                     -0.991                   -0.992   \n",
       "\n",
       "       r2_keras_loss_epoch_225  r2_keras_loss_epoch_226  \\\n",
       "count                    9.000                    8.000   \n",
       "mean                    -0.997                   -0.998   \n",
       "std                      0.002                    0.001   \n",
       "min                     -1.000                   -0.999   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.998   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.992                   -0.996   \n",
       "\n",
       "       r2_keras_loss_epoch_227  r2_keras_loss_epoch_228  \\\n",
       "count                    7.000                    7.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.001                    0.001   \n",
       "min                     -0.999                   -0.999   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.996                   -0.996   \n",
       "\n",
       "       r2_keras_loss_epoch_229  r2_keras_loss_epoch_230  \\\n",
       "count                    7.000                    6.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.001                    0.001   \n",
       "min                     -0.999                   -0.999   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.998   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.996                   -0.997   \n",
       "\n",
       "       r2_keras_loss_epoch_231  r2_keras_loss_epoch_232  \\\n",
       "count                    5.000                    5.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.001                    0.001   \n",
       "min                     -0.999                   -0.999   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.997                   -0.997   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.997                   -0.997   \n",
       "\n",
       "       r2_keras_loss_epoch_233  r2_keras_loss_epoch_234  \\\n",
       "count                    5.000                    5.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.001                    0.001   \n",
       "min                     -0.999                   -0.999   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.997                   -0.997   \n",
       "\n",
       "       r2_keras_loss_epoch_235  r2_keras_loss_epoch_236  \\\n",
       "count                    5.000                    5.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.001                    0.001   \n",
       "min                     -0.999                   -0.999   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.997                   -0.997   \n",
       "\n",
       "       r2_keras_loss_epoch_237  r2_keras_loss_epoch_238  \\\n",
       "count                    4.000                    4.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.001                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.997                   -0.997   \n",
       "\n",
       "       r2_keras_loss_epoch_239  r2_keras_loss_epoch_240  \\\n",
       "count                    4.000                    4.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.001                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.997   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.997                   -0.997   \n",
       "\n",
       "       r2_keras_loss_epoch_241  r2_keras_loss_epoch_242  \\\n",
       "count                    4.000                    4.000   \n",
       "mean                    -0.997                   -0.997   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.997                   -0.997   \n",
       "\n",
       "       r2_keras_loss_epoch_243  r2_keras_loss_epoch_244  \\\n",
       "count                    4.000                    4.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.997                   -0.997   \n",
       "\n",
       "       r2_keras_loss_epoch_245  r2_keras_loss_epoch_246  \\\n",
       "count                    4.000                    4.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.997                   -0.997   \n",
       "max                     -0.997                   -0.997   \n",
       "\n",
       "       r2_keras_loss_epoch_247  r2_keras_loss_epoch_248  \\\n",
       "count                    4.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.997                   -0.998   \n",
       "max                     -0.997                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_249  r2_keras_loss_epoch_250  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_251  r2_keras_loss_epoch_252  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_253  r2_keras_loss_epoch_254  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_255  r2_keras_loss_epoch_256  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_257  r2_keras_loss_epoch_258  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_259  r2_keras_loss_epoch_260  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_261  r2_keras_loss_epoch_262  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_263  r2_keras_loss_epoch_264  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_265  r2_keras_loss_epoch_266  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_267  r2_keras_loss_epoch_268  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_269  r2_keras_loss_epoch_270  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_271  r2_keras_loss_epoch_272  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_273  r2_keras_loss_epoch_274  \\\n",
       "count                    3.000                    3.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                    0.000   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_275  r2_keras_loss_epoch_276  \\\n",
       "count                    2.000                    1.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                      0.000                      NaN   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_277  r2_keras_loss_epoch_278  \\\n",
       "count                    1.000                    1.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                        NaN                      NaN   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_279  r2_keras_loss_epoch_280  \\\n",
       "count                    1.000                    1.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                        NaN                      NaN   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_281  r2_keras_loss_epoch_282  \\\n",
       "count                    1.000                    1.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                        NaN                      NaN   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_283  r2_keras_loss_epoch_284  \\\n",
       "count                    1.000                    1.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                        NaN                      NaN   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_285  r2_keras_loss_epoch_286  \\\n",
       "count                    1.000                    1.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                        NaN                      NaN   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_287  r2_keras_loss_epoch_288  \\\n",
       "count                    1.000                    1.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                        NaN                      NaN   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_289  r2_keras_loss_epoch_290  \\\n",
       "count                    1.000                    1.000   \n",
       "mean                    -0.998                   -0.998   \n",
       "std                        NaN                      NaN   \n",
       "min                     -0.998                   -0.998   \n",
       "25%                     -0.998                   -0.998   \n",
       "50%                     -0.998                   -0.998   \n",
       "75%                     -0.998                   -0.998   \n",
       "max                     -0.998                   -0.998   \n",
       "\n",
       "       r2_keras_loss_epoch_291  r2_keras_loss_epoch_292  \n",
       "count                    1.000                    1.000  \n",
       "mean                    -0.998                   -0.998  \n",
       "std                        NaN                      NaN  \n",
       "min                     -0.998                   -0.998  \n",
       "25%                     -0.998                   -0.998  \n",
       "50%                     -0.998                   -0.998  \n",
       "75%                     -0.998                   -0.998  \n",
       "max                     -0.998                   -0.998  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.880Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T17:24:29.213914Z",
     "iopub.status.busy": "2021-09-24T17:24:29.213726Z",
     "iopub.status.idle": "2021-09-24T17:24:29.923573Z",
     "shell.execute_reply": "2021-09-24T17:24:29.922892Z",
     "shell.execute_reply.started": "2021-09-24T17:24:29.213890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>val_r2_keras_loss_epoch_1</th>\n",
       "      <th>val_r2_keras_loss_epoch_2</th>\n",
       "      <th>val_r2_keras_loss_epoch_3</th>\n",
       "      <th>val_r2_keras_loss_epoch_4</th>\n",
       "      <th>val_r2_keras_loss_epoch_5</th>\n",
       "      <th>val_r2_keras_loss_epoch_6</th>\n",
       "      <th>val_r2_keras_loss_epoch_7</th>\n",
       "      <th>val_r2_keras_loss_epoch_8</th>\n",
       "      <th>val_r2_keras_loss_epoch_9</th>\n",
       "      <th>val_r2_keras_loss_epoch_10</th>\n",
       "      <th>val_r2_keras_loss_epoch_11</th>\n",
       "      <th>val_r2_keras_loss_epoch_12</th>\n",
       "      <th>val_r2_keras_loss_epoch_13</th>\n",
       "      <th>val_r2_keras_loss_epoch_14</th>\n",
       "      <th>val_r2_keras_loss_epoch_15</th>\n",
       "      <th>val_r2_keras_loss_epoch_16</th>\n",
       "      <th>val_r2_keras_loss_epoch_17</th>\n",
       "      <th>val_r2_keras_loss_epoch_18</th>\n",
       "      <th>val_r2_keras_loss_epoch_19</th>\n",
       "      <th>val_r2_keras_loss_epoch_20</th>\n",
       "      <th>val_r2_keras_loss_epoch_21</th>\n",
       "      <th>val_r2_keras_loss_epoch_22</th>\n",
       "      <th>val_r2_keras_loss_epoch_23</th>\n",
       "      <th>val_r2_keras_loss_epoch_24</th>\n",
       "      <th>val_r2_keras_loss_epoch_25</th>\n",
       "      <th>val_r2_keras_loss_epoch_26</th>\n",
       "      <th>val_r2_keras_loss_epoch_27</th>\n",
       "      <th>val_r2_keras_loss_epoch_28</th>\n",
       "      <th>val_r2_keras_loss_epoch_29</th>\n",
       "      <th>val_r2_keras_loss_epoch_30</th>\n",
       "      <th>val_r2_keras_loss_epoch_31</th>\n",
       "      <th>val_r2_keras_loss_epoch_32</th>\n",
       "      <th>val_r2_keras_loss_epoch_33</th>\n",
       "      <th>val_r2_keras_loss_epoch_34</th>\n",
       "      <th>val_r2_keras_loss_epoch_35</th>\n",
       "      <th>val_r2_keras_loss_epoch_36</th>\n",
       "      <th>val_r2_keras_loss_epoch_37</th>\n",
       "      <th>val_r2_keras_loss_epoch_38</th>\n",
       "      <th>val_r2_keras_loss_epoch_39</th>\n",
       "      <th>val_r2_keras_loss_epoch_40</th>\n",
       "      <th>val_r2_keras_loss_epoch_41</th>\n",
       "      <th>val_r2_keras_loss_epoch_42</th>\n",
       "      <th>val_r2_keras_loss_epoch_43</th>\n",
       "      <th>val_r2_keras_loss_epoch_44</th>\n",
       "      <th>val_r2_keras_loss_epoch_45</th>\n",
       "      <th>val_r2_keras_loss_epoch_46</th>\n",
       "      <th>val_r2_keras_loss_epoch_47</th>\n",
       "      <th>val_r2_keras_loss_epoch_48</th>\n",
       "      <th>val_r2_keras_loss_epoch_49</th>\n",
       "      <th>val_r2_keras_loss_epoch_50</th>\n",
       "      <th>val_r2_keras_loss_epoch_51</th>\n",
       "      <th>val_r2_keras_loss_epoch_52</th>\n",
       "      <th>val_r2_keras_loss_epoch_53</th>\n",
       "      <th>val_r2_keras_loss_epoch_54</th>\n",
       "      <th>val_r2_keras_loss_epoch_55</th>\n",
       "      <th>val_r2_keras_loss_epoch_56</th>\n",
       "      <th>val_r2_keras_loss_epoch_57</th>\n",
       "      <th>val_r2_keras_loss_epoch_58</th>\n",
       "      <th>val_r2_keras_loss_epoch_59</th>\n",
       "      <th>val_r2_keras_loss_epoch_60</th>\n",
       "      <th>val_r2_keras_loss_epoch_61</th>\n",
       "      <th>val_r2_keras_loss_epoch_62</th>\n",
       "      <th>val_r2_keras_loss_epoch_63</th>\n",
       "      <th>val_r2_keras_loss_epoch_64</th>\n",
       "      <th>val_r2_keras_loss_epoch_65</th>\n",
       "      <th>val_r2_keras_loss_epoch_66</th>\n",
       "      <th>val_r2_keras_loss_epoch_67</th>\n",
       "      <th>val_r2_keras_loss_epoch_68</th>\n",
       "      <th>val_r2_keras_loss_epoch_69</th>\n",
       "      <th>val_r2_keras_loss_epoch_70</th>\n",
       "      <th>val_r2_keras_loss_epoch_71</th>\n",
       "      <th>val_r2_keras_loss_epoch_72</th>\n",
       "      <th>val_r2_keras_loss_epoch_73</th>\n",
       "      <th>val_r2_keras_loss_epoch_74</th>\n",
       "      <th>val_r2_keras_loss_epoch_75</th>\n",
       "      <th>val_r2_keras_loss_epoch_76</th>\n",
       "      <th>val_r2_keras_loss_epoch_77</th>\n",
       "      <th>val_r2_keras_loss_epoch_78</th>\n",
       "      <th>val_r2_keras_loss_epoch_79</th>\n",
       "      <th>val_r2_keras_loss_epoch_80</th>\n",
       "      <th>val_r2_keras_loss_epoch_81</th>\n",
       "      <th>val_r2_keras_loss_epoch_82</th>\n",
       "      <th>val_r2_keras_loss_epoch_83</th>\n",
       "      <th>val_r2_keras_loss_epoch_84</th>\n",
       "      <th>val_r2_keras_loss_epoch_85</th>\n",
       "      <th>val_r2_keras_loss_epoch_86</th>\n",
       "      <th>val_r2_keras_loss_epoch_87</th>\n",
       "      <th>val_r2_keras_loss_epoch_88</th>\n",
       "      <th>val_r2_keras_loss_epoch_89</th>\n",
       "      <th>val_r2_keras_loss_epoch_90</th>\n",
       "      <th>val_r2_keras_loss_epoch_91</th>\n",
       "      <th>val_r2_keras_loss_epoch_92</th>\n",
       "      <th>val_r2_keras_loss_epoch_93</th>\n",
       "      <th>val_r2_keras_loss_epoch_94</th>\n",
       "      <th>val_r2_keras_loss_epoch_95</th>\n",
       "      <th>val_r2_keras_loss_epoch_96</th>\n",
       "      <th>val_r2_keras_loss_epoch_97</th>\n",
       "      <th>val_r2_keras_loss_epoch_98</th>\n",
       "      <th>val_r2_keras_loss_epoch_99</th>\n",
       "      <th>val_r2_keras_loss_epoch_100</th>\n",
       "      <th>val_r2_keras_loss_epoch_101</th>\n",
       "      <th>val_r2_keras_loss_epoch_102</th>\n",
       "      <th>val_r2_keras_loss_epoch_103</th>\n",
       "      <th>val_r2_keras_loss_epoch_104</th>\n",
       "      <th>val_r2_keras_loss_epoch_105</th>\n",
       "      <th>val_r2_keras_loss_epoch_106</th>\n",
       "      <th>val_r2_keras_loss_epoch_107</th>\n",
       "      <th>val_r2_keras_loss_epoch_108</th>\n",
       "      <th>val_r2_keras_loss_epoch_109</th>\n",
       "      <th>val_r2_keras_loss_epoch_110</th>\n",
       "      <th>val_r2_keras_loss_epoch_111</th>\n",
       "      <th>val_r2_keras_loss_epoch_112</th>\n",
       "      <th>val_r2_keras_loss_epoch_113</th>\n",
       "      <th>val_r2_keras_loss_epoch_114</th>\n",
       "      <th>val_r2_keras_loss_epoch_115</th>\n",
       "      <th>val_r2_keras_loss_epoch_116</th>\n",
       "      <th>val_r2_keras_loss_epoch_117</th>\n",
       "      <th>val_r2_keras_loss_epoch_118</th>\n",
       "      <th>val_r2_keras_loss_epoch_119</th>\n",
       "      <th>val_r2_keras_loss_epoch_120</th>\n",
       "      <th>val_r2_keras_loss_epoch_121</th>\n",
       "      <th>val_r2_keras_loss_epoch_122</th>\n",
       "      <th>val_r2_keras_loss_epoch_123</th>\n",
       "      <th>val_r2_keras_loss_epoch_124</th>\n",
       "      <th>val_r2_keras_loss_epoch_125</th>\n",
       "      <th>val_r2_keras_loss_epoch_126</th>\n",
       "      <th>val_r2_keras_loss_epoch_127</th>\n",
       "      <th>val_r2_keras_loss_epoch_128</th>\n",
       "      <th>val_r2_keras_loss_epoch_129</th>\n",
       "      <th>val_r2_keras_loss_epoch_130</th>\n",
       "      <th>val_r2_keras_loss_epoch_131</th>\n",
       "      <th>val_r2_keras_loss_epoch_132</th>\n",
       "      <th>val_r2_keras_loss_epoch_133</th>\n",
       "      <th>val_r2_keras_loss_epoch_134</th>\n",
       "      <th>val_r2_keras_loss_epoch_135</th>\n",
       "      <th>val_r2_keras_loss_epoch_136</th>\n",
       "      <th>val_r2_keras_loss_epoch_137</th>\n",
       "      <th>val_r2_keras_loss_epoch_138</th>\n",
       "      <th>val_r2_keras_loss_epoch_139</th>\n",
       "      <th>val_r2_keras_loss_epoch_140</th>\n",
       "      <th>val_r2_keras_loss_epoch_141</th>\n",
       "      <th>val_r2_keras_loss_epoch_142</th>\n",
       "      <th>val_r2_keras_loss_epoch_143</th>\n",
       "      <th>val_r2_keras_loss_epoch_144</th>\n",
       "      <th>val_r2_keras_loss_epoch_145</th>\n",
       "      <th>val_r2_keras_loss_epoch_146</th>\n",
       "      <th>val_r2_keras_loss_epoch_147</th>\n",
       "      <th>val_r2_keras_loss_epoch_148</th>\n",
       "      <th>val_r2_keras_loss_epoch_149</th>\n",
       "      <th>val_r2_keras_loss_epoch_150</th>\n",
       "      <th>val_r2_keras_loss_epoch_151</th>\n",
       "      <th>val_r2_keras_loss_epoch_152</th>\n",
       "      <th>val_r2_keras_loss_epoch_153</th>\n",
       "      <th>val_r2_keras_loss_epoch_154</th>\n",
       "      <th>val_r2_keras_loss_epoch_155</th>\n",
       "      <th>val_r2_keras_loss_epoch_156</th>\n",
       "      <th>val_r2_keras_loss_epoch_157</th>\n",
       "      <th>val_r2_keras_loss_epoch_158</th>\n",
       "      <th>val_r2_keras_loss_epoch_159</th>\n",
       "      <th>val_r2_keras_loss_epoch_160</th>\n",
       "      <th>val_r2_keras_loss_epoch_161</th>\n",
       "      <th>val_r2_keras_loss_epoch_162</th>\n",
       "      <th>val_r2_keras_loss_epoch_163</th>\n",
       "      <th>val_r2_keras_loss_epoch_164</th>\n",
       "      <th>val_r2_keras_loss_epoch_165</th>\n",
       "      <th>val_r2_keras_loss_epoch_166</th>\n",
       "      <th>val_r2_keras_loss_epoch_167</th>\n",
       "      <th>val_r2_keras_loss_epoch_168</th>\n",
       "      <th>val_r2_keras_loss_epoch_169</th>\n",
       "      <th>val_r2_keras_loss_epoch_170</th>\n",
       "      <th>val_r2_keras_loss_epoch_171</th>\n",
       "      <th>val_r2_keras_loss_epoch_172</th>\n",
       "      <th>val_r2_keras_loss_epoch_173</th>\n",
       "      <th>val_r2_keras_loss_epoch_174</th>\n",
       "      <th>val_r2_keras_loss_epoch_175</th>\n",
       "      <th>val_r2_keras_loss_epoch_176</th>\n",
       "      <th>val_r2_keras_loss_epoch_177</th>\n",
       "      <th>val_r2_keras_loss_epoch_178</th>\n",
       "      <th>val_r2_keras_loss_epoch_179</th>\n",
       "      <th>val_r2_keras_loss_epoch_180</th>\n",
       "      <th>val_r2_keras_loss_epoch_181</th>\n",
       "      <th>val_r2_keras_loss_epoch_182</th>\n",
       "      <th>val_r2_keras_loss_epoch_183</th>\n",
       "      <th>val_r2_keras_loss_epoch_184</th>\n",
       "      <th>val_r2_keras_loss_epoch_185</th>\n",
       "      <th>val_r2_keras_loss_epoch_186</th>\n",
       "      <th>val_r2_keras_loss_epoch_187</th>\n",
       "      <th>val_r2_keras_loss_epoch_188</th>\n",
       "      <th>val_r2_keras_loss_epoch_189</th>\n",
       "      <th>val_r2_keras_loss_epoch_190</th>\n",
       "      <th>val_r2_keras_loss_epoch_191</th>\n",
       "      <th>val_r2_keras_loss_epoch_192</th>\n",
       "      <th>val_r2_keras_loss_epoch_193</th>\n",
       "      <th>val_r2_keras_loss_epoch_194</th>\n",
       "      <th>val_r2_keras_loss_epoch_195</th>\n",
       "      <th>val_r2_keras_loss_epoch_196</th>\n",
       "      <th>val_r2_keras_loss_epoch_197</th>\n",
       "      <th>val_r2_keras_loss_epoch_198</th>\n",
       "      <th>val_r2_keras_loss_epoch_199</th>\n",
       "      <th>val_r2_keras_loss_epoch_200</th>\n",
       "      <th>val_r2_keras_loss_epoch_201</th>\n",
       "      <th>val_r2_keras_loss_epoch_202</th>\n",
       "      <th>val_r2_keras_loss_epoch_203</th>\n",
       "      <th>val_r2_keras_loss_epoch_204</th>\n",
       "      <th>val_r2_keras_loss_epoch_205</th>\n",
       "      <th>val_r2_keras_loss_epoch_206</th>\n",
       "      <th>val_r2_keras_loss_epoch_207</th>\n",
       "      <th>val_r2_keras_loss_epoch_208</th>\n",
       "      <th>val_r2_keras_loss_epoch_209</th>\n",
       "      <th>val_r2_keras_loss_epoch_210</th>\n",
       "      <th>val_r2_keras_loss_epoch_211</th>\n",
       "      <th>val_r2_keras_loss_epoch_212</th>\n",
       "      <th>val_r2_keras_loss_epoch_213</th>\n",
       "      <th>val_r2_keras_loss_epoch_214</th>\n",
       "      <th>val_r2_keras_loss_epoch_215</th>\n",
       "      <th>val_r2_keras_loss_epoch_216</th>\n",
       "      <th>val_r2_keras_loss_epoch_217</th>\n",
       "      <th>val_r2_keras_loss_epoch_218</th>\n",
       "      <th>val_r2_keras_loss_epoch_219</th>\n",
       "      <th>val_r2_keras_loss_epoch_220</th>\n",
       "      <th>val_r2_keras_loss_epoch_221</th>\n",
       "      <th>val_r2_keras_loss_epoch_222</th>\n",
       "      <th>val_r2_keras_loss_epoch_223</th>\n",
       "      <th>val_r2_keras_loss_epoch_224</th>\n",
       "      <th>val_r2_keras_loss_epoch_225</th>\n",
       "      <th>val_r2_keras_loss_epoch_226</th>\n",
       "      <th>val_r2_keras_loss_epoch_227</th>\n",
       "      <th>val_r2_keras_loss_epoch_228</th>\n",
       "      <th>val_r2_keras_loss_epoch_229</th>\n",
       "      <th>val_r2_keras_loss_epoch_230</th>\n",
       "      <th>val_r2_keras_loss_epoch_231</th>\n",
       "      <th>val_r2_keras_loss_epoch_232</th>\n",
       "      <th>val_r2_keras_loss_epoch_233</th>\n",
       "      <th>val_r2_keras_loss_epoch_234</th>\n",
       "      <th>val_r2_keras_loss_epoch_235</th>\n",
       "      <th>val_r2_keras_loss_epoch_236</th>\n",
       "      <th>val_r2_keras_loss_epoch_237</th>\n",
       "      <th>val_r2_keras_loss_epoch_238</th>\n",
       "      <th>val_r2_keras_loss_epoch_239</th>\n",
       "      <th>val_r2_keras_loss_epoch_240</th>\n",
       "      <th>val_r2_keras_loss_epoch_241</th>\n",
       "      <th>val_r2_keras_loss_epoch_242</th>\n",
       "      <th>val_r2_keras_loss_epoch_243</th>\n",
       "      <th>val_r2_keras_loss_epoch_244</th>\n",
       "      <th>val_r2_keras_loss_epoch_245</th>\n",
       "      <th>val_r2_keras_loss_epoch_246</th>\n",
       "      <th>val_r2_keras_loss_epoch_247</th>\n",
       "      <th>val_r2_keras_loss_epoch_248</th>\n",
       "      <th>val_r2_keras_loss_epoch_249</th>\n",
       "      <th>val_r2_keras_loss_epoch_250</th>\n",
       "      <th>val_r2_keras_loss_epoch_251</th>\n",
       "      <th>val_r2_keras_loss_epoch_252</th>\n",
       "      <th>val_r2_keras_loss_epoch_253</th>\n",
       "      <th>val_r2_keras_loss_epoch_254</th>\n",
       "      <th>val_r2_keras_loss_epoch_255</th>\n",
       "      <th>val_r2_keras_loss_epoch_256</th>\n",
       "      <th>val_r2_keras_loss_epoch_257</th>\n",
       "      <th>val_r2_keras_loss_epoch_258</th>\n",
       "      <th>val_r2_keras_loss_epoch_259</th>\n",
       "      <th>val_r2_keras_loss_epoch_260</th>\n",
       "      <th>val_r2_keras_loss_epoch_261</th>\n",
       "      <th>val_r2_keras_loss_epoch_262</th>\n",
       "      <th>val_r2_keras_loss_epoch_263</th>\n",
       "      <th>val_r2_keras_loss_epoch_264</th>\n",
       "      <th>val_r2_keras_loss_epoch_265</th>\n",
       "      <th>val_r2_keras_loss_epoch_266</th>\n",
       "      <th>val_r2_keras_loss_epoch_267</th>\n",
       "      <th>val_r2_keras_loss_epoch_268</th>\n",
       "      <th>val_r2_keras_loss_epoch_269</th>\n",
       "      <th>val_r2_keras_loss_epoch_270</th>\n",
       "      <th>val_r2_keras_loss_epoch_271</th>\n",
       "      <th>val_r2_keras_loss_epoch_272</th>\n",
       "      <th>val_r2_keras_loss_epoch_273</th>\n",
       "      <th>val_r2_keras_loss_epoch_274</th>\n",
       "      <th>val_r2_keras_loss_epoch_275</th>\n",
       "      <th>val_r2_keras_loss_epoch_276</th>\n",
       "      <th>val_r2_keras_loss_epoch_277</th>\n",
       "      <th>val_r2_keras_loss_epoch_278</th>\n",
       "      <th>val_r2_keras_loss_epoch_279</th>\n",
       "      <th>val_r2_keras_loss_epoch_280</th>\n",
       "      <th>val_r2_keras_loss_epoch_281</th>\n",
       "      <th>val_r2_keras_loss_epoch_282</th>\n",
       "      <th>val_r2_keras_loss_epoch_283</th>\n",
       "      <th>val_r2_keras_loss_epoch_284</th>\n",
       "      <th>val_r2_keras_loss_epoch_285</th>\n",
       "      <th>val_r2_keras_loss_epoch_286</th>\n",
       "      <th>val_r2_keras_loss_epoch_287</th>\n",
       "      <th>val_r2_keras_loss_epoch_288</th>\n",
       "      <th>val_r2_keras_loss_epoch_289</th>\n",
       "      <th>val_r2_keras_loss_epoch_290</th>\n",
       "      <th>val_r2_keras_loss_epoch_291</th>\n",
       "      <th>val_r2_keras_loss_epoch_292</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>9997.000</td>\n",
       "      <td>9996.000</td>\n",
       "      <td>9992.000</td>\n",
       "      <td>9982.000</td>\n",
       "      <td>9962.000</td>\n",
       "      <td>9933.000</td>\n",
       "      <td>9879.000</td>\n",
       "      <td>9803.000</td>\n",
       "      <td>9713.000</td>\n",
       "      <td>9623.000</td>\n",
       "      <td>9547.000</td>\n",
       "      <td>9451.000</td>\n",
       "      <td>9350.000</td>\n",
       "      <td>9253.000</td>\n",
       "      <td>9165.000</td>\n",
       "      <td>9085.000</td>\n",
       "      <td>9010.000</td>\n",
       "      <td>8945.000</td>\n",
       "      <td>8886.000</td>\n",
       "      <td>8830.000</td>\n",
       "      <td>8774.000</td>\n",
       "      <td>8701.000</td>\n",
       "      <td>8634.000</td>\n",
       "      <td>8582.000</td>\n",
       "      <td>8508.000</td>\n",
       "      <td>8436.000</td>\n",
       "      <td>8355.000</td>\n",
       "      <td>8265.000</td>\n",
       "      <td>8167.000</td>\n",
       "      <td>8053.000</td>\n",
       "      <td>7937.000</td>\n",
       "      <td>7797.000</td>\n",
       "      <td>7660.000</td>\n",
       "      <td>7517.000</td>\n",
       "      <td>7374.000</td>\n",
       "      <td>7205.000</td>\n",
       "      <td>7035.000</td>\n",
       "      <td>6871.000</td>\n",
       "      <td>6685.000</td>\n",
       "      <td>6493.000</td>\n",
       "      <td>6323.000</td>\n",
       "      <td>6142.000</td>\n",
       "      <td>5943.000</td>\n",
       "      <td>5764.000</td>\n",
       "      <td>5561.000</td>\n",
       "      <td>5382.000</td>\n",
       "      <td>5198.000</td>\n",
       "      <td>4997.000</td>\n",
       "      <td>4824.000</td>\n",
       "      <td>4667.000</td>\n",
       "      <td>4475.000</td>\n",
       "      <td>4298.000</td>\n",
       "      <td>4128.000</td>\n",
       "      <td>3963.000</td>\n",
       "      <td>3838.000</td>\n",
       "      <td>3685.000</td>\n",
       "      <td>3538.000</td>\n",
       "      <td>3398.000</td>\n",
       "      <td>3290.000</td>\n",
       "      <td>3154.000</td>\n",
       "      <td>3032.000</td>\n",
       "      <td>2917.000</td>\n",
       "      <td>2796.000</td>\n",
       "      <td>2671.000</td>\n",
       "      <td>2586.000</td>\n",
       "      <td>2508.000</td>\n",
       "      <td>2416.000</td>\n",
       "      <td>2323.000</td>\n",
       "      <td>2232.000</td>\n",
       "      <td>2137.000</td>\n",
       "      <td>2054.000</td>\n",
       "      <td>1991.000</td>\n",
       "      <td>1925.000</td>\n",
       "      <td>1861.000</td>\n",
       "      <td>1791.000</td>\n",
       "      <td>1723.000</td>\n",
       "      <td>1672.000</td>\n",
       "      <td>1629.000</td>\n",
       "      <td>1574.000</td>\n",
       "      <td>1522.000</td>\n",
       "      <td>1474.000</td>\n",
       "      <td>1421.000</td>\n",
       "      <td>1373.000</td>\n",
       "      <td>1333.000</td>\n",
       "      <td>1286.000</td>\n",
       "      <td>1253.000</td>\n",
       "      <td>1207.000</td>\n",
       "      <td>1169.000</td>\n",
       "      <td>1122.000</td>\n",
       "      <td>1081.000</td>\n",
       "      <td>1048.000</td>\n",
       "      <td>1016.000</td>\n",
       "      <td>986.000</td>\n",
       "      <td>950.000</td>\n",
       "      <td>921.000</td>\n",
       "      <td>887.000</td>\n",
       "      <td>858.000</td>\n",
       "      <td>823.000</td>\n",
       "      <td>794.000</td>\n",
       "      <td>764.000</td>\n",
       "      <td>728.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>677.000</td>\n",
       "      <td>666.000</td>\n",
       "      <td>641.000</td>\n",
       "      <td>609.000</td>\n",
       "      <td>582.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>499.000</td>\n",
       "      <td>484.000</td>\n",
       "      <td>467.000</td>\n",
       "      <td>445.000</td>\n",
       "      <td>432.000</td>\n",
       "      <td>412.000</td>\n",
       "      <td>393.000</td>\n",
       "      <td>381.000</td>\n",
       "      <td>372.000</td>\n",
       "      <td>358.000</td>\n",
       "      <td>344.000</td>\n",
       "      <td>334.000</td>\n",
       "      <td>321.000</td>\n",
       "      <td>311.000</td>\n",
       "      <td>290.000</td>\n",
       "      <td>280.000</td>\n",
       "      <td>273.000</td>\n",
       "      <td>260.000</td>\n",
       "      <td>256.000</td>\n",
       "      <td>241.000</td>\n",
       "      <td>235.000</td>\n",
       "      <td>227.000</td>\n",
       "      <td>218.000</td>\n",
       "      <td>207.000</td>\n",
       "      <td>202.000</td>\n",
       "      <td>192.000</td>\n",
       "      <td>185.000</td>\n",
       "      <td>176.000</td>\n",
       "      <td>173.000</td>\n",
       "      <td>164.000</td>\n",
       "      <td>159.000</td>\n",
       "      <td>150.000</td>\n",
       "      <td>147.000</td>\n",
       "      <td>143.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>131.000</td>\n",
       "      <td>124.000</td>\n",
       "      <td>119.000</td>\n",
       "      <td>115.000</td>\n",
       "      <td>112.000</td>\n",
       "      <td>106.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>101.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>91.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>77.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>70.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>68.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>63.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>52.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>44.000</td>\n",
       "      <td>43.000</td>\n",
       "      <td>41.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>39.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>36.000</td>\n",
       "      <td>35.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>32.000</td>\n",
       "      <td>31.000</td>\n",
       "      <td>29.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>23.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>19.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4999.500</td>\n",
       "      <td>530.777</td>\n",
       "      <td>220.435</td>\n",
       "      <td>84.180</td>\n",
       "      <td>41.939</td>\n",
       "      <td>29.081</td>\n",
       "      <td>13.705</td>\n",
       "      <td>6.784</td>\n",
       "      <td>2.894</td>\n",
       "      <td>1.231</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.597</td>\n",
       "      <td>-0.656</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>-0.827</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-0.914</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.896</td>\n",
       "      <td>34236.449</td>\n",
       "      <td>15396.830</td>\n",
       "      <td>6488.106</td>\n",
       "      <td>3212.267</td>\n",
       "      <td>2306.998</td>\n",
       "      <td>1010.552</td>\n",
       "      <td>495.970</td>\n",
       "      <td>214.680</td>\n",
       "      <td>113.373</td>\n",
       "      <td>64.067</td>\n",
       "      <td>37.066</td>\n",
       "      <td>24.899</td>\n",
       "      <td>17.056</td>\n",
       "      <td>11.424</td>\n",
       "      <td>8.419</td>\n",
       "      <td>7.608</td>\n",
       "      <td>8.398</td>\n",
       "      <td>5.782</td>\n",
       "      <td>14.604</td>\n",
       "      <td>20.034</td>\n",
       "      <td>3.721</td>\n",
       "      <td>22.964</td>\n",
       "      <td>4.176</td>\n",
       "      <td>3.080</td>\n",
       "      <td>2.843</td>\n",
       "      <td>2.677</td>\n",
       "      <td>2.708</td>\n",
       "      <td>2.322</td>\n",
       "      <td>2.297</td>\n",
       "      <td>1.421</td>\n",
       "      <td>1.916</td>\n",
       "      <td>1.313</td>\n",
       "      <td>1.376</td>\n",
       "      <td>1.243</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1.138</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.595</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.731</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2499.750</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.633</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>-0.847</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4999.500</td>\n",
       "      <td>1.264</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>-0.915</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7499.250</td>\n",
       "      <td>3.074</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.761</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-0.814</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.873</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-0.915</td>\n",
       "      <td>-0.922</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9999.000</td>\n",
       "      <td>3370479.750</td>\n",
       "      <td>1523124.000</td>\n",
       "      <td>645042.750</td>\n",
       "      <td>319532.938</td>\n",
       "      <td>229758.359</td>\n",
       "      <td>99957.914</td>\n",
       "      <td>48586.758</td>\n",
       "      <td>20411.232</td>\n",
       "      <td>10558.509</td>\n",
       "      <td>5993.642</td>\n",
       "      <td>3502.091</td>\n",
       "      <td>2325.281</td>\n",
       "      <td>1537.912</td>\n",
       "      <td>962.798</td>\n",
       "      <td>652.744</td>\n",
       "      <td>565.669</td>\n",
       "      <td>729.646</td>\n",
       "      <td>412.306</td>\n",
       "      <td>1411.594</td>\n",
       "      <td>1972.471</td>\n",
       "      <td>195.110</td>\n",
       "      <td>2271.153</td>\n",
       "      <td>275.705</td>\n",
       "      <td>175.401</td>\n",
       "      <td>173.592</td>\n",
       "      <td>179.379</td>\n",
       "      <td>172.847</td>\n",
       "      <td>163.015</td>\n",
       "      <td>153.365</td>\n",
       "      <td>87.906</td>\n",
       "      <td>155.058</td>\n",
       "      <td>83.226</td>\n",
       "      <td>87.598</td>\n",
       "      <td>80.308</td>\n",
       "      <td>87.306</td>\n",
       "      <td>71.384</td>\n",
       "      <td>69.536</td>\n",
       "      <td>129.795</td>\n",
       "      <td>63.289</td>\n",
       "      <td>57.011</td>\n",
       "      <td>93.708</td>\n",
       "      <td>63.985</td>\n",
       "      <td>49.097</td>\n",
       "      <td>46.666</td>\n",
       "      <td>48.530</td>\n",
       "      <td>47.385</td>\n",
       "      <td>49.688</td>\n",
       "      <td>43.781</td>\n",
       "      <td>8.149</td>\n",
       "      <td>8.848</td>\n",
       "      <td>7.105</td>\n",
       "      <td>6.841</td>\n",
       "      <td>6.310</td>\n",
       "      <td>5.746</td>\n",
       "      <td>5.675</td>\n",
       "      <td>5.508</td>\n",
       "      <td>4.648</td>\n",
       "      <td>4.312</td>\n",
       "      <td>4.136</td>\n",
       "      <td>5.090</td>\n",
       "      <td>5.808</td>\n",
       "      <td>3.261</td>\n",
       "      <td>3.971</td>\n",
       "      <td>2.888</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>-0.583</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>-0.587</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>-0.583</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>-0.666</td>\n",
       "      <td>-0.668</td>\n",
       "      <td>-0.703</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>-0.827</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>-0.847</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.992</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  val_r2_keras_loss_epoch_1  val_r2_keras_loss_epoch_2  \\\n",
       "count 10000.000                  10000.000                  10000.000   \n",
       "mean   4999.500                    530.777                    220.435   \n",
       "std    2886.896                  34236.449                  15396.830   \n",
       "min       0.000                     -0.974                     -0.989   \n",
       "25%    2499.750                     -0.353                     -0.633   \n",
       "50%    4999.500                      1.264                     -0.131   \n",
       "75%    7499.250                      3.074                      0.403   \n",
       "max    9999.000                3370479.750                1523124.000   \n",
       "\n",
       "       val_r2_keras_loss_epoch_3  val_r2_keras_loss_epoch_4  \\\n",
       "count                  10000.000                  10000.000   \n",
       "mean                      84.180                     41.939   \n",
       "std                     6488.106                   3212.267   \n",
       "min                       -0.993                     -0.996   \n",
       "25%                       -0.787                     -0.847   \n",
       "50%                       -0.416                     -0.603   \n",
       "75%                       -0.093                     -0.370   \n",
       "max                   645042.750                 319532.938   \n",
       "\n",
       "       val_r2_keras_loss_epoch_5  val_r2_keras_loss_epoch_6  \\\n",
       "count                  10000.000                  10000.000   \n",
       "mean                      29.081                     13.705   \n",
       "std                     2306.998                   1010.552   \n",
       "min                       -0.998                     -0.999   \n",
       "25%                       -0.879                     -0.908   \n",
       "50%                       -0.729                     -0.803   \n",
       "75%                       -0.548                     -0.654   \n",
       "max                   229758.359                  99957.914   \n",
       "\n",
       "       val_r2_keras_loss_epoch_7  val_r2_keras_loss_epoch_8  \\\n",
       "count                  10000.000                  10000.000   \n",
       "mean                       6.784                      2.894   \n",
       "std                      495.970                    214.680   \n",
       "min                       -1.000                     -1.000   \n",
       "25%                       -0.930                     -0.942   \n",
       "50%                       -0.844                     -0.867   \n",
       "75%                       -0.720                     -0.761   \n",
       "max                    48586.758                  20411.232   \n",
       "\n",
       "       val_r2_keras_loss_epoch_9  val_r2_keras_loss_epoch_10  \\\n",
       "count                  10000.000                   10000.000   \n",
       "mean                       1.231                       0.327   \n",
       "std                      113.373                      64.067   \n",
       "min                       -1.000                      -1.000   \n",
       "25%                       -0.951                      -0.957   \n",
       "50%                       -0.884                      -0.897   \n",
       "75%                       -0.785                      -0.803   \n",
       "max                    10558.509                    5993.642   \n",
       "\n",
       "       val_r2_keras_loss_epoch_11  val_r2_keras_loss_epoch_12  \\\n",
       "count                   10000.000                   10000.000   \n",
       "mean                       -0.157                      -0.368   \n",
       "std                        37.066                      24.899   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.962                      -0.966   \n",
       "50%                        -0.907                      -0.915   \n",
       "75%                        -0.814                      -0.822   \n",
       "max                      3502.091                    2325.281   \n",
       "\n",
       "       val_r2_keras_loss_epoch_13  val_r2_keras_loss_epoch_14  \\\n",
       "count                   10000.000                   10000.000   \n",
       "mean                       -0.497                      -0.597   \n",
       "std                        17.056                      11.424   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.969                      -0.972   \n",
       "50%                        -0.920                      -0.925   \n",
       "75%                        -0.829                      -0.835   \n",
       "max                      1537.912                     962.798   \n",
       "\n",
       "       val_r2_keras_loss_epoch_15  val_r2_keras_loss_epoch_16  \\\n",
       "count                   10000.000                   10000.000   \n",
       "mean                       -0.656                      -0.683   \n",
       "std                         8.419                       7.608   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.975                      -0.977   \n",
       "50%                        -0.930                      -0.934   \n",
       "75%                        -0.840                      -0.846   \n",
       "max                       652.744                     565.669   \n",
       "\n",
       "       val_r2_keras_loss_epoch_17  val_r2_keras_loss_epoch_18  \\\n",
       "count                   10000.000                    9997.000   \n",
       "mean                       -0.694                      -0.737   \n",
       "std                         8.398                       5.782   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.980                      -0.982   \n",
       "50%                        -0.938                      -0.942   \n",
       "75%                        -0.850                      -0.855   \n",
       "max                       729.646                     412.306   \n",
       "\n",
       "       val_r2_keras_loss_epoch_19  val_r2_keras_loss_epoch_20  \\\n",
       "count                    9996.000                    9992.000   \n",
       "mean                       -0.651                      -0.608   \n",
       "std                        14.604                      20.034   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.984                      -0.986   \n",
       "50%                        -0.945                      -0.948   \n",
       "75%                        -0.861                      -0.867   \n",
       "max                      1411.594                    1972.471   \n",
       "\n",
       "       val_r2_keras_loss_epoch_21  val_r2_keras_loss_epoch_22  \\\n",
       "count                    9982.000                    9962.000   \n",
       "mean                       -0.798                      -0.598   \n",
       "std                         3.721                      22.964   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.988                      -0.989   \n",
       "50%                        -0.952                      -0.956   \n",
       "75%                        -0.873                      -0.881   \n",
       "max                       195.110                    2271.153   \n",
       "\n",
       "       val_r2_keras_loss_epoch_23  val_r2_keras_loss_epoch_24  \\\n",
       "count                    9933.000                    9879.000   \n",
       "mean                       -0.801                      -0.827   \n",
       "std                         4.176                       3.080   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.990                      -0.991   \n",
       "50%                        -0.959                      -0.962   \n",
       "75%                        -0.887                      -0.893   \n",
       "max                       275.705                     175.401   \n",
       "\n",
       "       val_r2_keras_loss_epoch_25  val_r2_keras_loss_epoch_26  \\\n",
       "count                    9803.000                    9713.000   \n",
       "mean                       -0.840                      -0.855   \n",
       "std                         2.843                       2.677   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.992                      -0.993   \n",
       "50%                        -0.966                      -0.969   \n",
       "75%                        -0.901                      -0.908   \n",
       "max                       173.592                     179.379   \n",
       "\n",
       "       val_r2_keras_loss_epoch_27  val_r2_keras_loss_epoch_28  \\\n",
       "count                    9623.000                    9547.000   \n",
       "mean                       -0.860                      -0.871   \n",
       "std                         2.708                       2.322   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.994                      -0.994   \n",
       "50%                        -0.972                      -0.974   \n",
       "75%                        -0.915                      -0.922   \n",
       "max                       172.847                     163.015   \n",
       "\n",
       "       val_r2_keras_loss_epoch_29  val_r2_keras_loss_epoch_30  \\\n",
       "count                    9451.000                    9350.000   \n",
       "mean                       -0.876                      -0.902   \n",
       "std                         2.297                       1.421   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.995                      -0.995   \n",
       "50%                        -0.977                      -0.979   \n",
       "75%                        -0.928                      -0.933   \n",
       "max                       153.365                      87.906   \n",
       "\n",
       "       val_r2_keras_loss_epoch_31  val_r2_keras_loss_epoch_32  \\\n",
       "count                    9253.000                    9165.000   \n",
       "mean                       -0.900                      -0.912   \n",
       "std                         1.916                       1.313   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.996                      -0.996   \n",
       "50%                        -0.981                      -0.983   \n",
       "75%                        -0.939                      -0.944   \n",
       "max                       155.058                      83.226   \n",
       "\n",
       "       val_r2_keras_loss_epoch_33  val_r2_keras_loss_epoch_34  \\\n",
       "count                    9085.000                    9010.000   \n",
       "mean                       -0.914                      -0.920   \n",
       "std                         1.376                       1.243   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.996                      -0.996   \n",
       "50%                        -0.984                      -0.985   \n",
       "75%                        -0.948                      -0.952   \n",
       "max                        87.598                      80.308   \n",
       "\n",
       "       val_r2_keras_loss_epoch_35  val_r2_keras_loss_epoch_36  \\\n",
       "count                    8945.000                    8886.000   \n",
       "mean                       -0.923                      -0.928   \n",
       "std                         1.284                       1.138   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.996                      -0.997   \n",
       "50%                        -0.987                      -0.987   \n",
       "75%                        -0.955                      -0.959   \n",
       "max                        87.306                      71.384   \n",
       "\n",
       "       val_r2_keras_loss_epoch_37  val_r2_keras_loss_epoch_38  \\\n",
       "count                    8830.000                    8774.000   \n",
       "mean                       -0.932                      -0.927   \n",
       "std                         1.100                       1.595   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.997                      -0.997   \n",
       "50%                        -0.988                      -0.989   \n",
       "75%                        -0.961                      -0.964   \n",
       "max                        69.536                     129.795   \n",
       "\n",
       "       val_r2_keras_loss_epoch_39  val_r2_keras_loss_epoch_40  \\\n",
       "count                    8701.000                    8634.000   \n",
       "mean                       -0.945                      -0.948   \n",
       "std                         0.805                       0.731   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.997                      -0.997   \n",
       "50%                        -0.989                      -0.990   \n",
       "75%                        -0.966                      -0.968   \n",
       "max                        63.289                      57.011   \n",
       "\n",
       "       val_r2_keras_loss_epoch_41  val_r2_keras_loss_epoch_42  \\\n",
       "count                    8582.000                    8508.000   \n",
       "mean                       -0.946                      -0.954   \n",
       "std                         1.100                       0.743   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.997                      -0.997   \n",
       "50%                        -0.990                      -0.991   \n",
       "75%                        -0.970                      -0.973   \n",
       "max                        93.708                      63.985   \n",
       "\n",
       "       val_r2_keras_loss_epoch_43  val_r2_keras_loss_epoch_44  \\\n",
       "count                    8436.000                    8355.000   \n",
       "mean                       -0.957                      -0.959   \n",
       "std                         0.589                       0.565   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.997                      -0.997   \n",
       "50%                        -0.991                      -0.991   \n",
       "75%                        -0.974                      -0.975   \n",
       "max                        49.097                      46.666   \n",
       "\n",
       "       val_r2_keras_loss_epoch_45  val_r2_keras_loss_epoch_46  \\\n",
       "count                    8265.000                    8167.000   \n",
       "mean                       -0.961                      -0.962   \n",
       "std                         0.577                       0.565   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.997                      -0.997   \n",
       "50%                        -0.991                      -0.992   \n",
       "75%                        -0.976                      -0.977   \n",
       "max                        48.530                      47.385   \n",
       "\n",
       "       val_r2_keras_loss_epoch_47  val_r2_keras_loss_epoch_48  \\\n",
       "count                    8053.000                    7937.000   \n",
       "mean                       -0.962                      -0.964   \n",
       "std                         0.591                       0.526   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.997                      -0.997   \n",
       "50%                        -0.992                      -0.992   \n",
       "75%                        -0.978                      -0.979   \n",
       "max                        49.688                      43.781   \n",
       "\n",
       "       val_r2_keras_loss_epoch_49  val_r2_keras_loss_epoch_50  \\\n",
       "count                    7797.000                    7660.000   \n",
       "mean                       -0.971                      -0.971   \n",
       "std                         0.145                       0.148   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.997                      -0.997   \n",
       "50%                        -0.992                      -0.992   \n",
       "75%                        -0.979                      -0.980   \n",
       "max                         8.149                       8.848   \n",
       "\n",
       "       val_r2_keras_loss_epoch_51  val_r2_keras_loss_epoch_52  \\\n",
       "count                    7517.000                    7374.000   \n",
       "mean                       -0.972                      -0.973   \n",
       "std                         0.131                       0.128   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.997                      -0.997   \n",
       "50%                        -0.992                      -0.992   \n",
       "75%                        -0.980                      -0.980   \n",
       "max                         7.105                       6.841   \n",
       "\n",
       "       val_r2_keras_loss_epoch_53  val_r2_keras_loss_epoch_54  \\\n",
       "count                    7205.000                    7035.000   \n",
       "mean                       -0.973                      -0.974   \n",
       "std                         0.121                       0.114   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.997                      -0.997   \n",
       "50%                        -0.992                      -0.992   \n",
       "75%                        -0.980                      -0.981   \n",
       "max                         6.310                       5.746   \n",
       "\n",
       "       val_r2_keras_loss_epoch_55  val_r2_keras_loss_epoch_56  \\\n",
       "count                    6871.000                    6685.000   \n",
       "mean                       -0.974                      -0.974   \n",
       "std                         0.111                       0.109   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.997                      -0.997   \n",
       "50%                        -0.992                      -0.992   \n",
       "75%                        -0.981                      -0.981   \n",
       "max                         5.675                       5.508   \n",
       "\n",
       "       val_r2_keras_loss_epoch_57  val_r2_keras_loss_epoch_58  \\\n",
       "count                    6493.000                    6323.000   \n",
       "mean                       -0.975                      -0.975   \n",
       "std                         0.105                       0.097   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.992                      -0.993   \n",
       "75%                        -0.981                      -0.981   \n",
       "max                         4.648                       4.312   \n",
       "\n",
       "       val_r2_keras_loss_epoch_59  val_r2_keras_loss_epoch_60  \\\n",
       "count                    6142.000                    5943.000   \n",
       "mean                       -0.975                      -0.975   \n",
       "std                         0.095                       0.104   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.992                      -0.993   \n",
       "75%                        -0.981                      -0.981   \n",
       "max                         4.136                       5.090   \n",
       "\n",
       "       val_r2_keras_loss_epoch_61  val_r2_keras_loss_epoch_62  \\\n",
       "count                    5764.000                    5561.000   \n",
       "mean                       -0.976                      -0.976   \n",
       "std                         0.103                       0.076   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.993                      -0.993   \n",
       "75%                        -0.981                      -0.981   \n",
       "max                         5.808                       3.261   \n",
       "\n",
       "       val_r2_keras_loss_epoch_63  val_r2_keras_loss_epoch_64  \\\n",
       "count                    5382.000                    5198.000   \n",
       "mean                       -0.977                      -0.977   \n",
       "std                         0.083                       0.073   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.993                      -0.993   \n",
       "75%                        -0.981                      -0.981   \n",
       "max                         3.971                       2.888   \n",
       "\n",
       "       val_r2_keras_loss_epoch_65  val_r2_keras_loss_epoch_66  \\\n",
       "count                    4997.000                    4824.000   \n",
       "mean                       -0.978                      -0.978   \n",
       "std                         0.048                       0.048   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.993                      -0.993   \n",
       "75%                        -0.982                      -0.982   \n",
       "max                        -0.103                       0.011   \n",
       "\n",
       "       val_r2_keras_loss_epoch_67  val_r2_keras_loss_epoch_68  \\\n",
       "count                    4667.000                    4475.000   \n",
       "mean                       -0.979                      -0.979   \n",
       "std                         0.047                       0.047   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.994                      -0.994   \n",
       "75%                        -0.982                      -0.982   \n",
       "max                        -0.155                      -0.188   \n",
       "\n",
       "       val_r2_keras_loss_epoch_69  val_r2_keras_loss_epoch_70  \\\n",
       "count                    4298.000                    4128.000   \n",
       "mean                       -0.979                      -0.979   \n",
       "std                         0.047                       0.046   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.994                      -0.994   \n",
       "75%                        -0.982                      -0.982   \n",
       "max                        -0.226                      -0.239   \n",
       "\n",
       "       val_r2_keras_loss_epoch_71  val_r2_keras_loss_epoch_72  \\\n",
       "count                    3963.000                    3838.000   \n",
       "mean                       -0.979                      -0.980   \n",
       "std                         0.046                       0.045   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.994                      -0.994   \n",
       "75%                        -0.983                      -0.983   \n",
       "max                        -0.193                      -0.264   \n",
       "\n",
       "       val_r2_keras_loss_epoch_73  val_r2_keras_loss_epoch_74  \\\n",
       "count                    3685.000                    3538.000   \n",
       "mean                       -0.980                      -0.980   \n",
       "std                         0.045                       0.044   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.994                      -0.994   \n",
       "75%                        -0.984                      -0.984   \n",
       "max                        -0.303                      -0.334   \n",
       "\n",
       "       val_r2_keras_loss_epoch_75  val_r2_keras_loss_epoch_76  \\\n",
       "count                    3398.000                    3290.000   \n",
       "mean                       -0.980                      -0.981   \n",
       "std                         0.044                       0.043   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.995                      -0.995   \n",
       "75%                        -0.984                      -0.984   \n",
       "max                        -0.349                      -0.371   \n",
       "\n",
       "       val_r2_keras_loss_epoch_77  val_r2_keras_loss_epoch_78  \\\n",
       "count                    3154.000                    3032.000   \n",
       "mean                       -0.981                      -0.982   \n",
       "std                         0.043                       0.042   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.995                      -0.995   \n",
       "75%                        -0.984                      -0.985   \n",
       "max                        -0.349                      -0.398   \n",
       "\n",
       "       val_r2_keras_loss_epoch_79  val_r2_keras_loss_epoch_80  \\\n",
       "count                    2917.000                    2796.000   \n",
       "mean                       -0.982                      -0.983   \n",
       "std                         0.040                       0.039   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.995                      -0.995   \n",
       "75%                        -0.985                      -0.986   \n",
       "max                        -0.547                      -0.573   \n",
       "\n",
       "       val_r2_keras_loss_epoch_81  val_r2_keras_loss_epoch_82  \\\n",
       "count                    2671.000                    2586.000   \n",
       "mean                       -0.983                      -0.983   \n",
       "std                         0.039                       0.038   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.995                      -0.995   \n",
       "75%                        -0.986                      -0.987   \n",
       "max                        -0.592                      -0.583   \n",
       "\n",
       "       val_r2_keras_loss_epoch_83  val_r2_keras_loss_epoch_84  \\\n",
       "count                    2508.000                    2416.000   \n",
       "mean                       -0.984                      -0.984   \n",
       "std                         0.037                       0.037   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.995                      -0.995   \n",
       "75%                        -0.987                      -0.987   \n",
       "max                        -0.551                      -0.587   \n",
       "\n",
       "       val_r2_keras_loss_epoch_85  val_r2_keras_loss_epoch_86  \\\n",
       "count                    2323.000                    2232.000   \n",
       "mean                       -0.984                      -0.984   \n",
       "std                         0.036                       0.036   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.995                      -0.995   \n",
       "75%                        -0.987                      -0.987   \n",
       "max                        -0.578                      -0.527   \n",
       "\n",
       "       val_r2_keras_loss_epoch_87  val_r2_keras_loss_epoch_88  \\\n",
       "count                    2137.000                    2054.000   \n",
       "mean                       -0.985                      -0.986   \n",
       "std                         0.035                       0.034   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.996                      -0.996   \n",
       "75%                        -0.988                      -0.988   \n",
       "max                        -0.553                      -0.564   \n",
       "\n",
       "       val_r2_keras_loss_epoch_89  val_r2_keras_loss_epoch_90  \\\n",
       "count                    1991.000                    1925.000   \n",
       "mean                       -0.986                      -0.987   \n",
       "std                         0.032                       0.032   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.996                      -0.996   \n",
       "75%                        -0.988                      -0.989   \n",
       "max                        -0.603                      -0.589   \n",
       "\n",
       "       val_r2_keras_loss_epoch_91  val_r2_keras_loss_epoch_92  \\\n",
       "count                    1861.000                    1791.000   \n",
       "mean                       -0.987                      -0.987   \n",
       "std                         0.031                       0.031   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.996                      -0.996   \n",
       "75%                        -0.989                      -0.989   \n",
       "max                        -0.581                      -0.559   \n",
       "\n",
       "       val_r2_keras_loss_epoch_93  val_r2_keras_loss_epoch_94  \\\n",
       "count                    1723.000                    1672.000   \n",
       "mean                       -0.988                      -0.988   \n",
       "std                         0.030                       0.030   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.996                      -0.996   \n",
       "75%                        -0.989                      -0.990   \n",
       "max                        -0.583                      -0.539   \n",
       "\n",
       "       val_r2_keras_loss_epoch_95  val_r2_keras_loss_epoch_96  \\\n",
       "count                    1629.000                    1574.000   \n",
       "mean                       -0.988                      -0.988   \n",
       "std                         0.028                       0.028   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.996                      -0.996   \n",
       "75%                        -0.990                      -0.990   \n",
       "max                        -0.586                      -0.578   \n",
       "\n",
       "       val_r2_keras_loss_epoch_97  val_r2_keras_loss_epoch_98  \\\n",
       "count                    1522.000                    1474.000   \n",
       "mean                       -0.989                      -0.989   \n",
       "std                         0.028                       0.027   \n",
       "min                        -1.000                      -1.000   \n",
       "25%                        -0.998                      -0.998   \n",
       "50%                        -0.996                      -0.996   \n",
       "75%                        -0.990                      -0.991   \n",
       "max                        -0.612                      -0.594   \n",
       "\n",
       "       val_r2_keras_loss_epoch_99  val_r2_keras_loss_epoch_100  \\\n",
       "count                    1421.000                     1373.000   \n",
       "mean                       -0.990                       -0.990   \n",
       "std                         0.026                        0.025   \n",
       "min                        -1.000                       -1.000   \n",
       "25%                        -0.998                       -0.998   \n",
       "50%                        -0.996                       -0.996   \n",
       "75%                        -0.991                       -0.991   \n",
       "max                        -0.561                       -0.593   \n",
       "\n",
       "       val_r2_keras_loss_epoch_101  val_r2_keras_loss_epoch_102  \\\n",
       "count                     1333.000                     1286.000   \n",
       "mean                        -0.990                       -0.990   \n",
       "std                          0.025                        0.025   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.996                       -0.996   \n",
       "75%                         -0.991                       -0.992   \n",
       "max                         -0.561                       -0.553   \n",
       "\n",
       "       val_r2_keras_loss_epoch_103  val_r2_keras_loss_epoch_104  \\\n",
       "count                     1253.000                     1207.000   \n",
       "mean                        -0.991                       -0.991   \n",
       "std                          0.022                        0.022   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.992                       -0.992   \n",
       "max                         -0.565                       -0.600   \n",
       "\n",
       "       val_r2_keras_loss_epoch_105  val_r2_keras_loss_epoch_106  \\\n",
       "count                     1169.000                     1122.000   \n",
       "mean                        -0.991                       -0.991   \n",
       "std                          0.021                        0.020   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.992                       -0.993   \n",
       "max                         -0.594                       -0.603   \n",
       "\n",
       "       val_r2_keras_loss_epoch_107  val_r2_keras_loss_epoch_108  \\\n",
       "count                     1081.000                     1048.000   \n",
       "mean                        -0.992                       -0.992   \n",
       "std                          0.020                        0.020   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.993                       -0.993   \n",
       "max                         -0.588                       -0.618   \n",
       "\n",
       "       val_r2_keras_loss_epoch_109  val_r2_keras_loss_epoch_110  \\\n",
       "count                     1016.000                      986.000   \n",
       "mean                        -0.992                       -0.992   \n",
       "std                          0.020                        0.019   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.993                       -0.993   \n",
       "max                         -0.602                       -0.645   \n",
       "\n",
       "       val_r2_keras_loss_epoch_111  val_r2_keras_loss_epoch_112  \\\n",
       "count                      950.000                      921.000   \n",
       "mean                        -0.992                       -0.992   \n",
       "std                          0.019                        0.017   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.993                       -0.993   \n",
       "max                         -0.666                       -0.668   \n",
       "\n",
       "       val_r2_keras_loss_epoch_113  val_r2_keras_loss_epoch_114  \\\n",
       "count                      887.000                      858.000   \n",
       "mean                        -0.992                       -0.992   \n",
       "std                          0.017                        0.016   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.993                       -0.993   \n",
       "max                         -0.703                       -0.753   \n",
       "\n",
       "       val_r2_keras_loss_epoch_115  val_r2_keras_loss_epoch_116  \\\n",
       "count                      823.000                      794.000   \n",
       "mean                        -0.993                       -0.993   \n",
       "std                          0.015                        0.015   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.994                       -0.994   \n",
       "max                         -0.776                       -0.803   \n",
       "\n",
       "       val_r2_keras_loss_epoch_117  val_r2_keras_loss_epoch_118  \\\n",
       "count                      764.000                      728.000   \n",
       "mean                        -0.993                       -0.993   \n",
       "std                          0.014                        0.014   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.993                       -0.994   \n",
       "max                         -0.831                       -0.825   \n",
       "\n",
       "       val_r2_keras_loss_epoch_119  val_r2_keras_loss_epoch_120  \\\n",
       "count                      705.000                      677.000   \n",
       "mean                        -0.993                       -0.993   \n",
       "std                          0.014                        0.013   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.994                       -0.994   \n",
       "max                         -0.827                       -0.839   \n",
       "\n",
       "       val_r2_keras_loss_epoch_121  val_r2_keras_loss_epoch_122  \\\n",
       "count                      666.000                      641.000   \n",
       "mean                        -0.993                       -0.993   \n",
       "std                          0.012                        0.012   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.994                       -0.994   \n",
       "max                         -0.848                       -0.852   \n",
       "\n",
       "       val_r2_keras_loss_epoch_123  val_r2_keras_loss_epoch_124  \\\n",
       "count                      609.000                      582.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.012                        0.012   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.994                       -0.994   \n",
       "max                         -0.848                       -0.837   \n",
       "\n",
       "       val_r2_keras_loss_epoch_125  val_r2_keras_loss_epoch_126  \\\n",
       "count                      549.000                      519.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.012                        0.012   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.994                       -0.994   \n",
       "max                         -0.832                       -0.837   \n",
       "\n",
       "       val_r2_keras_loss_epoch_127  val_r2_keras_loss_epoch_128  \\\n",
       "count                      499.000                      484.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.012                        0.012   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.994                       -0.994   \n",
       "max                         -0.848                       -0.846   \n",
       "\n",
       "       val_r2_keras_loss_epoch_129  val_r2_keras_loss_epoch_130  \\\n",
       "count                      467.000                      445.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.011                        0.012   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.994                       -0.995   \n",
       "max                         -0.849                       -0.835   \n",
       "\n",
       "       val_r2_keras_loss_epoch_131  val_r2_keras_loss_epoch_132  \\\n",
       "count                      432.000                      412.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.011                        0.011   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.849                       -0.847   \n",
       "\n",
       "       val_r2_keras_loss_epoch_133  val_r2_keras_loss_epoch_134  \\\n",
       "count                      393.000                      381.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.012                        0.012   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.994                       -0.994   \n",
       "max                         -0.842                       -0.834   \n",
       "\n",
       "       val_r2_keras_loss_epoch_135  val_r2_keras_loss_epoch_136  \\\n",
       "count                      372.000                      358.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.012                        0.012   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.994                       -0.995   \n",
       "max                         -0.840                       -0.845   \n",
       "\n",
       "       val_r2_keras_loss_epoch_137  val_r2_keras_loss_epoch_138  \\\n",
       "count                      344.000                      334.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.012                        0.013   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.846                       -0.822   \n",
       "\n",
       "       val_r2_keras_loss_epoch_139  val_r2_keras_loss_epoch_140  \\\n",
       "count                      321.000                      311.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.012                        0.012   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.853                       -0.850   \n",
       "\n",
       "       val_r2_keras_loss_epoch_141  val_r2_keras_loss_epoch_142  \\\n",
       "count                      290.000                      280.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.012                        0.013   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.856                       -0.844   \n",
       "\n",
       "       val_r2_keras_loss_epoch_143  val_r2_keras_loss_epoch_144  \\\n",
       "count                      273.000                      260.000   \n",
       "mean                        -0.994                       -0.994   \n",
       "std                          0.013                        0.012   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.853                       -0.860   \n",
       "\n",
       "       val_r2_keras_loss_epoch_145  val_r2_keras_loss_epoch_146  \\\n",
       "count                      256.000                      241.000   \n",
       "mean                        -0.994                       -0.995   \n",
       "std                          0.013                        0.009   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.835                       -0.921   \n",
       "\n",
       "       val_r2_keras_loss_epoch_147  val_r2_keras_loss_epoch_148  \\\n",
       "count                      235.000                      227.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.009                        0.010   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.924                       -0.916   \n",
       "\n",
       "       val_r2_keras_loss_epoch_149  val_r2_keras_loss_epoch_150  \\\n",
       "count                      218.000                      207.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.009                        0.009   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.925                       -0.928   \n",
       "\n",
       "       val_r2_keras_loss_epoch_151  val_r2_keras_loss_epoch_152  \\\n",
       "count                      202.000                      192.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.009                        0.009   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.926                       -0.925   \n",
       "\n",
       "       val_r2_keras_loss_epoch_153  val_r2_keras_loss_epoch_154  \\\n",
       "count                      185.000                      176.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.009                        0.009   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.996   \n",
       "max                         -0.929                       -0.933   \n",
       "\n",
       "       val_r2_keras_loss_epoch_155  val_r2_keras_loss_epoch_156  \\\n",
       "count                      173.000                      164.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.009                        0.009   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.996   \n",
       "max                         -0.931                       -0.929   \n",
       "\n",
       "       val_r2_keras_loss_epoch_157  val_r2_keras_loss_epoch_158  \\\n",
       "count                      159.000                      150.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.009                        0.009   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.928                       -0.931   \n",
       "\n",
       "       val_r2_keras_loss_epoch_159  val_r2_keras_loss_epoch_160  \\\n",
       "count                      147.000                      143.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.009                        0.009   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.998   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.932                       -0.930   \n",
       "\n",
       "       val_r2_keras_loss_epoch_161  val_r2_keras_loss_epoch_162  \\\n",
       "count                      138.000                      131.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.010                        0.010   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.999   \n",
       "50%                         -0.998                       -0.997   \n",
       "75%                         -0.995                       -0.996   \n",
       "max                         -0.923                       -0.933   \n",
       "\n",
       "       val_r2_keras_loss_epoch_163  val_r2_keras_loss_epoch_164  \\\n",
       "count                      124.000                      119.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.009                        0.009   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.998   \n",
       "50%                         -0.998                       -0.997   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.937                       -0.934   \n",
       "\n",
       "       val_r2_keras_loss_epoch_165  val_r2_keras_loss_epoch_166  \\\n",
       "count                      115.000                      112.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.010                        0.010   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.997   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.931                       -0.936   \n",
       "\n",
       "       val_r2_keras_loss_epoch_167  val_r2_keras_loss_epoch_168  \\\n",
       "count                      106.000                      103.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.010                        0.009   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.937                       -0.938   \n",
       "\n",
       "       val_r2_keras_loss_epoch_169  val_r2_keras_loss_epoch_170  \\\n",
       "count                      101.000                       94.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.009                        0.010   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.999   \n",
       "50%                         -0.997                       -0.998   \n",
       "75%                         -0.995                       -0.996   \n",
       "max                         -0.936                       -0.930   \n",
       "\n",
       "       val_r2_keras_loss_epoch_171  val_r2_keras_loss_epoch_172  \\\n",
       "count                       91.000                       90.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.010                        0.010   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.997   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.932                       -0.936   \n",
       "\n",
       "       val_r2_keras_loss_epoch_173  val_r2_keras_loss_epoch_174  \\\n",
       "count                       86.000                       84.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.010                        0.010   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.998   \n",
       "50%                         -0.997                       -0.998   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.933                       -0.935   \n",
       "\n",
       "       val_r2_keras_loss_epoch_175  val_r2_keras_loss_epoch_176  \\\n",
       "count                       77.000                       73.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.010                        0.011   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.940                       -0.931   \n",
       "\n",
       "       val_r2_keras_loss_epoch_177  val_r2_keras_loss_epoch_178  \\\n",
       "count                       70.000                       69.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.011                        0.011   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.927                       -0.937   \n",
       "\n",
       "       val_r2_keras_loss_epoch_179  val_r2_keras_loss_epoch_180  \\\n",
       "count                       68.000                       66.000   \n",
       "mean                        -0.995                       -0.995   \n",
       "std                          0.010                        0.010   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.997   \n",
       "75%                         -0.996                       -0.995   \n",
       "max                         -0.938                       -0.935   \n",
       "\n",
       "       val_r2_keras_loss_epoch_181  val_r2_keras_loss_epoch_182  \\\n",
       "count                       63.000                       62.000   \n",
       "mean                        -0.994                       -0.995   \n",
       "std                          0.011                        0.011   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.995                       -0.995   \n",
       "max                         -0.930                       -0.938   \n",
       "\n",
       "       val_r2_keras_loss_epoch_183  val_r2_keras_loss_epoch_184  \\\n",
       "count                       55.000                       55.000   \n",
       "mean                        -0.995                       -0.996   \n",
       "std                          0.009                        0.008   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.945                       -0.953   \n",
       "\n",
       "       val_r2_keras_loss_epoch_185  val_r2_keras_loss_epoch_186  \\\n",
       "count                       53.000                       52.000   \n",
       "mean                        -0.996                       -0.996   \n",
       "std                          0.005                        0.005   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.966                       -0.964   \n",
       "\n",
       "       val_r2_keras_loss_epoch_187  val_r2_keras_loss_epoch_188  \\\n",
       "count                       50.000                       48.000   \n",
       "mean                        -0.996                       -0.997   \n",
       "std                          0.005                        0.002   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.967                       -0.990   \n",
       "\n",
       "       val_r2_keras_loss_epoch_189  val_r2_keras_loss_epoch_190  \\\n",
       "count                       47.000                       44.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.002   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.989                       -0.990   \n",
       "\n",
       "       val_r2_keras_loss_epoch_191  val_r2_keras_loss_epoch_192  \\\n",
       "count                       43.000                       41.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.003                        0.003   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.990                       -0.987   \n",
       "\n",
       "       val_r2_keras_loss_epoch_193  val_r2_keras_loss_epoch_194  \\\n",
       "count                       40.000                       39.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.002   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.990                       -0.990   \n",
       "\n",
       "       val_r2_keras_loss_epoch_195  val_r2_keras_loss_epoch_196  \\\n",
       "count                       38.000                       36.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.003   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.991                       -0.989   \n",
       "\n",
       "       val_r2_keras_loss_epoch_197  val_r2_keras_loss_epoch_198  \\\n",
       "count                       35.000                       34.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.003                        0.002   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.996                       -0.997   \n",
       "max                         -0.988                       -0.990   \n",
       "\n",
       "       val_r2_keras_loss_epoch_199  val_r2_keras_loss_epoch_200  \\\n",
       "count                       33.000                       32.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.003   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.997                       -0.998   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.991                       -0.990   \n",
       "\n",
       "       val_r2_keras_loss_epoch_201  val_r2_keras_loss_epoch_202  \\\n",
       "count                       31.000                       29.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.003                        0.003   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.990                       -0.990   \n",
       "\n",
       "       val_r2_keras_loss_epoch_203  val_r2_keras_loss_epoch_204  \\\n",
       "count                       26.000                       25.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.003   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.997                       -0.998   \n",
       "75%                         -0.997                       -0.996   \n",
       "max                         -0.991                       -0.991   \n",
       "\n",
       "       val_r2_keras_loss_epoch_205  val_r2_keras_loss_epoch_206  \\\n",
       "count                       25.000                       24.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.003                        0.003   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.997                       -0.998   \n",
       "75%                         -0.996                       -0.997   \n",
       "max                         -0.990                       -0.990   \n",
       "\n",
       "       val_r2_keras_loss_epoch_207  val_r2_keras_loss_epoch_208  \\\n",
       "count                       24.000                       24.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.003   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.991                       -0.987   \n",
       "\n",
       "       val_r2_keras_loss_epoch_209  val_r2_keras_loss_epoch_210  \\\n",
       "count                       23.000                       21.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.003                        0.003   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.998                       -0.997   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.991                       -0.991   \n",
       "\n",
       "       val_r2_keras_loss_epoch_211  val_r2_keras_loss_epoch_212  \\\n",
       "count                       21.000                       21.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.003                        0.003   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.991                       -0.991   \n",
       "\n",
       "       val_r2_keras_loss_epoch_213  val_r2_keras_loss_epoch_214  \\\n",
       "count                       19.000                       18.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.003                        0.002   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.999                       -0.999   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.996                       -0.997   \n",
       "max                         -0.991                       -0.992   \n",
       "\n",
       "       val_r2_keras_loss_epoch_215  val_r2_keras_loss_epoch_216  \\\n",
       "count                       16.000                       14.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.002   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.996                       -0.996   \n",
       "max                         -0.992                       -0.992   \n",
       "\n",
       "       val_r2_keras_loss_epoch_217  val_r2_keras_loss_epoch_218  \\\n",
       "count                       14.000                       13.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.002   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.996                       -0.997   \n",
       "max                         -0.992                       -0.992   \n",
       "\n",
       "       val_r2_keras_loss_epoch_219  val_r2_keras_loss_epoch_220  \\\n",
       "count                       13.000                       13.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.002   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.991                       -0.992   \n",
       "\n",
       "       val_r2_keras_loss_epoch_221  val_r2_keras_loss_epoch_222  \\\n",
       "count                       12.000                       11.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.002   \n",
       "min                         -1.000                       -0.999   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.996                       -0.997   \n",
       "max                         -0.992                       -0.992   \n",
       "\n",
       "       val_r2_keras_loss_epoch_223  val_r2_keras_loss_epoch_224  \\\n",
       "count                       10.000                       10.000   \n",
       "mean                        -0.997                       -0.997   \n",
       "std                          0.002                        0.003   \n",
       "min                         -1.000                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.992                       -0.990   \n",
       "\n",
       "       val_r2_keras_loss_epoch_225  val_r2_keras_loss_epoch_226  \\\n",
       "count                        9.000                        8.000   \n",
       "mean                        -0.997                       -0.998   \n",
       "std                          0.002                        0.001   \n",
       "min                         -0.999                       -1.000   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.997   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.992                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_227  val_r2_keras_loss_epoch_228  \\\n",
       "count                        7.000                        7.000   \n",
       "mean                        -0.998                       -0.997   \n",
       "std                          0.001                        0.001   \n",
       "min                         -0.999                       -0.999   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.997                       -0.995   \n",
       "\n",
       "       val_r2_keras_loss_epoch_229  val_r2_keras_loss_epoch_230  \\\n",
       "count                        7.000                        6.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.001                        0.001   \n",
       "min                         -0.999                       -0.999   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.997   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.997                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_231  val_r2_keras_loss_epoch_232  \\\n",
       "count                        5.000                        5.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.001                        0.001   \n",
       "min                         -0.999                       -0.999   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.997                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_233  val_r2_keras_loss_epoch_234  \\\n",
       "count                        5.000                        5.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.001                        0.001   \n",
       "min                         -0.999                       -0.999   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.997                       -0.998   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.997                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_235  val_r2_keras_loss_epoch_236  \\\n",
       "count                        5.000                        5.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.001                        0.001   \n",
       "min                         -0.999                       -0.999   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.997                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_237  val_r2_keras_loss_epoch_238  \\\n",
       "count                        4.000                        4.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.997                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_239  val_r2_keras_loss_epoch_240  \\\n",
       "count                        4.000                        4.000   \n",
       "mean                        -0.998                       -0.997   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.997                       -0.997   \n",
       "max                         -0.997                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_241  val_r2_keras_loss_epoch_242  \\\n",
       "count                        4.000                        4.000   \n",
       "mean                        -0.998                       -0.997   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.997   \n",
       "max                         -0.997                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_243  val_r2_keras_loss_epoch_244  \\\n",
       "count                        4.000                        4.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.997                       -0.998   \n",
       "max                         -0.997                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_245  val_r2_keras_loss_epoch_246  \\\n",
       "count                        4.000                        4.000   \n",
       "mean                        -0.997                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.997                       -0.998   \n",
       "max                         -0.997                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_247  val_r2_keras_loss_epoch_248  \\\n",
       "count                        4.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.997                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_249  val_r2_keras_loss_epoch_250  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_251  val_r2_keras_loss_epoch_252  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_253  val_r2_keras_loss_epoch_254  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_255  val_r2_keras_loss_epoch_256  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_257  val_r2_keras_loss_epoch_258  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_259  val_r2_keras_loss_epoch_260  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_261  val_r2_keras_loss_epoch_262  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_263  val_r2_keras_loss_epoch_264  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_265  val_r2_keras_loss_epoch_266  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.997   \n",
       "\n",
       "       val_r2_keras_loss_epoch_267  val_r2_keras_loss_epoch_268  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_269  val_r2_keras_loss_epoch_270  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_271  val_r2_keras_loss_epoch_272  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_273  val_r2_keras_loss_epoch_274  \\\n",
       "count                        3.000                        3.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                        0.000   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_275  val_r2_keras_loss_epoch_276  \\\n",
       "count                        2.000                        1.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                          0.000                          NaN   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_277  val_r2_keras_loss_epoch_278  \\\n",
       "count                        1.000                        1.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                            NaN                          NaN   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_279  val_r2_keras_loss_epoch_280  \\\n",
       "count                        1.000                        1.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                            NaN                          NaN   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_281  val_r2_keras_loss_epoch_282  \\\n",
       "count                        1.000                        1.000   \n",
       "mean                        -0.997                       -0.998   \n",
       "std                            NaN                          NaN   \n",
       "min                         -0.997                       -0.998   \n",
       "25%                         -0.997                       -0.998   \n",
       "50%                         -0.997                       -0.998   \n",
       "75%                         -0.997                       -0.998   \n",
       "max                         -0.997                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_283  val_r2_keras_loss_epoch_284  \\\n",
       "count                        1.000                        1.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                            NaN                          NaN   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_285  val_r2_keras_loss_epoch_286  \\\n",
       "count                        1.000                        1.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                            NaN                          NaN   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_287  val_r2_keras_loss_epoch_288  \\\n",
       "count                        1.000                        1.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                            NaN                          NaN   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_289  val_r2_keras_loss_epoch_290  \\\n",
       "count                        1.000                        1.000   \n",
       "mean                        -0.998                       -0.998   \n",
       "std                            NaN                          NaN   \n",
       "min                         -0.998                       -0.998   \n",
       "25%                         -0.998                       -0.998   \n",
       "50%                         -0.998                       -0.998   \n",
       "75%                         -0.998                       -0.998   \n",
       "max                         -0.998                       -0.998   \n",
       "\n",
       "       val_r2_keras_loss_epoch_291  val_r2_keras_loss_epoch_292  \n",
       "count                        1.000                        1.000  \n",
       "mean                        -0.998                       -0.998  \n",
       "std                            NaN                          NaN  \n",
       "min                         -0.998                       -0.998  \n",
       "25%                         -0.998                       -0.998  \n",
       "50%                         -0.998                       -0.998  \n",
       "75%                         -0.998                       -0.998  \n",
       "max                         -0.998                       -0.998  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.881Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T17:24:29.924807Z",
     "iopub.status.busy": "2021-09-24T17:24:29.924620Z",
     "iopub.status.idle": "2021-09-24T17:24:31.604271Z",
     "shell.execute_reply": "2021-09-24T17:24:31.603477Z",
     "shell.execute_reply.started": "2021-09-24T17:24:29.924768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + path_identifier_lambda_net_data + '/' + list(history.keys())[1] + '_epoch_' + str(epochs_lambda).zfill(3) + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 10#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model ' + list(history.keys())[1])\n",
    "plt.ylabel(list(history.keys())[1])\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.883Z"
    },
    "execution": {
     "iopub.execute_input": "2021-09-24T17:24:31.612044Z",
     "iopub.status.busy": "2021-09-24T17:24:31.610629Z",
     "iopub.status.idle": "2021-09-24T17:24:33.047503Z",
     "shell.execute_reply": "2021-09-24T17:24:33.046540Z",
     "shell.execute_reply.started": "2021-09-24T17:24:31.612003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + path_identifier_lambda_net_data + '/' + list(history.keys())[0] + '_epoch_' + str(epochs_lambda).zfill(3) + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True).iloc[:,1:]\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model ' + list(history.keys())[0])\n",
    "plt.ylabel(list(history.keys())[0])\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
