{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:29.564916Z",
     "iopub.status.busy": "2021-11-08T21:42:29.564375Z",
     "iopub.status.idle": "2021-11-08T21:42:29.594447Z",
     "shell.execute_reply": "2021-11-08T21:42:29.593737Z",
     "shell.execute_reply.started": "2021-11-08T21:42:29.564802Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 5, #degree\n",
    "        'n': 1, #number of variables\n",
    "        'monomial_vars': None, #int or None\n",
    "        'laurent': False, #use Laurent polynomials (negative degree with up to -d)  \n",
    "        'neg_d': 0,#int or None\n",
    "        'neg_d_prob': 0,\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': 3,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 1,\n",
    "        'a_min': -1,\n",
    "        'lambda_nets_total': 100,\n",
    "        'noise': 0,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'shift_polynomial': False,\n",
    "        \n",
    "        'border_min': 0.2, # defines an intervall. Value is randomly chosen and defines the minimum gap between x_min / x_max and the outermost stationary points => two values (left and right gap will be generated per variable)\n",
    "        'border_max': 0.4,\n",
    "        'lower_degree_prob': 0.5, # probability that the degree of the whole polynomial will be reduced\n",
    "        'a_random_prob': 0.5, # probability that a random generated function is used without adjustement\n",
    "                \n",
    "        'global_stationary_prob': 1, # probability that all variables are used for adjustement (0 recommended for higher number of variables)\n",
    "        'bulge_min': 1, # bulge_min and bulge_max define an intervall of how much the function is bulged\n",
    "        'bulge_max': 4,\n",
    "        'min_variables_used': 2, # defines an Intervall of how many variables are used to get stationary points and therefore adjust the function\n",
    "        'max_variables_used': 6,\n",
    "        'max_monomials': 7, # maximum number of monomials, before adjusting the function (monomial of degree 0 is always defined, but is included in this number)\n",
    "        'max_monomials_random': 10, #maximum number of monomials for random generated functions\n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True,  #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [128],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000,\n",
    "    },\n",
    "    'i_net': {\n",
    "        'optimizer': 'adam',#adam\n",
    "        'inet_loss': 'mae',\n",
    "        'inet_metrics': ['r2'],\n",
    "        'dropout': 0,\n",
    "        'dropout_output': 0,\n",
    "        'epochs': 2000, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "        'dense_layers': [4096, 2048, 1024, 512],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'interpretation_dataset_size': 100,\n",
    "                \n",
    "        'interpretation_net_output_monomials': 3, #(None, int) #CONSTANT IS NOT INCLUDED\n",
    "        'interpretation_net_output_shape': None, #calculated automatically later\n",
    "        'test_size': 100, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'normalize_inet_data': False,\n",
    "        'inet_training_without_noise': True, #dataset size without noise hardcoded to 50k in generate_paths\n",
    "        'sparse_poly_representation_version': 1, #(1, 2); 1=old, 2=new\n",
    "\n",
    "        'evaluate_with_real_function': False, #False\n",
    "        'consider_labels_training': False, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },\n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': True,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "        \n",
    "        'max_optimization_minutes': 60,\n",
    "        \n",
    "        #set if samples analysis should be performed\n",
    "        'samples_list': None,#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "       \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "        \n",
    "        'adjusted_symbolic_metamodeling_code': False,\n",
    "        'symbolic_metamodeling_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_evaluation': False,\n",
    "        'symbolic_metamodeling_function_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_function_evaluation': False,\n",
    "         \n",
    "        'symbolic_regression_evaluation': True,\n",
    "        'per_network_evaluation': False,\n",
    "    },\n",
    "    'computation':{\n",
    "        'train_model': False,\n",
    "        'n_jobs': 12,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:29.595963Z",
     "iopub.status.busy": "2021-11-08T21:42:29.595634Z",
     "iopub.status.idle": "2021-11-08T21:42:29.603581Z",
     "shell.execute_reply": "2021-11-08T21:42:29.602958Z",
     "shell.execute_reply.started": "2021-11-08T21:42:29.595929Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:29.605583Z",
     "iopub.status.busy": "2021-11-08T21:42:29.605206Z",
     "iopub.status.idle": "2021-11-08T21:42:34.242248Z",
     "shell.execute_reply": "2021-11-08T21:42:34.241472Z",
     "shell.execute_reply.started": "2021-11-08T21:42:29.605550Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('WARNING')\n",
    "tf.autograph.set_verbosity(2)\n",
    "\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random \n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:34.243892Z",
     "iopub.status.busy": "2021-11-08T21:42:34.243537Z",
     "iopub.status.idle": "2021-11-08T21:42:34.252067Z",
     "shell.execute_reply": "2021-11-08T21:42:34.251418Z",
     "shell.execute_reply.started": "2021-11-08T21:42:34.243861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:34.253277Z",
     "iopub.status.busy": "2021-11-08T21:42:34.253003Z",
     "iopub.status.idle": "2021-11-08T21:42:34.265493Z",
     "shell.execute_reply": "2021-11-08T21:42:34.264826Z",
     "shell.execute_reply.started": "2021-11-08T21:42:34.253255Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "n_jobs = min((epochs_lambda//each_epochs_save_lambda+1, n_jobs)) if multi_epoch_analysis else min(len(samples_list), n_jobs) if samples_list!=None else 1\n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "data_reshape_version = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if use_gpu else ''\n",
    "\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/local/cuda-10.1'\n",
    "\n",
    "#os.environ['XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda' if use_gpu else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2, --tf_xla_enable_xla_devices' if use_gpu else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:34.266847Z",
     "iopub.status.busy": "2021-11-08T21:42:34.266558Z",
     "iopub.status.idle": "2021-11-08T21:42:35.113004Z",
     "shell.execute_reply": "2021-11-08T21:42:35.112472Z",
     "shell.execute_reply.started": "2021-11-08T21:42:34.266816Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 6\n",
      "[[5], [4], [3], [2], [1], [0]]\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import flatten, rec_gen, gen_monomial_identifier_list\n",
    "\n",
    "list_of_monomial_identifiers_extended = []\n",
    "\n",
    "if laurent:\n",
    "    variable_sets = [list(flatten([[_d for _d in range(d+1)], [-_d for _d in range(1, neg_d+1)]])) for _ in range(n)]\n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)    \n",
    "        \n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity:' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)     \n",
    "        \n",
    "    list_of_monomial_identifiers = []\n",
    "    for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "        if np.sum(monomial_identifier) <= d:\n",
    "            if monomial_vars == None or len(list(filter(lambda x: x != 0, monomial_identifier))) <= monomial_vars:\n",
    "                list_of_monomial_identifiers.append(monomial_identifier)        \n",
    "else:\n",
    "    variable_list = ['x'+ str(i) for i in range(n)]\n",
    "    list_of_monomial_identifiers = gen_monomial_identifier_list(variable_list, d, n)\n",
    "            \n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "#print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "#print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:35.114489Z",
     "iopub.status.busy": "2021-11-08T21:42:35.114142Z",
     "iopub.status.idle": "2021-11-08T21:42:37.347785Z",
     "shell.execute_reply": "2021-11-08T21:42:37.346949Z",
     "shell.execute_reply.started": "2021-11-08T21:42:35.114452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape:  21\n"
     ]
    }
   ],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "config['i_net']['data_reshape_version'] = data_reshape_version\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d']) if not laurent else len(list_of_monomial_identifiers)\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "\n",
    "config['i_net']['interpretation_net_output_shape'] = config['data']['sparsity'] if config['i_net']['interpretation_net_output_monomials'] is None else config['data']['sparsity']*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials'] if config['i_net']['sparse_poly_representation_version'] == 1 else config['data']['n']*(config['data']['d']+1)*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials']  \n",
    "print('Output Shape: ', config['i_net']['interpretation_net_output_shape'])\n",
    "\n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sample_sparsity')*config['data']['sample_sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "\n",
    "\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='interpretation_net'))\n",
    "create_folders_inet()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:37.350334Z",
     "iopub.status.busy": "2021-11-08T21:42:37.349797Z",
     "iopub.status.idle": "2021-11-08T21:42:37.356401Z",
     "shell.execute_reply": "2021-11-08T21:42:37.355555Z",
     "shell.execute_reply.started": "2021-11-08T21:42:37.350266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inet_dense4096-2048-1024-512-output_21_drop0e2000b256_adam/lnets_100_128-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_1_d_5_negd_0_prob_0_spars_3_amin_-1_amax_1_xdist_uniform_noise_normal_0\n",
      "lnets_100_128-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_1_d_5_negd_0_prob_0_spars_3_amin_-1_amax_1_xdist_uniform_noise_normal_0\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net_data)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:37.358588Z",
     "iopub.status.busy": "2021-11-08T21:42:37.357938Z",
     "iopub.status.idle": "2021-11-08T21:42:37.365179Z",
     "shell.execute_reply": "2021-11-08T21:42:37.364523Z",
     "shell.execute_reply.started": "2021-11-08T21:42:37.358540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:37.366317Z",
     "iopub.status.busy": "2021-11-08T21:42:37.366053Z",
     "iopub.status.idle": "2021-11-08T21:42:37.374565Z",
     "shell.execute_reply": "2021-11-08T21:42:37.373775Z",
     "shell.execute_reply.started": "2021-11-08T21:42:37.366298Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index, no_noise=False):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 95:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path_identifier_lambda_net_data_loading = None \n",
    "                \n",
    "    if no_noise==True:\n",
    "        path_identifier_lambda_net_data_loading = generate_paths(path_type='interpretation_net_no_noise')['path_identifier_lambda_net_data']\n",
    "    else:\n",
    "        path_identifier_lambda_net_data_loading = path_identifier_lambda_net_data \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_identifier_lambda_net_data_loading + '/'\n",
    "    path_weights = directory + 'weights_epoch_' + str(index).zfill(3) + '.txt'\n",
    "    path_X_data = directory + 'lambda_X_test_data.txt'\n",
    "    path_y_data = directory + 'lambda_y_test_data.txt'        \n",
    "    \n",
    "    weight_data = pd.read_csv(path_weights, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        weight_data = weight_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_X_test_data = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_y_test_data = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "    lambda_nets = [None] * weight_data.shape[0]\n",
    "    for i, (row_weights, row_lambda_X_test_data, row_lambda_y_test_data) in enumerate(zip(weight_data.values, lambda_X_test_data.values, lambda_y_test_data.values)):        \n",
    "        lambda_net = LambdaNet(row_weights, row_lambda_X_test_data, row_lambda_y_test_data)\n",
    "        lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:42:37.376337Z",
     "iopub.status.busy": "2021-11-08T21:42:37.375915Z",
     "iopub.status.idle": "2021-11-08T21:44:14.609928Z",
     "shell.execute_reply": "2021-11-08T21:44:14.609174Z",
     "shell.execute_reply.started": "2021-11-08T21:42:37.376297Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend MultiprocessingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=12)]: Using backend MultiprocessingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 out of   1 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if inet_training_without_noise:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list_without_noise = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1, no_noise=True) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "else:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:30:49.711839Z",
     "start_time": "2021-01-05T09:29:48.873305Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:14.612806Z",
     "iopub.status.busy": "2021-11-08T21:44:14.612591Z",
     "iopub.status.idle": "2021-11-08T21:44:14.913059Z",
     "shell.execute_reply": "2021-11-08T21:44:14.912365Z",
     "shell.execute_reply.started": "2021-11-08T21:44:14.612785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>5-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>0-target</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "      <th>wb_176</th>\n",
       "      <th>wb_177</th>\n",
       "      <th>wb_178</th>\n",
       "      <th>wb_179</th>\n",
       "      <th>wb_180</th>\n",
       "      <th>wb_181</th>\n",
       "      <th>wb_182</th>\n",
       "      <th>wb_183</th>\n",
       "      <th>wb_184</th>\n",
       "      <th>wb_185</th>\n",
       "      <th>wb_186</th>\n",
       "      <th>wb_187</th>\n",
       "      <th>wb_188</th>\n",
       "      <th>wb_189</th>\n",
       "      <th>wb_190</th>\n",
       "      <th>wb_191</th>\n",
       "      <th>wb_192</th>\n",
       "      <th>wb_193</th>\n",
       "      <th>wb_194</th>\n",
       "      <th>wb_195</th>\n",
       "      <th>wb_196</th>\n",
       "      <th>wb_197</th>\n",
       "      <th>wb_198</th>\n",
       "      <th>wb_199</th>\n",
       "      <th>wb_200</th>\n",
       "      <th>wb_201</th>\n",
       "      <th>wb_202</th>\n",
       "      <th>wb_203</th>\n",
       "      <th>wb_204</th>\n",
       "      <th>wb_205</th>\n",
       "      <th>wb_206</th>\n",
       "      <th>wb_207</th>\n",
       "      <th>wb_208</th>\n",
       "      <th>wb_209</th>\n",
       "      <th>wb_210</th>\n",
       "      <th>wb_211</th>\n",
       "      <th>wb_212</th>\n",
       "      <th>wb_213</th>\n",
       "      <th>wb_214</th>\n",
       "      <th>wb_215</th>\n",
       "      <th>wb_216</th>\n",
       "      <th>wb_217</th>\n",
       "      <th>wb_218</th>\n",
       "      <th>wb_219</th>\n",
       "      <th>wb_220</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "      <th>wb_321</th>\n",
       "      <th>wb_322</th>\n",
       "      <th>wb_323</th>\n",
       "      <th>wb_324</th>\n",
       "      <th>wb_325</th>\n",
       "      <th>wb_326</th>\n",
       "      <th>wb_327</th>\n",
       "      <th>wb_328</th>\n",
       "      <th>wb_329</th>\n",
       "      <th>wb_330</th>\n",
       "      <th>wb_331</th>\n",
       "      <th>wb_332</th>\n",
       "      <th>wb_333</th>\n",
       "      <th>wb_334</th>\n",
       "      <th>wb_335</th>\n",
       "      <th>wb_336</th>\n",
       "      <th>wb_337</th>\n",
       "      <th>wb_338</th>\n",
       "      <th>wb_339</th>\n",
       "      <th>wb_340</th>\n",
       "      <th>wb_341</th>\n",
       "      <th>wb_342</th>\n",
       "      <th>wb_343</th>\n",
       "      <th>wb_344</th>\n",
       "      <th>wb_345</th>\n",
       "      <th>wb_346</th>\n",
       "      <th>wb_347</th>\n",
       "      <th>wb_348</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.188</td>\n",
       "      <td>1.418</td>\n",
       "      <td>-1.083</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.934</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.528</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.717</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.784</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.703</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>1.525</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.554</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.703</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.274</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.408</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-1.307</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.729</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.594</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.729</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.597</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.525</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.449</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.751</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          seed  5-target  4-target  3-target  2-target  1-target  0-target  \\\n",
       "83  1373158606     0.840     0.000     0.000    -0.545    -0.371     0.000   \n",
       "53  1373158606     0.000     0.703    -0.168     0.000    -0.533     0.000   \n",
       "70  1373158606     0.000    -0.843     0.000    -0.867    -0.083     0.000   \n",
       "45  1373158606     0.000     0.000     0.244    -0.140     0.000     0.356   \n",
       "44  1373158606     0.729    -0.355     0.000     0.000     0.000    -0.178   \n",
       "\n",
       "    5-lstsq_lambda  4-lstsq_lambda  3-lstsq_lambda  2-lstsq_lambda  \\\n",
       "83           0.188           1.418          -1.083          -0.201   \n",
       "53          -0.372           1.525          -0.801           0.198   \n",
       "70           0.202          -1.307           0.376          -0.995   \n",
       "45           0.018          -0.048           0.048          -0.021   \n",
       "44           0.594          -0.028          -0.291           0.114   \n",
       "\n",
       "    1-lstsq_lambda  0-lstsq_lambda  5-lstsq_target  4-lstsq_target  \\\n",
       "83          -0.412           0.001           0.840           0.000   \n",
       "53          -0.554           0.001           0.000           0.703   \n",
       "70          -0.067          -0.000          -0.000          -0.843   \n",
       "45           0.066           0.334          -0.000           0.000   \n",
       "44          -0.018          -0.178           0.729          -0.355   \n",
       "\n",
       "    3-lstsq_target  2-lstsq_target  1-lstsq_target  0-lstsq_target   wb_0  \\\n",
       "83          -0.000          -0.545          -0.371          -0.000 -0.005   \n",
       "53          -0.168          -0.000          -0.533          -0.000 -0.005   \n",
       "70          -0.000          -0.867          -0.083           0.000 -0.005   \n",
       "45           0.244          -0.140          -0.000           0.356 -0.005   \n",
       "44           0.000          -0.000           0.000          -0.178 -0.005   \n",
       "\n",
       "     wb_1  wb_2  wb_3  wb_4  wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  \\\n",
       "83 -0.111 0.176 0.123 0.253 0.248 0.153 -0.146 0.106 0.183  0.237 -0.043   \n",
       "53 -0.111 0.158 0.106 0.187 0.174 0.135 -0.146 0.090 0.160  0.218 -0.043   \n",
       "70 -0.111 0.272 0.228 0.046 0.035 0.276 -0.146 0.213 0.351  0.354 -0.043   \n",
       "45 -0.111 0.141 0.092 0.120 0.089 0.120 -0.146 0.077 0.118  0.199 -0.043   \n",
       "44 -0.111 0.110 0.058 0.191 0.181 0.087 -0.146 0.042 0.100  0.170 -0.043   \n",
       "\n",
       "    wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  \\\n",
       "83 -0.009  0.177  0.259  0.230  0.000  0.293  0.250  0.268  0.301  0.078   \n",
       "53 -0.009  0.158  0.183  0.211  0.004  0.218  0.202  0.208  0.199  0.064   \n",
       "70 -0.009  0.289  0.034  0.338 -0.000  0.180  0.068  0.120  0.012  0.192   \n",
       "45 -0.009  0.142  0.087  0.193  0.020  0.143  0.163  0.151  0.038  0.016   \n",
       "44 -0.009  0.110  0.189  0.163  0.136  0.206  0.200  0.203  0.188  0.016   \n",
       "\n",
       "    wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  \\\n",
       "83  0.048 -0.021 -0.198 -0.044  0.084 -0.143 -0.102 -0.114  0.232  0.140   \n",
       "53  0.029 -0.021 -0.198 -0.044  0.070 -0.143 -0.102 -0.114  0.179  0.233   \n",
       "70  0.237 -0.021 -0.198 -0.044  0.192 -0.143 -0.102 -0.114  0.045  0.202   \n",
       "45 -0.000 -0.021 -0.198 -0.044  0.021 -0.143 -0.102 -0.114  0.119  0.191   \n",
       "44 -0.019 -0.021 -0.198 -0.044  0.022 -0.143 -0.102 -0.114  0.185  0.217   \n",
       "\n",
       "    wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  \\\n",
       "83  0.241  0.197 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.262  0.100   \n",
       "53  0.184  0.178 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.179  0.258   \n",
       "70  0.046  0.302 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.033  0.321   \n",
       "45  0.128  0.161 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.084  0.113   \n",
       "44  0.187  0.131 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.189  0.242   \n",
       "\n",
       "    wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  \\\n",
       "83  0.222 -0.097  0.206  0.105  0.048  0.243  0.238 -0.198 -0.019  0.110   \n",
       "53  0.201 -0.097  0.186  0.089  0.017  0.224  0.209 -0.198 -0.019  0.075   \n",
       "70  0.356 -0.097  0.340  0.216  0.272  0.356  0.072 -0.198 -0.019  0.320   \n",
       "45  0.179 -0.097  0.167  0.076  0.006  0.205  0.180 -0.198 -0.019  0.072   \n",
       "44  0.139 -0.097  0.131  0.041 -0.025  0.176  0.200 -0.198 -0.019  0.067   \n",
       "\n",
       "    wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  \\\n",
       "83 -0.190 -0.043 -0.162  0.246  0.240 -0.202 -0.024 -0.117  0.108  0.146   \n",
       "53 -0.190 -0.043 -0.162  0.227  0.221 -0.202 -0.024 -0.117  0.092  0.230   \n",
       "70 -0.190 -0.043 -0.162  0.353  0.360 -0.202 -0.024 -0.117  0.256  0.139   \n",
       "45 -0.190 -0.043 -0.162  0.208  0.202 -0.202 -0.024 -0.117  0.079  0.191   \n",
       "44 -0.190 -0.043 -0.162  0.179  0.172 -0.202 -0.024 -0.117  0.043  0.216   \n",
       "\n",
       "    wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  \\\n",
       "83  0.192  0.258  0.182 -0.057 -0.156  0.174  0.066 -0.115  0.210 -0.213   \n",
       "53  0.173  0.170  0.164 -0.057 -0.156  0.250  0.051 -0.115  0.171 -0.213   \n",
       "70  0.292  0.026  0.275 -0.057 -0.156  0.365  0.187 -0.115  0.404 -0.213   \n",
       "45  0.156  0.072  0.147 -0.057 -0.156  0.194  0.008 -0.115  0.189 -0.213   \n",
       "44  0.125  0.178  0.116 -0.057 -0.156  0.223  0.003 -0.115  0.205 -0.213   \n",
       "\n",
       "    wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78  wb_79  wb_80  wb_81  \\\n",
       "83 -0.006  0.175  0.008  0.156  0.183 -0.192  0.155  0.238 -0.056  0.116   \n",
       "53 -0.006  0.264  0.198  0.131  0.274 -0.192  0.237  0.217 -0.056  0.098   \n",
       "70 -0.006  0.356  0.004  0.335  0.380 -0.192  0.172  0.385 -0.056  0.214   \n",
       "45 -0.006  0.199  0.024  0.095  0.197 -0.192  0.200  0.184 -0.056  0.084   \n",
       "44 -0.006  0.228  0.180  0.085  0.226 -0.192  0.226  0.116 -0.056  0.050   \n",
       "\n",
       "    wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  wb_91  \\\n",
       "83  0.230 -0.020  0.150 -0.085  0.294 -0.197 -0.067  0.140  0.124 -0.196   \n",
       "53  0.179 -0.020  0.133 -0.085  0.224 -0.197 -0.067  0.124  0.106 -0.196   \n",
       "70  0.045 -0.020  0.280 -0.085  0.209 -0.197 -0.067  0.273  0.220 -0.196   \n",
       "45  0.121 -0.020  0.118 -0.085  0.147 -0.197 -0.067  0.110  0.092 -0.196   \n",
       "44  0.184 -0.020  0.084 -0.085  0.207 -0.197 -0.067  0.075  0.058 -0.196   \n",
       "\n",
       "    wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  wb_98  wb_99  wb_100  wb_101  \\\n",
       "83 -0.107  0.056 -0.061 -0.192  0.099 -0.185  0.148 -0.125  -0.157   0.080   \n",
       "53 -0.107  0.039 -0.061 -0.192  0.083 -0.185  0.232 -0.125  -0.157   0.061   \n",
       "70 -0.107  0.218 -0.061 -0.192  0.206 -0.185  0.179 -0.125  -0.157   0.265   \n",
       "45 -0.107  0.003 -0.061 -0.192  0.071 -0.185  0.194 -0.125  -0.157   0.023   \n",
       "44 -0.107 -0.010 -0.061 -0.192  0.035 -0.185  0.220 -0.125  -0.157   0.012   \n",
       "\n",
       "    wb_102  wb_103  wb_104  wb_105  wb_106  wb_107  wb_108  wb_109  wb_110  \\\n",
       "83   0.062  -0.215   0.218  -0.067  -0.129   0.219  -0.011  -0.208  -0.211   \n",
       "53   0.047  -0.215   0.153  -0.067  -0.129   0.200  -0.011  -0.208  -0.211   \n",
       "70   0.192  -0.215   0.017  -0.067  -0.129   0.324  -0.011  -0.208  -0.211   \n",
       "45   0.006  -0.215   0.055  -0.067  -0.129   0.182  -0.011  -0.208  -0.211   \n",
       "44  -0.001  -0.215   0.152  -0.067  -0.129   0.152  -0.011  -0.208  -0.211   \n",
       "\n",
       "    wb_111  wb_112  wb_113  wb_114  wb_115  wb_116  wb_117  wb_118  wb_119  \\\n",
       "83  -0.203   0.068  -0.041   0.145  -0.144   0.071   0.229   0.035  -0.003   \n",
       "53  -0.203   0.250  -0.041   0.126  -0.144   0.048   0.214   0.018  -0.001   \n",
       "70  -0.203   0.300  -0.041   0.240  -0.144   0.273   0.102   0.192  -0.003   \n",
       "45  -0.203   0.076  -0.041   0.111  -0.144   0.015   0.184  -0.005   0.012   \n",
       "44  -0.203   0.228  -0.041   0.078  -0.144  -0.001   0.208  -0.031   0.162   \n",
       "\n",
       "    wb_120  wb_121  wb_122  wb_123  wb_124  wb_125  wb_126  wb_127  wb_128  \\\n",
       "83   0.109   0.173   0.228  -0.081   0.242  -0.023  -0.093   0.176   0.000   \n",
       "53   0.093   0.245   0.154  -0.081   0.223  -0.023  -0.093   0.145   0.000   \n",
       "70   0.235   0.135   0.022  -0.081   0.360  -0.023  -0.093   0.362   0.000   \n",
       "45   0.080   0.216   0.064  -0.081   0.204  -0.023  -0.093   0.133   0.000   \n",
       "44   0.045   0.242   0.159  -0.081   0.174  -0.023  -0.093   0.137   0.000   \n",
       "\n",
       "    wb_129  wb_130  wb_131  wb_132  wb_133  wb_134  wb_135  wb_136  wb_137  \\\n",
       "83   0.000  -0.000  -0.009  -0.190  -0.207  -0.014   0.000  -0.013  -0.031   \n",
       "53   0.000  -0.000  -0.000  -0.137  -0.135  -0.000   0.000  -0.000  -0.001   \n",
       "70   0.000  -0.103  -0.131  -0.052  -0.043  -0.143   0.000  -0.151  -0.194   \n",
       "45   0.000  -0.000  -0.000   0.068   0.068  -0.000   0.000  -0.000  -0.000   \n",
       "44   0.000   0.041   0.041  -0.127  -0.144   0.038   0.000   0.041  -0.003   \n",
       "\n",
       "    wb_138  wb_139  wb_140  wb_141  wb_142  wb_143  wb_144  wb_145  wb_146  \\\n",
       "83  -0.000   0.000   0.000  -0.009  -0.200  -0.000  -0.015  -0.250  -0.149   \n",
       "53  -0.000   0.000   0.000  -0.000  -0.156  -0.000  -0.013  -0.164  -0.126   \n",
       "70  -0.048   0.000   0.000  -0.124  -0.042  -0.028  -0.016  -0.165  -0.014   \n",
       "45  -0.002   0.000   0.000  -0.000   0.069  -0.002   0.069   0.070   0.068   \n",
       "44   0.038   0.000   0.000   0.038  -0.145   0.040  -0.123  -0.122  -0.088   \n",
       "\n",
       "    wb_147  wb_148  wb_149  wb_150  wb_151  wb_152  wb_153  wb_154  wb_155  \\\n",
       "83  -0.195  -0.277  -0.021  -0.015   0.000   0.000   0.000  -0.020   0.000   \n",
       "53  -0.147  -0.178  -0.000  -0.001   0.000   0.000   0.000  -0.000   0.000   \n",
       "70  -0.057  -0.024  -0.159  -0.217   0.000   0.000   0.000  -0.156   0.000   \n",
       "45   0.069   0.075  -0.028  -0.015   0.000   0.000   0.000  -0.029   0.000   \n",
       "44  -0.106  -0.165   0.041   0.041   0.000   0.000   0.000   0.041   0.000   \n",
       "\n",
       "    wb_156  wb_157  wb_158  wb_159  wb_160  wb_161  wb_162  wb_163  wb_164  \\\n",
       "83   0.000   0.000  -0.156  -0.031  -0.169  -0.001   0.000   0.000   0.000   \n",
       "53   0.000   0.000  -0.123  -0.122  -0.125  -0.000   0.000   0.000   0.000   \n",
       "70   0.000   0.000  -0.053  -0.094  -0.055  -0.106   0.000   0.000   0.000   \n",
       "45   0.000   0.000   0.067   0.069   0.068  -0.001   0.000   0.000   0.000   \n",
       "44   0.000   0.000  -0.117  -0.001  -0.136   0.039   0.000   0.000   0.000   \n",
       "\n",
       "    wb_165  wb_166  wb_167  wb_168  wb_169  wb_170  wb_171  wb_172  wb_173  \\\n",
       "83   0.000   0.000   0.000  -0.243  -0.032  -0.017   0.000  -0.019  -0.014   \n",
       "53   0.000   0.000   0.000  -0.161  -0.212  -0.001   0.000  -0.001  -0.000   \n",
       "70   0.000   0.000   0.000  -0.040  -0.258  -0.160   0.000  -0.161  -0.153   \n",
       "45   0.000   0.000   0.000   0.069   0.075  -0.001   0.000  -0.001  -0.000   \n",
       "44   0.000   0.000   0.000  -0.140  -0.202   0.015   0.000   0.023   0.041   \n",
       "\n",
       "    wb_174  wb_175  wb_176  wb_177  wb_178  wb_179  wb_180  wb_181  wb_182  \\\n",
       "83  -0.016  -0.000  -0.131   0.000   0.000  -0.030   0.000   0.000   0.000   \n",
       "53  -0.002  -0.000  -0.119   0.000   0.000  -0.002   0.000   0.000   0.000   \n",
       "70  -0.239  -0.015  -0.014   0.000   0.000  -0.249   0.000   0.000   0.000   \n",
       "45  -0.012  -0.002   0.067   0.000   0.000   0.083   0.000   0.000   0.000   \n",
       "44   0.042   0.039  -0.001   0.000   0.000  -0.002   0.000   0.000   0.000   \n",
       "\n",
       "    wb_183  wb_184  wb_185  wb_186  wb_187  wb_188  wb_189  wb_190  wb_191  \\\n",
       "83  -0.000  -0.000   0.000   0.000   0.000  -0.026  -0.022  -0.000  -0.228   \n",
       "53  -0.000  -0.000   0.000   0.000   0.000  -0.000  -0.109  -0.000  -0.153   \n",
       "70  -0.001  -0.065   0.000   0.000   0.000  -0.203  -0.024  -0.074  -0.037   \n",
       "45  -0.003  -0.002   0.000   0.000   0.000  -0.000   0.068  -0.001   0.069   \n",
       "44   0.041   0.037   0.000   0.000   0.000   0.036  -0.001   0.041  -0.152   \n",
       "\n",
       "    wb_192  wb_193  wb_194  wb_195  wb_196  wb_197  wb_198  wb_199  wb_200  \\\n",
       "83  -0.000   0.000   0.000  -0.039  -0.019   0.000  -0.037   0.000   0.000   \n",
       "53  -0.000   0.000   0.000  -0.139  -0.000   0.000  -0.002   0.000   0.000   \n",
       "70  -0.088   0.000   0.000  -0.267  -0.171   0.000  -0.240   0.000   0.000   \n",
       "45  -0.001   0.000   0.000   0.072  -0.023   0.000   0.075   0.000   0.000   \n",
       "44   0.041   0.000   0.000  -0.001   0.041   0.000  -0.002   0.000   0.000   \n",
       "\n",
       "    wb_201  wb_202  wb_203  wb_204  wb_205  wb_206  wb_207  wb_208  wb_209  \\\n",
       "83  -0.035  -0.017  -0.033  -0.040   0.000  -0.020  -0.028   0.000  -0.003   \n",
       "53  -0.090  -0.175  -0.001  -0.073   0.000  -0.104  -0.001   0.000   0.000   \n",
       "70  -0.261  -0.019  -0.220  -0.259   0.000  -0.024  -0.155   0.000  -0.131   \n",
       "45   0.071   0.077   0.037   0.072   0.000   0.068  -0.001   0.000  -0.000   \n",
       "44  -0.001  -0.165  -0.002  -0.002   0.000  -0.001  -0.022   0.000   0.041   \n",
       "\n",
       "    wb_210  wb_211  wb_212  wb_213  wb_214  wb_215  wb_216  wb_217  wb_218  \\\n",
       "83  -0.147   0.000  -0.025   0.000  -0.240   0.000   0.000  -0.026  -0.000   \n",
       "53  -0.117   0.000  -0.000   0.000  -0.169   0.000   0.000  -0.000   0.000   \n",
       "70  -0.054   0.000  -0.182   0.000  -0.193   0.000   0.000  -0.185  -0.119   \n",
       "45   0.067   0.000  -0.000   0.000   0.070   0.000   0.000  -0.000  -0.000   \n",
       "44  -0.128   0.000   0.035   0.000  -0.117   0.000   0.000   0.035   0.041   \n",
       "\n",
       "    wb_219  wb_220  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  \\\n",
       "83   0.000   0.000  -0.017   0.000   0.000  -0.015   0.000  -0.023   0.000   \n",
       "53   0.000   0.000  -0.000   0.000   0.000  -0.000   0.000  -0.116   0.000   \n",
       "70   0.000   0.000  -0.199   0.000   0.000  -0.157   0.000  -0.029   0.000   \n",
       "45   0.000   0.000  -0.019   0.000   0.000   0.000   0.000   0.069   0.000   \n",
       "44   0.000   0.000   0.041   0.000   0.000   0.041   0.000  -0.001   0.000   \n",
       "\n",
       "    wb_228  wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  \\\n",
       "83   0.000  -0.024  -0.019   0.000  -0.175   0.000   0.000  -0.001   0.000   \n",
       "53   0.000  -0.001  -0.000   0.000  -0.123   0.000   0.000  -0.000   0.000   \n",
       "70   0.000  -0.227  -0.166   0.000  -0.031   0.000   0.000  -0.093   0.000   \n",
       "45   0.000  -0.024  -0.022   0.000   0.068   0.000   0.000  -0.001   0.000   \n",
       "44   0.000   0.033   0.041   0.000  -0.140   0.000   0.000   0.039   0.000   \n",
       "\n",
       "    wb_237  wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  \\\n",
       "83   0.000   0.000   0.000  -0.023   0.000  -0.001   0.000  -0.022  -0.113   \n",
       "53   0.000   0.000   0.000  -0.200   0.000  -0.000   0.000  -0.001  -0.128   \n",
       "70   0.000   0.000   0.000  -0.254   0.000  -0.119   0.000  -0.242  -0.019   \n",
       "45   0.000   0.000   0.000   0.078   0.000  -0.000   0.000  -0.021   0.068   \n",
       "44   0.000   0.000   0.000  -0.185   0.000   0.041   0.000   0.033  -0.000   \n",
       "\n",
       "    wb_246  wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  \\\n",
       "83  -0.011  -0.012  -0.023  -0.013  -0.199   0.000  -0.001   0.000   0.000   \n",
       "53  -0.000  -0.011  -0.000  -0.095  -0.131   0.000  -0.000   0.000   0.000   \n",
       "70  -0.176  -0.012  -0.180  -0.022  -0.034   0.000  -0.078   0.000   0.000   \n",
       "45  -0.006   0.073  -0.000   0.067   0.068   0.000  -0.002   0.000   0.000   \n",
       "44   0.043  -0.143   0.039  -0.000  -0.134   0.000   0.037   0.000   0.000   \n",
       "\n",
       "    wb_255  wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  \\\n",
       "83  -0.035  -0.125  -0.176  -0.174  -0.166   0.539   0.689  -0.094  -0.200   \n",
       "53  -0.002  -0.125  -0.176  -0.159  -0.152   0.340   0.408  -0.079  -0.200   \n",
       "70  -0.226  -0.125  -0.176  -0.325  -0.385   0.053   0.084  -0.295  -0.200   \n",
       "45   0.071  -0.125  -0.176  -0.145  -0.144   0.135   0.153  -0.067  -0.200   \n",
       "44  -0.002  -0.125  -0.176  -0.126  -0.128   0.360   0.473  -0.049  -0.200   \n",
       "\n",
       "    wb_264  wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  \\\n",
       "83  -0.176  -0.044  -0.111  -0.173  -0.178  -0.104   0.649  -0.148   0.189   \n",
       "53  -0.162  -0.026  -0.095  -0.173  -0.178  -0.089   0.399  -0.133   0.191   \n",
       "70  -0.438  -0.257  -0.226  -0.173  -0.178  -0.273   0.052  -0.255   0.189   \n",
       "45  -0.155  -0.009  -0.079  -0.173  -0.178  -0.076   0.122  -0.116   0.261   \n",
       "44  -0.143   0.002  -0.058  -0.173  -0.178  -0.056   0.444  -0.096   0.814   \n",
       "\n",
       "    wb_273  wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  \\\n",
       "83   0.476   0.398   0.419   0.934  -0.190  -0.039  -0.128  -0.188  -0.029   \n",
       "53   0.270   0.294   0.265   0.508  -0.177  -0.027  -0.128  -0.188  -0.029   \n",
       "70  -0.174   0.012  -0.067   0.019  -0.509  -0.501  -0.128  -0.188  -0.029   \n",
       "45   0.075   0.150   0.093   0.088  -0.147  -0.018  -0.128  -0.188  -0.029   \n",
       "44   0.249   0.245   0.230   0.597  -0.170  -0.058  -0.128  -0.188  -0.029   \n",
       "\n",
       "    wb_282  wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  \\\n",
       "83  -0.206   0.071  -0.126  -0.154   0.562   0.005   0.528  -0.125   0.193   \n",
       "53  -0.192   0.071  -0.126  -0.154   0.402   0.183   0.357  -0.110   0.193   \n",
       "70  -0.507   0.071  -0.126  -0.154   0.128  -0.109   0.090  -0.271   0.193   \n",
       "45  -0.159   0.071  -0.126  -0.154   0.206   0.071   0.175  -0.095   0.193   \n",
       "44  -0.182   0.071  -0.126  -0.154   0.417   0.075   0.403  -0.075   0.193   \n",
       "\n",
       "    wb_291  wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  \\\n",
       "83  -0.080  -0.141  -0.210   0.164  -0.062   0.717  -0.016  -0.058   0.007   \n",
       "53  -0.080  -0.141  -0.210   0.164  -0.062   0.405   0.315  -0.042   0.007   \n",
       "70  -0.080  -0.141  -0.210   0.164  -0.062   0.050  -0.339  -0.227   0.007   \n",
       "45  -0.080  -0.141  -0.210   0.164  -0.062   0.119   0.044  -0.025   0.007   \n",
       "44  -0.080  -0.141  -0.210   0.164  -0.062   0.437   0.392  -0.005   0.007   \n",
       "\n",
       "    wb_300  wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  \\\n",
       "83  -0.061  -0.160  -0.016  -0.128   0.384  -0.033   0.024  -0.029   0.114   \n",
       "53  -0.045  -0.146  -0.002  -0.113   0.322  -0.033   0.024  -0.010   0.114   \n",
       "70  -0.239  -0.426  -0.479  -0.234   0.059  -0.033   0.024  -0.358   0.114   \n",
       "45  -0.029  -0.139   0.008  -0.096   0.197  -0.033   0.024   0.037   0.114   \n",
       "44  -0.009  -0.128  -0.033  -0.075   0.197  -0.033   0.024   0.006   0.114   \n",
       "\n",
       "    wb_309  wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  \\\n",
       "83   0.054   0.120  -0.205  -0.097  -0.214   0.102   0.137  -0.071   0.037   \n",
       "53   0.054   0.120  -0.189  -0.082  -0.214   0.102   0.137  -0.054   0.210   \n",
       "70   0.054   0.120  -0.311  -0.217  -0.214   0.102   0.137  -0.368  -0.030   \n",
       "45   0.054   0.120  -0.172  -0.065  -0.214   0.102   0.137  -0.046   0.105   \n",
       "44   0.054   0.120  -0.151  -0.044  -0.214   0.102   0.137  -0.034   0.109   \n",
       "\n",
       "    wb_318  wb_319  wb_320  wb_321  wb_322  wb_323  wb_324  wb_325  wb_326  \\\n",
       "83  -0.172   0.784  -0.201  -0.045   0.133  -0.019  -0.168   0.111  -0.033   \n",
       "53  -0.157   0.450  -0.186  -0.045   0.133   0.153  -0.155   0.111  -0.012   \n",
       "70  -0.301   0.076  -0.338  -0.045   0.133  -0.260  -0.527   0.111  -0.255   \n",
       "45  -0.143   0.142  -0.172  -0.045   0.133   0.038  -0.132   0.111   0.025   \n",
       "44  -0.123   0.525  -0.153  -0.045   0.133   0.043  -0.160   0.111   0.030   \n",
       "\n",
       "    wb_327  wb_328  wb_329  wb_330  wb_331  wb_332  wb_333  wb_334  wb_335  \\\n",
       "83   0.070  -0.070  -0.017   0.020  -0.038  -0.022   0.111   0.023  -0.052   \n",
       "53   0.070  -0.070   0.125   0.557  -0.021   0.116   0.111   0.182  -0.035   \n",
       "70   0.070  -0.070  -0.250   0.019  -0.288  -0.257   0.111  -0.048  -0.214   \n",
       "45   0.070  -0.070   0.040   0.091   0.009   0.035   0.111   0.089  -0.017   \n",
       "44   0.070  -0.070   0.046   0.656   0.003   0.041   0.111   0.095  -0.001   \n",
       "\n",
       "    wb_336  wb_337  wb_338  wb_339  wb_340  wb_341  wb_342  wb_343  wb_344  \\\n",
       "83   0.069  -0.211   0.561   0.190  -0.076   0.196   0.442  -0.058   0.102   \n",
       "53   0.069  -0.198   0.412   0.190  -0.059   0.196   0.261  -0.058   0.102   \n",
       "70   0.069  -0.439   0.148   0.190  -0.305   0.196  -0.200  -0.058   0.102   \n",
       "45   0.069  -0.190   0.226   0.190  -0.048   0.196   0.068  -0.058   0.102   \n",
       "44   0.069  -0.176   0.456   0.190  -0.029   0.196   0.225  -0.058   0.102   \n",
       "\n",
       "    wb_345  wb_346  wb_347  wb_348  wb_349  wb_350  wb_351  wb_352  wb_353  \\\n",
       "83  -0.075  -0.228  -0.182  -0.190  -0.071   0.051  -0.189  -0.177  -0.015   \n",
       "53  -0.059  -0.214  -0.182  -0.190  -0.058   0.051  -0.189  -0.163  -0.015   \n",
       "70  -0.316  -0.433  -0.182  -0.190  -0.491   0.051  -0.189  -0.454  -0.015   \n",
       "45  -0.048  -0.206  -0.182  -0.190  -0.042   0.051  -0.189  -0.155  -0.015   \n",
       "44  -0.031  -0.190  -0.182  -0.190  -0.076   0.051  -0.189  -0.146  -0.015   \n",
       "\n",
       "    wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  wb_361  wb_362  \\\n",
       "83   0.017   0.199  -0.113  -0.043  -0.154  -0.196   0.903   0.081  -0.152   \n",
       "53   0.187   0.199  -0.113  -0.028  -0.141  -0.196   0.580   0.081  -0.152   \n",
       "70  -0.058   0.199  -0.113  -0.410  -0.526  -0.196   0.191   0.081  -0.152   \n",
       "45   0.083   0.199  -0.113   0.003  -0.121  -0.196   0.256   0.081  -0.152   \n",
       "44   0.089   0.199  -0.113  -0.020  -0.151  -0.196   0.670   0.081  -0.152   \n",
       "\n",
       "    wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  wb_370  wb_371  \\\n",
       "83  -0.116  -0.178   0.094  -0.048   0.069  -0.011  -0.115  -0.184  -0.056   \n",
       "53  -0.101  -0.178   0.094  -0.048   0.069   0.385  -0.115  -0.170  -0.056   \n",
       "70  -0.247  -0.178   0.094  -0.048   0.069  -0.399  -0.115  -0.369  -0.056   \n",
       "45  -0.085  -0.178   0.094  -0.048   0.069   0.049  -0.115  -0.159  -0.056   \n",
       "44  -0.064  -0.178   0.094  -0.048   0.069   0.449  -0.115  -0.141  -0.056   \n",
       "\n",
       "    wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  wb_379  wb_380  \\\n",
       "83  -0.033   0.266  -0.082   0.078  -0.106   0.063   0.888   0.215  -0.093   \n",
       "53  -0.018   0.255  -0.073   0.079  -0.090   0.204   0.551   0.215  -0.078   \n",
       "70  -0.432  -0.004  -0.591   0.078  -0.385  -0.006   0.177   0.215  -0.215   \n",
       "45   0.009   0.129  -0.087   0.153  -0.082   0.128   0.243   0.215  -0.061   \n",
       "44  -0.019   0.133  -0.139   0.751  -0.069   0.135   0.616   0.215  -0.040   \n",
       "\n",
       "    wb_381  wb_382  wb_383  wb_384  \n",
       "83  -0.131  -0.124  -0.034  -0.000  \n",
       "53  -0.131  -0.124  -0.015  -0.001  \n",
       "70  -0.131  -0.124  -0.270   0.000  \n",
       "45  -0.131  -0.124   0.024   0.066  \n",
       "44  -0.131  -0.124   0.013  -0.043  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:31:56.898548Z",
     "start_time": "2021-01-05T09:30:49.715497Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:14.914342Z",
     "iopub.status.busy": "2021-11-08T21:44:14.914017Z",
     "iopub.status.idle": "2021-11-08T21:44:15.661019Z",
     "shell.execute_reply": "2021-11-08T21:44:15.660327Z",
     "shell.execute_reply.started": "2021-11-08T21:44:14.914299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>5-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>0-target</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "      <th>wb_176</th>\n",
       "      <th>wb_177</th>\n",
       "      <th>wb_178</th>\n",
       "      <th>wb_179</th>\n",
       "      <th>wb_180</th>\n",
       "      <th>wb_181</th>\n",
       "      <th>wb_182</th>\n",
       "      <th>wb_183</th>\n",
       "      <th>wb_184</th>\n",
       "      <th>wb_185</th>\n",
       "      <th>wb_186</th>\n",
       "      <th>wb_187</th>\n",
       "      <th>wb_188</th>\n",
       "      <th>wb_189</th>\n",
       "      <th>wb_190</th>\n",
       "      <th>wb_191</th>\n",
       "      <th>wb_192</th>\n",
       "      <th>wb_193</th>\n",
       "      <th>wb_194</th>\n",
       "      <th>wb_195</th>\n",
       "      <th>wb_196</th>\n",
       "      <th>wb_197</th>\n",
       "      <th>wb_198</th>\n",
       "      <th>wb_199</th>\n",
       "      <th>wb_200</th>\n",
       "      <th>wb_201</th>\n",
       "      <th>wb_202</th>\n",
       "      <th>wb_203</th>\n",
       "      <th>wb_204</th>\n",
       "      <th>wb_205</th>\n",
       "      <th>wb_206</th>\n",
       "      <th>wb_207</th>\n",
       "      <th>wb_208</th>\n",
       "      <th>wb_209</th>\n",
       "      <th>wb_210</th>\n",
       "      <th>wb_211</th>\n",
       "      <th>wb_212</th>\n",
       "      <th>wb_213</th>\n",
       "      <th>wb_214</th>\n",
       "      <th>wb_215</th>\n",
       "      <th>wb_216</th>\n",
       "      <th>wb_217</th>\n",
       "      <th>wb_218</th>\n",
       "      <th>wb_219</th>\n",
       "      <th>wb_220</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "      <th>wb_321</th>\n",
       "      <th>wb_322</th>\n",
       "      <th>wb_323</th>\n",
       "      <th>wb_324</th>\n",
       "      <th>wb_325</th>\n",
       "      <th>wb_326</th>\n",
       "      <th>wb_327</th>\n",
       "      <th>wb_328</th>\n",
       "      <th>wb_329</th>\n",
       "      <th>wb_330</th>\n",
       "      <th>wb_331</th>\n",
       "      <th>wb_332</th>\n",
       "      <th>wb_333</th>\n",
       "      <th>wb_334</th>\n",
       "      <th>wb_335</th>\n",
       "      <th>wb_336</th>\n",
       "      <th>wb_337</th>\n",
       "      <th>wb_338</th>\n",
       "      <th>wb_339</th>\n",
       "      <th>wb_340</th>\n",
       "      <th>wb_341</th>\n",
       "      <th>wb_342</th>\n",
       "      <th>wb_343</th>\n",
       "      <th>wb_344</th>\n",
       "      <th>wb_345</th>\n",
       "      <th>wb_346</th>\n",
       "      <th>wb_347</th>\n",
       "      <th>wb_348</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.817</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.509</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-7.512</td>\n",
       "      <td>-2.265</td>\n",
       "      <td>-12.291</td>\n",
       "      <td>-1.464</td>\n",
       "      <td>-1.025</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-1.616</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.693</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-1.112</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-1.045</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>-0.585</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-1.376</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-1.015</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.706</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-1.368</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-1.040</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.631</td>\n",
       "      <td>-0.708</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>-0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.726</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.251</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.901</td>\n",
       "      <td>17.036</td>\n",
       "      <td>3.406</td>\n",
       "      <td>3.458</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.981</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.593</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.426</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.586</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.628</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.815</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.676</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.247</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.109</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.934</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.383</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.636</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.492</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.435</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.784</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.447</td>\n",
       "      <td>1.361</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.414</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.552</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>1.227</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>1.341</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.675</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>1.218</td>\n",
       "      <td>0.446</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                seed  5-target  4-target  3-target  2-target  1-target  \\\n",
       "count        100.000   100.000   100.000   100.000   100.000   100.000   \n",
       "mean  1373158606.000    -0.006     0.030     0.033     0.001    -0.003   \n",
       "std            0.000     0.387     0.402     0.445     0.419     0.435   \n",
       "min   1373158606.000    -0.911    -0.963    -0.994    -0.979    -0.999   \n",
       "25%   1373158606.000     0.000     0.000    -0.017    -0.056     0.000   \n",
       "50%   1373158606.000     0.000     0.000     0.000     0.000     0.000   \n",
       "75%   1373158606.000     0.000     0.000     0.338     0.019     0.009   \n",
       "max   1373158606.000     0.952     0.995     0.951     0.980     0.966   \n",
       "\n",
       "       0-target  5-lstsq_lambda  4-lstsq_lambda  3-lstsq_lambda  \\\n",
       "count   100.000         100.000         100.000         100.000   \n",
       "mean     -0.048          -0.094           0.234          -0.118   \n",
       "std       0.344           0.817           2.009           1.509   \n",
       "min      -0.975          -7.512          -2.265         -12.291   \n",
       "25%      -0.001          -0.253          -0.726          -0.586   \n",
       "50%       0.000          -0.005           0.012          -0.063   \n",
       "75%       0.000           0.191           0.824           0.394   \n",
       "max       0.981           0.901          17.036           3.406   \n",
       "\n",
       "       2-lstsq_lambda  1-lstsq_lambda  0-lstsq_lambda  5-lstsq_target  \\\n",
       "count         100.000         100.000         100.000         100.000   \n",
       "mean            0.059          -0.023          -0.045          -0.006   \n",
       "std             0.557           0.416           0.338           0.387   \n",
       "min            -1.464          -1.025          -0.973          -0.911   \n",
       "25%            -0.205          -0.067          -0.003          -0.000   \n",
       "50%             0.047          -0.001           0.000           0.000   \n",
       "75%             0.223           0.092           0.001           0.000   \n",
       "max             3.458           0.881           0.965           0.952   \n",
       "\n",
       "       4-lstsq_target  3-lstsq_target  2-lstsq_target  1-lstsq_target  \\\n",
       "count         100.000         100.000         100.000         100.000   \n",
       "mean            0.030           0.033           0.001          -0.003   \n",
       "std             0.402           0.445           0.419           0.435   \n",
       "min            -0.963          -0.994          -0.979          -0.999   \n",
       "25%            -0.000          -0.018          -0.056          -0.000   \n",
       "50%             0.000           0.000           0.000          -0.000   \n",
       "75%             0.000           0.338           0.019           0.009   \n",
       "max             0.995           0.951           0.980           0.966   \n",
       "\n",
       "       0-lstsq_target    wb_0    wb_1    wb_2    wb_3    wb_4    wb_5    wb_6  \\\n",
       "count         100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean           -0.048  -0.005  -0.111   0.143   0.100   0.144   0.119   0.133   \n",
       "std             0.344   0.000   0.000   0.060   0.063   0.072   0.076   0.068   \n",
       "min            -0.975  -0.005  -0.111   0.039  -0.004  -0.017  -0.049   0.026   \n",
       "25%            -0.001  -0.005  -0.111   0.098   0.039   0.088   0.055   0.079   \n",
       "50%            -0.000  -0.005  -0.111   0.141   0.088   0.141   0.118   0.122   \n",
       "75%             0.000  -0.005  -0.111   0.184   0.152   0.193   0.175   0.178   \n",
       "max             0.981  -0.005  -0.111   0.274   0.228   0.317   0.299   0.301   \n",
       "\n",
       "         wb_7    wb_8    wb_9   wb_10   wb_11   wb_12   wb_13   wb_14   wb_15  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.146   0.087   0.200   0.208  -0.043  -0.009   0.150   0.120   0.195   \n",
       "std     0.000   0.065   0.094   0.056   0.000   0.000   0.060   0.080   0.059   \n",
       "min    -0.146  -0.020   0.024   0.116  -0.043  -0.009   0.052  -0.055   0.080   \n",
       "25%    -0.146   0.030   0.128   0.161  -0.043  -0.009   0.104   0.053   0.152   \n",
       "50%    -0.146   0.073   0.182   0.200  -0.043  -0.009   0.142   0.117   0.193   \n",
       "75%    -0.146   0.145   0.260   0.244  -0.043  -0.009   0.185   0.174   0.235   \n",
       "max    -0.146   0.214   0.530   0.354  -0.043  -0.009   0.289   0.315   0.346   \n",
       "\n",
       "        wb_16   wb_17   wb_18   wb_19   wb_20   wb_21   wb_22   wb_23   wb_24  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.052   0.196   0.180   0.190   0.081   0.058   0.028  -0.021  -0.198   \n",
       "std     0.079   0.115   0.068   0.095   0.096   0.064   0.091   0.000   0.000   \n",
       "min    -0.115  -0.001   0.031   0.013  -0.113  -0.047  -0.393  -0.021  -0.198   \n",
       "25%    -0.001   0.128   0.130   0.123   0.012   0.015  -0.001  -0.021  -0.198   \n",
       "50%     0.017   0.178   0.183   0.179   0.039   0.027  -0.000  -0.021  -0.198   \n",
       "75%     0.101   0.233   0.223   0.228   0.143   0.105   0.054  -0.021  -0.198   \n",
       "max     0.292   0.791   0.342   0.705   0.311   0.196   0.250  -0.021  -0.198   \n",
       "\n",
       "        wb_25   wb_26   wb_27   wb_28   wb_29   wb_30   wb_31   wb_32   wb_33  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.044   0.068  -0.143  -0.102  -0.114   0.139   0.233   0.149   0.166   \n",
       "std     0.000   0.066   0.000   0.000   0.000   0.069   0.103   0.070   0.059   \n",
       "min    -0.044  -0.040  -0.143  -0.102  -0.114  -0.014   0.055  -0.005   0.071   \n",
       "25%    -0.044   0.019  -0.143  -0.102  -0.114   0.084   0.175   0.095   0.120   \n",
       "50%    -0.044   0.047  -0.143  -0.102  -0.114   0.138   0.219   0.150   0.162   \n",
       "75%    -0.044   0.126  -0.143  -0.102  -0.114   0.189   0.265   0.199   0.207   \n",
       "max    -0.044   0.254  -0.143  -0.102  -0.114   0.301   0.803   0.313   0.302   \n",
       "\n",
       "        wb_34   wb_35   wb_36   wb_37   wb_38   wb_39   wb_40   wb_41   wb_42  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.040  -0.168  -0.149  -0.008  -0.108  -0.129   0.118   0.168   0.223   \n",
       "std     0.000   0.000   0.000   0.000   0.000   0.000   0.081   0.094   0.088   \n",
       "min    -0.040  -0.168  -0.149  -0.008  -0.108  -0.129  -0.059  -0.046   0.099   \n",
       "25%    -0.040  -0.168  -0.149  -0.008  -0.108  -0.129   0.046   0.100   0.167   \n",
       "50%    -0.040  -0.168  -0.149  -0.008  -0.108  -0.129   0.116   0.154   0.205   \n",
       "75%    -0.040  -0.168  -0.149  -0.008  -0.108  -0.129   0.171   0.239   0.259   \n",
       "max    -0.040  -0.168  -0.149  -0.008  -0.108  -0.129   0.313   0.385   0.593   \n",
       "\n",
       "        wb_43   wb_44   wb_45   wb_46   wb_47   wb_48   wb_49   wb_50   wb_51  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.097   0.205   0.087   0.040   0.212   0.197  -0.198  -0.019   0.135   \n",
       "std     0.000   0.084   0.066   0.103   0.057   0.068   0.000   0.000   0.108   \n",
       "min    -0.097   0.082  -0.022  -0.312   0.121   0.051  -0.198  -0.019  -0.079   \n",
       "25%    -0.097   0.151   0.029   0.004   0.165   0.149  -0.198  -0.019   0.048   \n",
       "50%    -0.097   0.185   0.072   0.006   0.206   0.200  -0.198  -0.019   0.125   \n",
       "75%    -0.097   0.235   0.145   0.071   0.250   0.236  -0.198  -0.019   0.221   \n",
       "max    -0.097   0.586   0.219   0.306   0.365   0.361  -0.198  -0.019   0.354   \n",
       "\n",
       "        wb_52   wb_53   wb_54   wb_55   wb_56   wb_57   wb_58   wb_59   wb_60  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.190  -0.043  -0.162   0.211   0.213  -0.202  -0.024  -0.117   0.101   \n",
       "std     0.000   0.000   0.000   0.061   0.056   0.000   0.000   0.000   0.081   \n",
       "min    -0.190  -0.043  -0.162   0.085   0.121  -0.202  -0.024  -0.117  -0.029   \n",
       "25%    -0.190  -0.043  -0.162   0.168   0.169  -0.202  -0.024  -0.117   0.035   \n",
       "50%    -0.190  -0.043  -0.162   0.210   0.203  -0.202  -0.024  -0.117   0.077   \n",
       "75%    -0.190  -0.043  -0.162   0.248   0.250  -0.202  -0.024  -0.117   0.165   \n",
       "max    -0.190  -0.043  -0.162   0.365   0.360  -0.202  -0.024  -0.117   0.276   \n",
       "\n",
       "        wb_61   wb_62   wb_63   wb_64   wb_65   wb_66   wb_67   wb_68   wb_69  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.217   0.158   0.104   0.148  -0.057  -0.156   0.242   0.045  -0.115   \n",
       "std     0.075   0.060   0.079   0.060   0.000   0.000   0.096   0.064   0.000   \n",
       "min     0.060   0.056  -0.069   0.038  -0.057  -0.156   0.045  -0.061  -0.115   \n",
       "25%     0.161   0.113   0.029   0.104  -0.057  -0.156   0.180   0.008  -0.115   \n",
       "50%     0.216   0.156   0.103   0.147  -0.057  -0.156   0.229   0.012  -0.115   \n",
       "75%     0.256   0.198   0.163   0.189  -0.057  -0.156   0.288   0.080  -0.115   \n",
       "max     0.576   0.294   0.296   0.280  -0.057  -0.156   0.829   0.188  -0.115   \n",
       "\n",
       "        wb_70   wb_71   wb_72   wb_73   wb_74   wb_75   wb_76   wb_77   wb_78  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.230  -0.213  -0.006   0.251   0.070   0.172   0.239  -0.192   0.228   \n",
       "std     0.086   0.000   0.000   0.104   0.099   0.100   0.078   0.000   0.077   \n",
       "min     0.022  -0.213  -0.006   0.054  -0.128  -0.060   0.047  -0.192   0.068   \n",
       "25%     0.168  -0.213  -0.006   0.186   0.004   0.095   0.183  -0.192   0.172   \n",
       "50%     0.218  -0.213  -0.006   0.235   0.024   0.160   0.226  -0.192   0.225   \n",
       "75%     0.287  -0.213  -0.006   0.290   0.133   0.238   0.287  -0.192   0.269   \n",
       "max     0.414  -0.213  -0.006   0.893   0.370   0.503   0.426  -0.192   0.606   \n",
       "\n",
       "        wb_79   wb_80   wb_81   wb_82   wb_83   wb_84   wb_85   wb_86   wb_87  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.246  -0.056   0.092   0.141  -0.020   0.151  -0.085   0.210  -0.197   \n",
       "std     0.091   0.000   0.062   0.068   0.000   0.093   0.000   0.131   0.000   \n",
       "min     0.115  -0.056  -0.011  -0.011  -0.020   0.022  -0.085   0.003  -0.197   \n",
       "25%     0.187  -0.056   0.034   0.087  -0.020   0.089  -0.085   0.137  -0.197   \n",
       "50%     0.225  -0.056   0.082   0.139  -0.020   0.136  -0.085   0.190  -0.197   \n",
       "75%     0.276  -0.056   0.147   0.190  -0.020   0.195  -0.085   0.241  -0.197   \n",
       "max     0.586  -0.056   0.214   0.298  -0.020   0.628  -0.085   0.815  -0.197   \n",
       "\n",
       "        wb_88   wb_89   wb_90   wb_91   wb_92   wb_93   wb_94   wb_95   wb_96  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.067   0.144   0.098  -0.196  -0.107   0.039  -0.061  -0.192   0.081   \n",
       "std     0.000   0.096   0.061   0.000   0.000   0.075   0.000   0.000   0.066   \n",
       "min    -0.067   0.012  -0.002  -0.196  -0.107  -0.087  -0.061  -0.192  -0.027   \n",
       "25%    -0.067   0.080   0.038  -0.196  -0.107   0.003  -0.061  -0.192   0.026   \n",
       "50%    -0.067   0.125   0.089  -0.196  -0.107   0.003  -0.061  -0.192   0.065   \n",
       "75%    -0.067   0.190   0.150  -0.196  -0.107   0.067  -0.061  -0.192   0.138   \n",
       "max    -0.067   0.633   0.224  -0.196  -0.107   0.219  -0.061  -0.192   0.213   \n",
       "\n",
       "        wb_97   wb_98   wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.185   0.228  -0.125  -0.157   0.126   0.041  -0.215   0.083  -0.067   \n",
       "std     0.000   0.090   0.000   0.000   0.145   0.064   0.000   0.074   0.000   \n",
       "min    -0.185   0.061  -0.125  -0.157  -0.076  -0.065  -0.215  -0.080  -0.067   \n",
       "25%    -0.185   0.173  -0.125  -0.157   0.023   0.006  -0.215   0.018  -0.067   \n",
       "50%    -0.185   0.220  -0.125  -0.157   0.093   0.008  -0.215   0.082  -0.067   \n",
       "75%    -0.185   0.263  -0.125  -0.157   0.202   0.074  -0.215   0.135  -0.067   \n",
       "max    -0.185   0.676  -0.125  -0.157   0.733   0.192  -0.215   0.264  -0.067   \n",
       "\n",
       "       wb_106  wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.129   0.188  -0.011  -0.208  -0.211  -0.203   0.140  -0.041   0.115   \n",
       "std     0.000   0.056   0.000   0.000   0.000   0.000   0.104   0.000   0.061   \n",
       "min    -0.129   0.097  -0.011  -0.208  -0.211  -0.203  -0.083  -0.041   0.007   \n",
       "25%    -0.129   0.141  -0.011  -0.208  -0.211  -0.203   0.068  -0.041   0.061   \n",
       "50%    -0.129   0.182  -0.011  -0.208  -0.211  -0.203   0.124  -0.041   0.109   \n",
       "75%    -0.129   0.226  -0.011  -0.208  -0.211  -0.203   0.225  -0.041   0.161   \n",
       "max    -0.129   0.328  -0.011  -0.208  -0.211  -0.203   0.374  -0.041   0.247   \n",
       "\n",
       "       wb_115  wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.144   0.108   0.208   0.015   0.046   0.094   0.238   0.093  -0.081   \n",
       "std     0.000   0.130   0.072   0.065   0.085   0.072   0.068   0.073   0.000   \n",
       "min    -0.144  -0.113   0.054  -0.103  -0.127  -0.019   0.089  -0.071  -0.081   \n",
       "25%    -0.144   0.015   0.156  -0.005  -0.003   0.033   0.185   0.023  -0.081   \n",
       "50%    -0.144   0.080   0.208  -0.005   0.001   0.077   0.239   0.095  -0.081   \n",
       "75%    -0.144   0.201   0.248   0.027   0.100   0.153   0.284   0.146  -0.081   \n",
       "max    -0.144   0.572   0.474   0.192   0.277   0.245   0.416   0.270  -0.081   \n",
       "\n",
       "       wb_124  wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.216  -0.023  -0.093   0.187   0.000   0.000  -0.020  -0.026  -0.044   \n",
       "std     0.056   0.000   0.000   0.095   0.000   0.000   0.073   0.078   0.086   \n",
       "min     0.115  -0.023  -0.093  -0.007   0.000   0.000  -0.223  -0.185  -0.252   \n",
       "25%     0.173  -0.023  -0.093   0.118   0.000   0.000  -0.066  -0.090  -0.105   \n",
       "50%     0.207  -0.023  -0.093   0.181   0.000   0.000  -0.007  -0.017  -0.033   \n",
       "75%     0.252  -0.023  -0.093   0.254   0.000   0.000   0.004   0.003  -0.000   \n",
       "max     0.368  -0.023  -0.093   0.392   0.000   0.000   0.126   0.128   0.141   \n",
       "\n",
       "       wb_133  wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.044  -0.031   0.000  -0.028  -0.066  -0.006   0.000   0.000  -0.023   \n",
       "std     0.088   0.085   0.000   0.079   0.120   0.059   0.000   0.000   0.078   \n",
       "min    -0.246  -0.263   0.000  -0.177  -0.355  -0.145   0.000   0.000  -0.238   \n",
       "25%    -0.097  -0.079   0.000  -0.093  -0.163  -0.038   0.000   0.000  -0.067   \n",
       "50%    -0.032  -0.016   0.000  -0.033  -0.045  -0.005   0.000   0.000  -0.009   \n",
       "75%    -0.000   0.002   0.000   0.004   0.006   0.002   0.000   0.000   0.002   \n",
       "max     0.141   0.132   0.000   0.128   0.141   0.127   0.000   0.000   0.129   \n",
       "\n",
       "       wb_142  wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.048  -0.005  -0.030  -0.074  -0.027  -0.054  -0.049  -0.021  -0.016   \n",
       "std     0.094   0.058   0.080   0.129   0.073   0.116   0.101   0.074   0.087   \n",
       "min    -0.268  -0.132  -0.259  -0.644  -0.200  -0.629  -0.279  -0.175  -0.226   \n",
       "25%    -0.108  -0.026  -0.074  -0.163  -0.075  -0.104  -0.117  -0.044  -0.016   \n",
       "50%    -0.034  -0.005  -0.016  -0.037  -0.010  -0.021  -0.024  -0.027  -0.015   \n",
       "75%    -0.000   0.003  -0.000  -0.000  -0.000  -0.000  -0.001   0.004   0.005   \n",
       "max     0.145   0.125   0.140   0.149   0.138   0.144   0.161   0.129   0.149   \n",
       "\n",
       "       wb_151  wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.000   0.000   0.000  -0.025   0.000   0.000   0.000  -0.038  -0.046   \n",
       "std     0.000   0.000   0.000   0.077   0.000   0.000   0.000   0.078   0.104   \n",
       "min     0.000   0.000   0.000  -0.202   0.000   0.000   0.000  -0.223  -0.523   \n",
       "25%     0.000   0.000   0.000  -0.066   0.000   0.000   0.000  -0.093  -0.102   \n",
       "50%     0.000   0.000   0.000  -0.029   0.000   0.000   0.000  -0.020  -0.015   \n",
       "75%     0.000   0.000   0.000   0.004   0.000   0.000   0.000  -0.000  -0.000   \n",
       "max     0.000   0.000   0.000   0.128   0.000   0.000   0.000   0.136   0.146   \n",
       "\n",
       "       wb_160  wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.036  -0.014   0.000   0.000   0.000   0.000   0.000   0.000  -0.051   \n",
       "std     0.078   0.069   0.000   0.000   0.000   0.000   0.000   0.000   0.096   \n",
       "min    -0.211  -0.178   0.000   0.000   0.000   0.000   0.000   0.000  -0.264   \n",
       "25%    -0.086  -0.068   0.000   0.000   0.000   0.000   0.000   0.000  -0.119   \n",
       "50%    -0.016  -0.007   0.000   0.000   0.000   0.000   0.000   0.000  -0.040   \n",
       "75%    -0.000   0.003   0.000   0.000   0.000   0.000   0.000   0.000  -0.000   \n",
       "max     0.138   0.127   0.000   0.000   0.000   0.000   0.000   0.000   0.146   \n",
       "\n",
       "       wb_169  wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  wb_176  wb_177  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.065  -0.047   0.000  -0.050  -0.029  -0.021  -0.000  -0.015   0.000   \n",
       "std     0.120   0.103   0.000   0.105   0.081   0.102   0.052   0.062   0.000   \n",
       "min    -0.314  -0.369   0.000  -0.330  -0.180  -0.270  -0.128  -0.157   0.000   \n",
       "25%    -0.180  -0.107   0.000  -0.107  -0.095  -0.034  -0.022  -0.049   0.000   \n",
       "50%    -0.030  -0.019   0.000  -0.025  -0.030  -0.011  -0.005  -0.004   0.000   \n",
       "75%     0.004  -0.000   0.000  -0.000   0.003   0.004   0.003  -0.000   0.000   \n",
       "max     0.164   0.133   0.000   0.133   0.129   0.163   0.126   0.135   0.000   \n",
       "\n",
       "       wb_178  wb_179  wb_180  wb_181  wb_182  wb_183  wb_184  wb_185  wb_186  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.000  -0.051   0.000   0.000   0.000   0.003  -0.009   0.000   0.000   \n",
       "std     0.000   0.134   0.000   0.000   0.000   0.050   0.062   0.000   0.000   \n",
       "min     0.000  -0.282   0.000   0.000   0.000  -0.127  -0.156   0.000   0.000   \n",
       "25%     0.000  -0.177   0.000   0.000   0.000  -0.014  -0.043   0.000   0.000   \n",
       "50%     0.000  -0.041   0.000   0.000   0.000  -0.001  -0.004   0.000   0.000   \n",
       "75%     0.000   0.077   0.000   0.000   0.000   0.004   0.002   0.000   0.000   \n",
       "max     0.000   0.177   0.000   0.000   0.000   0.123   0.127   0.000   0.000   \n",
       "\n",
       "       wb_187  wb_188  wb_189  wb_190  wb_191  wb_192  wb_193  wb_194  wb_195  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.000  -0.045  -0.018  -0.013  -0.047  -0.016   0.000   0.000  -0.058   \n",
       "std     0.000   0.099   0.068   0.068   0.092   0.069   0.000   0.000   0.113   \n",
       "min     0.000  -0.250  -0.200  -0.197  -0.253  -0.183   0.000   0.000  -0.372   \n",
       "25%     0.000  -0.133  -0.031  -0.069  -0.108  -0.067   0.000   0.000  -0.141   \n",
       "50%     0.000  -0.035  -0.004  -0.006  -0.036  -0.006   0.000   0.000  -0.027   \n",
       "75%     0.000   0.000  -0.000   0.004  -0.000   0.004   0.000   0.000   0.001   \n",
       "max     0.000   0.138   0.141   0.125   0.144   0.125   0.000   0.000   0.156   \n",
       "\n",
       "       wb_196  wb_197  wb_198  wb_199  wb_200  wb_201  wb_202  wb_203  wb_204  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.018   0.000  -0.052   0.000   0.000  -0.062  -0.045  -0.059  -0.055   \n",
       "std     0.073   0.000   0.116   0.000   0.000   0.132   0.100   0.124   0.110   \n",
       "min    -0.171   0.000  -0.272   0.000   0.000  -0.747  -0.282  -0.296  -0.303   \n",
       "25%    -0.024   0.000  -0.145   0.000   0.000  -0.144  -0.107  -0.167  -0.114   \n",
       "50%    -0.023   0.000  -0.031   0.000   0.000  -0.024  -0.019  -0.032  -0.018   \n",
       "75%     0.003   0.000   0.031   0.000   0.000   0.001  -0.001   0.013   0.008   \n",
       "max     0.131   0.000   0.164   0.000   0.000   0.155   0.165   0.151   0.158   \n",
       "\n",
       "       wb_205  wb_206  wb_207  wb_208  wb_209  wb_210  wb_211  wb_212  wb_213  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.000  -0.021  -0.053   0.000  -0.026  -0.034   0.000  -0.054   0.000   \n",
       "std     0.000   0.083   0.105   0.000   0.076   0.075   0.000   0.116   0.000   \n",
       "min     0.000  -0.519  -0.299   0.000  -0.175  -0.195   0.000  -0.534   0.000   \n",
       "25%     0.000  -0.036  -0.132   0.000  -0.084  -0.085   0.000  -0.131   0.000   \n",
       "50%     0.000  -0.005  -0.025   0.000  -0.028  -0.012   0.000  -0.026   0.000   \n",
       "75%     0.000  -0.000   0.000   0.000   0.004   0.000   0.000   0.000   0.000   \n",
       "max     0.000   0.142   0.133   0.000   0.127   0.135   0.000   0.134   0.000   \n",
       "\n",
       "       wb_214  wb_215  wb_216  wb_217  wb_218  wb_219  wb_220  wb_221  wb_222  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.081   0.000   0.000  -0.056  -0.022   0.000   0.000  -0.020   0.000   \n",
       "std     0.132   0.000   0.000   0.116   0.072   0.000   0.000   0.083   0.000   \n",
       "min    -0.570   0.000   0.000  -0.452  -0.160   0.000   0.000  -0.202   0.000   \n",
       "25%    -0.175   0.000   0.000  -0.141  -0.083   0.000   0.000  -0.019   0.000   \n",
       "50%    -0.040   0.000   0.000  -0.028  -0.012   0.000   0.000  -0.018   0.000   \n",
       "75%    -0.000   0.000   0.000   0.000   0.004   0.000   0.000   0.000   0.000   \n",
       "max     0.150   0.000   0.000   0.135   0.126   0.000   0.000   0.141   0.000   \n",
       "\n",
       "       wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  wb_229  wb_230  wb_231  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.000  -0.027   0.000  -0.032   0.000   0.000  -0.074  -0.017   0.000   \n",
       "std     0.000   0.079   0.000   0.091   0.000   0.000   0.135   0.073   0.000   \n",
       "min     0.000  -0.188   0.000  -0.514   0.000   0.000  -0.460  -0.176   0.000   \n",
       "25%     0.000  -0.079   0.000  -0.067   0.000   0.000  -0.169  -0.022   0.000   \n",
       "50%     0.000  -0.031   0.000  -0.007   0.000   0.000  -0.035  -0.021   0.000   \n",
       "75%     0.000   0.004   0.000  -0.000   0.000   0.000  -0.000   0.003   0.000   \n",
       "max     0.000   0.128   0.000   0.144   0.000   0.000   0.149   0.132   0.000   \n",
       "\n",
       "       wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  wb_238  wb_239  wb_240  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.035   0.000   0.000  -0.011   0.000   0.000   0.000   0.000  -0.065   \n",
       "std     0.081   0.000   0.000   0.065   0.000   0.000   0.000   0.000   0.123   \n",
       "min    -0.236   0.000   0.000  -0.157   0.000   0.000   0.000   0.000  -0.314   \n",
       "25%    -0.085   0.000   0.000  -0.039   0.000   0.000   0.000   0.000  -0.182   \n",
       "50%    -0.029   0.000   0.000  -0.006   0.000   0.000   0.000   0.000  -0.028   \n",
       "75%     0.000   0.000   0.000   0.003   0.000   0.000   0.000   0.000   0.005   \n",
       "max     0.137   0.000   0.000   0.127   0.000   0.000   0.000   0.000   0.169   \n",
       "\n",
       "       wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  wb_247  wb_248  wb_249  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.000  -0.021   0.000  -0.062  -0.018  -0.003  -0.030  -0.036  -0.004   \n",
       "std     0.000   0.072   0.000   0.132   0.065   0.070   0.085   0.088   0.052   \n",
       "min     0.000  -0.162   0.000  -0.409  -0.169  -0.176  -0.248  -0.218  -0.130   \n",
       "25%     0.000  -0.065   0.000  -0.166  -0.041  -0.006  -0.071  -0.103  -0.013   \n",
       "50%     0.000  -0.009   0.000  -0.022  -0.004  -0.006  -0.012  -0.037  -0.001   \n",
       "75%     0.000   0.004   0.000   0.002  -0.000   0.009  -0.000   0.002  -0.000   \n",
       "max     0.000   0.126   0.000   0.161   0.139   0.138   0.152   0.133   0.138   \n",
       "\n",
       "       wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  wb_256  wb_257  wb_258  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.037   0.000  -0.009   0.000   0.000  -0.050  -0.125  -0.176  -0.185   \n",
       "std     0.082   0.000   0.062   0.000   0.000   0.122   0.000   0.000   0.096   \n",
       "min    -0.242   0.000  -0.152   0.000   0.000  -0.264  -0.125  -0.176  -0.552   \n",
       "25%    -0.091   0.000  -0.038   0.000   0.000  -0.159  -0.125  -0.176  -0.215   \n",
       "50%    -0.032   0.000  -0.004   0.000   0.000  -0.038  -0.125  -0.176  -0.161   \n",
       "75%     0.000   0.000   0.001   0.000   0.000   0.042  -0.125  -0.176  -0.128   \n",
       "max     0.137   0.000   0.127   0.000   0.000   0.166  -0.125  -0.176  -0.026   \n",
       "\n",
       "       wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  wb_265  wb_266  wb_267  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.216   0.202   0.238  -0.116  -0.200  -0.245  -0.004  -0.091  -0.173   \n",
       "std     0.128   0.144   0.174   0.136   0.000   0.151   0.172   0.081   0.000   \n",
       "min    -0.623   0.038   0.049  -0.622  -0.200  -0.730  -0.396  -0.286  -0.173   \n",
       "25%    -0.293   0.108   0.116  -0.159  -0.200  -0.337  -0.065  -0.130  -0.173   \n",
       "50%    -0.170   0.147   0.167  -0.089  -0.200  -0.185  -0.016  -0.086  -0.173   \n",
       "75%    -0.121   0.252   0.301  -0.043  -0.200  -0.130   0.041  -0.050  -0.173   \n",
       "max    -0.074   0.634   0.763   0.305  -0.200  -0.088   0.631   0.217  -0.173   \n",
       "\n",
       "       wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  wb_274  wb_275  wb_276  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.178  -0.116   0.210  -0.132   0.362   0.089   0.186   0.110   0.188   \n",
       "std     0.000   0.104   0.182   0.076   0.287   0.284   0.107   0.260   0.223   \n",
       "min    -0.178  -0.483   0.019  -0.323   0.127  -1.570   0.012  -1.616  -0.010   \n",
       "25%    -0.178  -0.145   0.084  -0.167   0.189   0.046   0.122   0.066   0.019   \n",
       "50%    -0.178  -0.092   0.137  -0.124   0.239   0.086   0.162   0.106   0.090   \n",
       "75%    -0.178  -0.060   0.273  -0.089   0.393   0.189   0.242   0.193   0.251   \n",
       "max    -0.178   0.103   0.745   0.127   2.109   0.567   0.525   0.552   0.934   \n",
       "\n",
       "       wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  wb_283  wb_284  wb_285  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.256  -0.129  -0.128  -0.188  -0.029  -0.286   0.071  -0.126  -0.154   \n",
       "std     0.150   0.181   0.000   0.000   0.000   0.177   0.000   0.000   0.000   \n",
       "min    -0.693  -0.606  -0.128  -0.188  -0.029  -1.112   0.071  -0.126  -0.154   \n",
       "25%    -0.275  -0.154  -0.128  -0.188  -0.029  -0.302   0.071  -0.126  -0.154   \n",
       "50%    -0.199  -0.038  -0.128  -0.188  -0.029  -0.219   0.071  -0.126  -0.154   \n",
       "75%    -0.147  -0.018  -0.128  -0.188  -0.029  -0.159   0.071  -0.126  -0.154   \n",
       "max    -0.093   0.383  -0.128  -0.188  -0.029  -0.103   0.071  -0.126  -0.154   \n",
       "\n",
       "       wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  wb_292  wb_293  wb_294  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.267   0.070   0.231  -0.124   0.193  -0.080  -0.141  -0.210   0.164   \n",
       "std     0.134   0.201   0.127   0.088   0.000   0.000   0.000   0.000   0.000   \n",
       "min     0.109  -1.045   0.080  -0.391   0.193  -0.080  -0.141  -0.210   0.164   \n",
       "25%     0.176   0.039   0.150  -0.160   0.193  -0.080  -0.141  -0.210   0.164   \n",
       "50%     0.219   0.077   0.186  -0.107   0.193  -0.080  -0.141  -0.210   0.164   \n",
       "75%     0.308   0.140   0.276  -0.077   0.193  -0.080  -0.141  -0.210   0.164   \n",
       "max     0.678   0.455   0.611   0.066   0.193  -0.080  -0.141  -0.210   0.164   \n",
       "\n",
       "       wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  wb_301  wb_302  wb_303  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.062   0.214   0.090  -0.018   0.007  -0.025  -0.232  -0.078  -0.107   \n",
       "std     0.000   0.188   0.192   0.146   0.000   0.156   0.153   0.221   0.075   \n",
       "min    -0.062   0.015  -0.495  -0.322   0.007  -0.341  -0.711  -0.585  -0.298   \n",
       "25%    -0.062   0.080   0.000  -0.079   0.007  -0.088  -0.325  -0.130  -0.147   \n",
       "50%    -0.062   0.136   0.052  -0.032   0.007  -0.038  -0.170  -0.006  -0.103   \n",
       "75%    -0.062   0.290   0.162   0.007   0.007   0.004  -0.114   0.008  -0.068   \n",
       "max    -0.062   0.764   0.644   0.691   0.007   0.636  -0.072   0.677   0.156   \n",
       "\n",
       "       wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  wb_310  wb_311  wb_312  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.224  -0.033   0.024   0.017   0.114   0.054   0.120  -0.186  -0.076   \n",
       "std     0.091   0.000   0.000   0.213   0.000   0.000   0.000   0.066   0.083   \n",
       "min     0.059  -0.033   0.024  -0.425   0.114   0.054   0.120  -0.373  -0.273   \n",
       "25%     0.171  -0.033   0.024  -0.066   0.114   0.054   0.120  -0.223  -0.117   \n",
       "50%     0.206  -0.033   0.024   0.008   0.114   0.054   0.120  -0.180  -0.072   \n",
       "75%     0.273  -0.033   0.024   0.101   0.114   0.054   0.120  -0.146  -0.037   \n",
       "max     0.492  -0.033   0.024   0.650   0.114   0.054   0.120  -0.048   0.258   \n",
       "\n",
       "       wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  wb_319  wb_320  wb_321  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.214   0.102   0.137  -0.152   0.122  -0.174   0.237  -0.208  -0.045   \n",
       "std     0.000   0.000   0.000   0.175   0.120   0.086   0.193   0.091   0.000   \n",
       "min    -0.214   0.102   0.137  -0.691  -0.648  -0.480   0.029  -0.530  -0.045   \n",
       "25%    -0.214   0.102   0.137  -0.283   0.077  -0.208   0.097  -0.241  -0.045   \n",
       "50%    -0.214   0.102   0.137  -0.079   0.111  -0.155   0.161  -0.187  -0.045   \n",
       "75%    -0.214   0.102   0.137  -0.023   0.173  -0.125   0.308  -0.155  -0.045   \n",
       "max    -0.214   0.102   0.137   0.115   0.435  -0.022   0.784  -0.053  -0.045   \n",
       "\n",
       "       wb_322  wb_323  wb_324  wb_325  wb_326  wb_327  wb_328  wb_329  wb_330  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.133   0.054  -0.237   0.111   0.037   0.070  -0.070   0.040   0.196   \n",
       "std     0.000   0.161   0.151   0.000   0.137   0.000   0.000   0.208   0.249   \n",
       "min     0.133  -0.774  -0.695   0.111  -0.296   0.070  -0.070  -1.376  -0.001   \n",
       "25%     0.133   0.003  -0.260   0.111  -0.018   0.070  -0.070   0.004   0.019   \n",
       "50%     0.133   0.042  -0.171   0.111   0.025   0.070  -0.070   0.043   0.075   \n",
       "75%     0.133   0.105  -0.132   0.111   0.070   0.070  -0.070   0.105   0.276   \n",
       "max     0.133   0.481  -0.041   0.111   0.485   0.070  -0.070   0.447   1.361   \n",
       "\n",
       "       wb_331  wb_332  wb_333  wb_334  wb_335  wb_336  wb_337  wb_338  wb_339  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.009   0.059   0.111   0.100  -0.005   0.069  -0.269   0.283   0.190   \n",
       "std     0.179   0.138   0.000   0.145   0.137   0.000   0.135   0.128   0.000   \n",
       "min    -0.398  -0.347   0.111  -1.015  -0.302   0.069  -0.706   0.130   0.190   \n",
       "25%    -0.066   0.001   0.111   0.062  -0.058   0.069  -0.358   0.199   0.190   \n",
       "50%    -0.012   0.037   0.111   0.096  -0.019   0.069  -0.219   0.239   0.190   \n",
       "75%     0.046   0.097   0.111   0.149   0.028   0.069  -0.166   0.324   0.190   \n",
       "max     0.588   0.467   0.111   0.430   0.455   0.069  -0.118   0.649   0.190   \n",
       "\n",
       "       wb_340  wb_341  wb_342  wb_343  wb_344  wb_345  wb_346  wb_347  wb_348  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.053   0.196   0.073  -0.058   0.102  -0.060  -0.272  -0.182  -0.190   \n",
       "std     0.252   0.000   0.291   0.000   0.000   0.261   0.119   0.000   0.000   \n",
       "min    -0.426   0.196  -1.368  -0.058   0.102  -0.451  -0.601  -0.182  -0.190   \n",
       "25%    -0.159   0.196   0.038  -0.058   0.102  -0.189  -0.339  -0.182  -0.190   \n",
       "50%    -0.068   0.196   0.080  -0.058   0.102  -0.069  -0.232  -0.182  -0.190   \n",
       "75%    -0.019   0.196   0.181  -0.058   0.102  -0.022  -0.183  -0.182  -0.190   \n",
       "max     1.414   0.196   0.552  -0.058   0.102   1.227  -0.135  -0.182  -0.190   \n",
       "\n",
       "       wb_349  wb_350  wb_351  wb_352  wb_353  wb_354  wb_355  wb_356  wb_357  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.158   0.051  -0.189  -0.248  -0.015   0.088   0.199  -0.113  -0.030   \n",
       "std     0.170   0.000   0.000   0.157   0.000   0.173   0.000   0.000   0.365   \n",
       "min    -0.658   0.051  -0.189  -0.754  -0.015  -1.040   0.199  -0.113  -0.631   \n",
       "25%    -0.179   0.051  -0.189  -0.313  -0.015   0.051   0.199  -0.113  -0.242   \n",
       "50%    -0.076   0.051  -0.189  -0.189  -0.015   0.089   0.199  -0.113  -0.052   \n",
       "75%    -0.042   0.051  -0.189  -0.129  -0.015   0.151   0.199  -0.113   0.004   \n",
       "max    -0.000   0.051  -0.189  -0.085  -0.015   0.442   0.199  -0.113   1.341   \n",
       "\n",
       "       wb_358  wb_359  wb_360  wb_361  wb_362  wb_363  wb_364  wb_365  wb_366  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean   -0.225  -0.196   0.345   0.081  -0.152  -0.102  -0.178   0.094  -0.048   \n",
       "std     0.154   0.000   0.198   0.000   0.000   0.086   0.000   0.000   0.000   \n",
       "min    -0.708  -0.196   0.117   0.081  -0.152  -0.311  -0.178   0.094  -0.048   \n",
       "25%    -0.247  -0.196   0.192   0.081  -0.152  -0.138  -0.178   0.094  -0.048   \n",
       "50%    -0.156  -0.196   0.270   0.081  -0.152  -0.093  -0.178   0.094  -0.048   \n",
       "75%    -0.121  -0.196   0.380   0.081  -0.152  -0.060  -0.178   0.094  -0.048   \n",
       "max    -0.026  -0.196   0.910   0.081  -0.152   0.214  -0.178   0.094  -0.048   \n",
       "\n",
       "       wb_367  wb_368  wb_369  wb_370  wb_371  wb_372  wb_373  wb_374  wb_375  \\\n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000   \n",
       "mean    0.069   0.107  -0.115  -0.212  -0.056  -0.018   0.151  -0.182   0.233   \n",
       "std     0.000   0.225   0.000   0.105   0.000   0.318   0.114   0.162   0.241   \n",
       "min     0.069  -0.607  -0.115  -0.496  -0.056  -0.589  -0.530  -0.683   0.012   \n",
       "25%     0.069   0.006  -0.115  -0.250  -0.056  -0.130   0.103  -0.209   0.078   \n",
       "50%     0.069   0.060  -0.115  -0.181  -0.056  -0.036   0.137  -0.087   0.116   \n",
       "75%     0.069   0.193  -0.115  -0.139  -0.056   0.016   0.205  -0.087   0.267   \n",
       "max     0.069   0.675  -0.115  -0.060  -0.056   1.218   0.446  -0.002   1.038   \n",
       "\n",
       "       wb_376  wb_377  wb_378  wb_379  wb_380  wb_381  wb_382  wb_383  wb_384  \n",
       "count 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000 100.000  \n",
       "mean   -0.177   0.147   0.334   0.215  -0.072  -0.131  -0.124   0.009  -0.004  \n",
       "std     0.159   0.079   0.190   0.000   0.083   0.000   0.000   0.159   0.052  \n",
       "min    -0.679  -0.006   0.143   0.215  -0.269  -0.131  -0.124  -0.380  -0.120  \n",
       "25%    -0.266   0.103   0.193   0.215  -0.113  -0.131  -0.124  -0.051  -0.006  \n",
       "50%    -0.113   0.137   0.260   0.215  -0.068  -0.131  -0.124   0.000   0.000  \n",
       "75%    -0.058   0.189   0.385   0.215  -0.033  -0.131  -0.124   0.062   0.003  \n",
       "max    -0.012   0.399   0.895   0.215   0.272  -0.131  -0.124   0.535   0.126  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:15.662508Z",
     "iopub.status.busy": "2021-11-08T21:44:15.662195Z",
     "iopub.status.idle": "2021-11-08T21:44:15.668415Z",
     "shell.execute_reply": "2021-11-08T21:44:15.667879Z",
     "shell.execute_reply.started": "2021-11-08T21:44:15.662474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63344783],\n",
       "       [0.11655709],\n",
       "       [0.22846924],\n",
       "       [0.76488222],\n",
       "       [0.63214648],\n",
       "       [0.21268677],\n",
       "       [0.99588013],\n",
       "       [0.06295321],\n",
       "       [0.5338167 ],\n",
       "       [0.3516838 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.X_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:15.669297Z",
     "iopub.status.busy": "2021-11-08T21:44:15.669130Z",
     "iopub.status.idle": "2021-11-08T21:44:15.674262Z",
     "shell.execute_reply": "2021-11-08T21:44:15.673806Z",
     "shell.execute_reply.started": "2021-11-08T21:44:15.669278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36828968],\n",
       "       [-0.05066266],\n",
       "       [-0.11276318],\n",
       "       [-0.38301036],\n",
       "       [-0.36778516],\n",
       "       [-0.10326417],\n",
       "       [-0.08737013],\n",
       "       [-0.02553174],\n",
       "       [-0.31715995],\n",
       "       [-0.19349375]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.y_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:32:09.782470Z",
     "start_time": "2021-01-05T09:31:56.901018Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:15.675267Z",
     "iopub.status.busy": "2021-11-08T21:44:15.675058Z",
     "iopub.status.idle": "2021-11-08T21:44:15.929567Z",
     "shell.execute_reply": "2021-11-08T21:44:15.928865Z",
     "shell.execute_reply.started": "2021-11-08T21:44:15.675247Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "\n",
    "if inet_training_without_noise:\n",
    "   \n",
    "    for lambda_net_dataset, lambda_net_dataset_without_noise in zip(lambda_net_dataset_list, lambda_net_dataset_list_without_noise):\n",
    "        if inet_holdout_seed_evaluation:\n",
    "            raise SystemExit('Holdout Evaluation not implemented with inet training without noise')\n",
    "            \n",
    "        else:\n",
    "            lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_dataset_without_noise, test_split=0.1)\n",
    "\n",
    "            lambda_net_test_dataset = lambda_net_dataset #_, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_dataset_list_without_noise\n",
    "        \n",
    "else:\n",
    "\n",
    "    for lambda_net_dataset in lambda_net_dataset_list:\n",
    "\n",
    "        if inet_holdout_seed_evaluation:\n",
    "\n",
    "            complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "\n",
    "            if isinstance(test_size, float):\n",
    "                test_size = int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-test_size)))\n",
    "\n",
    "            test_seeds = random.sample(complete_seed_list, test_size)\n",
    "            lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "            valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-0.1))))\n",
    "            lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "            train_seeds = complete_seed_list\n",
    "            lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset\n",
    "        else:\n",
    "\n",
    "            lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "            lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.1)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "\n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:06.495716Z",
     "start_time": "2021-01-05T09:32:09.784760Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:15.930960Z",
     "iopub.status.busy": "2021-11-08T21:44:15.930670Z",
     "iopub.status.idle": "2021-11-08T21:44:24.252595Z",
     "shell.execute_reply": "2021-11-08T21:44:24.252118Z",
     "shell.execute_reply.started": "2021-11-08T21:44:15.930932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 404)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:08.945802Z",
     "start_time": "2021-01-05T09:33:06.499150Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:24.253564Z",
     "iopub.status.busy": "2021-11-08T21:44:24.253417Z",
     "iopub.status.idle": "2021-11-08T21:44:24.938351Z",
     "shell.execute_reply": "2021-11-08T21:44:24.937700Z",
     "shell.execute_reply.started": "2021-11-08T21:44:24.253544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 404)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:11.543306Z",
     "start_time": "2021-01-05T09:33:08.947468Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:24.939304Z",
     "iopub.status.busy": "2021-11-08T21:44:24.939155Z",
     "iopub.status.idle": "2021-11-08T21:44:24.971628Z",
     "shell.execute_reply": "2021-11-08T21:44:24.971130Z",
     "shell.execute_reply.started": "2021-11-08T21:44:24.939285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 404)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:24.972689Z",
     "iopub.status.busy": "2021-11-08T21:44:24.972432Z",
     "iopub.status.idle": "2021-11-08T21:44:32.912996Z",
     "shell.execute_reply": "2021-11-08T21:44:32.912372Z",
     "shell.execute_reply.started": "2021-11-08T21:44:24.972669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>5-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>0-target</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "      <th>wb_176</th>\n",
       "      <th>wb_177</th>\n",
       "      <th>wb_178</th>\n",
       "      <th>wb_179</th>\n",
       "      <th>wb_180</th>\n",
       "      <th>wb_181</th>\n",
       "      <th>wb_182</th>\n",
       "      <th>wb_183</th>\n",
       "      <th>wb_184</th>\n",
       "      <th>wb_185</th>\n",
       "      <th>wb_186</th>\n",
       "      <th>wb_187</th>\n",
       "      <th>wb_188</th>\n",
       "      <th>wb_189</th>\n",
       "      <th>wb_190</th>\n",
       "      <th>wb_191</th>\n",
       "      <th>wb_192</th>\n",
       "      <th>wb_193</th>\n",
       "      <th>wb_194</th>\n",
       "      <th>wb_195</th>\n",
       "      <th>wb_196</th>\n",
       "      <th>wb_197</th>\n",
       "      <th>wb_198</th>\n",
       "      <th>wb_199</th>\n",
       "      <th>wb_200</th>\n",
       "      <th>wb_201</th>\n",
       "      <th>wb_202</th>\n",
       "      <th>wb_203</th>\n",
       "      <th>wb_204</th>\n",
       "      <th>wb_205</th>\n",
       "      <th>wb_206</th>\n",
       "      <th>wb_207</th>\n",
       "      <th>wb_208</th>\n",
       "      <th>wb_209</th>\n",
       "      <th>wb_210</th>\n",
       "      <th>wb_211</th>\n",
       "      <th>wb_212</th>\n",
       "      <th>wb_213</th>\n",
       "      <th>wb_214</th>\n",
       "      <th>wb_215</th>\n",
       "      <th>wb_216</th>\n",
       "      <th>wb_217</th>\n",
       "      <th>wb_218</th>\n",
       "      <th>wb_219</th>\n",
       "      <th>wb_220</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "      <th>wb_321</th>\n",
       "      <th>wb_322</th>\n",
       "      <th>wb_323</th>\n",
       "      <th>wb_324</th>\n",
       "      <th>wb_325</th>\n",
       "      <th>wb_326</th>\n",
       "      <th>wb_327</th>\n",
       "      <th>wb_328</th>\n",
       "      <th>wb_329</th>\n",
       "      <th>wb_330</th>\n",
       "      <th>wb_331</th>\n",
       "      <th>wb_332</th>\n",
       "      <th>wb_333</th>\n",
       "      <th>wb_334</th>\n",
       "      <th>wb_335</th>\n",
       "      <th>wb_336</th>\n",
       "      <th>wb_337</th>\n",
       "      <th>wb_338</th>\n",
       "      <th>wb_339</th>\n",
       "      <th>wb_340</th>\n",
       "      <th>wb_341</th>\n",
       "      <th>wb_342</th>\n",
       "      <th>wb_343</th>\n",
       "      <th>wb_344</th>\n",
       "      <th>wb_345</th>\n",
       "      <th>wb_346</th>\n",
       "      <th>wb_347</th>\n",
       "      <th>wb_348</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15722</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-1.196</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.247</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.634</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49742</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.369</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.516</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.526</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>0.857</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-1.202</td>\n",
       "      <td>1.090</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>0.857</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37857</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.556</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.153</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>1.189</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>1.302</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35691</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             seed  5-target  4-target  3-target  2-target  1-target  0-target  \\\n",
       "15722  1373158606     0.000    -0.948     0.000    -0.567     0.277     0.000   \n",
       "49742  1373158606     0.000    -0.902     0.000    -0.560     0.000    -0.191   \n",
       "15559  1373158606     0.000    -0.885     0.857    -0.553     0.000     0.000   \n",
       "37857  1373158606     0.770     0.000    -0.019     0.000     0.000     0.530   \n",
       "35691  1373158606    -0.204    -0.875    -0.308     0.000     0.000     0.000   \n",
       "\n",
       "       5-lstsq_lambda  4-lstsq_lambda  3-lstsq_lambda  2-lstsq_lambda  \\\n",
       "15722           0.118          -1.196           0.180          -0.620   \n",
       "49742           0.076          -0.868          -0.262          -0.348   \n",
       "15559           0.150          -1.202           1.090          -0.624   \n",
       "37857           0.585           0.209           0.045          -0.133   \n",
       "35691          -0.221          -0.829          -0.345           0.010   \n",
       "\n",
       "       1-lstsq_lambda  0-lstsq_lambda  5-lstsq_target  4-lstsq_target  \\\n",
       "15722           0.282          -0.000          -0.000          -0.948   \n",
       "49742          -0.058          -0.186           0.000          -0.902   \n",
       "15559           0.009          -0.000           0.000          -0.885   \n",
       "37857           0.036           0.528           0.770           0.000   \n",
       "35691          -0.001           0.000          -0.204          -0.875   \n",
       "\n",
       "       3-lstsq_target  2-lstsq_target  1-lstsq_target  0-lstsq_target   wb_0  \\\n",
       "15722          -0.000          -0.567           0.277           0.000 -0.005   \n",
       "49742           0.000          -0.560          -0.000          -0.191 -0.005   \n",
       "15559           0.857          -0.553           0.000          -0.000 -0.005   \n",
       "37857          -0.019           0.000          -0.000           0.530 -0.005   \n",
       "35691          -0.308           0.000          -0.000           0.000 -0.005   \n",
       "\n",
       "        wb_1  wb_2  wb_3  wb_4  wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  \\\n",
       "15722 -0.111 0.256 0.220 0.077 0.044 0.253 -0.146 0.208 0.299  0.320 -0.043   \n",
       "49742 -0.111 0.126 0.074 0.045 0.037 0.262 -0.146 0.058 0.404  0.189 -0.043   \n",
       "15559 -0.111 0.172 0.145 0.075 0.045 0.168 -0.146 0.140 0.205  0.242 -0.043   \n",
       "37857 -0.111 0.166 0.038 0.096 0.064 0.135 -0.146 0.030 0.074  0.225 -0.043   \n",
       "35691 -0.111 0.253 0.222 0.029 0.001 0.253 -0.146 0.215 0.304  0.314 -0.043   \n",
       "\n",
       "       wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  \\\n",
       "15722 -0.009  0.264  0.044  0.306 -0.001  0.086  0.118  0.109 -0.355  0.190   \n",
       "49742 -0.009  0.198  0.036  0.180 -0.001  0.275  0.172  0.186  0.012  0.032   \n",
       "15559 -0.009  0.174  0.043  0.233 -0.001  0.097  0.117  0.107  0.006  0.125   \n",
       "37857 -0.009  0.167  0.060  0.217 -0.006  0.115  0.140  0.126  0.001  0.015   \n",
       "35691 -0.009  0.263  0.006  0.298 -0.001  0.133  0.066  0.108  0.011  0.197   \n",
       "\n",
       "       wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  \\\n",
       "15722  0.204 -0.021 -0.198 -0.044  0.191 -0.143 -0.102 -0.114  0.071  0.132   \n",
       "49742  0.272 -0.021 -0.198 -0.044  0.037 -0.143 -0.102 -0.114  0.044  0.288   \n",
       "15559  0.147 -0.021 -0.198 -0.044  0.128 -0.143 -0.102 -0.114  0.073  0.145   \n",
       "37857 -0.001 -0.021 -0.198 -0.044  0.018 -0.143 -0.102 -0.114  0.096  0.166   \n",
       "35691  0.223 -0.021 -0.198 -0.044  0.196 -0.143 -0.102 -0.114  0.017  0.183   \n",
       "\n",
       "       wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  \\\n",
       "15722  0.082  0.280 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.041  0.252   \n",
       "49742  0.044  0.149 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.034  0.462   \n",
       "15559  0.083  0.196 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.040  0.111   \n",
       "37857  0.106  0.187 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.057  0.078   \n",
       "35691  0.029  0.277 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.005  0.290   \n",
       "\n",
       "       wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  \\\n",
       "15722  0.319 -0.097  0.305  0.208  0.223  0.334  0.133 -0.198 -0.019  0.272   \n",
       "49742  0.369 -0.097  0.360  0.057  0.357  0.194  0.190 -0.198 -0.019  0.418   \n",
       "15559  0.226 -0.097  0.204  0.140  0.166  0.248  0.133 -0.198 -0.019  0.207   \n",
       "37857  0.633 -0.097  0.606  0.029  0.006  0.230  0.158 -0.198 -0.019  0.028   \n",
       "35691  0.318 -0.097  0.304  0.219  0.247  0.321  0.078 -0.198 -0.019  0.293   \n",
       "\n",
       "       wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  \\\n",
       "15722 -0.190 -0.043 -0.162  0.336  0.330 -0.202 -0.024 -0.117  0.237  0.151   \n",
       "49742 -0.190 -0.043 -0.162  0.195  0.259 -0.202 -0.024 -0.117  0.301  0.200   \n",
       "15559 -0.190 -0.043 -0.162  0.251  0.246 -0.202 -0.024 -0.117  0.156  0.146   \n",
       "37857 -0.190 -0.043 -0.162  0.232  0.228 -0.202 -0.024 -0.117  0.034  0.168   \n",
       "35691 -0.190 -0.043 -0.162  0.319  0.311 -0.202 -0.024 -0.117  0.248  0.125   \n",
       "\n",
       "       wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  \\\n",
       "15722  0.267  0.026  0.258 -0.057 -0.156  0.217  0.185 -0.115  0.338 -0.213   \n",
       "49742  0.142  0.028  0.132 -0.057 -0.156  0.502  0.019 -0.115  0.516 -0.213   \n",
       "15559  0.186  0.028  0.175 -0.057 -0.156  0.187  0.124 -0.115  0.248 -0.213   \n",
       "37857  0.181  0.045  0.172 -0.057 -0.156  0.165  0.008 -0.115  0.156 -0.213   \n",
       "35691  0.264  0.001  0.255 -0.057 -0.156  0.312  0.196 -0.115  0.353 -0.213   \n",
       "\n",
       "       wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78  wb_79  wb_80  wb_81  \\\n",
       "15722 -0.006  0.212  0.001  0.289  0.303 -0.192  0.157  0.341 -0.056  0.210   \n",
       "49742 -0.006  0.487  0.004  0.403  0.526 -0.192  0.204  0.397 -0.056  0.065   \n",
       "15559 -0.006  0.187  0.003  0.198  0.196 -0.192  0.155  0.244 -0.056  0.137   \n",
       "37857 -0.006  0.171 -0.016  0.054  0.168 -0.192  0.176  0.194 -0.056  0.034   \n",
       "35691 -0.006  0.291  0.004  0.300  0.324 -0.192  0.158  0.339 -0.056  0.211   \n",
       "\n",
       "       wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  wb_91  \\\n",
       "15722  0.073 -0.020  0.259 -0.085  0.093 -0.197 -0.067  0.253  0.212 -0.196   \n",
       "49742  0.044 -0.020  0.298 -0.085  0.316 -0.197 -0.067  0.302  0.074 -0.196   \n",
       "15559  0.074 -0.020  0.169 -0.085  0.098 -0.197 -0.067  0.168  0.142 -0.196   \n",
       "37857  0.099 -0.020  0.541 -0.085  0.119 -0.197 -0.067  0.558  0.038 -0.196   \n",
       "35691  0.018 -0.020  0.263 -0.085  0.154 -0.197 -0.067  0.257  0.215 -0.196   \n",
       "\n",
       "       wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  wb_98  wb_99  wb_100  wb_101  \\\n",
       "15722 -0.107  0.197 -0.061 -0.192  0.202 -0.185  0.147 -0.125  -0.157   0.237   \n",
       "49742 -0.107  0.204 -0.061 -0.192  0.051 -0.185  0.199 -0.125  -0.157   0.311   \n",
       "15559 -0.107  0.138 -0.061 -0.192  0.134 -0.185  0.150 -0.125  -0.157   0.162   \n",
       "37857 -0.107  0.003 -0.061 -0.192  0.026 -0.185  0.170 -0.125  -0.157   0.623   \n",
       "35691 -0.107  0.213 -0.061 -0.192  0.214 -0.185  0.163 -0.125  -0.157   0.251   \n",
       "\n",
       "       wb_102  wb_103  wb_104  wb_105  wb_106  wb_107  wb_108  wb_109  wb_110  \\\n",
       "15722   0.185  -0.215   0.012  -0.067  -0.129   0.299  -0.011  -0.208  -0.211   \n",
       "49742   0.015  -0.215   0.018  -0.067  -0.129   0.171  -0.011  -0.208  -0.211   \n",
       "15559   0.124  -0.215   0.005  -0.067  -0.129   0.220  -0.011  -0.208  -0.211   \n",
       "37857   0.006  -0.215   0.031  -0.067  -0.129   0.208  -0.011  -0.208  -0.211   \n",
       "35691   0.198  -0.215   0.014  -0.067  -0.129   0.293  -0.011  -0.208  -0.211   \n",
       "\n",
       "       wb_111  wb_112  wb_113  wb_114  wb_115  wb_116  wb_117  wb_118  wb_119  \\\n",
       "15722  -0.203   0.247  -0.041   0.229  -0.144   0.232   0.142   0.176  -0.004   \n",
       "49742  -0.203   0.439  -0.041   0.094  -0.144   0.341   0.195  -0.013  -0.003   \n",
       "15559  -0.203   0.076  -0.041   0.156  -0.144   0.172   0.139   0.120  -0.003   \n",
       "37857  -0.203   0.037  -0.041   0.047  -0.144   0.157   0.162  -0.005  -0.021   \n",
       "35691  -0.203   0.265  -0.041   0.232  -0.144   0.252   0.091   0.191  -0.003   \n",
       "\n",
       "       wb_120  wb_121  wb_122  wb_123  wb_124  wb_125  wb_126  wb_127  wb_128  \\\n",
       "15722   0.223   0.174   0.015  -0.081   0.336  -0.023  -0.093   0.309   0.000   \n",
       "49742   0.065   0.228   0.023  -0.081   0.278  -0.023  -0.093   0.441   0.000   \n",
       "15559   0.146   0.171   0.017  -0.081   0.248  -0.023  -0.093   0.217   0.000   \n",
       "37857   0.032   0.194   0.040  -0.081   0.230  -0.023  -0.093   0.096   0.000   \n",
       "35691   0.227   0.123   0.014  -0.081   0.327  -0.023  -0.093   0.318   0.000   \n",
       "\n",
       "       wb_129  wb_130  wb_131  wb_132  wb_133  wb_134  wb_135  wb_136  wb_137  \\\n",
       "15722   0.000  -0.148  -0.162   0.003   0.003  -0.165   0.000  -0.160  -0.185   \n",
       "49742   0.000   0.044   0.044  -0.048  -0.041  -0.053   0.000   0.044  -0.192   \n",
       "15559   0.000  -0.083  -0.110  -0.006  -0.005  -0.094   0.000  -0.090  -0.110   \n",
       "37857   0.000  -0.000  -0.044   0.098   0.099  -0.000   0.000  -0.038   0.046   \n",
       "35691   0.000  -0.152  -0.170  -0.010  -0.005  -0.177   0.000  -0.169  -0.194   \n",
       "\n",
       "       wb_138  wb_139  wb_140  wb_141  wb_142  wb_143  wb_144  wb_145  wb_146  \\\n",
       "15722  -0.119   0.000   0.000  -0.161  -0.000  -0.123  -0.017  -0.001   0.004   \n",
       "49742   0.040   0.000   0.000  -0.031  -0.040   0.043  -0.016  -0.246  -0.000   \n",
       "15559  -0.021   0.000   0.000  -0.090  -0.005  -0.028  -0.016  -0.012  -0.008   \n",
       "37857  -0.000   0.000   0.000  -0.000   0.100  -0.000   0.099   0.101   0.097   \n",
       "35691  -0.119   0.000   0.000  -0.165  -0.004  -0.122  -0.017  -0.013  -0.001   \n",
       "\n",
       "       wb_147  wb_148  wb_149  wb_150  wb_151  wb_152  wb_153  wb_154  wb_155  \\\n",
       "15722  -0.000   0.038  -0.164  -0.190   0.000   0.000   0.000  -0.168   0.000   \n",
       "49742  -0.029  -0.024   0.045  -0.248   0.000   0.000   0.000   0.045   0.000   \n",
       "15559  -0.009  -0.007  -0.097  -0.129   0.000   0.000   0.000  -0.087   0.000   \n",
       "37857   0.099   0.111  -0.028  -0.015   0.000   0.000   0.000  -0.031   0.000   \n",
       "35691  -0.019  -0.027  -0.183  -0.208   0.000   0.000   0.000  -0.179   0.000   \n",
       "\n",
       "       wb_156  wb_157  wb_158  wb_159  wb_160  wb_161  wb_162  wb_163  wb_164  \\\n",
       "15722   0.000   0.000   0.005  -0.001   0.005  -0.140   0.000   0.000   0.000   \n",
       "49742   0.000   0.000  -0.050  -0.144  -0.049   0.042   0.000   0.000   0.000   \n",
       "15559   0.000   0.000  -0.006  -0.011  -0.006  -0.058   0.000   0.000   0.000   \n",
       "37857   0.000   0.000   0.096   0.099   0.097  -0.000   0.000   0.000   0.000   \n",
       "35691   0.000   0.000  -0.001  -0.007  -0.007  -0.145   0.000   0.000   0.000   \n",
       "\n",
       "       wb_165  wb_166  wb_167  wb_168  wb_169  wb_170  wb_171  wb_172  wb_173  \\\n",
       "15722   0.000   0.000   0.000  -0.000  -0.217  -0.151   0.000  -0.158  -0.171   \n",
       "49742   0.000   0.000   0.000  -0.040  -0.349  -0.152   0.000  -0.136   0.044   \n",
       "15559   0.000   0.000   0.000  -0.005   0.002  -0.057   0.000  -0.081  -0.093   \n",
       "37857   0.000   0.000   0.000   0.101   0.111  -0.396   0.000  -0.275  -0.037   \n",
       "35691   0.000   0.000   0.000  -0.004  -0.244  -0.160   0.000  -0.170  -0.179   \n",
       "\n",
       "       wb_174  wb_175  wb_176  wb_177  wb_178  wb_179  wb_180  wb_181  wb_182  \\\n",
       "15722  -0.203  -0.090   0.006   0.000   0.000  -0.213   0.000   0.000   0.000   \n",
       "49742  -0.316   0.042  -0.000   0.000   0.000  -0.329   0.000   0.000   0.000   \n",
       "15559  -0.152  -0.015  -0.008   0.000   0.000  -0.170   0.000   0.000   0.000   \n",
       "37857  -0.011  -0.000   0.095   0.000   0.000   0.122   0.000   0.000   0.000   \n",
       "35691  -0.219  -0.099  -0.001   0.000   0.000  -0.233   0.000   0.000   0.000   \n",
       "\n",
       "       wb_183  wb_184  wb_185  wb_186  wb_187  wb_188  wb_189  wb_190  wb_191  \\\n",
       "15722  -0.072  -0.113   0.000   0.000   0.000  -0.191  -0.000  -0.142   0.001   \n",
       "49742   0.044  -0.048   0.000   0.000   0.000  -0.075  -0.001   0.044  -0.036   \n",
       "15559  -0.009  -0.018   0.000   0.000   0.000  -0.133  -0.008  -0.067  -0.003   \n",
       "37857  -0.000  -0.000   0.000   0.000   0.000  -0.037   0.097  -0.000   0.100   \n",
       "35691  -0.076  -0.136   0.000   0.000   0.000  -0.208  -0.023  -0.144  -0.007   \n",
       "\n",
       "       wb_192  wb_193  wb_194  wb_195  wb_196  wb_197  wb_198  wb_199  wb_200  \\\n",
       "15722  -0.144   0.000   0.000   0.005  -0.169   0.000  -0.200   0.000   0.000   \n",
       "49742   0.045   0.000   0.000  -0.346   0.044   0.000  -0.274   0.000   0.000   \n",
       "15559  -0.077   0.000   0.000   0.001  -0.111   0.000  -0.173   0.000   0.000   \n",
       "37857  -0.000   0.000   0.000   0.104  -0.023   0.000   0.108   0.000   0.000   \n",
       "35691  -0.147   0.000   0.000  -0.235  -0.179   0.000  -0.215   0.000   0.000   \n",
       "\n",
       "       wb_201  wb_202  wb_203  wb_204  wb_205  wb_206  wb_207  wb_208  wb_209  \\\n",
       "15722   0.008  -0.019  -0.203  -0.051   0.000  -0.000  -0.146   0.000  -0.158   \n",
       "49742  -0.351  -0.019  -0.230  -0.335   0.000  -0.001  -0.175   0.000   0.045   \n",
       "15559  -0.002  -0.016  -0.162   0.004   0.000  -0.007  -0.054   0.000  -0.108   \n",
       "37857   0.103   0.114   0.086   0.104   0.000   0.098  -0.000   0.000  -0.041   \n",
       "35691  -0.044  -0.021  -0.211  -0.237   0.000  -0.020  -0.156   0.000  -0.169   \n",
       "\n",
       "       wb_210  wb_211  wb_212  wb_213  wb_214  wb_215  wb_216  wb_217  wb_218  \\\n",
       "15722   0.006   0.000  -0.173   0.000  -0.003   0.000   0.000  -0.179  -0.156   \n",
       "49742  -0.050   0.000  -0.103   0.000  -0.271   0.000   0.000  -0.089   0.045   \n",
       "15559  -0.006   0.000  -0.102   0.000  -0.019   0.000   0.000  -0.103  -0.104   \n",
       "37857   0.096   0.000  -0.492   0.000   0.102   0.000   0.000  -0.474  -0.045   \n",
       "35691  -0.001   0.000  -0.179   0.000   0.001   0.000   0.000  -0.192  -0.156   \n",
       "\n",
       "       wb_219  wb_220  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  \\\n",
       "15722   0.000   0.000  -0.184   0.000   0.000  -0.166   0.000  -0.000   0.000   \n",
       "49742   0.000   0.000  -0.137   0.000   0.000   0.044   0.000  -0.031   0.000   \n",
       "15559   0.000   0.000  -0.125   0.000   0.000  -0.096   0.000  -0.008   0.000   \n",
       "37857   0.000   0.000  -0.019   0.000   0.000  -0.035   0.000   0.098   0.000   \n",
       "35691   0.000   0.000  -0.199   0.000   0.000  -0.165   0.000  -0.018   0.000   \n",
       "\n",
       "       wb_228  wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  \\\n",
       "15722   0.000  -0.195  -0.168   0.000   0.008   0.000   0.000  -0.135   0.000   \n",
       "49742   0.000  -0.283   0.044   0.000  -0.030   0.000   0.000   0.041   0.000   \n",
       "15559   0.000  -0.145  -0.112   0.000  -0.005   0.000   0.000  -0.039   0.000   \n",
       "37857   0.000  -0.469  -0.022   0.000   0.097   0.000   0.000  -0.000   0.000   \n",
       "35691   0.000  -0.223  -0.173   0.000  -0.032   0.000   0.000  -0.142   0.000   \n",
       "\n",
       "       wb_237  wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  \\\n",
       "15722   0.000   0.000   0.000  -0.212   0.000  -0.156   0.000  -0.207   0.003   \n",
       "49742   0.000   0.000   0.000  -0.360   0.000   0.044   0.000  -0.291  -0.001   \n",
       "15559   0.000   0.000   0.000  -0.013   0.000  -0.090   0.000  -0.145  -0.009   \n",
       "37857   0.000   0.000   0.000   0.115   0.000  -0.050   0.000  -0.000   0.097   \n",
       "35691   0.000   0.000   0.000  -0.230   0.000  -0.156   0.000  -0.225  -0.005   \n",
       "\n",
       "       wb_246  wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  \\\n",
       "15722  -0.163  -0.014  -0.172   0.003   0.005   0.000  -0.103   0.000   0.000   \n",
       "49742   0.043  -0.013   0.033  -0.001  -0.033   0.000  -0.090   0.000   0.000   \n",
       "15559  -0.109  -0.013  -0.110  -0.006  -0.004   0.000  -0.018   0.000   0.000   \n",
       "37857  -0.006   0.106  -0.039   0.096   0.097   0.000  -0.000   0.000   0.000   \n",
       "35691  -0.178  -0.013  -0.191  -0.006  -0.033   0.000  -0.113   0.000   0.000   \n",
       "\n",
       "       wb_255  wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  \\\n",
       "15722  -0.199  -0.125  -0.176  -0.381  -0.452   0.058   0.067  -0.339  -0.200   \n",
       "49742  -0.265  -0.125  -0.176  -0.144  -0.144   0.052   0.086  -0.222  -0.200   \n",
       "15559  -0.174  -0.125  -0.176  -0.237  -0.337   0.059   0.070  -0.196  -0.200   \n",
       "37857   0.108  -0.125  -0.176  -0.168  -0.105   0.147   0.170  -0.075  -0.200   \n",
       "35691  -0.208  -0.125  -0.176  -0.392  -0.478   0.001   0.052  -0.363  -0.200   \n",
       "\n",
       "       wb_264  wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  \\\n",
       "15722  -0.485  -0.270  -0.255  -0.173  -0.178  -0.322   0.036  -0.298   0.188   \n",
       "49742  -0.158  -0.305  -0.074  -0.173  -0.178  -0.134   0.054  -0.113   0.189   \n",
       "15559  -0.339  -0.139  -0.121  -0.173  -0.178  -0.175   0.038  -0.159   0.189   \n",
       "37857  -0.121   0.008  -0.101  -0.173  -0.178  -0.096   0.139  -0.140   0.290   \n",
       "35691  -0.518  -0.290  -0.247  -0.173  -0.178  -0.333  -0.000  -0.288   0.189   \n",
       "\n",
       "       wb_273  wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  \\\n",
       "15722   0.007   0.078   0.022  -0.350  -0.560  -0.528  -0.128  -0.188  -0.029   \n",
       "49742  -0.348   0.132  -0.125   0.019  -0.181  -0.579  -0.128  -0.188  -0.029   \n",
       "15559   0.004   0.081   0.023  -0.005  -0.428  -0.430  -0.128  -0.188  -0.029   \n",
       "37857   0.085   0.157   0.102   0.118  -0.146  -0.018  -0.128  -0.188  -0.029   \n",
       "35691  -0.042   0.018  -0.025   0.018  -0.605  -0.582  -0.128  -0.188  -0.029   \n",
       "\n",
       "       wb_282  wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  \\\n",
       "15722  -0.565   0.071  -0.126  -0.154   0.128   0.006   0.099  -0.306   0.193   \n",
       "49742  -0.193   0.071  -0.126  -0.154   0.128  -0.220   0.086  -0.092   0.193   \n",
       "15559  -0.403   0.071  -0.126  -0.154   0.130   0.005   0.100  -0.151   0.193   \n",
       "37857  -0.158   0.071  -0.126  -0.154   0.217   0.076   0.185  -0.118   0.193   \n",
       "35691  -0.599   0.071  -0.126  -0.154   0.062  -0.046   0.034  -0.316   0.193   \n",
       "\n",
       "       wb_291  wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  \\\n",
       "15722  -0.080  -0.141  -0.210   0.164  -0.062   0.032  -0.335  -0.229   0.007   \n",
       "49742  -0.080  -0.141  -0.210   0.164  -0.062   0.051  -0.567  -0.242   0.007   \n",
       "15559  -0.080  -0.141  -0.210   0.164  -0.062   0.034  -0.020  -0.078   0.007   \n",
       "37857  -0.080  -0.141  -0.210   0.164  -0.062   0.137   0.061   0.734   0.007   \n",
       "35691  -0.080  -0.141  -0.210   0.164  -0.062  -0.001  -0.385  -0.240   0.007   \n",
       "\n",
       "       wb_300  wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  \\\n",
       "15722  -0.248  -0.483  -0.499  -0.259   0.128  -0.033   0.024  -0.367   0.114   \n",
       "49742  -0.246  -0.142  -0.649  -0.092   0.184  -0.033   0.024  -0.523   0.114   \n",
       "15559  -0.099  -0.330  -0.412  -0.138   0.130  -0.033   0.024  -0.283   0.114   \n",
       "37857   0.556  -0.105   0.009  -0.119   0.202  -0.033   0.024   0.063   0.114   \n",
       "35691  -0.265  -0.517  -0.547  -0.248   0.066  -0.033   0.024  -0.411   0.114   \n",
       "\n",
       "       wb_309  wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  \\\n",
       "15722   0.054   0.120  -0.331  -0.239  -0.214   0.102   0.137  -0.399   0.036   \n",
       "49742   0.054   0.120  -0.169  -0.128  -0.214   0.102   0.137  -0.294   0.090   \n",
       "15559   0.054   0.120  -0.214  -0.107  -0.214   0.102   0.137  -0.284   0.039   \n",
       "37857   0.054   0.120  -0.195  -0.086  -0.214   0.102   0.137  -0.010   0.110   \n",
       "35691   0.054   0.120  -0.307  -0.241  -0.214   0.102   0.137  -0.440  -0.013   \n",
       "\n",
       "       wb_318  wb_319  wb_320  wb_321  wb_322  wb_323  wb_324  wb_325  wb_326  \\\n",
       "15722  -0.359   0.045  -0.399  -0.045   0.133  -0.031  -0.580   0.111  -0.244   \n",
       "49742  -0.140   0.077  -0.170  -0.045   0.133  -0.431  -0.164   0.111  -0.373   \n",
       "15559  -0.203   0.052  -0.248  -0.045   0.133  -0.021  -0.470   0.111  -0.175   \n",
       "37857  -0.166   0.162  -0.196  -0.045   0.133   0.045  -0.131   0.111   0.034   \n",
       "35691  -0.364   0.053  -0.407  -0.045   0.133  -0.281  -0.621   0.111  -0.269   \n",
       "\n",
       "       wb_327  wb_328  wb_329  wb_330  wb_331  wb_332  wb_333  wb_334  wb_335  \\\n",
       "15722   0.070  -0.070  -0.028   0.016  -0.306  -0.125   0.111   0.022  -0.212   \n",
       "49742   0.070  -0.070  -0.425   0.019  -0.356  -0.425   0.111   0.071  -0.247   \n",
       "15559   0.070  -0.070  -0.019   0.018  -0.213  -0.024   0.111   0.024  -0.067   \n",
       "37857   0.070  -0.070   0.047   0.125   0.031   0.042   0.111   0.093  -0.030   \n",
       "35691   0.070  -0.070  -0.106   0.019  -0.331  -0.282   0.111  -0.029  -0.221   \n",
       "\n",
       "       wb_336  wb_337  wb_338  wb_339  wb_340  wb_341  wb_342  wb_343  wb_344  \\\n",
       "15722   0.069  -0.506   0.148   0.190  -0.328   0.196   0.002  -0.058   0.102   \n",
       "49742   0.069  -0.192   0.148   0.190  -0.260   0.196  -0.384  -0.058   0.102   \n",
       "15559   0.069  -0.392   0.150   0.190  -0.187   0.196  -0.001  -0.058   0.102   \n",
       "37857   0.069  -0.154   0.237   0.190   1.153   0.196   0.078  -0.058   0.102   \n",
       "35691   0.069  -0.538   0.082   0.190  -0.349   0.196  -0.050  -0.058   0.102   \n",
       "\n",
       "       wb_345  wb_346  wb_347  wb_348  wb_349  wb_350  wb_351  wb_352  wb_353  \\\n",
       "15722  -0.345  -0.505  -0.182  -0.190  -0.530   0.051  -0.189  -0.504  -0.015   \n",
       "49742  -0.263  -0.207  -0.182  -0.190  -0.404   0.051  -0.189  -0.160  -0.015   \n",
       "15559  -0.202  -0.390  -0.182  -0.190  -0.429   0.051  -0.189  -0.362  -0.015   \n",
       "37857   1.189  -0.167  -0.182  -0.190  -0.042   0.051  -0.189  -0.124  -0.015   \n",
       "35691  -0.375  -0.522  -0.182  -0.190  -0.579   0.051  -0.189  -0.528  -0.015   \n",
       "\n",
       "       wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  wb_361  wb_362  \\\n",
       "15722   0.016   0.199  -0.113  -0.432  -0.578  -0.196   0.141   0.081  -0.152   \n",
       "49742  -0.085   0.199  -0.113  -0.510  -0.153  -0.196   0.191   0.081  -0.152   \n",
       "15559   0.017   0.199  -0.113  -0.335  -0.471  -0.196   0.149   0.081  -0.152   \n",
       "37857   0.088   0.199  -0.113   1.302  -0.121  -0.196   0.278   0.081  -0.152   \n",
       "35691  -0.035   0.199  -0.113  -0.483  -0.617  -0.196   0.188   0.081  -0.152   \n",
       "\n",
       "       wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  wb_370  wb_371  \\\n",
       "15722  -0.279  -0.178   0.094  -0.048   0.069  -0.403  -0.115  -0.432  -0.056   \n",
       "49742  -0.080  -0.178   0.094  -0.048   0.069  -0.665  -0.115  -0.158  -0.056   \n",
       "15559  -0.130  -0.178   0.094  -0.048   0.069  -0.020  -0.115  -0.293  -0.056   \n",
       "37857  -0.107  -0.178   0.094  -0.048   0.069   0.073  -0.115  -0.112  -0.056   \n",
       "35691  -0.282  -0.178   0.094  -0.048   0.069  -0.441  -0.115  -0.446  -0.056   \n",
       "\n",
       "       wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  wb_379  wb_380  \\\n",
       "15722  -0.454   0.060  -0.634   0.077  -0.416   0.060   0.128   0.215  -0.230   \n",
       "49742  -0.560   0.115  -0.114   0.078  -0.075   0.118   0.178   0.215  -0.162   \n",
       "15559  -0.355   0.062  -0.544   0.078  -0.290   0.063   0.144   0.215  -0.103   \n",
       "37857  -0.110   0.134  -0.087   0.186  -0.046   0.130   0.263   0.215  -0.082   \n",
       "35691  -0.501   0.004  -0.689   0.077  -0.457   0.004   0.171   0.215  -0.227   \n",
       "\n",
       "       wb_381  wb_382  wb_383  wb_384  \n",
       "15722  -0.131  -0.124  -0.275   0.009  \n",
       "49742  -0.131  -0.124  -0.366  -0.047  \n",
       "15559  -0.131  -0.124  -0.200   0.000  \n",
       "37857  -0.131  -0.124   0.039   0.093  \n",
       "35691  -0.131  -0.124  -0.298   0.000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:32.914261Z",
     "iopub.status.busy": "2021-11-08T21:44:32.914098Z",
     "iopub.status.idle": "2021-11-08T21:44:33.743878Z",
     "shell.execute_reply": "2021-11-08T21:44:33.743383Z",
     "shell.execute_reply.started": "2021-11-08T21:44:32.914240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>5-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>0-target</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "      <th>wb_176</th>\n",
       "      <th>wb_177</th>\n",
       "      <th>wb_178</th>\n",
       "      <th>wb_179</th>\n",
       "      <th>wb_180</th>\n",
       "      <th>wb_181</th>\n",
       "      <th>wb_182</th>\n",
       "      <th>wb_183</th>\n",
       "      <th>wb_184</th>\n",
       "      <th>wb_185</th>\n",
       "      <th>wb_186</th>\n",
       "      <th>wb_187</th>\n",
       "      <th>wb_188</th>\n",
       "      <th>wb_189</th>\n",
       "      <th>wb_190</th>\n",
       "      <th>wb_191</th>\n",
       "      <th>wb_192</th>\n",
       "      <th>wb_193</th>\n",
       "      <th>wb_194</th>\n",
       "      <th>wb_195</th>\n",
       "      <th>wb_196</th>\n",
       "      <th>wb_197</th>\n",
       "      <th>wb_198</th>\n",
       "      <th>wb_199</th>\n",
       "      <th>wb_200</th>\n",
       "      <th>wb_201</th>\n",
       "      <th>wb_202</th>\n",
       "      <th>wb_203</th>\n",
       "      <th>wb_204</th>\n",
       "      <th>wb_205</th>\n",
       "      <th>wb_206</th>\n",
       "      <th>wb_207</th>\n",
       "      <th>wb_208</th>\n",
       "      <th>wb_209</th>\n",
       "      <th>wb_210</th>\n",
       "      <th>wb_211</th>\n",
       "      <th>wb_212</th>\n",
       "      <th>wb_213</th>\n",
       "      <th>wb_214</th>\n",
       "      <th>wb_215</th>\n",
       "      <th>wb_216</th>\n",
       "      <th>wb_217</th>\n",
       "      <th>wb_218</th>\n",
       "      <th>wb_219</th>\n",
       "      <th>wb_220</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "      <th>wb_321</th>\n",
       "      <th>wb_322</th>\n",
       "      <th>wb_323</th>\n",
       "      <th>wb_324</th>\n",
       "      <th>wb_325</th>\n",
       "      <th>wb_326</th>\n",
       "      <th>wb_327</th>\n",
       "      <th>wb_328</th>\n",
       "      <th>wb_329</th>\n",
       "      <th>wb_330</th>\n",
       "      <th>wb_331</th>\n",
       "      <th>wb_332</th>\n",
       "      <th>wb_333</th>\n",
       "      <th>wb_334</th>\n",
       "      <th>wb_335</th>\n",
       "      <th>wb_336</th>\n",
       "      <th>wb_337</th>\n",
       "      <th>wb_338</th>\n",
       "      <th>wb_339</th>\n",
       "      <th>wb_340</th>\n",
       "      <th>wb_341</th>\n",
       "      <th>wb_342</th>\n",
       "      <th>wb_343</th>\n",
       "      <th>wb_344</th>\n",
       "      <th>wb_345</th>\n",
       "      <th>wb_346</th>\n",
       "      <th>wb_347</th>\n",
       "      <th>wb_348</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35587</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.721</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.721</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.285</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32681</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.945</td>\n",
       "      <td>1.198</td>\n",
       "      <td>-3.195</td>\n",
       "      <td>2.334</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.945</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40971</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21022</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>0.897</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6403</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             seed  5-target  4-target  3-target  2-target  1-target  0-target  \\\n",
       "35587  1373158606     0.000     0.000    -0.595     0.000     0.867     0.721   \n",
       "32681  1373158606     0.000     0.000    -0.805     0.736     0.000     0.945   \n",
       "40971  1373158606     0.000     0.000     0.580    -0.013    -0.402     0.000   \n",
       "21022  1373158606     0.000     0.000     0.431    -0.887     0.000     0.331   \n",
       "6403   1373158606     0.000     0.000     0.346     0.000    -0.613    -0.343   \n",
       "\n",
       "       5-lstsq_lambda  4-lstsq_lambda  3-lstsq_lambda  2-lstsq_lambda  \\\n",
       "35587           0.264          -0.531          -0.275          -0.049   \n",
       "32681           1.198          -3.195           2.334          -0.645   \n",
       "40971          -0.161           0.326           0.358           0.046   \n",
       "21022          -0.431           0.897          -0.204          -0.709   \n",
       "6403           -0.166           0.366           0.089           0.057   \n",
       "\n",
       "       1-lstsq_lambda  0-lstsq_lambda  5-lstsq_target  4-lstsq_target  \\\n",
       "35587           0.862           0.721          -0.000           0.000   \n",
       "32681           0.259           0.930           0.000          -0.000   \n",
       "40971          -0.407          -0.000           0.000          -0.000   \n",
       "21022          -0.017           0.331           0.000          -0.000   \n",
       "6403           -0.613          -0.343           0.000          -0.000   \n",
       "\n",
       "       3-lstsq_target  2-lstsq_target  1-lstsq_target  0-lstsq_target   wb_0  \\\n",
       "35587          -0.595           0.000           0.867           0.721 -0.005   \n",
       "32681          -0.805           0.736           0.000           0.945 -0.005   \n",
       "40971           0.580          -0.013          -0.402          -0.000 -0.005   \n",
       "21022           0.431          -0.887           0.000           0.331 -0.005   \n",
       "6403            0.346          -0.000          -0.613          -0.343 -0.005   \n",
       "\n",
       "        wb_1  wb_2  wb_3  wb_4  wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  \\\n",
       "35587 -0.111 0.220 0.039 0.155 0.125 0.259 -0.146 0.030 0.137  0.301 -0.043   \n",
       "32681 -0.111 0.221 0.041 0.090 0.058 0.188 -0.146 0.029 0.048  0.276 -0.043   \n",
       "40971 -0.111 0.135 0.083 0.164 0.155 0.113 -0.146 0.067 0.115  0.195 -0.043   \n",
       "21022 -0.111 0.177 0.118 0.080 0.049 0.156 -0.146 0.109 0.186  0.244 -0.043   \n",
       "6403  -0.111 0.155 0.103 0.155 0.133 0.131 -0.146 0.087 0.156  0.215 -0.043   \n",
       "\n",
       "       wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  \\\n",
       "35587 -0.009  0.256  0.122  0.285  0.056  0.177  0.199  0.186  0.069  0.016   \n",
       "32681 -0.009  0.197  0.052  0.254 -0.012  0.107  0.137  0.119 -0.015  0.015   \n",
       "40971 -0.009  0.136  0.153  0.188  0.105  0.181  0.194  0.191  0.142  0.042   \n",
       "21022 -0.009  0.179  0.047  0.236 -0.022  0.104  0.123  0.112 -0.004  0.092   \n",
       "6403  -0.009  0.155  0.145  0.208 -0.001  0.190  0.186  0.183  0.012  0.062   \n",
       "\n",
       "       wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  \\\n",
       "35587 -0.001 -0.021 -0.198 -0.044  0.018 -0.143 -0.102 -0.114  0.155  0.226   \n",
       "32681 -0.000 -0.021 -0.198 -0.044  0.019 -0.143 -0.102 -0.114  0.093  0.159   \n",
       "40971  0.005 -0.021 -0.198 -0.044  0.048 -0.143 -0.102 -0.114  0.162  0.230   \n",
       "21022  0.074 -0.021 -0.198 -0.044  0.096 -0.143 -0.102 -0.114  0.078  0.153   \n",
       "6403   0.024 -0.021 -0.198 -0.044  0.068 -0.143 -0.102 -0.114  0.147  0.223   \n",
       "\n",
       "       wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  \\\n",
       "35587  0.165  0.252 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.118  0.145   \n",
       "32681  0.102  0.238 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.048  0.063   \n",
       "40971  0.170  0.156 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.153  0.189   \n",
       "21022  0.088  0.202 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.043  0.076   \n",
       "6403   0.160  0.176 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.141  0.158   \n",
       "\n",
       "       wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  \\\n",
       "35587  0.131 -0.097  0.108  0.029  0.006  0.312  0.217 -0.198 -0.019  0.103   \n",
       "32681  0.182 -0.097  0.218  0.029  0.006  0.273  0.156 -0.198 -0.019  0.008   \n",
       "40971  0.177 -0.097  0.163  0.066 -0.253  0.201  0.206 -0.198 -0.019  0.184   \n",
       "21022  0.232 -0.097  0.214  0.104  0.083  0.249  0.139 -0.198 -0.019  0.027   \n",
       "6403   0.198 -0.097  0.182  0.086  0.018  0.221  0.201 -0.198 -0.019  0.077   \n",
       "\n",
       "       wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  \\\n",
       "35587 -0.190 -0.043 -0.162  0.305  0.309 -0.202 -0.024 -0.117  0.215  0.227   \n",
       "32681 -0.190 -0.043 -0.162  0.259  0.279 -0.202 -0.024 -0.117  0.144  0.164   \n",
       "40971 -0.190 -0.043 -0.162  0.205  0.198 -0.202 -0.024 -0.117  0.069  0.230   \n",
       "21022 -0.190 -0.043 -0.162  0.251  0.248 -0.202 -0.024 -0.117  0.117  0.152   \n",
       "6403  -0.190 -0.043 -0.162  0.225  0.218 -0.202 -0.024 -0.117  0.087  0.227   \n",
       "\n",
       "       wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  \\\n",
       "35587  0.233  0.107  0.219 -0.057 -0.156  0.228  0.008 -0.115  0.223 -0.213   \n",
       "32681  0.226  0.038  0.215 -0.057 -0.156  0.155  0.008 -0.115  0.144 -0.213   \n",
       "40971  0.151  0.142  0.141 -0.057 -0.156  0.232  0.029 -0.115  0.238 -0.213   \n",
       "21022  0.195  0.031  0.184 -0.057 -0.156  0.158  0.080 -0.115  0.178 -0.213   \n",
       "6403   0.171  0.134  0.162 -0.057 -0.156  0.214  0.048 -0.115  0.177 -0.213   \n",
       "\n",
       "       wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78  wb_79  wb_80  wb_81  \\\n",
       "35587 -0.006  0.233  0.054  0.123  0.231 -0.192  0.235  0.173 -0.056  0.034   \n",
       "32681 -0.006  0.161 -0.035  0.029  0.158 -0.192  0.171  0.113 -0.056  0.034   \n",
       "40971 -0.006  0.238  0.134  0.074  0.235 -0.192  0.246  0.190 -0.056  0.076   \n",
       "21022 -0.006  0.163 -0.020  0.154  0.161 -0.192  0.161  0.250 -0.056  0.107   \n",
       "6403  -0.006  0.246  0.004  0.128  0.135 -0.192  0.242  0.213 -0.056  0.096   \n",
       "\n",
       "       wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  wb_91  \\\n",
       "35587  0.158 -0.020  0.285 -0.085  0.181 -0.197 -0.067  0.291  0.039 -0.196   \n",
       "32681  0.096 -0.020  0.199 -0.085  0.110 -0.197 -0.067  0.195  0.040 -0.196   \n",
       "40971  0.159 -0.020  0.110 -0.085  0.186 -0.197 -0.067  0.101  0.084 -0.196   \n",
       "21022  0.080 -0.020  0.153 -0.085  0.109 -0.197 -0.067  0.142  0.116 -0.196   \n",
       "6403   0.148 -0.020  0.129 -0.085  0.194 -0.197 -0.067  0.120  0.104 -0.196   \n",
       "\n",
       "       wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  wb_98  wb_99  wb_100  wb_101  \\\n",
       "35587 -0.107  0.003 -0.061 -0.192  0.026 -0.185  0.229 -0.125  -0.157   0.023   \n",
       "32681 -0.107  0.003 -0.061 -0.192  0.026 -0.185  0.165 -0.125  -0.157   0.023   \n",
       "40971 -0.107  0.017 -0.061 -0.192  0.061 -0.185  0.237 -0.125  -0.157   0.034   \n",
       "21022 -0.107  0.075 -0.061 -0.192  0.104 -0.185  0.156 -0.125  -0.157   0.104   \n",
       "6403  -0.107  0.035 -0.061 -0.192  0.081 -0.185  0.231 -0.125  -0.157   0.056   \n",
       "\n",
       "       wb_102  wb_103  wb_104  wb_105  wb_106  wb_107  wb_108  wb_109  wb_110  \\\n",
       "35587   0.006  -0.215   0.092  -0.067  -0.129   0.273  -0.011  -0.208  -0.211   \n",
       "32681   0.006  -0.215   0.027  -0.067  -0.129   0.261  -0.011  -0.208  -0.211   \n",
       "40971   0.025  -0.215   0.125  -0.067  -0.129   0.177  -0.011  -0.208  -0.211   \n",
       "21022   0.076  -0.215   0.014  -0.067  -0.129   0.225  -0.011  -0.208  -0.211   \n",
       "6403    0.044  -0.215   0.019  -0.067  -0.129   0.197  -0.011  -0.208  -0.211   \n",
       "\n",
       "       wb_111  wb_112  wb_113  wb_114  wb_115  wb_116  wb_117  wb_118  wb_119  \\\n",
       "35587  -0.203   0.107  -0.041   0.209  -0.144  -0.038   0.220  -0.005   0.045   \n",
       "32681  -0.203   0.019  -0.041   0.164  -0.144  -0.181   0.158  -0.005  -0.034   \n",
       "40971  -0.203   0.174  -0.041   0.104  -0.144   0.008   0.218   0.003   0.109   \n",
       "21022  -0.203   0.036  -0.041   0.141  -0.144   0.099   0.144  -0.005  -0.031   \n",
       "6403   -0.203   0.182  -0.041   0.124  -0.144   0.043   0.205   0.014  -0.003   \n",
       "\n",
       "       wb_120  wb_121  wb_122  wb_123  wb_124  wb_125  wb_126  wb_127  wb_128  \\\n",
       "35587   0.033   0.252   0.100  -0.081   0.308  -0.023  -0.093   0.165   0.000   \n",
       "32681   0.033   0.191   0.036  -0.081   0.280  -0.023  -0.093   0.079   0.000   \n",
       "40971   0.071   0.259   0.134  -0.081   0.200  -0.023  -0.093   0.111   0.000   \n",
       "21022   0.115   0.176   0.023  -0.081   0.250  -0.023  -0.093   0.156   0.000   \n",
       "6403    0.090   0.254   0.027  -0.081   0.219  -0.023  -0.093   0.145   0.000   \n",
       "\n",
       "       wb_129  wb_130  wb_131  wb_132  wb_133  wb_134  wb_135  wb_136  wb_137  \\\n",
       "35587   0.000  -0.152  -0.044   0.107   0.106  -0.216   0.000  -0.038   0.102   \n",
       "32681   0.000  -0.108  -0.043   0.132   0.132  -0.000   0.000  -0.038   0.140   \n",
       "40971   0.000   0.001   0.001  -0.113  -0.091   0.001   0.000   0.001  -0.004   \n",
       "21022   0.000  -0.035  -0.051   0.072   0.073  -0.040   0.000  -0.031  -0.066   \n",
       "6403    0.000   0.057   0.057  -0.117  -0.112   0.058   0.000   0.057   0.064   \n",
       "\n",
       "       wb_138  wb_139  wb_140  wb_141  wb_142  wb_143  wb_144  wb_145  wb_146  \\\n",
       "35587  -0.127   0.000   0.000  -0.195   0.109  -0.106   0.104   0.114   0.105   \n",
       "32681  -0.215   0.000   0.000  -0.000   0.137  -0.190   0.131   0.141   0.128   \n",
       "40971   0.001   0.000   0.000   0.001  -0.106   0.001  -0.093  -0.131  -0.101   \n",
       "21022  -0.005   0.000   0.000  -0.039   0.073  -0.005   0.076   0.070   0.071   \n",
       "6403    0.058   0.000   0.000   0.058  -0.114   0.057  -0.016  -0.146  -0.109   \n",
       "\n",
       "       wb_147  wb_148  wb_149  wb_150  wb_151  wb_152  wb_153  wb_154  wb_155  \\\n",
       "35587   0.110   0.121  -0.028  -0.016   0.000   0.000   0.000  -0.030   0.000   \n",
       "32681   0.136   0.157  -0.028  -0.015   0.000   0.000   0.000  -0.030   0.000   \n",
       "40971  -0.106  -0.126   0.002  -0.002   0.000   0.000   0.000   0.002   0.000   \n",
       "21022   0.070   0.082  -0.000  -0.000   0.000   0.000   0.000  -0.000   0.000   \n",
       "6403   -0.133  -0.024   0.057   0.064   0.000   0.000   0.000   0.057   0.000   \n",
       "\n",
       "       wb_156  wb_157  wb_158  wb_159  wb_160  wb_161  wb_162  wb_163  wb_164  \\\n",
       "35587   0.000   0.000   0.103   0.113   0.104  -0.166   0.000   0.000   0.000   \n",
       "32681   0.000   0.000   0.126   0.139   0.128  -0.207   0.000   0.000   0.000   \n",
       "40971   0.000   0.000  -0.102  -0.087  -0.102   0.001   0.000   0.000   0.000   \n",
       "21022   0.000   0.000   0.072   0.067   0.071  -0.021   0.000   0.000   0.000   \n",
       "6403    0.000   0.000  -0.103  -0.127  -0.100   0.057   0.000   0.000   0.000   \n",
       "\n",
       "       wb_165  wb_166  wb_167  wb_168  wb_169  wb_170  wb_171  wb_172  wb_173  \\\n",
       "35587   0.000   0.000   0.000   0.110   0.128   0.021   0.000  -0.003  -0.037   \n",
       "32681   0.000   0.000   0.000   0.138   0.162   0.004   0.000  -0.000  -0.038   \n",
       "40971   0.000   0.000   0.000  -0.114  -0.150  -0.001   0.000  -0.001   0.001   \n",
       "21022   0.000   0.000   0.000   0.073   0.070  -0.028   0.000  -0.038  -0.040   \n",
       "6403    0.000   0.000   0.000  -0.122  -0.030   0.061   0.000   0.060   0.057   \n",
       "\n",
       "       wb_174  wb_175  wb_176  wb_177  wb_178  wb_179  wb_180  wb_181  wb_182  \\\n",
       "35587  -0.011  -0.095   0.102   0.000   0.000   0.139   0.000   0.000   0.000   \n",
       "32681  -0.011  -0.191   0.125   0.000   0.000   0.183   0.000   0.000   0.000   \n",
       "40971   0.035   0.001  -0.094   0.000   0.000  -0.149   0.000   0.000   0.000   \n",
       "21022  -0.000  -0.011   0.071   0.000   0.000   0.089   0.000   0.000   0.000   \n",
       "6403    0.071   0.057  -0.099   0.000   0.000   0.065   0.000   0.000   0.000   \n",
       "\n",
       "       wb_183  wb_184  wb_185  wb_186  wb_187  wb_188  wb_189  wb_190  wb_191  \\\n",
       "35587  -0.065  -0.144   0.000   0.000   0.000  -0.193   0.108  -0.136   0.108   \n",
       "32681  -0.158  -0.238   0.000   0.000   0.000  -0.000   0.132  -0.150   0.135   \n",
       "40971   0.002   0.001   0.000   0.000   0.000  -0.000  -0.079   0.001  -0.114   \n",
       "21022  -0.003  -0.007   0.000   0.000   0.000  -0.035   0.069  -0.026   0.074   \n",
       "6403    0.056   0.058   0.000   0.000   0.000   0.060  -0.092   0.057  -0.110   \n",
       "\n",
       "       wb_192  wb_193  wb_194  wb_195  wb_196  wb_197  wb_198  wb_199  wb_200  \\\n",
       "35587  -0.136   0.000   0.000   0.124  -0.023   0.000   0.132   0.000   0.000   \n",
       "32681  -0.118   0.000   0.000   0.153  -0.023   0.000   0.164   0.000   0.000   \n",
       "40971   0.002   0.000   0.000  -0.114   0.001   0.000  -0.129   0.000   0.000   \n",
       "21022  -0.029   0.000   0.000   0.055  -0.000   0.000   0.007   0.000   0.000   \n",
       "6403    0.057   0.000   0.000  -0.046   0.057   0.000   0.049   0.000   0.000   \n",
       "\n",
       "       wb_201  wb_202  wb_203  wb_204  wb_205  wb_206  wb_207  wb_208  wb_209  \\\n",
       "35587   0.122   0.124   0.117   0.125   0.000   0.110   0.069   0.000  -0.041   \n",
       "32681   0.150   0.163   0.158   0.154   0.000   0.134   0.078   0.000  -0.041   \n",
       "40971  -0.102  -0.120  -0.009  -0.115   0.000  -0.065  -0.002   0.000   0.002   \n",
       "21022   0.057   0.085  -0.078   0.050   0.000   0.068  -0.028   0.000  -0.053   \n",
       "6403   -0.056  -0.019   0.065  -0.009   0.000  -0.084   0.062   0.000   0.057   \n",
       "\n",
       "       wb_210  wb_211  wb_212  wb_213  wb_214  wb_215  wb_216  wb_217  wb_218  \\\n",
       "35587   0.102   0.000  -0.247   0.000   0.116   0.000   0.000  -0.232  -0.044   \n",
       "32681   0.125   0.000  -0.000   0.000   0.143   0.000   0.000  -0.000  -0.043   \n",
       "40971  -0.106   0.000   0.000   0.000  -0.134   0.000   0.000   0.000   0.002   \n",
       "21022   0.072   0.000  -0.049   0.000   0.069   0.000   0.000  -0.054  -0.052   \n",
       "6403   -0.099   0.000   0.060   0.000  -0.161   0.000   0.000   0.060   0.057   \n",
       "\n",
       "       wb_219  wb_220  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  \\\n",
       "35587   0.000   0.000  -0.019   0.000   0.000  -0.036   0.000   0.111   0.000   \n",
       "32681   0.000   0.000  -0.019   0.000   0.000  -0.036   0.000   0.135   0.000   \n",
       "40971   0.000   0.000   0.000   0.000   0.000   0.001   0.000  -0.074   0.000   \n",
       "21022   0.000   0.000  -0.000   0.000   0.000  -0.025   0.000   0.068   0.000   \n",
       "6403    0.000   0.000   0.060   0.000   0.000   0.057   0.000  -0.100   0.000   \n",
       "\n",
       "       wb_228  wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  \\\n",
       "35587   0.000  -0.024  -0.021   0.000   0.103   0.000   0.000  -0.148   0.000   \n",
       "32681   0.000  -0.024  -0.021   0.000   0.128   0.000   0.000  -0.212   0.000   \n",
       "40971   0.000  -0.001   0.001   0.000  -0.106   0.000   0.000   0.001   0.000   \n",
       "21022   0.000  -0.000  -0.000   0.000   0.074   0.000   0.000  -0.013   0.000   \n",
       "6403    0.000   0.064   0.058   0.000  -0.030   0.000   0.000   0.058   0.000   \n",
       "\n",
       "       wb_237  wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  \\\n",
       "35587   0.000   0.000   0.000   0.131   0.000  -0.151   0.000   0.130   0.106   \n",
       "32681   0.000   0.000   0.000   0.169   0.000  -0.000   0.000   0.219   0.129   \n",
       "40971   0.000   0.000   0.000  -0.151   0.000   0.001   0.000  -0.005  -0.087   \n",
       "21022   0.000   0.000   0.000   0.080   0.000  -0.047   0.000  -0.000   0.070   \n",
       "6403    0.000   0.000   0.000  -0.153   0.000   0.057   0.000   0.067  -0.109   \n",
       "\n",
       "       wb_246  wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  \\\n",
       "35587  -0.006   0.113  -0.039   0.105   0.103   0.000  -0.157   0.000   0.000   \n",
       "32681  -0.006   0.145  -0.039   0.128   0.128   0.000  -0.244   0.000   0.000   \n",
       "40971   0.005  -0.097   0.001  -0.058  -0.102   0.000   0.001   0.000   0.000   \n",
       "21022  -0.006   0.081  -0.032   0.070   0.073   0.000  -0.009   0.000   0.000   \n",
       "6403    0.059  -0.013   0.058  -0.074  -0.031   0.000   0.058   0.000   0.000   \n",
       "\n",
       "       wb_255  wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  \\\n",
       "35587   0.131  -0.125  -0.176  -0.385  -0.105   0.182   0.202  -0.405  -0.200   \n",
       "32681   0.168  -0.125  -0.176  -0.335  -0.105   0.176   0.204  -0.136  -0.200   \n",
       "40971  -0.002  -0.125  -0.176  -0.138  -0.132   0.278   0.308  -0.058  -0.200   \n",
       "21022  -0.001  -0.125  -0.176  -0.186  -0.186   0.121   0.148  -0.108  -0.200   \n",
       "6403    0.066  -0.125  -0.176  -0.171  -0.171   0.247   0.301  -0.093  -0.200   \n",
       "\n",
       "       wb_264  wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  \\\n",
       "35587  -0.121   0.054  -0.238  -0.173  -0.178  -0.346   0.171  -0.273   0.307   \n",
       "32681  -0.121   0.056  -0.288  -0.173  -0.178  -0.131   0.175  -0.318   0.339   \n",
       "40971  -0.142  -0.005  -0.074  -0.173  -0.178  -0.068   0.294  -0.112   0.520   \n",
       "21022  -0.197  -0.053  -0.121  -0.173  -0.178  -0.116   0.117  -0.158   0.291   \n",
       "6403   -0.184  -0.038  -0.104  -0.173  -0.178  -0.101   0.277  -0.141   0.189   \n",
       "\n",
       "       wb_273  wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  \\\n",
       "35587   0.123   0.196   0.140   0.138  -0.147  -0.018  -0.128  -0.188  -0.029   \n",
       "32681   0.114   0.181   0.129   0.168  -0.147  -0.018  -0.128  -0.188  -0.029   \n",
       "40971   0.208   0.246   0.202   0.333  -0.157  -0.002  -0.128  -0.188  -0.029   \n",
       "21022   0.055   0.128   0.073   0.105  -0.221  -0.095  -0.128  -0.188  -0.029   \n",
       "6403    0.181   0.231   0.188   0.019  -0.206  -0.079  -0.128  -0.188  -0.029   \n",
       "\n",
       "       wb_282  wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  \\\n",
       "35587  -0.159   0.071  -0.126  -0.154   0.253   0.117   0.222  -0.314   0.193   \n",
       "32681  -0.159   0.071  -0.126  -0.154   0.246   0.100   0.213  -0.331   0.193   \n",
       "40971  -0.172   0.071  -0.126  -0.154   0.342   0.144   0.301  -0.089   0.193   \n",
       "21022  -0.233   0.071  -0.126  -0.154   0.193   0.045   0.159  -0.136   0.193   \n",
       "6403   -0.219   0.071  -0.126  -0.154   0.312   0.144   0.271  -0.120   0.193   \n",
       "\n",
       "       wb_291  wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  \\\n",
       "35587  -0.080  -0.141  -0.210   0.164  -0.062   0.168   0.094   0.009   0.007   \n",
       "32681  -0.080  -0.141  -0.210   0.164  -0.062   0.174   0.097  -0.036   0.007   \n",
       "40971  -0.080  -0.141  -0.210   0.164  -0.062   0.305   0.217  -0.020   0.007   \n",
       "21022  -0.080  -0.141  -0.210   0.164  -0.062   0.116   0.029  -0.066   0.007   \n",
       "6403   -0.080  -0.141  -0.210   0.164  -0.062   0.284   0.075  -0.051   0.007   \n",
       "\n",
       "       wb_300  wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  \\\n",
       "35587   0.002  -0.106   0.008  -0.245   0.243  -0.033   0.024   0.089   0.114   \n",
       "32681  -0.066  -0.106   0.008  -0.290   0.224  -0.033   0.024   0.109   0.114   \n",
       "40971  -0.024  -0.127   0.165  -0.092   0.277  -0.033   0.024   0.253   0.114   \n",
       "21022  -0.070  -0.182  -0.068  -0.138   0.174  -0.033   0.024   0.039   0.114   \n",
       "6403   -0.055  -0.168  -0.055  -0.121   0.265  -0.033   0.024  -0.032   0.114   \n",
       "\n",
       "       wb_309  wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  \\\n",
       "35587   0.054   0.120  -0.316  -0.230  -0.214   0.102   0.137  -0.463   0.151   \n",
       "32681   0.054   0.120  -0.344  -0.274  -0.214   0.102   0.137  -0.108   0.132   \n",
       "40971   0.054   0.120  -0.168  -0.060  -0.214   0.102   0.137  -0.034   0.174   \n",
       "21022   0.054   0.120  -0.214  -0.107  -0.214   0.102   0.137  -0.089   0.079   \n",
       "6403    0.054   0.120  -0.197  -0.090  -0.214   0.102   0.137  -0.076   0.167   \n",
       "\n",
       "       wb_318  wb_319  wb_320  wb_321  wb_322  wb_323  wb_324  wb_325  wb_326  \\\n",
       "35587  -0.348   0.192  -0.389  -0.045   0.133   0.085  -0.131   0.111   0.073   \n",
       "32681  -0.354   0.201  -0.361  -0.045   0.133   0.071  -0.132   0.111   0.062   \n",
       "40971  -0.136   0.342  -0.165  -0.045   0.133   0.124  -0.134   0.111   0.126   \n",
       "21022  -0.183   0.144  -0.212  -0.045   0.133   0.010  -0.206   0.111  -0.008   \n",
       "6403   -0.168   0.326  -0.197  -0.045   0.133   0.062  -0.191   0.111  -0.018   \n",
       "\n",
       "       wb_327  wb_328  wb_329  wb_330  wb_331  wb_332  wb_333  wb_334  wb_335  \\\n",
       "35587   0.070  -0.070   0.087   0.140   0.069   0.082   0.111   0.135   0.029   \n",
       "32681   0.070  -0.070   0.071   0.179   0.080   0.068   0.111   0.115   0.006   \n",
       "40971   0.070  -0.070   0.117   0.353  -0.001   0.121   0.111   0.155  -0.013   \n",
       "21022   0.070  -0.070   0.012   0.116  -0.056   0.006   0.111   0.063  -0.059   \n",
       "6403    0.070  -0.070   0.081   0.019  -0.035  -0.001   0.111   0.146  -0.043   \n",
       "\n",
       "       wb_336  wb_337  wb_338  wb_339  wb_340  wb_341  wb_342  wb_343  wb_344  \\\n",
       "35587   0.069  -0.154   0.273   0.190  -0.412   0.196   0.115  -0.058   0.102   \n",
       "32681   0.069  -0.154   0.265   0.190  -0.118   0.196   0.107  -0.058   0.102   \n",
       "40971   0.069  -0.178   0.363   0.190  -0.039   0.196   0.199  -0.058   0.102   \n",
       "21022   0.069  -0.233   0.212   0.190  -0.089   0.196   0.047  -0.058   0.102   \n",
       "6403    0.069  -0.218   0.328   0.190  -0.074   0.196   0.174  -0.058   0.102   \n",
       "\n",
       "       wb_345  wb_346  wb_347  wb_348  wb_349  wb_350  wb_351  wb_352  wb_353  \\\n",
       "35587  -0.434  -0.167  -0.182  -0.190  -0.042   0.051  -0.189  -0.124  -0.015   \n",
       "32681  -0.124  -0.167  -0.182  -0.190  -0.043   0.051  -0.189  -0.124  -0.015   \n",
       "40971  -0.038  -0.194  -0.182  -0.190  -0.035   0.051  -0.189  -0.143  -0.015   \n",
       "21022  -0.090  -0.248  -0.182  -0.190  -0.119   0.051  -0.189  -0.199  -0.015   \n",
       "6403   -0.075  -0.232  -0.182  -0.190  -0.102   0.051  -0.189  -0.186  -0.015   \n",
       "\n",
       "       wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  wb_361  wb_362  \\\n",
       "35587   0.129   0.199  -0.113   0.005  -0.121  -0.196   0.305   0.081  -0.152   \n",
       "32681   0.111   0.199  -0.113   0.005  -0.121  -0.196   0.318   0.081  -0.152   \n",
       "40971   0.151   0.199  -0.113  -0.008  -0.120  -0.196   0.471   0.081  -0.152   \n",
       "21022   0.057   0.199  -0.113  -0.073  -0.195  -0.196   0.267   0.081  -0.152   \n",
       "6403    0.147   0.199  -0.113  -0.058  -0.180  -0.196   0.192   0.081  -0.152   \n",
       "\n",
       "       wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  wb_370  wb_371  \\\n",
       "35587  -0.267  -0.178   0.094  -0.048   0.069   0.101  -0.115  -0.461  -0.056   \n",
       "32681  -0.307  -0.178   0.094  -0.048   0.069   0.117  -0.115  -0.230  -0.056   \n",
       "40971  -0.079  -0.178   0.094  -0.048   0.069   0.263  -0.115  -0.149  -0.056   \n",
       "21022  -0.126  -0.178   0.094  -0.048   0.069   0.048  -0.115  -0.200  -0.056   \n",
       "6403   -0.110  -0.178   0.094  -0.048   0.069   0.204  -0.115  -0.185  -0.056   \n",
       "\n",
       "       wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  wb_379  wb_380  \\\n",
       "35587   0.109   0.175  -0.087   0.198  -0.047   0.173   0.292   0.215  -0.229   \n",
       "32681   0.241   0.157  -0.087   0.240  -0.047   0.150   0.301   0.215  -0.268   \n",
       "40971   0.000   0.205  -0.054   0.430  -0.070   0.187   0.443   0.215  -0.057   \n",
       "21022  -0.069   0.105  -0.087   0.186  -0.124   0.100   0.249   0.215  -0.103   \n",
       "6403   -0.053   0.197  -0.147   0.077  -0.111   0.182   0.179   0.215  -0.086   \n",
       "\n",
       "       wb_381  wb_382  wb_383  wb_384  \n",
       "35587  -0.131  -0.124   0.075   0.095  \n",
       "32681  -0.131  -0.124   0.075   0.116  \n",
       "40971  -0.131  -0.124   0.001  -0.002  \n",
       "21022  -0.131  -0.124  -0.022   0.072  \n",
       "6403   -0.131  -0.124  -0.028  -0.055  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:33.744855Z",
     "iopub.status.busy": "2021-11-08T21:44:33.744615Z",
     "iopub.status.idle": "2021-11-08T21:44:33.932069Z",
     "shell.execute_reply": "2021-11-08T21:44:33.931245Z",
     "shell.execute_reply.started": "2021-11-08T21:44:33.744835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>5-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>0-target</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "      <th>wb_176</th>\n",
       "      <th>wb_177</th>\n",
       "      <th>wb_178</th>\n",
       "      <th>wb_179</th>\n",
       "      <th>wb_180</th>\n",
       "      <th>wb_181</th>\n",
       "      <th>wb_182</th>\n",
       "      <th>wb_183</th>\n",
       "      <th>wb_184</th>\n",
       "      <th>wb_185</th>\n",
       "      <th>wb_186</th>\n",
       "      <th>wb_187</th>\n",
       "      <th>wb_188</th>\n",
       "      <th>wb_189</th>\n",
       "      <th>wb_190</th>\n",
       "      <th>wb_191</th>\n",
       "      <th>wb_192</th>\n",
       "      <th>wb_193</th>\n",
       "      <th>wb_194</th>\n",
       "      <th>wb_195</th>\n",
       "      <th>wb_196</th>\n",
       "      <th>wb_197</th>\n",
       "      <th>wb_198</th>\n",
       "      <th>wb_199</th>\n",
       "      <th>wb_200</th>\n",
       "      <th>wb_201</th>\n",
       "      <th>wb_202</th>\n",
       "      <th>wb_203</th>\n",
       "      <th>wb_204</th>\n",
       "      <th>wb_205</th>\n",
       "      <th>wb_206</th>\n",
       "      <th>wb_207</th>\n",
       "      <th>wb_208</th>\n",
       "      <th>wb_209</th>\n",
       "      <th>wb_210</th>\n",
       "      <th>wb_211</th>\n",
       "      <th>wb_212</th>\n",
       "      <th>wb_213</th>\n",
       "      <th>wb_214</th>\n",
       "      <th>wb_215</th>\n",
       "      <th>wb_216</th>\n",
       "      <th>wb_217</th>\n",
       "      <th>wb_218</th>\n",
       "      <th>wb_219</th>\n",
       "      <th>wb_220</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "      <th>wb_321</th>\n",
       "      <th>wb_322</th>\n",
       "      <th>wb_323</th>\n",
       "      <th>wb_324</th>\n",
       "      <th>wb_325</th>\n",
       "      <th>wb_326</th>\n",
       "      <th>wb_327</th>\n",
       "      <th>wb_328</th>\n",
       "      <th>wb_329</th>\n",
       "      <th>wb_330</th>\n",
       "      <th>wb_331</th>\n",
       "      <th>wb_332</th>\n",
       "      <th>wb_333</th>\n",
       "      <th>wb_334</th>\n",
       "      <th>wb_335</th>\n",
       "      <th>wb_336</th>\n",
       "      <th>wb_337</th>\n",
       "      <th>wb_338</th>\n",
       "      <th>wb_339</th>\n",
       "      <th>wb_340</th>\n",
       "      <th>wb_341</th>\n",
       "      <th>wb_342</th>\n",
       "      <th>wb_343</th>\n",
       "      <th>wb_344</th>\n",
       "      <th>wb_345</th>\n",
       "      <th>wb_346</th>\n",
       "      <th>wb_347</th>\n",
       "      <th>wb_348</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.188</td>\n",
       "      <td>1.418</td>\n",
       "      <td>-1.083</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.934</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.528</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.717</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.784</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.703</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>1.525</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.554</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.703</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.274</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.408</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-1.307</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.729</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.594</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.729</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.597</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.525</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.449</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.751</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          seed  5-target  4-target  3-target  2-target  1-target  0-target  \\\n",
       "83  1373158606     0.840     0.000     0.000    -0.545    -0.371     0.000   \n",
       "53  1373158606     0.000     0.703    -0.168     0.000    -0.533     0.000   \n",
       "70  1373158606     0.000    -0.843     0.000    -0.867    -0.083     0.000   \n",
       "45  1373158606     0.000     0.000     0.244    -0.140     0.000     0.356   \n",
       "44  1373158606     0.729    -0.355     0.000     0.000     0.000    -0.178   \n",
       "\n",
       "    5-lstsq_lambda  4-lstsq_lambda  3-lstsq_lambda  2-lstsq_lambda  \\\n",
       "83           0.188           1.418          -1.083          -0.201   \n",
       "53          -0.372           1.525          -0.801           0.198   \n",
       "70           0.202          -1.307           0.376          -0.995   \n",
       "45           0.018          -0.048           0.048          -0.021   \n",
       "44           0.594          -0.028          -0.291           0.114   \n",
       "\n",
       "    1-lstsq_lambda  0-lstsq_lambda  5-lstsq_target  4-lstsq_target  \\\n",
       "83          -0.412           0.001           0.840           0.000   \n",
       "53          -0.554           0.001           0.000           0.703   \n",
       "70          -0.067          -0.000          -0.000          -0.843   \n",
       "45           0.066           0.334          -0.000           0.000   \n",
       "44          -0.018          -0.178           0.729          -0.355   \n",
       "\n",
       "    3-lstsq_target  2-lstsq_target  1-lstsq_target  0-lstsq_target   wb_0  \\\n",
       "83          -0.000          -0.545          -0.371          -0.000 -0.005   \n",
       "53          -0.168          -0.000          -0.533          -0.000 -0.005   \n",
       "70          -0.000          -0.867          -0.083           0.000 -0.005   \n",
       "45           0.244          -0.140          -0.000           0.356 -0.005   \n",
       "44           0.000          -0.000           0.000          -0.178 -0.005   \n",
       "\n",
       "     wb_1  wb_2  wb_3  wb_4  wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  \\\n",
       "83 -0.111 0.176 0.123 0.253 0.248 0.153 -0.146 0.106 0.183  0.237 -0.043   \n",
       "53 -0.111 0.158 0.106 0.187 0.174 0.135 -0.146 0.090 0.160  0.218 -0.043   \n",
       "70 -0.111 0.272 0.228 0.046 0.035 0.276 -0.146 0.213 0.351  0.354 -0.043   \n",
       "45 -0.111 0.141 0.092 0.120 0.089 0.120 -0.146 0.077 0.118  0.199 -0.043   \n",
       "44 -0.111 0.110 0.058 0.191 0.181 0.087 -0.146 0.042 0.100  0.170 -0.043   \n",
       "\n",
       "    wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  \\\n",
       "83 -0.009  0.177  0.259  0.230  0.000  0.293  0.250  0.268  0.301  0.078   \n",
       "53 -0.009  0.158  0.183  0.211  0.004  0.218  0.202  0.208  0.199  0.064   \n",
       "70 -0.009  0.289  0.034  0.338 -0.000  0.180  0.068  0.120  0.012  0.192   \n",
       "45 -0.009  0.142  0.087  0.193  0.020  0.143  0.163  0.151  0.038  0.016   \n",
       "44 -0.009  0.110  0.189  0.163  0.136  0.206  0.200  0.203  0.188  0.016   \n",
       "\n",
       "    wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  \\\n",
       "83  0.048 -0.021 -0.198 -0.044  0.084 -0.143 -0.102 -0.114  0.232  0.140   \n",
       "53  0.029 -0.021 -0.198 -0.044  0.070 -0.143 -0.102 -0.114  0.179  0.233   \n",
       "70  0.237 -0.021 -0.198 -0.044  0.192 -0.143 -0.102 -0.114  0.045  0.202   \n",
       "45 -0.000 -0.021 -0.198 -0.044  0.021 -0.143 -0.102 -0.114  0.119  0.191   \n",
       "44 -0.019 -0.021 -0.198 -0.044  0.022 -0.143 -0.102 -0.114  0.185  0.217   \n",
       "\n",
       "    wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  \\\n",
       "83  0.241  0.197 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.262  0.100   \n",
       "53  0.184  0.178 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.179  0.258   \n",
       "70  0.046  0.302 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.033  0.321   \n",
       "45  0.128  0.161 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.084  0.113   \n",
       "44  0.187  0.131 -0.040 -0.168 -0.149 -0.008 -0.108 -0.129  0.189  0.242   \n",
       "\n",
       "    wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  \\\n",
       "83  0.222 -0.097  0.206  0.105  0.048  0.243  0.238 -0.198 -0.019  0.110   \n",
       "53  0.201 -0.097  0.186  0.089  0.017  0.224  0.209 -0.198 -0.019  0.075   \n",
       "70  0.356 -0.097  0.340  0.216  0.272  0.356  0.072 -0.198 -0.019  0.320   \n",
       "45  0.179 -0.097  0.167  0.076  0.006  0.205  0.180 -0.198 -0.019  0.072   \n",
       "44  0.139 -0.097  0.131  0.041 -0.025  0.176  0.200 -0.198 -0.019  0.067   \n",
       "\n",
       "    wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  \\\n",
       "83 -0.190 -0.043 -0.162  0.246  0.240 -0.202 -0.024 -0.117  0.108  0.146   \n",
       "53 -0.190 -0.043 -0.162  0.227  0.221 -0.202 -0.024 -0.117  0.092  0.230   \n",
       "70 -0.190 -0.043 -0.162  0.353  0.360 -0.202 -0.024 -0.117  0.256  0.139   \n",
       "45 -0.190 -0.043 -0.162  0.208  0.202 -0.202 -0.024 -0.117  0.079  0.191   \n",
       "44 -0.190 -0.043 -0.162  0.179  0.172 -0.202 -0.024 -0.117  0.043  0.216   \n",
       "\n",
       "    wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  \\\n",
       "83  0.192  0.258  0.182 -0.057 -0.156  0.174  0.066 -0.115  0.210 -0.213   \n",
       "53  0.173  0.170  0.164 -0.057 -0.156  0.250  0.051 -0.115  0.171 -0.213   \n",
       "70  0.292  0.026  0.275 -0.057 -0.156  0.365  0.187 -0.115  0.404 -0.213   \n",
       "45  0.156  0.072  0.147 -0.057 -0.156  0.194  0.008 -0.115  0.189 -0.213   \n",
       "44  0.125  0.178  0.116 -0.057 -0.156  0.223  0.003 -0.115  0.205 -0.213   \n",
       "\n",
       "    wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78  wb_79  wb_80  wb_81  \\\n",
       "83 -0.006  0.175  0.008  0.156  0.183 -0.192  0.155  0.238 -0.056  0.116   \n",
       "53 -0.006  0.264  0.198  0.131  0.274 -0.192  0.237  0.217 -0.056  0.098   \n",
       "70 -0.006  0.356  0.004  0.335  0.380 -0.192  0.172  0.385 -0.056  0.214   \n",
       "45 -0.006  0.199  0.024  0.095  0.197 -0.192  0.200  0.184 -0.056  0.084   \n",
       "44 -0.006  0.228  0.180  0.085  0.226 -0.192  0.226  0.116 -0.056  0.050   \n",
       "\n",
       "    wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  wb_91  \\\n",
       "83  0.230 -0.020  0.150 -0.085  0.294 -0.197 -0.067  0.140  0.124 -0.196   \n",
       "53  0.179 -0.020  0.133 -0.085  0.224 -0.197 -0.067  0.124  0.106 -0.196   \n",
       "70  0.045 -0.020  0.280 -0.085  0.209 -0.197 -0.067  0.273  0.220 -0.196   \n",
       "45  0.121 -0.020  0.118 -0.085  0.147 -0.197 -0.067  0.110  0.092 -0.196   \n",
       "44  0.184 -0.020  0.084 -0.085  0.207 -0.197 -0.067  0.075  0.058 -0.196   \n",
       "\n",
       "    wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  wb_98  wb_99  wb_100  wb_101  \\\n",
       "83 -0.107  0.056 -0.061 -0.192  0.099 -0.185  0.148 -0.125  -0.157   0.080   \n",
       "53 -0.107  0.039 -0.061 -0.192  0.083 -0.185  0.232 -0.125  -0.157   0.061   \n",
       "70 -0.107  0.218 -0.061 -0.192  0.206 -0.185  0.179 -0.125  -0.157   0.265   \n",
       "45 -0.107  0.003 -0.061 -0.192  0.071 -0.185  0.194 -0.125  -0.157   0.023   \n",
       "44 -0.107 -0.010 -0.061 -0.192  0.035 -0.185  0.220 -0.125  -0.157   0.012   \n",
       "\n",
       "    wb_102  wb_103  wb_104  wb_105  wb_106  wb_107  wb_108  wb_109  wb_110  \\\n",
       "83   0.062  -0.215   0.218  -0.067  -0.129   0.219  -0.011  -0.208  -0.211   \n",
       "53   0.047  -0.215   0.153  -0.067  -0.129   0.200  -0.011  -0.208  -0.211   \n",
       "70   0.192  -0.215   0.017  -0.067  -0.129   0.324  -0.011  -0.208  -0.211   \n",
       "45   0.006  -0.215   0.055  -0.067  -0.129   0.182  -0.011  -0.208  -0.211   \n",
       "44  -0.001  -0.215   0.152  -0.067  -0.129   0.152  -0.011  -0.208  -0.211   \n",
       "\n",
       "    wb_111  wb_112  wb_113  wb_114  wb_115  wb_116  wb_117  wb_118  wb_119  \\\n",
       "83  -0.203   0.068  -0.041   0.145  -0.144   0.071   0.229   0.035  -0.003   \n",
       "53  -0.203   0.250  -0.041   0.126  -0.144   0.048   0.214   0.018  -0.001   \n",
       "70  -0.203   0.300  -0.041   0.240  -0.144   0.273   0.102   0.192  -0.003   \n",
       "45  -0.203   0.076  -0.041   0.111  -0.144   0.015   0.184  -0.005   0.012   \n",
       "44  -0.203   0.228  -0.041   0.078  -0.144  -0.001   0.208  -0.031   0.162   \n",
       "\n",
       "    wb_120  wb_121  wb_122  wb_123  wb_124  wb_125  wb_126  wb_127  wb_128  \\\n",
       "83   0.109   0.173   0.228  -0.081   0.242  -0.023  -0.093   0.176   0.000   \n",
       "53   0.093   0.245   0.154  -0.081   0.223  -0.023  -0.093   0.145   0.000   \n",
       "70   0.235   0.135   0.022  -0.081   0.360  -0.023  -0.093   0.362   0.000   \n",
       "45   0.080   0.216   0.064  -0.081   0.204  -0.023  -0.093   0.133   0.000   \n",
       "44   0.045   0.242   0.159  -0.081   0.174  -0.023  -0.093   0.137   0.000   \n",
       "\n",
       "    wb_129  wb_130  wb_131  wb_132  wb_133  wb_134  wb_135  wb_136  wb_137  \\\n",
       "83   0.000  -0.000  -0.009  -0.190  -0.207  -0.014   0.000  -0.013  -0.031   \n",
       "53   0.000  -0.000  -0.000  -0.137  -0.135  -0.000   0.000  -0.000  -0.001   \n",
       "70   0.000  -0.103  -0.131  -0.052  -0.043  -0.143   0.000  -0.151  -0.194   \n",
       "45   0.000  -0.000  -0.000   0.068   0.068  -0.000   0.000  -0.000  -0.000   \n",
       "44   0.000   0.041   0.041  -0.127  -0.144   0.038   0.000   0.041  -0.003   \n",
       "\n",
       "    wb_138  wb_139  wb_140  wb_141  wb_142  wb_143  wb_144  wb_145  wb_146  \\\n",
       "83  -0.000   0.000   0.000  -0.009  -0.200  -0.000  -0.015  -0.250  -0.149   \n",
       "53  -0.000   0.000   0.000  -0.000  -0.156  -0.000  -0.013  -0.164  -0.126   \n",
       "70  -0.048   0.000   0.000  -0.124  -0.042  -0.028  -0.016  -0.165  -0.014   \n",
       "45  -0.002   0.000   0.000  -0.000   0.069  -0.002   0.069   0.070   0.068   \n",
       "44   0.038   0.000   0.000   0.038  -0.145   0.040  -0.123  -0.122  -0.088   \n",
       "\n",
       "    wb_147  wb_148  wb_149  wb_150  wb_151  wb_152  wb_153  wb_154  wb_155  \\\n",
       "83  -0.195  -0.277  -0.021  -0.015   0.000   0.000   0.000  -0.020   0.000   \n",
       "53  -0.147  -0.178  -0.000  -0.001   0.000   0.000   0.000  -0.000   0.000   \n",
       "70  -0.057  -0.024  -0.159  -0.217   0.000   0.000   0.000  -0.156   0.000   \n",
       "45   0.069   0.075  -0.028  -0.015   0.000   0.000   0.000  -0.029   0.000   \n",
       "44  -0.106  -0.165   0.041   0.041   0.000   0.000   0.000   0.041   0.000   \n",
       "\n",
       "    wb_156  wb_157  wb_158  wb_159  wb_160  wb_161  wb_162  wb_163  wb_164  \\\n",
       "83   0.000   0.000  -0.156  -0.031  -0.169  -0.001   0.000   0.000   0.000   \n",
       "53   0.000   0.000  -0.123  -0.122  -0.125  -0.000   0.000   0.000   0.000   \n",
       "70   0.000   0.000  -0.053  -0.094  -0.055  -0.106   0.000   0.000   0.000   \n",
       "45   0.000   0.000   0.067   0.069   0.068  -0.001   0.000   0.000   0.000   \n",
       "44   0.000   0.000  -0.117  -0.001  -0.136   0.039   0.000   0.000   0.000   \n",
       "\n",
       "    wb_165  wb_166  wb_167  wb_168  wb_169  wb_170  wb_171  wb_172  wb_173  \\\n",
       "83   0.000   0.000   0.000  -0.243  -0.032  -0.017   0.000  -0.019  -0.014   \n",
       "53   0.000   0.000   0.000  -0.161  -0.212  -0.001   0.000  -0.001  -0.000   \n",
       "70   0.000   0.000   0.000  -0.040  -0.258  -0.160   0.000  -0.161  -0.153   \n",
       "45   0.000   0.000   0.000   0.069   0.075  -0.001   0.000  -0.001  -0.000   \n",
       "44   0.000   0.000   0.000  -0.140  -0.202   0.015   0.000   0.023   0.041   \n",
       "\n",
       "    wb_174  wb_175  wb_176  wb_177  wb_178  wb_179  wb_180  wb_181  wb_182  \\\n",
       "83  -0.016  -0.000  -0.131   0.000   0.000  -0.030   0.000   0.000   0.000   \n",
       "53  -0.002  -0.000  -0.119   0.000   0.000  -0.002   0.000   0.000   0.000   \n",
       "70  -0.239  -0.015  -0.014   0.000   0.000  -0.249   0.000   0.000   0.000   \n",
       "45  -0.012  -0.002   0.067   0.000   0.000   0.083   0.000   0.000   0.000   \n",
       "44   0.042   0.039  -0.001   0.000   0.000  -0.002   0.000   0.000   0.000   \n",
       "\n",
       "    wb_183  wb_184  wb_185  wb_186  wb_187  wb_188  wb_189  wb_190  wb_191  \\\n",
       "83  -0.000  -0.000   0.000   0.000   0.000  -0.026  -0.022  -0.000  -0.228   \n",
       "53  -0.000  -0.000   0.000   0.000   0.000  -0.000  -0.109  -0.000  -0.153   \n",
       "70  -0.001  -0.065   0.000   0.000   0.000  -0.203  -0.024  -0.074  -0.037   \n",
       "45  -0.003  -0.002   0.000   0.000   0.000  -0.000   0.068  -0.001   0.069   \n",
       "44   0.041   0.037   0.000   0.000   0.000   0.036  -0.001   0.041  -0.152   \n",
       "\n",
       "    wb_192  wb_193  wb_194  wb_195  wb_196  wb_197  wb_198  wb_199  wb_200  \\\n",
       "83  -0.000   0.000   0.000  -0.039  -0.019   0.000  -0.037   0.000   0.000   \n",
       "53  -0.000   0.000   0.000  -0.139  -0.000   0.000  -0.002   0.000   0.000   \n",
       "70  -0.088   0.000   0.000  -0.267  -0.171   0.000  -0.240   0.000   0.000   \n",
       "45  -0.001   0.000   0.000   0.072  -0.023   0.000   0.075   0.000   0.000   \n",
       "44   0.041   0.000   0.000  -0.001   0.041   0.000  -0.002   0.000   0.000   \n",
       "\n",
       "    wb_201  wb_202  wb_203  wb_204  wb_205  wb_206  wb_207  wb_208  wb_209  \\\n",
       "83  -0.035  -0.017  -0.033  -0.040   0.000  -0.020  -0.028   0.000  -0.003   \n",
       "53  -0.090  -0.175  -0.001  -0.073   0.000  -0.104  -0.001   0.000   0.000   \n",
       "70  -0.261  -0.019  -0.220  -0.259   0.000  -0.024  -0.155   0.000  -0.131   \n",
       "45   0.071   0.077   0.037   0.072   0.000   0.068  -0.001   0.000  -0.000   \n",
       "44  -0.001  -0.165  -0.002  -0.002   0.000  -0.001  -0.022   0.000   0.041   \n",
       "\n",
       "    wb_210  wb_211  wb_212  wb_213  wb_214  wb_215  wb_216  wb_217  wb_218  \\\n",
       "83  -0.147   0.000  -0.025   0.000  -0.240   0.000   0.000  -0.026  -0.000   \n",
       "53  -0.117   0.000  -0.000   0.000  -0.169   0.000   0.000  -0.000   0.000   \n",
       "70  -0.054   0.000  -0.182   0.000  -0.193   0.000   0.000  -0.185  -0.119   \n",
       "45   0.067   0.000  -0.000   0.000   0.070   0.000   0.000  -0.000  -0.000   \n",
       "44  -0.128   0.000   0.035   0.000  -0.117   0.000   0.000   0.035   0.041   \n",
       "\n",
       "    wb_219  wb_220  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  \\\n",
       "83   0.000   0.000  -0.017   0.000   0.000  -0.015   0.000  -0.023   0.000   \n",
       "53   0.000   0.000  -0.000   0.000   0.000  -0.000   0.000  -0.116   0.000   \n",
       "70   0.000   0.000  -0.199   0.000   0.000  -0.157   0.000  -0.029   0.000   \n",
       "45   0.000   0.000  -0.019   0.000   0.000   0.000   0.000   0.069   0.000   \n",
       "44   0.000   0.000   0.041   0.000   0.000   0.041   0.000  -0.001   0.000   \n",
       "\n",
       "    wb_228  wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  \\\n",
       "83   0.000  -0.024  -0.019   0.000  -0.175   0.000   0.000  -0.001   0.000   \n",
       "53   0.000  -0.001  -0.000   0.000  -0.123   0.000   0.000  -0.000   0.000   \n",
       "70   0.000  -0.227  -0.166   0.000  -0.031   0.000   0.000  -0.093   0.000   \n",
       "45   0.000  -0.024  -0.022   0.000   0.068   0.000   0.000  -0.001   0.000   \n",
       "44   0.000   0.033   0.041   0.000  -0.140   0.000   0.000   0.039   0.000   \n",
       "\n",
       "    wb_237  wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  \\\n",
       "83   0.000   0.000   0.000  -0.023   0.000  -0.001   0.000  -0.022  -0.113   \n",
       "53   0.000   0.000   0.000  -0.200   0.000  -0.000   0.000  -0.001  -0.128   \n",
       "70   0.000   0.000   0.000  -0.254   0.000  -0.119   0.000  -0.242  -0.019   \n",
       "45   0.000   0.000   0.000   0.078   0.000  -0.000   0.000  -0.021   0.068   \n",
       "44   0.000   0.000   0.000  -0.185   0.000   0.041   0.000   0.033  -0.000   \n",
       "\n",
       "    wb_246  wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  \\\n",
       "83  -0.011  -0.012  -0.023  -0.013  -0.199   0.000  -0.001   0.000   0.000   \n",
       "53  -0.000  -0.011  -0.000  -0.095  -0.131   0.000  -0.000   0.000   0.000   \n",
       "70  -0.176  -0.012  -0.180  -0.022  -0.034   0.000  -0.078   0.000   0.000   \n",
       "45  -0.006   0.073  -0.000   0.067   0.068   0.000  -0.002   0.000   0.000   \n",
       "44   0.043  -0.143   0.039  -0.000  -0.134   0.000   0.037   0.000   0.000   \n",
       "\n",
       "    wb_255  wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  \\\n",
       "83  -0.035  -0.125  -0.176  -0.174  -0.166   0.539   0.689  -0.094  -0.200   \n",
       "53  -0.002  -0.125  -0.176  -0.159  -0.152   0.340   0.408  -0.079  -0.200   \n",
       "70  -0.226  -0.125  -0.176  -0.325  -0.385   0.053   0.084  -0.295  -0.200   \n",
       "45   0.071  -0.125  -0.176  -0.145  -0.144   0.135   0.153  -0.067  -0.200   \n",
       "44  -0.002  -0.125  -0.176  -0.126  -0.128   0.360   0.473  -0.049  -0.200   \n",
       "\n",
       "    wb_264  wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  \\\n",
       "83  -0.176  -0.044  -0.111  -0.173  -0.178  -0.104   0.649  -0.148   0.189   \n",
       "53  -0.162  -0.026  -0.095  -0.173  -0.178  -0.089   0.399  -0.133   0.191   \n",
       "70  -0.438  -0.257  -0.226  -0.173  -0.178  -0.273   0.052  -0.255   0.189   \n",
       "45  -0.155  -0.009  -0.079  -0.173  -0.178  -0.076   0.122  -0.116   0.261   \n",
       "44  -0.143   0.002  -0.058  -0.173  -0.178  -0.056   0.444  -0.096   0.814   \n",
       "\n",
       "    wb_273  wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  \\\n",
       "83   0.476   0.398   0.419   0.934  -0.190  -0.039  -0.128  -0.188  -0.029   \n",
       "53   0.270   0.294   0.265   0.508  -0.177  -0.027  -0.128  -0.188  -0.029   \n",
       "70  -0.174   0.012  -0.067   0.019  -0.509  -0.501  -0.128  -0.188  -0.029   \n",
       "45   0.075   0.150   0.093   0.088  -0.147  -0.018  -0.128  -0.188  -0.029   \n",
       "44   0.249   0.245   0.230   0.597  -0.170  -0.058  -0.128  -0.188  -0.029   \n",
       "\n",
       "    wb_282  wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  \\\n",
       "83  -0.206   0.071  -0.126  -0.154   0.562   0.005   0.528  -0.125   0.193   \n",
       "53  -0.192   0.071  -0.126  -0.154   0.402   0.183   0.357  -0.110   0.193   \n",
       "70  -0.507   0.071  -0.126  -0.154   0.128  -0.109   0.090  -0.271   0.193   \n",
       "45  -0.159   0.071  -0.126  -0.154   0.206   0.071   0.175  -0.095   0.193   \n",
       "44  -0.182   0.071  -0.126  -0.154   0.417   0.075   0.403  -0.075   0.193   \n",
       "\n",
       "    wb_291  wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  \\\n",
       "83  -0.080  -0.141  -0.210   0.164  -0.062   0.717  -0.016  -0.058   0.007   \n",
       "53  -0.080  -0.141  -0.210   0.164  -0.062   0.405   0.315  -0.042   0.007   \n",
       "70  -0.080  -0.141  -0.210   0.164  -0.062   0.050  -0.339  -0.227   0.007   \n",
       "45  -0.080  -0.141  -0.210   0.164  -0.062   0.119   0.044  -0.025   0.007   \n",
       "44  -0.080  -0.141  -0.210   0.164  -0.062   0.437   0.392  -0.005   0.007   \n",
       "\n",
       "    wb_300  wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  \\\n",
       "83  -0.061  -0.160  -0.016  -0.128   0.384  -0.033   0.024  -0.029   0.114   \n",
       "53  -0.045  -0.146  -0.002  -0.113   0.322  -0.033   0.024  -0.010   0.114   \n",
       "70  -0.239  -0.426  -0.479  -0.234   0.059  -0.033   0.024  -0.358   0.114   \n",
       "45  -0.029  -0.139   0.008  -0.096   0.197  -0.033   0.024   0.037   0.114   \n",
       "44  -0.009  -0.128  -0.033  -0.075   0.197  -0.033   0.024   0.006   0.114   \n",
       "\n",
       "    wb_309  wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  \\\n",
       "83   0.054   0.120  -0.205  -0.097  -0.214   0.102   0.137  -0.071   0.037   \n",
       "53   0.054   0.120  -0.189  -0.082  -0.214   0.102   0.137  -0.054   0.210   \n",
       "70   0.054   0.120  -0.311  -0.217  -0.214   0.102   0.137  -0.368  -0.030   \n",
       "45   0.054   0.120  -0.172  -0.065  -0.214   0.102   0.137  -0.046   0.105   \n",
       "44   0.054   0.120  -0.151  -0.044  -0.214   0.102   0.137  -0.034   0.109   \n",
       "\n",
       "    wb_318  wb_319  wb_320  wb_321  wb_322  wb_323  wb_324  wb_325  wb_326  \\\n",
       "83  -0.172   0.784  -0.201  -0.045   0.133  -0.019  -0.168   0.111  -0.033   \n",
       "53  -0.157   0.450  -0.186  -0.045   0.133   0.153  -0.155   0.111  -0.012   \n",
       "70  -0.301   0.076  -0.338  -0.045   0.133  -0.260  -0.527   0.111  -0.255   \n",
       "45  -0.143   0.142  -0.172  -0.045   0.133   0.038  -0.132   0.111   0.025   \n",
       "44  -0.123   0.525  -0.153  -0.045   0.133   0.043  -0.160   0.111   0.030   \n",
       "\n",
       "    wb_327  wb_328  wb_329  wb_330  wb_331  wb_332  wb_333  wb_334  wb_335  \\\n",
       "83   0.070  -0.070  -0.017   0.020  -0.038  -0.022   0.111   0.023  -0.052   \n",
       "53   0.070  -0.070   0.125   0.557  -0.021   0.116   0.111   0.182  -0.035   \n",
       "70   0.070  -0.070  -0.250   0.019  -0.288  -0.257   0.111  -0.048  -0.214   \n",
       "45   0.070  -0.070   0.040   0.091   0.009   0.035   0.111   0.089  -0.017   \n",
       "44   0.070  -0.070   0.046   0.656   0.003   0.041   0.111   0.095  -0.001   \n",
       "\n",
       "    wb_336  wb_337  wb_338  wb_339  wb_340  wb_341  wb_342  wb_343  wb_344  \\\n",
       "83   0.069  -0.211   0.561   0.190  -0.076   0.196   0.442  -0.058   0.102   \n",
       "53   0.069  -0.198   0.412   0.190  -0.059   0.196   0.261  -0.058   0.102   \n",
       "70   0.069  -0.439   0.148   0.190  -0.305   0.196  -0.200  -0.058   0.102   \n",
       "45   0.069  -0.190   0.226   0.190  -0.048   0.196   0.068  -0.058   0.102   \n",
       "44   0.069  -0.176   0.456   0.190  -0.029   0.196   0.225  -0.058   0.102   \n",
       "\n",
       "    wb_345  wb_346  wb_347  wb_348  wb_349  wb_350  wb_351  wb_352  wb_353  \\\n",
       "83  -0.075  -0.228  -0.182  -0.190  -0.071   0.051  -0.189  -0.177  -0.015   \n",
       "53  -0.059  -0.214  -0.182  -0.190  -0.058   0.051  -0.189  -0.163  -0.015   \n",
       "70  -0.316  -0.433  -0.182  -0.190  -0.491   0.051  -0.189  -0.454  -0.015   \n",
       "45  -0.048  -0.206  -0.182  -0.190  -0.042   0.051  -0.189  -0.155  -0.015   \n",
       "44  -0.031  -0.190  -0.182  -0.190  -0.076   0.051  -0.189  -0.146  -0.015   \n",
       "\n",
       "    wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  wb_361  wb_362  \\\n",
       "83   0.017   0.199  -0.113  -0.043  -0.154  -0.196   0.903   0.081  -0.152   \n",
       "53   0.187   0.199  -0.113  -0.028  -0.141  -0.196   0.580   0.081  -0.152   \n",
       "70  -0.058   0.199  -0.113  -0.410  -0.526  -0.196   0.191   0.081  -0.152   \n",
       "45   0.083   0.199  -0.113   0.003  -0.121  -0.196   0.256   0.081  -0.152   \n",
       "44   0.089   0.199  -0.113  -0.020  -0.151  -0.196   0.670   0.081  -0.152   \n",
       "\n",
       "    wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  wb_370  wb_371  \\\n",
       "83  -0.116  -0.178   0.094  -0.048   0.069  -0.011  -0.115  -0.184  -0.056   \n",
       "53  -0.101  -0.178   0.094  -0.048   0.069   0.385  -0.115  -0.170  -0.056   \n",
       "70  -0.247  -0.178   0.094  -0.048   0.069  -0.399  -0.115  -0.369  -0.056   \n",
       "45  -0.085  -0.178   0.094  -0.048   0.069   0.049  -0.115  -0.159  -0.056   \n",
       "44  -0.064  -0.178   0.094  -0.048   0.069   0.449  -0.115  -0.141  -0.056   \n",
       "\n",
       "    wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  wb_379  wb_380  \\\n",
       "83  -0.033   0.266  -0.082   0.078  -0.106   0.063   0.888   0.215  -0.093   \n",
       "53  -0.018   0.255  -0.073   0.079  -0.090   0.204   0.551   0.215  -0.078   \n",
       "70  -0.432  -0.004  -0.591   0.078  -0.385  -0.006   0.177   0.215  -0.215   \n",
       "45   0.009   0.129  -0.087   0.153  -0.082   0.128   0.243   0.215  -0.061   \n",
       "44  -0.019   0.133  -0.139   0.751  -0.069   0.135   0.616   0.215  -0.040   \n",
       "\n",
       "    wb_381  wb_382  wb_383  wb_384  \n",
       "83  -0.131  -0.124  -0.034  -0.000  \n",
       "53  -0.131  -0.124  -0.015  -0.001  \n",
       "70  -0.131  -0.124  -0.270   0.000  \n",
       "45  -0.131  -0.124   0.024   0.066  \n",
       "44  -0.131  -0.124   0.013  -0.043  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:33.933488Z",
     "iopub.status.busy": "2021-11-08T21:44:33.933208Z",
     "iopub.status.idle": "2021-11-08T21:44:33.936787Z",
     "shell.execute_reply": "2021-11-08T21:44:33.936293Z",
     "shell.execute_reply.started": "2021-11-08T21:44:33.933459Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:33.937904Z",
     "iopub.status.busy": "2021-11-08T21:44:33.937637Z",
     "iopub.status.idle": "2021-11-08T21:44:33.942379Z",
     "shell.execute_reply": "2021-11-08T21:44:33.941872Z",
     "shell.execute_reply.started": "2021-11-08T21:44:33.937871Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[0].weight_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:33.943527Z",
     "iopub.status.busy": "2021-11-08T21:44:33.943238Z",
     "iopub.status.idle": "2021-11-08T21:44:37.964418Z",
     "shell.execute_reply": "2021-11-08T21:44:37.963849Z",
     "shell.execute_reply.started": "2021-11-08T21:44:33.943497Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------- TRAINING INTERPRETATION NET -----------------------------------------------\n",
      "Training Time: 0:00:00\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "((X_valid_list, y_valid_list), \n",
    " (X_test_list, y_test_list),\n",
    " history_list, \n",
    "\n",
    " #scores_valid_list,\n",
    " #scores_test_list, \n",
    "\n",
    " #function_values_valid_list, \n",
    " #function_values_test_list, \n",
    "\n",
    " #polynomial_dict_valid_list,\n",
    " #polynomial_dict_test_list,\n",
    "\n",
    " #distrib_dict_valid_list,\n",
    " #distrib_dict_test_list,\n",
    "\n",
    " model_list) = interpretation_net_training(lambda_net_train_dataset_list, \n",
    "                                           lambda_net_valid_dataset_list, \n",
    "                                           lambda_net_test_dataset_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:37.965655Z",
     "iopub.status.busy": "2021-11-08T21:44:37.965371Z",
     "iopub.status.idle": "2021-11-08T21:44:37.969855Z",
     "shell.execute_reply": "2021-11-08T21:44:37.969349Z",
     "shell.execute_reply.started": "2021-11-08T21:44:37.965634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r2']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inet_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:37.970798Z",
     "iopub.status.busy": "2021-11-08T21:44:37.970641Z",
     "iopub.status.idle": "2021-11-08T21:44:37.977932Z",
     "shell.execute_reply": "2021-11-08T21:44:37.977351Z",
     "shell.execute_reply.started": "2021-11-08T21:44:37.970779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mae'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:37.978946Z",
     "iopub.status.busy": "2021-11-08T21:44:37.978712Z",
     "iopub.status.idle": "2021-11-08T21:44:37.988210Z",
     "shell.execute_reply": "2021-11-08T21:44:37.987734Z",
     "shell.execute_reply.started": "2021-11-08T21:44:37.978921Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 385)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden1_4096 (Dense)            (None, 4096)         1581056     input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation1_relu (Activation)   (None, 4096)         0           hidden1_4096[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hidden2_2048 (Dense)            (None, 2048)         8390656     activation1_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation2_relu (Activation)   (None, 2048)         0           hidden2_2048[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hidden3_1024 (Dense)            (None, 1024)         2098176     activation2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation3_relu (Activation)   (None, 1024)         0           hidden3_1024[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hidden4_512 (Dense)             (None, 512)          524800      activation3_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation4_relu (Activation)   (None, 512)          0           hidden4_512[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output_coeff_3 (Dense)          (None, 3)            1539        activation4_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier1_6 (Dense)    (None, 6)            3078        activation4_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier2_6 (Dense)    (None, 6)            3078        activation4_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier3_6 (Dense)    (None, 6)            3078        activation4_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_combined (Concatenate)   (None, 21)           0           output_coeff_3[0][0]             \n",
      "                                                                 output_identifier1_6[0][0]       \n",
      "                                                                 output_identifier2_6[0][0]       \n",
      "                                                                 output_identifier3_6[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 12,605,461\n",
      "Trainable params: 12,605,461\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_list[-1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:37.989108Z",
     "iopub.status.busy": "2021-11-08T21:44:37.988892Z",
     "iopub.status.idle": "2021-11-08T21:44:37.993811Z",
     "shell.execute_reply": "2021-11-08T21:44:37.993287Z",
     "shell.execute_reply.started": "2021-11-08T21:44:37.989077Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#polynomial_dict_valid_list = []\n",
    "polynomial_dict_test_list = []  \n",
    "runtimes_list = []\n",
    "\n",
    "for lambda_net_valid_dataset, lambda_net_test_dataset in zip(lambda_net_valid_dataset_list, lambda_net_test_dataset_list):\n",
    "\n",
    "    #polynomial_dict_valid = {'lstsq_lambda_pred_polynomials': lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "    #                        'lstsq_target_polynomials': lambda_net_valid_dataset.lstsq_target_polynomial_list,\n",
    "    #                        'target_polynomials': lambda_net_valid_dataset.target_polynomial_list}    \n",
    "\n",
    "    polynomial_dict_test = {'lstsq_lambda_pred_polynomials': lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "                            'lstsq_target_polynomials': lambda_net_test_dataset.lstsq_target_polynomial_list,\n",
    "                            'target_polynomials': lambda_net_test_dataset.target_polynomial_list}    \n",
    "\n",
    "    #polynomial_dict_valid_list.append(polynomial_dict_valid)  \n",
    "    polynomial_dict_test_list.append(polynomial_dict_test)\n",
    "    runtimes_list.append({})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:37.995000Z",
     "iopub.status.busy": "2021-11-08T21:44:37.994734Z",
     "iopub.status.idle": "2021-11-08T21:44:38.287601Z",
     "shell.execute_reply": "2021-11-08T21:44:38.286808Z",
     "shell.execute_reply.started": "2021-11-08T21:44:37.994971Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------- PREDICT INET ------------------------------------------------------\n",
      "Predict Time: 0:00:00\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------- PREDICT INET ------------------------------------------------------')\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "for i, (X_test, model) in enumerate(zip(X_test_list, model_list)):\n",
    "    #y_test_pred = model.predict(X_test)    \n",
    "    #print(model.summary())\n",
    "    #print(X_test.shape)\n",
    "    y_test_pred, inet_runtime = make_inet_prediction(model, X_test, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    #print(y_test_pred.shape)   \n",
    "    polynomial_dict_test_list[i]['inet_polynomials'] = y_test_pred\n",
    "    runtimes_list[i]['inet_runtime'] =  np.array([inet_runtime/len(lambda_net_test_dataset.target_polynomial_list) for _ in range(len(lambda_net_test_dataset.target_polynomial_list))])\n",
    "    \n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Predict Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:38.289248Z",
     "iopub.status.busy": "2021-11-08T21:44:38.288966Z",
     "iopub.status.idle": "2021-11-08T21:44:39.222903Z",
     "shell.execute_reply": "2021-11-08T21:44:39.222171Z",
     "shell.execute_reply.started": "2021-11-08T21:44:38.289213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T21:44:39.224355Z",
     "iopub.status.busy": "2021-11-08T21:44:39.224038Z",
     "iopub.status.idle": "2021-11-08T22:19:37.427095Z",
     "shell.execute_reply": "2021-11-08T22:19:37.425392Z",
     "shell.execute_reply.started": "2021-11-08T21:44:39.224326Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- CALCULATE METAMODEL POLY -----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "100%|██████████| 1/1 [01:57<00:00, 117.53s/it]\n",
      "100%|██████████| 1/1 [02:10<00:00, 130.38s/it]\n",
      "100%|██████████| 1/1 [02:11<00:00, 131.89s/it]\n",
      "100%|██████████| 1/1 [02:12<00:00, 132.44s/it]\n",
      "100%|██████████| 1/1 [02:15<00:00, 135.17s/it]\n",
      "100%|██████████| 1/1 [02:19<00:00, 139.03s/it]\n",
      "100%|██████████| 10/10 [00:22<00:00,  2.26s/it][Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:  2.7min\n",
      "100%|██████████| 1/1 [02:20<00:00, 140.83s/it]\n",
      " 30%|███       | 3/10 [00:05<00:12,  1.78s/it]\n",
      " 50%|█████     | 5/10 [00:12<00:11,  2.37s/it][Parallel(n_jobs=12)]: Done   2 tasks      | elapsed:  2.8min\n",
      "\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.87s/it]\n",
      "100%|██████████| 10/10 [00:20<00:00,  2.02s/it][Parallel(n_jobs=12)]: Done   3 tasks      | elapsed:  2.9min\n",
      " 70%|███████   | 7/10 [00:17<00:07,  2.46s/it]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [02:37<00:00, 157.10s/it]\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.79s/it][Parallel(n_jobs=12)]: Done   4 tasks      | elapsed:  3.0min\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.66s/it][Parallel(n_jobs=12)]: Done   5 tasks      | elapsed:  3.0min\n",
      " 10%|█         | 1/10 [00:01<00:15,  1.74s/it]]\n",
      " 80%|████████  | 8/10 [00:19<00:05,  2.59s/it]\n",
      "100%|██████████| 1/1 [02:39<00:00, 159.62s/it]\n",
      "100%|██████████| 10/10 [00:20<00:00,  2.01s/it][Parallel(n_jobs=12)]: Done   6 tasks      | elapsed:  3.0min\n",
      " 20%|██        | 2/10 [00:04<00:17,  2.19s/it]]\n",
      "100%|██████████| 10/10 [00:25<00:00,  2.72s/it][Parallel(n_jobs=12)]: Done   7 tasks      | elapsed:  3.1min\n",
      " 80%|████████  | 8/10 [00:13<00:03,  1.77s/it]]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.75s/it][Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:  3.1min\n",
      " 80%|████████  | 8/10 [00:13<00:03,  1.55s/it]\n",
      "100%|██████████| 1/1 [02:51<00:00, 171.64s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.64s/it][Parallel(n_jobs=12)]: Done   9 tasks      | elapsed:  3.2min\n",
      " 70%|███████   | 7/10 [00:15<00:06,  2.22s/it]\n",
      " 50%|█████     | 5/10 [00:07<00:07,  1.45s/it][Parallel(n_jobs=12)]: Done  10 tasks      | elapsed:  3.3min\n",
      "100%|██████████| 10/10 [00:22<00:00,  2.26s/it]\n",
      "100%|██████████| 10/10 [00:22<00:00,  2.08s/it][Parallel(n_jobs=12)]: Done  11 tasks      | elapsed:  3.4min\n",
      " 70%|███████   | 7/10 [00:10<00:04,  1.60s/it]]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.70s/it][Parallel(n_jobs=12)]: Done  12 tasks      | elapsed:  3.5min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.60s/it]\n",
      "100%|██████████| 1/1 [02:10<00:00, 130.66s/it]\n",
      "100%|██████████| 1/1 [02:05<00:00, 125.69s/it]\n",
      "100%|██████████| 1/1 [02:04<00:00, 124.67s/it]\n",
      " 40%|████      | 4/10 [00:05<00:07,  1.33s/it][Parallel(n_jobs=12)]: Done  13 tasks      | elapsed:  5.1min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.60s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.63s/it][Parallel(n_jobs=12)]: Done  14 tasks      | elapsed:  5.3min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n",
      "100%|██████████| 1/1 [02:18<00:00, 138.14s/it][Parallel(n_jobs=12)]: Done  15 tasks      | elapsed:  5.3min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.57s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [02:21<00:00, 141.51s/it]\n",
      "100%|██████████| 1/1 [02:37<00:00, 157.79s/it]\n",
      " 60%|██████    | 6/10 [00:09<00:06,  1.73s/it]\n",
      " 30%|███       | 3/10 [00:05<00:12,  1.85s/it][Parallel(n_jobs=12)]: Done  16 tasks      | elapsed:  5.6min\n",
      " 40%|████      | 4/10 [00:06<00:09,  1.66s/it]]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:  5.6min\n",
      " 40%|████      | 4/10 [00:07<00:10,  1.82s/it]]\n",
      "100%|██████████| 1/1 [02:27<00:00, 147.36s/it]\n",
      "100%|██████████| 1/1 [02:23<00:00, 143.20s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.46s/it][Parallel(n_jobs=12)]: Done  18 tasks      | elapsed:  5.8min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.59s/it]\n",
      " 40%|████      | 4/10 [00:06<00:09,  1.64s/it][Parallel(n_jobs=12)]: Done  19 tasks      | elapsed:  5.8min\n",
      " 10%|█         | 1/10 [00:01<00:09,  1.01s/it]]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.69s/it][Parallel(n_jobs=12)]: Done  20 tasks      | elapsed:  5.9min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.64s/it]\n",
      "100%|██████████| 1/1 [02:48<00:00, 168.37s/it]\n",
      "100%|██████████| 1/1 [02:31<00:00, 151.87s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.63s/it][Parallel(n_jobs=12)]: Done  21 tasks      | elapsed:  6.0min\n",
      " 30%|███       | 3/10 [00:04<00:11,  1.64s/it]\n",
      " 70%|███████   | 7/10 [00:11<00:04,  1.63s/it]\n",
      " 90%|█████████ | 9/10 [00:14<00:01,  1.73s/it][Parallel(n_jobs=12)]: Done  22 tasks      | elapsed:  6.2min\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.74s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.72s/it][Parallel(n_jobs=12)]: Done  23 tasks      | elapsed:  6.3min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it]\n",
      " 30%|███       | 3/10 [00:15<00:34,  4.95s/it][Parallel(n_jobs=12)]: Done  24 tasks      | elapsed:  6.5min\n",
      " 30%|███       | 3/10 [00:16<00:38,  5.48s/it]\n",
      "100%|██████████| 1/1 [02:19<00:00, 139.34s/it]\n",
      "100%|██████████| 1/1 [02:21<00:00, 141.48s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.68s/it][Parallel(n_jobs=12)]: Done  25 tasks      | elapsed:  7.7min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n",
      "100%|██████████| 1/1 [02:11<00:00, 131.10s/it]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.53s/it][Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:  7.9min\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.39s/it]\n",
      " 60%|██████    | 6/10 [00:08<00:06,  1.58s/it]\n",
      "100%|██████████| 1/1 [02:41<00:00, 161.43s/it]\n",
      "100%|██████████| 1/1 [02:15<00:00, 135.24s/it]\n",
      " 40%|████      | 4/10 [00:06<00:09,  1.61s/it][Parallel(n_jobs=12)]: Done  27 tasks      | elapsed:  8.1min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n",
      " 60%|██████    | 6/10 [00:09<00:06,  1.66s/it]\n",
      " 10%|█         | 1/10 [00:05<00:52,  5.83s/it][Parallel(n_jobs=12)]: Done  28 tasks      | elapsed:  8.2min\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Parallel(n_jobs=12)]: Done  29 tasks      | elapsed:  8.2min\n",
      " 80%|████████  | 8/10 [00:13<00:03,  1.82s/it]]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.81s/it][Parallel(n_jobs=12)]: Done  30 tasks      | elapsed:  8.3min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.82s/it][Parallel(n_jobs=12)]: Done  31 tasks      | elapsed:  8.3min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it]\n",
      "100%|██████████| 1/1 [02:21<00:00, 141.52s/it]\n",
      "100%|██████████| 1/1 [02:07<00:00, 127.51s/it]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.40s/it][Parallel(n_jobs=12)]: Done  32 tasks      | elapsed:  8.6min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it][Parallel(n_jobs=12)]: Done  33 tasks      | elapsed:  8.6min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.44s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [02:45<00:00, 165.92s/it]\n",
      "100%|██████████| 1/1 [02:22<00:00, 142.46s/it]\n",
      " 50%|█████     | 5/10 [00:06<00:06,  1.39s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.84s/it][Parallel(n_jobs=12)]: Done  34 tasks      | elapsed:  9.0min\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.74s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.61s/it][Parallel(n_jobs=12)]: Done  35 tasks      | elapsed:  9.1min\n",
      " 50%|█████     | 5/10 [00:07<00:08,  1.62s/it]]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.72s/it][Parallel(n_jobs=12)]: Done  36 tasks      | elapsed:  9.2min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.65s/it]\n",
      "100%|██████████| 1/1 [02:16<00:00, 136.90s/it]\n",
      "100%|██████████| 1/1 [02:08<00:00, 128.58s/it]\n",
      "100%|██████████| 1/1 [02:14<00:00, 134.25s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.66s/it][Parallel(n_jobs=12)]: Done  37 tasks      | elapsed: 10.6min\n",
      " 90%|█████████ | 9/10 [00:14<00:01,  1.68s/it]]\n",
      "100%|██████████| 1/1 [02:19<00:00, 139.91s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it][Parallel(n_jobs=12)]: Done  38 tasks      | elapsed: 10.7min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.62s/it]\n",
      " 90%|█████████ | 9/10 [00:15<00:01,  1.75s/it]\n",
      " 50%|█████     | 5/10 [00:06<00:07,  1.47s/it][Parallel(n_jobs=12)]: Done  39 tasks      | elapsed: 10.8min\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.75s/it]\n",
      "100%|██████████| 1/1 [02:30<00:00, 150.29s/it]\n",
      " 60%|██████    | 6/10 [00:10<00:06,  1.72s/it][Parallel(n_jobs=12)]: Done  40 tasks      | elapsed: 10.9min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it]\n",
      "100%|██████████| 1/1 [01:58<00:00, 118.55s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.67s/it][Parallel(n_jobs=12)]: Done  41 tasks      | elapsed: 11.0min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.66s/it][Parallel(n_jobs=12)]: Done  42 tasks      | elapsed: 11.1min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.59s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.64s/it][Parallel(n_jobs=12)]: Done  43 tasks      | elapsed: 11.3min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.60s/it]\n",
      "100%|██████████| 1/1 [02:44<00:00, 164.45s/it]\n",
      "100%|██████████| 1/1 [02:19<00:00, 139.76s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.70s/it][Parallel(n_jobs=12)]: Done  44 tasks      | elapsed: 11.7min\n",
      "\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it][Parallel(n_jobs=12)]: Done  45 tasks      | elapsed: 11.7min\n",
      "\n",
      "100%|██████████| 1/1 [02:48<00:00, 168.26s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.73s/it][Parallel(n_jobs=12)]: Done  46 tasks      | elapsed: 12.3min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.67s/it]\n",
      "100%|██████████| 1/1 [02:10<00:00, 130.67s/it]\n",
      " 60%|██████    | 6/10 [00:07<00:05,  1.38s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.58s/it][Parallel(n_jobs=12)]: Done  47 tasks      | elapsed: 13.1min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.45s/it]\n",
      " 50%|█████     | 5/10 [00:07<00:07,  1.51s/it]\n",
      " 40%|████      | 4/10 [00:05<00:08,  1.40s/it]\n",
      "100%|██████████| 1/1 [02:07<00:00, 127.21s/it][Parallel(n_jobs=12)]: Done  48 tasks      | elapsed: 13.2min\n",
      "\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.53s/it]\n",
      " 70%|███████   | 7/10 [00:10<00:05,  1.67s/it]\n",
      " 60%|██████    | 6/10 [00:10<00:07,  1.88s/it][Parallel(n_jobs=12)]: Done  49 tasks      | elapsed: 13.3min\n",
      " 50%|█████     | 5/10 [00:08<00:08,  1.77s/it]]\n",
      " 70%|███████   | 7/10 [00:11<00:04,  1.67s/it][Parallel(n_jobs=12)]: Done  50 tasks      | elapsed: 13.5min\n",
      " 90%|█████████ | 9/10 [00:15<00:01,  1.78s/it]]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.75s/it][Parallel(n_jobs=12)]: Done  51 tasks      | elapsed: 13.5min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.69s/it][Parallel(n_jobs=12)]: Done  52 tasks      | elapsed: 13.5min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.62s/it]\n",
      "100%|██████████| 1/1 [02:46<00:00, 166.12s/it]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.41s/it][Parallel(n_jobs=12)]: Done  53 tasks      | elapsed: 13.9min\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [02:14<00:00, 134.39s/it]\n",
      " 90%|█████████ | 9/10 [00:14<00:01,  1.66s/it][Parallel(n_jobs=12)]: Done  54 tasks      | elapsed: 14.2min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.61s/it]\n",
      "100%|██████████| 1/1 [02:53<00:00, 173.91s/it]\n",
      "100%|██████████| 1/1 [02:17<00:00, 137.11s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it][Parallel(n_jobs=12)]: Done  55 tasks      | elapsed: 14.9min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.61s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.77s/it][Parallel(n_jobs=12)]: Done  56 tasks      | elapsed: 14.9min\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.72s/it]\n",
      "100%|██████████| 1/1 [02:04<00:00, 124.70s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it][Parallel(n_jobs=12)]: Done  57 tasks      | elapsed: 15.4min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.62s/it]\n",
      "100%|██████████| 1/1 [02:13<00:00, 133.14s/it]\n",
      "100%|██████████| 1/1 [02:07<00:00, 127.48s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.58s/it][Parallel(n_jobs=12)]: Done  58 tasks      | elapsed: 15.7min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.50s/it]\n",
      "100%|██████████| 1/1 [02:12<00:00, 132.93s/it]\n",
      " 40%|████      | 4/10 [00:06<00:09,  1.64s/it][Parallel(n_jobs=12)]: Done  59 tasks      | elapsed: 15.8min\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.36s/it]\n",
      "100%|██████████| 1/1 [02:35<00:00, 155.48s/it]\n",
      " 10%|█         | 1/10 [00:01<00:10,  1.17s/it][Parallel(n_jobs=12)]: Done  60 tasks      | elapsed: 16.0min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.69s/it]\n",
      "100%|██████████| 1/1 [02:12<00:00, 132.72s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.77s/it][Parallel(n_jobs=12)]: Done  61 tasks      | elapsed: 16.2min\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.72s/it]\n",
      "100%|██████████| 1/1 [02:45<00:00, 165.85s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it][Parallel(n_jobs=12)]: Done  62 tasks      | elapsed: 16.4min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.73s/it][Parallel(n_jobs=12)]: Done  63 tasks      | elapsed: 16.6min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.67s/it]\n",
      "100%|██████████| 1/1 [02:22<00:00, 142.84s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it][Parallel(n_jobs=12)]: Done  64 tasks      | elapsed: 16.8min\n",
      "\n",
      "100%|██████████| 1/1 [02:31<00:00, 151.13s/it]\n",
      "100%|██████████| 1/1 [02:32<00:00, 152.33s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.74s/it][Parallel(n_jobs=12)]: Done  65 tasks      | elapsed: 17.7min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it]\n",
      "100%|██████████| 1/1 [02:17<00:00, 137.26s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.59s/it][Parallel(n_jobs=12)]: Done  66 tasks      | elapsed: 17.7min\n",
      "\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.66s/it][Parallel(n_jobs=12)]: Done  67 tasks      | elapsed: 18.0min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it]\n",
      "100%|██████████| 1/1 [02:10<00:00, 130.95s/it]\n",
      "100%|██████████| 1/1 [02:37<00:00, 157.86s/it][Parallel(n_jobs=12)]: Done  68 tasks      | elapsed: 18.5min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.62s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [02:14<00:00, 134.92s/it]\n",
      "100%|██████████| 1/1 [02:05<00:00, 125.90s/it]\n",
      " 10%|█         | 1/10 [00:00<00:08,  1.08it/s][Parallel(n_jobs=12)]: Done  69 tasks      | elapsed: 18.7min\n",
      " 80%|████████  | 8/10 [00:13<00:03,  1.72s/it]]\n",
      " 30%|███       | 3/10 [00:04<00:09,  1.42s/it][Parallel(n_jobs=12)]: Done  70 tasks      | elapsed: 18.8min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it]\n",
      "100%|██████████| 1/1 [02:25<00:00, 145.93s/it]\n",
      " 80%|████████  | 8/10 [00:11<00:03,  1.54s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.49s/it][Parallel(n_jobs=12)]: Done  71 tasks      | elapsed: 18.9min\n",
      "\n",
      " 20%|██        | 2/10 [00:08<00:34,  4.30s/it][Parallel(n_jobs=12)]: Done  72 tasks      | elapsed: 19.0min\n",
      "\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.61s/it][Parallel(n_jobs=12)]: Done  73 tasks      | elapsed: 19.1min\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it]\n",
      "100%|██████████| 1/1 [02:32<00:00, 152.53s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.70s/it][Parallel(n_jobs=12)]: Done  74 tasks      | elapsed: 19.7min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it]\n",
      "100%|██████████| 1/1 [02:17<00:00, 137.02s/it]\n",
      "100%|██████████| 1/1 [02:29<00:00, 149.63s/it]\n",
      " 30%|███       | 3/10 [00:03<00:09,  1.31s/it][Parallel(n_jobs=12)]: Done  75 tasks      | elapsed: 20.3min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it]\n",
      " 90%|█████████ | 9/10 [00:13<00:01,  1.52s/it][Parallel(n_jobs=12)]: Done  76 tasks      | elapsed: 20.4min\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n",
      "100%|██████████| 1/1 [02:42<00:00, 162.72s/it]\n",
      "100%|██████████| 1/1 [05:00<00:00, 300.68s/it]\n",
      " 50%|█████     | 5/10 [00:07<00:08,  1.67s/it]]\n",
      "100%|██████████| 1/1 [02:15<00:00, 135.59s/it][Parallel(n_jobs=12)]: Done  77 tasks      | elapsed: 21.0min\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.70s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.67s/it]\n",
      "100%|██████████| 1/1 [02:15<00:00, 135.87s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n",
      "100%|██████████| 1/1 [02:10<00:00, 130.49s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.62s/it]\n",
      "100%|██████████| 1/1 [02:12<00:00, 132.93s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.67s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.72s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.72s/it]\n",
      "100%|██████████| 1/1 [02:40<00:00, 160.48s/it]\n",
      "100%|██████████| 1/1 [02:06<00:00, 126.68s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.69s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/it]\n",
      "100%|██████████| 1/1 [02:16<00:00, 136.31s/it]\n",
      " 20%|██        | 2/10 [00:02<00:11,  1.43s/it]\n",
      "100%|██████████| 10/10 [00:19<00:00,  1.93s/it]\n",
      " 90%|█████████ | 9/10 [00:17<00:01,  1.93s/it][Parallel(n_jobs=12)]: Done  87 out of 100 | elapsed: 23.1min remaining:  3.5min\n",
      "100%|██████████| 10/10 [00:19<00:00,  1.93s/it]\n",
      "100%|██████████| 1/1 [02:14<00:00, 134.74s/it]\n",
      "100%|██████████| 1/1 [02:22<00:00, 142.37s/it]\n",
      "100%|██████████| 1/1 [02:16<00:00, 136.99s/it]\n",
      "100%|██████████| 1/1 [02:13<00:00, 133.27s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.65s/it]\n",
      "100%|██████████| 1/1 [02:21<00:00, 141.48s/it]]\n",
      "100%|██████████| 1/1 [02:18<00:00, 138.91s/it]]\n",
      "100%|██████████| 1/1 [02:11<00:00, 131.67s/it]\n",
      "100%|██████████| 1/1 [02:16<00:00, 136.92s/it]]\n",
      "100%|██████████| 1/1 [01:51<00:00, 111.26s/it]]\n",
      "100%|██████████| 1/1 [01:55<00:00, 115.16s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.25s/it][Parallel(n_jobs=12)]: Done  97 out of 100 | elapsed: 25.2min remaining:   46.8s\n",
      "100%|██████████| 1/1 [01:47<00:00, 107.35s/it]]\n",
      "100%|██████████| 1/1 [20:04<00:00, 1204.73s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.20s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [13:15<00:00, 795.52s/it]\n",
      " 90%|█████████ | 9/10 [00:10<00:01,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metamodel Poly Optimization Time: 0:34:58\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.25s/it][Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed: 35.0min finished\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "if symbolic_metamodeling_poly_evaluation:\n",
    "    print('-------------------------------------------------- CALCULATE METAMODEL POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test, metamodel_runtimes = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_poly'] = metamodel_functions_test     \n",
    "        runtimes_list[i]['metamodel_poly_runtime'] = metamodel_runtimes\n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Poly Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T22:19:37.431177Z",
     "iopub.status.busy": "2021-11-08T22:19:37.430175Z",
     "iopub.status.idle": "2021-11-08T22:19:38.623938Z",
     "shell.execute_reply": "2021-11-08T22:19:38.621907Z",
     "shell.execute_reply.started": "2021-11-08T22:19:37.431104Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.19s/it]"
     ]
    }
   ],
   "source": [
    "if symbolic_metamodeling_evaluation:\n",
    "    print('---------------------------------------------------- CALCULATE METAMODEL --------------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test, metamodel_runtimes = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=False)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions'] = metamodel_functions_test       \n",
    "        runtimes_list[i]['metamodel_functions_runtime'] = metamodel_runtimes\n",
    "        \n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T22:19:38.627803Z",
     "iopub.status.busy": "2021-11-08T22:19:38.627170Z",
     "iopub.status.idle": "2021-11-08T22:19:38.729637Z",
     "shell.execute_reply": "2021-11-08T22:19:38.728678Z",
     "shell.execute_reply.started": "2021-11-08T22:19:38.627745Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_function_evaluation:\n",
    "    print('----------------------------------------------- CALCULATE METAMODEL FUNCTION ----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test, metamodel_runtimes = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=True, force_polynomial=False)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions_no_GD'] = metamodel_functions_test   \n",
    "        runtimes_list[i]['metamodel_functions_no_GD_runtime'] = metamodel_runtimes\n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Function Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-11-08T22:19:38.731321Z",
     "iopub.status.busy": "2021-11-08T22:19:38.731075Z",
     "iopub.status.idle": "2021-11-09T05:21:36.947815Z",
     "shell.execute_reply": "2021-11-09T05:21:36.946273Z",
     "shell.execute_reply.started": "2021-11-08T22:19:38.731296Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------- CALCULATE METAMODEL POLY FUNCTION ----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=12)]: Done   2 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=12)]: Done   3 tasks      | elapsed: 38.4min\n",
      "[Parallel(n_jobs=12)]: Done   4 tasks      | elapsed: 45.6min\n",
      "[Parallel(n_jobs=12)]: Done   5 tasks      | elapsed: 47.1min\n",
      "[Parallel(n_jobs=12)]: Done   6 tasks      | elapsed: 48.9min\n",
      "[Parallel(n_jobs=12)]: Done   7 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=12)]: Done   9 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=12)]: Done  10 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=12)]: Done  11 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=12)]: Done  12 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=12)]: Done  13 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=12)]: Done  14 tasks      | elapsed: 63.0min\n",
      "[Parallel(n_jobs=12)]: Done  15 tasks      | elapsed: 70.5min\n",
      "[Parallel(n_jobs=12)]: Done  16 tasks      | elapsed: 73.9min\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed: 78.7min\n",
      "[Parallel(n_jobs=12)]: Done  18 tasks      | elapsed: 83.2min\n",
      "[Parallel(n_jobs=12)]: Done  19 tasks      | elapsed: 85.8min\n",
      "[Parallel(n_jobs=12)]: Done  20 tasks      | elapsed: 89.9min\n",
      "[Parallel(n_jobs=12)]: Done  21 tasks      | elapsed: 105.7min\n",
      "[Parallel(n_jobs=12)]: Done  22 tasks      | elapsed: 107.1min\n",
      "[Parallel(n_jobs=12)]: Done  23 tasks      | elapsed: 113.6min\n",
      "[Parallel(n_jobs=12)]: Done  24 tasks      | elapsed: 118.4min\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sci[Parallel(n_jobs=12)]: Done  25 tasks      | elapsed: 120.2min\n",
      "py/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-3451>\", line 2, in _lambdifygenerated\n",
      "    return (x**15*(-0.0135716174860377 + 0.0404268688472111*1j) + x**14*(0.110652178221367 - 0.329608545275869*1j) + x**13*(-0.424225795516083 + 1.26367550622292*1j) + x**12*(1.01606357649336 - 3.02663031798414*1j) + x**11*(-1.70367640659529 + 5.0748779737104*1j) + x**10*(2.12426115156342 - 6.3277075897423*1j) + x**9*(-2.04284094892808 + 6.08517468186758*1j) + x**8*(1.55208036792771 - 4.62330665737499*1j) + x**7*(-0.948340252761133 + 2.82489740521698*1j) + x**6*(0.473059557095762 - 1.40914056053414*1j) + x**5*(-0.200358675563211 + 0.596824505828278*1j) + x**4*(0.0631747649067674 - 0.188183754660545*1j) + x**3*(-0.0534479886894938 + 0.159209823819477*1j) + x**2*(-0.00376669952389538 + 0.0112201709041705*1j) + x*(9.94447173730956e-5 - 0.000296223980002855*1j) - 2.02542832406991e-6 + 6.0333062953527e-6*1j)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(mod[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed: 120.2min\n",
      "el, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 249, in fun\n",
      "    self._update_fun()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-6928>\", line 2, in _lambdifygenerated\n",
      "    return (x**15*(-0.0205484091016267 + 0.118098803586597*1j) + x**14*(0.167526588092146 - 0.96283315777769*1j) + x**13*(-0.642237841743159 + 3.69116267603903*1j) + x**12*(1.53812902145332 - 8.84015868562202*1j) + x**11*(-2.57887788200221 + 14.8217018141948*1j) + x**10*(3.21531973441654 - 18.4795529378918*1j) + x**9*(-3.0919246664971 + 17.7703588675528*1j) + x**8*(2.34914787591765 - 13.5013641310026*1j) + x**7*(-1.43540605681314 + 8.24977433202682*1j) + x**6*(0.717178432149225 - 4.12187213015093*1j) + x**5*(-0.300627113850264 + 1.72780784613623*1j) + x**4*(0.107648987005943 - 0.618695905353791*1j) + x**3*(-0.0684315756391393 + 0.393299898330731*1j) + x**2*(-0.00607098041787208 + 0.034892021100144*1j) + x*(0.000153293880853583 - 0.000881032873952303*1j) - 3.09695952804487e-6 + 1.77992959556617e-5*1j)\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/[Parallel(n_jobs=12)]: Done  27 tasks      | elapsed: 120.2min\n",
      "smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1720, in predict\n",
      "    data_handler = data_adapter.get_data_handler(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1383, in get_data_handler\n",
      "    return DataHandler(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1138, in __init__\n",
      "    self._adapter = adapter_cls(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 286, in __init__\n",
      "    indices_dataset = indices_dataset.map(permutation).prefetch(1)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1172, in prefetch\n",
      "    return PrefetchDataset(self, buffer_size)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 5256, in __init__\n",
      "    super(PrefetchDataset, self).__init__(input_dataset, variant_tensor)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3812, in __init__\n",
      "    super(UnaryUnchangedStructureDataset, self).__init__(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3801, in __init__\n",
      "    super(UnaryDataset, self).__init__(variant_tensor)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 240, in __init__\n",
      "    self._options_attr = self._options_attr.merge(input_options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/[Parallel(n_jobs=12)]: Done  28 tasks      | elapsed: 120.2min\n",
      "dataset_ops.py\", line 3786, in merge\n",
      "    return options_lib.merge_options(self, options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/util/options.py\", line 161, in merge_options\n",
      "    default = getattr(default_options, name)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/util/options.py\", line 105, in get_fn\n",
      "    option._options[name] = default_factory()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/util/options.py\", line 39, in __init__\n",
      "    object.__setattr__(self, \"_options\", {})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 144, in evaluate\n",
      "    'cython': lambdify([x], self.approx_expression(), modules=['math']), #ufuncify([x], self.approx_expression()),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 637, in simplify\n",
      "    expr2 = shorter(together(expr, deep=True), together(expr1, deep=True))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 84, in together\n",
      "    return _together(sympify(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 67, in _together\n",
      "    return gcd_terms(list(map(_together, Add.make_args(expr))), fraction=fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 78, in _together\n",
      "    return expr.__class__(*[ _together(arg) for arg in expr.args ])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 78, in <listcomp>\n",
      "    return expr.__class__(*[ _together(arg) for arg in expr.args ])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 67, in _together\n",
      "    return gcd_terms(list(map(_together, Add.make_args(expr))), fraction=fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 1077, in gcd_terms\n",
      "    cont, numer, denom = _gcd_terms(terms, isprimitive, fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 962, in _gcd_terms\n",
      "    terms[i] = term.quo(cont)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 877, in quo\n",
      "    return self.mul(other.inv())\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 867, in mul\n",
      "    denom = self.denom.mul(other.denom)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 479, in mul\n",
      "    if any(f.is_zero for f in (self, other)):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1751, in predict\n",
      "    tmp_batch_outputs = self.predict_function(iterator)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3039, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 591, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 249, in fun\n",
      "    self._update_fun()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/utilities/lambdify.py\", line 883, in lambdify\n",
      "    expr_str = str(expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/_print_helpers.py\", line 29, in __str__\n",
      "    return sstr(self, order=None)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/printing/printer.py\", line 373, in __call__\n",
      "    return self.__wrapped__(*args, **kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 454, in getit\n",
      "    return self._assumptions[fact]\n",
      "KeyError: 'zero'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/[Parallel(n_jobs=12)]: Done  29 tasks      | elapsed: 123.0min\n",
      "smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 635, in simplify\n",
      "    _e = cancel(expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polytools.py\", line 6706, in cancel\n",
      "    R, (F, G) = sring((p, q), *gens, **args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 164, in sring\n",
      "    reps, opt = _parallel_dict_from_expr(exprs, opt)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 326, in _parallel_dict_from_expr\n",
      "    exprs = [ expr.expand() for expr in exprs ]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 326, in <listcomp>\n",
      "    exprs = [ expr.expand() for expr in exprs ]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 72, in wrapper\n",
      "    retval = cfunc(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3598, in expand\n",
      "    expr, hit = Expr._expand_hint(expr, hint, deep=deep, **hints)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3531, in _expand_hint\n",
      "    arg, arghit = Expr._expand_hint(arg, hint, **hints)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3531, in _expand_hint\n",
      "    arg, arghit = Expr._expand_hint(arg, hint, **hints)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3539, in _expand_hint\n",
      "    newexpr = getattr(expr, hint)(**hints)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/power.py\", line 1207, in _eval_expand_multinomial\n",
      "    return basic_from_dict(expansion_dict, *p)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 387, in expr_from_dict\n",
      "    return Add(*result)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 72, in wrapper\n",
      "    retval = cfunc(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/operations.py\", line 85, in __new__\n",
      "    c_part, nc_part, order_symbols = cls.flatten(args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/add.py\", line 313, in flatten\n",
      "    newseq.append(Mul(c, s))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 72, in wrapper\n",
      "    retval = cfunc(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/operations.py\", line 85, in __new__\n",
      "    c_part, nc_part, order_symbols = cls.flatten(args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/mul.py\", line 672, in flatten\n",
      "    elif coeff.is_zero:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 458, in getit\n",
      "    return _ask(fact, self)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 503, in _ask\n",
      "    assumptions.deduce_all_facts(((fact, a),))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/facts.py\", line 532, in deduce_all_facts\n",
      "    bcond, bimpl = beta_rules[bidx]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sci[Parallel(n_jobs=12)]: Done  30 tasks      | elapsed: 130.5min\n",
      "py/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 456, in _hyp2f1\n",
      "    v = ctx.hypercomb(h, [a,b], **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 443, in _hyp2f1\n",
      "    return ctx.hypsum(2, 1, (atype, btype, ctype), [a, b, c], z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp.py\", line 714, in hypsum\n",
      "    zv, have_complex, magnitude = summator(coeffs, v, prec, wp, \\\n",
      "  File \"<string>\", line 41, in hypsum_2_1_RR_R_R\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sci[Parallel(n_jobs=12)]: Done  31 tasks      | elapsed: 138.7min\n",
      "py/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1751, in predict\n",
      "    tmp_batch_outputs = self.predict_function(iterator)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3039, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 591, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "[Parallel(n_jobs=12)]: Done  32 tasks      | elapsed: 145.4min\n",
      "[Parallel(n_jobs=12)]: Done  33 tasks      | elapsed: 145.8min\n",
      "[Parallel(n_jobs=12)]: Done  34 tasks      | elapsed: 146.6min\n",
      "[Parallel(n_jobs=12)]: Done  35 tasks      | elapsed: 149.9min\n",
      "[Parallel(n_jobs=12)]: Done  36 tasks      | elapsed: 153.1min\n",
      "[Parallel(n_jobs=12)]: Done  37 tasks      | elapsed: 154.2min\n",
      "[Parallel(n_jobs=12)]: Done  38 tasks      | elapsed: 160.9min\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sci[Parallel(n_jobs=12)]: Done  39 tasks      | elapsed: 165.7min\n",
      "py/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 635, in simplify\n",
      "    _e = cancel(expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polytools.py\", line 6706, in cancel\n",
      "    R, (F, G) = sring((p, q), *gens, **args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 164, in sring\n",
      "    reps, opt = _parallel_dict_from_expr(exprs, opt)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 326, in _parallel_dict_from_expr\n",
      "    exprs = [ expr.expand() for expr in exprs ]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 326, in <listcomp>\n",
      "    exprs = [ expr.expand() for expr in exprs ]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 72, in wrapper\n",
      "    retval = cfunc(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3598, in expand\n",
      "    expr, hit = Expr._expand_hint(expr, hint, deep=deep, **hints)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3531, in _expand_hint\n",
      "    arg, arghit = Expr._expand_hint(arg, hint, **hints)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3536, in _expand_hint\n",
      "    expr = expr.func(*sargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 72, in wrapper\n",
      "    retval = cfunc(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/operations.py\", line 85, in __new__\n",
      "    c_part, nc_part, order_symbols = cls.flatten(args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/mul.py\", line 702, in flatten\n",
      "    c_part = [Add(*[coeff*f for f in c_part[1].args])]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 72, in wrapper\n",
      "    retval = cfunc(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/operations.py\", line 85, in __new__\n",
      "    c_part, nc_part, order_symbols = cls.flatten(args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/add.py\", line 356, in flatten\n",
      "    _addsort(newseq)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/add.py\", line 21, in _addsort\n",
      "    args.sort(key=_args_sortkey)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/basic.py\", line 211, in compare\n",
      "    if self is other:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sci[Parallel(n_jobs=12)]: Done  40 tasks      | elapsed: 173.6min\n",
      "py/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 637, in simplify\n",
      "    expr2 = shorter(together(expr, deep=True), together(expr1, deep=True))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 84, in together\n",
      "    return _together(sympify(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 67, in _together\n",
      "    return gcd_terms(list(map(_together, Add.make_args(expr))), fraction=fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 78, in _together\n",
      "    return expr.__class__(*[ _together(arg) for arg in expr.args ])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 78, in <listcomp>\n",
      "    return expr.__class__(*[ _together(arg) for arg in expr.args ])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 67, in _together\n",
      "    return gcd_terms(list(map(_together, Add.make_args(expr))), fraction=fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 1077, in gcd_terms\n",
      "    cont, numer, denom = _gcd_terms(terms, isprimitive, fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 943, in _gcd_terms\n",
      "    terms = list(map(Term, [t for t in terms if t]))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 840, in __init__\n",
      "    numer = Factors(numer)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 376, in __init__\n",
      "    if k is I or k in (-1, 1):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 1400, in __eq__\n",
      "    return other.__eq__(self)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 2241, in __eq__\n",
      "    return Rational.__eq__(self, other)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 1865, in __eq__\n",
      "    other = _sympify(other)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/sympify.py\", line 512, in _sympify\n",
      "    return sympify(a, strict=True)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/sympify.py\", line 345, in sympify\n",
      "    is_sympy = getattr(a, '__sympy__', None)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/core.py\", line 71, in <lambda>\n",
      "    cls.__sympy__ = property(lambda self: True)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "[Parallel(n_jobs=12)]: Done  41 tasks      | elapsed: 175.8min\n",
      "[Parallel(n_jobs=12)]: Done  42 tasks      | elapsed: 176.8min\n",
      "[Parallel(n_jobs=12)]: Done  43 tasks      | elapsed: 180.2min\n",
      "[Parallel(n_jobs=12)]: Done  44 tasks      | elapsed: 180.2min\n",
      "[Parallel(n_jobs=12)]: Done  45 tasks      | elapsed: 180.2min\n",
      "[Parallel(n_jobs=12)]: Done  46 tasks      | elapsed: 195.2min\n",
      "[Parallel(n_jobs=12)]: Done  47 tasks      | elapsed: 202.7min\n",
      "[Parallel(n_jobs=12)]: Done  48 tasks      | elapsed: 205.1min\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 454, in getit\n",
      "    return self._assumptions[fact]\n",
      "KeyError: 'negative'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-[Parallel(n_jobs=12)]: Done  49 tasks      | elapsed: 206.6min\n",
      "ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 249, in fun\n",
      "    self._update_fun()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 594, in simplify\n",
      "    original_expr = expr = collect_abs(signsimp(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 405, in signsimp\n",
      "    return e.func(*[signsimp(a, evaluate) for a in e.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 405, in <listcomp>\n",
      "    return e.func(*[signsimp(a, evaluate) for a in e.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 401, in signsimp\n",
      "    e = sub_post(sub_pre(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/cse_opts.py\", line 14, in sub_pre\n",
      "    adds = [a for a in e.atoms(Add) if a.could_extract_minus_sign()]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/cse_opts.py\", line 14, in <listcomp>\n",
      "    adds = [a for a in e.atoms(Add) if a.could_extract_minus_sign()]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2471, in could_extract_minus_sign\n",
      "    negative_args = len([False for arg in self.args if arg.could_extract_minus_sign()])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2471, in <listcomp>\n",
      "    negative_args = len([False for arg in self.args if arg.could_extract_minus_sign()])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2464, in could_extract_minus_sign\n",
      "    (negative_self).extract_multiplicatively(-1) is not None)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2258, in extract_multiplicatively\n",
      "    elif self.is_positive and quotient.is_negative:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 458, in getit\n",
      "    return _ask(fact, self)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 503, in _ask\n",
      "    assumptions.deduce_all_facts(((fact, a),))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/facts.py\", line 525, in deduce_all_facts\n",
      "    self._tell(key, value)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/facts.py\", line 488, in _tell\n",
      "    if self[k] == v:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 454, in getit\n",
      "    return self._assumptions[fact]\n",
      "KeyError: 'zero'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "[Parallel(n_jobs=12)]: Done  50 tasks      | elapsed: 213.1min\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 443, in _hyp2f1\n",
      "    return ctx.hypsum(2, 1, (atype, btype, ctype), [a, b, c], z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp.py\", line 714, in hypsum\n",
      "    zv, have_complex, magnitude = summator(coeffs, v, prec, wp, \\\n",
      "  File \"<string>\", line 41, in hypsum_2_1_RR_R_R\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 722, in simplify\n",
      "    short = shorter(powsimp(expr, combine='exp', deep=True), powsimp(expr), expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in powsimp\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in <listcomp>\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in powsimp\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in <listcomp>\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 119, in powsimp\n",
      "    return recurse(expr*_y, deep=False)/_y\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 160, in powsimp\n",
      "    for b, e in ordered(iter(c_powers.items())):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 604, in ordered\n",
      "    yield from d[k]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 585, in ordered\n",
      "    d[f(a)].append(a)\n",
      "[Parallel(n_jobs=12)]: Done  51 tasks      | elapsed: 214.0min\n",
      "[Parallel(n_jobs=12)]: Done  52 tasks      | elapsed: 214.2min\n",
      "[Parallel(n_jobs=12)]: Done  53 tasks      | elapsed: 214.2min\n",
      "[Parallel(n_jobs=12)]: Done  54 tasks      | elapsed: 221.8min\n",
      "[Parallel(n_jobs=12)]: Done  55 tasks      | elapsed: 224.8min\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_di[Parallel(n_jobs=12)]: Done  56 tasks      | elapsed: 225.7min\n",
      "fference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1751, in predict\n",
      "    tmp_batch_outputs = self.predict_function(iterator)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3039, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 591, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 722, in simplify\n",
      "    short = shorter(powsimp(expr, combine='exp', deep=True), powsimp(expr), expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in powsimp\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in <listcomp>\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in powsimp\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in <listcomp>\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in powsimp\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_di[Parallel(n_jobs=12)]: Done  57 tasks      | elapsed: 235.8min\n",
      "fference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 637, in simplify\n",
      "    expr2 = shorter(together(expr, deep=True), together(expr1, deep=True))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 573, in shorter\n",
      "    return min(choices, key=measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/function.py\", line 3178, in count_ops\n",
      "    n, d = fraction(a)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/radsimp.py\", line 1111, in fraction\n",
      "    return Mul(*numer, evaluate=not exact), Mul(*denom, evaluate=not exact)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 72, in wrapper\n",
      "    retval = cfunc(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/operations.py\", line 85, in __new__\n",
      "    c_part, nc_part, order_symbols = cls.flatten(args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/mul.py\", line 521, in flatten\n",
      "    if b != bi:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/basic.py\", line 371, in __ne__\n",
      "    return not self == other\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 646, in simplify\n",
      "    expr = factor_terms(expr, sign=False)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 1274, in factor_terms\n",
      "    return do(expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 1264, in do\n",
      "    p = gcd_terms(p,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 1077, in gcd_terms\n",
      "    cont, numer, denom = _gcd_terms(terms, isprimitive, fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 962, in _gcd_terms\n",
      "    terms[i] = term.quo(cont)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 877, in quo\n",
      "    return self.mul(other.inv())\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/printing/str.py\", line 916, in sstr\n",
      "    s = p.doprint(expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/printing/printer.py\", line 291, in doprint\n",
      "    return self._str(self._print(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/printing/printer.py\", line 329, in _print\n",
      "    return getattr(self, printmethod)(expr, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/printing/str.py\", line 54, in _print_Add\n",
      "    t = self._print(term)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/printing/printer.py\", line 329, in _print\n",
      "    return getattr(self, printmethod)(expr, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/printing/str.py\", line 298, in _print_Mul\n",
      "    a_str = [self.parenthesize(x, prec, strict=False) for x in a]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI[Parallel(n_jobs=12)]: Done  58 tasks      | elapsed: 240.2min\n",
      "/lib/python3.8/site-packages/sympy/printing/str.py\", line 298, in <listcomp>\n",
      "    a_str = [self.parenthesize(x, prec, strict=False) for x in a]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/printing/str.py\", line 35, in parenthesize\n",
      "    return self._print(item)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/printing/printer.py\", line 329, in _print\n",
      "    return getattr(self, printmethod)(expr, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/printing/str.py\", line 682, in _print_Float\n",
      "    rv = mlib_to_str(expr._mpf_, dps, strip_zeros=strip, min_fixed=low, max_fixed=high)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/libmp/libmpf.py\", line 1242, in to_str\n",
      "    sign, digits, exponent = to_digits_exp(s, dps+3)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/libmp/libmpf.py\", line 1172, in to_digits_exp\n",
      "    bitprec = int(dps * math.log(10,2)) + 10\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1751, in predict\n",
      "    tmp_batch_outputs = self.predict_function(iterator)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3039, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 591, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "[Parallel(n_jobs=12)]: Done  59 tasks      | elapsed: 255.2min\n",
      "[Parallel(n_jobs=12)]: Done  60 tasks      | elapsed: 262.2min\n",
      "[Parallel(n_jobs=12)]: Done  61 tasks      | elapsed: 262.7min\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 249, in fun\n",
      "    self._update_fun()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-14290>\", line 2, in _lambdifygenerated\n",
      "    return (4.43746220913571*x**15 - 36.7564032539584*x**14 + 143.679202043868*x**13 - 352.553425522952*x**12 + 609.583003040116*x**11 - 790.828987360695*x**10 + 800.994162330013*x**9 - 651.228660298066*x**8 + 433.51092444009*x**7 - 238.616979447115*x**6 + 107.191226678405*x**5 - 36.39328041542*x**4 + 6.83194181965251*x**3 + 0.50637373662726*x**2 - 0.0180023185382889*x + 0.000411294010488082)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 144, in evaluate\n",
      "    'cython': lambdify([x], self.approx_expression(), modules=['math']), #ufuncify([x], self.approx_expression()),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 634, in simplify\n",
      "    expr = Mul(*powsimp(expr).as_content_primitive())\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in powsimp\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in <listcomp>\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt    [Parallel(n_jobs=12)]: Done  62 tasks      | elapsed: 265.1min\n",
      "   = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-13075>\", line 2, in _lambdifygenerated\n",
      "    return (-3397.78765460486*x**15 + 26955.8806628119*x**14 - 100132.638155721*x**13 + 231134.956652274*x**12 - 370955.71747546*x**11 + 438752.327187938*x**10 - 395383.179866939*x**9 + 276699.442679398*x**8 - 151804.12827503*x**7 + 65393.206224959*x**6 - 21983.2157630815*x**5 + 5678.5322876012*x**4 - 1094.24451250309*x**3 + 148.411917924932*x**2 - 11.7813708482301*x + 0.846160151073027)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 144, in evaluate\n",
      "    'cython': lambdify([x], self.approx_expression(), modules=['math']), #ufuncify([x], self.approx_expression()),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "[Parallel(n_jobs=12)]: Done  63 tasks      | elapsed: 266.6min\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 249, in fun\n",
      "    self._update_fun()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 635, in simplify\n",
      "    _e = cancel(expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polytools.py\", line 6706, in cancel\n",
      "    R, (F, G) = sring((p, q), *gens, **args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 164, in sring\n",
      "    reps, opt = _parallel_dict_from_expr(exprs, opt)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 326, in _parallel_dict_from_expr\n",
      "    exprs = [ expr.expand() for expr in exprs ]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 326, in <listcomp>\n",
      "    exprs = [ expr.expand() for expr in exprs ]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 72, in wrapper\n",
      "    retval = cfunc(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3598, in expand\n",
      "    expr, hit = Expr._expand_hint(expr, hint, deep=deep, **hints)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3531, in _expand_hint\n",
      "    arg, arghit = Expr._expand_hint(arg, hint, **hints)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3531, in _expand_hint\n",
      "    arg, arghit = Expr._expand_hint(arg, hint, **hints)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3539, in _expand_hint\n",
      "    newexpr = getattr(expr, hint)(**hints)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/power.py\", line 1207, in _eval_expand_multinomial\n",
      "    return basic_from_dict(expansion_dict, *p)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 387, in expr_from_dict\n",
      "    return Add(*result)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 72, in wrapper\n",
      "    retval = cfunc(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/operations.py\", line 85, in __new__\n",
      "    c_part, nc_part, order_symbols = cls.flatten(args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/add.py\", line 313, in flatten\n",
      "    newseq.append(Mul(c, s))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 72, in wrapper\n",
      "    retval = cfunc(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/operations.py\", line 85, in __new__\n",
      "    c_part, nc_part, order_symbols = cls.flatten(args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/mul.py\", line 672, in flatten\n",
      "    elif coeff.is_zero:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 458, in getit\n",
      "    return _ask(fact, self)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 503, in _ask\n",
      "    assumptions.deduce_all_facts(((fact, a),))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/facts.py\", line 533, in deduce_all_facts\n",
      "    if all(self.get(k) is v for k, v in bcond):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 249, in fun\n",
      "    self._update_fun()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "[Parallel(n_jobs=12)]: Done  64 tasks      | elapsed: 273.1min\n",
      "[Parallel(n_jobs=12)]: Done  65 tasks      | elapsed: 274.2min\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1759, in predict\n",
      "    tf.__internal__.nest.map_structure_up_to(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 1376, in map_structure_up_to\n",
      "    return map_structure_with_tuple_paths_up_to(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 1474, in map_structure_with_tuple_paths_up_to\n",
      "    results = [\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 1475, in <listcomp>\n",
      "    func(*args, **kwargs) for args in zip(flat_path_gen, *flat_value_gen)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 1378, in <lambda>\n",
      "    lambda _, *values: func(*values),  # Discards the path arg.\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1761, in <lambda>\n",
      "    lambda output, batch_output: output.append(batch_output),\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-16919>\", line 2, in _lambdifygenerated\n",
      "    return (14.4782977886172*x**15 - 119.573643553226*x**14 + 465.891470484706*x**13 - 1139.2799912535*x**12 + 1963.52960788303*x**11 - 2541.74919636206*x**10 + 2575.7356360375*x**9 - 2106.91447273893*x**8 + 1423.76515648282*x**7 - 802.329751456278*x**6 + 366.250767397556*x**5 - 118.899500709999*x**4 + 16.851243165252*x**3 + 1.88655507593107*x**2 - 0.0647555689628843*x + 0.00146124180663473)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in L[Parallel(n_jobs=12)]: Done  66 tasks      | elapsed: 274.2min\n",
      "oss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-11926>\", line 2, in _lambdifygenerated\n",
      "    return (-2.14419894627993*x**15 + 17.759420086482*x**14 - 69.4256019958688*x**13 + 170.432608273196*x**12 - 295.091937947154*x**11 + 384.075950321933*x**10 - 391.572600129051*x**9 + 321.719813514272*x**8 - 215.79035981401*x**7 + 114.885424075057*x**6 - 41.3019857555937*x**5 + 3.18156070897216*x**4 + 3.73643349343431*x**3 - 0.157891995834016*x**2 + 0.0071549375874443*x - 0.000176080429517529)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1751, in predict\n",
      "    tmp_batch_outputs = self.predict_function(iterator)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3039, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 591, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/li[Parallel(n_jobs=12)]: Done  67 tasks      | elapsed: 281.8min\n",
      "b/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-12367>\", line 2, in _lambdifygenerated\n",
      "    return (947.392440555891*x**15 - 7497.03102483642*x**14 + 27768.7633829429*x**13 - 63884.7797443439*x**12 + 102131.622226278*x**11 - 120240.198604898*x**10 + 107752.201995532*x**9 - 74891.1185007231*x**8 + 40730.7183148188*x**7 - 17345.9798099594*x**6 + 5739.62194006839*x**5 - 1448.03941576285*x**4 + 268.132847850812*x**3 - 33.3420337624283*x**2 + 2.06926264981552*x - 0.563653602534642)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyoptions.py\", line 69, in getter\n",
      "    return self[cls.option]\n",
      "KeyError: 'expand'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 723, in simplify\n",
      "    short = shorter(short, cancel(short))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polytools.py\", line 6737, in cancel\n",
      "    c, (P, Q) = 1, F.cancel(G)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 2220, in cancel\n",
      "    _, p, q = f.cofactors(g)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 2132, in cofactors\n",
      "    h, cfg, cff = g._gcd_monom(f)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_differenc[Parallel(n_jobs=12)]: Done  68 tasks      | elapsed: 284.8min\n",
      "e\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 456, in _hyp2f1\n",
      "    v = ctx.hypercomb(h, [a,b], **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 443, in _hyp2f1\n",
      "    return ctx.hypsum(2, 1, (atype, btype, ctype), [a, b, c], z, **kwargs)\n",
      "[Parallel(n_jobs=12)]: Done  69 tasks      | elapsed: 285.7min\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update[Parallel(n_jobs=12)]: Done  70 tasks      | elapsed: 300.2min\n",
      "_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-26045>\", line 2, in _lambdifygenerated\n",
      "    return (-2857.24142466233*x**15 + 22743.0021302347*x**14 - 84801.3073452888*x**13 + 196586.930642323*x**12 - 317070.479027442*x**11 + 377182.743784759*x**10 - 342216.843631692*x**9 + 241453.893873712*x**8 - 133798.433673442*x**7 + 58366.361856527*x**6 - 19945.4060282628*x**5 + 5269.41006507871*x**4 - 1050.07709771871*x**3 + 151.113868467366*x**2 - 13.3646429218357*x + 0.829308222739082)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "[Parallel(n_jobs=12)]: Done  71 tasks      | elapsed: 302.0min\n",
      "[Parallel(n_jobs=12)]: Done  72 tasks      | elapsed: 308.1min\n",
      "[Parallel(n_jobs=12)]: Done  73 tasks      | elapsed: 311.6min\n",
      "[Parallel(n_jobs=12)]: Done  74 tasks      | elapsed: 315.2min\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 249, in fun\n",
      "    self._update_fun()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-14035>\", line 2, in _lambdifygenerated\n",
      "    return (-6.52436810301414*x**15 + 52.9025911082718*x**14 - 201.507032629692*x**13 + 478.87971735064*x**12 - 795.34267189284*x**11 + 979.955145580834*x**10 - 928.101294771273*x**9 + 690.955391372304*x**8 - 410.34947633897*x**7 + 196.475693410837*x**6 - 76.6722840499457*x**5 + 24.9276812556375*x**4 - 7.30955969721649*x**3 + 3.38429426850643*x**2 + 0.11839310906078*x - 0.00158622218918851)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 249, in fun\n",
      "    self._update_fun()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-22777>\", line 2, in _lambdifygenerated\n",
      "    return (x**15*(-0.187961710192845 + 0.152180620973662*1j) + x**14*(1.53423574591942 - 1.24217293136172*1j) + x**13*(-5.89002418433509 + 4.76877730577343*1j) + x**12*(14.1303026118809 - 11.4404057284622*1j) + x**11*(-23.7407035789908 + 19.2213350756158*1j) + x**10*(29.6770218939345 - 24.0275937893644*1j) + x**9*(-28.6342913598912 + 23.1833613123556*1j) + x**8*(21.8528168674754 - 17.6928334898855*1j) + x**7*(-13.439907417107 + 10.8814367270091*1j) + x**6*(6.75443635013819 - 5.46863676137286*1j) + x**5*(-2.99007821251269 + 2.42087581919288*1j) + x**4*(0.667432045514907 - 0.540377202569454*1j) + x**3*(-1.32926243023906 + 1.07621909730017*1j) + x**2*(-0.0398152996468433 + 0.0322359113369026*1j) + x*(0.00122679704219331 - 0.00099325940106687*1j) - 2.59783279386028e-5 + 2.10329969519418e-5*1j)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "[Parallel(n_jobs=12)]: Done  75 tasks      | elapsed: 322.2min\n",
      "[Parallel(n_jobs=12)]: Done  76 tasks      | elapsed: 325.1min\n",
      "[Parallel(n_jobs=12)]: Done  77 tasks      | elapsed: 331.8min\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 491, in _nodes\n",
      "    return 1 + sum(_nodes(ei) for ei in e)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 491, in <genexpr>\n",
      "    return 1 + sum(_nodes(ei) for ei in e)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 491, in _nodes\n",
      "    return 1 + sum(_nodes(ei) for ei in e)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 491, in <genexpr>\n",
      "    return 1 + sum(_nodes(ei) for ei in e)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 489, in _nodes\n",
      "    return e.count(Basic)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/basic.py\", line 1520, in count\n",
      "    query = _make_find_query(query)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/basic.py\", line 2055, in _make_find_query\n",
      "    query = _sympify(query)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/sympify.py\", line 512, in _sympify\n",
      "    return sympify(a, strict=True)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/sympify.py\", line 345, in sympify\n",
      "    is_sympy = getattr(a, '__sympy__', None)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1747, in predict\n",
      "    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1180, in enumerate_epochs\n",
      "    data_iterator = iter(self._dataset)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 411, in __iter__\n",
      "    return iterator_ops.OwnedIterator(self)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 696, in __init__\n",
      "    self._create_iterator(dataset)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 719, in _create_iterator\n",
      "    gen_dataset_ops.make_iterator(ds_variant, self._iterator_resource)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 3120, in make_iterator\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 144, in evaluate\n",
      "    'cython': lambdify([x], self.approx_expression(), modules=['math']), #ufuncify([x], self.approx_expression()),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 469, in _hyp2f1\n",
      "    v = ctx.hyp2f1(a, c-b, c, z/(z-1)) / (1-z)**a\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 251, in hyp2f1\n",
      "    return ctx.hyper([a,b],[c],z,**kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 443, in _hyp2f1\n",
      "    return ctx.hypsum(2, 1, (atype, btype, ctype), [a, b, c], z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp.py\", line 714, in hypsum\n",
      "    zv, have_complex, magnitude = summator(coeffs, v, prec, wp, \\\n",
      "  File \"<string>\", line 39, in hypsum_2_1_RR_R_R\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 722, in simplify\n",
      "    short = shorter(powsimp(expr, combine='exp', deep=True), powsimp(expr), expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in powsimp\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in <listcomp>\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in powsimp\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in <listcomp>\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 119, in powsimp\n",
      "    return recurse(expr*_y, deep=False)/_y\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 160, in powsimp\n",
      "    for b, e in ordered(iter(c_powers.items())):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 604, in ordered\n",
      "    yield from d[k]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 604, in ordered\n",
      "    yield from d[k]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 585, in ordered\n",
      "    d[f(a)].append(a)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 449, in default_sort_key\n",
      "    args = [default_sort_key(arg, order=order) for arg in args]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 449, in <listcomp>\n",
      "    args = [default_sort_key(arg, order=order) for arg in args]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 432, in default_sort_key\n",
      "    from .compatibility import iterable\n",
      "  File \"<frozen importlib._bootstrap>\", line 393, in parent\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "[Parallel(n_jobs=12)]: Done  87 out of 100 | elapsed: 356.9min remaining: 53.3min\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 160, in powsimp\n",
      "    for b, e in ordered(iter(c_powers.items())):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 604, in ordered\n",
      "    yield from d[k]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 585, in ordered\n",
      "    d[f(a)].append(a)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 491, in _nodes\n",
      "    return 1 + sum(_nodes(ei) for ei in e)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 491, in <genexpr>\n",
      "    return 1 + sum(_nodes(ei) for ei in e)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 489, in _nodes\n",
      "    return e.count(Basic)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/basic.py\", line 1521, in count\n",
      "    return sum(bool(query(sub)) for sub in preorder_traversal(self))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/basic.py\", line 1521, in <genexpr>\n",
      "    return sum(bool(query(sub)) for sub in preorder_traversal(self))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/basic.py\", line 2046, in __next__\n",
      "    return next(self._pt)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/basic.py\", line 2006, in _preorder_traversal\n",
      "    if isinstance(node, Basic):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 143, in evaluate\n",
      "    evaluators_ = {'numpy': lambdify([x], self.approx_expression(), modules=['math']),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 456, in _hyp2f1\n",
      "    v = ctx.hypercomb(h, [a,b], **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 127, in hypercomb\n",
      "    [ctx.rgamma(b) for b in beta_s] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 127, in <listcomp>\n",
      "    [ctx.rgamma(b) for b in beta_s] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp_python.py\", line 998, in f\n",
      "    return ctx.make_mpf(mpf_f(x._mpf_, prec, rounding))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/libmp/gammazeta.py\", line 2141, in mpf_rgamma\n",
      "    return mpf_gamma(x, prec, rnd, 2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/libmp/gammazeta.py\", line 1838, in mpf_gamma\n",
      "    return gamma_fixed_taylor(x, absxman, wp, prec, rnd, type)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/libmp/gammazeta.py\", line 1527, in gamma_fixed_taylor\n",
      "    p = c + ((x*p)>>wp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in <listcomp>\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 160, in powsimp\n",
      "    for b, e in ordered(iter(c_powers.items())):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 604, in ordered\n",
      "    yield from d[k]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 604, in ordered\n",
      "    yield from d[k]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/compatibility.py\", line 605, in ordered\n",
      "    d.pop(k)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 2283, in __hash__\n",
      "    return hash(self.p)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-28573>\", line 2, in _lambdifygenerated\n",
      "    return (164.489806497232*x**15 - 1326.01172270785*x**14 + 5016.55030277196*x**13 - 11826.1507545265*x**12 + 19452.0804723377*x**11 - 23684.6350693137*x**10 + 22100.3020586563*x**9 - 16140.5541138361*x**8 + 9342.24312223344*x**7 - 4313.65467124206*x**6 + 1593.09045775723*x**5 - 471.505598768115*x**4 + 113.402682879541*x**3 - 24.226589690902*x**2 + 4.1330047956749*x + 0.124802511287655)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.012387\n",
      "         Iterations: 19\n",
      "         Function evaluations: 222\n",
      "         Gradient evaluations: 74\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.012382\n",
      "         Iterations: 19\n",
      "         Function evaluations: 378\n",
      "         Gradient evaluations: 63\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000028\n",
      "         Iterations: 77\n",
      "         Function evaluations: 1128\n",
      "         Gradient evaluations: 188\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.125977\n",
      "         Iterations: 13\n",
      "         Function evaluations: 178\n",
      "         Gradient evaluations: 56\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003445\n",
      "         Iterations: 13\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 39\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.042624\n",
      "         Iterations: 4\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 12\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.560124\n",
      "         Iterations: 28\n",
      "         Function evaluations: 192\n",
      "         Gradient evaluations: 64\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.028814\n",
      "         Iterations: 13\n",
      "         Function evaluations: 316\n",
      "         Gradient evaluations: 99\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.315350\n",
      "         Iterations: 5\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 13\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.315350\n",
      "         Iterations: 4\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 12\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000387\n",
      "         Iterations: 25\n",
      "         Function evaluations: 557\n",
      "         Gradient evaluations: 91\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.279416\n",
      "         Iterations: 9\n",
      "         Function evaluations: 207\n",
      "         Gradient evaluations: 49\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001721\n",
      "         Iterations: 14\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 44\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 144, in evaluate\n",
      "    'cython': lambdify([x], self.approx_expression(), modules=['math']), #ufuncify([x], self.approx_expression()),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 594, in simplify\n",
      "    original_expr = expr = collect_abs(signsimp(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 405, in signsimp\n",
      "    return e.func(*[signsimp(a, evaluate) for a in e.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 405, in <listcomp>\n",
      "    return e.func(*[signsimp(a, evaluate) for a in e.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 401, in signsimp\n",
      "    e = sub_post(sub_pre(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/cse_opts.py\", line 14, in sub_pre\n",
      "    adds = [a for a in e.atoms(Add) if a.could_extract_minus_sign()]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/cse_opts.py\", line 14, in <listcomp>\n",
      "    adds = [a for a in e.atoms(Add) if a.could_extract_minus_sign()]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2471, in could_extract_minus_sign\n",
      "    negative_args = len([False for arg in self.args if arg.could_extract_minus_sign()])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2471, in <listcomp>\n",
      "    negative_args = len([False for arg in self.args if arg.could_extract_minus_sign()])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2460, in could_extract_minus_sign\n",
      "    if self == negative_self:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 152, in __eq__\n",
      "    type(self) != type(other)):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 866, in mul\n",
      "    numer = self.numer.mul(other.numer)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 493, in mul\n",
      "    return Factors(factors)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 376, in __init__\n",
      "    if k is I or k in (-1, 1):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 1386, in __eq__\n",
      "    other = _sympify(other)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/sympify.py\", line 512, in _sympify\n",
      "    return sympify(a, strict=True)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/sympify.py\", line 361, in sympify\n",
      "    return conv(a)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/cache.py\", line 71, in wrapper\n",
      "    try:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 144, in evaluate\n",
      "    'cython': lambdify([x], self.approx_expression(), modules=['math']), #ufuncify([x], self.approx_expression()),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 126, in hypercomb\n",
      "    [ctx.gamma(a) for a in alpha_s] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 126, in <listcomp>\n",
      "    [ctx.gamma(a) for a in alpha_s] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp_python.py\", line 998, in f\n",
      "    return ctx.make_mpf(mpf_f(x._mpf_, prec, rounding))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/libmp/gammazeta.py\", line 1838, in mpf_gamma\n",
      "    return gamma_fixed_taylor(x, absxman, wp, prec, rnd, type)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/libmp/gammazeta.py\", line 1527, in gamma_fixed_taylor\n",
      "    p = c + ((x*p)>>wp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.002717\n",
      "         Iterations: 5\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 29\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000551\n",
      "         Iterations: 44\n",
      "         Function evaluations: 774\n",
      "         Gradient evaluations: 129\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.011123\n",
      "         Iterations: 7\n",
      "         Function evaluations: 264\n",
      "         Gradient evaluations: 42\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.119492\n",
      "         Iterations: 9\n",
      "         Function evaluations: 239\n",
      "         Gradient evaluations: 57\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.040059\n",
      "         Iterations: 17\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 42\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.108419\n",
      "         Iterations: 3\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 10\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.001776\n",
      "         Iterations: 49\n",
      "         Function evaluations: 1122\n",
      "         Gradient evaluations: 185\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.107753\n",
      "         Iterations: 13\n",
      "         Function evaluations: 314\n",
      "         Gradient evaluations: 77\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000011\n",
      "         Iterations: 11\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 38\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000717\n",
      "         Iterations: 25\n",
      "         Function evaluations: 189\n",
      "         Gradient evaluations: 63\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.002618\n",
      "         Iterations: 4\n",
      "         Function evaluations: 51\n",
      "         Gradient evaluations: 17\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000613\n",
      "         Iterations: 32\n",
      "         Function evaluations: 594\n",
      "         Gradient evaluations: 99\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000004\n",
      "         Iterations: 79\n",
      "         Function evaluations: 1134\n",
      "         Gradient evaluations: 189\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.039703\n",
      "         Iterations: 28\n",
      "         Function evaluations: 245\n",
      "         Gradient evaluations: 81\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001533\n",
      "         Iterations: 13\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 38\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000523\n",
      "         Iterations: 26\n",
      "         Function evaluations: 504\n",
      "         Gradient evaluations: 84\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.405523\n",
      "         Iterations: 4\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 10\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.405523\n",
      "         Iterations: 5\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 15\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000969\n",
      "         Iterations: 16\n",
      "         Function evaluations: 355\n",
      "         Gradient evaluations: 58\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.374815\n",
      "         Iterations: 5\n",
      "         Function evaluations: 180\n",
      "         Gradient evaluations: 42\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.670175\n",
      "         Iterations: 16\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 42\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.181384\n",
      "         Iterations: 6\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 21\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.003141\n",
      "         Iterations: 69\n",
      "         Function evaluations: 1265\n",
      "         Gradient evaluations: 209\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1.143395\n",
      "         Iterations: 4\n",
      "         Function evaluations: 166\n",
      "         Gradient evaluations: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 723, in simplify\n",
      "    short = shorter(short, cancel(short))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polytools.py\", line 6706, in cancel\n",
      "    R, (F, G) = sring((p, q), *gens, **args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 164, in sring\n",
      "    reps, opt = _parallel_dict_from_expr(exprs, opt)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 334, in _parallel_dict_from_expr\n",
      "    reps, gens = _parallel_dict_from_expr_no_gens(exprs, opt)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 258, in _parallel_dict_from_expr_no_gens\n",
      "    if not _not_a_coeff(factor) and (factor.is_Number or _is_coeff(factor)):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 173, in _not_a_coeff\n",
      "    if type(expr) in illegal_types or expr in finf:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 1389, in __eq__\n",
      "    if isinstance(other, Boolean):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1720, in predict\n",
      "    data_handler = data_adapter.get_data_handler(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1383, in get_data_handler\n",
      "    return DataHandler(*args, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1138, in __init__\n",
      "    self._adapter = adapter_cls(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 320, in __init__\n",
      "    indices_dataset = indices_dataset.flat_map(slice_batch_indices)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1903, in flat_map\n",
      "    return FlatMapDataset(self, map_func)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 5062, in __init__\n",
      "    self._map_func = StructuredFunctionWrapper(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 4218, in __init__\n",
      "    self._function = fn_factory()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3150, in get_concrete_function\n",
      "    graph_function = self._get_concrete_function_garbage_collected(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3116, in _get_concrete_function_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3463, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3298, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 892, in func_graph_from_py_func\n",
      "    func_graph = FuncGraph(name, collections=collections,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 189, in __init__\n",
      "    super(FuncGraph, self).__init__()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 2959, in __init__\n",
      "    self._group_lock = lock_util.GroupLock(num_groups=2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/lock_util.py\", line 72, in __init__\n",
      "    self._ready = threading.Condition(threading.Lock())\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/threading.py\", line 222, in __init__\n",
      "    def __init__(self, lock=None):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000108\n",
      "         Iterations: 20\n",
      "         Function evaluations: 168\n",
      "         Gradient evaluations: 56\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000095\n",
      "         Iterations: 11\n",
      "         Function evaluations: 228\n",
      "         Gradient evaluations: 38\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.408063\n",
      "         Iterations: 5\n",
      "         Function evaluations: 36\n",
      "         Gradient evaluations: 12\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000072\n",
      "         Iterations: 11\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.196296\n",
      "         Iterations: 7\n",
      "         Function evaluations: 75\n",
      "         Gradient evaluations: 25\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000028\n",
      "         Iterations: 37\n",
      "         Function evaluations: 294\n",
      "         Gradient evaluations: 98\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.001248\n",
      "         Iterations: 13\n",
      "         Function evaluations: 382\n",
      "         Gradient evaluations: 120\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000263\n",
      "         Iterations: 115\n",
      "         Function evaluations: 1758\n",
      "         Gradient evaluations: 291\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.737735\n",
      "         Iterations: 13\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 31\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.895778\n",
      "         Iterations: 4\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 17\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.041491\n",
      "         Iterations: 11\n",
      "         Function evaluations: 195\n",
      "         Gradient evaluations: 62\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.012023\n",
      "         Iterations: 42\n",
      "         Function evaluations: 911\n",
      "         Gradient evaluations: 150\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.008963\n",
      "         Iterations: 13\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 44\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.008932\n",
      "         Iterations: 16\n",
      "         Function evaluations: 300\n",
      "         Gradient evaluations: 50\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000006\n",
      "         Iterations: 86\n",
      "         Function evaluations: 1422\n",
      "         Gradient evaluations: 237\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.046706\n",
      "         Iterations: 25\n",
      "         Function evaluations: 198\n",
      "         Gradient evaluations: 66\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.002934\n",
      "         Iterations: 13\n",
      "         Function evaluations: 234\n",
      "         Gradient evaluations: 75\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.002195\n",
      "         Iterations: 52\n",
      "         Function evaluations: 1007\n",
      "         Gradient evaluations: 166\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.001117\n",
      "         Iterations: 30\n",
      "         Function evaluations: 610\n",
      "         Gradient evaluations: 100\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.164369\n",
      "         Iterations: 4\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 32\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.600657\n",
      "         Iterations: 38\n",
      "         Function evaluations: 282\n",
      "         Gradient evaluations: 94\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.910774\n",
      "         Iterations: 2\n",
      "         Function evaluations: 141\n",
      "         Gradient evaluations: 21\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.057474\n",
      "         Iterations: 61\n",
      "         Function evaluations: 814\n",
      "         Gradient evaluations: 134\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000066\n",
      "         Iterations: 9\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 32\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.071426\n",
      "         Iterations: 6\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 24\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.071426\n",
      "         Iterations: 2\n",
      "         Function evaluations: 54\n",
      "         Gradient evaluations: 9\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.005093\n",
      "         Iterations: 19\n",
      "         Function evaluations: 151\n",
      "         Gradient evaluations: 50\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.015444\n",
      "         Iterations: 4\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 16\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000002\n",
      "         Iterations: 45\n",
      "         Function evaluations: 636\n",
      "         Gradient evaluations: 106\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.047742\n",
      "         Iterations: 23\n",
      "         Function evaluations: 210\n",
      "         Gradient evaluations: 70\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001436\n",
      "         Iterations: 1\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000137\n",
      "         Iterations: 15\n",
      "         Function evaluations: 408\n",
      "         Gradient evaluations: 68\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-35291>\", line 2, in _lambdifygenerated\n",
      "    return (-16.4636093852145*x**15 + 131.675505171613*x**14 - 493.687890384012*x**13 + 1151.83636613299*x**12 - 1871.89946450133*x**11 + 2247.15406563211*x**10 - 2061.76969313635*x**9 + 1475.4220132789*x**8 - 832.95755985186*x**7 + 372.960335096993*x**6 - 132.749259984993*x**5 + 38.280460047871*x**4 - 9.13601254723033*x**3 + 1.41630743227679*x**2 + 0.00476161742443742*x + 0.00989613977508339)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 223, in hyper\n",
      "    elif q == 2: return ctx._hyp1f2(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 891, in _hyp1f2\n",
      "    return ctx.hypsum(1, 2, (a1type, b1type, b2type), [a1, b1, b2], z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp.py\", line 714, in hypsum\n",
      "    zv, have_complex, magnitude = summator(coeffs, v, prec, wp, \\\n",
      "  File \"<string>\", line 43, in hypsum_1_2_R_RR_R\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 454, in getit\n",
      "    return self._assumptions[fact]\n",
      "KeyError: 'positive'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 144, in evaluate\n",
      "    'cython': lambdify([x], self.approx_expression(), modules=['math']), #ufuncify([x], self.approx_expression()),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 594, in simplify\n",
      "    original_expr = expr = collect_abs(signsimp(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 401, in signsimp\n",
      "    e = sub_post(sub_pre(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/cse_opts.py\", line 35, in sub_pre\n",
      "    elif a.could_extract_minus_sign():\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2471, in could_extract_minus_sign\n",
      "    negative_args = len([False for arg in self.args if arg.could_extract_minus_sign()])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2471, in <listcomp>\n",
      "    negative_args = len([False for arg in self.args if arg.could_extract_minus_sign()])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2464, in could_extract_minus_sign\n",
      "    (negative_self).extract_multiplicatively(-1) is not None)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2299, in extract_multiplicatively\n",
      "    newarg = arg.extract_multiplicatively(c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009314\n",
      "         Iterations: 38\n",
      "         Function evaluations: 300\n",
      "         Gradient evaluations: 100\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.182531\n",
      "         Iterations: 17\n",
      "         Function evaluations: 305\n",
      "         Gradient evaluations: 98\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.046915\n",
      "         Iterations: 16\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 40\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.147753\n",
      "         Iterations: 5\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 16\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.001638\n",
      "         Iterations: 37\n",
      "         Function evaluations: 783\n",
      "         Gradient evaluations: 128\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.148106\n",
      "         Iterations: 8\n",
      "         Function evaluations: 221\n",
      "         Gradient evaluations: 53\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.014753\n",
      "         Iterations: 74\n",
      "         Function evaluations: 721\n",
      "         Gradient evaluations: 237\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.038923\n",
      "         Iterations: 10\n",
      "         Function evaluations: 165\n",
      "         Gradient evaluations: 52\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.007366\n",
      "         Iterations: 22\n",
      "         Function evaluations: 266\n",
      "         Gradient evaluations: 85\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.021051\n",
      "         Iterations: 77\n",
      "         Function evaluations: 1494\n",
      "         Gradient evaluations: 247\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.091996\n",
      "         Iterations: 3\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 14\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.091996\n",
      "         Iterations: 4\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 14\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in <listcomp>\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"<lambdifygenerated-26940>\", line 2, in _lambdifygenerated\n",
      "    return (-2646.56692929434*x**15 + 21089.0743086626*x**14 - 78731.3280077328*x**13 + 182771.906485496*x**12 - 295264.845616071*x**11 + 351902.829046245*x**10 - 319986.701403133*x**9 + 226366.483724291*x**8 - 125842.381679135*x**7 + 55116.6868398401*x**6 - 18932.6281658583*x**5 + 5036.89549364524*x**4 - 1014.06701888244*x**3 + 148.811142323608*x**2 - 13.5857709890079*x + 0.923131046363742)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 148, in evaluate\n",
      "    Y           = np.array([evaluater_(X[k]) for k in range(len(X))])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.022697\n",
      "         Iterations: 11\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 29\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.034959\n",
      "         Iterations: 4\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 13\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000741\n",
      "         Iterations: 7\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 27\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.001173\n",
      "         Iterations: 14\n",
      "         Function evaluations: 291\n",
      "         Gradient evaluations: 93\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.001444\n",
      "         Iterations: 15\n",
      "         Function evaluations: 624\n",
      "         Gradient evaluations: 102\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000144\n",
      "         Iterations: 22\n",
      "         Function evaluations: 207\n",
      "         Gradient evaluations: 69\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.050383\n",
      "         Iterations: 4\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 11\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000050\n",
      "         Iterations: 53\n",
      "         Function evaluations: 840\n",
      "         Gradient evaluations: 140\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.038092\n",
      "         Iterations: 3\n",
      "         Function evaluations: 36\n",
      "         Gradient evaluations: 12\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.038092\n",
      "         Iterations: 4\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 16\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.025071\n",
      "         Iterations: 14\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 50\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.025065\n",
      "         Iterations: 27\n",
      "         Function evaluations: 540\n",
      "         Gradient evaluations: 90\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.282835\n",
      "         Iterations: 14\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 41\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.282747\n",
      "         Iterations: 44\n",
      "         Function evaluations: 864\n",
      "         Gradient evaluations: 144\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 2258, in extract_multiplicatively\n",
      "    elif self.is_positive and quotient.is_negative:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 457, in getit\n",
      "    self._assumptions = self.default_assumptions.copy()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 437, in copy\n",
      "    return self.__class__(self)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/assumptions.py\", line 434, in __init__\n",
      "    self.deduce_all_facts(facts)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/facts.py\", line 532, in deduce_all_facts\n",
      "    bcond, bimpl = beta_rules[bidx]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 126, in hypercomb\n",
      "    [ctx.gamma(a) for a in alpha_s] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 126, in <listcomp>\n",
      "    [ctx.gamma(a) for a in alpha_s] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp_python.py\", line 998, in f\n",
      "    return ctx.make_mpf(mpf_f(x._mpf_, prec, rounding))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/libmp/gammazeta.py\", line 1838, in mpf_gamma\n",
      "    return gamma_fixed_taylor(x, absxman, wp, prec, rnd, type)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/libmp/gammazeta.py\", line 1527, in gamma_fixed_taylor\n",
      "    p = c + ((x*p)>>wp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 144, in evaluate\n",
      "    'cython': lambdify([x], self.approx_expression(), modules=['math']), #ufuncify([x], self.approx_expression()),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 637, in simplify\n",
      "    expr2 = shorter(together(expr, deep=True), together(expr1, deep=True))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 84, in together\n",
      "    return _together(sympify(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 67, in _together\n",
      "    return gcd_terms(list(map(_together, Add.make_args(expr))), fraction=fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 1077, in gcd_terms\n",
      "    cont, numer, denom = _gcd_terms(terms, isprimitive, fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 962, in _gcd_terms\n",
      "    terms[i] = term.quo(cont)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 877, in quo\n",
      "    return self.mul(other.inv())\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 866, in mul\n",
      "    numer = self.numer.mul(other.numer)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 481, in mul\n",
      "    factors = dict(self.factors)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 2160, in _gcd_monom\n",
      "    cfg = f.new([(monomial_ldiv(mg, _mgcd), ground_quo(cg, _cgcd)) for mg, cg in g.iterterms()])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 2160, in <listcomp>\n",
      "    cfg = f.new([(monomial_ldiv(mg, _mgcd), ground_quo(cg, _cgcd)) for mg, cg in g.iterterms()])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/domains/field.py\", line 29, in quo\n",
      "    return a / b\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/domains/expressiondomain.py\", line 109, in __truediv__\n",
      "    return f.simplify(f.ex/g.ex)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/domains/expressiondomain.py\", line 50, in simplify\n",
      "    return f.__class__(ex.cancel().expand(**eflags))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3707, in cancel\n",
      "    return cancel(self, *gens, **args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polytools.py\", line 6706, in cancel\n",
      "    R, (F, G) = sring((p, q), *gens, **args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 164, in sring\n",
      "    reps, opt = _parallel_dict_from_expr(exprs, opt)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyutils.py\", line 325, in _parallel_dict_from_expr\n",
      "    if opt.expand is not False:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polyoptions.py\", line 69, in getter\n",
      "    return self[cls.option]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 144, in evaluate\n",
      "    'cython': lambdify([x], self.approx_expression(), modules=['math']), #ufuncify([x], self.approx_expression()),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 86, in hypercomb\n",
      "    _check_need_perturb(ctx, terms, orig, discard_known_zeros)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 22, in _check_need_perturb\n",
      "    n, d = ctx.nint_distance(x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp.py\", line 1209, in nint_distance\n",
      "    return n, max(re_dist, im_dist)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp_python.py\", line 178, in __gt__\n",
      "    def __gt__(s, t): return s._cmp(t, mpf_gt)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp_python.py\", line 168, in _cmp\n",
      "    if hasattr(t, '_mpf_'):\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.024313\n",
      "         Iterations: 4\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 15\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.010014\n",
      "         Iterations: 1\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 4\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.006574\n",
      "         Iterations: 11\n",
      "         Function evaluations: 156\n",
      "         Gradient evaluations: 49\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.008708\n",
      "         Iterations: 44\n",
      "         Function evaluations: 1026\n",
      "         Gradient evaluations: 171\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.237463\n",
      "         Iterations: 11\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 38\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.751858\n",
      "         Iterations: 3\n",
      "         Function evaluations: 48\n",
      "         Gradient evaluations: 8\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.004895\n",
      "         Iterations: 23\n",
      "         Function evaluations: 437\n",
      "         Gradient evaluations: 71\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.729809\n",
      "         Iterations: 5\n",
      "         Function evaluations: 272\n",
      "         Gradient evaluations: 65\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.057334\n",
      "         Iterations: 1\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 4\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000334\n",
      "         Iterations: 11\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 32\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000207\n",
      "         Iterations: 50\n",
      "         Function evaluations: 984\n",
      "         Gradient evaluations: 164\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000035\n",
      "         Iterations: 36\n",
      "         Function evaluations: 318\n",
      "         Gradient evaluations: 106\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.154697\n",
      "         Iterations: 12\n",
      "         Function evaluations: 188\n",
      "         Gradient evaluations: 59\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 249, in fun\n",
      "    self._update_fun()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 233, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 722, in simplify\n",
      "    short = shorter(powsimp(expr, combine='exp', deep=True), powsimp(expr), expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in powsimp\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 116, in <listcomp>\n",
      "    expr = expr.func(*[recurse(w) for w in expr.args])\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 107, in recurse\n",
      "    return powsimp(arg, _deep, _combine, _force, _measure)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/powsimp.py\", line 190, in powsimp\n",
      "    if b != binv and binv in c_powers:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 1414, in __ne__\n",
      "    return not self == other\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 1384, in __eq__\n",
      "    from sympy.logic.boolalg import Boolean\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.012973\n",
      "         Iterations: 13\n",
      "         Function evaluations: 197\n",
      "         Gradient evaluations: 62\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000006\n",
      "         Iterations: 9\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 40\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.320040\n",
      "         Iterations: 4\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 10\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.211907\n",
      "         Iterations: 5\n",
      "         Function evaluations: 186\n",
      "         Gradient evaluations: 29\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.173177\n",
      "         Iterations: 5\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 11\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.173177\n",
      "         Iterations: 3\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.001258\n",
      "         Iterations: 66\n",
      "         Function evaluations: 1059\n",
      "         Gradient evaluations: 175\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.147471\n",
      "         Iterations: 8\n",
      "         Function evaluations: 310\n",
      "         Gradient evaluations: 75\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.045975\n",
      "         Iterations: 13\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 48\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.045964\n",
      "         Iterations: 34\n",
      "         Function evaluations: 645\n",
      "         Gradient evaluations: 107\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000591\n",
      "         Iterations: 58\n",
      "         Function evaluations: 904\n",
      "         Gradient evaluations: 149\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.045435\n",
      "         Iterations: 10\n",
      "         Function evaluations: 327\n",
      "         Gradient evaluations: 79\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000041\n",
      "         Iterations: 13\n",
      "         Function evaluations: 164\n",
      "         Gradient evaluations: 54\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.085626\n",
      "         Iterations: 14\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 46\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000655\n",
      "         Iterations: 7\n",
      "         Function evaluations: 75\n",
      "         Gradient evaluations: 25\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000330\n",
      "         Iterations: 41\n",
      "         Function evaluations: 786\n",
      "         Gradient evaluations: 131\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.014817\n",
      "         Iterations: 51\n",
      "         Function evaluations: 555\n",
      "         Gradient evaluations: 181\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 443, in _hyp2f1\n",
      "    return ctx.hypsum(2, 1, (atype, btype, ctype), [a, b, c], z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp.py\", line 714, in hypsum\n",
      "    zv, have_complex, magnitude = summator(coeffs, v, prec, wp, \\\n",
      "  File \"<string>\", line 43, in hypsum_2_1_RR_R_R\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "[Parallel(n_jobs=12)]: Done  97 out of 100 | elapsed: 410.3min remaining: 12.7min\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 443, in _hyp2f1\n",
      "    return ctx.hypsum(2, 1, (atype, btype, ctype), [a, b, c], z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp.py\", line 714, in hypsum\n",
      "    zv, have_complex, magnitude = summator(coeffs, v, prec, wp, \\\n",
      "  File \"<string>\", line 43, in hypsum_2_1_RR_R_R\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.032812\n",
      "         Iterations: 13\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 39\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.067199\n",
      "         Iterations: 4\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.005508\n",
      "         Iterations: 55\n",
      "         Function evaluations: 1188\n",
      "         Gradient evaluations: 195\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.066596\n",
      "         Iterations: 14\n",
      "         Function evaluations: 344\n",
      "         Gradient evaluations: 83\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.069401\n",
      "         Iterations: 26\n",
      "         Function evaluations: 205\n",
      "         Gradient evaluations: 68\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000129\n",
      "         Iterations: 7\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 26\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 17\n",
      "         Function evaluations: 264\n",
      "         Gradient evaluations: 44\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.047718\n",
      "         Iterations: 3\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 11\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.047718\n",
      "         Iterations: 3\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 13\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.002163\n",
      "         Iterations: 33\n",
      "         Function evaluations: 744\n",
      "         Gradient evaluations: 122\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.032968\n",
      "         Iterations: 6\n",
      "         Function evaluations: 272\n",
      "         Gradient evaluations: 65\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.351296\n",
      "         Iterations: 5\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 15\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.351296\n",
      "         Iterations: 3\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 12\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001070\n",
      "         Iterations: 18\n",
      "         Function evaluations: 177\n",
      "         Gradient evaluations: 59\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.621357\n",
      "         Iterations: 4\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 10\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.621357\n",
      "         Iterations: 3\n",
      "         Function evaluations: 48\n",
      "         Gradient evaluations: 8\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.013392\n",
      "         Iterations: 53\n",
      "         Function evaluations: 1359\n",
      "         Gradient evaluations: 223\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.511127\n",
      "         Iterations: 20\n",
      "         Function evaluations: 372\n",
      "         Gradient evaluations: 90\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000056\n",
      "         Iterations: 50\n",
      "         Function evaluations: 525\n",
      "         Gradient evaluations: 171\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000007\n",
      "         Iterations: 46\n",
      "         Function evaluations: 351\n",
      "         Gradient evaluations: 117\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.006389\n",
      "         Iterations: 15\n",
      "         Function evaluations: 186\n",
      "         Gradient evaluations: 62\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.006390\n",
      "         Iterations: 24\n",
      "         Function evaluations: 480\n",
      "         Gradient evaluations: 80\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000055\n",
      "         Iterations: 130\n",
      "         Function evaluations: 1926\n",
      "         Gradient evaluations: 321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp.py\", line 714, in hypsum\n",
      "    zv, have_complex, magnitude = summator(coeffs, v, prec, wp, \\\n",
      "  File \"<string>\", line 43, in hypsum_2_1_RR_R_R\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/keras/engine/training.py\", line 1751, in predict\n",
      "    tmp_batch_outputs = self.predict_function(iterator)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3039, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 591, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000802\n",
      "         Iterations: 7\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 26\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000655\n",
      "         Iterations: 71\n",
      "         Function evaluations: 1296\n",
      "         Gradient evaluations: 216\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003516\n",
      "         Iterations: 15\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 44\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003504\n",
      "         Iterations: 16\n",
      "         Function evaluations: 294\n",
      "         Gradient evaluations: 49\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001468\n",
      "         Iterations: 11\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 43\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.002776\n",
      "         Iterations: 4\n",
      "         Function ev"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 443, in _hyp2f1\n",
      "    return ctx.hypsum(2, 1, (atype, btype, ctype), [a, b, c], z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp.py\", line 714, in hypsum\n",
      "    zv, have_complex, magnitude = summator(coeffs, v, prec, wp, \\\n",
      "  File \"<string>\", line 42, in hypsum_2_1_RR_R_R\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['d"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aluations: 78\n",
      "         Gradient evaluations: 13\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.019968\n",
      "         Iterations: 18\n",
      "         Function evaluations: 262\n",
      "         Gradient evaluations: 84\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.032578\n",
      "         Iterations: 30\n",
      "         Function evaluations: 942\n",
      "         Gradient evaluations: 155\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.152305\n",
      "         Iterations: 11\n",
      "         Function evaluations: 257\n",
      "         Gradient evaluations: 82\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.174392\n",
      "         Iterations: 5\n",
      "         Function evaluations: 36\n",
      "         Gradient evaluations: 12\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.174392\n",
      "         Iterations: 5\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 21\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000253\n",
      "         Iterations: 114\n",
      "         Function evaluations: 1798\n",
      "         Gradient evaluations: 298\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.144845\n",
      "         Iterations: 8\n",
      "         Function evaluations: 179\n",
      "         Gradient evaluations: 42\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.010833\n",
      "         Iterations: 5\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 22\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.008527\n",
      "         Iterations: 28\n",
      "         Function evaluations: 910\n",
      "         Gradient evaluations: 150\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 144, in evaluate\n",
      "    'cython': lambdify([x], self.approx_expression(), modules=['math']), #ufuncify([x], self.approx_expression()),\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 637, in simplify\n",
      "    expr2 = shorter(together(expr, deep=True), together(expr1, deep=True))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 84, in together\n",
      "    return _together(sympify(expr))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rationaltools.py\", line 67, in _together\n",
      "    return gcd_terms(list(map(_together, Add.make_args(expr))), fraction=fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 1077, in gcd_terms\n",
      "    cont, numer, denom = _gcd_terms(terms, isprimitive, fraction)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 962, in _gcd_terms\n",
      "    terms[i] = term.quo(cont)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 877, in quo\n",
      "    return self.mul(other.inv())\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 869, in mul\n",
      "    numer, denom = numer.normal(denom)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 563, in normal\n",
      "    return Factors(self_factors), Factors(other_factors)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/exprtools.py\", line 406, in __init__\n",
      "    self.gens = frozenset(keys())\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 1479, in __hash__\n",
      "    return super().__hash__()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000931\n",
      "         Iterations: 10\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 40\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000012\n",
      "         Iterations: 23\n",
      "         Function evaluations: 210\n",
      "         Gradient evaluations: 69\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.003593\n",
      "         Iterations: 26\n",
      "         Function evaluations: 543\n",
      "         Gradient evaluations: 177\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.001059\n",
      "         Iterations: 19\n",
      "         Function evaluations: 509\n",
      "         Gradient evaluations: 84\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001056\n",
      "         Iterations: 20\n",
      "         Function evaluations: 159\n",
      "         Gradient evaluations: 53\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000005\n",
      "         Iterations: 12\n",
      "         Function evaluations: 218\n",
      "         Gradient evaluations: 36\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000369\n",
      "         Iterations: 13\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 39\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000051\n",
      "         Iterations: 36\n",
      "         Function evaluations: 898\n",
      "         Gradient evaluations: 147\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.134163\n",
      "         Iterations: 6\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 14\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.134163\n",
      "         Iterations: 6\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 19\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000814\n",
      "         Iterations: 48\n",
      "         Function evaluations: 1025\n",
      "         Gradient evaluations: 169\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.008690\n",
      "         Iterations: 40\n",
      "         Function evaluations: 682\n",
      "         Gradient evaluations: 168\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.005479\n",
      "         Iterations: 11\n",
      "         Function evaluations: 129\n",
      "         Gradient evaluations: 43\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.010819\n",
      "         Iterations: 22\n",
      "         Function evaluations: 236\n",
      "         Gradient evaluations: 76\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000100\n",
      "         Iterations: 13\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 45\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000071\n",
      "         Iterations: 11\n",
      "         Function evaluations: 270\n",
      "         Gradient evaluations: 45\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.003018\n",
      "         Iterations: 12\n",
      "         Function evaluations: 268\n",
      "         Gradient evaluations: 88\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000380\n",
      "         Iterations: 158\n",
      "         Function evaluations: 2561\n",
      "         Gradient evaluations: 425\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 125, in approx_expression\n",
      "    self.approx_expr = simplify(self.approx_expr)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/simplify/simplify.py\", line 636, in simplify\n",
      "    expr1 = shorter(_e, _mexpand(_e).cancel())  # issue 6829\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/expr.py\", line 3707, in cancel\n",
      "    return cancel(self, *gens, **args)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/polytools.py\", line 6742, in cancel\n",
      "    return c*(P.as_expr()/Q.as_expr())\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 621, in as_expr\n",
      "    return expr_from_dict(self.as_expr_dict(), *symbols)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 625, in as_expr_dict\n",
      "    return {monom: to_sympy(coeff) for monom, coeff in self.iterterms()}\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/rings.py\", line 625, in <dictcomp>\n",
      "    return {monom: to_sympy(coeff) for monom, coeff in self.iterterms()}\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/polys/domains/realfield.py\", line 64, in to_sympy\n",
      "    return Float(element, self.dps)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 1189, in __new__\n",
      "    return cls._new(_mpf_, precision, zero=False)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sympy/core/numbers.py\", line 1198, in _new\n",
      "    elif _mpf_ == _mpf_inf:\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metamodel Function Optimization Time: 7:01:58\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed: 422.0min finished\n"
     ]
    }
   ],
   "source": [
    "if symbolic_metamodeling_poly_function_evaluation:\n",
    "    print('----------------------------------------------- CALCULATE METAMODEL POLY FUNCTION ----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test, metamodel_runtimes = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=True, force_polynomial=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions_no_GD_poly'] = metamodel_functions_test   \n",
    "        runtimes_list[i]['metamodel_functions_no_GD_runtime_poly'] = metamodel_runtimes\n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Function Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:21:36.950952Z",
     "iopub.status.busy": "2021-11-09T05:21:36.950367Z",
     "iopub.status.idle": "2021-11-09T05:21:42.835583Z",
     "shell.execute_reply": "2021-11-09T05:21:42.834220Z",
     "shell.execute_reply.started": "2021-11-09T05:21:36.950899Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if symbolic_regression_evaluation:\n",
    "    print('----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        symbolic_regression_functions_test, symbolic_regression_runtimes = symbolic_regression_function_generation(lambda_net_test_dataset)\n",
    "        polynomial_dict_test_list[i]['symbolic_regression_functions'] = symbolic_regression_functions_test    \n",
    "        runtimes_list[i]['symbolic_regression_runtime'] = symbolic_regression_runtimes\n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Symbolic Regression Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:21:42.838311Z",
     "iopub.status.busy": "2021-11-09T05:21:42.837722Z",
     "iopub.status.idle": "2021-11-09T05:21:42.934905Z",
     "shell.execute_reply": "2021-11-09T05:21:42.933528Z",
     "shell.execute_reply.started": "2021-11-09T05:21:42.838255Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if per_network_evaluation:\n",
    "    print('------------------------------------------------ CALCULATE PER NETWORK POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        per_network_poly_test = per_network_poly_generation(lambda_net_test_dataset, optimization_type='scipy')\n",
    "        polynomial_dict_test_list[i]['per_network_polynomials'] = per_network_poly_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Per Network Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:21:42.937345Z",
     "iopub.status.busy": "2021-11-09T05:21:42.936809Z",
     "iopub.status.idle": "2021-11-09T05:21:42.993000Z",
     "shell.execute_reply": "2021-11-09T05:21:42.992508Z",
     "shell.execute_reply.started": "2021-11-09T05:21:42.937304Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:13:16.806554Z",
     "iopub.status.busy": "2021-11-09T08:13:16.806215Z",
     "iopub.status.idle": "2021-11-09T08:13:33.346705Z",
     "shell.execute_reply": "2021-11-09T08:13:33.345994Z",
     "shell.execute_reply.started": "2021-11-09T08:13:16.806524Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  33 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  33 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  29 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=12)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metamodel_poly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=12)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metamodel_functions\n",
      "Exit <class 'KeyError'>\n",
      "metamodel_functions_no_GD\n",
      "Exit <class 'KeyError'>\n",
      "metamodel_functions_no_GD_poly\n",
      "symbolic_regression_functions\n",
      "Exit <class 'KeyError'>\n",
      "per_network_polynomials\n",
      "Exit <class 'KeyError'>\n",
      "FV Calculation Time: 0:00:14\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "print('------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "function_values_test_list = []\n",
    "for lambda_net_test_dataset, polynomial_dict_test in zip(lambda_net_test_dataset_list, polynomial_dict_test_list):\n",
    "    function_values_test = calculate_all_function_values(lambda_net_test_dataset, polynomial_dict_test)\n",
    "    function_values_test_list.append(function_values_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('FV Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:13:33.348351Z",
     "iopub.status.busy": "2021-11-09T08:13:33.348188Z",
     "iopub.status.idle": "2021-11-09T08:13:35.315078Z",
     "shell.execute_reply": "2021-11-09T08:13:35.314480Z",
     "shell.execute_reply.started": "2021-11-09T08:13:33.348329Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------\n",
      "lambda_preds_VS_target_polynomials\n",
      "lambda_preds_VS_lstsq_lambda_pred_polynomials\n",
      "lambda_preds_VS_lstsq_target_polynomials\n",
      "lambda_preds_VS_inet_polynomials\n",
      "lambda_preds_VS_metamodel_poly\n",
      "lambda_preds_VS_metamodel_functions_no_GD_poly\n",
      "target_polynomials_VS_lstsq_lambda_pred_polynomials\n",
      "target_polynomials_VS_lstsq_target_polynomials\n",
      "target_polynomials_VS_inet_polynomials\n",
      "target_polynomials_VS_metamodel_poly\n",
      "target_polynomials_VS_metamodel_functions_no_GD_poly\n",
      "lstsq_lambda_pred_polynomials_VS_lstsq_target_polynomials\n",
      "lstsq_lambda_pred_polynomials_VS_inet_polynomials\n",
      "lstsq_lambda_pred_polynomials_VS_metamodel_poly\n",
      "lstsq_lambda_pred_polynomials_VS_metamodel_functions_no_GD_poly\n",
      "lstsq_target_polynomials_VS_inet_polynomials\n",
      "lstsq_target_polynomials_VS_metamodel_poly\n",
      "lstsq_target_polynomials_VS_metamodel_functions_no_GD_poly\n",
      "inet_polynomials_VS_metamodel_poly\n",
      "inet_polynomials_VS_metamodel_functions_no_GD_poly\n",
      "metamodel_poly_VS_metamodel_functions_no_GD_poly\n",
      "Score Calculation Time: 0:00:01\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "scores_test_list = []\n",
    "distrib_dict_test_list = []\n",
    "runtime_distrib_list = []\n",
    "for function_values_test, polynomial_dict_test, runtimes_dict in zip(function_values_test_list, polynomial_dict_test_list, runtimes_list):\n",
    "    scores_test, distrib_test = evaluate_all_predictions(function_values_test, polynomial_dict_test)\n",
    "    scores_test_list.append(scores_test)\n",
    "    distrib_dict_test_list.append(distrib_test)\n",
    "    runtimes_list.append(pd.DataFrame(runtimes_dict))\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Score Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('---------------------------------------------------------------------------------------------------------------------------')         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:11:19.570599Z",
     "iopub.status.busy": "2021-11-09T08:11:19.570252Z",
     "iopub.status.idle": "2021-11-09T08:11:19.687775Z",
     "shell.execute_reply": "2021-11-09T08:11:19.686945Z",
     "shell.execute_reply.started": "2021-11-09T08:11:19.570577Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier_type = 'epochs' if samples_list == None else 'samples'\n",
    "save_results(scores_list=scores_test_list, by=identifier_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:11:19.906434Z",
     "iopub.status.busy": "2021-11-09T08:11:19.905954Z",
     "iopub.status.idle": "2021-11-09T08:11:19.995989Z",
     "shell.execute_reply": "2021-11-09T08:11:19.994935Z",
     "shell.execute_reply.started": "2021-11-09T08:11:19.906382Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:11:20.136939Z",
     "iopub.status.busy": "2021-11-09T08:11:20.136465Z",
     "iopub.status.idle": "2021-11-09T08:11:20.228066Z",
     "shell.execute_reply": "2021-11-09T08:11:20.227024Z",
     "shell.execute_reply.started": "2021-11-09T08:11:20.136893Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 385)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden1_4096 (Dense)            (None, 4096)         1581056     input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation1_relu (Activation)   (None, 4096)         0           hidden1_4096[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hidden2_2048 (Dense)            (None, 2048)         8390656     activation1_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation2_relu (Activation)   (None, 2048)         0           hidden2_2048[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hidden3_1024 (Dense)            (None, 1024)         2098176     activation2_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation3_relu (Activation)   (None, 1024)         0           hidden3_1024[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hidden4_512 (Dense)             (None, 512)          524800      activation3_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation4_relu (Activation)   (None, 512)          0           hidden4_512[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output_coeff_3 (Dense)          (None, 3)            1539        activation4_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier1_6 (Dense)    (None, 6)            3078        activation4_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier2_6 (Dense)    (None, 6)            3078        activation4_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_identifier3_6 (Dense)    (None, 6)            3078        activation4_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_combined (Concatenate)   (None, 21)           0           output_coeff_3[0][0]             \n",
      "                                                                 output_identifier1_6[0][0]       \n",
      "                                                                 output_identifier2_6[0][0]       \n",
      "                                                                 output_identifier3_6[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 12,605,461\n",
      "Trainable params: 12,605,461\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "{'name': 'model', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 385), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input'}, 'name': 'input', 'inbound_nodes': []}, {'class_name': 'Dense', 'config': {'name': 'hidden1_4096', 'trainable': True, 'dtype': 'float32', 'units': 4096, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'hidden1_4096', 'inbound_nodes': [[['input', 0, 0, {}]]]}, {'class_name': 'Activation', 'config': {'name': 'activation1_relu', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}, 'name': 'activation1_relu', 'inbound_nodes': [[['hidden1_4096', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'hidden2_2048', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'hidden2_2048', 'inbound_nodes': [[['activation1_relu', 0, 0, {}]]]}, {'class_name': 'Activation', 'config': {'name': 'activation2_relu', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}, 'name': 'activation2_relu', 'inbound_nodes': [[['hidden2_2048', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'hidden3_1024', 'trainable': True, 'dtype': 'float32', 'units': 1024, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'hidden3_1024', 'inbound_nodes': [[['activation2_relu', 0, 0, {}]]]}, {'class_name': 'Activation', 'config': {'name': 'activation3_relu', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}, 'name': 'activation3_relu', 'inbound_nodes': [[['hidden3_1024', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'hidden4_512', 'trainable': True, 'dtype': 'float32', 'units': 512, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'hidden4_512', 'inbound_nodes': [[['activation3_relu', 0, 0, {}]]]}, {'class_name': 'Activation', 'config': {'name': 'activation4_relu', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}, 'name': 'activation4_relu', 'inbound_nodes': [[['hidden4_512', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'output_coeff_3', 'trainable': True, 'dtype': 'float32', 'units': 3, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'output_coeff_3', 'inbound_nodes': [[['activation4_relu', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'output_identifier1_6', 'trainable': True, 'dtype': 'float32', 'units': 6, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'output_identifier1_6', 'inbound_nodes': [[['activation4_relu', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'output_identifier2_6', 'trainable': True, 'dtype': 'float32', 'units': 6, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'output_identifier2_6', 'inbound_nodes': [[['activation4_relu', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'output_identifier3_6', 'trainable': True, 'dtype': 'float32', 'units': 6, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'output_identifier3_6', 'inbound_nodes': [[['activation4_relu', 0, 0, {}]]]}, {'class_name': 'Concatenate', 'config': {'name': 'output_combined', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'name': 'output_combined', 'inbound_nodes': [[['output_coeff_3', 0, 0, {}], ['output_identifier1_6', 0, 0, {}], ['output_identifier2_6', 0, 0, {}], ['output_identifier3_6', 0, 0, {}]]]}], 'input_layers': [['input', 0, 0]], 'output_layers': [['output_combined', 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:11:20.305492Z",
     "iopub.status.busy": "2021-11-09T08:11:20.304985Z",
     "iopub.status.idle": "2021-11-09T08:11:20.395913Z",
     "shell.execute_reply": "2021-11-09T08:11:20.395196Z",
     "shell.execute_reply.started": "2021-11-09T08:11:20.305422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:11:20.494176Z",
     "iopub.status.busy": "2021-11-09T08:11:20.493695Z",
     "iopub.status.idle": "2021-11-09T08:11:20.565238Z",
     "shell.execute_reply": "2021-11-09T08:11:20.564445Z",
     "shell.execute_reply.started": "2021-11-09T08:11:20.494128Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#0.183\t0.234\t3.604\t0.143\t0.687\t2.559\t0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T08:11:21.291344Z",
     "iopub.status.busy": "2021-11-09T08:11:21.290775Z",
     "iopub.status.idle": "2021-11-09T08:11:21.388017Z",
     "shell.execute_reply": "2021-11-09T08:11:21.387224Z",
     "shell.execute_reply.started": "2021-11-09T08:11:21.291283Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Multilabel</th>\n",
       "      <th>MAE FV</th>\n",
       "      <th>RMSE FV</th>\n",
       "      <th>MAPE FV</th>\n",
       "      <th>R2 FV</th>\n",
       "      <th>RAAE FV</th>\n",
       "      <th>RMAE FV</th>\n",
       "      <th>MEAN STD FV DIFF</th>\n",
       "      <th>MEAN FV1</th>\n",
       "      <th>MEAN FV2</th>\n",
       "      <th>STD FV1</th>\n",
       "      <th>STD FV2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_target_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.018</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_target_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_inet_polynomials</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_metamodel_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001463936.000</td>\n",
       "      <td>4165046272.000</td>\n",
       "      <td>1288436224.000</td>\n",
       "      <td>-31879060954315262787584.000</td>\n",
       "      <td>8579871081.023</td>\n",
       "      <td>2423282176.000</td>\n",
       "      <td>3694552320.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>1922989696.000</td>\n",
       "      <td>0.509</td>\n",
       "      <td>41606041600.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>0.361</td>\n",
       "      <td>1.041</td>\n",
       "      <td>16.882</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.931</td>\n",
       "      <td>28.334</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_metamodel_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001463936.000</td>\n",
       "      <td>4165046272.000</td>\n",
       "      <td>1287990016.000</td>\n",
       "      <td>-32116408850299142275072.000</td>\n",
       "      <td>8611751792.485</td>\n",
       "      <td>2432286720.000</td>\n",
       "      <td>3694552320.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>1922989696.000</td>\n",
       "      <td>0.510</td>\n",
       "      <td>41606041600.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.510</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.361</td>\n",
       "      <td>1.041</td>\n",
       "      <td>1.445</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.646</td>\n",
       "      <td>1.401</td>\n",
       "      <td>9.788</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.534</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_metamodel_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001463936.000</td>\n",
       "      <td>4165046272.000</td>\n",
       "      <td>1288113664.000</td>\n",
       "      <td>-31851926875986181750784.000</td>\n",
       "      <td>8576218672.166</td>\n",
       "      <td>2422250752.000</td>\n",
       "      <td>3694552320.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>1922989696.000</td>\n",
       "      <td>0.509</td>\n",
       "      <td>41606041600.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.024</td>\n",
       "      <td>4.671</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.931</td>\n",
       "      <td>28.333</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_metamodel_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001463936.000</td>\n",
       "      <td>4165046272.000</td>\n",
       "      <td>1287990016.000</td>\n",
       "      <td>-32116410493349509726208.000</td>\n",
       "      <td>8611751792.485</td>\n",
       "      <td>2432286720.000</td>\n",
       "      <td>3694552320.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>1922989696.000</td>\n",
       "      <td>0.510</td>\n",
       "      <td>41606041600.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.510</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_metamodel_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001463936.000</td>\n",
       "      <td>4165046272.000</td>\n",
       "      <td>1282589056.000</td>\n",
       "      <td>-32299431348702199939072.000</td>\n",
       "      <td>8636254959.584</td>\n",
       "      <td>2439207424.000</td>\n",
       "      <td>3694552320.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>1922989696.000</td>\n",
       "      <td>0.508</td>\n",
       "      <td>41606041600.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-4.069</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.508</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metamodel_poly_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.280</td>\n",
       "      <td>21.017</td>\n",
       "      <td>-879065263544741654119921483776.000</td>\n",
       "      <td>147855680860857.844</td>\n",
       "      <td>164086994173952.000</td>\n",
       "      <td>0.129</td>\n",
       "      <td>1922989696.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41606041600.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     MAE  RMSE   MAPE  \\\n",
       "lambda_preds_VS_target_polynomials                   NaN   NaN    NaN   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials        NaN   NaN    NaN   \n",
       "lambda_preds_VS_lstsq_target_polynomials             NaN   NaN    NaN   \n",
       "lambda_preds_VS_inet_polynomials                     NaN   NaN    NaN   \n",
       "lambda_preds_VS_metamodel_poly                       NaN   NaN    NaN   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   NaN    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.361 1.041 16.882   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000  0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.530 0.931 28.334   \n",
       "target_polynomials_VS_metamodel_poly                 NaN   NaN    NaN   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.361 1.041  1.445   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.646 1.401  9.788   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly      NaN   NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.530 0.931 28.333   \n",
       "lstsq_target_polynomials_VS_metamodel_poly           NaN   NaN    NaN   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                   NaN   NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   NaN    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN   NaN    NaN   \n",
       "\n",
       "                                                    Accuracy  \\\n",
       "lambda_preds_VS_target_polynomials                       NaN   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials            NaN   \n",
       "lambda_preds_VS_lstsq_target_polynomials                 NaN   \n",
       "lambda_preds_VS_inet_polynomials                         NaN   \n",
       "lambda_preds_VS_metamodel_poly                           NaN   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly           NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...     0.372   \n",
       "target_polynomials_VS_lstsq_target_polynomials         1.000   \n",
       "target_polynomials_VS_inet_polynomials                 0.173   \n",
       "target_polynomials_VS_metamodel_poly                     NaN   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...       NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...     0.372   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials      0.187   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly          NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...       NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials           0.173   \n",
       "lstsq_target_polynomials_VS_metamodel_poly               NaN   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...       NaN   \n",
       "inet_polynomials_VS_metamodel_poly                       NaN   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly       NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly         NaN   \n",
       "\n",
       "                                                    Accuracy Multilabel  \\\n",
       "lambda_preds_VS_target_polynomials                                  NaN   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                       NaN   \n",
       "lambda_preds_VS_lstsq_target_polynomials                            NaN   \n",
       "lambda_preds_VS_inet_polynomials                                    NaN   \n",
       "lambda_preds_VS_metamodel_poly                                      NaN   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly                      NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...                0.030   \n",
       "target_polynomials_VS_lstsq_target_polynomials                    1.000   \n",
       "target_polynomials_VS_inet_polynomials                            0.000   \n",
       "target_polynomials_VS_metamodel_poly                                NaN   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...                  NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...                0.030   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials                 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly                     NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...                  NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                      0.000   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                          NaN   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...                  NaN   \n",
       "inet_polynomials_VS_metamodel_poly                                  NaN   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly                  NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly                    NaN   \n",
       "\n",
       "                                                           MAE FV  \\\n",
       "lambda_preds_VS_target_polynomials                          0.003   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials               0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    0.003   \n",
       "lambda_preds_VS_inet_polynomials                            0.023   \n",
       "lambda_preds_VS_metamodel_poly                     2001463936.000   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly              0.020   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...          0.002   \n",
       "target_polynomials_VS_lstsq_target_polynomials              0.000   \n",
       "target_polynomials_VS_inet_polynomials                      0.025   \n",
       "target_polynomials_VS_metamodel_poly               2001463936.000   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...          0.023   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          0.002   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           0.023   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    2001463936.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          0.019   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                0.025   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         2001463936.000   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...          0.023   \n",
       "inet_polynomials_VS_metamodel_poly                 2001463936.000   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          0.029   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly            0.249   \n",
       "\n",
       "                                                          RMSE FV  \\\n",
       "lambda_preds_VS_target_polynomials                          0.004   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials               0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    0.004   \n",
       "lambda_preds_VS_inet_polynomials                            0.029   \n",
       "lambda_preds_VS_metamodel_poly                     4165046272.000   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly              0.024   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...          0.004   \n",
       "target_polynomials_VS_lstsq_target_polynomials              0.000   \n",
       "target_polynomials_VS_inet_polynomials                      0.032   \n",
       "target_polynomials_VS_metamodel_poly               4165046272.000   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...          0.029   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          0.004   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           0.029   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    4165046272.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          0.024   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                0.032   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         4165046272.000   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...          0.029   \n",
       "inet_polynomials_VS_metamodel_poly                 4165046272.000   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          0.036   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly            0.280   \n",
       "\n",
       "                                                          MAPE FV  \\\n",
       "lambda_preds_VS_target_polynomials                          0.075   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials               0.018   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    0.075   \n",
       "lambda_preds_VS_inet_polynomials                            0.772   \n",
       "lambda_preds_VS_metamodel_poly                     1288436224.000   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly              0.564   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...          0.025   \n",
       "target_polynomials_VS_lstsq_target_polynomials              0.000   \n",
       "target_polynomials_VS_inet_polynomials                      0.798   \n",
       "target_polynomials_VS_metamodel_poly               1287990016.000   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...          0.573   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          0.086   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           1.534   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    1288113664.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          4.671   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                0.797   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         1287990016.000   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...          0.573   \n",
       "inet_polynomials_VS_metamodel_poly                 1282589056.000   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          0.601   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly           21.017   \n",
       "\n",
       "                                                                                 R2 FV  \\\n",
       "lambda_preds_VS_target_polynomials                                               0.927   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                                    1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                                         0.927   \n",
       "lambda_preds_VS_inet_polynomials                                                 0.920   \n",
       "lambda_preds_VS_metamodel_poly                            -31879060954315262787584.000   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly                                   0.860   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...                               0.976   \n",
       "target_polynomials_VS_lstsq_target_polynomials                                   1.000   \n",
       "target_polynomials_VS_inet_polynomials                                           0.892   \n",
       "target_polynomials_VS_metamodel_poly                      -32116408850299142275072.000   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...                               0.813   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...                               0.927   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials                                0.920   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly           -31851926875986181750784.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...                               0.860   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                                     0.892   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                -32116410493349509726208.000   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...                               0.813   \n",
       "inet_polynomials_VS_metamodel_poly                        -32299431348702199939072.000   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly                              -4.069   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -879065263544741654119921483776.000   \n",
       "\n",
       "                                                               RAAE FV  \\\n",
       "lambda_preds_VS_target_polynomials                               0.048   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                    0.005   \n",
       "lambda_preds_VS_lstsq_target_polynomials                         0.048   \n",
       "lambda_preds_VS_inet_polynomials                                 0.164   \n",
       "lambda_preds_VS_metamodel_poly                          8579871081.023   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly                   0.193   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...               0.030   \n",
       "target_polynomials_VS_lstsq_target_polynomials                   0.000   \n",
       "target_polynomials_VS_inet_polynomials                           0.179   \n",
       "target_polynomials_VS_metamodel_poly                    8611751792.485   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...               0.215   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...               0.046   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials                0.164   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly         8576218672.166   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...               0.192   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                     0.179   \n",
       "lstsq_target_polynomials_VS_metamodel_poly              8611751792.485   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...               0.215   \n",
       "inet_polynomials_VS_metamodel_poly                      8636254959.584   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly               0.538   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   147855680860857.844   \n",
       "\n",
       "                                                               RMAE FV  \\\n",
       "lambda_preds_VS_target_polynomials                               0.184   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                    0.036   \n",
       "lambda_preds_VS_lstsq_target_polynomials                         0.184   \n",
       "lambda_preds_VS_inet_polynomials                                 0.382   \n",
       "lambda_preds_VS_metamodel_poly                          2423282176.000   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly                   0.379   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...               0.075   \n",
       "target_polynomials_VS_lstsq_target_polynomials                   0.000   \n",
       "target_polynomials_VS_inet_polynomials                           0.429   \n",
       "target_polynomials_VS_metamodel_poly                    2432286720.000   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...               0.464   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...               0.159   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials                0.393   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly         2422250752.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...               0.389   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                     0.429   \n",
       "lstsq_target_polynomials_VS_metamodel_poly              2432286720.000   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...               0.464   \n",
       "inet_polynomials_VS_metamodel_poly                      2439207424.000   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly               0.763   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   164086994173952.000   \n",
       "\n",
       "                                                    MEAN STD FV DIFF  \\\n",
       "lambda_preds_VS_target_polynomials                             0.004   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                  0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials                       0.004   \n",
       "lambda_preds_VS_inet_polynomials                               0.027   \n",
       "lambda_preds_VS_metamodel_poly                        3694552320.000   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly                 0.024   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...             0.003   \n",
       "target_polynomials_VS_lstsq_target_polynomials                 0.000   \n",
       "target_polynomials_VS_inet_polynomials                         0.029   \n",
       "target_polynomials_VS_metamodel_poly                  3694552320.000   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...             0.027   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...             0.003   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials              0.027   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly       3694552320.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...             0.024   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                   0.029   \n",
       "lstsq_target_polynomials_VS_metamodel_poly            3694552320.000   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...             0.027   \n",
       "inet_polynomials_VS_metamodel_poly                    3694552320.000   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly             0.033   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly               0.129   \n",
       "\n",
       "                                                         MEAN FV1  \\\n",
       "lambda_preds_VS_target_polynomials                         -0.037   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials              -0.037   \n",
       "lambda_preds_VS_lstsq_target_polynomials                   -0.037   \n",
       "lambda_preds_VS_inet_polynomials                           -0.037   \n",
       "lambda_preds_VS_metamodel_poly                             -0.037   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly             -0.037   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...         -0.037   \n",
       "target_polynomials_VS_lstsq_target_polynomials             -0.037   \n",
       "target_polynomials_VS_inet_polynomials                     -0.037   \n",
       "target_polynomials_VS_metamodel_poly                       -0.037   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...         -0.037   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...         -0.037   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials          -0.037   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -0.037   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...         -0.037   \n",
       "lstsq_target_polynomials_VS_inet_polynomials               -0.037   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -0.037   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...         -0.037   \n",
       "inet_polynomials_VS_metamodel_poly                         -0.036   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly         -0.036   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   1922989696.000   \n",
       "\n",
       "                                                         MEAN FV2  \\\n",
       "lambda_preds_VS_target_polynomials                         -0.037   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials              -0.037   \n",
       "lambda_preds_VS_lstsq_target_polynomials                   -0.037   \n",
       "lambda_preds_VS_inet_polynomials                           -0.036   \n",
       "lambda_preds_VS_metamodel_poly                     1922989696.000   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly                NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...         -0.037   \n",
       "target_polynomials_VS_lstsq_target_polynomials             -0.037   \n",
       "target_polynomials_VS_inet_polynomials                     -0.036   \n",
       "target_polynomials_VS_metamodel_poly               1922989696.000   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...            NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...         -0.037   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials          -0.036   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    1922989696.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...            NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials               -0.036   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         1922989696.000   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...            NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 1922989696.000   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly            NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly              NaN   \n",
       "\n",
       "                                                           STD FV1  \\\n",
       "lambda_preds_VS_target_polynomials                           0.509   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                0.509   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     0.509   \n",
       "lambda_preds_VS_inet_polynomials                             0.509   \n",
       "lambda_preds_VS_metamodel_poly                               0.509   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly               0.509   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           0.510   \n",
       "target_polynomials_VS_lstsq_target_polynomials               0.510   \n",
       "target_polynomials_VS_inet_polynomials                       0.510   \n",
       "target_polynomials_VS_metamodel_poly                         0.510   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...           0.510   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           0.509   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.509   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly              0.509   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           0.509   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.510   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                   0.510   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...           0.510   \n",
       "inet_polynomials_VS_metamodel_poly                           0.508   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly           0.508   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   41606041600.000   \n",
       "\n",
       "                                                           STD FV2  \n",
       "lambda_preds_VS_target_polynomials                           0.510  \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                0.509  \n",
       "lambda_preds_VS_lstsq_target_polynomials                     0.510  \n",
       "lambda_preds_VS_inet_polynomials                             0.508  \n",
       "lambda_preds_VS_metamodel_poly                     41606041600.000  \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly                 NaN  \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           0.509  \n",
       "target_polynomials_VS_lstsq_target_polynomials               0.510  \n",
       "target_polynomials_VS_inet_polynomials                       0.508  \n",
       "target_polynomials_VS_metamodel_poly               41606041600.000  \n",
       "target_polynomials_VS_metamodel_functions_no_GD...             NaN  \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           0.510  \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.508  \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    41606041600.000  \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...             NaN  \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.508  \n",
       "lstsq_target_polynomials_VS_metamodel_poly         41606041600.000  \n",
       "lstsq_target_polynomials_VS_metamodel_functions...             NaN  \n",
       "inet_polynomials_VS_metamodel_poly                 41606041600.000  \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly             NaN  \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly               NaN  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T08:12:08.578488Z",
     "iopub.status.busy": "2021-11-09T08:12:08.577958Z",
     "iopub.status.idle": "2021-11-09T08:12:09.555903Z",
     "shell.execute_reply": "2021-11-09T08:12:09.554970Z",
     "shell.execute_reply.started": "2021-11-09T08:12:08.578440Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L-0</th>\n",
       "      <th>L-1</th>\n",
       "      <th>L-2</th>\n",
       "      <th>L-3</th>\n",
       "      <th>L-4</th>\n",
       "      <th>L-5</th>\n",
       "      <th>L-6</th>\n",
       "      <th>L-7</th>\n",
       "      <th>L-8</th>\n",
       "      <th>L-9</th>\n",
       "      <th>L-10</th>\n",
       "      <th>L-11</th>\n",
       "      <th>L-12</th>\n",
       "      <th>L-13</th>\n",
       "      <th>L-14</th>\n",
       "      <th>L-15</th>\n",
       "      <th>L-16</th>\n",
       "      <th>L-17</th>\n",
       "      <th>L-18</th>\n",
       "      <th>L-19</th>\n",
       "      <th>L-20</th>\n",
       "      <th>L-21</th>\n",
       "      <th>L-22</th>\n",
       "      <th>L-23</th>\n",
       "      <th>L-24</th>\n",
       "      <th>L-25</th>\n",
       "      <th>L-26</th>\n",
       "      <th>L-27</th>\n",
       "      <th>L-28</th>\n",
       "      <th>L-29</th>\n",
       "      <th>L-30</th>\n",
       "      <th>L-31</th>\n",
       "      <th>L-32</th>\n",
       "      <th>L-33</th>\n",
       "      <th>L-34</th>\n",
       "      <th>L-35</th>\n",
       "      <th>L-36</th>\n",
       "      <th>L-37</th>\n",
       "      <th>L-38</th>\n",
       "      <th>L-39</th>\n",
       "      <th>L-40</th>\n",
       "      <th>L-41</th>\n",
       "      <th>L-42</th>\n",
       "      <th>L-43</th>\n",
       "      <th>L-44</th>\n",
       "      <th>L-45</th>\n",
       "      <th>L-46</th>\n",
       "      <th>L-47</th>\n",
       "      <th>L-48</th>\n",
       "      <th>L-49</th>\n",
       "      <th>L-50</th>\n",
       "      <th>L-51</th>\n",
       "      <th>L-52</th>\n",
       "      <th>L-53</th>\n",
       "      <th>L-54</th>\n",
       "      <th>L-55</th>\n",
       "      <th>L-56</th>\n",
       "      <th>L-57</th>\n",
       "      <th>L-58</th>\n",
       "      <th>L-59</th>\n",
       "      <th>L-60</th>\n",
       "      <th>L-61</th>\n",
       "      <th>L-62</th>\n",
       "      <th>L-63</th>\n",
       "      <th>L-64</th>\n",
       "      <th>L-65</th>\n",
       "      <th>L-66</th>\n",
       "      <th>L-67</th>\n",
       "      <th>L-68</th>\n",
       "      <th>L-69</th>\n",
       "      <th>L-70</th>\n",
       "      <th>L-71</th>\n",
       "      <th>L-72</th>\n",
       "      <th>L-73</th>\n",
       "      <th>L-74</th>\n",
       "      <th>L-75</th>\n",
       "      <th>L-76</th>\n",
       "      <th>L-77</th>\n",
       "      <th>L-78</th>\n",
       "      <th>L-79</th>\n",
       "      <th>L-80</th>\n",
       "      <th>L-81</th>\n",
       "      <th>L-82</th>\n",
       "      <th>L-83</th>\n",
       "      <th>L-84</th>\n",
       "      <th>L-85</th>\n",
       "      <th>L-86</th>\n",
       "      <th>L-87</th>\n",
       "      <th>L-88</th>\n",
       "      <th>L-89</th>\n",
       "      <th>L-90</th>\n",
       "      <th>L-91</th>\n",
       "      <th>L-92</th>\n",
       "      <th>L-93</th>\n",
       "      <th>L-94</th>\n",
       "      <th>L-95</th>\n",
       "      <th>L-96</th>\n",
       "      <th>L-97</th>\n",
       "      <th>L-98</th>\n",
       "      <th>L-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_target_polynomials</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_inet_polynomials</th>\n",
       "      <td>0.022</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_metamodel_poly</th>\n",
       "      <td>0.225</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.028</td>\n",
       "      <td>757.652</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.356</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.222</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.293</td>\n",
       "      <td>1.325</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.018</td>\n",
       "      <td>200146403328.000</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.179</td>\n",
       "      <td>1.073</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.022</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_metamodel_poly</th>\n",
       "      <td>0.225</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.028</td>\n",
       "      <td>757.652</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.357</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.222</td>\n",
       "      <td>1.281</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.293</td>\n",
       "      <td>1.325</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.018</td>\n",
       "      <td>200146403328.000</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.179</td>\n",
       "      <td>1.073</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.022</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_metamodel_poly</th>\n",
       "      <td>0.225</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.028</td>\n",
       "      <td>757.652</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.356</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.222</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.293</td>\n",
       "      <td>1.325</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.018</td>\n",
       "      <td>200146403328.000</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.179</td>\n",
       "      <td>1.073</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.022</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_metamodel_poly</th>\n",
       "      <td>0.225</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.028</td>\n",
       "      <td>757.652</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.357</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.222</td>\n",
       "      <td>1.281</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.293</td>\n",
       "      <td>1.325</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.018</td>\n",
       "      <td>200146403328.000</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.179</td>\n",
       "      <td>1.073</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_metamodel_poly</th>\n",
       "      <td>0.224</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.026</td>\n",
       "      <td>757.656</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.047</td>\n",
       "      <td>1.365</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.219</td>\n",
       "      <td>1.304</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.285</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.022</td>\n",
       "      <td>200146403328.000</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.176</td>\n",
       "      <td>1.072</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metamodel_poly_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.176</td>\n",
       "      <td>1.068</td>\n",
       "      <td>0.134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     L-0   L-1   L-2   L-3  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.001 0.001 0.014   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.001 0.000 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.001 0.001 0.014   \n",
       "lambda_preds_VS_inet_polynomials                   0.022 0.026 0.010 0.004   \n",
       "lambda_preds_VS_metamodel_poly                     0.225 0.167 0.496 0.032   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.059   NaN   NaN 0.005   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.001 0.000 0.014   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.022 0.026 0.010 0.011   \n",
       "target_polynomials_VS_metamodel_poly               0.225 0.167 0.496 0.043   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.060   NaN   NaN 0.018   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.001 0.000 0.014   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.022 0.026 0.010 0.004   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.225 0.167 0.496 0.032   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.059   NaN   NaN 0.005   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.022 0.026 0.010 0.011   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.225 0.167 0.496 0.043   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.060   NaN   NaN 0.018   \n",
       "inet_polynomials_VS_metamodel_poly                 0.224 0.178 0.490 0.034   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.059   NaN   NaN 0.007   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.232   NaN   NaN 0.031   \n",
       "\n",
       "                                                     L-4   L-5   L-6   L-7  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.001 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.001 0.000 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.015 0.013 0.019 0.007   \n",
       "lambda_preds_VS_metamodel_poly                     0.150 0.169 0.067 0.115   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   NaN   NaN 0.019   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.000 0.000 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.015 0.013 0.020 0.007   \n",
       "target_polynomials_VS_metamodel_poly               0.150 0.169 0.067 0.116   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   NaN   NaN 0.019   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.000 0.000 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.015 0.012 0.019 0.007   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.150 0.169 0.067 0.116   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN   NaN 0.019   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.015 0.013 0.020 0.007   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.150 0.169 0.067 0.116   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   NaN   NaN 0.019   \n",
       "inet_polynomials_VS_metamodel_poly                 0.144 0.160 0.070 0.110   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   NaN   NaN 0.022   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN   NaN   NaN 0.114   \n",
       "\n",
       "                                                     L-8   L-9  L-10  L-11  \\\n",
       "lambda_preds_VS_target_polynomials                 0.000 0.000 0.001 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.000 0.000 0.001 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.002 0.044 0.006 0.031   \n",
       "lambda_preds_VS_metamodel_poly                     0.031 0.523 0.078 0.116   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   NaN 0.008 0.004   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.002 0.044 0.006 0.031   \n",
       "target_polynomials_VS_metamodel_poly               0.031 0.523 0.077 0.116   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   NaN 0.008 0.004   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.000 0.000 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.002 0.044 0.006 0.031   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.031 0.523 0.078 0.116   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN 0.009 0.004   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.002 0.044 0.006 0.031   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.031 0.523 0.077 0.116   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   NaN 0.008 0.004   \n",
       "inet_polynomials_VS_metamodel_poly                 0.031 0.517 0.074 0.137   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   NaN 0.012 0.032   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN   NaN 0.073 0.115   \n",
       "\n",
       "                                                    L-12  L-13  L-14  L-15  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.000 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.000 0.000 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.005 0.005 0.036 0.041   \n",
       "lambda_preds_VS_metamodel_poly                     0.446 0.252 0.294 0.493   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN 0.004 0.038   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.000 0.000 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.005 0.005 0.035 0.041   \n",
       "target_polynomials_VS_metamodel_poly               0.446 0.252 0.294 0.493   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN 0.004 0.038   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.000 0.000 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.005 0.005 0.036 0.041   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.446 0.252 0.294 0.493   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN 0.004 0.038   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.005 0.005 0.035 0.041   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.446 0.252 0.294 0.493   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN 0.004 0.038   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.446 0.250 0.305 0.473   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN 0.009 0.043   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN 0.254 0.303   NaN   \n",
       "\n",
       "                                                    L-16  L-17  L-18  L-19  \\\n",
       "lambda_preds_VS_target_polynomials                 0.000 0.002 0.042 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.000 0.002 0.042 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.057 0.020 0.025 0.016   \n",
       "lambda_preds_VS_metamodel_poly                     0.627 0.403 0.085 0.035   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN 0.041   NaN   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.002 0.042 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.057 0.022 0.061 0.017   \n",
       "target_polynomials_VS_metamodel_poly               0.627 0.403 0.101 0.035   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN 0.042   NaN   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.002 0.042 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.057 0.020 0.025 0.016   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.627 0.403 0.085 0.035   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN 0.041   NaN   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.057 0.022 0.061 0.017   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.627 0.403 0.101 0.035   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN 0.042   NaN   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.631 0.393 0.074 0.034   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN 0.026   NaN   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN 0.386   NaN   NaN   \n",
       "\n",
       "                                                    L-20  L-21  L-22  L-23  \\\n",
       "lambda_preds_VS_target_polynomials                 0.000 0.001 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.001 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.000 0.001 0.000 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.007 0.022 0.027 0.014   \n",
       "lambda_preds_VS_metamodel_poly                     0.154 0.431 0.079 0.092   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.003   NaN   NaN 0.034   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.001 0.000 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.007 0.023 0.027 0.014   \n",
       "target_polynomials_VS_metamodel_poly               0.154 0.431 0.079 0.092   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.003   NaN   NaN 0.034   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.001 0.000 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.007 0.022 0.027 0.014   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.154 0.431 0.079 0.092   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.003   NaN   NaN 0.034   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.007 0.023 0.027 0.014   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.154 0.431 0.079 0.092   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.003   NaN   NaN 0.034   \n",
       "inet_polynomials_VS_metamodel_poly                 0.147 0.428 0.077 0.094   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.007   NaN   NaN 0.033   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.152   NaN   NaN 0.102   \n",
       "\n",
       "                                                    L-24  L-25  L-26  L-27  \\\n",
       "lambda_preds_VS_target_polynomials                 0.000 0.000 0.001 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.001 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.000 0.000 0.001 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.009 0.012 0.014 0.012   \n",
       "lambda_preds_VS_metamodel_poly                     0.011 0.566 0.146 0.091   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.002   NaN   NaN 0.007   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.000 0.001 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.009 0.012 0.014 0.011   \n",
       "target_polynomials_VS_metamodel_poly               0.011 0.566 0.146 0.090   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.002   NaN   NaN 0.007   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.000 0.001 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.009 0.012 0.014 0.012   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.011 0.566 0.146 0.091   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.002   NaN   NaN 0.007   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.009 0.012 0.014 0.011   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.011 0.566 0.146 0.090   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.002   NaN   NaN 0.007   \n",
       "inet_polynomials_VS_metamodel_poly                 0.008 0.562 0.147 0.079   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.008   NaN   NaN 0.008   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.010   NaN   NaN 0.088   \n",
       "\n",
       "                                                    L-28  L-29    L-30  L-31  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.000   0.001 0.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.000   0.000 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.000   0.001 0.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.213 0.004   0.020 0.016   \n",
       "lambda_preds_VS_metamodel_poly                     0.306 0.028 757.652 0.032   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN 0.003     NaN   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.001   0.001 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000   0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.213 0.004   0.021 0.016   \n",
       "target_polynomials_VS_metamodel_poly               0.306 0.028 757.652 0.032   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN 0.002     NaN   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.001   0.001 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.213 0.004   0.020 0.016   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.306 0.028 757.652 0.032   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN 0.003     NaN   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.213 0.004   0.021 0.016   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.306 0.028 757.652 0.032   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN 0.002     NaN   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.127 0.026 757.656 0.026   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN 0.002     NaN   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN 0.026     NaN   NaN   \n",
       "\n",
       "                                                    L-32  L-33  L-34  L-35  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.000 0.002 0.017   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.002 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.000 0.002 0.017   \n",
       "lambda_preds_VS_inet_polynomials                   0.009 0.007 0.026 0.036   \n",
       "lambda_preds_VS_metamodel_poly                     0.157 0.046 1.356 0.199   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN 0.000   NaN 0.044   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.000 0.001 0.017   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.009 0.007 0.026 0.050   \n",
       "target_polynomials_VS_metamodel_poly               0.156 0.046 1.357 0.205   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN 0.001   NaN 0.059   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.000 0.001 0.017   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.009 0.007 0.026 0.036   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.157 0.046 1.356 0.199   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN 0.000   NaN 0.044   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.009 0.007 0.026 0.050   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.156 0.046 1.357 0.205   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN 0.001   NaN 0.059   \n",
       "inet_polynomials_VS_metamodel_poly                 0.150 0.047 1.365 0.201   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN 0.007   NaN 0.018   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN 0.045   NaN 0.183   \n",
       "\n",
       "                                                    L-36  L-37  L-38  L-39  \\\n",
       "lambda_preds_VS_target_polynomials                 0.012 0.001 0.001 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.005 0.000 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.012 0.001 0.001 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.020 0.027 0.032 0.009   \n",
       "lambda_preds_VS_metamodel_poly                     0.932 0.041 0.352 0.037   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   NaN 0.035   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.012 0.001 0.000 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.019 0.027 0.032 0.009   \n",
       "target_polynomials_VS_metamodel_poly               0.929 0.041 0.352 0.037   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   NaN 0.036   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.012 0.001 0.000 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.018 0.027 0.032 0.009   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.932 0.041 0.352 0.037   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN 0.035   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.019 0.027 0.032 0.009   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.929 0.041 0.352 0.037   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   NaN 0.036   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.945 0.034 0.353 0.039   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   NaN 0.046   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN   NaN 0.355   NaN   \n",
       "\n",
       "                                                    L-40  L-41  L-42  L-43  \\\n",
       "lambda_preds_VS_target_polynomials                 0.050 0.000 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.050 0.000 0.000 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.007 0.024 0.009 0.010   \n",
       "lambda_preds_VS_metamodel_poly                     0.413 0.246 0.157 0.404   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.033 0.002 0.004 0.006   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.050 0.000 0.000 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.057 0.024 0.010 0.010   \n",
       "target_polynomials_VS_metamodel_poly               0.430 0.246 0.157 0.405   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.079 0.002 0.004 0.006   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.050 0.000 0.000 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.007 0.024 0.009 0.010   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.413 0.246 0.157 0.405   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.033 0.002 0.004 0.006   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.057 0.024 0.010 0.010   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.430 0.246 0.157 0.405   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.079 0.002 0.004 0.006   \n",
       "inet_polynomials_VS_metamodel_poly                 0.408 0.243 0.147 0.395   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.026 0.025 0.007 0.015   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.400 0.247 0.154 0.409   \n",
       "\n",
       "                                                    L-44  L-45  L-46  L-47  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.001 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.001 0.000 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.001 0.000 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.008 0.020 0.005 0.026   \n",
       "lambda_preds_VS_metamodel_poly                     0.034 0.548 0.030 0.141   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.002   NaN 0.018   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.001 0.000 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.008 0.020 0.005 0.026   \n",
       "target_polynomials_VS_metamodel_poly               0.035 0.548 0.030 0.142   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.002   NaN 0.018   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.001 0.000 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.008 0.020 0.005 0.026   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.034 0.548 0.030 0.141   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.001   NaN 0.018   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.008 0.020 0.005 0.026   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.035 0.548 0.030 0.142   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.002   NaN 0.018   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.031 0.561 0.035 0.148   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.007   NaN 0.023   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.035   NaN 0.016   NaN   \n",
       "\n",
       "                                                    L-48  L-49  L-50  L-51  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.001 0.004 0.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.004 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.001 0.004 0.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.037 0.009 0.027 0.006   \n",
       "lambda_preds_VS_metamodel_poly                     0.138 0.222 1.279 0.219   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN 0.026   NaN 0.007   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.000 0.002 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.037 0.009 0.026 0.006   \n",
       "target_polynomials_VS_metamodel_poly               0.138 0.222 1.281 0.219   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN 0.026   NaN 0.007   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.000 0.002 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.037 0.009 0.026 0.006   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.138 0.222 1.279 0.219   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN 0.026   NaN 0.007   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.037 0.009 0.026 0.006   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.138 0.222 1.281 0.219   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN 0.026   NaN 0.007   \n",
       "inet_polynomials_VS_metamodel_poly                 0.151 0.219 1.304 0.218   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN 0.026   NaN 0.012   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN 0.213   NaN 0.221   \n",
       "\n",
       "                                                    L-52  L-53  L-54  L-55  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.001 0.002 0.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.002 0.000 0.001 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.001 0.002 0.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.021 0.019 0.066 0.009   \n",
       "lambda_preds_VS_metamodel_poly                     0.238 0.342 0.821 0.149   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN 0.031 0.066 0.006   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.002 0.000 0.001 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.022 0.019 0.066 0.009   \n",
       "target_polynomials_VS_metamodel_poly               0.239 0.342 0.822 0.149   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN 0.031 0.066 0.007   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.002 0.000 0.001 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.021 0.019 0.066 0.009   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.238 0.342 0.821 0.149   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN 0.031 0.066 0.006   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.022 0.019 0.066 0.009   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.239 0.342 0.822 0.149   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN 0.031 0.066 0.007   \n",
       "inet_polynomials_VS_metamodel_poly                 0.230 0.344 0.803 0.146   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN 0.032 0.130 0.010   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN 0.323 0.806 0.146   \n",
       "\n",
       "                                                    L-56  L-57  L-58  L-59  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.000 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.000 0.000 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.000 0.000 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.013 0.045 0.019 0.007   \n",
       "lambda_preds_VS_metamodel_poly                     0.226 0.506 0.301 0.224   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   NaN   NaN   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.000 0.000 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.014 0.045 0.020 0.007   \n",
       "target_polynomials_VS_metamodel_poly               0.226 0.506 0.301 0.224   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   NaN   NaN   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.000 0.000 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.013 0.045 0.019 0.007   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.226 0.506 0.301 0.224   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN   NaN   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.014 0.045 0.020 0.007   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.226 0.506 0.301 0.224   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   NaN   NaN   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.225 0.499 0.290 0.227   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   NaN   NaN   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN   NaN   NaN   NaN   \n",
       "\n",
       "                                                    L-60  L-61  L-62  L-63  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.001 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.001 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.001 0.000 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.016 0.137 0.007 0.014   \n",
       "lambda_preds_VS_metamodel_poly                     0.293 1.325 0.090 0.262   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   NaN 0.006   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.001 0.000 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.015 0.137 0.007 0.015   \n",
       "target_polynomials_VS_metamodel_poly               0.293 1.325 0.090 0.261   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   NaN 0.005   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.001 0.000 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.016 0.137 0.007 0.014   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.293 1.325 0.090 0.262   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN 0.006   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.015 0.137 0.007 0.015   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.293 1.325 0.090 0.261   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   NaN 0.005   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.285 1.347 0.088 0.258   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   NaN 0.003   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN   NaN 0.091   NaN   \n",
       "\n",
       "                                                    L-64  L-65  L-66  L-67  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.000 0.001 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.000 0.001 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.000 0.001 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.029 0.037 0.032 0.023   \n",
       "lambda_preds_VS_metamodel_poly                     0.176 0.268 0.126 0.312   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   NaN   NaN   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.000 0.001 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.029 0.036 0.033 0.023   \n",
       "target_polynomials_VS_metamodel_poly               0.176 0.268 0.127 0.312   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   NaN   NaN   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.000 0.001 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.029 0.037 0.032 0.023   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.176 0.268 0.126 0.312   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN   NaN   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.029 0.036 0.033 0.023   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.176 0.268 0.127 0.312   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   NaN   NaN   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.184 0.274 0.124 0.298   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   NaN   NaN   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN   NaN   NaN   NaN   \n",
       "\n",
       "                                                    L-68  L-69  L-70  L-71  \\\n",
       "lambda_preds_VS_target_polynomials                 0.000 0.001 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.001 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.000 0.001 0.000 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.014 0.019 0.003 0.050   \n",
       "lambda_preds_VS_metamodel_poly                     0.113 0.375 0.311 0.547   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.002   NaN   NaN   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.001 0.000 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.014 0.019 0.003 0.051   \n",
       "target_polynomials_VS_metamodel_poly               0.113 0.375 0.311 0.548   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.002   NaN   NaN   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.001 0.000 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.014 0.019 0.003 0.050   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.113 0.375 0.311 0.547   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.002   NaN   NaN   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.014 0.019 0.003 0.051   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.113 0.375 0.311 0.548   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.002   NaN   NaN   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.110 0.372 0.310 0.552   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.015   NaN   NaN   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.113   NaN   NaN   NaN   \n",
       "\n",
       "                                                    L-72  L-73  L-74  L-75  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.000 0.006 0.002   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.000 0.005 0.002   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.000 0.006 0.002   \n",
       "lambda_preds_VS_inet_polynomials                   0.044 0.005 0.014 0.012   \n",
       "lambda_preds_VS_metamodel_poly                     0.590 0.033 0.753 0.203   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.017   NaN 0.114   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.000 0.003 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.044 0.005 0.015 0.012   \n",
       "target_polynomials_VS_metamodel_poly               0.590 0.033 0.754 0.203   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.017   NaN 0.112   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.000 0.003 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.044 0.005 0.014 0.012   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.590 0.033 0.753 0.203   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.017   NaN 0.113   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.044 0.005 0.015 0.012   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.590 0.033 0.754 0.203   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.017   NaN 0.112   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.591 0.033 0.739 0.203   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.049   NaN 0.112   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.583   NaN 0.730   NaN   \n",
       "\n",
       "                                                    L-76  L-77  L-78  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.001 0.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.001 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.001 0.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.015 0.027 0.009   \n",
       "lambda_preds_VS_metamodel_poly                     0.318 0.169 0.018   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   NaN   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.000 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.015 0.027 0.009   \n",
       "target_polynomials_VS_metamodel_poly               0.318 0.168 0.018   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   NaN   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.000 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.015 0.027 0.009   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.318 0.169 0.018   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.015 0.027 0.009   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.318 0.168 0.018   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   NaN   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.307 0.181 0.022   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   NaN   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN   NaN   NaN   \n",
       "\n",
       "                                                               L-79  L-80  \\\n",
       "lambda_preds_VS_target_polynomials                            0.006 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 0.005 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      0.006 0.001   \n",
       "lambda_preds_VS_inet_polynomials                              0.012 0.012   \n",
       "lambda_preds_VS_metamodel_poly                     200146403328.000 0.557   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly                  NaN 0.023   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            0.003 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials                0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.011 0.012   \n",
       "target_polynomials_VS_metamodel_poly               200146403328.000 0.557   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...              NaN 0.024   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            0.003 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.012 0.012   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    200146403328.000 0.557   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...              NaN 0.023   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.011 0.012   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         200146403328.000 0.557   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...              NaN 0.024   \n",
       "inet_polynomials_VS_metamodel_poly                 200146403328.000 0.550   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly              NaN 0.023   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly                NaN 0.551   \n",
       "\n",
       "                                                    L-81  L-82  L-83  L-84  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.001 0.001 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.001 0.001 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.001 0.001 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.020 0.025 0.006 0.009   \n",
       "lambda_preds_VS_metamodel_poly                     0.405 0.045 0.055 0.137   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.015   NaN   NaN 0.007   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.001 0.000 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.019 0.026 0.006 0.009   \n",
       "target_polynomials_VS_metamodel_poly               0.405 0.046 0.054 0.137   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.015   NaN   NaN 0.007   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.001 0.000 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.020 0.025 0.006 0.009   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.405 0.045 0.054 0.137   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.015   NaN   NaN 0.007   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.019 0.026 0.006 0.009   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.405 0.046 0.054 0.137   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.015   NaN   NaN 0.007   \n",
       "inet_polynomials_VS_metamodel_poly                 0.410 0.046 0.052 0.129   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.021   NaN   NaN 0.012   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.396   NaN   NaN 0.138   \n",
       "\n",
       "                                                    L-85  L-86  L-87  L-88  \\\n",
       "lambda_preds_VS_target_polynomials                 0.054 0.001 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.001 0.000 0.001   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.054 0.001 0.000 0.001   \n",
       "lambda_preds_VS_inet_polynomials                   0.014 0.010 0.003 0.128   \n",
       "lambda_preds_VS_metamodel_poly                     0.634 0.016 0.179 1.073   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.034   NaN 0.005 0.046   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.054 0.001 0.000 0.001   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.042 0.010 0.003 0.128   \n",
       "target_polynomials_VS_metamodel_poly               0.667 0.016 0.179 1.073   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.087   NaN 0.004 0.047   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.054 0.001 0.000 0.001   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.014 0.010 0.003 0.128   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.634 0.015 0.179 1.073   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.034   NaN 0.005 0.046   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.042 0.010 0.003 0.128   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.667 0.016 0.179 1.073   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.087   NaN 0.004 0.047   \n",
       "inet_polynomials_VS_metamodel_poly                 0.632 0.018 0.176 1.072   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.048   NaN 0.002 0.107   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.622   NaN 0.176 1.068   \n",
       "\n",
       "                                                    L-89  L-90  L-91  L-92  \\\n",
       "lambda_preds_VS_target_polynomials                 0.001 0.000 0.002 0.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.000 0.002 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.000 0.002 0.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.088 0.021 0.018 0.004   \n",
       "lambda_preds_VS_metamodel_poly                     0.134 0.256 0.542 0.088   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.002   NaN   NaN 0.002   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.001 0.001 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.089 0.021 0.018 0.003   \n",
       "target_polynomials_VS_metamodel_poly               0.135 0.256 0.542 0.088   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.002   NaN   NaN 0.002   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.001 0.001 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.088 0.021 0.017 0.004   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.134 0.256 0.542 0.088   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.002   NaN   NaN 0.002   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.089 0.021 0.018 0.003   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.135 0.256 0.542 0.088   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.002   NaN   NaN 0.002   \n",
       "inet_polynomials_VS_metamodel_poly                 0.105 0.257 0.531 0.085   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.088   NaN   NaN 0.004   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.134   NaN   NaN 0.089   \n",
       "\n",
       "                                                    L-93  L-94  L-95  L-96  \\\n",
       "lambda_preds_VS_target_polynomials                 0.000 0.001 0.001 0.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.000 0.001 0.001 0.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.000 0.001 0.001 0.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.026 0.034 0.020 0.007   \n",
       "lambda_preds_VS_metamodel_poly                     0.389 0.163 0.013 0.082   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   NaN 0.007 0.008   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.000 0.000 0.001 0.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000 0.000   \n",
       "target_polynomials_VS_inet_polynomials             0.026 0.034 0.020 0.008   \n",
       "target_polynomials_VS_metamodel_poly               0.388 0.163 0.012 0.083   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   NaN 0.007 0.008   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.000 0.000 0.001 0.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.026 0.034 0.020 0.007   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.389 0.163 0.012 0.082   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN 0.007 0.008   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.026 0.034 0.020 0.008   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.388 0.163 0.012 0.083   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   NaN 0.007 0.008   \n",
       "inet_polynomials_VS_metamodel_poly                 0.385 0.172 0.015 0.081   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   NaN 0.018 0.013   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN   NaN 0.011 0.080   \n",
       "\n",
       "                                                    L-97  L-98  L-99  \n",
       "lambda_preds_VS_target_polynomials                 0.001 0.000 0.001  \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      0.001 0.000 0.001  \n",
       "lambda_preds_VS_lstsq_target_polynomials           0.001 0.000 0.001  \n",
       "lambda_preds_VS_inet_polynomials                   0.019 0.015 0.025  \n",
       "lambda_preds_VS_metamodel_poly                     0.095 0.205 0.108  \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   NaN   NaN  \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 0.001 0.000 0.001  \n",
       "target_polynomials_VS_lstsq_target_polynomials     0.000 0.000 0.000  \n",
       "target_polynomials_VS_inet_polynomials             0.018 0.015 0.025  \n",
       "target_polynomials_VS_metamodel_poly               0.094 0.205 0.108  \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   NaN   NaN  \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 0.001 0.000 0.001  \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.019 0.015 0.025  \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.095 0.205 0.108  \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   NaN   NaN  \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.018 0.015 0.025  \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.094 0.205 0.108  \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   NaN   NaN  \n",
       "inet_polynomials_VS_metamodel_poly                 0.102 0.213 0.116  \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   NaN   NaN  \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN   NaN   NaN  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T08:12:09.557917Z",
     "iopub.status.busy": "2021-11-09T08:12:09.557550Z",
     "iopub.status.idle": "2021-11-09T08:12:09.665144Z",
     "shell.execute_reply": "2021-11-09T08:12:09.664646Z",
     "shell.execute_reply.started": "2021-11-09T08:12:09.557880Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L-0</th>\n",
       "      <th>L-1</th>\n",
       "      <th>L-2</th>\n",
       "      <th>L-3</th>\n",
       "      <th>L-4</th>\n",
       "      <th>L-5</th>\n",
       "      <th>L-6</th>\n",
       "      <th>L-7</th>\n",
       "      <th>L-8</th>\n",
       "      <th>L-9</th>\n",
       "      <th>L-10</th>\n",
       "      <th>L-11</th>\n",
       "      <th>L-12</th>\n",
       "      <th>L-13</th>\n",
       "      <th>L-14</th>\n",
       "      <th>L-15</th>\n",
       "      <th>L-16</th>\n",
       "      <th>L-17</th>\n",
       "      <th>L-18</th>\n",
       "      <th>L-19</th>\n",
       "      <th>L-20</th>\n",
       "      <th>L-21</th>\n",
       "      <th>L-22</th>\n",
       "      <th>L-23</th>\n",
       "      <th>L-24</th>\n",
       "      <th>L-25</th>\n",
       "      <th>L-26</th>\n",
       "      <th>L-27</th>\n",
       "      <th>L-28</th>\n",
       "      <th>L-29</th>\n",
       "      <th>L-30</th>\n",
       "      <th>L-31</th>\n",
       "      <th>L-32</th>\n",
       "      <th>L-33</th>\n",
       "      <th>L-34</th>\n",
       "      <th>L-35</th>\n",
       "      <th>L-36</th>\n",
       "      <th>L-37</th>\n",
       "      <th>L-38</th>\n",
       "      <th>L-39</th>\n",
       "      <th>L-40</th>\n",
       "      <th>L-41</th>\n",
       "      <th>L-42</th>\n",
       "      <th>L-43</th>\n",
       "      <th>L-44</th>\n",
       "      <th>L-45</th>\n",
       "      <th>L-46</th>\n",
       "      <th>L-47</th>\n",
       "      <th>L-48</th>\n",
       "      <th>L-49</th>\n",
       "      <th>L-50</th>\n",
       "      <th>L-51</th>\n",
       "      <th>L-52</th>\n",
       "      <th>L-53</th>\n",
       "      <th>L-54</th>\n",
       "      <th>L-55</th>\n",
       "      <th>L-56</th>\n",
       "      <th>L-57</th>\n",
       "      <th>L-58</th>\n",
       "      <th>L-59</th>\n",
       "      <th>L-60</th>\n",
       "      <th>L-61</th>\n",
       "      <th>L-62</th>\n",
       "      <th>L-63</th>\n",
       "      <th>L-64</th>\n",
       "      <th>L-65</th>\n",
       "      <th>L-66</th>\n",
       "      <th>L-67</th>\n",
       "      <th>L-68</th>\n",
       "      <th>L-69</th>\n",
       "      <th>L-70</th>\n",
       "      <th>L-71</th>\n",
       "      <th>L-72</th>\n",
       "      <th>L-73</th>\n",
       "      <th>L-74</th>\n",
       "      <th>L-75</th>\n",
       "      <th>L-76</th>\n",
       "      <th>L-77</th>\n",
       "      <th>L-78</th>\n",
       "      <th>L-79</th>\n",
       "      <th>L-80</th>\n",
       "      <th>L-81</th>\n",
       "      <th>L-82</th>\n",
       "      <th>L-83</th>\n",
       "      <th>L-84</th>\n",
       "      <th>L-85</th>\n",
       "      <th>L-86</th>\n",
       "      <th>L-87</th>\n",
       "      <th>L-88</th>\n",
       "      <th>L-89</th>\n",
       "      <th>L-90</th>\n",
       "      <th>L-91</th>\n",
       "      <th>L-92</th>\n",
       "      <th>L-93</th>\n",
       "      <th>L-94</th>\n",
       "      <th>L-95</th>\n",
       "      <th>L-96</th>\n",
       "      <th>L-97</th>\n",
       "      <th>L-98</th>\n",
       "      <th>L-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_target_polynomials</th>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>-2.149</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_lstsq_target_polynomials</th>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>-2.149</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_inet_polynomials</th>\n",
       "      <td>0.955</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.976</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.443</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_metamodel_poly</th>\n",
       "      <td>-3.336</td>\n",
       "      <td>-4.086</td>\n",
       "      <td>-1.018</td>\n",
       "      <td>-4.123</td>\n",
       "      <td>-2.359</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.604</td>\n",
       "      <td>-5.995</td>\n",
       "      <td>0.639</td>\n",
       "      <td>-2.223</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-4.445</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-1.987</td>\n",
       "      <td>-46.771</td>\n",
       "      <td>-2.330</td>\n",
       "      <td>0.864</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.967</td>\n",
       "      <td>-1756.647</td>\n",
       "      <td>-8.454</td>\n",
       "      <td>-0.777</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.902</td>\n",
       "      <td>-3417194.079</td>\n",
       "      <td>0.890</td>\n",
       "      <td>-1.482</td>\n",
       "      <td>0.947</td>\n",
       "      <td>-8.682</td>\n",
       "      <td>-6.111</td>\n",
       "      <td>-63.719</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-4.353</td>\n",
       "      <td>-0.752</td>\n",
       "      <td>-136.702</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-3.752</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-10.457</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>-4.360</td>\n",
       "      <td>-10.561</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-7.350</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-14.648</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.674</td>\n",
       "      <td>-4.759</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-1.975</td>\n",
       "      <td>-91.265</td>\n",
       "      <td>0.936</td>\n",
       "      <td>-8.878</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.981</td>\n",
       "      <td>-3187906223119148521619456.000</td>\n",
       "      <td>-73.492</td>\n",
       "      <td>-10.200</td>\n",
       "      <td>0.827</td>\n",
       "      <td>-2.583</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>-101.721</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.735</td>\n",
       "      <td>-51.471</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-2.723</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.937</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-2.253</td>\n",
       "      <td>-0.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_preds_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.434</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.848</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_lambda_pred_polynomials</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.571</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.473</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.956</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.908</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.819</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.858</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.417</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_metamodel_poly</th>\n",
       "      <td>-3.301</td>\n",
       "      <td>-4.037</td>\n",
       "      <td>-1.018</td>\n",
       "      <td>-2.102</td>\n",
       "      <td>-2.342</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.598</td>\n",
       "      <td>-6.160</td>\n",
       "      <td>0.643</td>\n",
       "      <td>-2.220</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-4.451</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-1.987</td>\n",
       "      <td>-47.619</td>\n",
       "      <td>-1.882</td>\n",
       "      <td>0.858</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-1674.427</td>\n",
       "      <td>-8.326</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-3413513.744</td>\n",
       "      <td>0.887</td>\n",
       "      <td>-1.487</td>\n",
       "      <td>0.947</td>\n",
       "      <td>-8.670</td>\n",
       "      <td>-6.955</td>\n",
       "      <td>-56.674</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-4.333</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-38.105</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-3.764</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-10.468</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>-4.348</td>\n",
       "      <td>-10.523</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-1.988</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-7.322</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-14.643</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>-0.672</td>\n",
       "      <td>-4.758</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>-90.943</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-8.859</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.981</td>\n",
       "      <td>-3211641042412869118853120.000</td>\n",
       "      <td>-70.926</td>\n",
       "      <td>-10.238</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-2.568</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>-29.220</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.733</td>\n",
       "      <td>-51.175</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-2.726</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.931</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-2.245</td>\n",
       "      <td>-0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.111</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.841</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_lstsq_target_polynomials</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-2.149</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.955</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.909</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.976</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.441</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_metamodel_poly</th>\n",
       "      <td>-3.328</td>\n",
       "      <td>-4.085</td>\n",
       "      <td>-1.018</td>\n",
       "      <td>-4.124</td>\n",
       "      <td>-2.353</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.604</td>\n",
       "      <td>-6.009</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-2.223</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-4.444</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-1.986</td>\n",
       "      <td>-46.694</td>\n",
       "      <td>-2.330</td>\n",
       "      <td>0.863</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.967</td>\n",
       "      <td>-1756.529</td>\n",
       "      <td>-8.415</td>\n",
       "      <td>-0.777</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.902</td>\n",
       "      <td>-3415996.382</td>\n",
       "      <td>0.890</td>\n",
       "      <td>-1.482</td>\n",
       "      <td>0.947</td>\n",
       "      <td>-8.684</td>\n",
       "      <td>-6.111</td>\n",
       "      <td>-64.666</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-4.353</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>-136.716</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-3.745</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>-10.446</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-4.352</td>\n",
       "      <td>-10.555</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-7.329</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-14.646</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-4.764</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>-90.971</td>\n",
       "      <td>0.936</td>\n",
       "      <td>-8.843</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.981</td>\n",
       "      <td>-3185192493007859221528576.000</td>\n",
       "      <td>-73.322</td>\n",
       "      <td>-10.184</td>\n",
       "      <td>0.824</td>\n",
       "      <td>-2.580</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>-101.720</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.735</td>\n",
       "      <td>-51.443</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-2.722</td>\n",
       "      <td>-0.693</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.937</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-2.249</td>\n",
       "      <td>-0.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_lambda_pred_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.434</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.847</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_inet_polynomials</th>\n",
       "      <td>0.956</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.908</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.819</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.858</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.417</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_metamodel_poly</th>\n",
       "      <td>-3.301</td>\n",
       "      <td>-4.037</td>\n",
       "      <td>-1.018</td>\n",
       "      <td>-2.102</td>\n",
       "      <td>-2.342</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.598</td>\n",
       "      <td>-6.160</td>\n",
       "      <td>0.643</td>\n",
       "      <td>-2.220</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-4.451</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-1.987</td>\n",
       "      <td>-47.619</td>\n",
       "      <td>-1.882</td>\n",
       "      <td>0.858</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-1674.427</td>\n",
       "      <td>-8.326</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-3413513.744</td>\n",
       "      <td>0.887</td>\n",
       "      <td>-1.487</td>\n",
       "      <td>0.947</td>\n",
       "      <td>-8.670</td>\n",
       "      <td>-6.955</td>\n",
       "      <td>-56.674</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-4.333</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-38.105</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-3.764</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-10.468</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>-4.348</td>\n",
       "      <td>-10.523</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-1.988</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-7.322</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-14.643</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>-0.672</td>\n",
       "      <td>-4.758</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>-90.943</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-8.859</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.981</td>\n",
       "      <td>-3211641042412869118853120.000</td>\n",
       "      <td>-70.926</td>\n",
       "      <td>-10.238</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-2.568</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>-29.220</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.733</td>\n",
       "      <td>-51.175</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-2.726</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.931</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-2.245</td>\n",
       "      <td>-0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstsq_target_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.111</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.841</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_metamodel_poly</th>\n",
       "      <td>-3.411</td>\n",
       "      <td>-6.071</td>\n",
       "      <td>-1.010</td>\n",
       "      <td>-6.364</td>\n",
       "      <td>-4.394</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.488</td>\n",
       "      <td>-4.044</td>\n",
       "      <td>0.658</td>\n",
       "      <td>-2.042</td>\n",
       "      <td>0.707</td>\n",
       "      <td>-1.025</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-5.759</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-53.963</td>\n",
       "      <td>-3.211</td>\n",
       "      <td>0.918</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.488</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-7717.357</td>\n",
       "      <td>-13.209</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.907</td>\n",
       "      <td>-3508060.166</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-1.448</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-8.827</td>\n",
       "      <td>-8.620</td>\n",
       "      <td>-90.919</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-5.050</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>-154.953</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-4.222</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>-9.204</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>-5.344</td>\n",
       "      <td>-10.945</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-2.079</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-7.237</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-22.769</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-1.134</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-6.008</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-2.214</td>\n",
       "      <td>-293.403</td>\n",
       "      <td>0.942</td>\n",
       "      <td>-9.270</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>0.968</td>\n",
       "      <td>-3229943413069873921654784.000</td>\n",
       "      <td>-69.179</td>\n",
       "      <td>-10.287</td>\n",
       "      <td>0.887</td>\n",
       "      <td>-2.586</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-111.884</td>\n",
       "      <td>-8.685</td>\n",
       "      <td>-0.703</td>\n",
       "      <td>-12077.521</td>\n",
       "      <td>0.466</td>\n",
       "      <td>-2.913</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.864</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>-2.128</td>\n",
       "      <td>-2.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inet_polynomials_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>0.587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.803</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.996</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-196.241</td>\n",
       "      <td>0.579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metamodel_poly_VS_metamodel_functions_no_GD_poly</th>\n",
       "      <td>-776843133.193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-135075988.571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.297</td>\n",
       "      <td>-1337499984.754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1918609312.947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-767756943.122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-18.702</td>\n",
       "      <td>0.964</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3213502.951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-470076490.151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1875717618.784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2010029295.871</td>\n",
       "      <td>-101.699</td>\n",
       "      <td>-686466070.362</td>\n",
       "      <td>-368.629</td>\n",
       "      <td>-14.655</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-663796089.344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1533464659.045</td>\n",
       "      <td>-9073500975.562</td>\n",
       "      <td>-556867958.363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2621034.305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-53112455.143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4320053404.762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7505285033.180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3843261107.398</td>\n",
       "      <td>-2140528716.025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-713075020.277</td>\n",
       "      <td>-4943249510.719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-941742167.500</td>\n",
       "      <td>-14505069579.078</td>\n",
       "      <td>-6.238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1450498.312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.964</td>\n",
       "      <td>-109001.371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              L-0    L-1  \\\n",
       "lambda_preds_VS_target_polynomials                          0.999  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials               1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    0.999  1.000   \n",
       "lambda_preds_VS_inet_polynomials                            0.955  0.838   \n",
       "lambda_preds_VS_metamodel_poly                             -3.336 -4.086   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly              0.621    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...          1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials              1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                      0.956  0.840   \n",
       "target_polynomials_VS_metamodel_poly                       -3.301 -4.037   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...          0.604    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           0.955  0.838   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -3.328 -4.085   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          0.619    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                0.956  0.840   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -3.301 -4.037   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...          0.604    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                         -3.411 -6.071   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          0.587    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -776843133.193    NaN   \n",
       "\n",
       "                                                      L-2    L-3    L-4   L-5  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000 -0.151  1.000 1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000  1.000  1.000 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000 -0.151  1.000 1.000   \n",
       "lambda_preds_VS_inet_polynomials                    0.999  0.932  0.910 0.994   \n",
       "lambda_preds_VS_metamodel_poly                     -1.018 -4.123 -2.359 0.126   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN  0.898    NaN   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000  0.571  1.000 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000  1.000  1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.999  0.691  0.907 0.994   \n",
       "target_polynomials_VS_metamodel_poly               -1.018 -2.102 -2.342 0.126   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN  0.358    NaN   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000 -0.150  1.000 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.999  0.932  0.909 0.994   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -1.018 -4.124 -2.353 0.126   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN  0.898    NaN   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.999  0.691  0.907 0.994   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -1.018 -2.102 -2.342 0.126   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN  0.358    NaN   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 -1.010 -6.364 -4.394 0.130   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN  0.739    NaN   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN  0.358    NaN   NaN   \n",
       "\n",
       "                                                     L-6    L-7   L-8    L-9  \\\n",
       "lambda_preds_VS_target_polynomials                 1.000  0.997 1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      1.000  1.000 1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           1.000  0.997 1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.950  0.971 0.998  0.979   \n",
       "lambda_preds_VS_metamodel_poly                     0.604 -5.995 0.639 -2.223   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN  0.759   NaN    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 1.000  0.998 1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     1.000  1.000 1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials             0.949  0.969 0.998  0.979   \n",
       "target_polynomials_VS_metamodel_poly               0.598 -6.160 0.643 -2.220   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN  0.738   NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 1.000  0.998 1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.950  0.971 0.998  0.979   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.604 -6.009 0.640 -2.223   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN  0.758   NaN    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.949  0.969 0.998  0.979   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.598 -6.160 0.643 -2.220   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN  0.738   NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.488 -4.044 0.658 -2.042   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN  0.765   NaN    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN -0.251   NaN    NaN   \n",
       "\n",
       "                                                    L-10           L-11  \\\n",
       "lambda_preds_VS_target_polynomials                 1.000          1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      1.000          1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           1.000          1.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.996          0.910   \n",
       "lambda_preds_VS_metamodel_poly                     0.688         -0.475   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     0.996          0.998   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 1.000          1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     1.000          1.000   \n",
       "target_polynomials_VS_inet_polynomials             0.996          0.908   \n",
       "target_polynomials_VS_metamodel_poly               0.688         -0.465   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 0.997          0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 1.000          1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.996          0.909   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.688         -0.473   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 0.997          0.998   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.996          0.908   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.688         -0.465   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 0.997          0.998   \n",
       "inet_polynomials_VS_metamodel_poly                 0.707         -1.025   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.989          0.893   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.365 -135075988.571   \n",
       "\n",
       "                                                     L-12   L-13  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                    1.000  1.000   \n",
       "lambda_preds_VS_metamodel_poly                     -0.867  0.271   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN  1.000   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials              1.000  1.000   \n",
       "target_polynomials_VS_metamodel_poly               -0.866  0.271   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -0.867  0.271   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN  1.000   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        1.000  1.000   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -0.866  0.271   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN  1.000   \n",
       "inet_polynomials_VS_metamodel_poly                 -0.889  0.269   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN  0.999   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN -1.297   \n",
       "\n",
       "                                                              L-14   L-15  \\\n",
       "lambda_preds_VS_target_polynomials                           1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                             0.895  0.991   \n",
       "lambda_preds_VS_metamodel_poly                              -4.445 -0.180   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly               0.896    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.896  0.991   \n",
       "target_polynomials_VS_metamodel_poly                        -4.451 -0.180   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...           0.896    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.895  0.991   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly             -4.444 -0.180   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           0.896    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.896  0.991   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                  -4.451 -0.180   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...           0.896    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                          -5.759 -0.221   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly           0.824    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -1337499984.754    NaN   \n",
       "\n",
       "                                                     L-16            L-17  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000           0.997   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000           1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000           0.997   \n",
       "lambda_preds_VS_inet_polynomials                    0.979           0.848   \n",
       "lambda_preds_VS_metamodel_poly                     -1.987         -46.771   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN           0.200   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000           0.997   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000           1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.979           0.819   \n",
       "target_polynomials_VS_metamodel_poly               -1.987         -47.619   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN           0.149   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000           0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.979           0.848   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -1.986         -46.694   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN           0.201   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.979           0.819   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -1.987         -47.619   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN           0.149   \n",
       "inet_polynomials_VS_metamodel_poly                 -1.798         -53.963   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN           0.586   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN -1918609312.947   \n",
       "\n",
       "                                                     L-18  L-19  \\\n",
       "lambda_preds_VS_target_polynomials                  0.062 1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            0.062 1.000   \n",
       "lambda_preds_VS_inet_polynomials                    0.653 0.976   \n",
       "lambda_preds_VS_metamodel_poly                     -2.330 0.864   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  0.473 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials             -0.149 0.973   \n",
       "target_polynomials_VS_metamodel_poly               -1.882 0.858   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  0.062 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.653 0.975   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -2.330 0.863   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       -0.149 0.973   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -1.882 0.858   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 -3.211 0.918   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN   NaN   \n",
       "\n",
       "                                                             L-20   L-21  \\\n",
       "lambda_preds_VS_target_polynomials                          1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials               1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                            0.998  0.996   \n",
       "lambda_preds_VS_metamodel_poly                             -0.604 -0.107   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly              1.000    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...          1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials              1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                      0.998  0.996   \n",
       "target_polynomials_VS_metamodel_poly                       -0.604 -0.107   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...          1.000    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           0.998  0.996   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -0.604 -0.107   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          1.000    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                0.998  0.996   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -0.604 -0.107   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...          1.000    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                         -0.542 -0.115   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          0.998    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -767756943.122    NaN   \n",
       "\n",
       "                                                    L-22    L-23  L-24  \\\n",
       "lambda_preds_VS_target_polynomials                 1.000   1.000 1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      1.000   1.000 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           1.000   1.000 1.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.936   0.976 0.976   \n",
       "lambda_preds_VS_metamodel_poly                     0.410  -0.215 0.967   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN   0.822 0.999   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 1.000   1.000 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     1.000   1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials             0.936   0.976 0.976   \n",
       "target_polynomials_VS_metamodel_poly               0.410  -0.216 0.966   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN   0.820 0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 1.000   1.000 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.936   0.976 0.976   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.410  -0.215 0.967   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN   0.822 0.999   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.936   0.976 0.976   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.410  -0.216 0.966   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN   0.820 0.999   \n",
       "inet_polynomials_VS_metamodel_poly                 0.488  -0.288 0.982   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN   0.793 0.971   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN -18.702 0.964   \n",
       "\n",
       "                                                        L-25    L-26  \\\n",
       "lambda_preds_VS_target_polynomials                     0.998   0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials          1.000   1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials               0.998   0.999   \n",
       "lambda_preds_VS_inet_polynomials                      -0.010   0.894   \n",
       "lambda_preds_VS_metamodel_poly                     -1756.647  -8.454   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly           NaN     NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...     0.999   1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials         1.000   1.000   \n",
       "target_polynomials_VS_inet_polynomials                 0.009   0.889   \n",
       "target_polynomials_VS_metamodel_poly               -1674.427  -8.326   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...       NaN     NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...     0.999   1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     -0.010   0.893   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -1756.529  -8.415   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...       NaN     NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials           0.009   0.889   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -1674.427  -8.326   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...       NaN     NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 -7717.357 -13.209   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly       NaN     NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly         NaN     NaN   \n",
       "\n",
       "                                                           L-27  L-28  L-29  \\\n",
       "lambda_preds_VS_target_polynomials                        1.000 1.000 1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials             1.000 1.000 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                  1.000 1.000 1.000   \n",
       "lambda_preds_VS_inet_polynomials                          0.984 0.690 0.998   \n",
       "lambda_preds_VS_metamodel_poly                           -0.777 0.417 0.902   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly            0.993   NaN 0.999   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...        1.000 1.000 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials            1.000 1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials                    0.985 0.692 0.998   \n",
       "target_polynomials_VS_metamodel_poly                     -0.779 0.417 0.900   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...        0.993   NaN 0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...        1.000 1.000 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials         0.984 0.690 0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly          -0.777 0.417 0.902   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...        0.993   NaN 0.999   \n",
       "lstsq_target_polynomials_VS_inet_polynomials              0.985 0.692 0.998   \n",
       "lstsq_target_polynomials_VS_metamodel_poly               -0.779 0.417 0.900   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...        0.993   NaN 0.999   \n",
       "inet_polynomials_VS_metamodel_poly                       -0.598 0.546 0.907   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly        0.989   NaN 0.999   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -3213502.951   NaN 0.862   \n",
       "\n",
       "                                                           L-30  L-31   L-32  \\\n",
       "lambda_preds_VS_target_polynomials                        1.000 1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials             1.000 1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                  1.000 1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                          0.997 0.967  0.992   \n",
       "lambda_preds_VS_metamodel_poly                     -3417194.079 0.890 -1.482   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly              NaN   NaN    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...        1.000 1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials            1.000 1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                    0.997 0.966  0.993   \n",
       "target_polynomials_VS_metamodel_poly               -3413513.744 0.887 -1.487   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...          NaN   NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...        1.000 1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials         0.997 0.967  0.992   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -3415996.382 0.890 -1.482   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          NaN   NaN    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials              0.997 0.966  0.993   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -3413513.744 0.887 -1.487   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...          NaN   NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 -3508060.166 0.950 -1.448   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          NaN   NaN    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly            NaN   NaN    NaN   \n",
       "\n",
       "                                                    L-33   L-34  \\\n",
       "lambda_preds_VS_target_polynomials                 1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.999  0.996   \n",
       "lambda_preds_VS_metamodel_poly                     0.947 -8.682   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly     1.000    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials             0.999  0.996   \n",
       "target_polynomials_VS_metamodel_poly               0.947 -8.670   \n",
       "target_polynomials_VS_metamodel_functions_no_GD... 1.000    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.999  0.996   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.947 -8.684   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func... 1.000    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.999  0.996   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.947 -8.670   \n",
       "lstsq_target_polynomials_VS_metamodel_functions... 1.000    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 0.940 -8.827   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly 0.999    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   0.930    NaN   \n",
       "\n",
       "                                                             L-35    L-36  \\\n",
       "lambda_preds_VS_target_polynomials                          0.920   0.982   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials               1.000   0.998   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    0.920   0.982   \n",
       "lambda_preds_VS_inet_polynomials                            0.749   0.956   \n",
       "lambda_preds_VS_metamodel_poly                             -6.111 -63.719   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly              0.549     NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...          0.914   0.984   \n",
       "target_polynomials_VS_lstsq_target_polynomials              1.000   1.000   \n",
       "target_polynomials_VS_inet_polynomials                      0.448   0.943   \n",
       "target_polynomials_VS_metamodel_poly                       -6.955 -56.674   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...          0.163     NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          0.920   0.982   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           0.749   0.960   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -6.111 -64.666   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          0.549     NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                0.448   0.943   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -6.955 -56.674   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...          0.163     NaN   \n",
       "inet_polynomials_VS_metamodel_poly                         -8.620 -90.919   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          0.899     NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -470076490.151     NaN   \n",
       "\n",
       "                                                    L-37            L-38  \\\n",
       "lambda_preds_VS_target_polynomials                 1.000           1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials      1.000           1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials           1.000           1.000   \n",
       "lambda_preds_VS_inet_polynomials                   0.772           0.940   \n",
       "lambda_preds_VS_metamodel_poly                     0.381          -4.353   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly       NaN           0.938   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom... 1.000           1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials     1.000           1.000   \n",
       "target_polynomials_VS_inet_polynomials             0.772           0.941   \n",
       "target_polynomials_VS_metamodel_poly               0.377          -4.333   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...   NaN           0.936   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p... 1.000           1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials  0.772           0.941   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    0.380          -4.353   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...   NaN           0.938   \n",
       "lstsq_target_polynomials_VS_inet_polynomials       0.772           0.941   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         0.377          -4.333   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...   NaN           0.936   \n",
       "inet_polynomials_VS_metamodel_poly                 0.257          -5.050   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly   NaN           0.860   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly     NaN -1875717618.784   \n",
       "\n",
       "                                                     L-39            L-40  \\\n",
       "lambda_preds_VS_target_polynomials                  0.996          -2.149   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       0.997           1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            0.996          -2.149   \n",
       "lambda_preds_VS_inet_polynomials                    0.871           0.915   \n",
       "lambda_preds_VS_metamodel_poly                     -0.752        -136.702   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN          -0.540   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000           0.187   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000           1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.858          -0.071   \n",
       "target_polynomials_VS_metamodel_poly               -0.729         -38.105   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN          -1.111   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000          -2.149   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.869           0.915   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -0.746        -136.716   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN          -0.540   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.858          -0.071   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -0.729         -38.105   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN          -1.111   \n",
       "inet_polynomials_VS_metamodel_poly                 -0.755        -154.953   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN          -0.089   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN -2010029295.871   \n",
       "\n",
       "                                                       L-41           L-42  \\\n",
       "lambda_preds_VS_target_polynomials                    1.000          1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         1.000          1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials              1.000          1.000   \n",
       "lambda_preds_VS_inet_polynomials                      0.983          0.996   \n",
       "lambda_preds_VS_metamodel_poly                       -0.180         -0.742   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        1.000          0.999   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    1.000          1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials        1.000          1.000   \n",
       "target_polynomials_VS_inet_polynomials                0.983          0.996   \n",
       "target_polynomials_VS_metamodel_poly                 -0.180         -0.740   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    1.000          0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    1.000          1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     0.983          0.996   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly      -0.180         -0.742   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    1.000          0.999   \n",
       "lstsq_target_polynomials_VS_inet_polynomials          0.983          0.996   \n",
       "lstsq_target_polynomials_VS_metamodel_poly           -0.180         -0.740   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    1.000          0.999   \n",
       "inet_polynomials_VS_metamodel_poly                   -0.210         -0.695   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    0.980          0.997   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -101.699 -686466070.362   \n",
       "\n",
       "                                                       L-43    L-44   L-45  \\\n",
       "lambda_preds_VS_target_polynomials                    1.000   0.999  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials         1.000   0.999  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials              1.000   0.999  1.000   \n",
       "lambda_preds_VS_inet_polynomials                      0.999   0.965  0.992   \n",
       "lambda_preds_VS_metamodel_poly                       -0.101   0.213 -3.752   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        1.000   0.999    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...    1.000   1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials        1.000   1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                0.999   0.961  0.992   \n",
       "target_polynomials_VS_metamodel_poly                 -0.100   0.209 -3.764   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    1.000   0.999    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...    1.000   1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials     0.999   0.965  0.992   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly      -0.101   0.213 -3.745   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    1.000   0.999    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials          0.999   0.961  0.992   \n",
       "lstsq_target_polynomials_VS_metamodel_poly           -0.100   0.209 -3.764   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    1.000   0.999    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                   -0.102   0.279 -4.222   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    0.999   0.959    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -368.629 -14.655    NaN   \n",
       "\n",
       "                                                     L-46   L-47   L-48  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000  1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                    0.983  0.954  0.918   \n",
       "lambda_preds_VS_metamodel_poly                      0.366 -0.509  0.073   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly      0.855    NaN    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000  1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000  1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.985  0.956  0.918   \n",
       "target_polynomials_VS_metamodel_poly                0.363 -0.520  0.072   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...  0.857    NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000  1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.984  0.954  0.918   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly     0.365 -0.510  0.074   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...  0.855    NaN    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.985  0.956  0.918   \n",
       "lstsq_target_polynomials_VS_metamodel_poly          0.363 -0.520  0.072   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...  0.857    NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                  0.321 -0.480 -0.058   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly  0.803    NaN    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -3.710    NaN    NaN   \n",
       "\n",
       "                                                             L-49    L-50  \\\n",
       "lambda_preds_VS_target_polynomials                          1.000   1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials               1.000   1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    1.000   1.000   \n",
       "lambda_preds_VS_inet_polynomials                            0.996   0.988   \n",
       "lambda_preds_VS_metamodel_poly                             -0.537 -10.457   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly              0.968     NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...          1.000   1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials              1.000   1.000   \n",
       "target_polynomials_VS_inet_polynomials                      0.996   0.989   \n",
       "target_polynomials_VS_metamodel_poly                       -0.534 -10.468   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...          0.969     NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          1.000   1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           0.996   0.989   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -0.536 -10.446   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          0.968     NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                0.996   0.989   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -0.534 -10.468   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...          0.969     NaN   \n",
       "inet_polynomials_VS_metamodel_poly                         -0.529  -9.204   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          0.974     NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -663796089.344     NaN   \n",
       "\n",
       "                                                     L-51   L-52  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                    0.999  0.988   \n",
       "lambda_preds_VS_metamodel_poly                      0.315 -0.453   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly      0.999    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.999  0.987   \n",
       "target_polynomials_VS_metamodel_poly                0.316 -0.439   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...  0.999    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.999  0.989   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly     0.315 -0.451   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...  0.999    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.999  0.987   \n",
       "lstsq_target_polynomials_VS_metamodel_poly          0.316 -0.439   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...  0.999    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                  0.309 -0.507   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly  0.998    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -1.760    NaN   \n",
       "\n",
       "                                                              L-53  \\\n",
       "lambda_preds_VS_target_polynomials                           1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     1.000   \n",
       "lambda_preds_VS_inet_polynomials                             0.976   \n",
       "lambda_preds_VS_metamodel_poly                              -4.360   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly               0.927   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.975   \n",
       "target_polynomials_VS_metamodel_poly                        -4.348   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...           0.927   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.975   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly             -4.352   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           0.927   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.975   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                  -4.348   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...           0.927   \n",
       "inet_polynomials_VS_metamodel_poly                          -5.344   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly           0.894   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -1533464659.045   \n",
       "\n",
       "                                                              L-54  \\\n",
       "lambda_preds_VS_target_polynomials                           1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     1.000   \n",
       "lambda_preds_VS_inet_polynomials                             0.912   \n",
       "lambda_preds_VS_metamodel_poly                             -10.561   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly               0.903   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.913   \n",
       "target_polynomials_VS_metamodel_poly                       -10.523   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...           0.902   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.912   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -10.555   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           0.903   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.913   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -10.523   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...           0.902   \n",
       "inet_polynomials_VS_metamodel_poly                         -10.945   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly           0.620   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -9073500975.562   \n",
       "\n",
       "                                                             L-55   L-56  \\\n",
       "lambda_preds_VS_target_polynomials                          1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials               1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                            0.996  0.997   \n",
       "lambda_preds_VS_metamodel_poly                             -0.614 -0.081   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly              0.998    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...          1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials              1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                      0.996  0.996   \n",
       "target_polynomials_VS_metamodel_poly                       -0.616 -0.081   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...          0.998    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           0.996  0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -0.614 -0.081   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          0.998    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                0.996  0.996   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -0.616 -0.081   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...          0.998    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                         -0.699 -0.089   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          0.996    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -556867958.363    NaN   \n",
       "\n",
       "                                                     L-57  L-58   L-59   L-60  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000 1.000  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000 1.000  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000 1.000  1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                    0.977 0.997  0.988  0.993   \n",
       "lambda_preds_VS_metamodel_poly                     -1.993 0.440 -7.350 -0.105   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN   NaN    NaN    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000 1.000  1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000 1.000  1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.976 0.997  0.989  0.993   \n",
       "target_polynomials_VS_metamodel_poly               -1.988 0.440 -7.322 -0.105   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN   NaN    NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000 1.000  1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.977 0.997  0.988  0.993   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -1.993 0.440 -7.329 -0.105   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN   NaN    NaN    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.976 0.997  0.989  0.993   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -1.988 0.440 -7.322 -0.105   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN   NaN    NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 -2.079 0.463 -7.237 -0.120   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN   NaN    NaN    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN   NaN    NaN    NaN   \n",
       "\n",
       "                                                      L-61         L-62  \\\n",
       "lambda_preds_VS_target_polynomials                   1.000        1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials        1.000        1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials             1.000        1.000   \n",
       "lambda_preds_VS_inet_polynomials                     0.781        0.995   \n",
       "lambda_preds_VS_metamodel_poly                     -14.648       -0.567   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly         NaN        0.997   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...   1.000        1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials       1.000        1.000   \n",
       "target_polynomials_VS_inet_polynomials               0.783        0.995   \n",
       "target_polynomials_VS_metamodel_poly               -14.643       -0.567   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...     NaN        0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...   1.000        1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials    0.781        0.995   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -14.646       -0.567   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...     NaN        0.997   \n",
       "lstsq_target_polynomials_VS_inet_polynomials         0.783        0.995   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -14.643       -0.567   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...     NaN        0.997   \n",
       "inet_polynomials_VS_metamodel_poly                 -22.769       -0.639   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly     NaN        0.998   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly       NaN -2621034.305   \n",
       "\n",
       "                                                     L-63   L-64   L-65  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000  1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                    0.993  0.929  0.971   \n",
       "lambda_preds_VS_metamodel_poly                     -0.151 -0.974 -0.528   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN    NaN    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000  1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000  1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.993  0.930  0.972   \n",
       "target_polynomials_VS_metamodel_poly               -0.151 -0.977 -0.525   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN    NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000  1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.993  0.930  0.971   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -0.151 -0.970 -0.528   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN    NaN    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.993  0.930  0.972   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -0.151 -0.977 -0.525   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN    NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 -0.175 -0.929 -0.530   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN    NaN    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN    NaN    NaN   \n",
       "\n",
       "                                                     L-66   L-67  \\\n",
       "lambda_preds_VS_target_polynomials                  0.999  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            0.999  1.000   \n",
       "lambda_preds_VS_inet_polynomials                    0.868  0.974   \n",
       "lambda_preds_VS_metamodel_poly                     -0.512 -0.674   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.864  0.973   \n",
       "target_polynomials_VS_metamodel_poly               -0.518 -0.672   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.867  0.974   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -0.514 -0.673   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.864  0.973   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -0.518 -0.672   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 -1.134 -0.987   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN    NaN   \n",
       "\n",
       "                                                            L-68   L-69  \\\n",
       "lambda_preds_VS_target_polynomials                         1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials              1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                   1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                           0.881  0.996   \n",
       "lambda_preds_VS_metamodel_poly                            -4.759 -0.097   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly             0.999    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...         1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials             1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                     0.884  0.996   \n",
       "target_polynomials_VS_metamodel_poly                      -4.758 -0.097   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...         0.998    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...         1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials          0.881  0.996   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly           -4.764 -0.097   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...         0.999    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials               0.884  0.996   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                -4.758 -0.097   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...         0.998    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                        -6.008 -0.103   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly         0.810    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -53112455.143    NaN   \n",
       "\n",
       "                                                     L-70   L-71  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                    1.000  0.972   \n",
       "lambda_preds_VS_metamodel_poly                     -0.062 -1.975   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials              1.000  0.972   \n",
       "target_polynomials_VS_metamodel_poly               -0.062 -1.974   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   1.000  0.972   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -0.062 -1.974   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        1.000  0.972   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -0.062 -1.974   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 -0.062 -2.214   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN    NaN   \n",
       "\n",
       "                                                              L-72  L-73  \\\n",
       "lambda_preds_VS_target_polynomials                           1.000 1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     1.000 1.000   \n",
       "lambda_preds_VS_inet_polynomials                             0.283 0.998   \n",
       "lambda_preds_VS_metamodel_poly                             -91.265 0.936   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly               0.815   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           1.000 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.283 0.998   \n",
       "target_polynomials_VS_metamodel_poly                       -90.943 0.935   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...           0.815   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           1.000 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.283 0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -90.971 0.936   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           0.816   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.283 0.998   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -90.943 0.935   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...           0.815   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                        -293.403 0.942   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          -3.082   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -4320053404.762   NaN   \n",
       "\n",
       "                                                              L-74   L-75  \\\n",
       "lambda_preds_VS_target_polynomials                           0.999  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                0.999  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     0.999  1.000   \n",
       "lambda_preds_VS_inet_polynomials                             0.995  0.997   \n",
       "lambda_preds_VS_metamodel_poly                              -8.878 -0.045   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly               0.729    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.995  0.997   \n",
       "target_polynomials_VS_metamodel_poly                        -8.859 -0.045   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...           0.728    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.995  0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly             -8.843 -0.045   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           0.731    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.995  0.997   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                  -8.859 -0.045   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...           0.728    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                          -9.270 -0.052   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly           0.719    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -7505285033.180    NaN   \n",
       "\n",
       "                                                     L-76   L-77  L-78  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000  1.000 1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000  1.000 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000  1.000 1.000   \n",
       "lambda_preds_VS_inet_polynomials                    0.997  0.958 0.992   \n",
       "lambda_preds_VS_metamodel_poly                     -0.100 -0.316 0.981   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN    NaN   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000  1.000 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000  1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.997  0.958 0.992   \n",
       "target_polynomials_VS_metamodel_poly               -0.100 -0.315 0.981   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN    NaN   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000  1.000 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.997  0.958 0.992   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -0.100 -0.315 0.981   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN    NaN   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.997  0.958 0.992   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -0.100 -0.315 0.981   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN    NaN   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 -0.102 -0.594 0.968   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN    NaN   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN    NaN   NaN   \n",
       "\n",
       "                                                                             L-79  \\\n",
       "lambda_preds_VS_target_polynomials                                          0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                               0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials                                    0.999   \n",
       "lambda_preds_VS_inet_polynomials                                            0.995   \n",
       "lambda_preds_VS_metamodel_poly                     -3187906223119148521619456.000   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly                                NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...                          1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                              1.000   \n",
       "target_polynomials_VS_inet_polynomials                                      0.997   \n",
       "target_polynomials_VS_metamodel_poly               -3211641042412869118853120.000   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...                            NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...                          1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials                           0.996   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -3185192493007859221528576.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...                            NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                                0.997   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -3211641042412869118853120.000   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...                            NaN   \n",
       "inet_polynomials_VS_metamodel_poly                 -3229943413069873921654784.000   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly                            NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly                              NaN   \n",
       "\n",
       "                                                              L-80  \\\n",
       "lambda_preds_VS_target_polynomials                           0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     0.999   \n",
       "lambda_preds_VS_inet_polynomials                             0.947   \n",
       "lambda_preds_VS_metamodel_poly                             -73.492   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly               0.757   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.946   \n",
       "target_polynomials_VS_metamodel_poly                       -70.926   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...           0.757   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.947   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -73.322   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           0.757   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.946   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -70.926   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...           0.757   \n",
       "inet_polynomials_VS_metamodel_poly                         -69.179   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly           0.755   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -3843261107.398   \n",
       "\n",
       "                                                              L-81  L-82  \\\n",
       "lambda_preds_VS_target_polynomials                           1.000 0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000 1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                     1.000 0.999   \n",
       "lambda_preds_VS_inet_polynomials                             0.966 0.951   \n",
       "lambda_preds_VS_metamodel_poly                             -10.200 0.827   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly               0.954   NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           1.000 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.968 0.943   \n",
       "target_polynomials_VS_metamodel_poly                       -10.238 0.810   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...           0.954   NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...           1.000 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.967 0.949   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -10.184 0.824   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           0.954   NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.968 0.943   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -10.238 0.810   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...           0.954   NaN   \n",
       "inet_polynomials_VS_metamodel_poly                         -10.287 0.887   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly           0.952   NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -2140528716.025   NaN   \n",
       "\n",
       "                                                     L-83           L-84  \\\n",
       "lambda_preds_VS_target_polynomials                  0.999          1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       0.999          1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            0.999          1.000   \n",
       "lambda_preds_VS_inet_polynomials                    0.959          0.997   \n",
       "lambda_preds_VS_metamodel_poly                     -2.583         -0.388   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN          0.999   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000          1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000          1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.961          0.997   \n",
       "target_polynomials_VS_metamodel_poly               -2.568         -0.388   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN          0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000          1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.960          0.997   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -2.580         -0.388   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN          0.999   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.961          0.997   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -2.568         -0.388   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN          0.999   \n",
       "inet_polynomials_VS_metamodel_poly                 -2.586         -0.356   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN          0.996   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN -713075020.277   \n",
       "\n",
       "                                                              L-85   L-86  \\\n",
       "lambda_preds_VS_target_polynomials                          -0.901  0.983   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                1.000  0.995   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    -0.901  0.983   \n",
       "lambda_preds_VS_inet_polynomials                             0.936  0.443   \n",
       "lambda_preds_VS_metamodel_poly                            -101.721 -0.108   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly               0.434    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...           0.506  0.993   \n",
       "target_polynomials_VS_lstsq_target_polynomials               1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                       0.585  0.417   \n",
       "target_polynomials_VS_metamodel_poly                       -29.220 -0.005   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...           0.092    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          -0.901  0.992   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials            0.936  0.441   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly           -101.720 -0.081   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...           0.434    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                 0.585  0.417   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -29.220 -0.005   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...           0.092    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                        -111.884 -8.685   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          -0.079    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -4943249510.719    NaN   \n",
       "\n",
       "                                                             L-87  \\\n",
       "lambda_preds_VS_target_polynomials                          1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials               1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                    1.000   \n",
       "lambda_preds_VS_inet_polynomials                            1.000   \n",
       "lambda_preds_VS_metamodel_poly                             -0.735   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly              0.999   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...          1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials              1.000   \n",
       "target_polynomials_VS_inet_polynomials                      1.000   \n",
       "target_polynomials_VS_metamodel_poly                       -0.733   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...          0.999   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...          1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials           1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly            -0.735   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...          0.999   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                1.000   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                 -0.733   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...          0.999   \n",
       "inet_polynomials_VS_metamodel_poly                         -0.703   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly          1.000   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -941742167.500   \n",
       "\n",
       "                                                               L-88   L-89  \\\n",
       "lambda_preds_VS_target_polynomials                            1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials                 1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                      1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                              0.033  0.729   \n",
       "lambda_preds_VS_metamodel_poly                              -51.471  0.228   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly                0.848  1.000   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...            1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials                1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                        0.032  0.728   \n",
       "target_polynomials_VS_metamodel_poly                        -51.175  0.228   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...            0.841  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...            1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials             0.033  0.728   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly             -51.443  0.228   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...            0.847  1.000   \n",
       "lstsq_target_polynomials_VS_inet_polynomials                  0.032  0.728   \n",
       "lstsq_target_polynomials_VS_metamodel_poly                  -51.175  0.228   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...            0.841  1.000   \n",
       "inet_polynomials_VS_metamodel_poly                       -12077.521  0.466   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly         -196.241  0.579   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -14505069579.078 -6.238   \n",
       "\n",
       "                                                     L-90   L-91         L-92  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000  1.000        1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000  1.000        1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000  1.000        1.000   \n",
       "lambda_preds_VS_inet_polynomials                    0.973  0.997        0.998   \n",
       "lambda_preds_VS_metamodel_poly                     -2.723 -0.695       -0.495   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN    NaN        1.000   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000  1.000        1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000  1.000        1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.974  0.997        0.998   \n",
       "target_polynomials_VS_metamodel_poly               -2.726 -0.694       -0.495   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN    NaN        1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000  1.000        1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.973  0.997        0.998   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -2.722 -0.693       -0.494   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN    NaN        1.000   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.974  0.997        0.998   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -2.726 -0.694       -0.495   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN    NaN        1.000   \n",
       "inet_polynomials_VS_metamodel_poly                 -2.913 -0.694       -0.484   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN    NaN        0.999   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN    NaN -1450498.312   \n",
       "\n",
       "                                                     L-93  L-94  L-95  \\\n",
       "lambda_preds_VS_target_polynomials                  1.000 1.000 0.999   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000 1.000 0.999   \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000 1.000 0.999   \n",
       "lambda_preds_VS_inet_polynomials                    0.992 0.966 0.850   \n",
       "lambda_preds_VS_metamodel_poly                     -0.084 0.353 0.937   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN   NaN 0.985   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000 1.000 1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000 1.000 1.000   \n",
       "target_polynomials_VS_inet_polynomials              0.992 0.967 0.850   \n",
       "target_polynomials_VS_metamodel_poly               -0.084 0.353 0.931   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN   NaN 0.983   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000 1.000 1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.992 0.967 0.853   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -0.084 0.353 0.937   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN   NaN 0.986   \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.992 0.967 0.850   \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -0.084 0.353 0.931   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN   NaN 0.983   \n",
       "inet_polynomials_VS_metamodel_poly                 -0.093 0.330 0.864   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN   NaN 0.824   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN   NaN 0.964   \n",
       "\n",
       "                                                          L-96   L-97   L-98  \\\n",
       "lambda_preds_VS_target_polynomials                       1.000  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials            1.000  1.000  1.000   \n",
       "lambda_preds_VS_lstsq_target_polynomials                 1.000  1.000  1.000   \n",
       "lambda_preds_VS_inet_polynomials                         0.973  0.928  0.980   \n",
       "lambda_preds_VS_metamodel_poly                          -0.525 -0.291 -2.253   \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly           0.979    NaN    NaN   \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...       1.000  1.000  1.000   \n",
       "target_polynomials_VS_lstsq_target_polynomials           1.000  1.000  1.000   \n",
       "target_polynomials_VS_inet_polynomials                   0.971  0.928  0.981   \n",
       "target_polynomials_VS_metamodel_poly                    -0.517 -0.292 -2.245   \n",
       "target_polynomials_VS_metamodel_functions_no_GD...       0.979    NaN    NaN   \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...       1.000  1.000  1.000   \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials        0.972  0.929  0.981   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly         -0.521 -0.289 -2.249   \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...       0.979    NaN    NaN   \n",
       "lstsq_target_polynomials_VS_inet_polynomials             0.971  0.928  0.981   \n",
       "lstsq_target_polynomials_VS_metamodel_poly              -0.517 -0.292 -2.245   \n",
       "lstsq_target_polynomials_VS_metamodel_functions...       0.979    NaN    NaN   \n",
       "inet_polynomials_VS_metamodel_poly                      -0.851 -0.519 -2.128   \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly       0.927    NaN    NaN   \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly   -109001.371    NaN    NaN   \n",
       "\n",
       "                                                     L-99  \n",
       "lambda_preds_VS_target_polynomials                  1.000  \n",
       "lambda_preds_VS_lstsq_lambda_pred_polynomials       1.000  \n",
       "lambda_preds_VS_lstsq_target_polynomials            1.000  \n",
       "lambda_preds_VS_inet_polynomials                    0.837  \n",
       "lambda_preds_VS_metamodel_poly                     -0.598  \n",
       "lambda_preds_VS_metamodel_functions_no_GD_poly        NaN  \n",
       "target_polynomials_VS_lstsq_lambda_pred_polynom...  1.000  \n",
       "target_polynomials_VS_lstsq_target_polynomials      1.000  \n",
       "target_polynomials_VS_inet_polynomials              0.837  \n",
       "target_polynomials_VS_metamodel_poly               -0.588  \n",
       "target_polynomials_VS_metamodel_functions_no_GD...    NaN  \n",
       "lstsq_lambda_pred_polynomials_VS_lstsq_target_p...  1.000  \n",
       "lstsq_lambda_pred_polynomials_VS_inet_polynomials   0.837  \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_poly    -0.597  \n",
       "lstsq_lambda_pred_polynomials_VS_metamodel_func...    NaN  \n",
       "lstsq_target_polynomials_VS_inet_polynomials        0.837  \n",
       "lstsq_target_polynomials_VS_metamodel_poly         -0.588  \n",
       "lstsq_target_polynomials_VS_metamodel_functions...    NaN  \n",
       "inet_polynomials_VS_metamodel_poly                 -2.108  \n",
       "inet_polynomials_VS_metamodel_functions_no_GD_poly    NaN  \n",
       "metamodel_poly_VS_metamodel_functions_no_GD_poly      NaN  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T07:39:07.087873Z",
     "iopub.status.idle": "2021-11-09T07:39:07.088408Z",
     "shell.execute_reply": "2021-11-09T07:39:07.088175Z",
     "shell.execute_reply.started": "2021-11-09T07:39:07.088147Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtimes_list[-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T07:39:07.090128Z",
     "iopub.status.idle": "2021-11-09T07:39:07.090771Z",
     "shell.execute_reply": "2021-11-09T07:39:07.090495Z",
     "shell.execute_reply.started": "2021-11-09T07:39:07.090466Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths_dict = generate_paths(path_type = 'interpretation_net_no_noise')\n",
    "\n",
    "path_mae = './data/results/' + paths_dict['path_identifier_interpretation_net_data'] + '/mae_distrib_test_data_' + 'noise' + str(noise) + '.csv'\n",
    "path_r2 = './data/results/' + paths_dict['path_identifier_interpretation_net_data'] + '/r2_distrib_test_data_' + 'noise' + str(noise) + '.csv'\n",
    "path_runtimes = './data/results/' + paths_dict['path_identifier_interpretation_net_data'] + '/runtimes_' + 'noise' + str(noise) + '.csv'\n",
    "path_fv = './data/results/' + paths_dict['path_identifier_interpretation_net_data'] + '/fvs_' + 'noise' + str(noise) + '.pkl'\n",
    "path_functions = './data/results/' + paths_dict['path_identifier_interpretation_net_data'] + '/functions_' + 'noise' + str(noise) + '.pkl'\n",
    "\n",
    "\n",
    "distrib_dict_test_list[-1]['MAE'].to_csv(path_mae)\n",
    "distrib_dict_test_list[-1]['R2'].to_csv(path_r2)\n",
    "runtimes_list[-1].to_csv(path_runtimes)\n",
    "\n",
    "with open(path_fv, 'wb') as f:\n",
    "    pickle.dump(function_values_test_list[-1], f, protocol=2)\n",
    "\n",
    "with open(path_functions, 'wb') as f:\n",
    "    pickle.dump(polynomial_dict_test_list[-1], f, protocol=2)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:06.729882Z",
     "iopub.status.busy": "2021-11-09T05:22:06.729726Z",
     "iopub.status.idle": "2021-11-09T05:22:07.076299Z",
     "shell.execute_reply": "2021-11-09T05:22:07.075827Z",
     "shell.execute_reply.started": "2021-11-09T05:22:06.729862Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:07.077253Z",
     "iopub.status.busy": "2021-11-09T05:22:07.077059Z",
     "iopub.status.idle": "2021-11-09T05:22:07.300715Z",
     "shell.execute_reply": "2021-11-09T05:22:07.300296Z",
     "shell.execute_reply.started": "2021-11-09T05:22:07.077232Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:07.301701Z",
     "iopub.status.busy": "2021-11-09T05:22:07.301460Z",
     "iopub.status.idle": "2021-11-09T05:22:07.535385Z",
     "shell.execute_reply": "2021-11-09T05:22:07.534954Z",
     "shell.execute_reply.started": "2021-11-09T05:22:07.301681Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:07.536367Z",
     "iopub.status.busy": "2021-11-09T05:22:07.536133Z",
     "iopub.status.idle": "2021-11-09T05:22:07.757113Z",
     "shell.execute_reply": "2021-11-09T05:22:07.756685Z",
     "shell.execute_reply.started": "2021-11-09T05:22:07.536346Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:07.758094Z",
     "iopub.status.busy": "2021-11-09T05:22:07.757862Z",
     "iopub.status.idle": "2021-11-09T05:22:08.146928Z",
     "shell.execute_reply": "2021-11-09T05:22:08.146363Z",
     "shell.execute_reply.started": "2021-11-09T05:22:07.758073Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    try:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:08.148250Z",
     "iopub.status.busy": "2021-11-09T05:22:08.147985Z",
     "iopub.status.idle": "2021-11-09T05:22:08.494448Z",
     "shell.execute_reply": "2021-11-09T05:22:08.493939Z",
     "shell.execute_reply.started": "2021-11-09T05:22:08.148227Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:08.495541Z",
     "iopub.status.busy": "2021-11-09T05:22:08.495382Z",
     "iopub.status.idle": "2021-11-09T05:22:08.543260Z",
     "shell.execute_reply": "2021-11-09T05:22:08.542595Z",
     "shell.execute_reply.started": "2021-11-09T05:22:08.495521Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:08.544441Z",
     "iopub.status.busy": "2021-11-09T05:22:08.544271Z",
     "iopub.status.idle": "2021-11-09T05:22:08.587704Z",
     "shell.execute_reply": "2021-11-09T05:22:08.586881Z",
     "shell.execute_reply.started": "2021-11-09T05:22:08.544422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:08.588850Z",
     "iopub.status.busy": "2021-11-09T05:22:08.588700Z",
     "iopub.status.idle": "2021-11-09T05:22:08.628799Z",
     "shell.execute_reply": "2021-11-09T05:22:08.628255Z",
     "shell.execute_reply.started": "2021-11-09T05:22:08.588830Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:08.630047Z",
     "iopub.status.busy": "2021-11-09T05:22:08.629899Z",
     "iopub.status.idle": "2021-11-09T05:22:09.525406Z",
     "shell.execute_reply": "2021-11-09T05:22:09.524770Z",
     "shell.execute_reply.started": "2021-11-09T05:22:08.630028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "target_polynomials\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.9689 a^{3} + 0.187 a^{2} + 0.6147$"
      ],
      "text/plain": [
       "-0.9689*a**3 + 0.187*a**2 + 0.6147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.0005117716\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "inet_polynomials\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.69650000333786 a^{3} - 0.0318000018596649 a^{2} + 0.619499981403351$"
      ],
      "text/plain": [
       "-0.69650000333786*a**3 - 0.0318000018596649*a**2 + 0.619499981403351"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.012501501\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "metamodel_poly\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1.10407391621583 X_{0}^{5} - 3.424499504174 X_{0}^{4} + 4.44389370336994 X_{0}^{3} - 3.02220039810572 X_{0}^{2} + 0.941664797087797 X_{0} + 0.361328449858491$"
      ],
      "text/plain": [
       "1.10407391621583*X0**5 - 3.424499504174*X0**4 + 4.44389370336994*X0**3 - 3.02220039810572*X0**2 + 0.941664797087797*X0 + 0.361328449858491"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.16864556\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "index = 5\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials']#['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions', 'metamodel_poly', 'symbolic_regression_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "        print(key)        \n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "        print('MAE: ', distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_' + key][index])\n",
    "        #print(polynomial_dict_test_list[-1][key][index])\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "        print(key)              \n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "        print('MAE: ', distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_' + key][index])\n",
    "        #print(polynomial_dict_test_list[-1][key][index])\n",
    "    elif key in sympy_representation_keys:\n",
    "        print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "        print(key)              \n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "        print('MAE: ', distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_' + key][index])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:09.526421Z",
     "iopub.status.busy": "2021-11-09T05:22:09.526272Z",
     "iopub.status.idle": "2021-11-09T05:22:09.574831Z",
     "shell.execute_reply": "2021-11-09T05:22:09.573768Z",
     "shell.execute_reply.started": "2021-11-09T05:22:09.526401Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:09.575932Z",
     "iopub.status.busy": "2021-11-09T05:22:09.575740Z",
     "iopub.status.idle": "2021-11-09T05:22:10.522307Z",
     "shell.execute_reply": "2021-11-09T05:22:10.521619Z",
     "shell.execute_reply.started": "2021-11-09T05:22:09.575911Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.15\n",
      "nan\n",
      "3.89\n"
     ]
    }
   ],
   "source": [
    "inet_terms = []\n",
    "sr_terms = []\n",
    "sm_terms = []\n",
    "\n",
    "for i in range(100):\n",
    "    inet_str = str(get_sympy_string_from_coefficients(polynomial_dict_test_list[-1]['inet_polynomials'][i]))\n",
    "    \n",
    "    inet_str_split = inet_str.split('-')\n",
    "    \n",
    "    inet_str_split_2 = []\n",
    "    for string in inet_str_split:\n",
    "        inet_str_split_2.append(string.split('+'))\n",
    "    \n",
    "    inet_str_split_2 = list(flatten(inet_str_split_2))\n",
    "    \n",
    "    inet_terms.append(len(inet_str_split_2))\n",
    "    \n",
    "    try:\n",
    "        sr_str = str(round_expr(polynomial_dict_test_list[-1]['symbolic_regression_functions'][i], 4))\n",
    "\n",
    "        sr_str_split = sr_str.split('-')\n",
    "\n",
    "        sr_str_split_2 = []\n",
    "        for string in sr_str_split:\n",
    "            sr_str_split_2.append(string.split('+'))\n",
    "\n",
    "        sr_str_split_2 = list(flatten(sr_str_split_2))\n",
    "        sr_terms.append(len(sr_str_split_2))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        sm_str = str(round_expr(polynomial_dict_test_list[-1]['metamodel_functions'][i], 4))\n",
    "    except:\n",
    "        sm_str = str(round_expr(polynomial_dict_test_list[-1]['metamodel_poly'][i], 4))\n",
    "    \n",
    "    sm_str_split = sm_str.split('-')\n",
    "    \n",
    "    sm_str_split_2 = []\n",
    "    for string in sm_str_split:\n",
    "        sm_str_split_2.append(string.split('+'))\n",
    "        \n",
    "    sm_str_split_2 = list(flatten(sm_str_split_2))\n",
    "    sm_terms.append(len(sm_str_split_2))  \n",
    "    \n",
    "inet_terms = np.array(inet_terms)    \n",
    "sr_terms = np.array(sr_terms)    \n",
    "sm_terms = np.array(sm_terms)    \n",
    "\n",
    "print(np.mean(inet_terms))\n",
    "print(np.mean(sr_terms))\n",
    "print(np.mean(sm_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:10.523318Z",
     "iopub.status.busy": "2021-11-09T05:22:10.523170Z",
     "iopub.status.idle": "2021-11-09T05:22:10.581392Z",
     "shell.execute_reply": "2021-11-09T05:22:10.580716Z",
     "shell.execute_reply.started": "2021-11-09T05:22:10.523298Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Poly:  -0.9689*a**3 + 0.187*a**2 + 0.6147\n",
      "Inet Poly:  -0.69650000333786*a**3 - 0.0318000018596649*a**2 + 0.619499981403351\n",
      "SM Poly:  1.1040739162*X0**5 - 3.4244995042*X0**4 + 4.4438937034*X0**3 - 3.0222003981*X0**2 + 0.9416647971*X0 + 0.3613284499\n"
     ]
    }
   ],
   "source": [
    "print('Target Poly: ', get_sympy_string_from_coefficients(polynomial_dict_test_list[-1]['target_polynomials'][index], force_complete_poly_representation=True, round_digits=4))\n",
    "print('Inet Poly: ', str(get_sympy_string_from_coefficients(polynomial_dict_test_list[-1]['inet_polynomials'][index], round_digits=4)))\n",
    "try:\n",
    "    print('SR Function: ', str(round_expr(polynomial_dict_test_list[-1]['symbolic_regression_functions'][index], 4)))\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    print('SM Poly: ', str(round_expr(polynomial_dict_test_list[-1]['metamodel_poly'][index], 10)))\n",
    "except:\n",
    "    print('SM Function: ', str(round_expr(polynomial_dict_test_list[-1]['metamodel_functions'][index], 10)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:10.582330Z",
     "iopub.status.busy": "2021-11-09T05:22:10.582185Z",
     "iopub.status.idle": "2021-11-09T05:22:10.629008Z",
     "shell.execute_reply": "2021-11-09T05:22:10.628270Z",
     "shell.execute_reply.started": "2021-11-09T05:22:10.582311Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00453873, -0.11112851,  0.21579176,  0.17696957,  0.03097132,\n",
       "       -0.00332653,  0.21296346, -0.14555614,  0.16320467,  0.2612385 ,\n",
       "        0.28736642, -0.0431616 , -0.00923875,  0.23393661, -0.01098115,\n",
       "        0.27839014, -0.07702895,  0.04759218,  0.08082925,  0.06249222,\n",
       "       -0.08819248,  0.15835355,  0.00147007, -0.02063097, -0.19795994,\n",
       "       -0.04415825,  0.15749438, -0.14272755, -0.10232126, -0.11422992,\n",
       "        0.03445603,  0.10564684,  0.04392982,  0.24066718, -0.04029316,\n",
       "       -0.16762559, -0.14931288, -0.00771625, -0.10803408, -0.12887979,\n",
       "       -0.01568735, -0.01220938,  0.29611593, -0.09694148,  0.2767331 ,\n",
       "        0.16787267,  0.00373805,  0.2979738 ,  0.10076348, -0.19827077,\n",
       "       -0.01924698, -0.07885841, -0.1898181 , -0.04336667, -0.16165075,\n",
       "        0.2995827 ,  0.2915928 , -0.20235258, -0.02371357, -0.11707045,\n",
       "        0.21234477,  0.11005617,  0.23166835, -0.02638592,  0.2207434 ,\n",
       "       -0.0574725 , -0.15600303,  0.09550893,  0.15595987, -0.11498156,\n",
       "        0.07196919, -0.21313159, -0.00637859,  0.10436746, -0.1082255 ,\n",
       "       -0.06017696,  0.09728752, -0.19160414,  0.11842944,  0.3082238 ,\n",
       "       -0.05604733,  0.16090457,  0.03746217, -0.0202965 ,  0.23157987,\n",
       "       -0.08463797,  0.05083022, -0.19672482, -0.06704111,  0.2260241 ,\n",
       "        0.17310934, -0.19627218, -0.10712875,  0.00668417, -0.06113172,\n",
       "       -0.19152963,  0.16212952, -0.18495944,  0.11152127, -0.1250079 ,\n",
       "       -0.15728478,  0.22663225,  0.01325833, -0.21531698, -0.03561697,\n",
       "       -0.06672868, -0.12920418,  0.26431236, -0.01094683, -0.20818311,\n",
       "       -0.21115135, -0.20299527, -0.06038294, -0.04126148,  0.19303846,\n",
       "       -0.1443674 ,  0.01753676,  0.10370228, -0.0048534 , -0.10283007,\n",
       "        0.1898619 ,  0.13837408, -0.02619627, -0.08146143,  0.29035148,\n",
       "       -0.02287355, -0.09316688, -0.00660216,  0.        ,  0.        ,\n",
       "       -0.11982618, -0.14864054,  0.10317754,  0.10487086, -0.17870378,\n",
       "        0.        , -0.14866336, -0.18307777, -0.10499071,  0.        ,\n",
       "        0.        , -0.13472924,  0.10829277, -0.09106753,  0.10742447,\n",
       "        0.10798998,  0.09954727,  0.10399488,  0.13048047, -0.11742963,\n",
       "       -0.01411937,  0.        ,  0.        ,  0.        , -0.12088096,\n",
       "        0.        ,  0.        ,  0.        ,  0.10009219,  0.10298176,\n",
       "        0.10051551, -0.128317  ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.10917936,  0.13113554,\n",
       "       -0.18246835,  0.        , -0.18830551, -0.1491116 , -0.01243205,\n",
       "       -0.08286525,  0.09776098,  0.        ,  0.        ,  0.1611022 ,\n",
       "        0.        ,  0.        ,  0.        , -0.06094527, -0.114602  ,\n",
       "        0.        ,  0.        ,  0.        , -0.1681325 ,  0.09999606,\n",
       "       -0.1119628 ,  0.10815864, -0.11107547,  0.        ,  0.        ,\n",
       "        0.11231128, -0.12751183,  0.        ,  0.12732811,  0.        ,\n",
       "        0.        ,  0.1098107 ,  0.13639419,  0.11491869,  0.11341868,\n",
       "        0.        ,  0.10045357, -0.20337532,  0.        , -0.14644769,\n",
       "        0.09948664,  0.        , -0.16923119,  0.        ,  0.10904046,\n",
       "        0.        ,  0.        , -0.16202141, -0.11241868,  0.        ,\n",
       "        0.        , -0.01637699,  0.        ,  0.        , -0.14347535,\n",
       "        0.        ,  0.10130542,  0.        ,  0.        , -0.19892736,\n",
       "       -0.01736566,  0.        ,  0.10324632,  0.        ,  0.        ,\n",
       "       -0.11856332,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14125638,  0.        , -0.11619379,  0.        , -0.01899819,\n",
       "        0.09924427, -0.00600528,  0.12061226, -0.15202627,  0.09797229,\n",
       "        0.1028906 ,  0.        , -0.12369523,  0.        ,  0.        ,\n",
       "        0.1412316 , -0.12458986, -0.17583999, -0.3150643 , -0.4005915 ,\n",
       "        0.14860639,  0.18876868, -0.30849156, -0.19989091, -0.4357531 ,\n",
       "       -0.22027808, -0.2068416 , -0.17268494, -0.17840064, -0.26055753,\n",
       "        0.16135651, -0.24094436,  0.3583675 ,  0.08121044,  0.14130333,\n",
       "        0.09280717,  0.18115929, -0.48211482, -0.01866622, -0.12779051,\n",
       "       -0.18773606, -0.02893648, -0.4847904 ,  0.07073802, -0.12649935,\n",
       "       -0.1542028 ,  0.21802975,  0.0562603 ,  0.18189888, -0.2577515 ,\n",
       "        0.19278449, -0.07952346, -0.14128253, -0.20958756,  0.1637111 ,\n",
       "       -0.06150722,  0.16179146,  0.08137834, -0.21695381,  0.00711253,\n",
       "       -0.23324908, -0.42488706,  0.00688961, -0.21570425,  0.18164371,\n",
       "       -0.03319903,  0.02376047,  0.11628753,  0.11393884,  0.05408227,\n",
       "        0.12044242, -0.28466222, -0.19776276, -0.21380141,  0.10244897,\n",
       "        0.13655037, -0.3473192 ,  0.0876123 , -0.29451624,  0.19439995,\n",
       "       -0.33072093, -0.04478428,  0.13268676,  0.03012392, -0.5099723 ,\n",
       "        0.11095795,  0.02844404,  0.07032135, -0.06979519,  0.02929794,\n",
       "        0.19720635,  0.05321496,  0.02737506,  0.11111304,  0.06997018,\n",
       "       -0.20994933,  0.06920713, -0.45267057,  0.23638125,  0.18963757,\n",
       "       -0.28871238,  0.19613808,  0.07292017, -0.05831952,  0.10164037,\n",
       "       -0.2916385 , -0.41986296, -0.18197675, -0.19025084, -0.04428091,\n",
       "        0.05067778, -0.18943585, -0.4475896 , -0.01544642,  0.06637239,\n",
       "        0.19926196, -0.11270665, -0.4137509 , -0.12375453, -0.1962714 ,\n",
       "        0.3199389 ,  0.0810205 , -0.15210357, -0.22783384, -0.17837653,\n",
       "        0.09391883, -0.04814655,  0.06869304,  0.11773147, -0.11452548,\n",
       "       -0.35511735, -0.05591729,  0.00053095,  0.11355766, -0.0872149 ,\n",
       "        0.2612061 , -0.3655749 ,  0.10271157,  0.29795057,  0.21525353,\n",
       "       -0.19591564, -0.1307618 , -0.12448446,  0.05788094,  0.09480473])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].weight_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:22:10.630004Z",
     "iopub.status.busy": "2021-11-09T05:22:10.629859Z",
     "iopub.status.idle": "2021-11-09T05:24:35.201878Z",
     "shell.execute_reply": "2021-11-09T05:24:35.201343Z",
     "shell.execute_reply.started": "2021-11-09T05:22:10.629986Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1219.75x540 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    clear_output(wait=True)\n",
    "    plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                          function_values_test_list, \n",
    "                                                          polynomial_dict_test_list,\n",
    "                                                          rand_index=i, \n",
    "                                                          plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T05:24:35.203063Z",
     "iopub.status.busy": "2021-11-09T05:24:35.202907Z",
     "iopub.status.idle": "2021-11-09T05:24:37.575381Z",
     "shell.execute_reply": "2021-11-09T05:24:37.574928Z",
     "shell.execute_reply.started": "2021-11-09T05:24:35.203041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1219.75x540 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T05:24:37.576316Z",
     "iopub.status.busy": "2021-11-09T05:24:37.576128Z",
     "iopub.status.idle": "2021-11-09T05:24:41.355634Z",
     "shell.execute_reply": "2021-11-09T05:24:41.355097Z",
     "shell.execute_reply.started": "2021-11-09T05:24:37.576295Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1579.75x1440 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:24:41.356777Z",
     "iopub.status.busy": "2021-11-09T05:24:41.356490Z",
     "iopub.status.idle": "2021-11-09T05:24:42.745807Z",
     "shell.execute_reply": "2021-11-09T05:24:42.745241Z",
     "shell.execute_reply.started": "2021-11-09T05:24:41.356756Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x864 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:24:42.746968Z",
     "iopub.status.busy": "2021-11-09T05:24:42.746814Z",
     "iopub.status.idle": "2021-11-09T05:24:42.792393Z",
     "shell.execute_reply": "2021-11-09T05:24:42.791910Z",
     "shell.execute_reply.started": "2021-11-09T05:24:42.746947Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 140.69811559,  159.16861105,  165.1988008 ,  180.32317209,\n",
       "        159.03151393,  161.48987007,  182.89963317,  188.29129624,\n",
       "        173.19603372,  151.77445674,  169.26832747,  156.30277014,\n",
       "        147.05699968,  175.62777472,  140.99090052,  159.02106738,\n",
       "        140.89572668,  155.0123198 ,  161.65584683,  186.30571485,\n",
       "        164.34345627, 1067.82586241,  159.98646116,  168.96035051,\n",
       "        155.27194476,  155.83562613,  178.44736052,  158.35916138,\n",
       "        146.6774714 ,  152.36468959,   40.77820539,  183.87006021,\n",
       "        156.45176029,  179.21715903,  141.92864847,  157.68163443,\n",
       "         31.06627965, 1217.07494378,  153.27152061,  152.32126808,\n",
       "        145.29995847,  166.72645283,  155.81674147,  182.17632961,\n",
       "        141.91699243,  135.10369968,  156.85119724,  185.52923822,\n",
       "        145.91626906,  152.64603519,  161.3179853 ,  180.16927004,\n",
       "        139.59999681,  144.41095424,  135.21641183,  151.08711481,\n",
       "        191.7274766 ,  153.71188951,  141.41239238,  148.51431108,\n",
       "        173.36161661,  141.48906422,  150.30132318,  183.14608908,\n",
       "        148.56965256,  158.19192624,  168.38650298,  168.69628501,\n",
       "        153.14611697,  317.88798904,  173.53386617,  147.60003519,\n",
       "        152.20694923,  161.91202712,  141.28076029,  169.63622141,\n",
       "        164.77786231,  154.09050822,  180.20908642,   34.6733222 ,\n",
       "        152.24862432,  152.95798874,  177.96107292,  148.25393295,\n",
       "        150.639328  ,  142.77329326,  168.7930069 ,  156.07757211,\n",
       "        160.35052085,  151.75394273,  156.77255821,  150.19332695,\n",
       "        154.80959606,  807.81560302,  152.28051996,  145.26844287,\n",
       "        149.8965807 ,  124.36580729,  127.56311274,  120.29877114])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtimes_list[-1].values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:24:42.793363Z",
     "iopub.status.busy": "2021-11-09T05:24:42.793217Z",
     "iopub.status.idle": "2021-11-09T05:24:42.835652Z",
     "shell.execute_reply": "2021-11-09T05:24:42.835156Z",
     "shell.execute_reply.started": "2021-11-09T05:24:42.793343Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2824.92210317,           nan,           nan, 1720.77770495,\n",
       "                 nan,           nan,           nan, 2295.82182097,\n",
       "                 nan,           nan,  820.1986692 , 2728.12741232,\n",
       "                 nan, 1203.67323065, 2685.54281807,           nan,\n",
       "                 nan, 2456.17545819,           nan,           nan,\n",
       "        619.5149591 ,           nan,           nan, 3205.10443664,\n",
       "        163.43654084,           nan,           nan,  290.23462009,\n",
       "                 nan,  156.1932435 ,           nan,           nan,\n",
       "                 nan,  679.61140633,           nan, 1692.66226387,\n",
       "                 nan,           nan, 2037.93134046,           nan,\n",
       "       2279.51168084,  890.38474917,  865.12774515, 1889.10703468,\n",
       "       2960.8274107 ,           nan, 3169.20232701,           nan,\n",
       "                 nan, 3181.21221638,           nan,  131.63479948,\n",
       "                 nan, 2878.09173131, 1492.36890459, 2040.99637914,\n",
       "                 nan,           nan,           nan,           nan,\n",
       "                 nan,           nan,  469.56799197,           nan,\n",
       "                 nan,           nan,           nan,           nan,\n",
       "       1584.54859018,           nan,           nan,           nan,\n",
       "       2356.84179807,           nan, 2699.97451496,           nan,\n",
       "                 nan,           nan,           nan,           nan,\n",
       "       1348.4455359 , 3360.7415235 ,           nan,           nan,\n",
       "       1587.01558614, 1000.14204288,           nan,  934.34328198,\n",
       "       2760.69757676, 2883.0013907 ,           nan,           nan,\n",
       "       1034.79130125,           nan,           nan,  727.28426337,\n",
       "       3296.42342567,           nan,           nan,           nan])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtimes_list[-1].values[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:24:42.836574Z",
     "iopub.status.busy": "2021-11-09T05:24:42.836430Z",
     "iopub.status.idle": "2021-11-09T05:24:42.880299Z",
     "shell.execute_reply": "2021-11-09T05:24:42.879831Z",
     "shell.execute_reply.started": "2021-11-09T05:24:42.836555Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.80153036e-03, 1.40698116e+02, 2.82492210e+03],\n",
       "       [2.80153036e-03, 1.59168611e+02,            nan],\n",
       "       [2.80153036e-03, 1.65198801e+02,            nan],\n",
       "       [2.80153036e-03, 1.80323172e+02, 1.72077770e+03],\n",
       "       [2.80153036e-03, 1.59031514e+02,            nan],\n",
       "       [2.80153036e-03, 1.61489870e+02,            nan],\n",
       "       [2.80153036e-03, 1.82899633e+02,            nan],\n",
       "       [2.80153036e-03, 1.88291296e+02, 2.29582182e+03],\n",
       "       [2.80153036e-03, 1.73196034e+02,            nan],\n",
       "       [2.80153036e-03, 1.51774457e+02,            nan],\n",
       "       [2.80153036e-03, 1.69268327e+02, 8.20198669e+02],\n",
       "       [2.80153036e-03, 1.56302770e+02, 2.72812741e+03],\n",
       "       [2.80153036e-03, 1.47057000e+02,            nan],\n",
       "       [2.80153036e-03, 1.75627775e+02, 1.20367323e+03],\n",
       "       [2.80153036e-03, 1.40990901e+02, 2.68554282e+03],\n",
       "       [2.80153036e-03, 1.59021067e+02,            nan],\n",
       "       [2.80153036e-03, 1.40895727e+02,            nan],\n",
       "       [2.80153036e-03, 1.55012320e+02, 2.45617546e+03],\n",
       "       [2.80153036e-03, 1.61655847e+02,            nan],\n",
       "       [2.80153036e-03, 1.86305715e+02,            nan],\n",
       "       [2.80153036e-03, 1.64343456e+02, 6.19514959e+02],\n",
       "       [2.80153036e-03, 1.06782586e+03,            nan],\n",
       "       [2.80153036e-03, 1.59986461e+02,            nan],\n",
       "       [2.80153036e-03, 1.68960351e+02, 3.20510444e+03],\n",
       "       [2.80153036e-03, 1.55271945e+02, 1.63436541e+02],\n",
       "       [2.80153036e-03, 1.55835626e+02,            nan],\n",
       "       [2.80153036e-03, 1.78447361e+02,            nan],\n",
       "       [2.80153036e-03, 1.58359161e+02, 2.90234620e+02],\n",
       "       [2.80153036e-03, 1.46677471e+02,            nan],\n",
       "       [2.80153036e-03, 1.52364690e+02, 1.56193244e+02],\n",
       "       [2.80153036e-03, 4.07782054e+01,            nan],\n",
       "       [2.80153036e-03, 1.83870060e+02,            nan],\n",
       "       [2.80153036e-03, 1.56451760e+02,            nan],\n",
       "       [2.80153036e-03, 1.79217159e+02, 6.79611406e+02],\n",
       "       [2.80153036e-03, 1.41928648e+02,            nan],\n",
       "       [2.80153036e-03, 1.57681634e+02, 1.69266226e+03],\n",
       "       [2.80153036e-03, 3.10662796e+01,            nan],\n",
       "       [2.80153036e-03, 1.21707494e+03,            nan],\n",
       "       [2.80153036e-03, 1.53271521e+02, 2.03793134e+03],\n",
       "       [2.80153036e-03, 1.52321268e+02,            nan],\n",
       "       [2.80153036e-03, 1.45299958e+02, 2.27951168e+03],\n",
       "       [2.80153036e-03, 1.66726453e+02, 8.90384749e+02],\n",
       "       [2.80153036e-03, 1.55816741e+02, 8.65127745e+02],\n",
       "       [2.80153036e-03, 1.82176330e+02, 1.88910703e+03],\n",
       "       [2.80153036e-03, 1.41916992e+02, 2.96082741e+03],\n",
       "       [2.80153036e-03, 1.35103700e+02,            nan],\n",
       "       [2.80153036e-03, 1.56851197e+02, 3.16920233e+03],\n",
       "       [2.80153036e-03, 1.85529238e+02,            nan],\n",
       "       [2.80153036e-03, 1.45916269e+02,            nan],\n",
       "       [2.80153036e-03, 1.52646035e+02, 3.18121222e+03],\n",
       "       [2.80153036e-03, 1.61317985e+02,            nan],\n",
       "       [2.80153036e-03, 1.80169270e+02, 1.31634799e+02],\n",
       "       [2.80153036e-03, 1.39599997e+02,            nan],\n",
       "       [2.80153036e-03, 1.44410954e+02, 2.87809173e+03],\n",
       "       [2.80153036e-03, 1.35216412e+02, 1.49236890e+03],\n",
       "       [2.80153036e-03, 1.51087115e+02, 2.04099638e+03],\n",
       "       [2.80153036e-03, 1.91727477e+02,            nan],\n",
       "       [2.80153036e-03, 1.53711890e+02,            nan],\n",
       "       [2.80153036e-03, 1.41412392e+02,            nan],\n",
       "       [2.80153036e-03, 1.48514311e+02,            nan],\n",
       "       [2.80153036e-03, 1.73361617e+02,            nan],\n",
       "       [2.80153036e-03, 1.41489064e+02,            nan],\n",
       "       [2.80153036e-03, 1.50301323e+02, 4.69567992e+02],\n",
       "       [2.80153036e-03, 1.83146089e+02,            nan],\n",
       "       [2.80153036e-03, 1.48569653e+02,            nan],\n",
       "       [2.80153036e-03, 1.58191926e+02,            nan],\n",
       "       [2.80153036e-03, 1.68386503e+02,            nan],\n",
       "       [2.80153036e-03, 1.68696285e+02,            nan],\n",
       "       [2.80153036e-03, 1.53146117e+02, 1.58454859e+03],\n",
       "       [2.80153036e-03, 3.17887989e+02,            nan],\n",
       "       [2.80153036e-03, 1.73533866e+02,            nan],\n",
       "       [2.80153036e-03, 1.47600035e+02,            nan],\n",
       "       [2.80153036e-03, 1.52206949e+02, 2.35684180e+03],\n",
       "       [2.80153036e-03, 1.61912027e+02,            nan],\n",
       "       [2.80153036e-03, 1.41280760e+02, 2.69997451e+03],\n",
       "       [2.80153036e-03, 1.69636221e+02,            nan],\n",
       "       [2.80153036e-03, 1.64777862e+02,            nan],\n",
       "       [2.80153036e-03, 1.54090508e+02,            nan],\n",
       "       [2.80153036e-03, 1.80209086e+02,            nan],\n",
       "       [2.80153036e-03, 3.46733222e+01,            nan],\n",
       "       [2.80153036e-03, 1.52248624e+02, 1.34844554e+03],\n",
       "       [2.80153036e-03, 1.52957989e+02, 3.36074152e+03],\n",
       "       [2.80153036e-03, 1.77961073e+02,            nan],\n",
       "       [2.80153036e-03, 1.48253933e+02,            nan],\n",
       "       [2.80153036e-03, 1.50639328e+02, 1.58701559e+03],\n",
       "       [2.80153036e-03, 1.42773293e+02, 1.00014204e+03],\n",
       "       [2.80153036e-03, 1.68793007e+02,            nan],\n",
       "       [2.80153036e-03, 1.56077572e+02, 9.34343282e+02],\n",
       "       [2.80153036e-03, 1.60350521e+02, 2.76069758e+03],\n",
       "       [2.80153036e-03, 1.51753943e+02, 2.88300139e+03],\n",
       "       [2.80153036e-03, 1.56772558e+02,            nan],\n",
       "       [2.80153036e-03, 1.50193327e+02,            nan],\n",
       "       [2.80153036e-03, 1.54809596e+02, 1.03479130e+03],\n",
       "       [2.80153036e-03, 8.07815603e+02,            nan],\n",
       "       [2.80153036e-03, 1.52280520e+02,            nan],\n",
       "       [2.80153036e-03, 1.45268443e+02, 7.27284263e+02],\n",
       "       [2.80153036e-03, 1.49896581e+02, 3.29642343e+03],\n",
       "       [2.80153036e-03, 1.24365807e+02,            nan],\n",
       "       [2.80153036e-03, 1.27563113e+02,            nan],\n",
       "       [2.80153036e-03, 1.20298771e+02,            nan]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtimes_list[-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:24:42.881411Z",
     "iopub.status.busy": "2021-11-09T05:24:42.881126Z",
     "iopub.status.idle": "2021-11-09T05:24:42.923114Z",
     "shell.execute_reply": "2021-11-09T05:24:42.922629Z",
     "shell.execute_reply.started": "2021-11-09T05:24:42.881391Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02199632, 0.02597777, 0.00981954, 0.00365153, 0.01485219,\n",
       "       0.0125015 , 0.01946353, 0.00701185, 0.00212021, 0.04421651,\n",
       "       0.00596291, 0.03085378, 0.00505773, 0.00492517, 0.03551589,\n",
       "       0.04111831, 0.05671116, 0.02028508, 0.02469802, 0.01636918,\n",
       "       0.00719718, 0.02208947, 0.02654091, 0.01435337, 0.00912373,\n",
       "       0.01209569, 0.0136839 , 0.01150517, 0.21306372, 0.00440717,\n",
       "       0.02031691, 0.0161034 , 0.00905194, 0.00726655, 0.02602512,\n",
       "       0.0356048 , 0.01969463, 0.02730483, 0.03221708, 0.0086889 ,\n",
       "       0.00730116, 0.0236337 , 0.00942989, 0.01002782, 0.00819136,\n",
       "       0.02020949, 0.00520556, 0.0259672 , 0.03713069, 0.00931987,\n",
       "       0.02709139, 0.00602789, 0.02109173, 0.0187728 , 0.06597324,\n",
       "       0.00870227, 0.01331592, 0.044572  , 0.01949307, 0.0073276 ,\n",
       "       0.0159371 , 0.13725965, 0.00729855, 0.01448597, 0.02926321,\n",
       "       0.03652346, 0.0319932 , 0.02309566, 0.01363053, 0.01881584,\n",
       "       0.0030203 , 0.05034573, 0.04436707, 0.00537341, 0.01415469,\n",
       "       0.01193171, 0.01545702, 0.02720635, 0.00898423, 0.01223985,\n",
       "       0.01193443, 0.01951637, 0.02484992, 0.00589201, 0.00936422,\n",
       "       0.01369976, 0.00968716, 0.00333429, 0.12761816, 0.08802149,\n",
       "       0.0208698 , 0.01760583, 0.00366954, 0.0260167 , 0.03429287,\n",
       "       0.0199385 , 0.00726615, 0.01867094, 0.0152952 , 0.02479498],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-09T05:24:42.924124Z",
     "iopub.status.busy": "2021-11-09T05:24:42.923979Z",
     "iopub.status.idle": "2021-11-09T05:24:44.806625Z",
     "shell.execute_reply": "2021-11-09T05:24:44.805584Z",
     "shell.execute_reply.started": "2021-11-09T05:24:42.924104Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lambda_preds_VS_symbolic_regression_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lambda_preds_VS_symbolic_regression_functions'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2590098/3823781696.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistrib_dict_test_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda_preds_VS_symbolic_regression_functions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3774\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected label or tuple of labels, got {key}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3775\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3776\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3778\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lambda_preds_VS_symbolic_regression_functions'"
     ]
    }
   ],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_symbolic_regression_functions'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.807719Z",
     "iopub.status.idle": "2021-11-09T05:24:44.808098Z",
     "shell.execute_reply": "2021-11-09T05:24:44.807915Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.807894Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_metamodel_functions'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.809518Z",
     "iopub.status.idle": "2021-11-09T05:24:44.810002Z",
     "shell.execute_reply": "2021-11-09T05:24:44.809815Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.809792Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.811295Z",
     "iopub.status.idle": "2021-11-09T05:24:44.811820Z",
     "shell.execute_reply": "2021-11-09T05:24:44.811631Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.811609Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_symbolic_regression_functions'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.813174Z",
     "iopub.status.idle": "2021-11-09T05:24:44.813551Z",
     "shell.execute_reply": "2021-11-09T05:24:44.813366Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.813345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_metamodel_functions'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.814947Z",
     "iopub.status.idle": "2021-11-09T05:24:44.815317Z",
     "shell.execute_reply": "2021-11-09T05:24:44.815137Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.815116Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    },
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.816569Z",
     "iopub.status.idle": "2021-11-09T05:24:44.817076Z",
     "shell.execute_reply": "2021-11-09T05:24:44.816891Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.816869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    },
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.818484Z",
     "iopub.status.idle": "2021-11-09T05:24:44.818856Z",
     "shell.execute_reply": "2021-11-09T05:24:44.818674Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.818654Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    },
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.820129Z",
     "iopub.status.idle": "2021-11-09T05:24:44.820628Z",
     "shell.execute_reply": "2021-11-09T05:24:44.820442Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.820421Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    },
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.821915Z",
     "iopub.status.idle": "2021-11-09T05:24:44.822432Z",
     "shell.execute_reply": "2021-11-09T05:24:44.822245Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.822223Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    },
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.823720Z",
     "iopub.status.idle": "2021-11-09T05:24:44.824237Z",
     "shell.execute_reply": "2021-11-09T05:24:44.824052Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.824030Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    },
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.825491Z",
     "iopub.status.idle": "2021-11-09T05:24:44.826110Z",
     "shell.execute_reply": "2021-11-09T05:24:44.825917Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.825893Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ],
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.827251Z",
     "iopub.status.idle": "2021-11-09T05:24:44.827733Z",
     "shell.execute_reply": "2021-11-09T05:24:44.827545Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.827523Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.829123Z",
     "iopub.status.idle": "2021-11-09T05:24:44.829578Z",
     "shell.execute_reply": "2021-11-09T05:24:44.829369Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.829336Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if evaluate_with_real_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.830980Z",
     "iopub.status.idle": "2021-11-09T05:24:44.831449Z",
     "shell.execute_reply": "2021-11-09T05:24:44.831262Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.831240Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.832697Z",
     "iopub.status.idle": "2021-11-09T05:24:44.833068Z",
     "shell.execute_reply": "2021-11-09T05:24:44.832886Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.832865Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.834381Z",
     "iopub.status.idle": "2021-11-09T05:24:44.834880Z",
     "shell.execute_reply": "2021-11-09T05:24:44.834693Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.834671Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.836218Z",
     "iopub.status.idle": "2021-11-09T05:24:44.836709Z",
     "shell.execute_reply": "2021-11-09T05:24:44.836524Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.836502Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.837880Z",
     "iopub.status.idle": "2021-11-09T05:24:44.838409Z",
     "shell.execute_reply": "2021-11-09T05:24:44.838223Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.838200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.839971Z",
     "iopub.status.idle": "2021-11-09T05:24:44.840531Z",
     "shell.execute_reply": "2021-11-09T05:24:44.840318Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.840284Z"
    }
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.841747Z",
     "iopub.status.idle": "2021-11-09T05:24:44.842118Z",
     "shell.execute_reply": "2021-11-09T05:24:44.841936Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.841915Z"
    }
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.843582Z",
     "iopub.status.idle": "2021-11-09T05:24:44.844028Z",
     "shell.execute_reply": "2021-11-09T05:24:44.843841Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.843819Z"
    }
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.845188Z",
     "iopub.status.idle": "2021-11-09T05:24:44.845657Z",
     "shell.execute_reply": "2021-11-09T05:24:44.845472Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.845450Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.846828Z",
     "iopub.status.idle": "2021-11-09T05:24:44.847354Z",
     "shell.execute_reply": "2021-11-09T05:24:44.847166Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.847144Z"
    }
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.848844Z",
     "iopub.status.idle": "2021-11-09T05:24:44.849365Z",
     "shell.execute_reply": "2021-11-09T05:24:44.849181Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.849159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.850683Z",
     "iopub.status.idle": "2021-11-09T05:24:44.851171Z",
     "shell.execute_reply": "2021-11-09T05:24:44.850985Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.850962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.852715Z",
     "iopub.status.idle": "2021-11-09T05:24:44.853092Z",
     "shell.execute_reply": "2021-11-09T05:24:44.852908Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.852886Z"
    }
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.854230Z",
     "iopub.status.idle": "2021-11-09T05:24:44.854725Z",
     "shell.execute_reply": "2021-11-09T05:24:44.854537Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.854514Z"
    }
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.855991Z",
     "iopub.status.idle": "2021-11-09T05:24:44.856615Z",
     "shell.execute_reply": "2021-11-09T05:24:44.856374Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.856345Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.858240Z",
     "iopub.status.idle": "2021-11-09T05:24:44.858805Z",
     "shell.execute_reply": "2021-11-09T05:24:44.858570Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.858543Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.860204Z",
     "iopub.status.idle": "2021-11-09T05:24:44.860998Z",
     "shell.execute_reply": "2021-11-09T05:24:44.860778Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.860752Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.862467Z",
     "iopub.status.idle": "2021-11-09T05:24:44.863211Z",
     "shell.execute_reply": "2021-11-09T05:24:44.862974Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.862945Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.864648Z",
     "iopub.status.idle": "2021-11-09T05:24:44.865239Z",
     "shell.execute_reply": "2021-11-09T05:24:44.864992Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.864962Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.867074Z",
     "iopub.status.idle": "2021-11-09T05:24:44.867728Z",
     "shell.execute_reply": "2021-11-09T05:24:44.867506Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.867479Z"
    }
   },
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.869334Z",
     "iopub.status.idle": "2021-11-09T05:24:44.869930Z",
     "shell.execute_reply": "2021-11-09T05:24:44.869710Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.869685Z"
    }
   },
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.871522Z",
     "iopub.status.idle": "2021-11-09T05:24:44.872226Z",
     "shell.execute_reply": "2021-11-09T05:24:44.872004Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.871978Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.877629Z",
     "iopub.status.idle": "2021-11-09T05:24:44.878164Z",
     "shell.execute_reply": "2021-11-09T05:24:44.877942Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.877917Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.879721Z",
     "iopub.status.idle": "2021-11-09T05:24:44.880244Z",
     "shell.execute_reply": "2021-11-09T05:24:44.880022Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.879996Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.881959Z",
     "iopub.status.idle": "2021-11-09T05:24:44.882487Z",
     "shell.execute_reply": "2021-11-09T05:24:44.882267Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.882242Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-09T05:24:44.883934Z",
     "iopub.status.idle": "2021-11-09T05:24:44.884448Z",
     "shell.execute_reply": "2021-11-09T05:24:44.884231Z",
     "shell.execute_reply.started": "2021-11-09T05:24:44.884205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.003425\n",
      "         Iterations: 12\n",
      "         Function evaluations: 263\n",
      "         Gradient evaluations: 85\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000045\n",
      "         Iterations: 33\n",
      "         Function evaluations: 754\n",
      "         Gradient evaluations: 124\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000171\n",
      "         Iterations: 10\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000022\n",
      "         Iterations: 34\n",
      "         Function evaluations: 546\n",
      "         Gradient evaluations: 91\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.164514\n",
      "         Iterations: 6\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 14\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.164514\n",
      "         Iterations: 3\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 12\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.001673\n",
      "         Iterations: 50\n",
      "         Function evaluations: 1084\n",
      "         Gradient evaluations: 179\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.139117\n",
      "         Iterations: 15\n",
      "         Function evaluations: 439\n",
      "         Gradient evaluations: 107\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000169\n",
      "         Iterations: 8\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 29\n",
      "\n",
      "None\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000415\n",
      "         Iterations: 51\n",
      "         Function evaluations: 570\n",
      "         Gradient evaluations: 186\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003366\n",
      "         Iterations: 6\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 18\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000627\n",
      "         Iterations: 40\n",
      "         Function evaluations: 651\n",
      "         Gradient evaluations: 107\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.003350\n",
      "         Iterations: 11\n",
      "         Function evaluations: 209\n",
      "         Gradient evaluations: 51\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.051688\n",
      "         Iterations: 19\n",
      "         Function evaluations: 135\n",
      "         Gradient evaluations: 45\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.353534\n",
      "         Iterations: 5\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 13\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.353534\n",
      "         Iterations: 5\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 18\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000171\n",
      "         Iterations: 74\n",
      "         Function evaluations: 1272\n",
      "         Gradient evaluations: 211\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.325600\n",
      "         Iterations: 3\n",
      "         Function evaluations: 128\n",
      "         Gradient evaluations: 29\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001099\n",
      "         Iterations: 7\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 24\n",
      "\n",
      "None\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.002941\n",
      "         Iterations: 22\n",
      "         Function evaluations: 195\n",
      "         Gradient evaluations: 65\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n",
      "Traceback (most recent call last):\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/utilities/utility_functions.py\", line 1847, in symbolic_metamodeling\n",
      "    symbolic_model, r2_score = get_symbolic_model(model, metamodeling_hyperparams['dataset_size'], [x_min, x_max], n_vars=config['n'])\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 101, in get_symbolic_model\n",
      "    symbolic_model, Loss_ = symbolic_modeling(f, hyperparameter_space['hyper_'+str(k+1)][1],\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 86, in symbolic_modeling\n",
      "    theta_opt, Loss_ = Optimize(Loss, theta_0)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 55, in Optimize\n",
      "    opt       = minimize(Loss, theta_0, method='CG', options={'xtol': 1e-8, 'disp': True})\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 616, in minimize\n",
      "    return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1586, in _minimize_cg\n",
      "    _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 1005, in _line_search_wolfe12\n",
      "    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 96, in line_search_wolfe1\n",
      "    stp, fval, old_fval = scalar_search_wolfe1(\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 173, in scalar_search_wolfe1\n",
      "    derphi1 = derphi(stp)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/linesearch.py\", line 87, in derphi\n",
      "    gval[0] = fprime(xk + s*pk, *newargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 255, in grad\n",
      "    self._update_grad()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 238, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 486, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 557, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 437, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 134, in fun_wrapped\n",
      "    return fun(np.copy(x), *args)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/algorithms/symbolic_expressions.py\", line 78, in Loss\n",
      "    loss_ = np.mean((f.predict(X) - G.evaluate(X))**2)\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 145, in evaluate\n",
      "    'theano': lambdify([x], self.approx_expression(), modules=['math'])} #theano_function([x], [self.approx_expression()])}\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 115, in approx_expression\n",
      "    self.Taylor_poly_ = taylor(self.math_expr, midpoint, self.approximation_order)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in taylor\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 576, in <listcomp>\n",
      "    return [ctx.chop(d)/ctx.factorial(i) for i, d in gen]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 284, in diffs\n",
      "    y, norm, workprec = hsteps(ctx, f, x, B, callprec, **options)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in hsteps\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/calculus/differentiation.py\", line 61, in <listcomp>\n",
      "    values = [f(x+k*h) for k in steps]\n",
      "  File \"/work-ceph/smarton/InES_XAI/02_polynomials/pysymbolic_adjusted/models/special_functions.py\", line 106, in math_expr\n",
      "    return mp.meijerg(a_p_, b_q_, self._const * x)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 1058, in meijerg\n",
      "    return ctx.hypercomb(h, a+b, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 456, in _hyp2f1\n",
      "    v = ctx.hypercomb(h, [a,b], **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 125, in hypercomb\n",
      "    v = ctx.fprod([ctx.hyper(a_s, b_s, z, **kwargs)] + \\\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 226, in hyper\n",
      "    if   q == 1: return ctx._hyp2f1(a_s, b_s, z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/functions/hypergeometric.py\", line 443, in _hyp2f1\n",
      "    return ctx.hypsum(2, 1, (atype, btype, ctype), [a, b, c], z, **kwargs)\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/mpmath/ctx_mp.py\", line 714, in hypsum\n",
      "    zv, have_complex, magnitude = summator(coeffs, v, prec, wp, \\\n",
      "  File \"<string>\", line 43, in hypsum_2_1_RR_R_R\n",
      "  File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/interruptingcow/__init__.py\", line 74, in handler\n",
      "    raise exception\n",
      "RuntimeError\n"
     ]
    }
   ],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
