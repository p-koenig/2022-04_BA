{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd1549c-3ec6-40dd-a7cf-69cd5beeca85",
   "metadata": {},
   "source": [
    "# Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f8e753-3ed4-47c2-a508-794c5ccff2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data': {\n",
    "        'n_datasets': 10_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 10, # the number of features\n",
    "        \n",
    "        'n_informative': 10,\n",
    "        # The number of informative features, i.e., the number of features used to build the linear model used to generate the output.\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar.\n",
    "    \n",
    "        'bias': 0.0,\n",
    "        # The bias term in the underlying linear model\n",
    "        \n",
    "        'effective_rank': 5,\n",
    "        # if not None:\n",
    "            # The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice.\n",
    "        # if None:\n",
    "            # The input set is well conditioned, centered and gaussian with unit variance.\n",
    "        \n",
    "        'tail_strength': 0.5,\n",
    "        # The relative importance of the fat noisy tail of the singular values profile if effective_rank is not None. When a float, it should be between 0 and 1.\n",
    "        \n",
    "        'noise': 5,\n",
    "        # The standard deviation of the gaussian noise applied to the output.\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': None,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },    \n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 150,\n",
    "            'verbose': '0',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 100,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '4',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24522e9e-6237-4c8e-b502-e555da2fef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 10:20:47.328469: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-18 10:20:47.328506: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44bbe3e-16e1-45f1-92cf-88dbd8a87318",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = config['computation']['gpu_numbers'] if config['computation']['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if config['computation']['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if config['computation']['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if config['computation']['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ca8427-e1ef-419f-a901-2f010ffea5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 10:20:50.251155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-18 10:20:50.251204: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-18 10:20:50.251239: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dws-02): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d80c2-ba75-403e-a3d1-6611a228d86f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6ef2fa-cee5-42ca-b1a5-b5fcf14185d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_model(datasetIndex):\n",
    " #   # with open(utilities.lambda_path(config, datasetIndex), 'r') as f:\n",
    "  #  #     model = keras.models.load_model(f)\n",
    "   # model = keras.models.load_model(utilities.lambda_path(config, datasetIndex))\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81c7b58-f1b9-468d-acfd-403f48b9febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_weightdata(datasetIndex):\n",
    "  #   weights_list = load_model(datasetIndex).get_weights()\n",
    "    # return np.concatenate([x.flatten() for x in weights_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21aa2853-27a3-40e7-94ee-7458467f7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lambda():\n",
    "    directory = utilities.lambda_path_LR(config)\n",
    "    \n",
    "    with open(directory + '/lambda_weights_list.npy', \"rb\") as f:\n",
    "        return np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d13a520-a726-4262-87db-6a0b41dd767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_models():\n",
    "#    # return np.array([get_weights(i) for i in range(config['data']['n_datasets'])])\n",
    "#    # return np.array([get_weightdata(i) for i in range(10)])\n",
    "#    with open(utilities.lambda_path_LR(config, 0) + '.npy', 'rb') as f:\n",
    "#        weights_list = np.load(f, allow_pickle=True)\n",
    "#    for i in range(1, config['data']['n_datasets']):\n",
    "#        with open(utilities.lambda_path(config, i) + '.npy', 'rb') as f:\n",
    "#            weights = np.load(f, allow_pickle=True)\n",
    "#        weights_list = np.vstack([weights_list, weights])\n",
    "#    return weights_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e2ee23-b5a6-40a9-ad60-216984958f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coef():\n",
    "    return np.load(utilities.data_path_LR(config) + '/coef_list_targetForInet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75fc6c1a-e279-438d-81dd-3000dd58b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=config['inets']['data_prep']['train_test_val_split']['test_size'] + config['inets']['data_prep']['train_test_val_split']['val_size'], \n",
    "                                                        random_state=config['inets']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                        shuffle=config['inets']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                        stratify=config['inets']['data_prep']['train_test_val_split']['stratify'])\n",
    "    X_test, X_val, y__test, y_val = train_test_split(X_test, \n",
    "                                                    y_test, \n",
    "                                                    test_size=config['inets']['data_prep']['train_test_val_split']['val_size'] / (config['inets']['data_prep']['train_test_val_split']['test_size'] + config['inets']['data_prep']['train_test_val_split']['val_size']), \n",
    "                                                    random_state=config['inets']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                    shuffle=config['inets']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                    stratify=config['inets']['data_prep']['train_test_val_split']['stratify'])\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16620e0f-0109-4177-b0a4-23afaf92a886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    path = utilities.inet_path_LR(config)\n",
    "    \n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    weights_list = model.get_weights()\n",
    "    \n",
    "    weights_linearized = np.concatenate([x.flatten() for x in weights_list])\n",
    "    \n",
    "    with open(path + '/inet_weights.npy', \"wb\") as f:\n",
    "        np.save(f, weights_linearized, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974df36-5052-4bbc-a191-4aadc84d02c1",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d47754df-11d9-4365-ba85-5d5ceb10bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e0a7289-6d7b-4817-88e1-393afcd91b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7261)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32794f7-c1f7-4523-b282-8d06e49cd305",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = get_coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb245c6-7f0c-4214-8655-e73ea7fb1963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca40ac-3db7-4eaf-b8c5-eee5c119ecb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 10:20:52.761959: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 7261)             29044     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6000)              43572000  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5000)              30005000  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4000)              20004000  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              4001000   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                10010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,621,054\n",
      "Trainable params: 97,606,532\n",
      "Non-trainable params: 14,522\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "219/219 [==============================] - 53s 238ms/step - loss: 49.0571 - mae: 49.0571 - root_mean_squared_error: 56.8933 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 2/1000\n",
      "219/219 [==============================] - 51s 235ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 3/1000\n",
      "219/219 [==============================] - 51s 235ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 4/1000\n",
      "219/219 [==============================] - 52s 236ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 5/1000\n",
      "219/219 [==============================] - 51s 234ms/step - loss: 49.0549 - mae: 49.0549 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 6/1000\n",
      "219/219 [==============================] - 51s 235ms/step - loss: 49.0549 - mae: 49.0549 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 7/1000\n",
      "219/219 [==============================] - 51s 235ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 8/1000\n",
      "219/219 [==============================] - 51s 235ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 9/1000\n",
      "219/219 [==============================] - 51s 235ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 10/1000\n",
      "219/219 [==============================] - 51s 235ms/step - loss: 49.0549 - mae: 49.0549 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 11/1000\n",
      "219/219 [==============================] - 52s 236ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 12/1000\n",
      "219/219 [==============================] - 52s 235ms/step - loss: 49.0549 - mae: 49.0549 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 13/1000\n",
      "219/219 [==============================] - 52s 236ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 14/1000\n",
      "219/219 [==============================] - 51s 232ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 15/1000\n",
      "219/219 [==============================] - 51s 231ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 16/1000\n",
      "219/219 [==============================] - 51s 231ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 17/1000\n",
      "219/219 [==============================] - 51s 233ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 18/1000\n",
      "219/219 [==============================] - 51s 231ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 19/1000\n",
      "219/219 [==============================] - 51s 231ms/step - loss: 49.0550 - mae: 49.0550 - root_mean_squared_error: 56.8914 - val_loss: 48.6468 - val_mae: 48.6468 - val_root_mean_squared_error: 56.5096\n",
      "Epoch 20/1000\n",
      "181/219 [=======================>......] - ETA: 8s - loss: 49.0023 - mae: 49.0023 - root_mean_squared_error: 56.8456"
     ]
    }
   ],
   "source": [
    "# Data Prep\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,\n",
    "                                                                      y)\n",
    "\n",
    "# Model Def\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_dim=X.shape[1]))\n",
    "model.add(Dense(6000, activation='relu'))\n",
    "model.add(Dense(5000, activation='relu'))\n",
    "model.add(Dense(4000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['mae', keras.metrics.RootMeanSquaredError()])\n",
    "model.summary()\n",
    "\n",
    "# Model fit\n",
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    batch_size=config['inets']['model_fit']['batch_size'],\n",
    "                    epochs=config['inets']['model_fit']['epochs'],\n",
    "                    verbose=config['inets']['model_fit']['verbose'],\n",
    "                    callbacks=config['inets']['model_fit']['callbacks'],\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    shuffle=config['inets']['model_fit']['shuffle'],\n",
    "                    class_weight=config['inets']['model_fit']['class_weight'],\n",
    "                    sample_weight=config['inets']['model_fit']['sample_weight'],\n",
    "                    initial_epoch=config['inets']['model_fit']['initial_epoch'],\n",
    "                    steps_per_epoch=config['inets']['model_fit']['steps_per_epoch'],\n",
    "                    validation_steps=config['inets']['model_fit']['validation_steps'],\n",
    "                    validation_batch_size=config['inets']['model_fit']['validation_batch_size'],\n",
    "                    validation_freq=config['inets']['model_fit']['validation_freq'],\n",
    "                   )\n",
    "print(history.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1961b-2c3d-4a8d-b9e1-2cace3d8f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a489da-c45b-4679-8bb8-aa0cc2545ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6acda-dd96-41f9-b841-16462ad6c4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
